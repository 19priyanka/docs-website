{
  "/docs/integrations/kubernetes-integration/troubleshooting/not-seeing-control-plane-data": [
    {
      "sections": [
        "Monitor services running on Kubernetes",
        "Get started",
        "What you need",
        "Enable monitoring of services",
        "Get the config YAML for the integration",
        "Example configuration",
        "Configuration options for each integration",
        "Monitor services in our Kubernetes integration installed with Helm",
        "Learn more",
        "Manually configure service monitoring",
        "How the service-specific YAML config works",
        "Add a service YAML to the Kubernetes integration config",
        "Add multiple services to the same config"
      ],
      "title": "Monitor services running on Kubernetes",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Link apps and services"
      ],
      "external_id": "4c67f6272bda36eda4ad7883e89697a203aa2153",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/link-apps-services/monitor-services-running-kubernetes/",
      "published_at": "2021-10-07T10:06:32Z",
      "updated_at": "2021-10-07T10:06:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic's Kubernetes integration you can monitor both Kubernetes and the services running on it, such as Cassandra, Redis, MySQL, and other supported services. Get started Our Kubernetes integration comes bundled with some of our on-host integrations (like Cassandra, MySQL, and Apache). This lets you get data for those supported services by adding a section to the Kubernetes integration's configuration, which lives as a ConfigMap inside a manifest. What you need Enable this feature for a service Details about how configuration works For an example of how to monitor Redis running on a Kubernetes PHP Guestbook, see this tutorial. What you need To monitor services running on Kubernetes, you only need a Kubernetes cluster running the Kubernetes integration, version 1.16.0 or higher (install | check version | update). We support the following services running on Kubernetes: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP Enable monitoring of services To enable our Kubernetes integration to monitor one or more services: Expand this dropdown and get the YAML snippets for the service(s) you want to monitor: Get the config YAML for the integration For the services you want to monitor, follow the links to GitHub to get the YAML snippets you'll need for the next step: Apache Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ Redis SNMP Add the snippet to the Kubernetes integration's ConfigMap, after the data: section: Example configuration This example shows the YAML config for the Apache integration ( highlighted ) added to the Kubernetes integration's config. Respect the indentation levels. --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: apache-config.yaml: | --- # Run auto discovery to find pods with label \"app=apache\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Use the optional arguments: # --namespaces: Comma separated namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: apache integrations: - name: nri-apache env: # Use the discovered IP as the host address STATUS_URL: http://${discovery.ip}/server-status?auto METRICS: 1 Copy You can add snippets for multiple services to the same config file. See an example. Depending on your environment, you may need or want to set additional config options. Expand the dropdown below for links to configuration options. Configuration options for each integration Select a service to see available config options: Apache Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ Redis SNMP Verify monitoring is enabled: Go to one.newrelic.com > Infrastructure, select Third party services, and then select the service's dashboard. You should see data being reported. Additional notes about enabling services: Enabling multiple services may use more resources than what is set in the resource limits of the Kubernetes integration config file. If this becomes an issue, raise the limit in the resources section. The Kubernetes integration does not automatically update. For best results, regularly update. Monitor services in our Kubernetes integration installed with Helm If you installed our Kubernetes integration using Helm, to monitor services you need to update the existing installation with the new configuration, which contains the services to monitor: helm upgrade --reuse-values -f values.yaml [RELEASE] [CHART] Copy If you use nri-bundle charts, you need to update the children's chart values. Find some examples here. Learn more More resources for learning about configuration: Learn technical details about how configuration works. Learn how to configure monitoring of multiple services with the same config file. See a step-by-step tutorial showing how to monitor a Redis service on Kubernetes. Manually configure service monitoring The enable procedure should be all you need to get monitoring working, but if you run into problems, understanding some technical details about configuration can be helpful. This section goes into more detail about how configuration works. For each service you wish to monitor, you must add a configuration file for that integration to our Kubernetes integration's configuration. This document will cover these subjects: How the service-specific configuration YAML snippet works Adding the service-specific YAML in the Kubernetes integration's config file Adding multiple services to the Kubernetes integration's config file How the service-specific YAML config works Our Kubernetes integration's configuration follows the ConfigMap format. Using a ConfigMap allows us to decouple the configuration for the integrations from the Kubernetes image. The other benefit is that a ConfigMap can be updated automatically without reloading the running container. Because the infrastructure agent uses YAML to configure its associated integrations, ConfigMaps are a good choice for storing YAML. (For more information on config file format, see the Integration config file format.) The Kubernetes integration image comes with an auto-discovery feature that simplifies the configuration of multiple instances of services using a single configuration file. For example, if you have several NGINX instances running, creating an NGINX integration configuration file for every instance would be hard to implement and hard to update. With our auto-discovery option, you can discover and monitor all your NGINX instances with a single configuration file. Each integration has its own specific configuration YAML. Our NGINX integration default config file looks like this: nginx-config.yml: | --- discovery: command: # Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --port: Port used to connect to the kubelet. Default is 10255 # --tls: Use secure (TLS) connection # Custom Example: # exec: /var/db/newrelic-infra/nri-discovery-kubernetes --namespaces namespace1,namespace2 --port 10250 --tls # Default exec: /var/db/newrelic-infra/nri-discovery-kubernetes match: label.app: nginx integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}/status STATUS_MODULE: discover METRICS: 1 Copy The above config enables the following: Runs nri-discovery-kubernetes to query the data for the node we are currently on. Parses the data that comes back and looks for any Kubernetes pod that has a Kubernetes container with an app= label with value nginx. For any matches, it attempts to run the NGINX integration. The status URL is built from: The pod's IP address The status page is pulled from the label on K8s pod called status_url This automatic discovery works the same as the container auto-discovery used by the infrastructure agent. For more advanced options, see Container auto-discovery. Add a service YAML to the Kubernetes integration config It's best practice to configure enabled integrations alongside the Kubernetes integration configuration. This is easier than maintaining configuration files for every single service/integration instance. Below is an example of a Kubernetes integration's ConfigMap. The highlighted section shows where an integration configuration YAML (in this case, NGINX) is placed. For more information on discovery:, see Container auto-discovery for on-host integrations. --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: nginx-config.yml: | --- # Run auto discovery to find pods with label \"app=nginx\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: nginx integrations: - name: nri-nginx env: # If you're using ngx_http_api_module be certain to use the full path up to and including the version number # Use the discovered IP as the host address STATUS_URL: http://${discovery.ip}/status # Comma separated list of ngx_http_api_module, NON PARAMETERIZED, Endpoints # endpoints: /nginx,/processes,/connections,/ssl,/slabs,/http,/http/requests,/http/server_zones,/http/caches,/http/upstreams,/http/keyvals,/stream,/stream/server_zones,/stream/upstreams,/stream/keyvals,/stream/zone_sync # Name of Nginx status module OHI is to query against. discover | ngx_http_stub_status_module | ngx_http_status_module | ngx_http_api_module STATUS_MODULE: discover METRICS: 1 Copy This configuration map can then be referenced in the DaemonSet, the same as the one that was generated via the command line. Make sure the namespace used is the same one used by the Kubernetes integration manifest. If you haven't changed it in the downloaded manifest file, the value is default. Add multiple services to the same config You can monitor several services using the same Kubernetes integration config file. To do this, add another integration configuration YAML to the same Kubernetes integration config file. Below is the Kubernetes config created in the last section, with a new section for the Cassandra integration's config (highlighted). --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: nginx-config.yml: | --- # Run auto discovery to find pods with label \"app=nginx\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Run discovery for Kubernetes. Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: nginx integrations: - name: nri-nginx env: # If you're using ngx_http_api_module be certain to use the full path up to and including the version number # Use the discovered IP as the host address STATUS_URL: http://${discovery.ip}/status # Comma separated list of ngx_http_api_module, NON PARAMETERIZED, Endpoints # endpoints: /nginx,/processes,/connections,/ssl,/slabs,/http,/http/requests,/http/server_zones,/http/caches,/http/upstreams,/http/keyvals,/stream,/stream/server_zones,/stream/upstreams,/stream/keyvals,/stream/zone_sync # Name of Nginx status module OHI is to query against. discover | ngx_http_stub_status_module | ngx_http_status_module | ngx_http_api_module STATUS_MODULE: discover METRICS: 1 cassandra-configuration.yml: | --- # Run auto discovery to find pods with label \"app=cassandra\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Run discovery for Kubernetes. Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: cassandra integrations: - name: nri-cassandra env: # Use the discovered IP as the host address HOSTNAME: ${discovery.ip} PORT: 7199 USERNAME: cassandra PASSWORD: cassandra METRICS: 1/mark Copy The Kubernetes integration config is now set up to monitor these two services. Additionally, depending on your environment, there may be some additional service-specific configuration you must do. When you've completed configuration, our infrastructure agent looks for any pod with a label cassandra and runs the integration against it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 125.779655,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running on <em>Kubernetes</em>",
        "sections": "Monitor services in our <em>Kubernetes</em> <em>integration</em> installed with Helm",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "With New Relic&#x27;s <em>Kubernetes</em> <em>integration</em> you can monitor both <em>Kubernetes</em> and the services running on it, such as Cassandra, Redis, MySQL, and other supported services. Get started Our <em>Kubernetes</em> <em>integration</em> comes bundled with some of our on-host <em>integrations</em> (like Cassandra, MySQL, and Apache"
      },
      "id": "6044e50c196a676012960f35"
    },
    {
      "sections": [
        "Install the Kubernetes integration manually using Helm",
        "Compatibility and requirements",
        "Install Kubernetes integration with Helm",
        "Install with Helm 3 and nri-bundle (recommended)",
        "Installing and configuring nri-bundle with Helm",
        "Install with Helm 2 and nri-bundle (legacy)",
        "Installation instructions for Helm 2",
        "Important",
        "Tip",
        "Helm configuration options",
        "Upgrade using Helm",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data",
        "Reduce data ingest"
      ],
      "title": "Install the Kubernetes integration manually using Helm",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "a3e4c960777df00f17ce0e4b0d1083612bdca527",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-using-helm/",
      "published_at": "2021-10-07T00:22:52Z",
      "updated_at": "2021-10-01T21:14:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Helm is a package manager on top of Kubernetes. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in Kubernetes. To install the integration using Helm, we recommend our Kubernetes automated installer, which will prompt for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our integration as plain manifests rather than a Helm release. See Kubernetes integration: install and configure for more details about how to use our automated installer. Start the installer This page describes in more depth how to install and configure the New Relic integration without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the Kubernetes integration. Our charts are also compatible with Helm 2, but support for it might be removed in the future. To install the Kubernetes integration using Helm, you will need your New Relic account license key and your Kubernetes cluster's name: Find and copy your New Relic license key. Find the name of your cluster with this command: kubectl config current-context Copy Note this values somewhere safe as you will need them later during the installation process. Install Kubernetes integration with Helm Install with Helm 3 and nri-bundle (recommended) New Relic has several charts for the different components which offer different features for the platform: newrelic-infrastructure: Contains the main Kubernetes integration and the infrastructure agent. This is the core component for the New Relic Kubernetes experience, responsible of reporting most of the data that is surfaced in dashboard and the Kubernetes Cluster Explorer. newrelic-logging: Provides a DaemonSet with New Relic's Fluent Bit output plugin to easily forward your logs to New Relic. nri-kube-events: Collects and reports cluster events (such as kubectl get events) to New Relic. nri-prometheus: New Relic's Prometheus OpenMetrics Integration, automatically scrapes Prometheus endpoints present in the cluser and reports metrics to New Relic. nri-metadata-injection: Sets up a minimal MutatingAdmissionWebhook that injects a couple of environment to containers. These contain metadata about the cluster and New Relic installation and will be later picked up by applications instrumented using APM, allowing to correlate APM and infrastructure data. nri-statsd: New Relic StatsD integration. Additionally, New Relic provides nri-bundle, a chart which pulls a selectable set of the charts mentioned above. nri-bundle can also install Kube State Metrics and Pixie for you if needed. While it is possible to install those charts separately, we strongly recommend using the nri-bundle chart for Kubernetes deployments, as it ensures that values across all the charts are consistent and provides full control over which components are installed, as well as the possibility to configure all of them as Helm dependencies. This is the same chart that is used and referenced by our automated installer. Installing and configuring nri-bundle with Helm Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Please notice and adjust the following flags: global.licenseKey=<var>YOUR_NEW_RELIC_LICENSE_KEY</var>: Must be set to a valid License Key for your account. global.cluster=<var>K8S_CLUSTER_NAME</var>: Is used to identify the cluster in the New Relic UI, so should be a descriptive value not used by any other Kubernetes cluster configured in your New Relic account. ksm.enabled=<mark>true</mark>: Setting this to true will automatically install Kube State Metrics (KSM) for you, which is required for our integration to run. You can set this to false if KSM is already present in your cluster, even if it is on a different namespace. newrelic-infrastructure.privileged=<mark>true</mark>: Can be set to false to install a trimmed down version of our integration that does not require extra privileges, such as hostPath mounts or running containers as root. Please note that this will disable detailed process collection from the host. For performance reasons, our logging solution still requires hostPath mounts, regardless of the value of this flag. If this is not allowedin your cluster, you will need to disable the logging solution by specifying logging.enabled=false. prometheus.enabled=true: Will deploy our Prometheus OpenMetrics integration, which automatically collects data from prometheus endpoints present in the cluster. webhook.enabled=true: Will install our minimal webhook, which adds environment variables that, in turn, allows linking applications instrumented with APM with infrastructure data. Our chart has a comprehensive set of flags and tunables that can be edited to configure our solution to your particular needs. For a full list of all the flags that can be configured, please check the chart's README and the default values.yaml file. Install the Kubernetes integration by running the customized command without --debug and --dry-run: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl -n newrelic get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node for newrelic-infrastructure, as well as one Deployment and one pod for any other component that you have enabled. Install with Helm 2 and nri-bundle (legacy) Installation instructions for Helm 2 Important Helm 2 has been deprecated and New Relic does not recommend using it for deployments. Instructions in this section are provided for legacy systems only. To install using Helm 2: Ensure that Helm 2, including their cluster-side components (i.e. Tiller) are properly installed and configured. Please check the official documentation for more details. Set the cluster where you want to install the agent: kubectl config set-cluster DESIRED_CLUSTER Copy To see the available clusters, run kubectl config get-clusters Make sure that kube-state-metrics is installed on your machine: kubectl get deployment --all-namespaces | grep kube-state-metrics Copy If it's not installed, follow the instructions in the kube-state-metrics GitHub repo to install it. Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. This step uses the --dry-run and --debug switches and therefore the agent is not installed. helm install newrelic/newrelic-infrastructure \\ --dry-run \\ --debug \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Install the New Relic Kubernetes integration: helm install newrelic/newrelic-infrastructure \\ --set licenseKey=your_new_relic_license_key \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Tip Note that the --dry-run and --debug switches have been removed. Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node. Helm configuration options When you install or upgrade the Kubernetes integration with Helm using the command line, you can pass your configuration variables with the --set flag. helm install newrelic/newrelic-infrastructure \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=YOUR_CLUSTER_NAME Copy A full list of the configuration parameters can be found in the newrelic-infrastructure chart README Upgrade using Helm To update your Kubernetes integration installed via Helm: Update the local chart repository: helm repo update Copy Update the release by running again the appropriate helm upgrade --install ... command in the section above. Monitor services running on Kubernetes After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages. Reduce data ingest Our charts support setting an option to reduce the amount of data ingest, at the cost of dropping detailed information. To enable it, set global.lowDataMode to true in the nri-bundle chart. More details can be found in the chart’s README.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 120.906586,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "sections": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "Helm is a package manager on top of <em>Kubernetes</em>. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in <em>Kubernetes</em>. To install the <em>integration</em> using Helm, we recommend our <em>Kubernetes</em> automated installer, which will prompt"
      },
      "id": "603eb326e7b9d2d5f82a080a"
    },
    {
      "sections": [
        "Configure control plane monitoring",
        "Features",
        "Compatibility and requirements",
        "Discovery of master nodes and control plane components",
        "Configuration",
        "Important",
        "ETCD",
        "API server",
        "OpenShift configuration",
        "Set up mTLS from the ETCD client CA",
        "Tip",
        "To ease future installations",
        "Set up mTLS for ETCD in OpenShift",
        "See your data"
      ],
      "title": "Configure control plane monitoring",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "322c24d8737fb1e9625f35fe495bf22b23f046f6",
      "image": "https://docs.newrelic.com/static/209f301630c770f87ea8cbb1cace8e6e/8c557/new-relic-one-k8s-cluster-explorer-control-plane-parameters.png",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/installation/configure-control-plane-monitoring/",
      "published_at": "2021-10-08T06:18:38Z",
      "updated_at": "2021-09-20T19:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic provides Control Plane support for your Kubernetes integration, allowing you to monitor and collect metrics from your cluster's Control Plane components. That data can then be found in New Relic and used to create queries and charts. Features We monitor and collect metrics from the following control plane components: ETCD: leader information, resident memory size, number of OS threads, consensus proposals data, etc. For a list of supported metrics, see ETCD data. API server: rate of apiserver requests, breakdown of apiserver requests by HTTP method and response code, etc. For the complete list of supported metrics, see API server data. Scheduler: requested CPU/memory vs available on the node, tolerations to taints, any set affinity or anti-affinity, etc. For the complete list of supported metrics, see Scheduler data. Controller manager: resident memory size, number of OS threads created, goroutines currently existing, etc. For the complete list of supported metrics, see Controller manager data. Compatibility and requirements Control plane monitoring requires Kubernetes integration version 1.11.0 or higher. Control plane monitoring support is not enabled for managed clusters. This is because providers (EKS, GKE, AKS, etc.) abstract away the concept of master nodes and control plane components, so that access to them is limited or non-existent. External control planes are not supported. The unprivileged version of the Kubernetes integration does not support control plane monitoring. OpenShift 4.x uses control plane component metric endpoints that are different than the default. Discovery of master nodes and control plane components The Kubernetes integration relies on the kubeadm labeling conventions to discover the master nodes and the control plane components. This means that master nodes should be labeled with node-role.kubernetes.io/master=\"\" or kubernetes.io/role=\"master\". The control plane components should have either the k8s-app or the tier and component labels. Refer to the following table for accepted label combinations and values: Component Label Endpoint API server Kubeadm / Kops / ClusterAPI k8s-app=kube-apiserver tier=control-plane component=kube-apiserver OpenShift app=openshift-kube-apiserver apiserver=true localhost:443/metrics by default (can be configured) if the request fails falls back to localhost:8080/metrics ETCD Kubeadm / Kops / ClusterAPI k8s-app=etcd-manager-main tier=control-plane component=etcd OpenShift k8s-app=etcd localhost:4001/metrics Scheduler Kubeadm / Kops / ClusterAPI k8s-app=kube-scheduler tier=control-plane component=kube-scheduler OpenShift app=openshift-kube-scheduler scheduler=true localhost:10251/metrics Controller manager Kubeadm / Kops / ClusterAPI k8s-app=kube-controller-manager tier=control-plane component=kube-controller-manager​ OpenShift app=kube-controller-manager kube-controller-manager=true localhost:10252/metrics When the integration detects that it is running inside a master node, it tries to find which components are running on the node by looking for pods that match the labels listed in the table above. For every running component, the integration makes a request to its metrics endpoint. Configuration Control plane monitoring is automatic for agents running inside master nodes. The only component that requires an extra step to run is ETCD, because it uses mutual TLS authentication (mTLS) for client requests. The API Server can also be configured to be queried using the Secure Port. Important Control plane monitoring for OpenShift 4.x requires additional configuration. For more information, see the OpenShift 4.x Configuration section. ETCD In order to set mTLS for querying ETCD, there are two configuration options that need to be set: Option Value ETCD_TLS_SECRET_NAME Name of a Kubernetes secret that contains the mTLS configuration. The secret should contain the following keys: cert: the certificate that identifies the client making the request. It should be signed by an ETCD trusted CA. key: the private key used to generate the client certificate. cacert: the root CA used to identify the ETCD server certificate. If the ETCD_TLS_SECRET_NAME option is not set, ETCD metrics won't be fetched. For step by step instructions on how to create a certificate and sign it with the ETCD client CA, see Set up mTLS from the ETCD client CA. ETCD_TLS_SECRET_NAMESPACE The namespace where the secret specified in the ETCD_TLS_SECRET_NAME was created. If not set, the default namespace is used. API server By default, the API server metrics are queried using the localhost:8080 unsecured endpoint. If this port is disabled, you can also query these metrics over the secure port. To enable this, set the following configuration option in the Kubernetes integration manifest file: Option Value API_SERVER_ENDPOINT_URL The (secure) URL to query the metrics. The API server uses localhost:443 by default Ensure that the ClusterRole has been updated to the newest version found in the manifest Added in version 1.15.0 Important Note that the port can be different according to the secure port used by the API server. For example, in Minikube the API server secure port is 8443 and therefore API_SERVER_ENDPOINT_URL should be set to https://localhost:8443 OpenShift configuration Control plane components on OpenShift 4.x use endpoint URLs that require SSL and service account based authentication. Therefore, the default endpoint URLs can not be used. To configure control plane monitoring on OpenShift, uncomment the following environment variables in the customized manifest. URL values are pre-configured to the default base URLs for the control plane monitoring metrics endpoints in OpenShift 4.x. - name: \"SCHEDULER_ENDPOINT_URL\" value: \"https://localhost:10259 - name: \"ETCD_ENDPOINT_URL\" value: \"https://localhost:9979\" - name: \"CONTROLLER_MANAGER_ENDPOINT_URL\" value: \"https://localhost:10257\" - name: \"API_SERVER_ENDPOINT_URL\" value: \"https://localhost:6443\" Copy Important Even though the custom ETCD_ENDPOINT_URL is defined, ETCD requires HTTPS and mTLS authentication to be configured. For more on configuring mTLS for ETCD in OpenShift, see Set up mTLS for ETCD in OpenShift. Important When installing through Helm openshift, specify the config to automatically include these endpoints. Setting openshift.enabled=true and openshift.version=\"4.x\" will include the secure endpoints and enable the /var/run/crio.sock runtime. Set up mTLS from the ETCD client CA The instructions below are based on the Kubernetes documentation. For more information, see Managing TLS certificates in a cluster. For OpenShift, see Set up mTLS for ETCD in OpenShift. To set up mTLS from the ETCD client CA: Download and install the tool cfssl, selecting the correct binaries for your OS from the list. Once installed, execute the following command: cat <<EOF | cfssl genkey - | cfssljson -bare server { \"hosts\": [ \"localhost\" ], \"CN\": \"newrelic-infra.pod.cluster.local\", \"key\": { \"algo\": \"ecdsa\", \"size\": 256 } } EOF Copy This command generates two files; server.csr containing the PEM encoded pkcs#10 certification request and server-key.pem containing the PEM encoded key to the certificate to be created. Use the generated certificate authority (CA) of ETCD to sign your CSR. Depending on your cluster configuration, you may already have this information. For default install configuration, download the CA certificate and the private key directly from ETCD with the following commands: kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.crt ./cacert -n kube-system kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.key ./cacert.key -n kube-system Copy Tip This requires that the etcd-manager-main pod has the label k8s-app=etcd-manager-main, which is a requirement for control plane monitoring . If your etc-manager-main pod is located in a different namespace, change the -n kube-system flags accordingly. With those files downloaded, use the following command to sign your CSRF: cfssl sign -ca cacert -ca-key cacert.key server.csr | cfssljson -bare cert Copy Create the secret that is used to retrieve the TLS config for making requests to ETC. We recommend renaming the certificate and the private key: cp cert.pem cert && cp server-key.pem key Copy kubectl -n default create secret generic newrelic-infra-etcd-tls-secret --from-file=./cert --from-file=./key --from-file=./cacert Copy To ease future installations Use the following commands to simultaneously create the CSR, retrieve the CA, generate the certificate by signing the CSR, and create the secret with all the required fields: cat <<EOF | cfssl genkey - | cfssljson -bare server && \\ kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.crt ./cacert -n kube-system && \\ kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.key ./cacert.key -n kube-system && \\ cp server-key.pem key && \\ cfssl sign -ca cacert -ca-key cacert.key server.csr | cfssljson -bare cert && \\ cp cert.pem cert && \\ kubectl -n default create secret generic newrelic-infra-etcd-tls-secret --from-file=./cert --from-file=./key --from-file=./cacert { \"hosts\": [ \"localhost\" ], \"CN\": \"newrelic-infra.pod.cluster.local\", \"key\": { \"algo\": \"ecdsa\", \"size\": 256 } } EOF Copy The last step is to update the configuration in the manifest and apply it. In the configuration section, there are two options related to ETCD mTLS: ETCD_TLS_SECRET_NAME with the name of the secret that we just created. ETCD_TLS_SECRET_NAMESPACE with the namespace that we used to create the secret. To complete the installation, add these variables to the container spec of the integration DaemonSet and apply the changes: - name: \"ETCD_TLS_SECRET_NAME” value: \"newrelic-infra-etcd-tls-secret\" - name: \"ETCD_TLS_SECRET_NAMESPACE\" value: \"default\" Copy Set up mTLS for ETCD in OpenShift Follow these instructions to set up mutual TLS authentication for ETCD in OpenShift 4.x: Export the ETCD client certificates from the cluster to an opaque secret. In a default managed OpenShift cluster, the secret is named kube-etcd-client-certs and it is stored in the openshift-monitoring namespace. kubectl get secret/kube-etcd-client-certs -n openshift-monitoring -o yaml > etcd-secret.yaml Copy Open the secret file and change the keys: Rename the certificate authority to cacert. Rename the client certificate to cert. Rename the client key to key. Optional: change the secret name and namespace to something meaningful. Remove these unnecessary keys in the metadata section: creationTimestamp resourceVersion selfLink uid Install the manifest with its new name and namespace: kubectl apply -f etcd-secret.yaml Copy Go to Update manifest configuration (the last step under Set up MTL from ETCD client) to configure the required environment variables. See your data If the integration has been been set up correctly, the Kubernetes cluster explorer contains all the Control Plane components and their status in a dedicated section, as shown below. one.newrelic.com > Kubernetes Cluster Explorer: Use the Kubernetes cluster explorer to monitor and collect metrics from your cluster's Control Plane components You can also check for Control Plane data with this NRQL query: SELECT latest(timestamp) FROM K8sApiServerSample, K8sEtcdSample, K8sSchedulerSample, K8sControllerManagerSample FACET entityName where clusterName = 'MY_CLUSTER_NAME' Copy Tip If you still can't see Control Plane data, try the solution described in Kubernetes integration troubleshooting: Not seeing data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 111.471794,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": ", K8sSchedulerSample, K8sControllerManagerSample FACET entityName where clusterName = &#x27;MY_CLUSTER_NAME&#x27; Copy Tip If you still can&#x27;t see Control Plane data, try the solution described in <em>Kubernetes</em> <em>integration</em> <em>troubleshooting</em>: Not seeing data."
      },
      "id": "603e7f98196a67beaea83dbf"
    }
  ],
  "/docs/integrations/kubernetes-integration/understand-use-data/find-use-your-kubernetes-data": [
    {
      "sections": [
        "Navigate the Kubernetes cluster explorer",
        "Meet the cluster explorer",
        "Cluster dashboard",
        "Cluster explorer node table",
        "Search and filter your cluster data",
        "Browse your Kubernetes events"
      ],
      "title": "Navigate the Kubernetes cluster explorer",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Understand and use data"
      ],
      "external_id": "a3ef4aa459ed4503201c686000dff3a75331c2f7",
      "image": "https://docs.newrelic.com/static/34f90215b59ab8d7b4ec986bbb110805/9b7bd/nr1-cluster-explorer-node-tooltip.png",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/understand-use-data/kubernetes-cluster-explorer/",
      "published_at": "2021-10-07T02:14:53Z",
      "updated_at": "2021-03-16T06:10:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes cluster explorer uses the data collected by the Kubernetes integration to show the status of your cluster, from the control plane to nodes and pods. You can find out about the health of each entity, explore logs, and see how your apps are performing. With the Events integration, everything that happens in your cluster becomes visible, and logs brought in using the logs plugin are also available. Meet the cluster explorer The cluster explorer represents your most relevant cluster data on a chart with the shape of a ship's wheel — which is also Kubernetes' logo. Outer ring: Contains up to 24 nodes of your cluster, the most relevant based on the amount of alerts. Hover over each node to check resource consumption and the percentage of allocable pods used. Inner rings: Contain the pods ( ) of each node. Pods with active alerts are shown in the third innermost ring, and pods that are pending or unable to run are in the center. Hover the mouse over each node or pod to get a quick overview of its resource usage. You can click each node and pod to view its resource usage over time or to get more information about its health and active alerts. Colors are based on predefined alert conditions: Yellow pods have active warning alerts, while red pods have active critical alerts. one.newrelic.com > Kubernetes cluster explorer: Click any pod to get more information about its status and health, and to dig deeper into application data and traces, logs, and events. Click a node to see the following data: Pod statistics CPU, memory, and storage consumption against allocatable amounts Amount of pods used by the node against the allocatable amount of pods For each pod, depending on the integrations and features you've enabled, you can see: Pod status and metadata, including namespace and deployment Container status and statistics Active alerts (both warning and critical) Kubernetes events that happened in that pod APM data and traces (if you've linked your APM data) A link to the pods' and containers' logs, collected using the Kubernetes plugin for New Relic Logs Cluster and control plane statistics are always visible on the left side. Cluster dashboard The cluster dashboard can be accessed at any time from the cluster explorer by clicking Kubernetes dashboard. It provides a curated dashboard experience for your Kubernetes cluster. one.newrelic.com > Kubernetes cluster explorer > Kubernetes dashboard: The Kubernetes dashboard can be accessed from the Kubernetes cluster explorer. It shows useful Kubernetes metric data. Cluster explorer node table Below the cluster explorer is the node table, which shows all the nodes of the cluster, namespace, or deployment. Like all other usage indicators, the table shows consumption against allocatable resources. Search and filter your cluster data The main way to modify the data view in the cluster explorer is by using the top bar to search for specific attributes or values. All the attributes and values collected by the Kubernetes integration can be combined to narrow down the cluster view. one.newrelic.com > Kubernetes cluster explorer: All your Kubernetes cluster's attributes and data points can be used to filter the cluster explorer view. You can also change the time frame using the time picker in the upper right corner. The Auto-refresh box turns the cluster explorer into a real-time dashboard that refreshes every 60 seconds. one.newrelic.com > Kubernetes cluster explorer: The time picker lets you select several predefined time spans. To reload the data every minute, check the auto-refresh box. Browse your Kubernetes events If you’ve enabled the Kubernetes events integration, you can click the Events tab to browse everything that happened in your cluster, from warnings to normal events. To set it up, select the Kubernetes events box in step 3 of our install wizard, or follow the instructions. one.newrelic.com > Kubernetes cluster explorer > Events: Browse and filter all your Kubernetes events, and dig into logs and infrastructure data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 198.2463,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Navigate the <em>Kubernetes</em> cluster explorer",
        "sections": "Navigate the <em>Kubernetes</em> cluster explorer",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "New Relic&#x27;s <em>Kubernetes</em> cluster explorer uses the <em>data</em> collected by the <em>Kubernetes</em> <em>integration</em> to show the status of your cluster, from the control plane to nodes and pods. You can find out about the health of each entity, explore logs, and see how your apps are performing. With the Events"
      },
      "id": "603eb9a364441f82484e8879"
    },
    {
      "sections": [
        "On-host integration data collection and reporting",
        "Data collection and reporting process",
        "File structure and specifications"
      ],
      "title": "On-host integration data collection and reporting",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "Understand and use data"
      ],
      "external_id": "76942c8b7c37f1eaf368770c80177e3a43d8ca4c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/understand-use-data/host-integration-data-collection-reporting/",
      "published_at": "2021-10-08T06:07:16Z",
      "updated_at": "2021-03-16T06:04:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Read on to learn how New Relic on-host integrations collect and report data to New Relic. Data collection and reporting process This is how an infrastructure on-host integration sends data to New Relic: On startup, the infrastructure agent scans the directory that contains the integration's definition files. The infrastructure agent registers every integration executable defined in the definition file. The agent scans a dedicated directory for integration configuration files. If those config files specify integrations that have been registered with the infrastructure agent, the agent sets up and schedules the integrations. At the scheduled interval (the default is 15 seconds), the agent harvests the data from the integration and prepares it for transmission. Every 60 seconds, it sends that data to New Relic, along with any other infrastructure data. After a successful collection pass, the integration executable exits. File structure and specifications Understanding the file structure of New Relic on-host integrations can help you customize your integration, understand and use your data, and troubleshoot problems. On-host integrations adhere to a set of open source specifications, allowing anyone to build their own infrastructure on-host integration. For an explanation of these file specifications, see File specs.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 141.6518,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integration</em> <em>data</em> collection <em>and</em> reporting",
        "sections": "On-host <em>integration</em> <em>data</em> collection <em>and</em> reporting",
        "tags": "<em>Understand</em> <em>and</em> <em>use</em> <em>data</em>",
        "body": ", the <em>integration</em> executable exits. File structure and specifications Understanding the file structure of New Relic on-host <em>integrations</em> can help you customize your <em>integration</em>, <em>understand</em> and <em>use</em> your <em>data</em>, and troubleshoot problems. On-host <em>integrations</em> adhere to a set of open source specifications, allowing anyone to build their own infrastructure on-host <em>integration</em>. For an explanation of these file specifications, see File specs."
      },
      "id": "603e8553196a67ca20a83dd5"
    },
    {
      "sections": [
        "Remote monitoring in on-host integrations",
        "Important",
        "Effects of activating remote_monitoring",
        "Alert verification",
        "New entity attributes",
        "Changes in recorded metrics",
        "Unrecorded attributes",
        "Updated hostname"
      ],
      "title": "Remote monitoring in on-host integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "Understand and use data"
      ],
      "external_id": "1cfea4c65b855ce9ac5078d2a36ba11b63a6101b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/understand-use-data/remote-monitoring-host-integrations/",
      "published_at": "2021-10-07T06:02:01Z",
      "updated_at": "2021-03-16T06:05:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "From a New Relic perspective, entity is a broad concept. An entity is anything New Relic can identify that has data you can monitor. Integrations can be configured to create their own entity, called a remote entity, by setting the remote_monitoring option to true. If set to false, an integration will be considered a local entity, and the data related to it will be attached to the host entity that the agent creates. Remote monitoring requires infrastructure agent version 1.2.25 or higher. For the Apache, Cassandra, MySQL, NGINX, and Redis integrations, remote monitoring (and multi-tenancy) is enabled by activating the configuration parameter remote_monitoring. Important If your Apache, Cassandra, MySQL, NGINX, or Redis service is located in the same host as the agent, when you activate remote monitoring the resulting entity will be considered as remote, regardless of its actual location. This may affect alerts, alter attributes, and have other effects, as explained here. Effects of activating remote_monitoring By enabling remote_monitoring, the integration becomes a different entity which is no longer attached to the infrastructure agent. As a result, the following items may be affected: Alert verification Enabling remote monitoring can affect your configured alerts in case they are using any of the values that are affected by this new feature. We strongly recommend checking your existing alerts to make sure they keep on working as expected. New entity attributes These attributes are modified in the resulting entity: Display name: New entity unique key (instead of using the display name) Entity GUID: New entity GUID Entity ID: New entity ID Entity key: New entity unique key (instead of using the display name) External key: Using integration entity name (instead of using the agent display) Changes in recorded metrics When remote monitoring is enabled, we will add the hostname and port values to all metrics. If the nricluster name or nriservice are defined in the integration configuration file, they will also be decorated. Unrecorded attributes Since the integration is now an independent entity which is not attached to the agent, the following agent attributes are not collected: agentName agentVersion coreCount criticalViolationCount fullHostname instanceType kernelVersion linuxDistribution entityType operatingSystem processorCount systemMemoryBytes warningViolationCount Your custom attributes Updated hostname For the ApacheSample, RedisSample, CassandraSample, and NginxSample integration metrics, we will use the integration configuration hostname instead of the short hostname from the agent. When the integration hostname is a loopback address, the agent will replace it in order to guarantee uniqueness.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 136.09915,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Remote monitoring in on-host <em>integrations</em>",
        "sections": "Remote monitoring in on-host <em>integrations</em>",
        "tags": "<em>Understand</em> <em>and</em> <em>use</em> <em>data</em>",
        "body": "From a New Relic perspective, entity is a broad concept. An entity is anything New Relic can identify that has <em>data</em> you can monitor. <em>Integrations</em> can be configured to create their own entity, called a remote entity, by setting the remote_monitoring option to true. If set to false, an <em>integration</em>"
      },
      "id": "603ec000e7b9d216732a07ef"
    }
  ],
  "/docs/integrations/kubernetes-integration/understand-use-data/kubernetes-cluster-explorer": [
    {
      "sections": [
        "Find and use your Kubernetes data",
        "Query Kubernetes data",
        "Event types",
        "Manage alerts",
        "Create an alert condition",
        "Use the predefined alert types and thresholds",
        "Select alert notifications",
        "Pod alert notification example",
        "Container resource notification example",
        "Create alert conditions using NRQL",
        "Kubernetes attributes and metrics",
        "Node data",
        "Namespace data",
        "Deployment data",
        "ReplicaSet data",
        "DaemonSet data",
        "StatefulSet data",
        "Pod data",
        "Cluster data",
        "Container data",
        "Volume data",
        "API server data",
        "Controller manager data",
        "Scheduler data",
        "ETCD data",
        "Endpoint data",
        "Service data",
        "Horizontal Pod Autoscaler data",
        "Kubernetes metadata in APM-monitored applications",
        "For more help"
      ],
      "title": "Find and use your Kubernetes data",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Understand and use data"
      ],
      "external_id": "d36002ee54b0e3573ec4efef9f9c5ee940f49f96",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/understand-use-data/find-use-your-kubernetes-data/",
      "published_at": "2021-10-07T08:42:11Z",
      "updated_at": "2021-08-08T13:47:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can build your own charts and query all your Kubernetes integration data using the query builder and the NerdGraph API. Our integration collects Kubernetes data by instrumenting the container orchestration layer. For a simpler and more visual experience, use the cluster explorer. one.newrelic.com > Dashboards: Using the query builder you can query your Kubernetes data and create clear visualizations. Query Kubernetes data The simplest way to query your Kubernetes data is using the query builder, which accepts NRQL queries. Alternatively, you can use the NerdGraph API to retrieve Kubernetes data. Event types Kubernetes data is attached to the following event types: Event name Type of Kubernetes data Available since K8sNodeSample Node data v1.0.0 K8sNamespaceSample Namespace data v1.0.0 K8sDeploymentSample Deployment data v1.0.0 K8sReplicasetSample ReplicaSet data v1.0.0 K8sDaemonsetSample DaemonSet data v1.13.0 K8sStatefulsetSample StatefulSet data v1.13.0 K8sPodSample Pod data v1.0.0 K8sClusterSample Cluster data v1.0.0 K8sContainerSample Container data v1.0.0 K8sVolumeSample Volume data v1.0.0 K8sApiServerSample API server data v1.11.0 K8sControllerManagerSample Controller manager data v1.11.0 K8sSchedulerSample Scheduler data v1.11.0 K8sEtcdSample ETCD data v1.11.0 K8sEndpointSample Endpoint data v1.13.0 K8sServiceSample Service data v1.13.0 K8sHpaSample Horizontal Pod Autoscaler data v2.3.0 Manage alerts You can be notified about alert violations for your Kubernetes data: Create an alert condition To create an alert condition for the Kubernetes integration: Go to one.newrelic.com > Infrastructure > Settings > Alerts > Kubernetes, then select Create alert condition. To filter the alert to Kubernetes entities that only have the chosen attributes, select Filter. Select the threshold settings. For more on the Trigger an alert when... options, see Alert types. Select an existing alert policy, or create a new one. Select Create. When an alert condition's threshold is triggered, New Relic sends a notification to the policy's notification channels. Use the predefined alert types and thresholds The Kubernetes integration comes with its own alert policy and alert conditions. To see what the predefined alert conditions are, see Kubernetes integration: Predefined alert policy. In addition, you can create an alert condition for any metric collected by any New Relic integration you use, including the Kubernetes integration: Select the alert type Integrations. From the Select a data source dropdown, select a Kubernetes (K8s) data source. Select alert notifications When an alert condition's threshold is triggered, New Relic sends a message to the notification channel(s) chosen in the alert policy. Depending on the type of notification, you may have the following options: View the incident. Acknowledge the incident. Go to a chart of the incident data by selecting the identifier name. The entity identifier that triggered the alert appears near the top of the notification message. The format of the identifier depends on the alert type: Available pods are less than desired pods alerts: K8s:CLUSTER_NAME:PARENT_NAMESPACE:replicaset:REPLICASET_NAME Copy CPU or memory usage alerts: K8s:CLUSTER_NAME:PARENT_NAMESPACE:POD_NAME:container:CONTAINER_NAME Copy Here are some examples. Pod alert notification example For Available pods are less than desired pods alerts, the ID of the ReplicaSet triggering the issue might look like this: k8s:beam-production:default:replicaset:nginx-deployment-1623441481 Copy This identifier contains the following information: Cluster name: beam-production Parent namespace: default ReplicaSet name: nginx-deployment-1623441481 Container resource notification example For container CPU or memory usage alerts, the entity might look like this: k8s:beam-production:kube-system:kube-state-metrics-797bb87c75-zncwn:container:kube-state-metrics Copy This identifier contains the following information: Cluster name: beam-production Parent namespace: kube-system Pod namespace: kube-state-metrics-797bb87c75-zncwn Container name: kube-state-metrics Create alert conditions using NRQL Follow standard procedures to create alert conditions for NRQL queries. Kubernetes attributes and metrics The Kubernetes integration collects the following metrics and other attributes. Node data Query the K8sNodeSample event for node data: Node attribute Description allocatableCpuCores Node allocatable CPU cores allocatableMemoryBytes Node allocatable memory bytes allocatablePods Node allocatable pods allocatableEphemeralStorageBytes Node allocatable ephemeral-storage bytes capacityCpuCores Node CPU capacity capacityMemoryBytes Node memory capacity (in bytes) capacityPods Pod capacity of the node capacityEphemeralStorageBytes Node ephemeral-storage capacity clusterName Name that you assigned to the cluster when you installed the Kubernetes integration condition.{conditionName}={conditionValue} Status of the current observed node condition. The reported conditions can vary depending on your Kubernetes flavor and installed operators. Examples of common conditions are: Ready, DiskPressure, MemoryPressure, PIDPressure and NetworkUnavailable. Condition values can be 1 (true), 0 (false), or -1 (unknown). cpuUsedCoreMilliseconds Node CPU usage measured in core milliseconds cpuUsedCores Node CPU usage measured in cores cpuRequestedCores Total amount of CPU cores requested allocatableCpuCoresUtilization Percentage of CPU cores actually used with respect to the CPU cores allocatable fsAvailableBytes Bytes available in the node filesystem fsCapacityBytes Total capacity of the node filesystem in bytes fsInodes Total number of inodes in the node filesystem fsInodesFree Free inodes in the node filesystem fsInodesUsed Used inodes in the node filesystem fsUsedBytes Used bytes in the node filesystem fsCapacityUtilization Percentage of used bytes in the node filesystem with respect to the capacity memoryAvailableBytes Bytes of memory available in the node memoryMajorPageFaultsPerSecond Number of major page faults per second in the node memoryPageFaults Number of page faults in the node memoryRssBytes Bytes of rss memory memoryUsedBytes Bytes of memory used memoryWorkingSetBytes Bytes of memory in the working set memoryRequestedBytes Total amount of requested memory allocatableMemoryUtilization Percentage of bytes of memory in the working set with respect to the node allocatable memory net.errorCountPerSecond Number of errors per second while receiving/transmitting over the network nodeName Host name that the pod is running on runtimeAvailableBytes Bytes available to the container runtime filesystem runtimeCapacityBytes Total capacity assigned to the container runtime filesystem in bytes runtimeInodes Total number of inodes in the container runtime filesystem runtimeInodesFree Free inodes in the container runtime filesystem runtimeInodesUsed Used inodes in the container runtime filesystem runtimeUsedBytes Used bytes in the container runtime filesystem unschedulable Status of node schedulability of new pods. Its value can be 0 (false) or 1 (true) label.LABEL_NAME Labels associated with your node, so you can filter and query for specific nodes Namespace data Query the K8sNamespaceSample event for namespace data: Namespace attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of the namespace when it was created namespace Name of the namespace to be used as an identifier label.LABEL_NAME Labels associated with your namespace, so you can filter and query for specific namespaces status Current status of the namespace. The value can be Active or Terminated Deployment data Query the K8sDeploymentSample event for deployment data: Deployment attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the deployment was created deploymentName Name of the deployment to be used as an identifier namespace Name of the namespace that the deployment belongs to label.LABEL_NAME Labels associated with your deployment, so you can filter and query for specific deployments podsAvailable Number of replicas that are currently available podsDesired Number of replicas that you defined in the deployment podsTotal Total number of replicas that are currently running podsUnavailable Number of replicas that are currently unavailable podsUpdated Number of replicas that have been updated to achieve the desired state of the deployment podsMissing Total number of replicas that are missing (number of desired replicas, podsDesired, minus the total number of replicas, podsTotal) ReplicaSet data Query the K8sReplicasetSample event for ReplicaSet data: Replica attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the ReplicaSet was created deploymentName Name of the deployment to be used as an identifier namespace Name of the namespace that the ReplicaSet belongs to observedGeneration Integer representing generation observed by the ReplicaSet podsDesired Number of replicas that you defined in the deployment podsFullyLabeled Number of pods that have labels that match the ReplicaSet pod template labels podsReady Number of replicas that are ready for this ReplicaSet podsTotal Total number of replicas that are currently running podsMissing Total number of replicas that are currently missing (number of desired replicas, podsDesired, minus the number of ready replicas, podsReady) replicasetName Name of the ReplicaSet to be used as an identifier DaemonSet data Query the K8sDaemonsetSample event for DaemonSet data: DaemonSet attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the DaemonSet was created namespaceName Name of the namespace that the DaemonSet belongs to label.LABEL_NAME Labels associated with your DaemonSet, so you can filter and query for specific DaemonSet daemonsetName Name associated with the DaemonSet podsDesired The number of nodes that should be running the daemon pod podsScheduled The number of nodes running at least one daemon pod and are supposed to podsAvailable The number of nodes that should be running the daemon pod and have one or more of the daemon pod running and available podsReady The number of nodes that should be running the daemon pod and have one or more of the daemon pod running and ready podsUnavailable The number of nodes that should be running the daemon pod and have none of the daemon pod running and available podsMisscheduled The number of nodes running a daemon pod but are not supposed to podsUpdatedScheduled The total number of nodes that are running updated daemon pod podsMissing Total number of replicas that are currently missing (number of desired replicas, podsDesired, minus the number of ready replicas, podsReady) metadataGeneration Sequence number representing a specific generation of the desired state StatefulSet data Query the K8sStatefulsetSample event for StatefulSet data: StatefulSet attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the StatefulSet was created namespaceName Name of the namespace that the StatefulSet belongs to label.LABEL_NAME Labels associated with your StatefulSet, so you can filter and query for specific StatefulSet statefulsetName Name associated with the StatefulSet podsDesired Number of desired pods for a StatefulSet podsReady The number of ready replicas per StatefulSet podsCurrent The number of current replicas per StatefulSet podsTotal The number of replicas per StatefulSet podsUpdated The number of updated replicas per StatefulSet podsMissing Total number of replicas that are currently missing (number of desired replicas, podsDesired, minus the number of ready replicas, podsReady) observedGeneration The generation observed by the StatefulSet controller metadataGeneration Sequence number representing a specific generation of the desired state for the StatefulSet currentRevision Indicates the version of the StatefulSet used to generate pods in the sequence. Value range: between 0 and podsCurrent updateRevision Indicates the version of the StatefulSet used to generate pods in the sequence. Value range: between podsDesired-podsUpdated and podsDesired Pod data Query the K8sPodSample event for pod data: Pod attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the pod was created in epoch seconds createdBy Name of the Kubernetes object that created the pod. For example, newrelic-infra createdKind Kind of Kubernetes object that created the pod. For example, DaemonSet. deploymentName Name of the deployment to be used as an identifier isReady Boolean representing whether or not the pod is ready to serve requests isScheduled Boolean representing whether or not the pod has been scheduled to run on a node label.LABEL_NAME Labels associated with your pod, so you can filter and query for specific pods message Details related to the last pod status change namespace Name of the namespace that the pod belongs to net.errorCountPerSecond Number of errors per second while receiving/transmitting over the network net.errorsPerSecond Number of errors per second net.rxBytesPerSecond Number of bytes per second received over the network net.txBytesPerSecond Number of bytes per second transmitted over the network nodeIP Host IP address that the pod is running on nodeName Host name that the pod is running on podIP IP address of the pod. If it doesn't have an IP, it'll be empty podName Name of the pod to be used as an identifier reason Reason why the pod is in the current status startTime Timestamp of when the pod started running in epoch seconds status Current status of the pod. Value can be Pending, Running, Succeeded, Failed, Unknown Cluster data Query the K8sClusterSample event to see cluster data: Cluster attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration clusterK8sVersion Kubernetes version that the cluster is running Container data Query the K8sContainerSample event for container data: Container attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration containerID Unique ID associated with the container. If you are running Docker, this is the Docker container id containerImage Name of the image that the container is running containerImageID Unique ID associated with the image that the container is running containerName Name associated with the container cpuLimitCores Integer representing limit CPU cores defined for the container in the pod specification cpuRequestedCores Requested CPU cores defined for the container in the pod specification cpuUsedCores CPU cores actually used by the container cpuCoresUtilization Percentage of CPU cores actually used by the container with respect to the CPU limit specified. This percentage is based on this calculation: (cpuUsedCores / cpuLimitCores) * 100 requestedCpuCoresUtilization Percentage of CPU cores actually used by the container with respect to the CPU request specified deploymentName Name of the deployment to be used as an identifier isReady Boolean. Whether or not the container's readiness check succeeded label.LABEL_NAME Labels associated with your container, so you can filter and query for specific containers memoryLimitBytes Integer representing limit bytes of memory defined for the container in the pod specification memoryRequestedBytes Integer. Requested bytes of memory defined for the container in the pod specification memoryUsedBytes Integer. Bytes of memory actually used by the container memoryUtilization Percentage of memory actually used by the container with respect to the memory limit specified requestedMemoryUtilization Percentage of memory actually used by the container with respect to the memory request specified memoryWorkingSetBytes Integer. Bytes of memory in the working set memoryWorkingSetUtilization Percentage of working set memory actually used by the container with respect to the memory limit specified requestedMemoryWorkingSetUtilization Percentage of working set memory actually used by the container with respect to the memory request specified namespace Name of the namespace that the container belongs to nodeIP Host IP address the container is running on nodeName Host name that the container is running on podName Name of the pod that the container is in, to be used as an identifier reason Provides a reason why the container is in the current status restartCount Number of times the container has been restarted status Current status of the container. Value can be Running, Terminated, or Unknown containerCpuCfsPeriodsDelta Delta change of elapsed enforcement period intervals containerCpuCfsThrottledPeriodsDelta Delta change of throttled period intervals containerCpuCfsThrottledSecondsDelta Delta change of duration the container has been throttled, in seconds containerCpuCfsPeriodsTotal Total number of elapsed enforcement period intervals containerCpuCfsThrottledPeriodsTotal Total number of throttled period intervals containerCpuCfsThrottledSecondsTotal Total time duration the container has been throttled, in seconds containerMemoryMappedFileBytes Total size of memory mapped files used by this container, in bytes Volume data Query the K8sVolumeSample event for volume data: Volume attribute Description volumeName Name that you assigned to the volume at creation clusterName Cluster where the volume is configured namespace Namespace where the volume is configured podName The pod that the volume is attached to. The Kubernetes monitoring integration lists Volumes that are attached to a pod persistent If this is a persistent volume, this value is set to true pvcNamespace Namespace where the Persistent Volume Claim is configured pvcName Name that you assigned to the Persistent Volume Claim at creation fsCapacityBytes Capacity of the volume, in bytes fsUsedBytes Usage of the volume, in bytes fsAvailableBytes Capacity available of the volume, in bytes fsUsedPercent Usage of the volume in percentage fsInodes Total inodes of the volume fsInodesUsed inodes used in the volume fsInodesFree inodes available in the volume Volume data is available for volume plugins that implement the MetricsProvider interface: AWSElasticBlockStore AzureDisk AzureFile Cinder Flexvolume Flocker GCEPersistentDisk GlusterFS iSCSI StorageOS VsphereVolume API server data Query the K8sApiServerSample event to see API Server data. For more information, see Configure control plane monitoring: API server attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent, in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist apiserverRequestDelta_verb_VERB_code_CODE Difference of the number of apiserver requests, broken out for each verb and HTTP response code apiserverRequestRate_verb_VERB_code_CODE Rate of apiserver requests, broken out for each verb and HTTP response code restClientRequestsDelta_code_CODE_method_METHOD Difference of the number of HTTP requests, partitioned by method and code restClientRequestsRate_code_CODE_method_METHOD Rate of the number of HTTP requests, partitioned by method and code etcdObjectCounts_resource_RESOURCE-KIND Number of stored objects at the time of last check, split by kind Controller manager data Query the K8sControllerManagerSample event to see Controller manager data. For more information, see Configure control plane monitoring: Controller manager attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist workqueueAddsDelta_name_WORK-QUEUE-NAME Difference of the total number of adds handled by workqueue workqueueDepth_name_WORK-QUEUE-NAME Current depth of workqueue workqueueRetriesDelta_name_WORK-QUEUE-NAME Difference of the total number of retries handled by workqueue leaderElectionMasterStatus Gauge of if the reporting system is master of the relevant lease, 0 indicates backup, 1 indicates master Scheduler data Query the K8sSchedulerSample event in New Relic Insights to see Scheduler data. For more information, see Configure control plane monitoring: Scheduler attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist leaderElectionMasterStatus Gauge of if the reporting system is master of the relevant lease, 0 indicates backup, 1 indicates master httpRequestDurationMicroseconds_handler_HANDLER_quantile_QUANTILE The HTTP request latencies in microseconds, per quantile httpRequestDurationMicroseconds_handler_HANDLER_sum The sum of the HTTP request latencies, in microseconds httpRequestDurationMicroseconds_handler_HANDLER_count The number of observed HTTP requests events restClientRequestsDelta_code_CODE_host_HOST_method_METHOD Difference of the number of HTTP requests, partitioned by status code, method, and host restClientRequestsRate_code_CODE_host_HOST_method_METHOD Rate of the number of HTTP requests, partitioned by status code, method, and host schedulerScheduleAttemptsDelta_result_RESULT Difference of the number of attempts to schedule pods, by the result. unschedulable means a pod could not be scheduled, while error means an internal scheduler problem schedulerScheduleAttemptsRate_result_RESULT Rate of the number of attempts to schedule pods, by the result. unschedulable means a pod could not be scheduled, while error means an internal scheduler problem schedulerSchedulingDurationSeconds_operation_OPERATION_quantile_QUANTILE Scheduling latency in seconds split by sub-parts of the scheduling operation schedulerSchedulingDurationSeconds_operation_OPERATION_sum The sum of scheduling latency in seconds split by sub-parts of the scheduling operation schedulerSchedulingDurationSeconds_operation_OPERATION_count The number of observed events of schedulings split by sub-parts of the scheduling operation. schedulerPreemptionAttemptsDelta Difference of the total preemption attempts in the cluster till now schedulerPodPreemptionVictims Number of selected preemption victims ETCD data Query the K8sEtcdSample event to see ETCD data. For more information, see Configure control plane monitoring: ETCD attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist etcdServerHasLeader Whether or not a leader exists. 1 is existence, 0 is not etcdServerLeaderChangesSeenDelta Difference of the number of leader changes seen etcdMvccDbTotalSizeInBytes Total size of the underlying database physically allocated, in bytes etcdServerProposalsCommittedDelta Difference of the total number of consensus proposals committed etcdServerProposalsCommittedRate Rate of the total number of consensus proposals committed etcdServerProposalsAppliedDelta Difference of the total number of consensus proposals applied etcdServerProposalsAppliedRate Rate of the total number of consensus proposals applied etcdServerProposalsPending The current number of pending proposals to commit etcdServerProposalsFailedDelta Difference of the total number of failed proposals seen etcdServerProposalsFailedRate Rate of the total number of failed proposals seen processOpenFds Number of open file descriptors processMaxFds Maximum number of open file descriptors processFdsUtilization Percentage open file descriptors with respect to the maximum number that can be opened etcdNetworkClientGrpcReceivedBytesRate Rate of the total number of bytes received from gRPC clients etcdNetworkClientGrpcSentBytesRate Rate of the total number of bytes sent to gRPC clients Endpoint data Query the K8sEndpointSample event in New Relic Insights for endpoint data: Endpoint attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the endpoint was created namespaceName Name of the namespace that the endpoint belongs to endpointName Name associated with the endpoint label.LABEL_NAME Labels associated with your endpoint, so you can filter and query for specific endpoints addressAvailable Number of addresses available in endpoint addressNotReady Number of addresses not ready in endpoint Service data Query the K8sServiceSample event for service data: Service attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the service was created namespaceName Name of the namespace that the service belongs to label.LABEL_NAME Labels associated with your service, so you can filter and query for specific service serviceName Name associated with the service loadBalancerIP The IP of the external load balancer, if Spectype is LoadBalancer. externalName The external name value, if Spectype is ExternalName clusterIP The internal cluster IP, if Spectype is ClusterIP specType Type of the service selector.LABEL_NAME The label selector that this service targets Horizontal Pod Autoscaler data Query the K8sHpaSample event in New Relic Insights for Horizontal Pod Autoscaler data: HPA attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration label.LABEL_NAME Labels associated with your HPA, so you can filter and query for specific autoscaler currentReplicas Current number of replicas of pods managed by this autoscaler desiredReplicas Desired number of replicas of pods managed by this autoscaler minReplicas Lower limit for the number of pods that can be set by the autoscaler, 1 by default maxReplicas Upper limit for the number of pods that can be set by the autoscaler; cannot be smaller than minReplicas targetMetric The metric specifications used by this autoscaler when calculating the desired replica count isAble Boolean representing whether or not the autoscaler is able to fetch and update scales, as well as whether or not any backoff-related conditions would prevent scaling isActive Boolean representing whether or not the autoscaler is enabled (if it's able to calculate the desired scales) isLimited Boolean representing whether or not the autoscaler is capped, either up or down, by the maximum or minimum replicas configured labels Number of Kubernetes labels converted to Prometheus labels metadataGeneration The generation observed by the HorizontalPodAutoscaler controller Kubernetes metadata in APM-monitored applications By linking your applications with Kubernetes, the following attributes are added to application trace and distributed trace: nodeName containerName podName clusterName deploymentName namespaceName For more help",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 217.94864,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Find <em>and</em> <em>use</em> your <em>Kubernetes</em> <em>data</em>",
        "sections": "Find <em>and</em> <em>use</em> your <em>Kubernetes</em> <em>data</em>",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": " Relic <em>integration</em> you <em>use</em>, including the <em>Kubernetes</em> <em>integration</em>: Select the alert type <em>Integrations</em>. From the Select a <em>data</em> source dropdown, select a <em>Kubernetes</em> (K8s) <em>data</em> source. Select alert notifications When an alert condition&#x27;s threshold is triggered, New Relic sends a message to the notification"
      },
      "id": "603eb9a4196a678bfca83dbb"
    },
    {
      "sections": [
        "On-host integration data collection and reporting",
        "Data collection and reporting process",
        "File structure and specifications"
      ],
      "title": "On-host integration data collection and reporting",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "Understand and use data"
      ],
      "external_id": "76942c8b7c37f1eaf368770c80177e3a43d8ca4c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/understand-use-data/host-integration-data-collection-reporting/",
      "published_at": "2021-10-08T06:07:16Z",
      "updated_at": "2021-03-16T06:04:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Read on to learn how New Relic on-host integrations collect and report data to New Relic. Data collection and reporting process This is how an infrastructure on-host integration sends data to New Relic: On startup, the infrastructure agent scans the directory that contains the integration's definition files. The infrastructure agent registers every integration executable defined in the definition file. The agent scans a dedicated directory for integration configuration files. If those config files specify integrations that have been registered with the infrastructure agent, the agent sets up and schedules the integrations. At the scheduled interval (the default is 15 seconds), the agent harvests the data from the integration and prepares it for transmission. Every 60 seconds, it sends that data to New Relic, along with any other infrastructure data. After a successful collection pass, the integration executable exits. File structure and specifications Understanding the file structure of New Relic on-host integrations can help you customize your integration, understand and use your data, and troubleshoot problems. On-host integrations adhere to a set of open source specifications, allowing anyone to build their own infrastructure on-host integration. For an explanation of these file specifications, see File specs.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 141.6518,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integration</em> <em>data</em> collection <em>and</em> reporting",
        "sections": "On-host <em>integration</em> <em>data</em> collection <em>and</em> reporting",
        "tags": "<em>Understand</em> <em>and</em> <em>use</em> <em>data</em>",
        "body": ", the <em>integration</em> executable exits. File structure and specifications Understanding the file structure of New Relic on-host <em>integrations</em> can help you customize your <em>integration</em>, <em>understand</em> and <em>use</em> your <em>data</em>, and troubleshoot problems. On-host <em>integrations</em> adhere to a set of open source specifications, allowing anyone to build their own infrastructure on-host <em>integration</em>. For an explanation of these file specifications, see File specs."
      },
      "id": "603e8553196a67ca20a83dd5"
    },
    {
      "sections": [
        "Remote monitoring in on-host integrations",
        "Important",
        "Effects of activating remote_monitoring",
        "Alert verification",
        "New entity attributes",
        "Changes in recorded metrics",
        "Unrecorded attributes",
        "Updated hostname"
      ],
      "title": "Remote monitoring in on-host integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "Understand and use data"
      ],
      "external_id": "1cfea4c65b855ce9ac5078d2a36ba11b63a6101b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/understand-use-data/remote-monitoring-host-integrations/",
      "published_at": "2021-10-07T06:02:01Z",
      "updated_at": "2021-03-16T06:05:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "From a New Relic perspective, entity is a broad concept. An entity is anything New Relic can identify that has data you can monitor. Integrations can be configured to create their own entity, called a remote entity, by setting the remote_monitoring option to true. If set to false, an integration will be considered a local entity, and the data related to it will be attached to the host entity that the agent creates. Remote monitoring requires infrastructure agent version 1.2.25 or higher. For the Apache, Cassandra, MySQL, NGINX, and Redis integrations, remote monitoring (and multi-tenancy) is enabled by activating the configuration parameter remote_monitoring. Important If your Apache, Cassandra, MySQL, NGINX, or Redis service is located in the same host as the agent, when you activate remote monitoring the resulting entity will be considered as remote, regardless of its actual location. This may affect alerts, alter attributes, and have other effects, as explained here. Effects of activating remote_monitoring By enabling remote_monitoring, the integration becomes a different entity which is no longer attached to the infrastructure agent. As a result, the following items may be affected: Alert verification Enabling remote monitoring can affect your configured alerts in case they are using any of the values that are affected by this new feature. We strongly recommend checking your existing alerts to make sure they keep on working as expected. New entity attributes These attributes are modified in the resulting entity: Display name: New entity unique key (instead of using the display name) Entity GUID: New entity GUID Entity ID: New entity ID Entity key: New entity unique key (instead of using the display name) External key: Using integration entity name (instead of using the agent display) Changes in recorded metrics When remote monitoring is enabled, we will add the hostname and port values to all metrics. If the nricluster name or nriservice are defined in the integration configuration file, they will also be decorated. Unrecorded attributes Since the integration is now an independent entity which is not attached to the agent, the following agent attributes are not collected: agentName agentVersion coreCount criticalViolationCount fullHostname instanceType kernelVersion linuxDistribution entityType operatingSystem processorCount systemMemoryBytes warningViolationCount Your custom attributes Updated hostname For the ApacheSample, RedisSample, CassandraSample, and NginxSample integration metrics, we will use the integration configuration hostname instead of the short hostname from the agent. When the integration hostname is a loopback address, the agent will replace it in order to guarantee uniqueness.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 136.09915,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Remote monitoring in on-host <em>integrations</em>",
        "sections": "Remote monitoring in on-host <em>integrations</em>",
        "tags": "<em>Understand</em> <em>and</em> <em>use</em> <em>data</em>",
        "body": "From a New Relic perspective, entity is a broad concept. An entity is anything New Relic can identify that has <em>data</em> you can monitor. <em>Integrations</em> can be configured to create their own entity, called a remote entity, by setting the remote_monitoring option to true. If set to false, an <em>integration</em>"
      },
      "id": "603ec000e7b9d216732a07ef"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-api-management-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.46896,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.32182,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-10-08T06:36:14Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.65663,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-app-service-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.46896,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.32182,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-10-08T06:36:14Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.65663,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-application-gateway-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.46896,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.32182,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-10-08T06:36:14Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.65663,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-containers-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.46896,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.32182,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-10-08T06:36:14Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.65663,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-cosmos-db-document-db-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.46896,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.32182,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-10-08T06:36:14Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.65663,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-cost-management-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.46895,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.32182,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-10-08T06:36:14Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.65663,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-data-factory-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.46895,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.32182,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-10-08T06:36:14Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.65663,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.46895,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.32182,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-10-08T06:36:14Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.65663,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.46895,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.32182,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-10-08T06:36:14Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.65663,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-postgresql-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.46895,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.32182,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-10-08T06:36:14Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.65662,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-event-hub-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.46895,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.32182,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-10-08T06:36:14Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.65662,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-express-route-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.46895,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.32181,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-10-08T06:36:14Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.65662,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-firewalls-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.46895,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.32181,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-10-08T06:36:14Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.65662,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-front-door-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.46895,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.32181,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-10-08T06:36:14Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.65662,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-functions-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.46893,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.32181,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-10-08T06:36:14Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.65662,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-key-vault-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.46893,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.32181,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-10-08T06:36:14Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.65662,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-load-balancer-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.46893,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.32181,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-10-08T06:36:14Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.65662,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-logic-apps-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.46893,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.32181,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-10-08T06:36:14Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.65662,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-machine-learning-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.46893,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.32181,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-10-08T06:36:14Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.65662,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-power-bi-dedicated-capacities-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.46893,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.32181,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-10-08T06:36:14Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.65662,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-redis-cache-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.46893,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.32181,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-10-08T06:36:14Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.65662,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-bus-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.46893,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.32181,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-10-08T06:36:14Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.65662,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration": [
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.32181,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-10-08T06:36:14Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.65662,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-10-08T06:28:25Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.29962,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.46893,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-10-08T06:36:14Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.65662,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-10-08T06:28:25Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.29962,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-managed-instances-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.46893,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.32181,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-10-08T06:36:14Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.65662,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-storage-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.46893,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.32181,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-10-08T06:36:14Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.65662,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-virtual-machine-scale-sets-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.46892,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.32181,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-10-08T06:36:14Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.65662,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-virtual-machines-scale-sets-monitoring-integration": [
    {
      "sections": [
        "Azure virtual machine scale sets monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Virtual machine scale sets ScaleSet data"
      ],
      "title": "Azure virtual machine scale sets monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2b25b6720032817e09a6e844210f020b3a4fc98b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-virtual-machine-scale-sets-monitoring-integration/",
      "published_at": "2021-10-08T06:35:16Z",
      "updated_at": "2021-03-16T04:41:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Azure virtual machine scale sets data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure virtual machine scale sets integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. You can query and explore your data using the following event type: Entity Event type Provider ScaleSet AzureVirtualMachineScaleSetSample AzureVirtualMachineScaleSet For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure virtual machine scale sets data for ScaleSet. Virtual machine scale sets ScaleSet data Metric Unit Description cpuPercent Percent The percentage of allocated compute units that are currently in use by the Virtual Machine(s) networkInBytes Bytes The number of billable bytes received on all network interfaces by the Virtual Machine(s) (Incoming Traffic) networkOutBytes Bytes The number of billable bytes out on all network interfaces by the Virtual Machine(s) (Outgoing Traffic) diskReadBytes Bytes Bytes read from disk during monitoring period diskWriteBytes Bytes Bytes written to disk during monitoring period diskReadOperationsCountPerSecond CountPerSecond Disk Read IOPS diskWriteOperationsCountPerSecond CountPerSecond Disk Write IOPS cpuCreditsRemaining Count Total number of credits available to burst cpuCreditsConsumed Count Total number of credits consumed by the Virtual Machine dataDiskReadBytesCountPerSecond CountPerSecond Bytes/Sec read from a single disk during monitoring period dataDiskWriteBytesCountPerSecond CountPerSecond Bytes/Sec written to a single disk during monitoring period dataDiskReadOperationsCountPerSecond CountPerSecond Read IOPS from a single disk during monitoring period dataDiskWriteOperationsCountPerSecond CountPerSecond Write IOPS from a single disk during monitoring period dataDiskQueueDepth Count Data Disk Queue Depth(or Queue Length) osDiskReadBytesCountPerSecond CountPerSecond Bytes/Sec read from a single disk during monitoring period for OS disk osDiskWriteBytesCountPerSecond CountPerSecond Bytes/Sec written to a single disk during monitoring period for OS disk osDiskReadOperationsCountPerSecond CountPerSecond Read IOPS from a single disk during monitoring period for OS disk osDiskWriteOperationsCountPerSecond CountPerSecond Write IOPS from a single disk during monitoring period for OS disk osDiskQueueDepth Count OS Disk Queue Depth(or Queue Length) inboundFlows Count Inbound Flows are number of current flows in the inbound direction (traffic going into the VM) outboundFlows Count Outbound Flows are number of current flows in the outbound direction (traffic going out of the VM) inboundFlowsMaximumCreationRateCountPerSecond CountPerSecond The maximum creation rate of inbound flows (traffic going into the VM) outboundFlowsMaximumCreationRateCountPerSecond CountPerSecond The maximum creation rate of outbound flows (traffic going out of the VM) premiumDataDiskCacheReadHitPercent Percent Premium Data Disk Cache Read Hit premiumDataDiskCacheReadMissPercent Percent Premium Data Disk Cache Read Miss premiumOSDiskCacheReadHitPercent Percent Premium OS Disk Cache Read Hit premiumOSDiskCacheReadMissPercent Percent Premium OS Disk Cache Read Miss networkInTotalBytes Bytes The number of bytes received on all network interfaces by the Virtual Machine(s) (Incoming Traffic) networkOutTotalBytes Bytes The number of bytes out on all network interfaces by the Virtual Machine(s) (Outgoing Traffic)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1151.0092,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> <em>virtual</em> <em>machine</em> <em>scale</em> <em>sets</em> <em>monitoring</em> <em>integration</em>",
        "sections": "<em>Azure</em> <em>virtual</em> <em>machine</em> <em>scale</em> <em>sets</em> <em>monitoring</em> <em>integration</em>",
        "tags": "Microsoft <em>Azure</em> <em>integrations</em>",
        "body": " data. Metric data This <em>integration</em> collects <em>Azure</em> <em>virtual</em> machine <em>scale</em> <em>sets</em> data for <em>ScaleSet</em>. <em>Virtual</em> machine <em>scale</em> <em>sets</em> <em>ScaleSet</em> data Metric Unit Description cpuPercent Percent The percentage of allocated compute units that are currently in use by the <em>Virtual</em> Machine(s) networkInBytes Bytes"
      },
      "id": "603ea1cfe7b9d2a8342a0819"
    },
    {
      "sections": [
        "Introduction to the Kubernetes integration",
        "Get started: Install the Kubernetes integration",
        "Tip",
        "Why it matters",
        "Navigate all your Kubernetes events",
        "Bring your cluster logs to New Relic",
        "Check the source code"
      ],
      "title": "Introduction to the Kubernetes integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "c641d1367f1f8fd2b589a2707112759becae609b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/get-started/introduction-kubernetes-integration/",
      "published_at": "2021-10-10T01:37:55Z",
      "updated_at": "2021-09-05T01:49:46Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration gives you full observability into the health and performance of your environment, no matter whether you run Kubernetes on-premises or in the cloud. With our cluster explorer, you can cut through layers of complexity to see how your cluster is performing, from the heights of the control plane down to applications running on a single pod. one.newrelic.com > Kubernetes cluster explorer: The cluster explorer is our powerful, fully visual answer to the challenges associated with running Kubernetes at a large scale. You can see the power of the Kubernetes integration in the cluster explorer, where the full picture of a cluster is made available on a single screen: nodes and pods are visualized according to their health and performance, with pending and alerting nodes in the innermost circles. Predefined alert conditions help you troubleshoot issues right from the start. Clicking each node reveals its status and how each app is performing. Get started: Install the Kubernetes integration We have an automated installer to help you with many types of installations: servers, virtual machines, and unprivileged environments. It can also help you with installations in managed services or platforms, but you'll need to review a few preliminary notes before getting started. Our automated installer will generate either a helm command or a set of plain manifests for you to install. Our automated installer: Allows users to select the cluster name and namespace for the installation. Allows users to selectively enable or disable bundling of Kube-state-metrics, a dependency of the Kubernetes integration. Allows users to seamlessly install our other products related to Kubernetes such as: Kubernetes events monitoring In-cluster prometheus services monitoring Service instrumentation without code changes using Pixie Automatically fills the required properties with the license keys the integration needs to work. Read the install docs Start the installer Tip If your New Relic account is in the EU region, access the automated installer from one.eu.newrelic.com. Why it matters Governing the complexity of Kubernetes can be challenging; there's so much going on at any given moment, with containers being created and deleted in a matter of minutes, applications crashing, and resources being consumed unexpectedly. Our integration helps you navigate Kubernetes abstractions across on-premises, cloud, and hybrid deployments. In New Relic, you can build your own charts and query all your Kubernetes data, which our integration collects by instrumenting the container orchestration layer. This gives you additional insight into nodes, namespaces, deployments, replica sets, pods, and containers. one.newrelic.com > Dashboards: Using the query builder you can turn any query on Kubernetes data to clear visuals. With the Kubernetes integration you can also: Link your APM data to Kubernetes to measure the performance of your web and mobile applications, with metrics such as request rate, throughput, error rate, and availability. Monitor services running on Kubernetes, such as Apache, NGINX, Cassandra, and many more (see our tutorial for monitoring Redis on Kubernetes). Create new alert policies and alert conditions based on your Kubernetes data, or extend the predefined alert conditions. These features are in addition to the data New Relic already reports for containerized processes running on instrumented hosts. Navigate all your Kubernetes events The Kubernetes events integration, which is installed separately, watches for events happening in your Kubernetes clusters and sends those events to New Relic. Events data is then visualized in the cluster explorer. To set it up, check the Kubernetes events box in step 3 of our install wizard, or follow the instructions. one.newrelic.com > Kubernetes cluster explorer > Events: Browse and filter all your Kubernetes events, and dig into application logs and infrastructure data. Bring your cluster logs to New Relic Our Kubernetes plugin for log monitoring can collect all your cluster's logs and send them to our platform, so that you can set up new alerts and charts. To set it up, check the Log data box in step 3 of our install wizard, or follow the instructions. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or you can create your own fork and build it. For more information, see the README.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 144.69113,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to the Kubernetes <em>integration</em>",
        "sections": "Introduction to the Kubernetes <em>integration</em>",
        "tags": "<em>Integrations</em>",
        "body": " from the start. Clicking each node reveals its status and how each app is performing. Get started: Install the Kubernetes <em>integration</em> We have an automated installer to help you with many types of installations: servers, <em>virtual</em> <em>machines</em>, and unprivileged environments. It can also help you"
      },
      "id": "6043a212196a678d86960f46"
    },
    {
      "sections": [
        "Infrastructure agent overhead",
        "Linux single-task host",
        "Linux Docker host",
        "Windows host",
        "Linux ARM64 host",
        "Manage data"
      ],
      "title": "Infrastructure agent overhead",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Manage your agent"
      ],
      "external_id": "cd4b0d49bf6d11a12ff3a8357b223786b4c3f881",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/manage-your-agent/infrastructure-agent-performance-overhead/",
      "published_at": "2021-10-07T16:47:00Z",
      "updated_at": "2021-10-07T16:47:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The infrastructure agent is a lightweight piece of software, designed to minimize its impact on the performance of your hosts. However, the exact load varies depending on your host's workload, particularly on the number of processes running on the host. This is because the agent collects detailed data from each individual process. As a general guideline, New Relic has collected benchmarks for some common types of hosts: Linux single-task host The agent has very low performance overhead on a classic, single-task host. For example, a server running Apache, Unicorn, or a single Java application. Our benchmarks for this type of host are based on an Amazon EC2 t3.2xlarge: vCPUs: 8 vCPUs Memory: 32.0 GB Storage: 160.0 GB Operating system: CentOS 7 For this type of classic, single-task host, typical usage is: CPU: about 0.3% Virtual memory: about 1 GB Resident memory: 25 to 35 MB Storage on disk: about 50 MB Linux Docker host The agent has very low performance overhead on a host running Docker, with exact usage depending on the number of Docker containers your machine hosts, and whether those processes are long- or short-lived. Our benchmarks for this type of host are based on an Amazon EC2 t3.2xlarge: vCPUs: 8 vCPUs Memory: 32.0 GB Storage: 160.0 GB Number of containers: 25 containers, about 100 long-lived processes running in containers Operating system: CentOS 7 For this type of Docker host, typical usage is: CPU: about 0.8% Virtual memory: about 1 GB Resident memory: 25 to 35 MB Storage on disk: about 50 MB Windows host The agent has very low performance overhead on a typical Windows host serving web apps and running the Windows/IIS stack. Our benchmarks for this type of host are based on an Amazon EC2 t2.small: vCPUs: 1 Memory: 2.0 GB Storage: 30.0 GB Operating system: Windows Server 2012 R2 For this type of Windows host, typical usage is: CPU: 2 to 3% Resident Memory: 30 MB Storage on disk: about 50 MB Linux ARM64 host The agent has similar performance overhead on an ARM64 (Graviton 2) host on EC2 when compared with AMD64 machines. The benchmark is based on Amazon EC2 t3.2xlarge vs. t4g.2xlarge instances. Amazon Linux 2 EC2 instance with infrastructure agent default settings: CPU: about 0.1% on ARM vs 0.13% AMD Virtual memory: about 0.75GB ARM vs 1 GB AMD Resident memory: 20MB ARM vs 22 MB AMD We are always improving the performance of the infrastructure agent. If you see unusually high agent performance overhead, get support at support.newrelic.com. Manage data For how to adjust how much data our infrastructure monitoring ingests and reports, see Manage infrastructure data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 139.30954,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": ". Our benchmarks for this type of host are based on an Amazon EC2 t3.2xlarge: vCPUs: 8 vCPUs Memory: 32.0 GB Storage: 160.0 GB Operating system: CentOS 7 For this type of classic, single-task host, typical usage is: CPU: about 0.3% <em>Virtual</em> memory: about 1 GB Resident memory: 25 to 35 MB Storage"
      },
      "id": "6043fa3464441f329a378f18"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-virtual-network-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.46892,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.32181,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-10-08T06:36:14Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.65662,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.46892,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.32181,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-10-08T06:28:25Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.29962,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vpn-gateway-integration": [
    {
      "sections": [
        "Azure Service Fabric Mesh monitoring integration",
        "Important",
        "Features",
        "Activate integration",
        "Activate the integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data"
      ],
      "title": "Azure Service Fabric Mesh monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "3fecc91dc6389feba2ba549c9c69ad0ecdee4b86",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-08-08T21:19:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure monitoring provides an integration for Microsoft Azure Service Fabric that reports data from your Service Fabric nodes to New Relic. Important Microsoft Azure has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may still be accessed based on the data retention policies in New Relic. Features Our Azure Service Fabric Mesh integration reports metrics about your node such as CPU usage, memory consumption, or network usage. It also collects metrics for containers and processes running on each node. You can monitor and alert on your Azure Service Fabric node data with our infrastructure monitoring, and you can create custom queries and chart dashboards. Activate integration To enable the integration follow the instructions in Azure extensions for Infrastructure. Azure Service Fabric Mesh monitoring integration We offer a cloud integration for reporting your Azure Service Fabric Mesh data to our platform. Here we explain how to activate the integration and what data it collects. Activate the integration To enable the integration follow standard procedures to activate your Azure service. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure Service Fabric integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select the integration. Data is attached to the following event types: Entity Event Type Provider ServiceFabricMeshApplications AzureService-fabricServiceFabricMeshApplicationsSample AzureService-fabricServiceFabricMeshApplications For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure Service Fabric data for ServiceFabricMeshApplications. Metric Unit Description allocatedCpu Count Cpu allocated to this container in milli cores allocatedMemoryBytes Bytes Memory allocated to this container in MB actualCpu Count Actual CPU usage in milli cores actualMemoryBytes Bytes Actual memory usage in MB cpuUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu memoryUtilizationPercent Percent Utilization of CPU for this container as percentage of AllocatedCpu applicationStatus Count Status of Service Fabric Mesh application serviceStatus Count Health Status of a service in Service Fabric Mesh application serviceReplicaStatus Count Health Status of a service replica in Service Fabric Mesh application containerStatus Count Status of the container in Service Fabric Mesh application restartCount Count Restart count of a container in Service Fabric Mesh application",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.46892,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Service Fabric Mesh monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Service Fabric that reports data from your Service Fabric nodes to New Relic. Important <em>Microsoft</em> <em>Azure</em> has communicated the retirement of the Service Fabric Mesh service. Hence, this integration is no longer active. Data may"
      },
      "id": "603e79f228ccbc18f3eba786"
    },
    {
      "sections": [
        "Azure SQL Database monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Elastic pool sample metrics",
        "Server sample metrics",
        "Inventory data",
        "azure/sql/database/",
        "azure/sql/elasticpool/",
        "azure/sql/firewall",
        "azure/sql/replication-link/",
        "azure/sql/restore-point/"
      ],
      "title": "Azure SQL Database monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2bb54d455dd3b6c66bed514aecc453aad3b8a394",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration/",
      "published_at": "2021-10-08T06:33:17Z",
      "updated_at": "2021-07-21T22:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from Azure's fully-managed relational cloud database service. Azure SQL provides single databases with their own set of resources, and elastic pools that share a set of resources. Both are associated with an Azure SQL Database logical server. If databases are protected by a firewall, they can be replicated and restored to a previous point in time. Using New Relic, you can: View Azure SQL Database data in pre-built Infrastructure dashboards. Run custom queries and visualize the data. Create alert conditions to notify you of changes in data. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. Polling frequency for the Azure SQL Database integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data is organized like this: Azure SQL Database data Organized in New Relic Single database Data about a single database is attached to AzureSqlDatabaseSample event type. Inventory data has a provider value of AzureSqlDatabase. Elastic pool Data about an elastic pool is attached to AzureSqlElasticPoolSampleevent type. Inventory data has a provider value of AzureSqlElasticPool. Firewall Data about a firewall is attached to AzureSqlFirewallSample event type. Inventory data has a provider value of AzureSqlFirewall. Database replication link Data about a database replication link is attached to AzureSqlReplicationLinkSample event type. Inventory data has a provider value of AzureSqlReplicationLink. Database restore point Data about a database restore point is attached to AzureSqlRestorePointSample event type. Inventory data has a provider value of AzureSqlRestorePoint. Logical server Data about a logical server is attached to AzureSqlServerSample event type. Inventory data has a provider value of AzureSqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description cpuPercent Average CPU percentage. physicalDataReadPercent Average data IO percentage. logWritePercent Average log IO percentage. dtuConsumptionPercent Average DTU percentage. storage Total database size, in bytes. connectionSuccessful Total number of successful connections. connectionFailed Total number of failed connections. blockedByFirewall Total number of requests blocked by firewall. deadlock Total number of deadlocks. storagePercent Database size percentage. xtpStoragePercent Average in-memory OLTP storage percent. workersPercent Average workers percentage. sessionsPercent Average sessions percentage. dtuLimit Average number of DTU limit. dtuUsed Average number of used DTU. dwuLimit DWU limit. dwuConsumptionPercent Percentage of DWU. dwuUsed Number of used DWU. dwCpuPercent Average DW node level CPU percentage. dwPhysicalDataReadPercent Average DW node level data IO percentage. databaseSizeCurrentBytes Total current database size in bytes. databaseSizeLimitBytes Database limit in bytes. cpuLimit Average limit of CPUs. Applies to vCore-based databases. Elastic pool sample metrics Metric Description cpuPercent Average CPU percentage. databaseCpuPercent Average CPU percentage, per database. physicalDataReadPercent Average data IO percentage. databasePhysicalDataReadPercent Average data IO percentage, per database. logWritePercent Average log IO percentage. databaseLogWritePercent Average log IO percentage, per database. dtuConsumptionPercent Average DTU percentage. databaseDtuConsumptionPercent Average DTU percentage, per database. storagePercent Average storage percentage. workersPercent Average workers percentage. databaseWorkersPercent Average workers percentage, per database. sessionsPercent Average sessions percentage. databaseSessionsPercent Average sessions percentage, per database. eDTULimit Average DTU limit. storageLimitBytes Average storage limit, in bytes. eDTUUsed Average used eDTU. storageUsedBytes Average used storage, in bytes. databaseStorageUsedBytes Average used storage per database, in bytes. xtpStoragePercent Average in-memory OLTP storage percent. Server sample metrics Metric Description dtuCurrent Average utilization percentage relative to the DTU of the database. dtuLimit Database DTU limit. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/sql/database/ collation creationDate defaultSecondaryLocation earliestRestoreDate edition elasticPoolName isDataWarehouse maxSizeBytes name regionName resourceGroupName sqlServerName status transparentDataEncryptionStatus type azure/sql/elasticpool/ creationDate databaseDtuMax databaseDtuMin dtu edition name regionName resourceGroupName sqlServerName state storageMb type azure/sql/firewall endIpAddress name regionName resourceGroupName sqlServerName startIpAddress azure/sql/replication-link/ databaseName name partnerDatabase partnerLocation regionName replicationState resourceGroupName role sqlServerName startTime azure/sql/restore-point/ databaseName earliestRestoreDate name regionName resourceGroupName restorePointType sqlServerName",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.3218,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "sections": "<em>Azure</em> SQL Database monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> SQL Database metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from <em>Azure</em>&#x27;s fully-managed"
      },
      "id": "603e866be7b9d2f4ff2a0806"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-10-08T06:36:14Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.65662,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/get-started/activate-azure-integrations": [
    {
      "sections": [
        "Introduction to Azure monitoring integrations",
        "Requirements",
        "Features"
      ],
      "title": "Introduction to Azure monitoring integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f99e6127548c87b6d54587ee8fba6f03ef3fdf2e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/introduction-azure-monitoring-integrations/",
      "published_at": "2021-10-08T06:38:20Z",
      "updated_at": "2021-09-14T07:27:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft Azure integrations allow you to monitor and report data about your Azure services to New Relic, providing a comprehensive view of your entire architecture in one place. The Azure integrations are not the same as APM's .NET support for Azure. Requirements Check the Azure integrations documentation for requirements on individual integrations. New Relic cannot obtain monitoring data from resources that are located in Azure Government or that were created through the classic deployment model. Features After you activate your Azure integration, New Relic begins to query your Azure platform services according to a regular polling interval. You can use our integrations UI to: View performance data from Integrations dashboards that automatically scale as you make changes to your ecosystem. Manage alert conditions with alerts. Query your data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 160.57564,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "sections": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our <em>Microsoft</em> <em>Azure</em> <em>integrations</em> allow you to monitor and report data about your <em>Azure</em> services to New Relic, providing a comprehensive view of your entire architecture in one place. The <em>Azure</em> <em>integrations</em> are not the same as APM&#x27;s .NET support for <em>Azure</em>. Requirements Check the <em>Azure</em> <em>integrations</em>"
      },
      "id": "6044e562e7b9d2e5c15799f8"
    },
    {
      "sections": [
        "Azure integration metrics",
        "BETA FEATURE",
        "Azure Metrics"
      ],
      "title": "Azure integration metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "762631e1209bb9abb60f1ea8b185a6def61735b9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/azure-integration-metrics/",
      "published_at": "2021-10-08T11:30:42Z",
      "updated_at": "2021-09-14T18:17:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. Azure Metrics The following table contains the metrics we collect for Azure. Integration Dimensional Metric Name (new) Sample Metric Name (previous) Azure API Management azure.apimanagement.service.Capacity capacityPercent Azure API Management azure.apimanagement.service.Duration durationMilliseconds Azure API Management azure.apimanagement.service.EventHubDroppedEvents eventHubDroppedEvents Azure API Management azure.apimanagement.service.EventHubRejectedEvents eventHubRejectedEvents Azure API Management azure.apimanagement.service.EventHubSuccessfulEvents eventHubSuccessfulEvents Azure API Management azure.apimanagement.service.EventHubThrottledEvents eventHubThrottledEvents Azure API Management azure.apimanagement.service.EventHubTimedoutEvents eventHubTimedoutEvents Azure API Management azure.apimanagement.service.EventHubTotalBytesSent eventHubTotalBytesSentBytes Azure API Management azure.apimanagement.service.EventHubTotalEvents eventHubTotalEvents Azure API Management azure.apimanagement.service.EventHubTotalFailedEvents eventHubTotalFailedEvents Azure API Management azure.apimanagement.service.FailedRequests failedRequests Azure API Management azure.apimanagement.service.OtherRequests otherRequests Azure API Management azure.apimanagement.service.SuccessfulRequests successfulRequests Azure API Management azure.apimanagement.service.TotalRequests totalRequests Azure API Management azure.apimanagement.service.UnauthorizedRequests unauthorizedRequests Azure App Gateway azure.network.applicationgateways.ApplicationGatewayTotalTime applicationGatewayTotalTimeMilliseconds Azure App Gateway azure.network.applicationgateways.AvgRequestCountPerHealthyHost avgRequestCountPerHealthyHost Azure App Gateway azure.network.applicationgateways.BackendConnectTime backendConnectTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendFirstByteResponseTime backendFirstByteResponseTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendLastByteResponseTime backendLastByteResponseTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendResponseStatus backendResponseStatus Azure App Gateway azure.network.applicationgateways.BlockedCount blockedCount Azure App Gateway azure.network.applicationgateways.BlockedReqCount blockedReqCount Azure App Gateway azure.network.applicationgateways.BytesReceived bytesReceivedBytes Azure App Gateway azure.network.applicationgateways.BytesSent bytesSentBytes Azure App Gateway azure.network.applicationgateways.CapacityUnits capacityUnits Azure App Gateway azure.network.applicationgateways.ClientRtt clientRttMilliseconds Azure App Gateway azure.network.applicationgateways.ComputeUnits computeUnits Azure App Gateway azure.network.applicationgateways.CpuUtilization cpuUtilizationPercent Azure App Gateway azure.network.applicationgateways.CurrentConnections currentConnections Azure App Gateway azure.network.applicationgateways.EstimatedBilledCapacityUnits estimatedBilledCapacityUnits Azure App Gateway azure.network.applicationgateways.FailedRequests failedRequests Azure App Gateway azure.network.applicationgateways.FixedBillableCapacityUnits fixedBillableCapacityUnits Azure App Gateway azure.network.applicationgateways.HealthyHostCount healthyHostCount Azure App Gateway azure.network.applicationgateways.MatchedCount matchedCount Azure App Gateway azure.network.applicationgateways.NewConnectionsPerSecond newConnectionsPerSecondCountPerSecond Azure App Gateway azure.network.applicationgateways.ResponseStatus responseStatus Azure App Gateway azure.network.applicationgateways.Throughput throughputBytesPerSecond Azure App Gateway azure.network.applicationgateways.TlsProtocol tlsProtocol Azure App Gateway azure.network.applicationgateways.TotalRequests totalRequests Azure App Gateway azure.network.applicationgateways.UnhealthyHostCount unhealthyHostCount Azure App Service azure.web.serverfarms.BytesReceived bytesReceivedBytes Azure App Service azure.web.serverfarms.BytesSent bytesSentBytes Azure App Service azure.web.serverfarms.CpuPercentage cpuPercent Azure App Service azure.web.serverfarms.DiskQueueLength diskQueueLength Azure App Service azure.web.serverfarms.HttpQueueLength httpQueueLength Azure App Service azure.web.serverfarms.MemoryPercentage memoryPercent Azure App Service azure.web.sites.AppConnections appConnections Azure App Service azure.web.sites.AverageMemoryWorkingSet.byWebApp averageMemoryWorkingSetBytes Azure App Service azure.web.sites.AverageResponseTime averageResponseTimeSeconds Azure App Service azure.web.sites.BytesReceived.byWebApp receivedBytes Azure App Service azure.web.sites.BytesSent.byWebApp sentBytes Azure App Service azure.web.sites.CpuTime cpuTimeSeconds Azure App Service azure.web.sites.CurrentAssemblies currentAssemblies Azure App Service azure.web.sites.Gen0Collections gen0Collections Azure App Service azure.web.sites.Gen1Collections gen1Collections Azure App Service azure.web.sites.Gen2Collections gen2Collections Azure App Service azure.web.sites.Handles handles Azure App Service azure.web.sites.Http101 http101 Azure App Service azure.web.sites.Http2xx http2xx Azure App Service azure.web.sites.Http3xx http3xx Azure App Service azure.web.sites.Http401 http401 Azure App Service azure.web.sites.Http403 http403 Azure App Service azure.web.sites.Http404 http404 Azure App Service azure.web.sites.Http406 http406 Azure App Service azure.web.sites.Http4xx http4xx Azure App Service azure.web.sites.Http5xx.byWebApp http5xx Azure App Service azure.web.sites.MemoryWorkingSet.byWebApp memoryWorkingSetBytes Azure App Service azure.web.sites.Requests requests Azure App Service azure.web.sites.Threads threads Azure App Service azure.web.sites.TotalAppDomains totalAppDomains Azure App Service azure.web.sites.TotalAppDomainsUnloaded totalAppDomainsUnloaded Azure Containers azure.containerinstance.containergroups.CpuUsage cpuUsage Azure Containers azure.containerinstance.containergroups.MemoryUsage memoryUsageBytes Azure Containers azure.containerinstance.containergroups.NetworkBytesReceivedPerSecond networkReceivedBytesPerSecond Azure Containers azure.containerinstance.containergroups.NetworkBytesTransmittedPerSecond networkTransmittedBytesPerSecond Azure Containers azure.containerregistry.registries.RunDuration runDurationMilliseconds Azure Containers azure.containerregistry.registries.SuccessfulPullCount successfulPullCount Azure Containers azure.containerregistry.registries.SuccessfulPushCount successfulPushCount Azure Containers azure.containerregistry.registries.TotalPullCount totalPullCount Azure Containers azure.containerregistry.registries.TotalPushCount totalPushCount Azure Containers azure.containerservice.managedclusters.kube_node_status_allocatable_cpu_cores kubeNodeStatusAllocatableCpuCores Azure Containers azure.containerservice.managedclusters.kube_node_status_allocatable_memory_bytes kubeNodeStatusAllocatableMemoryBytes Azure Containers azure.containerservice.managedclusters.kube_node_status_condition kubeNodeStatusCondition Azure Containers azure.containerservice.managedclusters.kube_pod_status_phase kubePodStatusPhase Azure Containers azure.containerservice.managedclusters.kube_pod_status_ready kubePodStatusReady Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byAccount availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byAccount cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byAccount cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byAccount cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byAccount dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byAccount documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byAccount documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byAccount indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byAccount metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byAccount mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byAccount mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byAccount provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byAccount replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byAccount serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byAccount totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byAccount totalRequestUnits Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byCollection availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byCollection cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byCollection cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byCollection cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byCollection dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byCollection documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byCollection documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byCollection indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byCollection metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byCollection mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byCollection mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byCollection provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byCollection replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byCollection serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byCollection totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byCollection totalRequestUnits Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byDatabase availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byDatabase cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byDatabase cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byDatabase cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byDatabase dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byDatabase documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byDatabase documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byDatabase indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byDatabase metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byDatabase mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byDatabase mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byDatabase provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byDatabase replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byDatabase serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byDatabase totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byDatabase totalRequestUnits Azure Cost Management azure.costmanagement.cost.byLocation cost Azure Cost Management azure.costmanagement.cost.byResourceGroup cost Azure Cost Management azure.costmanagement.cost.byService cost Azure Cost Management azure.costmanagement.cost.byTag cost Azure Data Factory azure.datafactory.datafactories.FailedRuns failedRuns Azure Data Factory azure.datafactory.datafactories.SuccessfulRuns successfulRuns Azure Data Factory azure.datafactory.factories.ActivityCancelledRuns activityCancelledRuns Azure Data Factory azure.datafactory.factories.ActivityFailedRuns activityFailedRuns Azure Data Factory azure.datafactory.factories.ActivitySucceededRuns activitySucceededRuns Azure Data Factory azure.datafactory.factories.FactorySizeInGbUnits factorySizeInGbUnits Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAvailableMemory integrationRuntimeAvailableMemoryBytes Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAvailableNodeNumber integrationRuntimeAvailableNodeNumber Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAverageTaskPickupDelay integrationRuntimeAverageTaskPickupDelaySeconds Azure Data Factory azure.datafactory.factories.IntegrationRuntimeCpuPercentage integrationRuntimeCpuPercentagePercent Azure Data Factory azure.datafactory.factories.IntegrationRuntimeQueueLength integrationRuntimeQueueLength Azure Data Factory azure.datafactory.factories.MaxAllowedFactorySizeInGbUnits maxAllowedFactorySizeInGbUnits Azure Data Factory azure.datafactory.factories.MaxAllowedResourceCount maxAllowedResourceCount Azure Data Factory azure.datafactory.factories.PipelineCancelledRuns pipelineCancelledRuns Azure Data Factory azure.datafactory.factories.PipelineFailedRuns pipelineFailedRuns Azure Data Factory azure.datafactory.factories.PipelineSucceededRuns pipelineSucceededRuns Azure Data Factory azure.datafactory.factories.ResourceCount resourceCount Azure Data Factory azure.datafactory.factories.TriggerCancelledRuns triggerCancelledRuns Azure Data Factory azure.datafactory.factories.TriggerFailedRuns triggerFailedRuns Azure Data Factory azure.datafactory.factories.TriggerSucceededRuns triggerSucceededRuns Azure Database for MariaDB azure.dbformariadb.servers.active_connections activeConnections Azure Database for MariaDB azure.dbformariadb.servers.backup_storage_used backupStorageUsedBytes Azure Database for MariaDB azure.dbformariadb.servers.connections_failed connectionsFailed Azure Database for MariaDB azure.dbformariadb.servers.cpu_percent cpuPercent Azure Database for MariaDB azure.dbformariadb.servers.io_consumption_percent ioConsumptionPercent Azure Database for MariaDB azure.dbformariadb.servers.memory_percent memoryPercent Azure Database for MariaDB azure.dbformariadb.servers.network_bytes_egress networkEgressBytes Azure Database for MariaDB azure.dbformariadb.servers.network_bytes_ingress networkIngressBytes Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for MariaDB azure.dbformariadb.servers.storage_limit storageLimitBytes Azure Database for MariaDB azure.dbformariadb.servers.storage_percent storagePercent Azure Database for MariaDB azure.dbformariadb.servers.storage_used storageUsedBytes Azure Database for MySQL azure.dbformysql.servers.active_connections activeConnections Azure Database for MySQL azure.dbformysql.servers.backup_storage_used backupStorageUsedBytes Azure Database for MySQL azure.dbformysql.servers.connections_failed connectionsFailed Azure Database for MySQL azure.dbformysql.servers.cpu_percent cpuPercent Azure Database for MySQL azure.dbformysql.servers.io_consumption_percent ioConsumptionPercent Azure Database for MySQL azure.dbformysql.servers.memory_percent memoryPercent Azure Database for MySQL azure.dbformysql.servers.network_bytes_egress networkEgressBytes Azure Database for MySQL azure.dbformysql.servers.network_bytes_ingress networkIngressBytes Azure Database for MySQL azure.dbformysql.servers.seconds_behind_master secondsBehindMaster Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for MySQL azure.dbformysql.servers.storage_limit storageLimitBytes Azure Database for MySQL azure.dbformysql.servers.storage_percent storagePercent Azure Database for MySQL azure.dbformysql.servers.storage_used storageUsedBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.active_connections activeConnections Azure Database for PostgreSQL azure.dbforpostgresql.servers.backup_storage_used backupStorageUsedBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.connections_failed connectionsFailed Azure Database for PostgreSQL azure.dbforpostgresql.servers.cpu_percent cpuPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.io_consumption_percent ioConsumptionPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.memory_percent memoryPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.network_bytes_egress networkEgressBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.network_bytes_ingress networkIngressBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.pg_replica_log_delay_in_bytes pgReplicaLogDelayBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.pg_replica_log_delay_in_seconds pgReplicaLogDelaySeconds Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_limit storageLimitBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_percent storagePercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_used storageUsedBytes Azure Event Hub azure.eventhub.namespaces.ActiveConnections activeConnections Azure Event Hub azure.eventhub.namespaces.CaptureBacklog captureBacklog Azure Event Hub azure.eventhub.namespaces.CapturedBytes capturedBytes Azure Event Hub azure.eventhub.namespaces.CapturedMessages capturedMessages Azure Event Hub azure.eventhub.namespaces.ConnectionsClosed connectionsClosed Azure Event Hub azure.eventhub.namespaces.ConnectionsOpened connectionsOpened Azure Event Hub azure.eventhub.namespaces.IncomingBytes incomingBytes Azure Event Hub azure.eventhub.namespaces.IncomingMessages incomingMessages Azure Event Hub azure.eventhub.namespaces.IncomingRequests incomingRequests Azure Event Hub azure.eventhub.namespaces.OutgoingBytes outgoingBytes Azure Event Hub azure.eventhub.namespaces.OutgoingMessages outgoingMessages Azure Event Hub azure.eventhub.namespaces.QuotaExceededErrors quotaExceededErrors Azure Event Hub azure.eventhub.namespaces.ServerErrors serverErrors Azure Event Hub azure.eventhub.namespaces.Size sizeBytes Azure Event Hub azure.eventhub.namespaces.SuccessfulRequests successfulRequests Azure Event Hub azure.eventhub.namespaces.ThrottledRequests throttledRequests Azure Event Hub azure.eventhub.namespaces.UserErrors userErrors Azure Event Hub azure.eventhub.clusters.ActiveConnections activeConnections Azure Event Hub azure.eventhub.clusters.AvailableMemory availableMemoryPercent Azure Event Hub azure.eventhub.clusters.CaptureBacklog captureBacklog Azure Event Hub azure.eventhub.clusters.CapturedBytes capturedBytes Azure Event Hub azure.eventhub.clusters.CapturedMessages capturedMessages Azure Event Hub azure.eventhub.clusters.ConnectionsClosed connectionsClosed Azure Event Hub azure.eventhub.clusters.ConnectionsOpened connectionsOpened Azure Event Hub azure.eventhub.clusters.CPU cpuPercent Azure Event Hub azure.eventhub.clusters.IncomingBytes incomingBytes Azure Event Hub azure.eventhub.clusters.IncomingMessages incomingMessages Azure Event Hub azure.eventhub.clusters.IncomingRequests incomingRequests Azure Event Hub azure.eventhub.clusters.OutgoingBytes outgoingBytes Azure Event Hub azure.eventhub.clusters.OutgoingMessages outgoingMessages Azure Event Hub azure.eventhub.clusters.QuotaExceededErrors quotaExceededErrors Azure Event Hub azure.eventhub.clusters.ServerErrors serverErrors Azure Event Hub azure.eventhub.clusters.Size sizeBytes Azure Event Hub azure.eventhub.clusters.SuccessfulRequests successfulRequests Azure Event Hub azure.eventhub.clusters.ThrottledRequests throttledRequests Azure Event Hub azure.eventhub.clusters.UserErrors userErrors Azure Express Route azure.network.expressrouteports.AdminState adminState Azure Express Route azure.network.expressrouteports.LineProtocol lineProtocol Azure Express Route azure.network.expressrouteports.PortBitsInPerSecond portBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressrouteports.PortBitsOutPerSecond portBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressrouteports.RxLightLevel rxLightLevel Azure Express Route azure.network.expressrouteports.TxLightLevel txLightLevel Azure Express Route azure.network.expressroutecircuits.ArpAvailability arpAvailabilityPercent Azure Express Route azure.network.expressroutecircuits.BgpAvailability bgpAvailabilityPercent Azure Express Route azure.network.expressroutecircuits.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.GlobalReachBitsInPerSecond globalReachBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.GlobalReachBitsOutPerSecond globalReachBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.QosDropBitsInPerSecond qosDropBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.QosDropBitsOutPerSecond qosDropBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.peerings.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.peerings.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.connections.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.connections.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutegateways.ErGatewayConnectionBitsInPerSecond erGatewayConnectionBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutegateways.ErGatewayConnectionBitsOutPerSecond erGatewayConnectionBitsOutPerSecondCountPerSecond Azure Firewalls azure.network.azurefirewalls.ApplicationRuleHit applicationRuleHit Azure Firewalls azure.network.azurefirewalls.DataProcessed dataProcessedBytes Azure Firewalls azure.network.azurefirewalls.FirewallHealth firewallHealthPercent Azure Firewalls azure.network.azurefirewalls.NetworkRuleHit networkRuleHit Azure Firewalls azure.network.azurefirewalls.SNATPortUtilization sNATPortUtilizationPercent Azure Firewalls azure.network.azurefirewalls.Throughput throughputBitsPerSecond Azure Front Door azure.network.frontdoors.BackendHealthPercentage backendHealthPercent Azure Front Door azure.network.frontdoors.BackendRequestCount backendRequestCount Azure Front Door azure.network.frontdoors.BackendRequestLatency backendRequestLatencyMilliseconds Azure Front Door azure.network.frontdoors.BillableResponseSize billableResponseSizeBytes Azure Front Door azure.network.frontdoors.RequestCount requestCount Azure Front Door azure.network.frontdoors.RequestSize requestSizeBytes Azure Front Door azure.network.frontdoors.ResponseSize responseSizeBytes Azure Front Door azure.network.frontdoors.TotalLatency totalLatencyMilliseconds Azure Front Door azure.network.frontdoors.WebApplicationFirewallRequestCount webApplicationFirewallRequestCount Azure Functions azure.web.sites.AverageMemoryWorkingSet.byFunctionsApp averageMemoryWorkingSetBytes Azure Functions azure.web.sites.BytesReceived.byFunctionsApp receivedBytes Azure Functions azure.web.sites.BytesSent.byFunctionsApp sentBytes Azure Functions azure.web.sites.FunctionExecutionCount functionExecutionCount Azure Functions azure.web.sites.FunctionExecutionUnits functionExecutionUnits Azure Functions azure.web.sites.Http5xx.byFunctionsApp http5xx Azure Functions azure.web.sites.MemoryWorkingSet.byFunctionsApp memoryWorkingSetBytes Azure Key Vault azure.keyvault.vaults.Availability availabilityPercent Azure Key Vault azure.keyvault.vaults.SaturationShoebox saturationShoeboxPercent Azure Key Vault azure.keyvault.vaults.ServiceApiHit serviceApiHit Azure Key Vault azure.keyvault.vaults.ServiceApiLatency serviceApiLatencyMilliseconds Azure Key Vault azure.keyvault.vaults.ServiceApiResult serviceApiResult Azure Load Balancer azure.network.loadbalancers.AllocatedSnatPorts allocatedSnatPorts Azure Load Balancer azure.network.loadbalancers.ByteCount byteCountBytes Azure Load Balancer azure.network.loadbalancers.DipAvailability dipAvailability Azure Load Balancer azure.network.loadbalancers.PacketCount packetCount Azure Load Balancer azure.network.loadbalancers.SnatConnectionCount snatConnectionCount Azure Load Balancer azure.network.loadbalancers.SYNCount synCount Azure Load Balancer azure.network.loadbalancers.UsedSnatPorts usedSnatPorts Azure Load Balancer azure.network.loadbalancers.VipAvailability vipAvailability Azure Logic Apps azure.logic.workflows.ActionLatency actionLatencySeconds Azure Logic Apps azure.logic.workflows.ActionsCompleted actionsCompleted Azure Logic Apps azure.logic.workflows.ActionsFailed actionsFailed Azure Logic Apps azure.logic.workflows.ActionsSkipped actionsSkipped Azure Logic Apps azure.logic.workflows.ActionsStarted actionsStarted Azure Logic Apps azure.logic.workflows.ActionsSucceeded actionsSucceeded Azure Logic Apps azure.logic.workflows.ActionSuccessLatency actionSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.ActionThrottledEvents actionThrottledEvents Azure Logic Apps azure.logic.workflows.BillableActionExecutions billableActionExecutions Azure Logic Apps azure.logic.workflows.BillableTriggerExecutions billableTriggerExecutions Azure Logic Apps azure.logic.workflows.BillingUsageNativeOperation billingUsageNativeOperation Azure Logic Apps azure.logic.workflows.BillingUsageStandardConnector billingUsageStandardConnector Azure Logic Apps azure.logic.workflows.BillingUsageStorageConsumption billingUsageStorageConsumption Azure Logic Apps azure.logic.workflows.RunFailurePercentage runFailurePercent Azure Logic Apps azure.logic.workflows.RunLatency runLatencySeconds Azure Logic Apps azure.logic.workflows.RunsCancelled runsCancelled Azure Logic Apps azure.logic.workflows.RunsCompleted runsCompleted Azure Logic Apps azure.logic.workflows.RunsFailed runsFailed Azure Logic Apps azure.logic.workflows.RunsStarted runsStarted Azure Logic Apps azure.logic.workflows.RunsSucceeded runsSucceeded Azure Logic Apps azure.logic.workflows.RunStartThrottledEvents runStartThrottledEvents Azure Logic Apps azure.logic.workflows.RunSuccessLatency runSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.RunThrottledEvents runThrottledEvents Azure Logic Apps azure.logic.workflows.TotalBillableExecutions totalBillableExecutions Azure Logic Apps azure.logic.workflows.TriggerFireLatency triggerFireLatencySeconds Azure Logic Apps azure.logic.workflows.TriggerLatency triggerLatencySeconds Azure Logic Apps azure.logic.workflows.TriggersCompleted triggersCompleted Azure Logic Apps azure.logic.workflows.TriggersFailed triggersFailed Azure Logic Apps azure.logic.workflows.TriggersFired triggersFired Azure Logic Apps azure.logic.workflows.TriggersSkipped triggersSkipped Azure Logic Apps azure.logic.workflows.TriggersStarted triggersStarted Azure Logic Apps azure.logic.workflows.TriggersSucceeded triggersSucceeded Azure Logic Apps azure.logic.workflows.TriggerSuccessLatency triggerSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.TriggerThrottledEvents triggerThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.ActionLatency actionLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsCompleted actionsCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsFailed actionsFailed Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsSkipped actionsSkipped Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsStarted actionsStarted Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsSucceeded actionsSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.ActionSuccessLatency actionSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.ActionThrottledEvents actionThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentConnectorMemoryUsage integrationServiceEnvironmentConnectorMemoryUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentConnectorProcessorUsage integrationServiceEnvironmentConnectorProcessorUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentWorkflowMemoryUsage integrationServiceEnvironmentWorkflowMemoryUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentWorkflowProcessorUsage integrationServiceEnvironmentWorkflowProcessorUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.RunFailurePercentage runFailurePercent Azure Logic Apps azure.logic.integrationserviceenvironments.RunLatency runLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.RunsCancelled runsCancelled Azure Logic Apps azure.logic.integrationserviceenvironments.RunsCompleted runsCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.RunsFailed runsFailed Azure Logic Apps azure.logic.integrationserviceenvironments.RunsStarted runsStarted Azure Logic Apps azure.logic.integrationserviceenvironments.RunsSucceeded runsSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.RunStartThrottledEvents runStartThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.RunSuccessLatency runSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.RunThrottledEvents runThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerFireLatency triggerFireLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerLatency triggerLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersCompleted triggersCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersFailed triggersFailed Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersFired triggersFired Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersSkipped triggersSkipped Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersStarted triggersStarted Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersSucceeded triggersSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerSuccessLatency triggerSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerThrottledEvents triggerThrottledEvents Azure Machine Learning azure.machinelearningservices.workspaces.ActiveCores activeCores Azure Machine Learning azure.machinelearningservices.workspaces.ActiveNodes activeNodes Azure Machine Learning azure.machinelearningservices.workspaces.CompletedRuns completedRuns Azure Machine Learning azure.machinelearningservices.workspaces.CpuUtilization cpuUtilization Azure Machine Learning azure.machinelearningservices.workspaces.FailedRuns failedRuns Azure Machine Learning azure.machinelearningservices.workspaces.GpuUtilization gpuUtilization Azure Machine Learning azure.machinelearningservices.workspaces.IdleCores idleCores Azure Machine Learning azure.machinelearningservices.workspaces.IdleNodes idleNodes Azure Machine Learning azure.machinelearningservices.workspaces.LeavingCores leavingCores Azure Machine Learning azure.machinelearningservices.workspaces.LeavingNodes leavingNodes Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeployFailed modelDeployFailed Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeployStarted modelDeployStarted Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeploySucceeded modelDeploySucceeded Azure Machine Learning azure.machinelearningservices.workspaces.ModelRegisterFailed modelRegisterFailed Azure Machine Learning azure.machinelearningservices.workspaces.ModelRegisterSucceeded modelRegisterSucceeded Azure Machine Learning azure.machinelearningservices.workspaces.PreemptedCores preemptedCores Azure Machine Learning azure.machinelearningservices.workspaces.PreemptedNodes preemptedNodes Azure Machine Learning azure.machinelearningservices.workspaces.QuotaUtilizationPercentage quotaUtilizationPercentage Azure Machine Learning azure.machinelearningservices.workspaces.StartedRuns startedRuns Azure Machine Learning azure.machinelearningservices.workspaces.TotalCores totalCores Azure Machine Learning azure.machinelearningservices.workspaces.TotalNodes totalNodes Azure Machine Learning azure.machinelearningservices.workspaces.UnusableCores unusableCores Azure Machine Learning azure.machinelearningservices.workspaces.UnusableNodes unusableNodes Azure Power BI Dedicated azure.powerbidedicated.capacities.memory_metric memoryMetricBytes Azure Power BI Dedicated azure.powerbidedicated.capacities.memory_thrashing_metric memoryThrashingMetricPercent Azure Power BI Dedicated azure.powerbidedicated.capacities.qpu_high_utilization_metric qpuHighUtilizationMetric Azure Power BI Dedicated azure.powerbidedicated.capacities.QueryDuration queryDurationMilliseconds Azure Power BI Dedicated azure.powerbidedicated.capacities.QueryPoolJobQueueLength queryPoolJobQueueLength Azure Redis azure.cache.redis.cachehits cacheHits Azure Redis azure.cache.redis.cachemisses cacheMisses Azure Redis azure.cache.redis.cacheRead cacheReadBytesPerSecond Azure Redis azure.cache.redis.cacheWrite cacheWriteBytesPerSecond Azure Redis azure.cache.redis.connectedclients connectedClients Azure Redis azure.cache.redis.evictedkeys evictedKeys Azure Redis azure.cache.redis.expiredkeys expiredKeys Azure Redis azure.cache.redis.getcommands getCommands Azure Redis azure.cache.redis.operationsPerSecond operationsPerSecond Azure Redis azure.cache.redis.percentProcessorTime processorTimePercent Azure Redis azure.cache.redis.serverLoad serverLoadPercent Azure Redis azure.cache.redis.setcommands setCommands Azure Redis azure.cache.redis.totalcommandsprocessed totalCommandsProcessed Azure Redis azure.cache.redis.totalkeys totalKeys Azure Redis azure.cache.redis.usedmemory usedMemoryBytes Azure Redis azure.cache.redis.usedmemoryRss usedMemoryRssBytes Azure Redis azure.cache.redis.cachehits0 cacheHits Azure Redis azure.cache.redis.cachemisses0 cacheMisses Azure Redis azure.cache.redis.cacheRead0 cacheReadBytesPerSecond Azure Redis azure.cache.redis.cacheWrite0 cacheWriteBytesPerSecond Azure Redis azure.cache.redis.connectedclients0 connectedClients Azure Redis azure.cache.redis.evictedkeys0 evictedKeys Azure Redis azure.cache.redis.expiredkeys0 expiredKeys Azure Redis azure.cache.redis.getcommands0 getCommands Azure Redis azure.cache.redis.operationsPerSecond0 operationsPerSecond Azure Redis azure.cache.redis.percentProcessorTime0 processorTimePercent Azure Redis azure.cache.redis.serverLoad0 serverLoadPercent Azure Redis azure.cache.redis.setcommands0 setCommands Azure Redis azure.cache.redis.totalcommandsprocessed0 totalCommandsProcessed Azure Redis azure.cache.redis.totalkeys0 totalKeys Azure Redis azure.cache.redis.usedmemory0 usedMemoryBytes Azure Redis azure.cache.redis.usedmemoryRss0 usedMemoryRssBytes Azure Service Bus azure.servicebus.namespaces.ActiveConnections activeConnections Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byNamespace activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byNamespace connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byNamespace connectionsOpened Azure Service Bus azure.servicebus.namespaces.CPUXNS cpuUsagePercent Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byNamespace deadletteredMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byNamespace incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byNamespace incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byNamespace messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byNamespace outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byNamespace scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byNamespace serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byNamespace sizeBytes Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byNamespace successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byNamespace throttledRequests Azure Service Bus azure.servicebus.namespaces.UserErrors.byNamespace userErrors Azure Service Bus azure.servicebus.namespaces.WSXNS memoryUsagePercent Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byQueue activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byQueue connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byQueue connectionsOpened Azure Service Bus azure.servicebus.namespaces.currentSizeInBytes.byQueue currentSizeBytes Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byQueue deadletteredMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.byQueue deadLetterMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byQueue incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byQueue incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byQueue messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byQueue outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byQueue scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byQueue serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byQueue sizeBytes Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byQueue successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byQueue throttledRequests Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.byQueue transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.byQueue transferMessages Azure Service Bus azure.servicebus.namespaces.UserErrors.byQueue userErrors Azure Service Bus azure.servicebus.namespaces.activeMessageCount activeMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.bySubscription deadLetterMessages Azure Service Bus azure.servicebus.namespaces.messageCount messages Azure Service Bus azure.servicebus.namespaces.scheduledMessageCount scheduledMessages Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.bySubscription transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.bySubscription transferMessages Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byTopic activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byTopic connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byTopic connectionsOpened Azure Service Bus azure.servicebus.namespaces.currentSizeInBytes.byTopic currentSizeInBytes Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byTopic deadletteredMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.byTopic deadLetterMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byTopic incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byTopic incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byTopic messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byTopic outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byTopic scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byTopic serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byTopic sizeBytes Azure Service Bus azure.servicebus.namespaces.subscriptionCount subscriptions Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byTopic successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byTopic throttledRequests Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.byTopic transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.byTopic transferMessages Azure Service Bus azure.servicebus.namespaces.UserErrors.byTopic userErrors Azure Service Fabric azure.servicefabricmesh.applications.ActualCpu actualCpu Azure Service Fabric azure.servicefabricmesh.applications.ActualMemory actualMemoryBytes Azure Service Fabric azure.servicefabricmesh.applications.AllocatedCpu allocatedCpu Azure Service Fabric azure.servicefabricmesh.applications.AllocatedMemory allocatedMemoryBytes Azure Service Fabric azure.servicefabricmesh.applications.ApplicationStatus applicationStatus Azure Service Fabric azure.servicefabricmesh.applications.ContainerStatus containerStatus Azure Service Fabric azure.servicefabricmesh.applications.CpuUtilization cpuUtilizationPercent Azure Service Fabric azure.servicefabricmesh.applications.MemoryUtilization memoryUtilizationPercent Azure Service Fabric azure.servicefabricmesh.applications.RestartCount restartCount Azure Service Fabric azure.servicefabricmesh.applications.ServiceReplicaStatus serviceReplicaStatus Azure Service Fabric azure.servicefabricmesh.applications.ServiceStatus serviceStatus Azure SQL azure.sql.servers.database.currentSize databaseSizeCurrentBytes Azure SQL azure.sql.servers.database.limitSize databaseSizeLimitBytes Azure SQL azure.sql.servers.databases.blocked_by_firewall blockedByFirewall Azure SQL azure.sql.servers.databases.connection_failed connectionFailed Azure SQL azure.sql.servers.databases.connection_successful connectionSuccessful Azure SQL azure.sql.servers.databases.cpu_percent cpuPercent Azure SQL azure.sql.servers.databases.deadlock deadlock Azure SQL azure.sql.servers.databases.dtu_consumption_percent dtuConsumptionPercent Azure SQL azure.sql.servers.databases.dtu_limit dtuLimit Azure SQL azure.sql.servers.databases.dtu_used dtuUsed Azure SQL azure.sql.servers.databases.dw_cpu_percent dwCpuPercent Azure SQL azure.sql.servers.databases.dw_physical_data_read_percent dwPhysicalDataReadPercent Azure SQL azure.sql.servers.databases.dwu_consumption_percent dwuConsumptionPercent Azure SQL azure.sql.servers.databases.dwu_limit dwuLimit Azure SQL azure.sql.servers.databases.dwu_used dwuUsed Azure SQL azure.sql.servers.databases.log_write_percent logWritePercent Azure SQL azure.sql.servers.databases.physical_data_read_percent physicalDataReadPercent Azure SQL azure.sql.servers.databases.sessions_percent sessionsPercent Azure SQL azure.sql.servers.databases.storage storageBytes Azure SQL azure.sql.servers.databases.storage_percent storagePercent Azure SQL azure.sql.servers.databases.workers_percent workersPercent Azure SQL azure.sql.servers.databases.xtp_storage_percent xtpStoragePercent Azure SQL azure.sql.elasticPool.database_physical_data_read_percent databasePhysicalDataRead Azure SQL azure.sql.elasticPool.database_storage_used databaseStorageUsed Azure SQL azure.sql.servers.elasticpools.cpu_percent cpuPercent Azure SQL azure.sql.servers.elasticpools.database_cpu_percent databaseCpuPercent Azure SQL azure.sql.servers.elasticpools.database_dtu_consumption_percent databaseDtuConsumptionPercent Azure SQL azure.sql.servers.elasticpools.database_log_write_percent databaseLogWritePercent Azure SQL azure.sql.servers.elasticpools.database_sessions_percent databaseSessionsPercent Azure SQL azure.sql.servers.elasticpools.database_workers_percent databaseWorkersPercent Azure SQL azure.sql.servers.elasticpools.dtu_consumption_percent dtuConsumptionPercent Azure SQL azure.sql.servers.elasticpools.eDTU_limit eDTULimit Azure SQL azure.sql.servers.elasticpools.eDTU_used eDTUUsed Azure SQL azure.sql.servers.elasticpools.log_write_percent logWritePercent Azure SQL azure.sql.servers.elasticpools.physical_data_read_percent physicalDataReadPercent Azure SQL azure.sql.servers.elasticpools.sessions_percent sessionsPercent Azure SQL azure.sql.servers.elasticpools.storage_limit storageLimitBytes Azure SQL azure.sql.servers.elasticpools.storage_percent storagePercent Azure SQL azure.sql.servers.elasticpools.storage_used storageUsedBytes Azure SQL azure.sql.servers.elasticpools.workers_percent workersPercent Azure SQL azure.sql.servers.elasticpools.xtp_storage_percent xtpStoragePercent Azure SQL azure.sql.server.dtuLimit dtuLimit Azure SQL azure.sql.servers.dtuCurrent dtuCurrent Azure SQL Managed Instance azure.sql.managedinstances.avg_cpu_percent avgCpuPercent Azure SQL Managed Instance azure.sql.managedinstances.io_bytes_read ioReadBytes Azure SQL Managed Instance azure.sql.managedinstances.io_bytes_written ioWrittenBytes Azure SQL Managed Instance azure.sql.managedinstances.io_requests ioRequests Azure SQL Managed Instance azure.sql.managedinstances.reserved_storage_mb reservedStorage Azure SQL Managed Instance azure.sql.managedinstances.storage_space_used_mb storageSpaceUsed Azure SQL Managed Instance azure.sql.managedinstances.virtual_core_count virtualCore Azure Storage Account azure.storage.storageaccounts.Availability availabilityPercent Azure Storage Account azure.storage.storageaccounts.blobservices.Availability blobs.availabilityPercent Azure Storage Account azure.storage.storageaccounts.blobservices.BlobCapacity blobs.blobCapacityBytes Azure Storage Account azure.storage.storageaccounts.blobservices.BlobCount blobs.blobCount Azure Storage Account azure.storage.storageaccounts.blobservices.ContainerCount blobs.containerCount Azure Storage Account azure.storage.storageaccounts.blobservices.Egress blobs.egressBytes Azure Storage Account azure.storage.storageaccounts.blobservices.Ingress blobs.ingressBytes Azure Storage Account azure.storage.storageaccounts.blobservices.SuccessE2ELatency blobs.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.blobservices.SuccessServerLatency blobs.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.blobservices.Transactions blobs.transactions Azure Storage Account azure.storage.storageaccounts.Egress egressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.Availability files.availabilityPercent Azure Storage Account azure.storage.storageaccounts.fileservices.Egress files.egressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.FileCapacity files.fileCapacityBytes Azure Storage Account azure.storage.storageaccounts.fileservices.FileCount files.fileCount Azure Storage Account azure.storage.storageaccounts.fileservices.FileShareCount files.fileShareCount Azure Storage Account azure.storage.storageaccounts.fileservices.Ingress files.ingressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.SuccessE2ELatency files.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.fileservices.SuccessServerLatency files.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.fileservices.Transactions files.transactions Azure Storage Account azure.storage.storageaccounts.Ingress ingressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.Availability queues.availabilityPercent Azure Storage Account azure.storage.storageaccounts.queueservices.Egress queues.egressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.Ingress queues.ingressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.QueueCapacity queues.queueCapacityBytes Azure Storage Account azure.storage.storageaccounts.queueservices.QueueCount queues.queueCount Azure Storage Account azure.storage.storageaccounts.queueservices.QueueMessageCount queues.queueMessagesCount Azure Storage Account azure.storage.storageaccounts.queueservices.SuccessE2ELatency queues.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.queueservices.SuccessServerLatency queues.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.queueservices.Transactions queues.transactions Azure Storage Account azure.storage.storageaccounts.SuccessE2ELatency successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.SuccessServerLatency successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.Availability tables.availabilityPercent Azure Storage Account azure.storage.storageaccounts.tableservices.Egress tables.egressBytes Azure Storage Account azure.storage.storageaccounts.tableservices.Ingress tables.ingressBytes Azure Storage Account azure.storage.storageaccounts.tableservices.SuccessE2ELatency tables.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.SuccessServerLatency tables.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.TableCapacity tables.tableCapacityBytes Azure Storage Account azure.storage.storageaccounts.tableservices.TableCount tables.tableCount Azure Storage Account azure.storage.storageaccounts.tableservices.TableEntityCount tables.tableEntityCount Azure Storage Account azure.storage.storageaccounts.tableservices.Transactions tables.transactions Azure Storage Account azure.storage.storageaccounts.Transactions transactions Azure Storage Account azure.storage.storageaccounts.UsedCapacity usedCapacityBytes Azure Virtual Network azure.network.virtualnetworks.PingMeshAverageRoundtripMs pingMeshAverageRoundtripMs Azure Virtual Network azure.network.virtualnetworks.PingMeshProbesFailedPercent pingMeshProbesFailedPercent Azure Virtual Network azure.network.publicipaddresses.BytesDroppedDDoS droppedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.BytesForwardedDDoS forwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.BytesInDDoS inDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.DDoSTriggerTCPPackets ddosTriggerTcpPacketsPerSecond Azure Virtual Network azure.network.publicipaddresses.DDoSTriggerUDPPackets ddosTriggerUdpPacketsPerSecond Azure Virtual Network azure.network.publicipaddresses.IfUnderDDoSAttack ifUnderDdosAttack Azure Virtual Network azure.network.publicipaddresses.PacketsDroppedDDoS packetsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.PacketsForwardedDDoS packetsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.PacketsInDDoS packetsInDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesDroppedDDoS tcpDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesForwardedDDoS tcpForwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesInDDoS tcpInDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsDroppedDDoS tcpPacketsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsForwardedDDoS tcpPacketsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsInDDoS tcpPacketsInDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesDroppedDDoS udpDroppedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesForwardedDDoS udpForwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesInDDoS udpInDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsDroppedDDoS udpPacketsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsForwardedDDoS udpPacketsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsInDDoS udpPacketsInDdosPerSecond Azure Virtual Network azure.network.virtualnetworks.availableAddresses availableAddresses Azure VMs Scale Sets azure.compute.virtualmachinescalesets.CPUCreditsConsumed cpuCreditsConsumed Azure VMs Scale Sets azure.compute.virtualmachinescalesets.CPUCreditsRemaining cpuCreditsRemaining Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskQueueDepth dataDiskQueueDepth Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskReadBytessec dataDiskReadBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskReadOperationsSec dataDiskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskWriteBytessec dataDiskWriteBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskWriteOperationsSec dataDiskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskReadBytes diskReadBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskReadOperationsSec diskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskWriteBytes diskWriteBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskWriteOperationsSec diskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.InboundFlows inboundFlows Azure VMs Scale Sets azure.compute.virtualmachinescalesets.InboundFlowsMaximumCreationRate inboundFlowsMaximumCreationRateCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.NetworkInTotal networkInTotalBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.NetworkOutTotal networkOutTotalBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskQueueDepth osDiskQueueDepth Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskReadBytessec osDiskReadBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskReadOperationsSec osDiskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskWriteBytessec osDiskWriteBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskWriteOperationsSec osDiskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OutboundFlows outboundFlows Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OutboundFlowsMaximumCreationRate outboundFlowsMaximumCreationRateCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PercentageCPU cpuPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumDataDiskCacheReadHit premiumDataDiskCacheReadHitPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumDataDiskCacheReadMiss premiumDataDiskCacheReadMissPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumOSDiskCacheReadHit premiumOsDiskCacheReadHitPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumOSDiskCacheReadMiss premiumOsDiskCacheReadMissPercent Azure VMs azure.compute.virtualmachines.DiskReadBytes diskReadBytes Azure VMs azure.compute.virtualmachines.DiskReadOperations.Sec diskReadOpsPerSecond Azure VMs azure.compute.virtualmachines.DiskWriteBytes diskWriteBytes Azure VMs azure.compute.virtualmachines.DiskWriteOperations.Sec diskWriteOpsPerSecond Azure VMs azure.compute.virtualmachines.NetworkIn networkInBytes Azure VMs azure.compute.virtualmachines.NetworkOut networkOutBytes Azure VMs azure.compute.virtualmachines.PercentageCPU cpuUsagePercent Azure VPN Gateways azure.network.virtualnetworkgateways.AverageBandwidth averageBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.P2SBandwidth p2SBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.P2SConnectionCount p2SConnectionCount Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelAverageBandwidth tunnelAverageBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressBytes tunnelEgressBytes Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressPacketDropTSMismatch tunnelEgressPacketDropTSMismatch Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressPackets tunnelEgressPackets Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressBytes tunnelIngressBytes Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressPacketDropTSMismatch tunnelIngressPacketDropTSMismatch Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressPackets tunnelIngressPackets",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 153.0643,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> <em>integration</em> metrics",
        "sections": "<em>Azure</em> <em>integration</em> metrics",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": " <em>azure</em>.logic.workflows.RunsFailed runsFailed <em>Azure</em> Logic Apps <em>azure.logic.workflows.RunsStarted</em> runs<em>Started</em> <em>Azure</em> Logic Apps <em>azure</em>.logic.workflows.RunsSucceeded runsSucceeded <em>Azure</em> Logic Apps <em>azure.logic.workflows.RunStart</em>ThrottledEvents run<em>Start</em>ThrottledEvents <em>Azure</em> Logic Apps"
      },
      "id": "603e8a8928ccbcacc0eba74e"
    },
    {
      "sections": [
        "Polling intervals for Azure integrations",
        "View polling data",
        "New Relic polling intervals"
      ],
      "title": "Polling intervals for Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "82db3eae120c4318365cf0d0e5bfee69930b969f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/getting-started/polling-intervals-azure-integrations/",
      "published_at": "2021-10-08T06:38:20Z",
      "updated_at": "2021-03-13T03:47:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Azure integrations query your Azure services according to a polling interval specific to the integration. The polling interval applies for every Azure entity related to the integrated service. For example, if you have thirteen CosmosDB instances, each of the thirteen instances will be polled every five minutes. View polling data After you activate an Azure integration, New Relic starts polling data from Azure and makes the data accessible through infrastructure Inventory and New Relic dashboards. You can query the Azure data along with additional data imported from any other New Relic features. You can also view dashboard data for a specific integration or across your account. For visualizations of polling intervals, API calls, and other data for your Azure integrations: Go to one.newrelic.com > Infrastructure > Azure. To view data for a specific integration: Select the Dashboards link for the integration's row. New Relic polling intervals For polling and resolution details, see the documentation for a specific integration.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 123.20318,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Polling intervals for <em>Azure</em> <em>integrations</em>",
        "sections": "Polling intervals for <em>Azure</em> <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>Azure</em> <em>integrations</em> query your <em>Azure</em> services according to a polling interval specific to the integration. The polling interval applies for every <em>Azure</em> entity related to the integrated service. For example, if you have thirteen CosmosDB instances, each of the thirteen instances"
      },
      "id": "6044e560196a671d6f960f72"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/get-started/azure-integration-metrics": [
    {
      "sections": [
        "Introduction to Azure monitoring integrations",
        "Requirements",
        "Features"
      ],
      "title": "Introduction to Azure monitoring integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f99e6127548c87b6d54587ee8fba6f03ef3fdf2e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/introduction-azure-monitoring-integrations/",
      "published_at": "2021-10-08T06:38:20Z",
      "updated_at": "2021-09-14T07:27:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft Azure integrations allow you to monitor and report data about your Azure services to New Relic, providing a comprehensive view of your entire architecture in one place. The Azure integrations are not the same as APM's .NET support for Azure. Requirements Check the Azure integrations documentation for requirements on individual integrations. New Relic cannot obtain monitoring data from resources that are located in Azure Government or that were created through the classic deployment model. Features After you activate your Azure integration, New Relic begins to query your Azure platform services according to a regular polling interval. You can use our integrations UI to: View performance data from Integrations dashboards that automatically scale as you make changes to your ecosystem. Manage alert conditions with alerts. Query your data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 160.57562,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "sections": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our <em>Microsoft</em> <em>Azure</em> <em>integrations</em> allow you to monitor and report data about your <em>Azure</em> services to New Relic, providing a comprehensive view of your entire architecture in one place. The <em>Azure</em> <em>integrations</em> are not the same as APM&#x27;s .NET support for <em>Azure</em>. Requirements Check the <em>Azure</em> <em>integrations</em>"
      },
      "id": "6044e562e7b9d2e5c15799f8"
    },
    {
      "sections": [
        "Activate Azure integrations",
        "Requirements",
        "Step 1: Get Azure subscription and tenant IDs",
        "Step 2: Register your app and get ID",
        "Step 3: Create a client secret in Azure",
        "Step 4: Provide permissions to services",
        "Step 5: Add app to New Relic",
        "Update application details and rotate client secrets",
        "Explore app data in New Relic Infrastructure's UI"
      ],
      "title": "Activate Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f65679179e13aa1b503b4b95010e296cbe269c29",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/activate-azure-integrations/",
      "published_at": "2021-10-07T10:05:25Z",
      "updated_at": "2021-08-02T13:06:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations allow you to report data from specific systems and supplement infrastructure's default, automatic monitoring. The Microsoft Azure integrations report data from various Azure platform services to your New Relic account. This document explains how to activate Azure integrations. Requirements The Azure integration activation process requires you to: A New Relic account. Don't have one? Sign up for free! No credit card required. Create a New Relic application and key in Azure. Grant this application access to the Azure services you want to monitor. Place required information in the New Relic's Integrations UI. To use these integration activation instructions directly from the Infrastructure UI, go to one.newrelic.com > Infrastructure > Azure > Add an Azure account. Step 1: Get Azure subscription and tenant IDs To get your Azure account's subscription id and tenantId, use your local terminal if you have Azure's tools installed, or use Azure's Cloud Shell terminal in the Azure portal. Open a terminal with access to your Azure account. Type the following: az account show Copy Copy and save the subscription id and tenantID from the output response for later use. The response should look similar to the response below. The subscription id and tenantID are highlighted. @Azure:~$ az account show { \"environmentName\": \"AzureCloud\", \"id\": \"9ffe9512-f4a2-42dd-1230-518aec34be21\" , \"isDefault\": true, \"name\": \"Beyond Team Sandbox\", \"state\": \"Enabled\", \"tenantId\": \"ac6692da-1231-422f-22a8-9eed6dbe83f1\" , \"user\": { \"name\": \"youremail@domain\", \"type\": \"user\" } Copy Step 2: Register your app and get ID You must have Azure permissions to register your application and copy its Application ID. To register your app in Azure: Sign in to the Azure portal and go to the Azure Active Directory. From Manage, select App registrations > New registration. Enter a name for the application. We recommend that you name your app NewRelic-Integrations. In Redirect URI select Weband add https://www.newrelic.com as the sign-on URI. Create the application by clicking Register. From the Overview of your app, copy the Application (client) ID, and save it for later use. Step 3: Create a client secret in Azure To create a client secret associated with your application: In Azure, under the application you've just created, select Certificates & secrets. Under Client secrets, click on New client secret and then on Add. Copy the value of Client Secret and save it for later use. Step 4: Provide permissions to services Your app must provide Reader permissions for each Azure service you want New Relic to monitor: In the Azure Subscriptions section, select the subscriptions that you want New Relic to monitor. Select Access control (IAM) > Add > Add role assignment. From the Role dropdown, select Reader. From the Select dropdown, select the app's name; for example, NewRelic-Integrations. From Selected members, verify your app name appears, then select Save. Some Azure services, including Azure CosmosDB and Azure VMs, require additional steps. See the Azure integration documentation for the services you want to enable. Step 5: Add app to New Relic Now you can activate the Azure integration in the Infrastructure UI. The UI will require the information you have saved in the previous steps, including: Your Azure account's subscription id and tenantId The application's application ID The application's client secret To add your Azure app to New Relic: Go to one.newrelic.com > Infrastructure > Azure and select the Azure Service you wish to add. Follow the steps in the UI to activate the integration in New Relic. If you have already completed the Azure account steps, skip to the end of the steps to fill out the form. (For Azure account name, enter the name you want to use to identify the account in your Integrations dashboard.) Update application details and rotate client secrets It's possible to update the application's name and authentication credentials using the Infrastructure UI or the Cloud Integrations API at any time. Follow these steps to rotate the Azure client secret in the Infratructure UI: Go to one.newrelic.com > Infrastructure > Azure and click on Manage Services on the Azure account you wish to edit. Select the edit action next to Account Name to see and edit any application value. Edit the Client Secret field with the new value and confirm with Save Changes. Explore app data in New Relic Infrastructure's UI After you activate an Azure integration, New Relic will start monitoring your Azure data at regular polling intervals. To find and use your data, use the data explorer or go to one.newrelic.com > Infrastructure > Azure",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 136.31354,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Activate <em>Azure</em> <em>integrations</em>",
        "sections": "Activate <em>Azure</em> <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> allow you to report data from specific systems and supplement infrastructure&#x27;s default, automatic monitoring. The <em>Microsoft</em> <em>Azure</em> <em>integrations</em> report data from various <em>Azure</em> platform services to your New Relic account. This document explains how to activate"
      },
      "id": "6044e5a9196a671bfa960f79"
    },
    {
      "sections": [
        "Polling intervals for Azure integrations",
        "View polling data",
        "New Relic polling intervals"
      ],
      "title": "Polling intervals for Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "82db3eae120c4318365cf0d0e5bfee69930b969f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/getting-started/polling-intervals-azure-integrations/",
      "published_at": "2021-10-08T06:38:20Z",
      "updated_at": "2021-03-13T03:47:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Azure integrations query your Azure services according to a polling interval specific to the integration. The polling interval applies for every Azure entity related to the integrated service. For example, if you have thirteen CosmosDB instances, each of the thirteen instances will be polled every five minutes. View polling data After you activate an Azure integration, New Relic starts polling data from Azure and makes the data accessible through infrastructure Inventory and New Relic dashboards. You can query the Azure data along with additional data imported from any other New Relic features. You can also view dashboard data for a specific integration or across your account. For visualizations of polling intervals, API calls, and other data for your Azure integrations: Go to one.newrelic.com > Infrastructure > Azure. To view data for a specific integration: Select the Dashboards link for the integration's row. New Relic polling intervals For polling and resolution details, see the documentation for a specific integration.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 123.20318,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Polling intervals for <em>Azure</em> <em>integrations</em>",
        "sections": "Polling intervals for <em>Azure</em> <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>Azure</em> <em>integrations</em> query your <em>Azure</em> services according to a polling interval specific to the integration. The polling interval applies for every <em>Azure</em> entity related to the integrated service. For example, if you have thirteen CosmosDB instances, each of the thirteen instances"
      },
      "id": "6044e560196a671d6f960f72"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/get-started/introduction-azure-monitoring-integrations": [
    {
      "sections": [
        "Azure integration metrics",
        "BETA FEATURE",
        "Azure Metrics"
      ],
      "title": "Azure integration metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "762631e1209bb9abb60f1ea8b185a6def61735b9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/azure-integration-metrics/",
      "published_at": "2021-10-08T11:30:42Z",
      "updated_at": "2021-09-14T18:17:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. Azure Metrics The following table contains the metrics we collect for Azure. Integration Dimensional Metric Name (new) Sample Metric Name (previous) Azure API Management azure.apimanagement.service.Capacity capacityPercent Azure API Management azure.apimanagement.service.Duration durationMilliseconds Azure API Management azure.apimanagement.service.EventHubDroppedEvents eventHubDroppedEvents Azure API Management azure.apimanagement.service.EventHubRejectedEvents eventHubRejectedEvents Azure API Management azure.apimanagement.service.EventHubSuccessfulEvents eventHubSuccessfulEvents Azure API Management azure.apimanagement.service.EventHubThrottledEvents eventHubThrottledEvents Azure API Management azure.apimanagement.service.EventHubTimedoutEvents eventHubTimedoutEvents Azure API Management azure.apimanagement.service.EventHubTotalBytesSent eventHubTotalBytesSentBytes Azure API Management azure.apimanagement.service.EventHubTotalEvents eventHubTotalEvents Azure API Management azure.apimanagement.service.EventHubTotalFailedEvents eventHubTotalFailedEvents Azure API Management azure.apimanagement.service.FailedRequests failedRequests Azure API Management azure.apimanagement.service.OtherRequests otherRequests Azure API Management azure.apimanagement.service.SuccessfulRequests successfulRequests Azure API Management azure.apimanagement.service.TotalRequests totalRequests Azure API Management azure.apimanagement.service.UnauthorizedRequests unauthorizedRequests Azure App Gateway azure.network.applicationgateways.ApplicationGatewayTotalTime applicationGatewayTotalTimeMilliseconds Azure App Gateway azure.network.applicationgateways.AvgRequestCountPerHealthyHost avgRequestCountPerHealthyHost Azure App Gateway azure.network.applicationgateways.BackendConnectTime backendConnectTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendFirstByteResponseTime backendFirstByteResponseTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendLastByteResponseTime backendLastByteResponseTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendResponseStatus backendResponseStatus Azure App Gateway azure.network.applicationgateways.BlockedCount blockedCount Azure App Gateway azure.network.applicationgateways.BlockedReqCount blockedReqCount Azure App Gateway azure.network.applicationgateways.BytesReceived bytesReceivedBytes Azure App Gateway azure.network.applicationgateways.BytesSent bytesSentBytes Azure App Gateway azure.network.applicationgateways.CapacityUnits capacityUnits Azure App Gateway azure.network.applicationgateways.ClientRtt clientRttMilliseconds Azure App Gateway azure.network.applicationgateways.ComputeUnits computeUnits Azure App Gateway azure.network.applicationgateways.CpuUtilization cpuUtilizationPercent Azure App Gateway azure.network.applicationgateways.CurrentConnections currentConnections Azure App Gateway azure.network.applicationgateways.EstimatedBilledCapacityUnits estimatedBilledCapacityUnits Azure App Gateway azure.network.applicationgateways.FailedRequests failedRequests Azure App Gateway azure.network.applicationgateways.FixedBillableCapacityUnits fixedBillableCapacityUnits Azure App Gateway azure.network.applicationgateways.HealthyHostCount healthyHostCount Azure App Gateway azure.network.applicationgateways.MatchedCount matchedCount Azure App Gateway azure.network.applicationgateways.NewConnectionsPerSecond newConnectionsPerSecondCountPerSecond Azure App Gateway azure.network.applicationgateways.ResponseStatus responseStatus Azure App Gateway azure.network.applicationgateways.Throughput throughputBytesPerSecond Azure App Gateway azure.network.applicationgateways.TlsProtocol tlsProtocol Azure App Gateway azure.network.applicationgateways.TotalRequests totalRequests Azure App Gateway azure.network.applicationgateways.UnhealthyHostCount unhealthyHostCount Azure App Service azure.web.serverfarms.BytesReceived bytesReceivedBytes Azure App Service azure.web.serverfarms.BytesSent bytesSentBytes Azure App Service azure.web.serverfarms.CpuPercentage cpuPercent Azure App Service azure.web.serverfarms.DiskQueueLength diskQueueLength Azure App Service azure.web.serverfarms.HttpQueueLength httpQueueLength Azure App Service azure.web.serverfarms.MemoryPercentage memoryPercent Azure App Service azure.web.sites.AppConnections appConnections Azure App Service azure.web.sites.AverageMemoryWorkingSet.byWebApp averageMemoryWorkingSetBytes Azure App Service azure.web.sites.AverageResponseTime averageResponseTimeSeconds Azure App Service azure.web.sites.BytesReceived.byWebApp receivedBytes Azure App Service azure.web.sites.BytesSent.byWebApp sentBytes Azure App Service azure.web.sites.CpuTime cpuTimeSeconds Azure App Service azure.web.sites.CurrentAssemblies currentAssemblies Azure App Service azure.web.sites.Gen0Collections gen0Collections Azure App Service azure.web.sites.Gen1Collections gen1Collections Azure App Service azure.web.sites.Gen2Collections gen2Collections Azure App Service azure.web.sites.Handles handles Azure App Service azure.web.sites.Http101 http101 Azure App Service azure.web.sites.Http2xx http2xx Azure App Service azure.web.sites.Http3xx http3xx Azure App Service azure.web.sites.Http401 http401 Azure App Service azure.web.sites.Http403 http403 Azure App Service azure.web.sites.Http404 http404 Azure App Service azure.web.sites.Http406 http406 Azure App Service azure.web.sites.Http4xx http4xx Azure App Service azure.web.sites.Http5xx.byWebApp http5xx Azure App Service azure.web.sites.MemoryWorkingSet.byWebApp memoryWorkingSetBytes Azure App Service azure.web.sites.Requests requests Azure App Service azure.web.sites.Threads threads Azure App Service azure.web.sites.TotalAppDomains totalAppDomains Azure App Service azure.web.sites.TotalAppDomainsUnloaded totalAppDomainsUnloaded Azure Containers azure.containerinstance.containergroups.CpuUsage cpuUsage Azure Containers azure.containerinstance.containergroups.MemoryUsage memoryUsageBytes Azure Containers azure.containerinstance.containergroups.NetworkBytesReceivedPerSecond networkReceivedBytesPerSecond Azure Containers azure.containerinstance.containergroups.NetworkBytesTransmittedPerSecond networkTransmittedBytesPerSecond Azure Containers azure.containerregistry.registries.RunDuration runDurationMilliseconds Azure Containers azure.containerregistry.registries.SuccessfulPullCount successfulPullCount Azure Containers azure.containerregistry.registries.SuccessfulPushCount successfulPushCount Azure Containers azure.containerregistry.registries.TotalPullCount totalPullCount Azure Containers azure.containerregistry.registries.TotalPushCount totalPushCount Azure Containers azure.containerservice.managedclusters.kube_node_status_allocatable_cpu_cores kubeNodeStatusAllocatableCpuCores Azure Containers azure.containerservice.managedclusters.kube_node_status_allocatable_memory_bytes kubeNodeStatusAllocatableMemoryBytes Azure Containers azure.containerservice.managedclusters.kube_node_status_condition kubeNodeStatusCondition Azure Containers azure.containerservice.managedclusters.kube_pod_status_phase kubePodStatusPhase Azure Containers azure.containerservice.managedclusters.kube_pod_status_ready kubePodStatusReady Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byAccount availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byAccount cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byAccount cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byAccount cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byAccount dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byAccount documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byAccount documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byAccount indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byAccount metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byAccount mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byAccount mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byAccount provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byAccount replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byAccount serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byAccount totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byAccount totalRequestUnits Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byCollection availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byCollection cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byCollection cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byCollection cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byCollection dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byCollection documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byCollection documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byCollection indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byCollection metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byCollection mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byCollection mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byCollection provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byCollection replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byCollection serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byCollection totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byCollection totalRequestUnits Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byDatabase availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byDatabase cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byDatabase cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byDatabase cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byDatabase dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byDatabase documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byDatabase documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byDatabase indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byDatabase metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byDatabase mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byDatabase mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byDatabase provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byDatabase replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byDatabase serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byDatabase totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byDatabase totalRequestUnits Azure Cost Management azure.costmanagement.cost.byLocation cost Azure Cost Management azure.costmanagement.cost.byResourceGroup cost Azure Cost Management azure.costmanagement.cost.byService cost Azure Cost Management azure.costmanagement.cost.byTag cost Azure Data Factory azure.datafactory.datafactories.FailedRuns failedRuns Azure Data Factory azure.datafactory.datafactories.SuccessfulRuns successfulRuns Azure Data Factory azure.datafactory.factories.ActivityCancelledRuns activityCancelledRuns Azure Data Factory azure.datafactory.factories.ActivityFailedRuns activityFailedRuns Azure Data Factory azure.datafactory.factories.ActivitySucceededRuns activitySucceededRuns Azure Data Factory azure.datafactory.factories.FactorySizeInGbUnits factorySizeInGbUnits Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAvailableMemory integrationRuntimeAvailableMemoryBytes Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAvailableNodeNumber integrationRuntimeAvailableNodeNumber Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAverageTaskPickupDelay integrationRuntimeAverageTaskPickupDelaySeconds Azure Data Factory azure.datafactory.factories.IntegrationRuntimeCpuPercentage integrationRuntimeCpuPercentagePercent Azure Data Factory azure.datafactory.factories.IntegrationRuntimeQueueLength integrationRuntimeQueueLength Azure Data Factory azure.datafactory.factories.MaxAllowedFactorySizeInGbUnits maxAllowedFactorySizeInGbUnits Azure Data Factory azure.datafactory.factories.MaxAllowedResourceCount maxAllowedResourceCount Azure Data Factory azure.datafactory.factories.PipelineCancelledRuns pipelineCancelledRuns Azure Data Factory azure.datafactory.factories.PipelineFailedRuns pipelineFailedRuns Azure Data Factory azure.datafactory.factories.PipelineSucceededRuns pipelineSucceededRuns Azure Data Factory azure.datafactory.factories.ResourceCount resourceCount Azure Data Factory azure.datafactory.factories.TriggerCancelledRuns triggerCancelledRuns Azure Data Factory azure.datafactory.factories.TriggerFailedRuns triggerFailedRuns Azure Data Factory azure.datafactory.factories.TriggerSucceededRuns triggerSucceededRuns Azure Database for MariaDB azure.dbformariadb.servers.active_connections activeConnections Azure Database for MariaDB azure.dbformariadb.servers.backup_storage_used backupStorageUsedBytes Azure Database for MariaDB azure.dbformariadb.servers.connections_failed connectionsFailed Azure Database for MariaDB azure.dbformariadb.servers.cpu_percent cpuPercent Azure Database for MariaDB azure.dbformariadb.servers.io_consumption_percent ioConsumptionPercent Azure Database for MariaDB azure.dbformariadb.servers.memory_percent memoryPercent Azure Database for MariaDB azure.dbformariadb.servers.network_bytes_egress networkEgressBytes Azure Database for MariaDB azure.dbformariadb.servers.network_bytes_ingress networkIngressBytes Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for MariaDB azure.dbformariadb.servers.storage_limit storageLimitBytes Azure Database for MariaDB azure.dbformariadb.servers.storage_percent storagePercent Azure Database for MariaDB azure.dbformariadb.servers.storage_used storageUsedBytes Azure Database for MySQL azure.dbformysql.servers.active_connections activeConnections Azure Database for MySQL azure.dbformysql.servers.backup_storage_used backupStorageUsedBytes Azure Database for MySQL azure.dbformysql.servers.connections_failed connectionsFailed Azure Database for MySQL azure.dbformysql.servers.cpu_percent cpuPercent Azure Database for MySQL azure.dbformysql.servers.io_consumption_percent ioConsumptionPercent Azure Database for MySQL azure.dbformysql.servers.memory_percent memoryPercent Azure Database for MySQL azure.dbformysql.servers.network_bytes_egress networkEgressBytes Azure Database for MySQL azure.dbformysql.servers.network_bytes_ingress networkIngressBytes Azure Database for MySQL azure.dbformysql.servers.seconds_behind_master secondsBehindMaster Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for MySQL azure.dbformysql.servers.storage_limit storageLimitBytes Azure Database for MySQL azure.dbformysql.servers.storage_percent storagePercent Azure Database for MySQL azure.dbformysql.servers.storage_used storageUsedBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.active_connections activeConnections Azure Database for PostgreSQL azure.dbforpostgresql.servers.backup_storage_used backupStorageUsedBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.connections_failed connectionsFailed Azure Database for PostgreSQL azure.dbforpostgresql.servers.cpu_percent cpuPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.io_consumption_percent ioConsumptionPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.memory_percent memoryPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.network_bytes_egress networkEgressBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.network_bytes_ingress networkIngressBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.pg_replica_log_delay_in_bytes pgReplicaLogDelayBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.pg_replica_log_delay_in_seconds pgReplicaLogDelaySeconds Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_limit storageLimitBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_percent storagePercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_used storageUsedBytes Azure Event Hub azure.eventhub.namespaces.ActiveConnections activeConnections Azure Event Hub azure.eventhub.namespaces.CaptureBacklog captureBacklog Azure Event Hub azure.eventhub.namespaces.CapturedBytes capturedBytes Azure Event Hub azure.eventhub.namespaces.CapturedMessages capturedMessages Azure Event Hub azure.eventhub.namespaces.ConnectionsClosed connectionsClosed Azure Event Hub azure.eventhub.namespaces.ConnectionsOpened connectionsOpened Azure Event Hub azure.eventhub.namespaces.IncomingBytes incomingBytes Azure Event Hub azure.eventhub.namespaces.IncomingMessages incomingMessages Azure Event Hub azure.eventhub.namespaces.IncomingRequests incomingRequests Azure Event Hub azure.eventhub.namespaces.OutgoingBytes outgoingBytes Azure Event Hub azure.eventhub.namespaces.OutgoingMessages outgoingMessages Azure Event Hub azure.eventhub.namespaces.QuotaExceededErrors quotaExceededErrors Azure Event Hub azure.eventhub.namespaces.ServerErrors serverErrors Azure Event Hub azure.eventhub.namespaces.Size sizeBytes Azure Event Hub azure.eventhub.namespaces.SuccessfulRequests successfulRequests Azure Event Hub azure.eventhub.namespaces.ThrottledRequests throttledRequests Azure Event Hub azure.eventhub.namespaces.UserErrors userErrors Azure Event Hub azure.eventhub.clusters.ActiveConnections activeConnections Azure Event Hub azure.eventhub.clusters.AvailableMemory availableMemoryPercent Azure Event Hub azure.eventhub.clusters.CaptureBacklog captureBacklog Azure Event Hub azure.eventhub.clusters.CapturedBytes capturedBytes Azure Event Hub azure.eventhub.clusters.CapturedMessages capturedMessages Azure Event Hub azure.eventhub.clusters.ConnectionsClosed connectionsClosed Azure Event Hub azure.eventhub.clusters.ConnectionsOpened connectionsOpened Azure Event Hub azure.eventhub.clusters.CPU cpuPercent Azure Event Hub azure.eventhub.clusters.IncomingBytes incomingBytes Azure Event Hub azure.eventhub.clusters.IncomingMessages incomingMessages Azure Event Hub azure.eventhub.clusters.IncomingRequests incomingRequests Azure Event Hub azure.eventhub.clusters.OutgoingBytes outgoingBytes Azure Event Hub azure.eventhub.clusters.OutgoingMessages outgoingMessages Azure Event Hub azure.eventhub.clusters.QuotaExceededErrors quotaExceededErrors Azure Event Hub azure.eventhub.clusters.ServerErrors serverErrors Azure Event Hub azure.eventhub.clusters.Size sizeBytes Azure Event Hub azure.eventhub.clusters.SuccessfulRequests successfulRequests Azure Event Hub azure.eventhub.clusters.ThrottledRequests throttledRequests Azure Event Hub azure.eventhub.clusters.UserErrors userErrors Azure Express Route azure.network.expressrouteports.AdminState adminState Azure Express Route azure.network.expressrouteports.LineProtocol lineProtocol Azure Express Route azure.network.expressrouteports.PortBitsInPerSecond portBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressrouteports.PortBitsOutPerSecond portBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressrouteports.RxLightLevel rxLightLevel Azure Express Route azure.network.expressrouteports.TxLightLevel txLightLevel Azure Express Route azure.network.expressroutecircuits.ArpAvailability arpAvailabilityPercent Azure Express Route azure.network.expressroutecircuits.BgpAvailability bgpAvailabilityPercent Azure Express Route azure.network.expressroutecircuits.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.GlobalReachBitsInPerSecond globalReachBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.GlobalReachBitsOutPerSecond globalReachBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.QosDropBitsInPerSecond qosDropBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.QosDropBitsOutPerSecond qosDropBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.peerings.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.peerings.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.connections.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.connections.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutegateways.ErGatewayConnectionBitsInPerSecond erGatewayConnectionBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutegateways.ErGatewayConnectionBitsOutPerSecond erGatewayConnectionBitsOutPerSecondCountPerSecond Azure Firewalls azure.network.azurefirewalls.ApplicationRuleHit applicationRuleHit Azure Firewalls azure.network.azurefirewalls.DataProcessed dataProcessedBytes Azure Firewalls azure.network.azurefirewalls.FirewallHealth firewallHealthPercent Azure Firewalls azure.network.azurefirewalls.NetworkRuleHit networkRuleHit Azure Firewalls azure.network.azurefirewalls.SNATPortUtilization sNATPortUtilizationPercent Azure Firewalls azure.network.azurefirewalls.Throughput throughputBitsPerSecond Azure Front Door azure.network.frontdoors.BackendHealthPercentage backendHealthPercent Azure Front Door azure.network.frontdoors.BackendRequestCount backendRequestCount Azure Front Door azure.network.frontdoors.BackendRequestLatency backendRequestLatencyMilliseconds Azure Front Door azure.network.frontdoors.BillableResponseSize billableResponseSizeBytes Azure Front Door azure.network.frontdoors.RequestCount requestCount Azure Front Door azure.network.frontdoors.RequestSize requestSizeBytes Azure Front Door azure.network.frontdoors.ResponseSize responseSizeBytes Azure Front Door azure.network.frontdoors.TotalLatency totalLatencyMilliseconds Azure Front Door azure.network.frontdoors.WebApplicationFirewallRequestCount webApplicationFirewallRequestCount Azure Functions azure.web.sites.AverageMemoryWorkingSet.byFunctionsApp averageMemoryWorkingSetBytes Azure Functions azure.web.sites.BytesReceived.byFunctionsApp receivedBytes Azure Functions azure.web.sites.BytesSent.byFunctionsApp sentBytes Azure Functions azure.web.sites.FunctionExecutionCount functionExecutionCount Azure Functions azure.web.sites.FunctionExecutionUnits functionExecutionUnits Azure Functions azure.web.sites.Http5xx.byFunctionsApp http5xx Azure Functions azure.web.sites.MemoryWorkingSet.byFunctionsApp memoryWorkingSetBytes Azure Key Vault azure.keyvault.vaults.Availability availabilityPercent Azure Key Vault azure.keyvault.vaults.SaturationShoebox saturationShoeboxPercent Azure Key Vault azure.keyvault.vaults.ServiceApiHit serviceApiHit Azure Key Vault azure.keyvault.vaults.ServiceApiLatency serviceApiLatencyMilliseconds Azure Key Vault azure.keyvault.vaults.ServiceApiResult serviceApiResult Azure Load Balancer azure.network.loadbalancers.AllocatedSnatPorts allocatedSnatPorts Azure Load Balancer azure.network.loadbalancers.ByteCount byteCountBytes Azure Load Balancer azure.network.loadbalancers.DipAvailability dipAvailability Azure Load Balancer azure.network.loadbalancers.PacketCount packetCount Azure Load Balancer azure.network.loadbalancers.SnatConnectionCount snatConnectionCount Azure Load Balancer azure.network.loadbalancers.SYNCount synCount Azure Load Balancer azure.network.loadbalancers.UsedSnatPorts usedSnatPorts Azure Load Balancer azure.network.loadbalancers.VipAvailability vipAvailability Azure Logic Apps azure.logic.workflows.ActionLatency actionLatencySeconds Azure Logic Apps azure.logic.workflows.ActionsCompleted actionsCompleted Azure Logic Apps azure.logic.workflows.ActionsFailed actionsFailed Azure Logic Apps azure.logic.workflows.ActionsSkipped actionsSkipped Azure Logic Apps azure.logic.workflows.ActionsStarted actionsStarted Azure Logic Apps azure.logic.workflows.ActionsSucceeded actionsSucceeded Azure Logic Apps azure.logic.workflows.ActionSuccessLatency actionSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.ActionThrottledEvents actionThrottledEvents Azure Logic Apps azure.logic.workflows.BillableActionExecutions billableActionExecutions Azure Logic Apps azure.logic.workflows.BillableTriggerExecutions billableTriggerExecutions Azure Logic Apps azure.logic.workflows.BillingUsageNativeOperation billingUsageNativeOperation Azure Logic Apps azure.logic.workflows.BillingUsageStandardConnector billingUsageStandardConnector Azure Logic Apps azure.logic.workflows.BillingUsageStorageConsumption billingUsageStorageConsumption Azure Logic Apps azure.logic.workflows.RunFailurePercentage runFailurePercent Azure Logic Apps azure.logic.workflows.RunLatency runLatencySeconds Azure Logic Apps azure.logic.workflows.RunsCancelled runsCancelled Azure Logic Apps azure.logic.workflows.RunsCompleted runsCompleted Azure Logic Apps azure.logic.workflows.RunsFailed runsFailed Azure Logic Apps azure.logic.workflows.RunsStarted runsStarted Azure Logic Apps azure.logic.workflows.RunsSucceeded runsSucceeded Azure Logic Apps azure.logic.workflows.RunStartThrottledEvents runStartThrottledEvents Azure Logic Apps azure.logic.workflows.RunSuccessLatency runSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.RunThrottledEvents runThrottledEvents Azure Logic Apps azure.logic.workflows.TotalBillableExecutions totalBillableExecutions Azure Logic Apps azure.logic.workflows.TriggerFireLatency triggerFireLatencySeconds Azure Logic Apps azure.logic.workflows.TriggerLatency triggerLatencySeconds Azure Logic Apps azure.logic.workflows.TriggersCompleted triggersCompleted Azure Logic Apps azure.logic.workflows.TriggersFailed triggersFailed Azure Logic Apps azure.logic.workflows.TriggersFired triggersFired Azure Logic Apps azure.logic.workflows.TriggersSkipped triggersSkipped Azure Logic Apps azure.logic.workflows.TriggersStarted triggersStarted Azure Logic Apps azure.logic.workflows.TriggersSucceeded triggersSucceeded Azure Logic Apps azure.logic.workflows.TriggerSuccessLatency triggerSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.TriggerThrottledEvents triggerThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.ActionLatency actionLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsCompleted actionsCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsFailed actionsFailed Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsSkipped actionsSkipped Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsStarted actionsStarted Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsSucceeded actionsSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.ActionSuccessLatency actionSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.ActionThrottledEvents actionThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentConnectorMemoryUsage integrationServiceEnvironmentConnectorMemoryUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentConnectorProcessorUsage integrationServiceEnvironmentConnectorProcessorUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentWorkflowMemoryUsage integrationServiceEnvironmentWorkflowMemoryUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentWorkflowProcessorUsage integrationServiceEnvironmentWorkflowProcessorUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.RunFailurePercentage runFailurePercent Azure Logic Apps azure.logic.integrationserviceenvironments.RunLatency runLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.RunsCancelled runsCancelled Azure Logic Apps azure.logic.integrationserviceenvironments.RunsCompleted runsCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.RunsFailed runsFailed Azure Logic Apps azure.logic.integrationserviceenvironments.RunsStarted runsStarted Azure Logic Apps azure.logic.integrationserviceenvironments.RunsSucceeded runsSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.RunStartThrottledEvents runStartThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.RunSuccessLatency runSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.RunThrottledEvents runThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerFireLatency triggerFireLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerLatency triggerLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersCompleted triggersCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersFailed triggersFailed Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersFired triggersFired Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersSkipped triggersSkipped Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersStarted triggersStarted Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersSucceeded triggersSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerSuccessLatency triggerSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerThrottledEvents triggerThrottledEvents Azure Machine Learning azure.machinelearningservices.workspaces.ActiveCores activeCores Azure Machine Learning azure.machinelearningservices.workspaces.ActiveNodes activeNodes Azure Machine Learning azure.machinelearningservices.workspaces.CompletedRuns completedRuns Azure Machine Learning azure.machinelearningservices.workspaces.CpuUtilization cpuUtilization Azure Machine Learning azure.machinelearningservices.workspaces.FailedRuns failedRuns Azure Machine Learning azure.machinelearningservices.workspaces.GpuUtilization gpuUtilization Azure Machine Learning azure.machinelearningservices.workspaces.IdleCores idleCores Azure Machine Learning azure.machinelearningservices.workspaces.IdleNodes idleNodes Azure Machine Learning azure.machinelearningservices.workspaces.LeavingCores leavingCores Azure Machine Learning azure.machinelearningservices.workspaces.LeavingNodes leavingNodes Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeployFailed modelDeployFailed Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeployStarted modelDeployStarted Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeploySucceeded modelDeploySucceeded Azure Machine Learning azure.machinelearningservices.workspaces.ModelRegisterFailed modelRegisterFailed Azure Machine Learning azure.machinelearningservices.workspaces.ModelRegisterSucceeded modelRegisterSucceeded Azure Machine Learning azure.machinelearningservices.workspaces.PreemptedCores preemptedCores Azure Machine Learning azure.machinelearningservices.workspaces.PreemptedNodes preemptedNodes Azure Machine Learning azure.machinelearningservices.workspaces.QuotaUtilizationPercentage quotaUtilizationPercentage Azure Machine Learning azure.machinelearningservices.workspaces.StartedRuns startedRuns Azure Machine Learning azure.machinelearningservices.workspaces.TotalCores totalCores Azure Machine Learning azure.machinelearningservices.workspaces.TotalNodes totalNodes Azure Machine Learning azure.machinelearningservices.workspaces.UnusableCores unusableCores Azure Machine Learning azure.machinelearningservices.workspaces.UnusableNodes unusableNodes Azure Power BI Dedicated azure.powerbidedicated.capacities.memory_metric memoryMetricBytes Azure Power BI Dedicated azure.powerbidedicated.capacities.memory_thrashing_metric memoryThrashingMetricPercent Azure Power BI Dedicated azure.powerbidedicated.capacities.qpu_high_utilization_metric qpuHighUtilizationMetric Azure Power BI Dedicated azure.powerbidedicated.capacities.QueryDuration queryDurationMilliseconds Azure Power BI Dedicated azure.powerbidedicated.capacities.QueryPoolJobQueueLength queryPoolJobQueueLength Azure Redis azure.cache.redis.cachehits cacheHits Azure Redis azure.cache.redis.cachemisses cacheMisses Azure Redis azure.cache.redis.cacheRead cacheReadBytesPerSecond Azure Redis azure.cache.redis.cacheWrite cacheWriteBytesPerSecond Azure Redis azure.cache.redis.connectedclients connectedClients Azure Redis azure.cache.redis.evictedkeys evictedKeys Azure Redis azure.cache.redis.expiredkeys expiredKeys Azure Redis azure.cache.redis.getcommands getCommands Azure Redis azure.cache.redis.operationsPerSecond operationsPerSecond Azure Redis azure.cache.redis.percentProcessorTime processorTimePercent Azure Redis azure.cache.redis.serverLoad serverLoadPercent Azure Redis azure.cache.redis.setcommands setCommands Azure Redis azure.cache.redis.totalcommandsprocessed totalCommandsProcessed Azure Redis azure.cache.redis.totalkeys totalKeys Azure Redis azure.cache.redis.usedmemory usedMemoryBytes Azure Redis azure.cache.redis.usedmemoryRss usedMemoryRssBytes Azure Redis azure.cache.redis.cachehits0 cacheHits Azure Redis azure.cache.redis.cachemisses0 cacheMisses Azure Redis azure.cache.redis.cacheRead0 cacheReadBytesPerSecond Azure Redis azure.cache.redis.cacheWrite0 cacheWriteBytesPerSecond Azure Redis azure.cache.redis.connectedclients0 connectedClients Azure Redis azure.cache.redis.evictedkeys0 evictedKeys Azure Redis azure.cache.redis.expiredkeys0 expiredKeys Azure Redis azure.cache.redis.getcommands0 getCommands Azure Redis azure.cache.redis.operationsPerSecond0 operationsPerSecond Azure Redis azure.cache.redis.percentProcessorTime0 processorTimePercent Azure Redis azure.cache.redis.serverLoad0 serverLoadPercent Azure Redis azure.cache.redis.setcommands0 setCommands Azure Redis azure.cache.redis.totalcommandsprocessed0 totalCommandsProcessed Azure Redis azure.cache.redis.totalkeys0 totalKeys Azure Redis azure.cache.redis.usedmemory0 usedMemoryBytes Azure Redis azure.cache.redis.usedmemoryRss0 usedMemoryRssBytes Azure Service Bus azure.servicebus.namespaces.ActiveConnections activeConnections Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byNamespace activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byNamespace connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byNamespace connectionsOpened Azure Service Bus azure.servicebus.namespaces.CPUXNS cpuUsagePercent Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byNamespace deadletteredMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byNamespace incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byNamespace incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byNamespace messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byNamespace outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byNamespace scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byNamespace serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byNamespace sizeBytes Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byNamespace successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byNamespace throttledRequests Azure Service Bus azure.servicebus.namespaces.UserErrors.byNamespace userErrors Azure Service Bus azure.servicebus.namespaces.WSXNS memoryUsagePercent Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byQueue activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byQueue connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byQueue connectionsOpened Azure Service Bus azure.servicebus.namespaces.currentSizeInBytes.byQueue currentSizeBytes Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byQueue deadletteredMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.byQueue deadLetterMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byQueue incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byQueue incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byQueue messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byQueue outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byQueue scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byQueue serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byQueue sizeBytes Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byQueue successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byQueue throttledRequests Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.byQueue transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.byQueue transferMessages Azure Service Bus azure.servicebus.namespaces.UserErrors.byQueue userErrors Azure Service Bus azure.servicebus.namespaces.activeMessageCount activeMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.bySubscription deadLetterMessages Azure Service Bus azure.servicebus.namespaces.messageCount messages Azure Service Bus azure.servicebus.namespaces.scheduledMessageCount scheduledMessages Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.bySubscription transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.bySubscription transferMessages Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byTopic activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byTopic connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byTopic connectionsOpened Azure Service Bus azure.servicebus.namespaces.currentSizeInBytes.byTopic currentSizeInBytes Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byTopic deadletteredMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.byTopic deadLetterMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byTopic incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byTopic incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byTopic messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byTopic outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byTopic scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byTopic serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byTopic sizeBytes Azure Service Bus azure.servicebus.namespaces.subscriptionCount subscriptions Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byTopic successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byTopic throttledRequests Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.byTopic transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.byTopic transferMessages Azure Service Bus azure.servicebus.namespaces.UserErrors.byTopic userErrors Azure Service Fabric azure.servicefabricmesh.applications.ActualCpu actualCpu Azure Service Fabric azure.servicefabricmesh.applications.ActualMemory actualMemoryBytes Azure Service Fabric azure.servicefabricmesh.applications.AllocatedCpu allocatedCpu Azure Service Fabric azure.servicefabricmesh.applications.AllocatedMemory allocatedMemoryBytes Azure Service Fabric azure.servicefabricmesh.applications.ApplicationStatus applicationStatus Azure Service Fabric azure.servicefabricmesh.applications.ContainerStatus containerStatus Azure Service Fabric azure.servicefabricmesh.applications.CpuUtilization cpuUtilizationPercent Azure Service Fabric azure.servicefabricmesh.applications.MemoryUtilization memoryUtilizationPercent Azure Service Fabric azure.servicefabricmesh.applications.RestartCount restartCount Azure Service Fabric azure.servicefabricmesh.applications.ServiceReplicaStatus serviceReplicaStatus Azure Service Fabric azure.servicefabricmesh.applications.ServiceStatus serviceStatus Azure SQL azure.sql.servers.database.currentSize databaseSizeCurrentBytes Azure SQL azure.sql.servers.database.limitSize databaseSizeLimitBytes Azure SQL azure.sql.servers.databases.blocked_by_firewall blockedByFirewall Azure SQL azure.sql.servers.databases.connection_failed connectionFailed Azure SQL azure.sql.servers.databases.connection_successful connectionSuccessful Azure SQL azure.sql.servers.databases.cpu_percent cpuPercent Azure SQL azure.sql.servers.databases.deadlock deadlock Azure SQL azure.sql.servers.databases.dtu_consumption_percent dtuConsumptionPercent Azure SQL azure.sql.servers.databases.dtu_limit dtuLimit Azure SQL azure.sql.servers.databases.dtu_used dtuUsed Azure SQL azure.sql.servers.databases.dw_cpu_percent dwCpuPercent Azure SQL azure.sql.servers.databases.dw_physical_data_read_percent dwPhysicalDataReadPercent Azure SQL azure.sql.servers.databases.dwu_consumption_percent dwuConsumptionPercent Azure SQL azure.sql.servers.databases.dwu_limit dwuLimit Azure SQL azure.sql.servers.databases.dwu_used dwuUsed Azure SQL azure.sql.servers.databases.log_write_percent logWritePercent Azure SQL azure.sql.servers.databases.physical_data_read_percent physicalDataReadPercent Azure SQL azure.sql.servers.databases.sessions_percent sessionsPercent Azure SQL azure.sql.servers.databases.storage storageBytes Azure SQL azure.sql.servers.databases.storage_percent storagePercent Azure SQL azure.sql.servers.databases.workers_percent workersPercent Azure SQL azure.sql.servers.databases.xtp_storage_percent xtpStoragePercent Azure SQL azure.sql.elasticPool.database_physical_data_read_percent databasePhysicalDataRead Azure SQL azure.sql.elasticPool.database_storage_used databaseStorageUsed Azure SQL azure.sql.servers.elasticpools.cpu_percent cpuPercent Azure SQL azure.sql.servers.elasticpools.database_cpu_percent databaseCpuPercent Azure SQL azure.sql.servers.elasticpools.database_dtu_consumption_percent databaseDtuConsumptionPercent Azure SQL azure.sql.servers.elasticpools.database_log_write_percent databaseLogWritePercent Azure SQL azure.sql.servers.elasticpools.database_sessions_percent databaseSessionsPercent Azure SQL azure.sql.servers.elasticpools.database_workers_percent databaseWorkersPercent Azure SQL azure.sql.servers.elasticpools.dtu_consumption_percent dtuConsumptionPercent Azure SQL azure.sql.servers.elasticpools.eDTU_limit eDTULimit Azure SQL azure.sql.servers.elasticpools.eDTU_used eDTUUsed Azure SQL azure.sql.servers.elasticpools.log_write_percent logWritePercent Azure SQL azure.sql.servers.elasticpools.physical_data_read_percent physicalDataReadPercent Azure SQL azure.sql.servers.elasticpools.sessions_percent sessionsPercent Azure SQL azure.sql.servers.elasticpools.storage_limit storageLimitBytes Azure SQL azure.sql.servers.elasticpools.storage_percent storagePercent Azure SQL azure.sql.servers.elasticpools.storage_used storageUsedBytes Azure SQL azure.sql.servers.elasticpools.workers_percent workersPercent Azure SQL azure.sql.servers.elasticpools.xtp_storage_percent xtpStoragePercent Azure SQL azure.sql.server.dtuLimit dtuLimit Azure SQL azure.sql.servers.dtuCurrent dtuCurrent Azure SQL Managed Instance azure.sql.managedinstances.avg_cpu_percent avgCpuPercent Azure SQL Managed Instance azure.sql.managedinstances.io_bytes_read ioReadBytes Azure SQL Managed Instance azure.sql.managedinstances.io_bytes_written ioWrittenBytes Azure SQL Managed Instance azure.sql.managedinstances.io_requests ioRequests Azure SQL Managed Instance azure.sql.managedinstances.reserved_storage_mb reservedStorage Azure SQL Managed Instance azure.sql.managedinstances.storage_space_used_mb storageSpaceUsed Azure SQL Managed Instance azure.sql.managedinstances.virtual_core_count virtualCore Azure Storage Account azure.storage.storageaccounts.Availability availabilityPercent Azure Storage Account azure.storage.storageaccounts.blobservices.Availability blobs.availabilityPercent Azure Storage Account azure.storage.storageaccounts.blobservices.BlobCapacity blobs.blobCapacityBytes Azure Storage Account azure.storage.storageaccounts.blobservices.BlobCount blobs.blobCount Azure Storage Account azure.storage.storageaccounts.blobservices.ContainerCount blobs.containerCount Azure Storage Account azure.storage.storageaccounts.blobservices.Egress blobs.egressBytes Azure Storage Account azure.storage.storageaccounts.blobservices.Ingress blobs.ingressBytes Azure Storage Account azure.storage.storageaccounts.blobservices.SuccessE2ELatency blobs.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.blobservices.SuccessServerLatency blobs.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.blobservices.Transactions blobs.transactions Azure Storage Account azure.storage.storageaccounts.Egress egressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.Availability files.availabilityPercent Azure Storage Account azure.storage.storageaccounts.fileservices.Egress files.egressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.FileCapacity files.fileCapacityBytes Azure Storage Account azure.storage.storageaccounts.fileservices.FileCount files.fileCount Azure Storage Account azure.storage.storageaccounts.fileservices.FileShareCount files.fileShareCount Azure Storage Account azure.storage.storageaccounts.fileservices.Ingress files.ingressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.SuccessE2ELatency files.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.fileservices.SuccessServerLatency files.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.fileservices.Transactions files.transactions Azure Storage Account azure.storage.storageaccounts.Ingress ingressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.Availability queues.availabilityPercent Azure Storage Account azure.storage.storageaccounts.queueservices.Egress queues.egressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.Ingress queues.ingressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.QueueCapacity queues.queueCapacityBytes Azure Storage Account azure.storage.storageaccounts.queueservices.QueueCount queues.queueCount Azure Storage Account azure.storage.storageaccounts.queueservices.QueueMessageCount queues.queueMessagesCount Azure Storage Account azure.storage.storageaccounts.queueservices.SuccessE2ELatency queues.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.queueservices.SuccessServerLatency queues.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.queueservices.Transactions queues.transactions Azure Storage Account azure.storage.storageaccounts.SuccessE2ELatency successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.SuccessServerLatency successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.Availability tables.availabilityPercent Azure Storage Account azure.storage.storageaccounts.tableservices.Egress tables.egressBytes Azure Storage Account azure.storage.storageaccounts.tableservices.Ingress tables.ingressBytes Azure Storage Account azure.storage.storageaccounts.tableservices.SuccessE2ELatency tables.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.SuccessServerLatency tables.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.TableCapacity tables.tableCapacityBytes Azure Storage Account azure.storage.storageaccounts.tableservices.TableCount tables.tableCount Azure Storage Account azure.storage.storageaccounts.tableservices.TableEntityCount tables.tableEntityCount Azure Storage Account azure.storage.storageaccounts.tableservices.Transactions tables.transactions Azure Storage Account azure.storage.storageaccounts.Transactions transactions Azure Storage Account azure.storage.storageaccounts.UsedCapacity usedCapacityBytes Azure Virtual Network azure.network.virtualnetworks.PingMeshAverageRoundtripMs pingMeshAverageRoundtripMs Azure Virtual Network azure.network.virtualnetworks.PingMeshProbesFailedPercent pingMeshProbesFailedPercent Azure Virtual Network azure.network.publicipaddresses.BytesDroppedDDoS droppedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.BytesForwardedDDoS forwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.BytesInDDoS inDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.DDoSTriggerTCPPackets ddosTriggerTcpPacketsPerSecond Azure Virtual Network azure.network.publicipaddresses.DDoSTriggerUDPPackets ddosTriggerUdpPacketsPerSecond Azure Virtual Network azure.network.publicipaddresses.IfUnderDDoSAttack ifUnderDdosAttack Azure Virtual Network azure.network.publicipaddresses.PacketsDroppedDDoS packetsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.PacketsForwardedDDoS packetsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.PacketsInDDoS packetsInDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesDroppedDDoS tcpDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesForwardedDDoS tcpForwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesInDDoS tcpInDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsDroppedDDoS tcpPacketsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsForwardedDDoS tcpPacketsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsInDDoS tcpPacketsInDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesDroppedDDoS udpDroppedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesForwardedDDoS udpForwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesInDDoS udpInDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsDroppedDDoS udpPacketsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsForwardedDDoS udpPacketsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsInDDoS udpPacketsInDdosPerSecond Azure Virtual Network azure.network.virtualnetworks.availableAddresses availableAddresses Azure VMs Scale Sets azure.compute.virtualmachinescalesets.CPUCreditsConsumed cpuCreditsConsumed Azure VMs Scale Sets azure.compute.virtualmachinescalesets.CPUCreditsRemaining cpuCreditsRemaining Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskQueueDepth dataDiskQueueDepth Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskReadBytessec dataDiskReadBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskReadOperationsSec dataDiskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskWriteBytessec dataDiskWriteBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskWriteOperationsSec dataDiskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskReadBytes diskReadBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskReadOperationsSec diskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskWriteBytes diskWriteBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskWriteOperationsSec diskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.InboundFlows inboundFlows Azure VMs Scale Sets azure.compute.virtualmachinescalesets.InboundFlowsMaximumCreationRate inboundFlowsMaximumCreationRateCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.NetworkInTotal networkInTotalBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.NetworkOutTotal networkOutTotalBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskQueueDepth osDiskQueueDepth Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskReadBytessec osDiskReadBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskReadOperationsSec osDiskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskWriteBytessec osDiskWriteBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskWriteOperationsSec osDiskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OutboundFlows outboundFlows Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OutboundFlowsMaximumCreationRate outboundFlowsMaximumCreationRateCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PercentageCPU cpuPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumDataDiskCacheReadHit premiumDataDiskCacheReadHitPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumDataDiskCacheReadMiss premiumDataDiskCacheReadMissPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumOSDiskCacheReadHit premiumOsDiskCacheReadHitPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumOSDiskCacheReadMiss premiumOsDiskCacheReadMissPercent Azure VMs azure.compute.virtualmachines.DiskReadBytes diskReadBytes Azure VMs azure.compute.virtualmachines.DiskReadOperations.Sec diskReadOpsPerSecond Azure VMs azure.compute.virtualmachines.DiskWriteBytes diskWriteBytes Azure VMs azure.compute.virtualmachines.DiskWriteOperations.Sec diskWriteOpsPerSecond Azure VMs azure.compute.virtualmachines.NetworkIn networkInBytes Azure VMs azure.compute.virtualmachines.NetworkOut networkOutBytes Azure VMs azure.compute.virtualmachines.PercentageCPU cpuUsagePercent Azure VPN Gateways azure.network.virtualnetworkgateways.AverageBandwidth averageBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.P2SBandwidth p2SBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.P2SConnectionCount p2SConnectionCount Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelAverageBandwidth tunnelAverageBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressBytes tunnelEgressBytes Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressPacketDropTSMismatch tunnelEgressPacketDropTSMismatch Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressPackets tunnelEgressPackets Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressBytes tunnelIngressBytes Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressPacketDropTSMismatch tunnelIngressPacketDropTSMismatch Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressPackets tunnelIngressPackets",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 153.0643,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> <em>integration</em> metrics",
        "sections": "<em>Azure</em> <em>integration</em> metrics",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": " <em>azure</em>.logic.workflows.RunsFailed runsFailed <em>Azure</em> Logic Apps <em>azure.logic.workflows.RunsStarted</em> runs<em>Started</em> <em>Azure</em> Logic Apps <em>azure</em>.logic.workflows.RunsSucceeded runsSucceeded <em>Azure</em> Logic Apps <em>azure.logic.workflows.RunStart</em>ThrottledEvents run<em>Start</em>ThrottledEvents <em>Azure</em> Logic Apps"
      },
      "id": "603e8a8928ccbcacc0eba74e"
    },
    {
      "sections": [
        "Activate Azure integrations",
        "Requirements",
        "Step 1: Get Azure subscription and tenant IDs",
        "Step 2: Register your app and get ID",
        "Step 3: Create a client secret in Azure",
        "Step 4: Provide permissions to services",
        "Step 5: Add app to New Relic",
        "Update application details and rotate client secrets",
        "Explore app data in New Relic Infrastructure's UI"
      ],
      "title": "Activate Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f65679179e13aa1b503b4b95010e296cbe269c29",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/activate-azure-integrations/",
      "published_at": "2021-10-07T10:05:25Z",
      "updated_at": "2021-08-02T13:06:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations allow you to report data from specific systems and supplement infrastructure's default, automatic monitoring. The Microsoft Azure integrations report data from various Azure platform services to your New Relic account. This document explains how to activate Azure integrations. Requirements The Azure integration activation process requires you to: A New Relic account. Don't have one? Sign up for free! No credit card required. Create a New Relic application and key in Azure. Grant this application access to the Azure services you want to monitor. Place required information in the New Relic's Integrations UI. To use these integration activation instructions directly from the Infrastructure UI, go to one.newrelic.com > Infrastructure > Azure > Add an Azure account. Step 1: Get Azure subscription and tenant IDs To get your Azure account's subscription id and tenantId, use your local terminal if you have Azure's tools installed, or use Azure's Cloud Shell terminal in the Azure portal. Open a terminal with access to your Azure account. Type the following: az account show Copy Copy and save the subscription id and tenantID from the output response for later use. The response should look similar to the response below. The subscription id and tenantID are highlighted. @Azure:~$ az account show { \"environmentName\": \"AzureCloud\", \"id\": \"9ffe9512-f4a2-42dd-1230-518aec34be21\" , \"isDefault\": true, \"name\": \"Beyond Team Sandbox\", \"state\": \"Enabled\", \"tenantId\": \"ac6692da-1231-422f-22a8-9eed6dbe83f1\" , \"user\": { \"name\": \"youremail@domain\", \"type\": \"user\" } Copy Step 2: Register your app and get ID You must have Azure permissions to register your application and copy its Application ID. To register your app in Azure: Sign in to the Azure portal and go to the Azure Active Directory. From Manage, select App registrations > New registration. Enter a name for the application. We recommend that you name your app NewRelic-Integrations. In Redirect URI select Weband add https://www.newrelic.com as the sign-on URI. Create the application by clicking Register. From the Overview of your app, copy the Application (client) ID, and save it for later use. Step 3: Create a client secret in Azure To create a client secret associated with your application: In Azure, under the application you've just created, select Certificates & secrets. Under Client secrets, click on New client secret and then on Add. Copy the value of Client Secret and save it for later use. Step 4: Provide permissions to services Your app must provide Reader permissions for each Azure service you want New Relic to monitor: In the Azure Subscriptions section, select the subscriptions that you want New Relic to monitor. Select Access control (IAM) > Add > Add role assignment. From the Role dropdown, select Reader. From the Select dropdown, select the app's name; for example, NewRelic-Integrations. From Selected members, verify your app name appears, then select Save. Some Azure services, including Azure CosmosDB and Azure VMs, require additional steps. See the Azure integration documentation for the services you want to enable. Step 5: Add app to New Relic Now you can activate the Azure integration in the Infrastructure UI. The UI will require the information you have saved in the previous steps, including: Your Azure account's subscription id and tenantId The application's application ID The application's client secret To add your Azure app to New Relic: Go to one.newrelic.com > Infrastructure > Azure and select the Azure Service you wish to add. Follow the steps in the UI to activate the integration in New Relic. If you have already completed the Azure account steps, skip to the end of the steps to fill out the form. (For Azure account name, enter the name you want to use to identify the account in your Integrations dashboard.) Update application details and rotate client secrets It's possible to update the application's name and authentication credentials using the Infrastructure UI or the Cloud Integrations API at any time. Follow these steps to rotate the Azure client secret in the Infratructure UI: Go to one.newrelic.com > Infrastructure > Azure and click on Manage Services on the Azure account you wish to edit. Select the edit action next to Account Name to see and edit any application value. Edit the Client Secret field with the new value and confirm with Save Changes. Explore app data in New Relic Infrastructure's UI After you activate an Azure integration, New Relic will start monitoring your Azure data at regular polling intervals. To find and use your data, use the data explorer or go to one.newrelic.com > Infrastructure > Azure",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 136.31354,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Activate <em>Azure</em> <em>integrations</em>",
        "sections": "Activate <em>Azure</em> <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> allow you to report data from specific systems and supplement infrastructure&#x27;s default, automatic monitoring. The <em>Microsoft</em> <em>Azure</em> <em>integrations</em> report data from various <em>Azure</em> platform services to your New Relic account. This document explains how to activate"
      },
      "id": "6044e5a9196a671bfa960f79"
    },
    {
      "sections": [
        "Polling intervals for Azure integrations",
        "View polling data",
        "New Relic polling intervals"
      ],
      "title": "Polling intervals for Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "82db3eae120c4318365cf0d0e5bfee69930b969f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/getting-started/polling-intervals-azure-integrations/",
      "published_at": "2021-10-08T06:38:20Z",
      "updated_at": "2021-03-13T03:47:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Azure integrations query your Azure services according to a polling interval specific to the integration. The polling interval applies for every Azure entity related to the integrated service. For example, if you have thirteen CosmosDB instances, each of the thirteen instances will be polled every five minutes. View polling data After you activate an Azure integration, New Relic starts polling data from Azure and makes the data accessible through infrastructure Inventory and New Relic dashboards. You can query the Azure data along with additional data imported from any other New Relic features. You can also view dashboard data for a specific integration or across your account. For visualizations of polling intervals, API calls, and other data for your Azure integrations: Go to one.newrelic.com > Infrastructure > Azure. To view data for a specific integration: Select the Dashboards link for the integration's row. New Relic polling intervals For polling and resolution details, see the documentation for a specific integration.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 123.20318,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Polling intervals for <em>Azure</em> <em>integrations</em>",
        "sections": "Polling intervals for <em>Azure</em> <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>Azure</em> <em>integrations</em> query your <em>Azure</em> services according to a polling interval specific to the integration. The polling interval applies for every <em>Azure</em> entity related to the integrated service. For example, if you have thirteen CosmosDB instances, each of the thirteen instances"
      },
      "id": "6044e560196a671d6f960f72"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/getting-started/polling-intervals-azure-integrations": [
    {
      "sections": [
        "Introduction to Azure monitoring integrations",
        "Requirements",
        "Features"
      ],
      "title": "Introduction to Azure monitoring integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f99e6127548c87b6d54587ee8fba6f03ef3fdf2e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/introduction-azure-monitoring-integrations/",
      "published_at": "2021-10-08T06:38:20Z",
      "updated_at": "2021-09-14T07:27:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft Azure integrations allow you to monitor and report data about your Azure services to New Relic, providing a comprehensive view of your entire architecture in one place. The Azure integrations are not the same as APM's .NET support for Azure. Requirements Check the Azure integrations documentation for requirements on individual integrations. New Relic cannot obtain monitoring data from resources that are located in Azure Government or that were created through the classic deployment model. Features After you activate your Azure integration, New Relic begins to query your Azure platform services according to a regular polling interval. You can use our integrations UI to: View performance data from Integrations dashboards that automatically scale as you make changes to your ecosystem. Manage alert conditions with alerts. Query your data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 160.57562,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "sections": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our <em>Microsoft</em> <em>Azure</em> <em>integrations</em> allow you to monitor and report data about your <em>Azure</em> services to New Relic, providing a comprehensive view of your entire architecture in one place. The <em>Azure</em> <em>integrations</em> are not the same as APM&#x27;s .NET support for <em>Azure</em>. Requirements Check the <em>Azure</em> <em>integrations</em>"
      },
      "id": "6044e562e7b9d2e5c15799f8"
    },
    {
      "sections": [
        "Azure integration metrics",
        "BETA FEATURE",
        "Azure Metrics"
      ],
      "title": "Azure integration metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "762631e1209bb9abb60f1ea8b185a6def61735b9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/azure-integration-metrics/",
      "published_at": "2021-10-08T11:30:42Z",
      "updated_at": "2021-09-14T18:17:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is currently in beta. Azure Metrics The following table contains the metrics we collect for Azure. Integration Dimensional Metric Name (new) Sample Metric Name (previous) Azure API Management azure.apimanagement.service.Capacity capacityPercent Azure API Management azure.apimanagement.service.Duration durationMilliseconds Azure API Management azure.apimanagement.service.EventHubDroppedEvents eventHubDroppedEvents Azure API Management azure.apimanagement.service.EventHubRejectedEvents eventHubRejectedEvents Azure API Management azure.apimanagement.service.EventHubSuccessfulEvents eventHubSuccessfulEvents Azure API Management azure.apimanagement.service.EventHubThrottledEvents eventHubThrottledEvents Azure API Management azure.apimanagement.service.EventHubTimedoutEvents eventHubTimedoutEvents Azure API Management azure.apimanagement.service.EventHubTotalBytesSent eventHubTotalBytesSentBytes Azure API Management azure.apimanagement.service.EventHubTotalEvents eventHubTotalEvents Azure API Management azure.apimanagement.service.EventHubTotalFailedEvents eventHubTotalFailedEvents Azure API Management azure.apimanagement.service.FailedRequests failedRequests Azure API Management azure.apimanagement.service.OtherRequests otherRequests Azure API Management azure.apimanagement.service.SuccessfulRequests successfulRequests Azure API Management azure.apimanagement.service.TotalRequests totalRequests Azure API Management azure.apimanagement.service.UnauthorizedRequests unauthorizedRequests Azure App Gateway azure.network.applicationgateways.ApplicationGatewayTotalTime applicationGatewayTotalTimeMilliseconds Azure App Gateway azure.network.applicationgateways.AvgRequestCountPerHealthyHost avgRequestCountPerHealthyHost Azure App Gateway azure.network.applicationgateways.BackendConnectTime backendConnectTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendFirstByteResponseTime backendFirstByteResponseTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendLastByteResponseTime backendLastByteResponseTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendResponseStatus backendResponseStatus Azure App Gateway azure.network.applicationgateways.BlockedCount blockedCount Azure App Gateway azure.network.applicationgateways.BlockedReqCount blockedReqCount Azure App Gateway azure.network.applicationgateways.BytesReceived bytesReceivedBytes Azure App Gateway azure.network.applicationgateways.BytesSent bytesSentBytes Azure App Gateway azure.network.applicationgateways.CapacityUnits capacityUnits Azure App Gateway azure.network.applicationgateways.ClientRtt clientRttMilliseconds Azure App Gateway azure.network.applicationgateways.ComputeUnits computeUnits Azure App Gateway azure.network.applicationgateways.CpuUtilization cpuUtilizationPercent Azure App Gateway azure.network.applicationgateways.CurrentConnections currentConnections Azure App Gateway azure.network.applicationgateways.EstimatedBilledCapacityUnits estimatedBilledCapacityUnits Azure App Gateway azure.network.applicationgateways.FailedRequests failedRequests Azure App Gateway azure.network.applicationgateways.FixedBillableCapacityUnits fixedBillableCapacityUnits Azure App Gateway azure.network.applicationgateways.HealthyHostCount healthyHostCount Azure App Gateway azure.network.applicationgateways.MatchedCount matchedCount Azure App Gateway azure.network.applicationgateways.NewConnectionsPerSecond newConnectionsPerSecondCountPerSecond Azure App Gateway azure.network.applicationgateways.ResponseStatus responseStatus Azure App Gateway azure.network.applicationgateways.Throughput throughputBytesPerSecond Azure App Gateway azure.network.applicationgateways.TlsProtocol tlsProtocol Azure App Gateway azure.network.applicationgateways.TotalRequests totalRequests Azure App Gateway azure.network.applicationgateways.UnhealthyHostCount unhealthyHostCount Azure App Service azure.web.serverfarms.BytesReceived bytesReceivedBytes Azure App Service azure.web.serverfarms.BytesSent bytesSentBytes Azure App Service azure.web.serverfarms.CpuPercentage cpuPercent Azure App Service azure.web.serverfarms.DiskQueueLength diskQueueLength Azure App Service azure.web.serverfarms.HttpQueueLength httpQueueLength Azure App Service azure.web.serverfarms.MemoryPercentage memoryPercent Azure App Service azure.web.sites.AppConnections appConnections Azure App Service azure.web.sites.AverageMemoryWorkingSet.byWebApp averageMemoryWorkingSetBytes Azure App Service azure.web.sites.AverageResponseTime averageResponseTimeSeconds Azure App Service azure.web.sites.BytesReceived.byWebApp receivedBytes Azure App Service azure.web.sites.BytesSent.byWebApp sentBytes Azure App Service azure.web.sites.CpuTime cpuTimeSeconds Azure App Service azure.web.sites.CurrentAssemblies currentAssemblies Azure App Service azure.web.sites.Gen0Collections gen0Collections Azure App Service azure.web.sites.Gen1Collections gen1Collections Azure App Service azure.web.sites.Gen2Collections gen2Collections Azure App Service azure.web.sites.Handles handles Azure App Service azure.web.sites.Http101 http101 Azure App Service azure.web.sites.Http2xx http2xx Azure App Service azure.web.sites.Http3xx http3xx Azure App Service azure.web.sites.Http401 http401 Azure App Service azure.web.sites.Http403 http403 Azure App Service azure.web.sites.Http404 http404 Azure App Service azure.web.sites.Http406 http406 Azure App Service azure.web.sites.Http4xx http4xx Azure App Service azure.web.sites.Http5xx.byWebApp http5xx Azure App Service azure.web.sites.MemoryWorkingSet.byWebApp memoryWorkingSetBytes Azure App Service azure.web.sites.Requests requests Azure App Service azure.web.sites.Threads threads Azure App Service azure.web.sites.TotalAppDomains totalAppDomains Azure App Service azure.web.sites.TotalAppDomainsUnloaded totalAppDomainsUnloaded Azure Containers azure.containerinstance.containergroups.CpuUsage cpuUsage Azure Containers azure.containerinstance.containergroups.MemoryUsage memoryUsageBytes Azure Containers azure.containerinstance.containergroups.NetworkBytesReceivedPerSecond networkReceivedBytesPerSecond Azure Containers azure.containerinstance.containergroups.NetworkBytesTransmittedPerSecond networkTransmittedBytesPerSecond Azure Containers azure.containerregistry.registries.RunDuration runDurationMilliseconds Azure Containers azure.containerregistry.registries.SuccessfulPullCount successfulPullCount Azure Containers azure.containerregistry.registries.SuccessfulPushCount successfulPushCount Azure Containers azure.containerregistry.registries.TotalPullCount totalPullCount Azure Containers azure.containerregistry.registries.TotalPushCount totalPushCount Azure Containers azure.containerservice.managedclusters.kube_node_status_allocatable_cpu_cores kubeNodeStatusAllocatableCpuCores Azure Containers azure.containerservice.managedclusters.kube_node_status_allocatable_memory_bytes kubeNodeStatusAllocatableMemoryBytes Azure Containers azure.containerservice.managedclusters.kube_node_status_condition kubeNodeStatusCondition Azure Containers azure.containerservice.managedclusters.kube_pod_status_phase kubePodStatusPhase Azure Containers azure.containerservice.managedclusters.kube_pod_status_ready kubePodStatusReady Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byAccount availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byAccount cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byAccount cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byAccount cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byAccount dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byAccount documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byAccount documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byAccount indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byAccount metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byAccount mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byAccount mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byAccount provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byAccount replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byAccount serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byAccount totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byAccount totalRequestUnits Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byCollection availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byCollection cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byCollection cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byCollection cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byCollection dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byCollection documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byCollection documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byCollection indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byCollection metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byCollection mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byCollection mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byCollection provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byCollection replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byCollection serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byCollection totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byCollection totalRequestUnits Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byDatabase availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byDatabase cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byDatabase cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byDatabase cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byDatabase dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byDatabase documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byDatabase documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byDatabase indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byDatabase metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byDatabase mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byDatabase mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byDatabase provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byDatabase replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byDatabase serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byDatabase totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byDatabase totalRequestUnits Azure Cost Management azure.costmanagement.cost.byLocation cost Azure Cost Management azure.costmanagement.cost.byResourceGroup cost Azure Cost Management azure.costmanagement.cost.byService cost Azure Cost Management azure.costmanagement.cost.byTag cost Azure Data Factory azure.datafactory.datafactories.FailedRuns failedRuns Azure Data Factory azure.datafactory.datafactories.SuccessfulRuns successfulRuns Azure Data Factory azure.datafactory.factories.ActivityCancelledRuns activityCancelledRuns Azure Data Factory azure.datafactory.factories.ActivityFailedRuns activityFailedRuns Azure Data Factory azure.datafactory.factories.ActivitySucceededRuns activitySucceededRuns Azure Data Factory azure.datafactory.factories.FactorySizeInGbUnits factorySizeInGbUnits Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAvailableMemory integrationRuntimeAvailableMemoryBytes Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAvailableNodeNumber integrationRuntimeAvailableNodeNumber Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAverageTaskPickupDelay integrationRuntimeAverageTaskPickupDelaySeconds Azure Data Factory azure.datafactory.factories.IntegrationRuntimeCpuPercentage integrationRuntimeCpuPercentagePercent Azure Data Factory azure.datafactory.factories.IntegrationRuntimeQueueLength integrationRuntimeQueueLength Azure Data Factory azure.datafactory.factories.MaxAllowedFactorySizeInGbUnits maxAllowedFactorySizeInGbUnits Azure Data Factory azure.datafactory.factories.MaxAllowedResourceCount maxAllowedResourceCount Azure Data Factory azure.datafactory.factories.PipelineCancelledRuns pipelineCancelledRuns Azure Data Factory azure.datafactory.factories.PipelineFailedRuns pipelineFailedRuns Azure Data Factory azure.datafactory.factories.PipelineSucceededRuns pipelineSucceededRuns Azure Data Factory azure.datafactory.factories.ResourceCount resourceCount Azure Data Factory azure.datafactory.factories.TriggerCancelledRuns triggerCancelledRuns Azure Data Factory azure.datafactory.factories.TriggerFailedRuns triggerFailedRuns Azure Data Factory azure.datafactory.factories.TriggerSucceededRuns triggerSucceededRuns Azure Database for MariaDB azure.dbformariadb.servers.active_connections activeConnections Azure Database for MariaDB azure.dbformariadb.servers.backup_storage_used backupStorageUsedBytes Azure Database for MariaDB azure.dbformariadb.servers.connections_failed connectionsFailed Azure Database for MariaDB azure.dbformariadb.servers.cpu_percent cpuPercent Azure Database for MariaDB azure.dbformariadb.servers.io_consumption_percent ioConsumptionPercent Azure Database for MariaDB azure.dbformariadb.servers.memory_percent memoryPercent Azure Database for MariaDB azure.dbformariadb.servers.network_bytes_egress networkEgressBytes Azure Database for MariaDB azure.dbformariadb.servers.network_bytes_ingress networkIngressBytes Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for MariaDB azure.dbformariadb.servers.storage_limit storageLimitBytes Azure Database for MariaDB azure.dbformariadb.servers.storage_percent storagePercent Azure Database for MariaDB azure.dbformariadb.servers.storage_used storageUsedBytes Azure Database for MySQL azure.dbformysql.servers.active_connections activeConnections Azure Database for MySQL azure.dbformysql.servers.backup_storage_used backupStorageUsedBytes Azure Database for MySQL azure.dbformysql.servers.connections_failed connectionsFailed Azure Database for MySQL azure.dbformysql.servers.cpu_percent cpuPercent Azure Database for MySQL azure.dbformysql.servers.io_consumption_percent ioConsumptionPercent Azure Database for MySQL azure.dbformysql.servers.memory_percent memoryPercent Azure Database for MySQL azure.dbformysql.servers.network_bytes_egress networkEgressBytes Azure Database for MySQL azure.dbformysql.servers.network_bytes_ingress networkIngressBytes Azure Database for MySQL azure.dbformysql.servers.seconds_behind_master secondsBehindMaster Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for MySQL azure.dbformysql.servers.storage_limit storageLimitBytes Azure Database for MySQL azure.dbformysql.servers.storage_percent storagePercent Azure Database for MySQL azure.dbformysql.servers.storage_used storageUsedBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.active_connections activeConnections Azure Database for PostgreSQL azure.dbforpostgresql.servers.backup_storage_used backupStorageUsedBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.connections_failed connectionsFailed Azure Database for PostgreSQL azure.dbforpostgresql.servers.cpu_percent cpuPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.io_consumption_percent ioConsumptionPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.memory_percent memoryPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.network_bytes_egress networkEgressBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.network_bytes_ingress networkIngressBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.pg_replica_log_delay_in_bytes pgReplicaLogDelayBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.pg_replica_log_delay_in_seconds pgReplicaLogDelaySeconds Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_limit storageLimitBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_percent storagePercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_used storageUsedBytes Azure Event Hub azure.eventhub.namespaces.ActiveConnections activeConnections Azure Event Hub azure.eventhub.namespaces.CaptureBacklog captureBacklog Azure Event Hub azure.eventhub.namespaces.CapturedBytes capturedBytes Azure Event Hub azure.eventhub.namespaces.CapturedMessages capturedMessages Azure Event Hub azure.eventhub.namespaces.ConnectionsClosed connectionsClosed Azure Event Hub azure.eventhub.namespaces.ConnectionsOpened connectionsOpened Azure Event Hub azure.eventhub.namespaces.IncomingBytes incomingBytes Azure Event Hub azure.eventhub.namespaces.IncomingMessages incomingMessages Azure Event Hub azure.eventhub.namespaces.IncomingRequests incomingRequests Azure Event Hub azure.eventhub.namespaces.OutgoingBytes outgoingBytes Azure Event Hub azure.eventhub.namespaces.OutgoingMessages outgoingMessages Azure Event Hub azure.eventhub.namespaces.QuotaExceededErrors quotaExceededErrors Azure Event Hub azure.eventhub.namespaces.ServerErrors serverErrors Azure Event Hub azure.eventhub.namespaces.Size sizeBytes Azure Event Hub azure.eventhub.namespaces.SuccessfulRequests successfulRequests Azure Event Hub azure.eventhub.namespaces.ThrottledRequests throttledRequests Azure Event Hub azure.eventhub.namespaces.UserErrors userErrors Azure Event Hub azure.eventhub.clusters.ActiveConnections activeConnections Azure Event Hub azure.eventhub.clusters.AvailableMemory availableMemoryPercent Azure Event Hub azure.eventhub.clusters.CaptureBacklog captureBacklog Azure Event Hub azure.eventhub.clusters.CapturedBytes capturedBytes Azure Event Hub azure.eventhub.clusters.CapturedMessages capturedMessages Azure Event Hub azure.eventhub.clusters.ConnectionsClosed connectionsClosed Azure Event Hub azure.eventhub.clusters.ConnectionsOpened connectionsOpened Azure Event Hub azure.eventhub.clusters.CPU cpuPercent Azure Event Hub azure.eventhub.clusters.IncomingBytes incomingBytes Azure Event Hub azure.eventhub.clusters.IncomingMessages incomingMessages Azure Event Hub azure.eventhub.clusters.IncomingRequests incomingRequests Azure Event Hub azure.eventhub.clusters.OutgoingBytes outgoingBytes Azure Event Hub azure.eventhub.clusters.OutgoingMessages outgoingMessages Azure Event Hub azure.eventhub.clusters.QuotaExceededErrors quotaExceededErrors Azure Event Hub azure.eventhub.clusters.ServerErrors serverErrors Azure Event Hub azure.eventhub.clusters.Size sizeBytes Azure Event Hub azure.eventhub.clusters.SuccessfulRequests successfulRequests Azure Event Hub azure.eventhub.clusters.ThrottledRequests throttledRequests Azure Event Hub azure.eventhub.clusters.UserErrors userErrors Azure Express Route azure.network.expressrouteports.AdminState adminState Azure Express Route azure.network.expressrouteports.LineProtocol lineProtocol Azure Express Route azure.network.expressrouteports.PortBitsInPerSecond portBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressrouteports.PortBitsOutPerSecond portBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressrouteports.RxLightLevel rxLightLevel Azure Express Route azure.network.expressrouteports.TxLightLevel txLightLevel Azure Express Route azure.network.expressroutecircuits.ArpAvailability arpAvailabilityPercent Azure Express Route azure.network.expressroutecircuits.BgpAvailability bgpAvailabilityPercent Azure Express Route azure.network.expressroutecircuits.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.GlobalReachBitsInPerSecond globalReachBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.GlobalReachBitsOutPerSecond globalReachBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.QosDropBitsInPerSecond qosDropBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.QosDropBitsOutPerSecond qosDropBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.peerings.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.peerings.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.connections.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.connections.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutegateways.ErGatewayConnectionBitsInPerSecond erGatewayConnectionBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutegateways.ErGatewayConnectionBitsOutPerSecond erGatewayConnectionBitsOutPerSecondCountPerSecond Azure Firewalls azure.network.azurefirewalls.ApplicationRuleHit applicationRuleHit Azure Firewalls azure.network.azurefirewalls.DataProcessed dataProcessedBytes Azure Firewalls azure.network.azurefirewalls.FirewallHealth firewallHealthPercent Azure Firewalls azure.network.azurefirewalls.NetworkRuleHit networkRuleHit Azure Firewalls azure.network.azurefirewalls.SNATPortUtilization sNATPortUtilizationPercent Azure Firewalls azure.network.azurefirewalls.Throughput throughputBitsPerSecond Azure Front Door azure.network.frontdoors.BackendHealthPercentage backendHealthPercent Azure Front Door azure.network.frontdoors.BackendRequestCount backendRequestCount Azure Front Door azure.network.frontdoors.BackendRequestLatency backendRequestLatencyMilliseconds Azure Front Door azure.network.frontdoors.BillableResponseSize billableResponseSizeBytes Azure Front Door azure.network.frontdoors.RequestCount requestCount Azure Front Door azure.network.frontdoors.RequestSize requestSizeBytes Azure Front Door azure.network.frontdoors.ResponseSize responseSizeBytes Azure Front Door azure.network.frontdoors.TotalLatency totalLatencyMilliseconds Azure Front Door azure.network.frontdoors.WebApplicationFirewallRequestCount webApplicationFirewallRequestCount Azure Functions azure.web.sites.AverageMemoryWorkingSet.byFunctionsApp averageMemoryWorkingSetBytes Azure Functions azure.web.sites.BytesReceived.byFunctionsApp receivedBytes Azure Functions azure.web.sites.BytesSent.byFunctionsApp sentBytes Azure Functions azure.web.sites.FunctionExecutionCount functionExecutionCount Azure Functions azure.web.sites.FunctionExecutionUnits functionExecutionUnits Azure Functions azure.web.sites.Http5xx.byFunctionsApp http5xx Azure Functions azure.web.sites.MemoryWorkingSet.byFunctionsApp memoryWorkingSetBytes Azure Key Vault azure.keyvault.vaults.Availability availabilityPercent Azure Key Vault azure.keyvault.vaults.SaturationShoebox saturationShoeboxPercent Azure Key Vault azure.keyvault.vaults.ServiceApiHit serviceApiHit Azure Key Vault azure.keyvault.vaults.ServiceApiLatency serviceApiLatencyMilliseconds Azure Key Vault azure.keyvault.vaults.ServiceApiResult serviceApiResult Azure Load Balancer azure.network.loadbalancers.AllocatedSnatPorts allocatedSnatPorts Azure Load Balancer azure.network.loadbalancers.ByteCount byteCountBytes Azure Load Balancer azure.network.loadbalancers.DipAvailability dipAvailability Azure Load Balancer azure.network.loadbalancers.PacketCount packetCount Azure Load Balancer azure.network.loadbalancers.SnatConnectionCount snatConnectionCount Azure Load Balancer azure.network.loadbalancers.SYNCount synCount Azure Load Balancer azure.network.loadbalancers.UsedSnatPorts usedSnatPorts Azure Load Balancer azure.network.loadbalancers.VipAvailability vipAvailability Azure Logic Apps azure.logic.workflows.ActionLatency actionLatencySeconds Azure Logic Apps azure.logic.workflows.ActionsCompleted actionsCompleted Azure Logic Apps azure.logic.workflows.ActionsFailed actionsFailed Azure Logic Apps azure.logic.workflows.ActionsSkipped actionsSkipped Azure Logic Apps azure.logic.workflows.ActionsStarted actionsStarted Azure Logic Apps azure.logic.workflows.ActionsSucceeded actionsSucceeded Azure Logic Apps azure.logic.workflows.ActionSuccessLatency actionSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.ActionThrottledEvents actionThrottledEvents Azure Logic Apps azure.logic.workflows.BillableActionExecutions billableActionExecutions Azure Logic Apps azure.logic.workflows.BillableTriggerExecutions billableTriggerExecutions Azure Logic Apps azure.logic.workflows.BillingUsageNativeOperation billingUsageNativeOperation Azure Logic Apps azure.logic.workflows.BillingUsageStandardConnector billingUsageStandardConnector Azure Logic Apps azure.logic.workflows.BillingUsageStorageConsumption billingUsageStorageConsumption Azure Logic Apps azure.logic.workflows.RunFailurePercentage runFailurePercent Azure Logic Apps azure.logic.workflows.RunLatency runLatencySeconds Azure Logic Apps azure.logic.workflows.RunsCancelled runsCancelled Azure Logic Apps azure.logic.workflows.RunsCompleted runsCompleted Azure Logic Apps azure.logic.workflows.RunsFailed runsFailed Azure Logic Apps azure.logic.workflows.RunsStarted runsStarted Azure Logic Apps azure.logic.workflows.RunsSucceeded runsSucceeded Azure Logic Apps azure.logic.workflows.RunStartThrottledEvents runStartThrottledEvents Azure Logic Apps azure.logic.workflows.RunSuccessLatency runSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.RunThrottledEvents runThrottledEvents Azure Logic Apps azure.logic.workflows.TotalBillableExecutions totalBillableExecutions Azure Logic Apps azure.logic.workflows.TriggerFireLatency triggerFireLatencySeconds Azure Logic Apps azure.logic.workflows.TriggerLatency triggerLatencySeconds Azure Logic Apps azure.logic.workflows.TriggersCompleted triggersCompleted Azure Logic Apps azure.logic.workflows.TriggersFailed triggersFailed Azure Logic Apps azure.logic.workflows.TriggersFired triggersFired Azure Logic Apps azure.logic.workflows.TriggersSkipped triggersSkipped Azure Logic Apps azure.logic.workflows.TriggersStarted triggersStarted Azure Logic Apps azure.logic.workflows.TriggersSucceeded triggersSucceeded Azure Logic Apps azure.logic.workflows.TriggerSuccessLatency triggerSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.TriggerThrottledEvents triggerThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.ActionLatency actionLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsCompleted actionsCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsFailed actionsFailed Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsSkipped actionsSkipped Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsStarted actionsStarted Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsSucceeded actionsSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.ActionSuccessLatency actionSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.ActionThrottledEvents actionThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentConnectorMemoryUsage integrationServiceEnvironmentConnectorMemoryUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentConnectorProcessorUsage integrationServiceEnvironmentConnectorProcessorUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentWorkflowMemoryUsage integrationServiceEnvironmentWorkflowMemoryUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentWorkflowProcessorUsage integrationServiceEnvironmentWorkflowProcessorUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.RunFailurePercentage runFailurePercent Azure Logic Apps azure.logic.integrationserviceenvironments.RunLatency runLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.RunsCancelled runsCancelled Azure Logic Apps azure.logic.integrationserviceenvironments.RunsCompleted runsCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.RunsFailed runsFailed Azure Logic Apps azure.logic.integrationserviceenvironments.RunsStarted runsStarted Azure Logic Apps azure.logic.integrationserviceenvironments.RunsSucceeded runsSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.RunStartThrottledEvents runStartThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.RunSuccessLatency runSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.RunThrottledEvents runThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerFireLatency triggerFireLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerLatency triggerLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersCompleted triggersCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersFailed triggersFailed Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersFired triggersFired Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersSkipped triggersSkipped Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersStarted triggersStarted Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersSucceeded triggersSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerSuccessLatency triggerSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerThrottledEvents triggerThrottledEvents Azure Machine Learning azure.machinelearningservices.workspaces.ActiveCores activeCores Azure Machine Learning azure.machinelearningservices.workspaces.ActiveNodes activeNodes Azure Machine Learning azure.machinelearningservices.workspaces.CompletedRuns completedRuns Azure Machine Learning azure.machinelearningservices.workspaces.CpuUtilization cpuUtilization Azure Machine Learning azure.machinelearningservices.workspaces.FailedRuns failedRuns Azure Machine Learning azure.machinelearningservices.workspaces.GpuUtilization gpuUtilization Azure Machine Learning azure.machinelearningservices.workspaces.IdleCores idleCores Azure Machine Learning azure.machinelearningservices.workspaces.IdleNodes idleNodes Azure Machine Learning azure.machinelearningservices.workspaces.LeavingCores leavingCores Azure Machine Learning azure.machinelearningservices.workspaces.LeavingNodes leavingNodes Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeployFailed modelDeployFailed Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeployStarted modelDeployStarted Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeploySucceeded modelDeploySucceeded Azure Machine Learning azure.machinelearningservices.workspaces.ModelRegisterFailed modelRegisterFailed Azure Machine Learning azure.machinelearningservices.workspaces.ModelRegisterSucceeded modelRegisterSucceeded Azure Machine Learning azure.machinelearningservices.workspaces.PreemptedCores preemptedCores Azure Machine Learning azure.machinelearningservices.workspaces.PreemptedNodes preemptedNodes Azure Machine Learning azure.machinelearningservices.workspaces.QuotaUtilizationPercentage quotaUtilizationPercentage Azure Machine Learning azure.machinelearningservices.workspaces.StartedRuns startedRuns Azure Machine Learning azure.machinelearningservices.workspaces.TotalCores totalCores Azure Machine Learning azure.machinelearningservices.workspaces.TotalNodes totalNodes Azure Machine Learning azure.machinelearningservices.workspaces.UnusableCores unusableCores Azure Machine Learning azure.machinelearningservices.workspaces.UnusableNodes unusableNodes Azure Power BI Dedicated azure.powerbidedicated.capacities.memory_metric memoryMetricBytes Azure Power BI Dedicated azure.powerbidedicated.capacities.memory_thrashing_metric memoryThrashingMetricPercent Azure Power BI Dedicated azure.powerbidedicated.capacities.qpu_high_utilization_metric qpuHighUtilizationMetric Azure Power BI Dedicated azure.powerbidedicated.capacities.QueryDuration queryDurationMilliseconds Azure Power BI Dedicated azure.powerbidedicated.capacities.QueryPoolJobQueueLength queryPoolJobQueueLength Azure Redis azure.cache.redis.cachehits cacheHits Azure Redis azure.cache.redis.cachemisses cacheMisses Azure Redis azure.cache.redis.cacheRead cacheReadBytesPerSecond Azure Redis azure.cache.redis.cacheWrite cacheWriteBytesPerSecond Azure Redis azure.cache.redis.connectedclients connectedClients Azure Redis azure.cache.redis.evictedkeys evictedKeys Azure Redis azure.cache.redis.expiredkeys expiredKeys Azure Redis azure.cache.redis.getcommands getCommands Azure Redis azure.cache.redis.operationsPerSecond operationsPerSecond Azure Redis azure.cache.redis.percentProcessorTime processorTimePercent Azure Redis azure.cache.redis.serverLoad serverLoadPercent Azure Redis azure.cache.redis.setcommands setCommands Azure Redis azure.cache.redis.totalcommandsprocessed totalCommandsProcessed Azure Redis azure.cache.redis.totalkeys totalKeys Azure Redis azure.cache.redis.usedmemory usedMemoryBytes Azure Redis azure.cache.redis.usedmemoryRss usedMemoryRssBytes Azure Redis azure.cache.redis.cachehits0 cacheHits Azure Redis azure.cache.redis.cachemisses0 cacheMisses Azure Redis azure.cache.redis.cacheRead0 cacheReadBytesPerSecond Azure Redis azure.cache.redis.cacheWrite0 cacheWriteBytesPerSecond Azure Redis azure.cache.redis.connectedclients0 connectedClients Azure Redis azure.cache.redis.evictedkeys0 evictedKeys Azure Redis azure.cache.redis.expiredkeys0 expiredKeys Azure Redis azure.cache.redis.getcommands0 getCommands Azure Redis azure.cache.redis.operationsPerSecond0 operationsPerSecond Azure Redis azure.cache.redis.percentProcessorTime0 processorTimePercent Azure Redis azure.cache.redis.serverLoad0 serverLoadPercent Azure Redis azure.cache.redis.setcommands0 setCommands Azure Redis azure.cache.redis.totalcommandsprocessed0 totalCommandsProcessed Azure Redis azure.cache.redis.totalkeys0 totalKeys Azure Redis azure.cache.redis.usedmemory0 usedMemoryBytes Azure Redis azure.cache.redis.usedmemoryRss0 usedMemoryRssBytes Azure Service Bus azure.servicebus.namespaces.ActiveConnections activeConnections Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byNamespace activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byNamespace connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byNamespace connectionsOpened Azure Service Bus azure.servicebus.namespaces.CPUXNS cpuUsagePercent Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byNamespace deadletteredMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byNamespace incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byNamespace incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byNamespace messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byNamespace outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byNamespace scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byNamespace serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byNamespace sizeBytes Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byNamespace successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byNamespace throttledRequests Azure Service Bus azure.servicebus.namespaces.UserErrors.byNamespace userErrors Azure Service Bus azure.servicebus.namespaces.WSXNS memoryUsagePercent Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byQueue activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byQueue connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byQueue connectionsOpened Azure Service Bus azure.servicebus.namespaces.currentSizeInBytes.byQueue currentSizeBytes Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byQueue deadletteredMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.byQueue deadLetterMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byQueue incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byQueue incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byQueue messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byQueue outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byQueue scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byQueue serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byQueue sizeBytes Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byQueue successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byQueue throttledRequests Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.byQueue transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.byQueue transferMessages Azure Service Bus azure.servicebus.namespaces.UserErrors.byQueue userErrors Azure Service Bus azure.servicebus.namespaces.activeMessageCount activeMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.bySubscription deadLetterMessages Azure Service Bus azure.servicebus.namespaces.messageCount messages Azure Service Bus azure.servicebus.namespaces.scheduledMessageCount scheduledMessages Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.bySubscription transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.bySubscription transferMessages Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byTopic activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byTopic connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byTopic connectionsOpened Azure Service Bus azure.servicebus.namespaces.currentSizeInBytes.byTopic currentSizeInBytes Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byTopic deadletteredMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.byTopic deadLetterMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byTopic incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byTopic incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byTopic messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byTopic outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byTopic scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byTopic serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byTopic sizeBytes Azure Service Bus azure.servicebus.namespaces.subscriptionCount subscriptions Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byTopic successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byTopic throttledRequests Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.byTopic transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.byTopic transferMessages Azure Service Bus azure.servicebus.namespaces.UserErrors.byTopic userErrors Azure Service Fabric azure.servicefabricmesh.applications.ActualCpu actualCpu Azure Service Fabric azure.servicefabricmesh.applications.ActualMemory actualMemoryBytes Azure Service Fabric azure.servicefabricmesh.applications.AllocatedCpu allocatedCpu Azure Service Fabric azure.servicefabricmesh.applications.AllocatedMemory allocatedMemoryBytes Azure Service Fabric azure.servicefabricmesh.applications.ApplicationStatus applicationStatus Azure Service Fabric azure.servicefabricmesh.applications.ContainerStatus containerStatus Azure Service Fabric azure.servicefabricmesh.applications.CpuUtilization cpuUtilizationPercent Azure Service Fabric azure.servicefabricmesh.applications.MemoryUtilization memoryUtilizationPercent Azure Service Fabric azure.servicefabricmesh.applications.RestartCount restartCount Azure Service Fabric azure.servicefabricmesh.applications.ServiceReplicaStatus serviceReplicaStatus Azure Service Fabric azure.servicefabricmesh.applications.ServiceStatus serviceStatus Azure SQL azure.sql.servers.database.currentSize databaseSizeCurrentBytes Azure SQL azure.sql.servers.database.limitSize databaseSizeLimitBytes Azure SQL azure.sql.servers.databases.blocked_by_firewall blockedByFirewall Azure SQL azure.sql.servers.databases.connection_failed connectionFailed Azure SQL azure.sql.servers.databases.connection_successful connectionSuccessful Azure SQL azure.sql.servers.databases.cpu_percent cpuPercent Azure SQL azure.sql.servers.databases.deadlock deadlock Azure SQL azure.sql.servers.databases.dtu_consumption_percent dtuConsumptionPercent Azure SQL azure.sql.servers.databases.dtu_limit dtuLimit Azure SQL azure.sql.servers.databases.dtu_used dtuUsed Azure SQL azure.sql.servers.databases.dw_cpu_percent dwCpuPercent Azure SQL azure.sql.servers.databases.dw_physical_data_read_percent dwPhysicalDataReadPercent Azure SQL azure.sql.servers.databases.dwu_consumption_percent dwuConsumptionPercent Azure SQL azure.sql.servers.databases.dwu_limit dwuLimit Azure SQL azure.sql.servers.databases.dwu_used dwuUsed Azure SQL azure.sql.servers.databases.log_write_percent logWritePercent Azure SQL azure.sql.servers.databases.physical_data_read_percent physicalDataReadPercent Azure SQL azure.sql.servers.databases.sessions_percent sessionsPercent Azure SQL azure.sql.servers.databases.storage storageBytes Azure SQL azure.sql.servers.databases.storage_percent storagePercent Azure SQL azure.sql.servers.databases.workers_percent workersPercent Azure SQL azure.sql.servers.databases.xtp_storage_percent xtpStoragePercent Azure SQL azure.sql.elasticPool.database_physical_data_read_percent databasePhysicalDataRead Azure SQL azure.sql.elasticPool.database_storage_used databaseStorageUsed Azure SQL azure.sql.servers.elasticpools.cpu_percent cpuPercent Azure SQL azure.sql.servers.elasticpools.database_cpu_percent databaseCpuPercent Azure SQL azure.sql.servers.elasticpools.database_dtu_consumption_percent databaseDtuConsumptionPercent Azure SQL azure.sql.servers.elasticpools.database_log_write_percent databaseLogWritePercent Azure SQL azure.sql.servers.elasticpools.database_sessions_percent databaseSessionsPercent Azure SQL azure.sql.servers.elasticpools.database_workers_percent databaseWorkersPercent Azure SQL azure.sql.servers.elasticpools.dtu_consumption_percent dtuConsumptionPercent Azure SQL azure.sql.servers.elasticpools.eDTU_limit eDTULimit Azure SQL azure.sql.servers.elasticpools.eDTU_used eDTUUsed Azure SQL azure.sql.servers.elasticpools.log_write_percent logWritePercent Azure SQL azure.sql.servers.elasticpools.physical_data_read_percent physicalDataReadPercent Azure SQL azure.sql.servers.elasticpools.sessions_percent sessionsPercent Azure SQL azure.sql.servers.elasticpools.storage_limit storageLimitBytes Azure SQL azure.sql.servers.elasticpools.storage_percent storagePercent Azure SQL azure.sql.servers.elasticpools.storage_used storageUsedBytes Azure SQL azure.sql.servers.elasticpools.workers_percent workersPercent Azure SQL azure.sql.servers.elasticpools.xtp_storage_percent xtpStoragePercent Azure SQL azure.sql.server.dtuLimit dtuLimit Azure SQL azure.sql.servers.dtuCurrent dtuCurrent Azure SQL Managed Instance azure.sql.managedinstances.avg_cpu_percent avgCpuPercent Azure SQL Managed Instance azure.sql.managedinstances.io_bytes_read ioReadBytes Azure SQL Managed Instance azure.sql.managedinstances.io_bytes_written ioWrittenBytes Azure SQL Managed Instance azure.sql.managedinstances.io_requests ioRequests Azure SQL Managed Instance azure.sql.managedinstances.reserved_storage_mb reservedStorage Azure SQL Managed Instance azure.sql.managedinstances.storage_space_used_mb storageSpaceUsed Azure SQL Managed Instance azure.sql.managedinstances.virtual_core_count virtualCore Azure Storage Account azure.storage.storageaccounts.Availability availabilityPercent Azure Storage Account azure.storage.storageaccounts.blobservices.Availability blobs.availabilityPercent Azure Storage Account azure.storage.storageaccounts.blobservices.BlobCapacity blobs.blobCapacityBytes Azure Storage Account azure.storage.storageaccounts.blobservices.BlobCount blobs.blobCount Azure Storage Account azure.storage.storageaccounts.blobservices.ContainerCount blobs.containerCount Azure Storage Account azure.storage.storageaccounts.blobservices.Egress blobs.egressBytes Azure Storage Account azure.storage.storageaccounts.blobservices.Ingress blobs.ingressBytes Azure Storage Account azure.storage.storageaccounts.blobservices.SuccessE2ELatency blobs.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.blobservices.SuccessServerLatency blobs.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.blobservices.Transactions blobs.transactions Azure Storage Account azure.storage.storageaccounts.Egress egressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.Availability files.availabilityPercent Azure Storage Account azure.storage.storageaccounts.fileservices.Egress files.egressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.FileCapacity files.fileCapacityBytes Azure Storage Account azure.storage.storageaccounts.fileservices.FileCount files.fileCount Azure Storage Account azure.storage.storageaccounts.fileservices.FileShareCount files.fileShareCount Azure Storage Account azure.storage.storageaccounts.fileservices.Ingress files.ingressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.SuccessE2ELatency files.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.fileservices.SuccessServerLatency files.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.fileservices.Transactions files.transactions Azure Storage Account azure.storage.storageaccounts.Ingress ingressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.Availability queues.availabilityPercent Azure Storage Account azure.storage.storageaccounts.queueservices.Egress queues.egressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.Ingress queues.ingressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.QueueCapacity queues.queueCapacityBytes Azure Storage Account azure.storage.storageaccounts.queueservices.QueueCount queues.queueCount Azure Storage Account azure.storage.storageaccounts.queueservices.QueueMessageCount queues.queueMessagesCount Azure Storage Account azure.storage.storageaccounts.queueservices.SuccessE2ELatency queues.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.queueservices.SuccessServerLatency queues.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.queueservices.Transactions queues.transactions Azure Storage Account azure.storage.storageaccounts.SuccessE2ELatency successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.SuccessServerLatency successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.Availability tables.availabilityPercent Azure Storage Account azure.storage.storageaccounts.tableservices.Egress tables.egressBytes Azure Storage Account azure.storage.storageaccounts.tableservices.Ingress tables.ingressBytes Azure Storage Account azure.storage.storageaccounts.tableservices.SuccessE2ELatency tables.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.SuccessServerLatency tables.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.TableCapacity tables.tableCapacityBytes Azure Storage Account azure.storage.storageaccounts.tableservices.TableCount tables.tableCount Azure Storage Account azure.storage.storageaccounts.tableservices.TableEntityCount tables.tableEntityCount Azure Storage Account azure.storage.storageaccounts.tableservices.Transactions tables.transactions Azure Storage Account azure.storage.storageaccounts.Transactions transactions Azure Storage Account azure.storage.storageaccounts.UsedCapacity usedCapacityBytes Azure Virtual Network azure.network.virtualnetworks.PingMeshAverageRoundtripMs pingMeshAverageRoundtripMs Azure Virtual Network azure.network.virtualnetworks.PingMeshProbesFailedPercent pingMeshProbesFailedPercent Azure Virtual Network azure.network.publicipaddresses.BytesDroppedDDoS droppedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.BytesForwardedDDoS forwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.BytesInDDoS inDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.DDoSTriggerTCPPackets ddosTriggerTcpPacketsPerSecond Azure Virtual Network azure.network.publicipaddresses.DDoSTriggerUDPPackets ddosTriggerUdpPacketsPerSecond Azure Virtual Network azure.network.publicipaddresses.IfUnderDDoSAttack ifUnderDdosAttack Azure Virtual Network azure.network.publicipaddresses.PacketsDroppedDDoS packetsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.PacketsForwardedDDoS packetsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.PacketsInDDoS packetsInDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesDroppedDDoS tcpDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesForwardedDDoS tcpForwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesInDDoS tcpInDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsDroppedDDoS tcpPacketsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsForwardedDDoS tcpPacketsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsInDDoS tcpPacketsInDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesDroppedDDoS udpDroppedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesForwardedDDoS udpForwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesInDDoS udpInDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsDroppedDDoS udpPacketsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsForwardedDDoS udpPacketsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsInDDoS udpPacketsInDdosPerSecond Azure Virtual Network azure.network.virtualnetworks.availableAddresses availableAddresses Azure VMs Scale Sets azure.compute.virtualmachinescalesets.CPUCreditsConsumed cpuCreditsConsumed Azure VMs Scale Sets azure.compute.virtualmachinescalesets.CPUCreditsRemaining cpuCreditsRemaining Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskQueueDepth dataDiskQueueDepth Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskReadBytessec dataDiskReadBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskReadOperationsSec dataDiskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskWriteBytessec dataDiskWriteBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskWriteOperationsSec dataDiskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskReadBytes diskReadBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskReadOperationsSec diskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskWriteBytes diskWriteBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskWriteOperationsSec diskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.InboundFlows inboundFlows Azure VMs Scale Sets azure.compute.virtualmachinescalesets.InboundFlowsMaximumCreationRate inboundFlowsMaximumCreationRateCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.NetworkInTotal networkInTotalBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.NetworkOutTotal networkOutTotalBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskQueueDepth osDiskQueueDepth Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskReadBytessec osDiskReadBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskReadOperationsSec osDiskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskWriteBytessec osDiskWriteBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskWriteOperationsSec osDiskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OutboundFlows outboundFlows Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OutboundFlowsMaximumCreationRate outboundFlowsMaximumCreationRateCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PercentageCPU cpuPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumDataDiskCacheReadHit premiumDataDiskCacheReadHitPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumDataDiskCacheReadMiss premiumDataDiskCacheReadMissPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumOSDiskCacheReadHit premiumOsDiskCacheReadHitPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumOSDiskCacheReadMiss premiumOsDiskCacheReadMissPercent Azure VMs azure.compute.virtualmachines.DiskReadBytes diskReadBytes Azure VMs azure.compute.virtualmachines.DiskReadOperations.Sec diskReadOpsPerSecond Azure VMs azure.compute.virtualmachines.DiskWriteBytes diskWriteBytes Azure VMs azure.compute.virtualmachines.DiskWriteOperations.Sec diskWriteOpsPerSecond Azure VMs azure.compute.virtualmachines.NetworkIn networkInBytes Azure VMs azure.compute.virtualmachines.NetworkOut networkOutBytes Azure VMs azure.compute.virtualmachines.PercentageCPU cpuUsagePercent Azure VPN Gateways azure.network.virtualnetworkgateways.AverageBandwidth averageBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.P2SBandwidth p2SBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.P2SConnectionCount p2SConnectionCount Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelAverageBandwidth tunnelAverageBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressBytes tunnelEgressBytes Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressPacketDropTSMismatch tunnelEgressPacketDropTSMismatch Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressPackets tunnelEgressPackets Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressBytes tunnelIngressBytes Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressPacketDropTSMismatch tunnelIngressPacketDropTSMismatch Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressPackets tunnelIngressPackets",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 153.06429,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> <em>integration</em> metrics",
        "sections": "<em>Azure</em> <em>integration</em> metrics",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": " <em>azure</em>.logic.workflows.RunsFailed runsFailed <em>Azure</em> Logic Apps <em>azure.logic.workflows.RunsStarted</em> runs<em>Started</em> <em>Azure</em> Logic Apps <em>azure</em>.logic.workflows.RunsSucceeded runsSucceeded <em>Azure</em> Logic Apps <em>azure.logic.workflows.RunStart</em>ThrottledEvents run<em>Start</em>ThrottledEvents <em>Azure</em> Logic Apps"
      },
      "id": "603e8a8928ccbcacc0eba74e"
    },
    {
      "sections": [
        "Activate Azure integrations",
        "Requirements",
        "Step 1: Get Azure subscription and tenant IDs",
        "Step 2: Register your app and get ID",
        "Step 3: Create a client secret in Azure",
        "Step 4: Provide permissions to services",
        "Step 5: Add app to New Relic",
        "Update application details and rotate client secrets",
        "Explore app data in New Relic Infrastructure's UI"
      ],
      "title": "Activate Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f65679179e13aa1b503b4b95010e296cbe269c29",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/activate-azure-integrations/",
      "published_at": "2021-10-07T10:05:25Z",
      "updated_at": "2021-08-02T13:06:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations allow you to report data from specific systems and supplement infrastructure's default, automatic monitoring. The Microsoft Azure integrations report data from various Azure platform services to your New Relic account. This document explains how to activate Azure integrations. Requirements The Azure integration activation process requires you to: A New Relic account. Don't have one? Sign up for free! No credit card required. Create a New Relic application and key in Azure. Grant this application access to the Azure services you want to monitor. Place required information in the New Relic's Integrations UI. To use these integration activation instructions directly from the Infrastructure UI, go to one.newrelic.com > Infrastructure > Azure > Add an Azure account. Step 1: Get Azure subscription and tenant IDs To get your Azure account's subscription id and tenantId, use your local terminal if you have Azure's tools installed, or use Azure's Cloud Shell terminal in the Azure portal. Open a terminal with access to your Azure account. Type the following: az account show Copy Copy and save the subscription id and tenantID from the output response for later use. The response should look similar to the response below. The subscription id and tenantID are highlighted. @Azure:~$ az account show { \"environmentName\": \"AzureCloud\", \"id\": \"9ffe9512-f4a2-42dd-1230-518aec34be21\" , \"isDefault\": true, \"name\": \"Beyond Team Sandbox\", \"state\": \"Enabled\", \"tenantId\": \"ac6692da-1231-422f-22a8-9eed6dbe83f1\" , \"user\": { \"name\": \"youremail@domain\", \"type\": \"user\" } Copy Step 2: Register your app and get ID You must have Azure permissions to register your application and copy its Application ID. To register your app in Azure: Sign in to the Azure portal and go to the Azure Active Directory. From Manage, select App registrations > New registration. Enter a name for the application. We recommend that you name your app NewRelic-Integrations. In Redirect URI select Weband add https://www.newrelic.com as the sign-on URI. Create the application by clicking Register. From the Overview of your app, copy the Application (client) ID, and save it for later use. Step 3: Create a client secret in Azure To create a client secret associated with your application: In Azure, under the application you've just created, select Certificates & secrets. Under Client secrets, click on New client secret and then on Add. Copy the value of Client Secret and save it for later use. Step 4: Provide permissions to services Your app must provide Reader permissions for each Azure service you want New Relic to monitor: In the Azure Subscriptions section, select the subscriptions that you want New Relic to monitor. Select Access control (IAM) > Add > Add role assignment. From the Role dropdown, select Reader. From the Select dropdown, select the app's name; for example, NewRelic-Integrations. From Selected members, verify your app name appears, then select Save. Some Azure services, including Azure CosmosDB and Azure VMs, require additional steps. See the Azure integration documentation for the services you want to enable. Step 5: Add app to New Relic Now you can activate the Azure integration in the Infrastructure UI. The UI will require the information you have saved in the previous steps, including: Your Azure account's subscription id and tenantId The application's application ID The application's client secret To add your Azure app to New Relic: Go to one.newrelic.com > Infrastructure > Azure and select the Azure Service you wish to add. Follow the steps in the UI to activate the integration in New Relic. If you have already completed the Azure account steps, skip to the end of the steps to fill out the form. (For Azure account name, enter the name you want to use to identify the account in your Integrations dashboard.) Update application details and rotate client secrets It's possible to update the application's name and authentication credentials using the Infrastructure UI or the Cloud Integrations API at any time. Follow these steps to rotate the Azure client secret in the Infratructure UI: Go to one.newrelic.com > Infrastructure > Azure and click on Manage Services on the Azure account you wish to edit. Select the edit action next to Account Name to see and edit any application value. Edit the Client Secret field with the new value and confirm with Save Changes. Explore app data in New Relic Infrastructure's UI After you activate an Azure integration, New Relic will start monitoring your Azure data at regular polling intervals. To find and use your data, use the data explorer or go to one.newrelic.com > Infrastructure > Azure",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 136.31354,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Activate <em>Azure</em> <em>integrations</em>",
        "sections": "Activate <em>Azure</em> <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> allow you to report data from specific systems and supplement infrastructure&#x27;s default, automatic monitoring. The <em>Microsoft</em> <em>Azure</em> <em>integrations</em> report data from various <em>Azure</em> platform services to your New Relic account. This document explains how to activate"
      },
      "id": "6044e5a9196a671bfa960f79"
    }
  ],
  "/docs/integrations/mlops-integrations/algorithmia-mlops-integration": [
    {
      "sections": [
        "Monitor services running on Kubernetes",
        "Get started",
        "What you need",
        "Enable monitoring of services",
        "Get the config YAML for the integration",
        "Example configuration",
        "Configuration options for each integration",
        "Monitor services in our Kubernetes integration installed with Helm",
        "Learn more",
        "Manually configure service monitoring",
        "How the service-specific YAML config works",
        "Add a service YAML to the Kubernetes integration config",
        "Add multiple services to the same config"
      ],
      "title": "Monitor services running on Kubernetes",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Link apps and services"
      ],
      "external_id": "4c67f6272bda36eda4ad7883e89697a203aa2153",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/link-apps-services/monitor-services-running-kubernetes/",
      "published_at": "2021-10-07T10:06:32Z",
      "updated_at": "2021-10-07T10:06:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic's Kubernetes integration you can monitor both Kubernetes and the services running on it, such as Cassandra, Redis, MySQL, and other supported services. Get started Our Kubernetes integration comes bundled with some of our on-host integrations (like Cassandra, MySQL, and Apache). This lets you get data for those supported services by adding a section to the Kubernetes integration's configuration, which lives as a ConfigMap inside a manifest. What you need Enable this feature for a service Details about how configuration works For an example of how to monitor Redis running on a Kubernetes PHP Guestbook, see this tutorial. What you need To monitor services running on Kubernetes, you only need a Kubernetes cluster running the Kubernetes integration, version 1.16.0 or higher (install | check version | update). We support the following services running on Kubernetes: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP Enable monitoring of services To enable our Kubernetes integration to monitor one or more services: Expand this dropdown and get the YAML snippets for the service(s) you want to monitor: Get the config YAML for the integration For the services you want to monitor, follow the links to GitHub to get the YAML snippets you'll need for the next step: Apache Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ Redis SNMP Add the snippet to the Kubernetes integration's ConfigMap, after the data: section: Example configuration This example shows the YAML config for the Apache integration ( highlighted ) added to the Kubernetes integration's config. Respect the indentation levels. --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: apache-config.yaml: | --- # Run auto discovery to find pods with label \"app=apache\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Use the optional arguments: # --namespaces: Comma separated namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: apache integrations: - name: nri-apache env: # Use the discovered IP as the host address STATUS_URL: http://${discovery.ip}/server-status?auto METRICS: 1 Copy You can add snippets for multiple services to the same config file. See an example. Depending on your environment, you may need or want to set additional config options. Expand the dropdown below for links to configuration options. Configuration options for each integration Select a service to see available config options: Apache Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ Redis SNMP Verify monitoring is enabled: Go to one.newrelic.com > Infrastructure, select Third party services, and then select the service's dashboard. You should see data being reported. Additional notes about enabling services: Enabling multiple services may use more resources than what is set in the resource limits of the Kubernetes integration config file. If this becomes an issue, raise the limit in the resources section. The Kubernetes integration does not automatically update. For best results, regularly update. Monitor services in our Kubernetes integration installed with Helm If you installed our Kubernetes integration using Helm, to monitor services you need to update the existing installation with the new configuration, which contains the services to monitor: helm upgrade --reuse-values -f values.yaml [RELEASE] [CHART] Copy If you use nri-bundle charts, you need to update the children's chart values. Find some examples here. Learn more More resources for learning about configuration: Learn technical details about how configuration works. Learn how to configure monitoring of multiple services with the same config file. See a step-by-step tutorial showing how to monitor a Redis service on Kubernetes. Manually configure service monitoring The enable procedure should be all you need to get monitoring working, but if you run into problems, understanding some technical details about configuration can be helpful. This section goes into more detail about how configuration works. For each service you wish to monitor, you must add a configuration file for that integration to our Kubernetes integration's configuration. This document will cover these subjects: How the service-specific configuration YAML snippet works Adding the service-specific YAML in the Kubernetes integration's config file Adding multiple services to the Kubernetes integration's config file How the service-specific YAML config works Our Kubernetes integration's configuration follows the ConfigMap format. Using a ConfigMap allows us to decouple the configuration for the integrations from the Kubernetes image. The other benefit is that a ConfigMap can be updated automatically without reloading the running container. Because the infrastructure agent uses YAML to configure its associated integrations, ConfigMaps are a good choice for storing YAML. (For more information on config file format, see the Integration config file format.) The Kubernetes integration image comes with an auto-discovery feature that simplifies the configuration of multiple instances of services using a single configuration file. For example, if you have several NGINX instances running, creating an NGINX integration configuration file for every instance would be hard to implement and hard to update. With our auto-discovery option, you can discover and monitor all your NGINX instances with a single configuration file. Each integration has its own specific configuration YAML. Our NGINX integration default config file looks like this: nginx-config.yml: | --- discovery: command: # Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --port: Port used to connect to the kubelet. Default is 10255 # --tls: Use secure (TLS) connection # Custom Example: # exec: /var/db/newrelic-infra/nri-discovery-kubernetes --namespaces namespace1,namespace2 --port 10250 --tls # Default exec: /var/db/newrelic-infra/nri-discovery-kubernetes match: label.app: nginx integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}/status STATUS_MODULE: discover METRICS: 1 Copy The above config enables the following: Runs nri-discovery-kubernetes to query the data for the node we are currently on. Parses the data that comes back and looks for any Kubernetes pod that has a Kubernetes container with an app= label with value nginx. For any matches, it attempts to run the NGINX integration. The status URL is built from: The pod's IP address The status page is pulled from the label on K8s pod called status_url This automatic discovery works the same as the container auto-discovery used by the infrastructure agent. For more advanced options, see Container auto-discovery. Add a service YAML to the Kubernetes integration config It's best practice to configure enabled integrations alongside the Kubernetes integration configuration. This is easier than maintaining configuration files for every single service/integration instance. Below is an example of a Kubernetes integration's ConfigMap. The highlighted section shows where an integration configuration YAML (in this case, NGINX) is placed. For more information on discovery:, see Container auto-discovery for on-host integrations. --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: nginx-config.yml: | --- # Run auto discovery to find pods with label \"app=nginx\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: nginx integrations: - name: nri-nginx env: # If you're using ngx_http_api_module be certain to use the full path up to and including the version number # Use the discovered IP as the host address STATUS_URL: http://${discovery.ip}/status # Comma separated list of ngx_http_api_module, NON PARAMETERIZED, Endpoints # endpoints: /nginx,/processes,/connections,/ssl,/slabs,/http,/http/requests,/http/server_zones,/http/caches,/http/upstreams,/http/keyvals,/stream,/stream/server_zones,/stream/upstreams,/stream/keyvals,/stream/zone_sync # Name of Nginx status module OHI is to query against. discover | ngx_http_stub_status_module | ngx_http_status_module | ngx_http_api_module STATUS_MODULE: discover METRICS: 1 Copy This configuration map can then be referenced in the DaemonSet, the same as the one that was generated via the command line. Make sure the namespace used is the same one used by the Kubernetes integration manifest. If you haven't changed it in the downloaded manifest file, the value is default. Add multiple services to the same config You can monitor several services using the same Kubernetes integration config file. To do this, add another integration configuration YAML to the same Kubernetes integration config file. Below is the Kubernetes config created in the last section, with a new section for the Cassandra integration's config (highlighted). --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: nginx-config.yml: | --- # Run auto discovery to find pods with label \"app=nginx\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Run discovery for Kubernetes. Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: nginx integrations: - name: nri-nginx env: # If you're using ngx_http_api_module be certain to use the full path up to and including the version number # Use the discovered IP as the host address STATUS_URL: http://${discovery.ip}/status # Comma separated list of ngx_http_api_module, NON PARAMETERIZED, Endpoints # endpoints: /nginx,/processes,/connections,/ssl,/slabs,/http,/http/requests,/http/server_zones,/http/caches,/http/upstreams,/http/keyvals,/stream,/stream/server_zones,/stream/upstreams,/stream/keyvals,/stream/zone_sync # Name of Nginx status module OHI is to query against. discover | ngx_http_stub_status_module | ngx_http_status_module | ngx_http_api_module STATUS_MODULE: discover METRICS: 1 cassandra-configuration.yml: | --- # Run auto discovery to find pods with label \"app=cassandra\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Run discovery for Kubernetes. Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: cassandra integrations: - name: nri-cassandra env: # Use the discovered IP as the host address HOSTNAME: ${discovery.ip} PORT: 7199 USERNAME: cassandra PASSWORD: cassandra METRICS: 1/mark Copy The Kubernetes integration config is now set up to monitor these two services. Additionally, depending on your environment, there may be some additional service-specific configuration you must do. When you've completed configuration, our infrastructure agent looks for any pod with a label cassandra and runs the integration against it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 44.199627,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Get the config YAML for the <em>integration</em>",
        "tags": "<em>Integrations</em>",
        "body": "With New Relic&#x27;s Kubernetes integration you can monitor both Kubernetes and the services running on it, such as Cassandra, Redis, MySQL, and other supported services. Get started Our Kubernetes integration comes bundled with some of our on-host <em>integrations</em> (like Cassandra, MySQL, and Apache"
      },
      "id": "6044e50c196a676012960f35"
    },
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Guided setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Tip",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from AWS API polling integrations",
        "Migration steps",
        "Query, dashboard, alert and inventory considerations",
        "Integrations not fully replaced by metric streams",
        "Infrastructure Agent metrics and EC2 metadata decoration"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-10-07T05:06:24Z",
      "updated_at": "2021-10-07T05:06:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All CloudWatch metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay, and therefore not included in the stream. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Guided setup using CloudFormation First, you need to link each of your AWS accounts with your New Relic account. To do so: Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. You may automate this step with NerdGraph. Next, set up the metric stream using the CloudFormation template we provide in the last step of our UI. This template is provided as a base to setup the integration on a single region, and can be customized and extended based on your requirements. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: New Relic Ensure the following settings are defined: New Relic configuration (Destination Settings) HTTP endpoint URL - US Datacenter: https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL - EU Datacenter: https://aws-api.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Change default output format to Open Telemetry 0.7 (JSON is not supported) Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Tip The following are the minimal permissions that should be granted on the AWS role configured in New Relic so that CloudWatch metrics can be enriched with additional service metadata and custom tags when applicable: config:BatchGetResourceConfig config:ListDiscoveredResources tag:GetResources Copy The New Relic UI currently recommends the ReadOnlyAccess policy over these individual items so that New Relic has proper permissions to collect service data that's not available in AWS CloudWatch Metric Streams. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = ‘cloudwatch-metric-streams’. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from most services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Note that not all metrics have their custom tags as dimensions. Currently, only metrics linked to entities in the New Relic Explorer have their custom tags associated. The AWS CloudWatch metric stream doesn't include tags as part of the stream message, hence, additional processing is required on the New Relic side. Metadata collection Like with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This metadata is accessible in New Relic as additional dimensions on the metrics provided by AWS CloudWatch. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 API Gateway (excluding API v1) ELB EBS DynamoDB ECS Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in the CloudWatch Metric Stream in order to select which services / namespaces are being collected. Consider using drop data rules to discard metrics based on custom filters (for example, drop metrics by namespace and tag, tag value, or any other valid NRQL criteria). Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from AWS API polling integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Migration steps On a typical deployment, migrating from API polling to metric stream involves the following steps (we recommend trying this on a dev / staging environment first): Go through the AWS UI in New Relic (or use NerdGraph APIs) to link your AWS account with New Relic. This is currently needed even if your AWS account is already linked with polling integrations. Make sure you complete the last step in the onboarding, which involves enabling AWS CloudWatch metric stream and the AWS Kinesis Data Firehose to push metrics to New Relic. Complete this step for any additional AWS region you want to monitor, since AWS CloudWatch requires one stream per region. Ensure metrics are received from all connected regions and namespaces. This may take several minutes. Disable all unnecessary polling integrations in the previous AWS provider account. The following integrations still need to be enabled since they aren't fully replaced by metric streams: AWS Billing, AWS CloudTrail, AWS Health, AWS Trusted Advisor. Query, dashboard, alert and inventory considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Dashboards: Custom dashboards that use poll-based AWS integration events will still work as expected. Alerts: Alert conditions that use poll-based AWS events will still work. We recommend adapting those to the dimensional metric format (using NRQL as source). Entities: New Relic Explorer might show duplicated entities for up to 24 hours. Inventory: the Inventory page is not supported with AWS CloudWatch metric streams (inventory telemetry is not included in the stream). Integrations not fully replaced by metric streams The AWS CloudWatch Metric Streams integration only collects CloudWatch metrics, resource metadata and custom tags. The following API polling integrations still need to be enabled to get complete visibility from AWS: AWS Billing AWS CloudTrail AWS Health AWS Trusted Advisor AWS VPC Infrastructure Agent metrics and EC2 metadata decoration As with the EC2 API polling integration, when the infrastructure agent is installed on a host and the EC2 namespace is active via AWS CloudWatch metric stream integration, then all the infrastructure agent events and metrics are decorated with additional metadata. The following attributes will decorate infrastructure samples (some might not be applicable on all environments): awsAvailabilityZone, ec2InstanceId, ec2PublicDnsName, ec2State, ec2EbsOptimized, ec2PublicIpAddress, ec2PrivateIpAddress, ec2VpcId, ec2AmiId, ec2PrivateDnsName, ec2KeyName, ec2SubnetId, ec2InstanceType, ec2Hypervisor, ec2Architecture, ec2RootDeviceType, ec2RootDeviceName, ec2VirtualizationType, ec2PlacementGroupName, ec2PlacementGroupTenancy.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 42.39988,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Amazon CloudWatch Metric Streams <em>integration</em>",
        "sections": "Migrating from AWS API polling <em>integrations</em>",
        "tags": "<em>Integrations</em>",
        "body": "New Relic currently provides independent <em>integrations</em> with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New"
      },
      "id": "606a036de7b9d2bfef9445f2"
    },
    {
      "sections": [
        "New Relic quickstarts overview",
        "Preview feature",
        "Why it matters",
        "What are quickstarts",
        "Some technical detail"
      ],
      "title": "New Relic quickstarts overview",
      "type": "docs",
      "tags": [
        "Full-Stack Observability",
        "Observe everything",
        "Get started",
        "Quickstarts",
        "Instant Observability"
      ],
      "external_id": "e12df6102b9361f953bdab2f4b49baa5756f7048",
      "image": "https://docs.newrelic.com/static/ec52d123f2853cc7fc4d3fab9f9f3be7/c1b63/quickstart-home.png",
      "url": "https://docs.newrelic.com/docs/full-stack-observability/observe-everything/get-started/new-relic-quickstarts-overview/",
      "published_at": "2021-10-06T23:40:45Z",
      "updated_at": "2021-10-06T22:17:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Monitor your tech stack without the burden of manual set up. New Relic I/O is a rich catalog of open source quickstarts - out-of-the-box bundles of integrations, dashboards, and alerts. Each quickstart is created by observability experts around the world, vetted by New Relic, and ready for you to install with one click. Leverage community expertise and get more value out of your telemetry data with New Relic I/O, your hub for instant observability. Ready to get started? Find your quickstart in New Relic I/O: New Relic I/O Preview feature Everyone has access to New Relic I/O for now. After the preview, only full users will be able to access the dashboards installed from a quickstart. Why it matters With our I/O catalog, you can choose from hundreds of quickstarts that bundle the necessary building blocks to get started with monitoring your technology stack; that includes instrumentation, integrations, dashboards, and alerts, all ready to install with a click. What are quickstarts Quickstarts are bundles of dashboards, alerts, and instrumentation that are ready to install with a single click, specific to each technology we support. Popular technologies such as Node.js, Python, and Ruby have full-featured quickstarts, while others contain a mixture of instrumentations, dashboards, and alerts. New Relic I/O is open source, which means that you can modify and improve existing quickstarts, or build new ones, to suit your needs. We thoroughly review external edit to our quickstarts for value and quality. Interested in contributing to the community? Check out our contributor guide. Some technical detail New Relic quickstarts use open source installation recipes to instrument integrations using our guided install process.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 41.37706,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": "Monitor your tech stack without the burden of manual set up. New Relic I&#x2F;O is a rich catalog of open source quickstarts - out-of-the-box bundles of <em>integrations</em>, dashboards, and alerts. Each quickstart is created by observability experts around the world, vetted by New Relic, and ready for you"
      },
      "id": "6157008964441f500d099617"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/dropwizard/dropwizard-reporter": [
    {
      "sections": [
        "Introduction to New Relic's open source telemetry integrations",
        "Types of integrations",
        "How they work"
      ],
      "title": "Introduction to New Relic's open source telemetry integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "Get started"
      ],
      "external_id": "239889ec292525fcfd6b417d243943ea7b3e0529",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/get-started/introduction-new-relics-open-source-telemetry-integrations/",
      "published_at": "2021-10-08T06:39:21Z",
      "updated_at": "2021-07-27T16:01:31Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic provides open source integrations that report telemetry data from telemetry tools to your New Relic account. Types of integrations We have open source integrations that report data from OpenCensus, OpenTelemetry, DropWizard, Prometheus, and more. With these solutions, you can aggregate all your telemetry data in one place: the New Relic platform. See our list of open source telemetry integrations (to browse all New Relic solutions, see our integrations page). How they work These integrations were built using our Telemetry SDKs, which are open-source language-specific libraries for reporting metrics, trace data, and other telemetry data to New Relic. If our pre-built integrations don't meet your needs, you can use the Telemetry SDKs to build your own telemetry tools. Under the hood, data reported by these solutions are ingested via our data ingest APIs. For example, metrics reported by the DropWizard exporter are ingested via the Metric API, so to understand how to query and chart that type of data, you could read Query metric data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 216.62387,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to New Relic&#x27;s <em>open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "sections": "Introduction to New Relic&#x27;s <em>open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "New Relic provides <em>open</em> <em>source</em> <em>integrations</em> that report <em>telemetry</em> data from <em>telemetry</em> tools to your New Relic account. Types of <em>integrations</em> We have <em>open</em> <em>source</em> <em>integrations</em> that report data from <em>Open</em>Census, <em>OpenTelemetry</em>, <em>DropWizard</em>, Prometheus, and more. With these solutions, you can aggregate"
      },
      "id": "603e95ab28ccbc036aeba789"
    },
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Find your service (entity)",
        "Narrow down your data with filters",
        "Summary page",
        "Distributed tracing",
        "Find traces and their associated spans",
        "Tip",
        "View spans with errors",
        "View span events",
        "Databases",
        "Errors",
        "Externals",
        "JVMs",
        "Logs",
        "Metrics explorer",
        "Transactions",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/490779aac8c10802682d86f0b785b6a2/a4078/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-10-07T01:52:56Z",
      "updated_at": "2021-09-27T15:53:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information about your services. It offers you a way to filter services and then filter the data to show a variety of views into your data. Find your service (entity) To get started in Explorer, you need to find your service: Go to one.newrelic.com. In the left sidebar, click Services - OpenTelemetry: In the center pane, click the service you want to know more about, or find your service by entering the name in the top filter bar. Narrow down your data with filters Once you have your entity, you can then filter for data from your service. With the filter bar Narrow data to..., you can highlight a specific facet of the telemetry recorded for your service. For example, you may want to see the error rate for a particular version of the service that you've deployed in a canary instance, so you add a filter for service.version='1.2.3'. Filters are preserved when navigating between different views of your data for a service. For example, the filter for service.version='1.2.3' carries over to the Transactions view, so that you would see telemetry on requests to the endpoints (transactions) that are running version 1.2.3 of your service, and not any other versions. Filters are preserved when navigating between the Summary, Transactions, Databases, Externals, Errors, and JVMs views. Filters are also preserved when navigating to the Distributed tracing view, but with limitations. Only filter conditions that use the equals operator (\"=\") are currently supported when navigating to Distributed tracing. If you navigate back from the Distributed tracing page, the filters you selected on the previous view will come back. Once you filter your data, the UI has various views in the left-navigation pane. See our descriptions below for view details. Summary page After you click on a service in the Explorer tab, you see the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. For your data to appear in this section, make sure it has the following: UI area Attribute Response time span.kind = server or consumer Throughput span.kind = server or consumer Error rate span.kind = server or consumer otel.status_code = ERROR Service instances pane service.instance.id (usually set via the OTel resource API) Distributed tracing In Distributed tracing, you can locate traces and examine span details. For your trace data to appear in the New Relic UI, it needs to conform to the OpenTelemetry trace semantic conventions. Also, in the Traces section of our best practices guide, you can find some tips about making sure your traces and spans appear in New Relic. The following attributes are typically not added explicitly to spans. Rather, they are usually set when creating a span or performing operations (for example, recording an error) on a span using the OpenTelemetry SDK. Attribute Description name A span’s name is generally set when starting a span. The name can generally be anything, but the OpenTelemetry specification provides guidance for certain types of spans like Database spans or HTTP spans. span.kind A span’s kind is generally set when starting a span. New Relic uses span.kind to infer that a span is an entry point to a given service. When span.kind = server or consumer it is considered an entry point. When span.kind = client or producer, it is considered a call to an external service or database system. otel.status_code A span’s status is set using the span API. The otel.status_code attribute is how it is manifested by New Relic. The UI primarily uses otel.status_code for the purpose of identifying errors. Find traces and their associated spans Here are some ways to target your searches: To find the traces you want, you can run queries like these in the filter bar: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID When you find an interesting trace, click on it to display a waterfall diagram showing the spans of the trace. Click on specific spans within the trace to display span details in the right panel. Tip For more ways to filter traces, see our distributed tracing UI page. View spans with errors After you click on a span in the trace waterfall view, you can see span errors in the right panel under Error details. The error details are populated by spans containing otel.status_code = ERROR and display the content of otel.status_description. To narrow your search for spans with errors, you can enter otel.status_code = ERROR directly in the distributed tracing filter bar. View span events If you send span events as described by the OpenTelemetry specification, you can view them in the New Relic UI. Span events have two general types: Exceptions Non-exceptions (for example, logs) If you have span events, links for these appear in the right pane: Click on a span in the waterfall view. In the right pane, click View span events, or you can expand Error details and click the link at the bottom of the errors. When you're in span events and only want to view exceptions, slide the toggle Only show exceptions. Tip OpenTelemetry exceptions handled by the app/service are displayed independently of span error status and are not necessarily associated with a span error status. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. For your data to appear in this section, make sure it has the following: UI area Attribute Top database calls span.kind = client or producer db.system Facets by span name Top databases (by query time) span.kind = client or producer db.system Facets by db.system Top databases (by throughput) span.kind = client or producer db.system Facets by db.system Errors On the Errors page, you can see total errors as well as charts showing error count and error rate. For your data to appear in this section, make sure it has the following: span.kind = server or consumer otel.status_code = ERROR Facets by span name Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. For your data to appear in this section, make sure it has the following: span.kind = client or producer db.system is not present JVMs When you drill into a specific JVM, the UI display charts driven by JVM metric data: JVM metrics follow the general semantic conventions for runtime environment metrics. The Java specific runtime metrics are not well documented. The implementation is effectively the documentation and may be subject to change. For your data to appear in this section, make sure it has the following: Requires a unique service.instance.id attribute for rendering the list of JVMs Service.instance.id is an OpenTelemetry resource attribute Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. For your data to appear in this section, make sure it has the following: service.name To correlate with trace data, the logs should contain the trace.id and span.id attributes. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. For your data to appear in this section, make sure it has the following: UI area Description Top Transactions span.kind = server or consumer Facets by span name Throughput span.kind = server or consumer Facets by span name Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.93936,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "Introduction to OpenTelemetry with New Relic",
        "Benefits of OpenTelemetry",
        "Should I use OpenTelemetry instrumentation or New Relic agents?",
        "OpenTelemetry: A work in progress",
        "New Relic's APM agents",
        "How OpenTelemetry works with New Relic",
        "Important",
        "Traces",
        "Metrics",
        "Logs",
        "Next steps"
      ],
      "title": "Introduction to OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "c87898d2d5835c00930c173eabd1bf93040badea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/introduction-opentelemetry-new-relic/",
      "published_at": "2021-10-07T00:14:26Z",
      "updated_at": "2021-09-27T15:16:23Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Are you already familiar with OpenTelemetry and want to begin the setup? Check out our quick start: Quick start If you don't have one already, create a New Relic account. It's free, forever. If you're just getting acquainted with OpenTelemetry, this is what we'll explore here: Benefits of OpenTelemetry Should I use OpenTelemetry or New Relic agents? How OpenTelemetry works with New Relic Benefits of OpenTelemetry OpenTelemetry provides a secure, vendor-neutral specification for service instrumentation so that you can export data to distinct backends of your choice, such as New Relic. OpenTelemetry offers a single set of APIs and libraries that standardize how you collect and transfer telemetry data for your services. The following components make up the OpenTelemetry project: Specifications for the core pillars of observability to drive consistency across all projects. New Relic supports all of these signals: traces, metrics, and logs (see details below) APIs that contain interfaces and implementations based on the specifications SDKs (reference implementations of the APIs) created specifically for languages like Java, Python, Go, Erlang, and more Collectors that offer a vendor-agnostic implementation for processing and exporting Exporters that enable you to send data to a backend of your choice The components of OpenTelemetry work together to create some distinct advantages for capturing telemetry data: Feature Description Ubiquitous instrumentation A single, open standard of instrumentation provides better coverage and flexibility as engineers from all over the world contribute to the instrumentation. Future proof As the instrumentation gets built into libraries and frameworks, and as more vendors move to support this open standard, you can be confident that you won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build integrations into OpenTelemetry or add instrumentation directly to source code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need to decide which instrumentation option to use (a proprietary option or one of the other open standards). Cross-platform compatibility OpenTelemetry supports a variety of languages and backends. It represents a vendor-neutral path for capturing and transmitting telemetry to backends without altering existing instrumentation. Streamlined observability It is easier for vendors to support and test against a single standard as they don’t need to develop their own agents or collectors. High dimensionality OpenTelemetry uses dimensional metrics, so you can filter and facet on more aspects of the data, such as AWS regions, Kubernetes clusters, or service versions. Dimensional metrics also lead to less time between occurrence and reporting. Efficiency OpenTelemetry’s fire-and-forget trace-centric approach to instrumentation often has lower overhead than New Relic agents, especially for asynchronous workloads. It will also result in better handling of trace data for asynchronous requests. Should I use OpenTelemetry instrumentation or New Relic agents? As you consider OpenTelemetry, you may also be looking at New Relic APM agents that also capture telemetry data. As you'd expect, there is a lot of overlap between features available from OpenTelemetry agents and SDKs versus those available from New Relic APM agents. This is especially true if you're interested in distributed tracing telemetry data. The choice you make depends on what you need. We recommend that you explore both New Relic and OpenTelemetry instrumentation or discuss this with us in our CNCF Slack channel to decide what works best for you. OpenTelemetry: A work in progress OpenTelemetry is still an emerging standard, so your choices may be affected by what's available. You can check on the current state of the specification at the OpenTelemetry site. The current state of language-specific OpenTelemetry APIs and SDKs varies: some languages are still pre-alpha and may be missing instructions on how to instrument your service. Most languages have some implementation of traces that is sufficient to start exporting data to New Relic. Check out this table in GitHub that provides an overview of the state of OpenTelemetry specification compliance for each language. For languages that New Relic does not currently provide an agent or SDK, OpenTelemetry may offer you a good alternative. Also, in cases where you want explicit control over sampling of your telemetry data, OpenTelemetry provides a lot of flexibility. As OpenTelemetry matures, New Relic will continue to support new OpenTelemetry data models and to provide a curated UI experience for our Full Stack Observability customers. New Relic's APM agents In general, New Relic's APM agents will collect more telemetry data for your services, and they offer a wide range of configuration options and an extensive set of auto-instrumentation capabilities. New Relic's APM agents offer detailed transaction trace visibility for individual services. They also offer predefined sampling to balance the performance impact of your instrumentation against the need to capture enough data to gain helpful insights. How OpenTelemetry works with New Relic New Relic supports the native OpenTelemetry Protocol (OTLP) for exporting telemetry data. This allows you to use the vendor neutral components developed by the OpenTelemetry community to export your data to New Relic. The following tables show the supported features for each telemetry signal. If you have questions about these or have an unsupported use case, please contact us in our CNCF Slack channel, and watch this page for future updates. Important New Relic's exporters for OpenTelemetry are now deprecated in favor of exporting data to New Relic using OTLP. Traces New Relic offers support for the OTLP ingest of trace signals. The maturity of the upstream specification is stable. OpenTelemetry traces and spans are compatible with New Relic traces and spans. OpenTelemetry spans optionally include attributes (name-value pairs) and resource attributes which map directly to dimensions that can be used to facet or filter span data at query time. OpenTelemetry span metadata (for example, name, kind, and trace_id) also map directly to dimensions on NewRelic spans. At this time, New Relic does not support span links or array attributes. For details, see the Traces section of our best practices guide. Feature Supported Span events ✅ Span linking ❌ Array of primitives (homogeneous) ❌ Metrics New Relic offers support for the OTLP ingest of metric signals. Note that the maturity of the upstream specification is experimental. We intend to follow potentially breaking upstream changes. Here are the OpenTelemetry data types we support and their associated mappings. For details, see the Metrics section of our best practices guide. Metric Type Supported Delta sums ✅ Cumulative sums ✅ Gauges ✅ Delta histograms ✅ Summary ✅ Cumulative histograms ❌ Exemplars ❌ Array of primitives (homogeneous) ❌ Logs New Relic offers support for the OTLP ingest of log signals. Note that the maturity of the upstream specification is experimental. We intend to follow potentially breaking upstream changes. OpenTelemetry logs are compatible with New Relic logs. OpenTelemetry logs optionally include attributes (name-value pairs) and resource attributes which map directly to dimensions that can be used to facet or filter log data at query time. OpenTelemetry log metadata (for example, name, severity_text, and trace_id) also map directly to dimensions on New Relic logs. NewRelic currently supports all OpenTelemetry log message types except for arrays. For more details, see the Logs section of our best practices guide. Feature Supported Description LogRecord body ✅ Supported types: string, boolean, int, double, bytes LogRecord attributes ✅ Supported types: string, boolean, int, double, bytes LogRecord fields ✅ Examples: name, severity_text, trace_id Array messages ❌ Array attributes ❌ Next steps Follow the OpenTelemetry quick start to help you get started. You'll also want to review the best practices guide for getting the most out of the data you export to New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.85803,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "sections": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build <em>integrations</em> into <em>OpenTelemetry</em> or add instrumentation directly to <em>source</em> code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need"
      },
      "id": "603e81ba196a67304da83dab"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/elixir/elixir-open-source-agent": [
    {
      "sections": [
        "Roku open-source agent",
        "Tip",
        "Get started",
        "For more help"
      ],
      "title": "Roku open-source agent",
      "type": "docs",
      "tags": [
        "Agents",
        "Open-source licensed agents",
        "Open-source licensed agents"
      ],
      "external_id": "f0982a0ff96c8a85683bf3ef27a2e4cde85ff274",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/roku/roku-open-source-video-agent/",
      "published_at": "2021-10-07T01:57:33Z",
      "updated_at": "2021-04-27T11:09:50Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Monitor Roku behavior with New Relic using the Roku open-source agent. The agent contains two parts, to capture two separate categories of Roku behavior: App events like app starts and HTTP requests Video playback within the app Tip This agent is released as open source on GitHub. A change log is also available there for the latest updates. Get started For requirements, installation, and configuration information, see the Open Source Roku Agent README on GitHub. Visit New Relic’s Roku repository on GitHub for questions about installation, usage, or other topics. Report issues or bugs as an issue in the GitHub repository. For more help Recommendations for learning more: Browse New Relic's Explorers Hub for community discussions about the open-source Roku agent. Review New Relic's licenses, attributions, data usage limits, and other notices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 348.995,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Roku <em>open</em>-<em>source</em> <em>agent</em>",
        "sections": "Roku <em>open</em>-<em>source</em> <em>agent</em>",
        "tags": "<em>Open</em>-<em>source</em> <em>licensed</em> <em>agents</em>",
        "body": "Monitor Roku behavior with New Relic using the Roku <em>open</em>-<em>source</em> <em>agent</em>. The <em>agent</em> contains two parts, to capture two separate categories of Roku behavior: App events like app starts and HTTP requests Video playback within the app Tip This <em>agent</em> is released as <em>open</em> <em>source</em> on GitHub. A change log"
      },
      "id": "6087f0ff64441f618a9d8533"
    },
    {
      "sections": [
        "IBM WebSphere Application Server"
      ],
      "title": "IBM WebSphere Application Server",
      "type": "docs",
      "tags": [
        "Agents",
        "Java agent",
        "Additional installation"
      ],
      "external_id": "e3c2ba33d026b8912f7dd2c1d8dfc00d63f25b9c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/agents/java-agent/additional-installation/ibm-websphere-application-server/",
      "published_at": "2021-10-07T11:16:32Z",
      "updated_at": "2021-10-07T11:16:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This describes how to configure New Relic's Java agent if you are using IBM WebSphere Application Server. For compatible versions, follow New Relic's procedures to install the Java agent on WebSphere. IBM WebSphere App Server Comments Compatible IBM JVM versions New Relic supports all versions of WebSphere that are compatible with the Java agent. However, certain versions of the IBM JVM are incompatible with the Java agent due to known issues in the IBM JVM. New Relic's Java agent is compatible with these major versions of the IBM JVM: 8: All versions Java 2 Security If you are using Java 2 Security and WebSphere, you must grant the Java agent additional permissions before it can execute properly. Browser monitoring To use browser monitoring when running the Java agent on WebSphere, you must manually enable browser monitoring. WebSphere PMI metrics You can configure the Java agent to capture additional WebSphere PMI metrics. These metrics will appear on the New Relic JVM metrics page. Instance and display names You can configure the Java agent to change the default behavior of instance names or display names.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 33.22845,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Agents</em>",
        "body": "This describes how to configure New Relic&#x27;s Java <em>agent</em> if you are using IBM WebSphere Application Server. For compatible versions, follow New Relic&#x27;s procedures to install the Java <em>agent</em> on WebSphere. IBM WebSphere App Server Comments Compatible IBM JVM versions New Relic supports all versions"
      },
      "id": "6043b986e7b9d279085799eb"
    },
    {
      "sections": [
        ".NET agent: compatibility and requirements for .NET Core",
        "Requirements",
        "Microsoft .NET Core version",
        "Target framework version",
        "Important",
        "App/web servers",
        "Operating system",
        "Microsoft Azure",
        "Processor architectures",
        "Permissions",
        "Security requirements",
        "Network requirements",
        "Automatic instrumentation",
        "App frameworks",
        "Datastores",
        "External call libraries",
        "Messaging",
        "Unavailable features",
        "Connect the agent to other New Relic products"
      ],
      "title": ".NET agent: compatibility and requirements for .NET Core",
      "type": "docs",
      "tags": [
        "Agents",
        "NET agent",
        "Getting started"
      ],
      "external_id": "ce9a62a0c6a2d98442118333f39152c38bf9fdda",
      "image": "",
      "url": "https://docs.newrelic.com/docs/agents/net-agent/getting-started/net-agent-compatibility-requirements-net-core/",
      "published_at": "2021-10-07T08:42:12Z",
      "updated_at": "2021-10-07T08:42:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's .NET agent supports both .NET Framework and .NET Core. This document describes compatibility and support for .NET Core applications. See Compatibility and requirements for .NET Framework for .NET Framework applications. New Relic's .NET agent includes built-in instrumentation for some of the most popular parts of the .NET Core ecosystem, including frameworks, databases, and message queuing systems. After installation, the agent runs within the monitored process; there is not a separate process or service created by the agent. For frameworks and libraries that are not automatically instrumented out of the box, you can extend the agent with .NET custom instrumentation. Want to try out our .NET agent? Create a New Relic account for free! No credit card required. Requirements Before you install the New Relic .NET agent on Windows or Linux, make sure your system meets these requirements: Microsoft .NET Core version The .NET agent supports .NET Core versions 2.0, 2.1, 2.2, 3.0, 3.1, and .NET 5.0. Table of minimum agent versions required per .NET Core version .NET Core Version Minimum Required .NET Agent Version .NET Core 2.0 > = 8.19.353.0 .NET Core 2.1 > = 8.19.353.0 .NET Core 2.2 > = 8.19.353.0 .NET Core 3.0 > = 8.21.34.0 .NET Core 3.1 > = 8.21.34.0 .NET 5.0 > = 8.35.0 The agent is not compatible with .NET Core versions 1.0 or 1.1. For .NET Core 2.1 or higher applications with tiered compilation enabled, the agent will disable tiered compilation. .NET Core 2.1 support requires .NET Core runtime 2.1.3 and .NET Core SDK 2.1.401 or higher due to a bug in the .NET Core profiling API. Target framework version The .NET agent only supports applications targeting .NET Core 2.0, 2.1, 2.2, 3.0, 3.1, and NET 5.0. You can find the target framework in your .csproj file: Supported: <TargetFramework>netcoreapp2.0</TargetFramework> Copy <TargetFramework>netcoreapp2.1</TargetFramework> Copy <TargetFramework>netcoreapp2.2</TargetFramework> Copy <TargetFramework>netcoreapp3.0</TargetFramework> Copy <TargetFramework>netcoreapp3.1</TargetFramework> Copy <TargetFramework>net5.0</TargetFramework> Copy Unsupported: <TargetFramework>net452</TargetFramework> Copy Important If you want to monitor an ASP.NET Core application targeting .NET Framework, ensure your install of the .NET agent has .NET Framework support enabled. App/web servers Ensure you use one of these app/web servers: Kestrel Kestrel with IIS reverse proxy via AspNetCoreModule Kestrel with IIS reverse proxy via AspNetCoreModuleV2 Kestrel with Nginx reverse proxy Kestrel with Apache reverse proxy Operating system The agent has been verified to work with the following operating systems: Operating system Supported versions Windows (32- and 64-bit Intel compatible architectures) Server 2008 R2 SP1 Server 2012 Server 2012 R2 Server 2016 Server 2019 Windows containers running on Server 2016 (NanoServer based images are not supported) Linux (64-bit Intel compatible only) All x64 Linux distributions supported by the .NET Core 2.0+/.NET 5 runtime are supported by the .NET agent. For a full list, refer to Microsoft's documentation for the version of the runtime you are using. Microsoft Azure For Azure-specific installation instructions, see: Install on Azure Cloud Services Install on Azure Service Fabric Install on Azure Web Apps Processor architectures The agent is available in both 32-bit (x86) and 64-bit (x64) versions on Windows as well as 64-bit (x64) on Linux. Permissions Installing and running the .NET agent requires these permissions: Component Necessary permissions Install the agent The process or user that installs the agent must have sufficient permissions to set environment variables and write access to the directory where the agent is installed. Run the agent The monitored process must have read/write access to the directory in which you installed the agent. The agent runs as a part of the monitored process and relies on those permissions to function. For applications using IIS via reverse proxy, the group IIS_IUSRS is often used. Security requirements As a standard security measure for data collection, your app server must support SHA-2 (256-bit). SHA-1 is not supported. Network requirements The agent requires your firewall to allow outgoing connections to specific networks and ports. Automatic instrumentation If your application is hosted in ASP.NET Core, the agent automatically creates and instruments transactions. The .NET agent will automatically instrument your application after install. If your app is not automatically instrumented, or if you want to add instrumentation, use custom instrumentation. App frameworks The .NET agent automatically instruments these application frameworks: ASP.NET Core MVC 2.0, 2.1, 2.2, 3.0, 3.1, and 5.0 (includes Web API) Datastores The .NET agent automatically instruments the performance of .NET application calls to these datastores: Datastore Instance details Notes Microsoft SQL Server Use System.Data.SqlClient version 4.3.1 or Microsoft.Data.SqlClient. PostgresSQL Use Npgsql 4.0. Prior versions of Npgsql may also be instrumented, but duplicate and/or missing metrics are possible. MongoDB Driver version 2.3.x - 2.13.x: The .NET agent will support instrumenting pre-exising and new API methods in 2.6.x, but not new methods introduced in 2.7.x and higher. MySQL Use MySql.Data version 6.10.4 or later, or use the MySQL Connector StackExchange.Redis The .NET agent does not directly monitor datastore processes. Also, the .NET SQL parameter capture in a query trace does not list parameters for a parameterized query or a stored procedure. Collecting instance details for supported datastores is enabled by default. To request instance-level information from datastores not currently listed, get support at support.newrelic.com. External call libraries The .NET agent automatically instruments these external call libraries : Libraries Supported methods HttpClient The agent instruments these HttpClient methods: SendAsync GetAsync PostAsync PutAsync DeleteAsync GetStringAsync GetStreamAsync GetByteArrayAsync Messaging The agent automatically instruments these message systems: RabbitMQ 5.1.0 or higher: Puts and takes on messages and queue purge. When receiving messages using an IBasicConsumer, the EventingBasicConsumer is the only implementation that is instrumented by the .NET agent. BasicGet is instrumented, but the agent does not support distributed tracing for BasicGet. The following methods are instrumented: IModel.BasicGet IModel.BasicPublish IModel.BasicComsume IModel.QueuePurge EventingBasicConsumer.HandleBasicDeliver Unavailable features The following features are not available for the .NET agent: Memory usage on Linux due to an ongoing .NET Core issue (try using .NET performance metrics to get this information) Automatic brower monitoring script injection (API or manual instrumentation is required) The .NET agent does not support trim self-contained deployments and executables, because the compiler can potentially trim assemblies that the agent depends on. Infinite Tracing is not supported on Alpine Linux due to a GRPC compatibility issue. See this agent issue for more information. Connect the agent to other New Relic products In addition to APM, the .NET agent integrates with other New Relic products to give you end-to-end visibility: Product Integration Browser monitoring The browser monitoring JavaScript agent will not be injected by the .NET agent for ASP.NET core applications. However, you can inject the browser agent by using the .NET agent API or the browser agent's copy/paste method. After enabling browser injection, you can view browser data in the APM Summary page and quickly switch between the APM and browser data for a particular app. For configuration options and manual instrumentation, see browser monitoring and the .NET agent. Infrastructure monitoring When you install the Infrastructure and APM agents on the same host, they automatically detect one another. You can then view a list of hosts in the APM UI, and filter your Infrastructure hosts by APM app in the Infrastructure UI. For more information, see APM data in Infrastructure. Dashboards The .NET agent sends default events and attributes for NRQL queries. You can also record custom events for advanced analysis.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 33.06521,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": ".NET <em>agent</em>: compatibility and requirements for .NET Core",
        "sections": ".NET <em>agent</em>: compatibility and requirements for .NET Core",
        "tags": "<em>Agents</em>",
        "body": " and manual instrumentation, see browser monitoring and the .NET <em>agent</em>. Infrastructure monitoring When you install the Infrastructure and APM <em>agents</em> on the same host, they automatically detect one another. You can then view a list of hosts in the APM UI, and filter your Infrastructure hosts by APM app"
      },
      "id": "603e8e7e196a675c41a83d9f"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/get-started/introduction-new-relics-open-source-telemetry-integrations": [
    {
      "sections": [
        "Introduction to OpenTelemetry with New Relic",
        "Benefits of OpenTelemetry",
        "Should I use OpenTelemetry instrumentation or New Relic agents?",
        "OpenTelemetry: A work in progress",
        "New Relic's APM agents",
        "How OpenTelemetry works with New Relic",
        "Important",
        "Traces",
        "Metrics",
        "Logs",
        "Next steps"
      ],
      "title": "Introduction to OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "c87898d2d5835c00930c173eabd1bf93040badea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/introduction-opentelemetry-new-relic/",
      "published_at": "2021-10-07T00:14:26Z",
      "updated_at": "2021-09-27T15:16:23Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Are you already familiar with OpenTelemetry and want to begin the setup? Check out our quick start: Quick start If you don't have one already, create a New Relic account. It's free, forever. If you're just getting acquainted with OpenTelemetry, this is what we'll explore here: Benefits of OpenTelemetry Should I use OpenTelemetry or New Relic agents? How OpenTelemetry works with New Relic Benefits of OpenTelemetry OpenTelemetry provides a secure, vendor-neutral specification for service instrumentation so that you can export data to distinct backends of your choice, such as New Relic. OpenTelemetry offers a single set of APIs and libraries that standardize how you collect and transfer telemetry data for your services. The following components make up the OpenTelemetry project: Specifications for the core pillars of observability to drive consistency across all projects. New Relic supports all of these signals: traces, metrics, and logs (see details below) APIs that contain interfaces and implementations based on the specifications SDKs (reference implementations of the APIs) created specifically for languages like Java, Python, Go, Erlang, and more Collectors that offer a vendor-agnostic implementation for processing and exporting Exporters that enable you to send data to a backend of your choice The components of OpenTelemetry work together to create some distinct advantages for capturing telemetry data: Feature Description Ubiquitous instrumentation A single, open standard of instrumentation provides better coverage and flexibility as engineers from all over the world contribute to the instrumentation. Future proof As the instrumentation gets built into libraries and frameworks, and as more vendors move to support this open standard, you can be confident that you won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build integrations into OpenTelemetry or add instrumentation directly to source code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need to decide which instrumentation option to use (a proprietary option or one of the other open standards). Cross-platform compatibility OpenTelemetry supports a variety of languages and backends. It represents a vendor-neutral path for capturing and transmitting telemetry to backends without altering existing instrumentation. Streamlined observability It is easier for vendors to support and test against a single standard as they don’t need to develop their own agents or collectors. High dimensionality OpenTelemetry uses dimensional metrics, so you can filter and facet on more aspects of the data, such as AWS regions, Kubernetes clusters, or service versions. Dimensional metrics also lead to less time between occurrence and reporting. Efficiency OpenTelemetry’s fire-and-forget trace-centric approach to instrumentation often has lower overhead than New Relic agents, especially for asynchronous workloads. It will also result in better handling of trace data for asynchronous requests. Should I use OpenTelemetry instrumentation or New Relic agents? As you consider OpenTelemetry, you may also be looking at New Relic APM agents that also capture telemetry data. As you'd expect, there is a lot of overlap between features available from OpenTelemetry agents and SDKs versus those available from New Relic APM agents. This is especially true if you're interested in distributed tracing telemetry data. The choice you make depends on what you need. We recommend that you explore both New Relic and OpenTelemetry instrumentation or discuss this with us in our CNCF Slack channel to decide what works best for you. OpenTelemetry: A work in progress OpenTelemetry is still an emerging standard, so your choices may be affected by what's available. You can check on the current state of the specification at the OpenTelemetry site. The current state of language-specific OpenTelemetry APIs and SDKs varies: some languages are still pre-alpha and may be missing instructions on how to instrument your service. Most languages have some implementation of traces that is sufficient to start exporting data to New Relic. Check out this table in GitHub that provides an overview of the state of OpenTelemetry specification compliance for each language. For languages that New Relic does not currently provide an agent or SDK, OpenTelemetry may offer you a good alternative. Also, in cases where you want explicit control over sampling of your telemetry data, OpenTelemetry provides a lot of flexibility. As OpenTelemetry matures, New Relic will continue to support new OpenTelemetry data models and to provide a curated UI experience for our Full Stack Observability customers. New Relic's APM agents In general, New Relic's APM agents will collect more telemetry data for your services, and they offer a wide range of configuration options and an extensive set of auto-instrumentation capabilities. New Relic's APM agents offer detailed transaction trace visibility for individual services. They also offer predefined sampling to balance the performance impact of your instrumentation against the need to capture enough data to gain helpful insights. How OpenTelemetry works with New Relic New Relic supports the native OpenTelemetry Protocol (OTLP) for exporting telemetry data. This allows you to use the vendor neutral components developed by the OpenTelemetry community to export your data to New Relic. The following tables show the supported features for each telemetry signal. If you have questions about these or have an unsupported use case, please contact us in our CNCF Slack channel, and watch this page for future updates. Important New Relic's exporters for OpenTelemetry are now deprecated in favor of exporting data to New Relic using OTLP. Traces New Relic offers support for the OTLP ingest of trace signals. The maturity of the upstream specification is stable. OpenTelemetry traces and spans are compatible with New Relic traces and spans. OpenTelemetry spans optionally include attributes (name-value pairs) and resource attributes which map directly to dimensions that can be used to facet or filter span data at query time. OpenTelemetry span metadata (for example, name, kind, and trace_id) also map directly to dimensions on NewRelic spans. At this time, New Relic does not support span links or array attributes. For details, see the Traces section of our best practices guide. Feature Supported Span events ✅ Span linking ❌ Array of primitives (homogeneous) ❌ Metrics New Relic offers support for the OTLP ingest of metric signals. Note that the maturity of the upstream specification is experimental. We intend to follow potentially breaking upstream changes. Here are the OpenTelemetry data types we support and their associated mappings. For details, see the Metrics section of our best practices guide. Metric Type Supported Delta sums ✅ Cumulative sums ✅ Gauges ✅ Delta histograms ✅ Summary ✅ Cumulative histograms ❌ Exemplars ❌ Array of primitives (homogeneous) ❌ Logs New Relic offers support for the OTLP ingest of log signals. Note that the maturity of the upstream specification is experimental. We intend to follow potentially breaking upstream changes. OpenTelemetry logs are compatible with New Relic logs. OpenTelemetry logs optionally include attributes (name-value pairs) and resource attributes which map directly to dimensions that can be used to facet or filter log data at query time. OpenTelemetry log metadata (for example, name, severity_text, and trace_id) also map directly to dimensions on New Relic logs. NewRelic currently supports all OpenTelemetry log message types except for arrays. For more details, see the Logs section of our best practices guide. Feature Supported Description LogRecord body ✅ Supported types: string, boolean, int, double, bytes LogRecord attributes ✅ Supported types: string, boolean, int, double, bytes LogRecord fields ✅ Examples: name, severity_text, trace_id Array messages ❌ Array attributes ❌ Next steps Follow the OpenTelemetry quick start to help you get started. You'll also want to review the best practices guide for getting the most out of the data you export to New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 208.89241,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "sections": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " the <em>OpenTelemetry</em> quick <em>start</em> to help you <em>get</em> <em>started</em>. You&#x27;ll also want to review the best practices guide for getting the most out of the data you export to New Relic."
      },
      "id": "603e81ba196a67304da83dab"
    },
    {
      "sections": [
        "OpenTelemetry quick start",
        "Step 1. Prerequisites",
        "Step 2. Instrument your service with OpenTelemetry",
        "Step 3. Export your telemetry data to New Relic",
        "Review New Relic settings for exports",
        "Important",
        "Complete the export configuration steps",
        "Export data to an OpenTelemetry Collector (optional)",
        "Step 4. View your data in the New Relic UI",
        "What's next?"
      ],
      "title": "OpenTelemetry quick start",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "1b846417a2958b61b047c838db49aea06f09a2a8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-quick-start/",
      "published_at": "2021-10-07T01:54:02Z",
      "updated_at": "2021-09-27T14:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "OpenTelemetry is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up OpenTelemetry with New Relic. Here's an overview of the process, followed by details for each step. Prerequisites Instrument your service with OpenTelemetry Export your telemetry data to New Relic View your data in the New Relic UI Step 1. Prerequisites First things first: If we don’t already know you, sign up for a free New Relic account. Copy your account license key. Step 2. Instrument your service with OpenTelemetry To get started, you instrument your service with OpenTelemetry. OpenTelemetry has language-specific products and SDKs to help you. Many languages offer out-the-box instrumentation for common libraries and frameworks. Each language also provides an API for further instrumenting your service manually. Go to the repository for your language and follow the instructions to instrument your service. When you're done, return here to complete Step 3. Export your telemetry data to New Relic. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...See a complete list of languages in GitHub Step 3. Export your telemetry data to New Relic The OpenTelemetry Protocol, or OTLP for short, is a general purpose telemetry data delivery protocol designed for the OpenTelemetry project. This protocol describes how to encode and transmit telemetry data, which makes it a natural choice for data transport. Each language SDK provides an OTLP exporter you can configure to export data over OTLP. In this step, we focus on how to configure an OTLP exporter in your service to export data directly to New Relic. If you prefer to export your data first to an OpenTelemetry collector, we have separate instructions. Here's an example of sending data from your service directly to New Relic. To complete this third step, first familiarize yourself with some required New Relic settings, and then complete the steps in the OTLP exporter documentation for your language. Review New Relic settings for exports Before you go to the external OTLP exporter documentation, consult the table below so you're ready to do the following: Configure the OTLP exporter to add a header ( api-key ) whose value is your account license key. Based on your region, configure the endpoint where the exporter sends data to New Relic. Region gRPC HTTP/1.1 Endpoint API Header Name API Header Value TLS encryption required US ✅ ❌ https://otlp.nr-data.net:4317 api-key License Key ✅ EU ✅ ❌ https://otlp.eu01.nr-data.net:4317 api-key License Key ✅ If you have FedRamp compliance constraints, see FedRAMP-compliant endpoints. Important In Node.js, the opentelemetry-collector-grpc library requires additional options to enable TLS. Complete the export configuration steps Click on the link below for your language and complete the configuration steps. When you're done, return here to complete Step 4. View your data in the New Relic UI. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...Find additional OTLP language support in GitHub Export data to an OpenTelemetry Collector (optional) The OpenTelemetry Collector is a configurable and extensible software component to receive, process, and export telemetry data. When you set up a collector, it can operate as a gateway or as an agent: Gateway: The collector receives data from a variety of sources and applies standard processing before exporting to some backend. Agent: The collector is deployed on each host in an environment and can collect telemetry data about the host and processes running on it. When you use a collector, you start by following the same routine as above for setting up OTLP in your service. In this case, instead of exporting data directly to New Relic, you export through a collector that you set up. In the collector, you configure the OTLP exporter to export data to New Relic. When your data goes through a collector, the transport looks like this: Here's a Docker example of how to set up and run an OpenTelemetry collector with the collector YAML: Save the following as otel-config.yaml: receivers: otlp: protocols: grpc: http: processors: batch: exporters: otlp: endpoint: ${OTEL_EXPORTER_OTLP_ENDPOINT} headers: api-key: ${NEW_RELIC_LICENSE_KEY} service: pipelines: traces: receivers: [otlp] processors: [batch] exporters: [otlp] metrics: receivers: [otlp] processors: [batch] exporters: [otlp] logs: receivers: [otlp] processors: [batch] exporters: [otlp] Copy Run the OpenTelemetry collector, making sure you replace OTLP_ENDPOINT_HERE with the appropriate endpoint and replace YOUR_KEY_HERE with your Account License Key: export OTEL_EXPORTER_OTLP_ENDPOINT=OTLP_ENDPOINT_HERE export NEW_RELIC_LICENSE_KEY=YOUR_KEY_HERE docker run --rm \\ -e OTEL_EXPORTER_OTLP_ENDPOINT \\ -e NEW_RELIC_LICENSE_KEY \\ -p 4317:4317 \\ -v \"${PWD}/otel-config.yaml\":/otel-config.yaml \\ --config otel-config.yaml \\ --name otelcol \\ otel/opentelemetry-collector Copy Step 4. View your data in the New Relic UI Once you’ve instrumented your service and configured it to export its data to New Relic, watch the New Relic One user interface for your traces, metrics, and logs! The UI for OpenTelemetry has some similarities to the APM agent UI, so if you are familiar with that, you can go right to the UI. If you need help understanding your OpenTelemetry UI options or how to make sure your data appears in the UI, see View your OpenTelemetry data in New Relic. What's next? After you do your initial setup, check out our best-practices guide for tips about various configurations to improve your use of OpenTelemetry and New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 208.81285,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> quick <em>start</em>",
        "sections": "<em>OpenTelemetry</em> quick <em>start</em>",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " your <em>telemetry</em> data to New Relic View your data in the New Relic UI Step 1. Prerequisites First things first: If we don’t already know you, sign up for a free New Relic account. Copy your account license key. Step 2. Instrument your service with <em>OpenTelemetry</em> To <em>get</em> <em>started</em>, you instrument your"
      },
      "id": "6044e5dfe7b9d2aadc5799d4"
    },
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Find your service (entity)",
        "Narrow down your data with filters",
        "Summary page",
        "Distributed tracing",
        "Find traces and their associated spans",
        "Tip",
        "View spans with errors",
        "View span events",
        "Databases",
        "Errors",
        "Externals",
        "JVMs",
        "Logs",
        "Metrics explorer",
        "Transactions",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/490779aac8c10802682d86f0b785b6a2/a4078/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-10-07T01:52:56Z",
      "updated_at": "2021-09-27T15:53:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information about your services. It offers you a way to filter services and then filter the data to show a variety of views into your data. Find your service (entity) To get started in Explorer, you need to find your service: Go to one.newrelic.com. In the left sidebar, click Services - OpenTelemetry: In the center pane, click the service you want to know more about, or find your service by entering the name in the top filter bar. Narrow down your data with filters Once you have your entity, you can then filter for data from your service. With the filter bar Narrow data to..., you can highlight a specific facet of the telemetry recorded for your service. For example, you may want to see the error rate for a particular version of the service that you've deployed in a canary instance, so you add a filter for service.version='1.2.3'. Filters are preserved when navigating between different views of your data for a service. For example, the filter for service.version='1.2.3' carries over to the Transactions view, so that you would see telemetry on requests to the endpoints (transactions) that are running version 1.2.3 of your service, and not any other versions. Filters are preserved when navigating between the Summary, Transactions, Databases, Externals, Errors, and JVMs views. Filters are also preserved when navigating to the Distributed tracing view, but with limitations. Only filter conditions that use the equals operator (\"=\") are currently supported when navigating to Distributed tracing. If you navigate back from the Distributed tracing page, the filters you selected on the previous view will come back. Once you filter your data, the UI has various views in the left-navigation pane. See our descriptions below for view details. Summary page After you click on a service in the Explorer tab, you see the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. For your data to appear in this section, make sure it has the following: UI area Attribute Response time span.kind = server or consumer Throughput span.kind = server or consumer Error rate span.kind = server or consumer otel.status_code = ERROR Service instances pane service.instance.id (usually set via the OTel resource API) Distributed tracing In Distributed tracing, you can locate traces and examine span details. For your trace data to appear in the New Relic UI, it needs to conform to the OpenTelemetry trace semantic conventions. Also, in the Traces section of our best practices guide, you can find some tips about making sure your traces and spans appear in New Relic. The following attributes are typically not added explicitly to spans. Rather, they are usually set when creating a span or performing operations (for example, recording an error) on a span using the OpenTelemetry SDK. Attribute Description name A span’s name is generally set when starting a span. The name can generally be anything, but the OpenTelemetry specification provides guidance for certain types of spans like Database spans or HTTP spans. span.kind A span’s kind is generally set when starting a span. New Relic uses span.kind to infer that a span is an entry point to a given service. When span.kind = server or consumer it is considered an entry point. When span.kind = client or producer, it is considered a call to an external service or database system. otel.status_code A span’s status is set using the span API. The otel.status_code attribute is how it is manifested by New Relic. The UI primarily uses otel.status_code for the purpose of identifying errors. Find traces and their associated spans Here are some ways to target your searches: To find the traces you want, you can run queries like these in the filter bar: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID When you find an interesting trace, click on it to display a waterfall diagram showing the spans of the trace. Click on specific spans within the trace to display span details in the right panel. Tip For more ways to filter traces, see our distributed tracing UI page. View spans with errors After you click on a span in the trace waterfall view, you can see span errors in the right panel under Error details. The error details are populated by spans containing otel.status_code = ERROR and display the content of otel.status_description. To narrow your search for spans with errors, you can enter otel.status_code = ERROR directly in the distributed tracing filter bar. View span events If you send span events as described by the OpenTelemetry specification, you can view them in the New Relic UI. Span events have two general types: Exceptions Non-exceptions (for example, logs) If you have span events, links for these appear in the right pane: Click on a span in the waterfall view. In the right pane, click View span events, or you can expand Error details and click the link at the bottom of the errors. When you're in span events and only want to view exceptions, slide the toggle Only show exceptions. Tip OpenTelemetry exceptions handled by the app/service are displayed independently of span error status and are not necessarily associated with a span error status. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. For your data to appear in this section, make sure it has the following: UI area Attribute Top database calls span.kind = client or producer db.system Facets by span name Top databases (by query time) span.kind = client or producer db.system Facets by db.system Top databases (by throughput) span.kind = client or producer db.system Facets by db.system Errors On the Errors page, you can see total errors as well as charts showing error count and error rate. For your data to appear in this section, make sure it has the following: span.kind = server or consumer otel.status_code = ERROR Facets by span name Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. For your data to appear in this section, make sure it has the following: span.kind = client or producer db.system is not present JVMs When you drill into a specific JVM, the UI display charts driven by JVM metric data: JVM metrics follow the general semantic conventions for runtime environment metrics. The Java specific runtime metrics are not well documented. The implementation is effectively the documentation and may be subject to change. For your data to appear in this section, make sure it has the following: Requires a unique service.instance.id attribute for rendering the list of JVMs Service.instance.id is an OpenTelemetry resource attribute Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. For your data to appear in this section, make sure it has the following: service.name To correlate with trace data, the logs should contain the trace.id and span.id attributes. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. For your data to appear in this section, make sure it has the following: UI area Description Top Transactions span.kind = server or consumer Facets by span name Throughput span.kind = server or consumer Facets by span name Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.8509,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: <em>Get</em> the big picture along with the details The New Relic Explorer tab is a good place to <em>start</em> gathering information"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/istio/istio-adapter": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Find your service (entity)",
        "Narrow down your data with filters",
        "Summary page",
        "Distributed tracing",
        "Find traces and their associated spans",
        "Tip",
        "View spans with errors",
        "View span events",
        "Databases",
        "Errors",
        "Externals",
        "JVMs",
        "Logs",
        "Metrics explorer",
        "Transactions",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/490779aac8c10802682d86f0b785b6a2/a4078/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-10-07T01:52:56Z",
      "updated_at": "2021-09-27T15:53:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information about your services. It offers you a way to filter services and then filter the data to show a variety of views into your data. Find your service (entity) To get started in Explorer, you need to find your service: Go to one.newrelic.com. In the left sidebar, click Services - OpenTelemetry: In the center pane, click the service you want to know more about, or find your service by entering the name in the top filter bar. Narrow down your data with filters Once you have your entity, you can then filter for data from your service. With the filter bar Narrow data to..., you can highlight a specific facet of the telemetry recorded for your service. For example, you may want to see the error rate for a particular version of the service that you've deployed in a canary instance, so you add a filter for service.version='1.2.3'. Filters are preserved when navigating between different views of your data for a service. For example, the filter for service.version='1.2.3' carries over to the Transactions view, so that you would see telemetry on requests to the endpoints (transactions) that are running version 1.2.3 of your service, and not any other versions. Filters are preserved when navigating between the Summary, Transactions, Databases, Externals, Errors, and JVMs views. Filters are also preserved when navigating to the Distributed tracing view, but with limitations. Only filter conditions that use the equals operator (\"=\") are currently supported when navigating to Distributed tracing. If you navigate back from the Distributed tracing page, the filters you selected on the previous view will come back. Once you filter your data, the UI has various views in the left-navigation pane. See our descriptions below for view details. Summary page After you click on a service in the Explorer tab, you see the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. For your data to appear in this section, make sure it has the following: UI area Attribute Response time span.kind = server or consumer Throughput span.kind = server or consumer Error rate span.kind = server or consumer otel.status_code = ERROR Service instances pane service.instance.id (usually set via the OTel resource API) Distributed tracing In Distributed tracing, you can locate traces and examine span details. For your trace data to appear in the New Relic UI, it needs to conform to the OpenTelemetry trace semantic conventions. Also, in the Traces section of our best practices guide, you can find some tips about making sure your traces and spans appear in New Relic. The following attributes are typically not added explicitly to spans. Rather, they are usually set when creating a span or performing operations (for example, recording an error) on a span using the OpenTelemetry SDK. Attribute Description name A span’s name is generally set when starting a span. The name can generally be anything, but the OpenTelemetry specification provides guidance for certain types of spans like Database spans or HTTP spans. span.kind A span’s kind is generally set when starting a span. New Relic uses span.kind to infer that a span is an entry point to a given service. When span.kind = server or consumer it is considered an entry point. When span.kind = client or producer, it is considered a call to an external service or database system. otel.status_code A span’s status is set using the span API. The otel.status_code attribute is how it is manifested by New Relic. The UI primarily uses otel.status_code for the purpose of identifying errors. Find traces and their associated spans Here are some ways to target your searches: To find the traces you want, you can run queries like these in the filter bar: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID When you find an interesting trace, click on it to display a waterfall diagram showing the spans of the trace. Click on specific spans within the trace to display span details in the right panel. Tip For more ways to filter traces, see our distributed tracing UI page. View spans with errors After you click on a span in the trace waterfall view, you can see span errors in the right panel under Error details. The error details are populated by spans containing otel.status_code = ERROR and display the content of otel.status_description. To narrow your search for spans with errors, you can enter otel.status_code = ERROR directly in the distributed tracing filter bar. View span events If you send span events as described by the OpenTelemetry specification, you can view them in the New Relic UI. Span events have two general types: Exceptions Non-exceptions (for example, logs) If you have span events, links for these appear in the right pane: Click on a span in the waterfall view. In the right pane, click View span events, or you can expand Error details and click the link at the bottom of the errors. When you're in span events and only want to view exceptions, slide the toggle Only show exceptions. Tip OpenTelemetry exceptions handled by the app/service are displayed independently of span error status and are not necessarily associated with a span error status. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. For your data to appear in this section, make sure it has the following: UI area Attribute Top database calls span.kind = client or producer db.system Facets by span name Top databases (by query time) span.kind = client or producer db.system Facets by db.system Top databases (by throughput) span.kind = client or producer db.system Facets by db.system Errors On the Errors page, you can see total errors as well as charts showing error count and error rate. For your data to appear in this section, make sure it has the following: span.kind = server or consumer otel.status_code = ERROR Facets by span name Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. For your data to appear in this section, make sure it has the following: span.kind = client or producer db.system is not present JVMs When you drill into a specific JVM, the UI display charts driven by JVM metric data: JVM metrics follow the general semantic conventions for runtime environment metrics. The Java specific runtime metrics are not well documented. The implementation is effectively the documentation and may be subject to change. For your data to appear in this section, make sure it has the following: Requires a unique service.instance.id attribute for rendering the list of JVMs Service.instance.id is an OpenTelemetry resource attribute Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. For your data to appear in this section, make sure it has the following: service.name To correlate with trace data, the logs should contain the trace.id and span.id attributes. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. For your data to appear in this section, make sure it has the following: UI area Description Top Transactions span.kind = server or consumer Facets by span name Throughput span.kind = server or consumer Facets by span name Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 195.09013,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "Introduction to OpenTelemetry with New Relic",
        "Benefits of OpenTelemetry",
        "Should I use OpenTelemetry instrumentation or New Relic agents?",
        "OpenTelemetry: A work in progress",
        "New Relic's APM agents",
        "How OpenTelemetry works with New Relic",
        "Important",
        "Traces",
        "Metrics",
        "Logs",
        "Next steps"
      ],
      "title": "Introduction to OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "c87898d2d5835c00930c173eabd1bf93040badea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/introduction-opentelemetry-new-relic/",
      "published_at": "2021-10-07T00:14:26Z",
      "updated_at": "2021-09-27T15:16:23Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Are you already familiar with OpenTelemetry and want to begin the setup? Check out our quick start: Quick start If you don't have one already, create a New Relic account. It's free, forever. If you're just getting acquainted with OpenTelemetry, this is what we'll explore here: Benefits of OpenTelemetry Should I use OpenTelemetry or New Relic agents? How OpenTelemetry works with New Relic Benefits of OpenTelemetry OpenTelemetry provides a secure, vendor-neutral specification for service instrumentation so that you can export data to distinct backends of your choice, such as New Relic. OpenTelemetry offers a single set of APIs and libraries that standardize how you collect and transfer telemetry data for your services. The following components make up the OpenTelemetry project: Specifications for the core pillars of observability to drive consistency across all projects. New Relic supports all of these signals: traces, metrics, and logs (see details below) APIs that contain interfaces and implementations based on the specifications SDKs (reference implementations of the APIs) created specifically for languages like Java, Python, Go, Erlang, and more Collectors that offer a vendor-agnostic implementation for processing and exporting Exporters that enable you to send data to a backend of your choice The components of OpenTelemetry work together to create some distinct advantages for capturing telemetry data: Feature Description Ubiquitous instrumentation A single, open standard of instrumentation provides better coverage and flexibility as engineers from all over the world contribute to the instrumentation. Future proof As the instrumentation gets built into libraries and frameworks, and as more vendors move to support this open standard, you can be confident that you won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build integrations into OpenTelemetry or add instrumentation directly to source code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need to decide which instrumentation option to use (a proprietary option or one of the other open standards). Cross-platform compatibility OpenTelemetry supports a variety of languages and backends. It represents a vendor-neutral path for capturing and transmitting telemetry to backends without altering existing instrumentation. Streamlined observability It is easier for vendors to support and test against a single standard as they don’t need to develop their own agents or collectors. High dimensionality OpenTelemetry uses dimensional metrics, so you can filter and facet on more aspects of the data, such as AWS regions, Kubernetes clusters, or service versions. Dimensional metrics also lead to less time between occurrence and reporting. Efficiency OpenTelemetry’s fire-and-forget trace-centric approach to instrumentation often has lower overhead than New Relic agents, especially for asynchronous workloads. It will also result in better handling of trace data for asynchronous requests. Should I use OpenTelemetry instrumentation or New Relic agents? As you consider OpenTelemetry, you may also be looking at New Relic APM agents that also capture telemetry data. As you'd expect, there is a lot of overlap between features available from OpenTelemetry agents and SDKs versus those available from New Relic APM agents. This is especially true if you're interested in distributed tracing telemetry data. The choice you make depends on what you need. We recommend that you explore both New Relic and OpenTelemetry instrumentation or discuss this with us in our CNCF Slack channel to decide what works best for you. OpenTelemetry: A work in progress OpenTelemetry is still an emerging standard, so your choices may be affected by what's available. You can check on the current state of the specification at the OpenTelemetry site. The current state of language-specific OpenTelemetry APIs and SDKs varies: some languages are still pre-alpha and may be missing instructions on how to instrument your service. Most languages have some implementation of traces that is sufficient to start exporting data to New Relic. Check out this table in GitHub that provides an overview of the state of OpenTelemetry specification compliance for each language. For languages that New Relic does not currently provide an agent or SDK, OpenTelemetry may offer you a good alternative. Also, in cases where you want explicit control over sampling of your telemetry data, OpenTelemetry provides a lot of flexibility. As OpenTelemetry matures, New Relic will continue to support new OpenTelemetry data models and to provide a curated UI experience for our Full Stack Observability customers. New Relic's APM agents In general, New Relic's APM agents will collect more telemetry data for your services, and they offer a wide range of configuration options and an extensive set of auto-instrumentation capabilities. New Relic's APM agents offer detailed transaction trace visibility for individual services. They also offer predefined sampling to balance the performance impact of your instrumentation against the need to capture enough data to gain helpful insights. How OpenTelemetry works with New Relic New Relic supports the native OpenTelemetry Protocol (OTLP) for exporting telemetry data. This allows you to use the vendor neutral components developed by the OpenTelemetry community to export your data to New Relic. The following tables show the supported features for each telemetry signal. If you have questions about these or have an unsupported use case, please contact us in our CNCF Slack channel, and watch this page for future updates. Important New Relic's exporters for OpenTelemetry are now deprecated in favor of exporting data to New Relic using OTLP. Traces New Relic offers support for the OTLP ingest of trace signals. The maturity of the upstream specification is stable. OpenTelemetry traces and spans are compatible with New Relic traces and spans. OpenTelemetry spans optionally include attributes (name-value pairs) and resource attributes which map directly to dimensions that can be used to facet or filter span data at query time. OpenTelemetry span metadata (for example, name, kind, and trace_id) also map directly to dimensions on NewRelic spans. At this time, New Relic does not support span links or array attributes. For details, see the Traces section of our best practices guide. Feature Supported Span events ✅ Span linking ❌ Array of primitives (homogeneous) ❌ Metrics New Relic offers support for the OTLP ingest of metric signals. Note that the maturity of the upstream specification is experimental. We intend to follow potentially breaking upstream changes. Here are the OpenTelemetry data types we support and their associated mappings. For details, see the Metrics section of our best practices guide. Metric Type Supported Delta sums ✅ Cumulative sums ✅ Gauges ✅ Delta histograms ✅ Summary ✅ Cumulative histograms ❌ Exemplars ❌ Array of primitives (homogeneous) ❌ Logs New Relic offers support for the OTLP ingest of log signals. Note that the maturity of the upstream specification is experimental. We intend to follow potentially breaking upstream changes. OpenTelemetry logs are compatible with New Relic logs. OpenTelemetry logs optionally include attributes (name-value pairs) and resource attributes which map directly to dimensions that can be used to facet or filter log data at query time. OpenTelemetry log metadata (for example, name, severity_text, and trace_id) also map directly to dimensions on New Relic logs. NewRelic currently supports all OpenTelemetry log message types except for arrays. For more details, see the Logs section of our best practices guide. Feature Supported Description LogRecord body ✅ Supported types: string, boolean, int, double, bytes LogRecord attributes ✅ Supported types: string, boolean, int, double, bytes LogRecord fields ✅ Examples: name, severity_text, trace_id Array messages ❌ Array attributes ❌ Next steps Follow the OpenTelemetry quick start to help you get started. You'll also want to review the best practices guide for getting the most out of the data you export to New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 195.00615,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "sections": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build <em>integrations</em> into <em>OpenTelemetry</em> or add instrumentation directly to <em>source</em> code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need"
      },
      "id": "603e81ba196a67304da83dab"
    },
    {
      "sections": [
        "Best practices for OpenTelemetry with New Relic",
        "Resources",
        "Batching",
        "Caution",
        "Compression",
        "Traces",
        "Required fields",
        "Sampling",
        "OpenTelemetry built-in samplers",
        "OpenTelemetry tail-based samplers",
        "New Relic tail-based sampling with Infinite Tracing",
        "Important",
        "Metrics",
        "Sum metrics",
        "Delta sums",
        "Cumulative sums",
        "Sum configuration examples",
        "Gauge metrics",
        "Histogram metrics",
        "Summary metrics",
        "Start time",
        "Array values for attributes",
        "Exemplars",
        "How to query metrics",
        "Query cumulative sums stored as gauges",
        "Example: Raw gauge value for cumulative sums",
        "Example: Rate of change with cumulative sums as gauges",
        "Query gauge metrics",
        "Query histogram metrics",
        "Example: Normal distribution",
        "Example: Heat map",
        "Logs",
        "Send logs to New Relic",
        "Application log correlation",
        "View OpenTelemetry logs",
        "The time field"
      ],
      "title": "Best practices for OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "2b624c5862a5a48d088268eccb310510f372b125",
      "image": "https://docs.newrelic.com/static/764474b6404e7a5c65e55d2eb985e93c/c1b63/sum-derivative-function.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-concepts/",
      "published_at": "2021-10-07T01:54:01Z",
      "updated_at": "2021-09-27T14:48:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Here are some best practices based on how OpenTelemetry works with New Relic: Resources Batching Compression Traces Metrics Logs Resources A resource in OpenTelemetry represents information about an entity generating telemetry data. All telemetry data sent to New Relic is expected to be associated with a resource so that it can be linked with the appropriate entity in New Relic. The OpenTelemetry Resource SDK specification defines the functionality implemented by all language SDKs for defining a resource. The following suites of attributes are defined by the OpenTelemetry resource semantic conventions. These attributes are usually set by creating a resource using the OpenTelemetry SDK. service.* attributes service.name attribute is required to associate your resource with an entity in the UI service.instance.id is required for certain panes to light up telemetry.sdk.language=java is required to see data in the JVM section Batching Caution Avoid getting rate limited! You should batch requests sent to the OTLP endpoint as described in this section. By default, the OpenTelemetry SDKs and Collector send one (1) data point per request. Using these defaults, it is likely your account will be rate limited. All OpenTelemetry SDKs and Collectors provide a BatchProcessor, which batches data points in memory. This batching allows requests to be sent with more than one (1) data point. Component Batch Processor Collector Batch Processor Go SDK BatchSpanProcessor JS SDK BatchSpanProcessor Python SDK BatchExportSpanProcessor Compression New Relic supports gzip compression for OTLP payloads exported over gRPC. To maximize the amount of data you can send per request, we recommend enabling compression in all OTLP exporters. If there are other compression formats you'd like to see us support, please let us know in the CNCF Slack channel. Traces Familiarize yourself with these trace topics to ensure your traces and spans appear in New Relic. Required fields The startTimeUnixNano and endTimeUnixNano fields on spans are required according to the OpenTelemetry protocol for trace data. When startTimeUnixNano is not present, the span is dropped and a NrIntegrationError is created. When endTimeUnixNano is not present, the duration of your span is large and negative. The timeUnixNano field on span events is required. When timeUnixNano is not present, the span event is dropped and a NrIntegrationError is created. The traceId and spanId fields on spans are required according to the OpenTelemetry protocol for trace data. When traceId or spanId are not present, the span is dropped and a NrIntegrationError is created. Sampling Trace data is the most mature OpenTelemetry data type. Because of this, New Relic's OpenTelemetry user experience is largely based on trace data and is therefore influenced by your sampling strategy. You can configure sampling in a number of places: Service: Use the OpenTelemetry SDK for your language. Collector: If you're running your own instance of the OpenTelemetry collector, you can configure it to do more sophisticated forms of sampling, such as tail-based sampling (see below). Check out this documentation about how to configure different types of sampling: OpenTelemetry built-in samplers Built-in samplers implemented by the OpenTelemetry SDK for each language. OpenTelemetry tail-based samplers The OpenTelemetry collector has a tail-based sampling processor. We have an example demonstrating the use of the tail-based sampling processor. New Relic tail-based sampling with Infinite Tracing Infinite Tracing is New Relic's tail-based sampling option. You can use this in conjunction with your OpenTelemetry instrumented services. In setting up Infinite Tracing, you need to override the default span endpoint and send telemetry data to the New Relic trace observer: Important Currently, Infinite Tracing does not support OTLP ingest. You must run your own instance of the OpenTelemetry Collector and configure it to use the New Relic exporter. Follow the steps in Set up the trace observer to get the value for YOUR_TRACE_OBSERVER_URL. Use the value of YOUR_TRACE_OBSERVER_URL to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your OpenTelemetry integrations use the AlwaysOn sampler. Metrics OpenTelemetry metrics are largely compatible with New Relic dimensional metrics. We support OpenTelemetry metrics v0.10. All of the supported metric types include an independent set of associated attributes (name-value pairs) which map directly to dimensions you can use to facet or filter metric data at query time. OpenTelemetry metrics are accompanied by a set of resource attributes that identify the originating entity that produced them and map to dimensions for faceting and filtering. The OpenTelemetry data model for metrics defines a number of different metric types: sum, gauge, histogram, and summary. Sum metrics OpenTelemetry sums are a scalar metric that is the sum of all data points over a given time window. Sums have a notion of temporality indicating whether reported values incorporate previous measurements (cumulative temporality) or not (delta temporality). In addition, sums can either be monotonic (only go up or only go down) or non-monotonic (go up and down). Delta sums In New Relic, delta metrics are handled differently depending on whether they are monotonic or non-monotonic: Monotonic delta sums are mapped to the count metric type. Non-monotonic delta sums are mapped to the gauge metric type. Cumulative sums Monotonic and non-monotonic cumulative sums are mapped to the New Relic gauge metric type. Sum configuration examples To understand how to configure aggregation temporality, see these examples using the Java and Go OpenTelemetry SDKs. Gauge metrics OpenTelemetry gauge metric data points represent a sampled value at a given time. These values are converted to the New Relic gauge metric type. OpenTelemetry gauges do not have an aggregation temporality, but the sampled values can be aggregated at query time. Histogram metrics OpenTelemetry histograms compactly represent a population of recorded values along with a total count and sum. Optionally, histograms may include a series of buckets with explicit bounds and a count value for that bucket’s population. OpenTelemetry histograms are converted to New Relic’s distribution metric type, which is backed by a scaled exponential base 2 histogram (see NrSketch for a more thorough explanation). Counts from OpenTelemetry histogram buckets are assigned to New Relic’s distribution metric buckets using linear interpolation. Also, OpenTelemetry has negative and positive infinity bound buckets which we represent in New Relic as zero-width buckets. We do this because we do not have a representation for negative and positive infinity. For example, an OpenTelemetry bucket with bounds [-∞, 10) will be represented by a [10,10) zero width New Relic bucket. You may see exaggerated bucket counts at the endpoints of your distribution due to this translation. Summary metrics OpenTelemetry summary metric data points are used to represent quantile summaries (for example, P99 latency). These map directly to the New Relic summary metric type. Summary metric data points include count, sum, and quantile values, with 0.0 as min and 1.0 as max. OpenTelemetry provides summary metrics for compatibility with other formats. Start time The startTimeUnixNano field is optional according to the OpenTelemetry specification. When this field is provided, it is used for the timestamp on the resulting NewRelic metric, and the duration is calculated as timeUnixNano - startTimeUnixNano. The duration field is used to calculate the queryable endTimeStamp attribute on the New Relic metric, but it serves no other semantic purpose. If startTimeUnixNano is not provided, then timeUnixNano is used for the timestamp field on the resulting NewRelic metric, and the duration field is set to zero. Array values for attributes OpenTelemetry metrics and other signals may include attributes that consist of a homogenous array of primitive types. These attributes are not supported by New Relic. Exemplars OpenTelemetry defines exemplar values that allow other signals, like traces, to be connected to a metric event and provide context. Exemplars are not supported by New Relic. How to query metrics Consider these tips for building metric NRQL queries in New Relic. Query cumulative sums stored as gauges Since cumulative sums are converted to gauges, here are some ways to query your data: Example: Raw gauge value for cumulative sums To view the raw gauge value for cumulative sums, you can use the latest() NRQL function: SELECT latest(totalApiBytesSent) FROM Metric Copy Example: Rate of change with cumulative sums as gauges To see the rate of change over a given time interval for a cumulative sum stored as a gauge, you can use the derivative() NRQL function: SELECT derivative(totalApiBytesSent, 1 minute) FROM Metric Copy New Relic does not currently support either reporting on resets and gaps or accounting for them with cumulative counters. Query gauge metrics When New Relic converts cumulative sums to gauges, you can query them using either the latest() or derivative() NRQL functions. The function you choose depends on whether you want to see the raw value or compute the rate of change. Query histogram metrics New Relic histograms translated from OpenTelemetry metrics have the same query semantics as other New Relic histograms. Namely, the histogram() NRQL function can be used to represent the histogram with a configurable number of buckets and bucket width. Note that you may see larger bucket counts at the endpoint buckets. This is because we are adding negative and positive infinity bound OpenTelemetry buckets into a zero width New Relic bucket. Example: Normal distribution FROM Metric SELECT histogram(test.histogram, buckets: 100, width: 1000) WHERE distributionType = 'Normal Distribution' SINCE 1 day ago Copy Example: Heat map The FACET keyword is also available to create heat map charts. FROM Metric SELECT histogram(test.histogram, buckets: 100, width: 1000) FACET distributionType SINCE 1 day ago Copy Important The TIMESERIES keyword is not supported for New Relic histograms. Logs Logs generated from your applications and environment are an important piece of telemetry. They may represent application logs, machine generated events, or system logs. OpenTelemetry has defined a log data model for representing log data. You can send logs using OpenTelemetry tooling, correlate them with applications, and view them in New Relic. Send logs to New Relic The OpenTelemetry Collector and OpenTelemetry Collector Contrib repositories contain a number of components for consuming log data. The general pattern is to configure the collector to: Receive logs from any of the log receivers. Some of the receiver options include Filelog Receiver, Fluent Forward Receiver, and Syslog Receiver. Process logs, potentially annotating them with resource information. Some of the processor options include Resource Detection Processor and Resource Processor. Export logs to New Relic via the OTLP exporter. Application log correlation Application logs are more useful if they're correlated with other telemetry data produced by the application. The OpenTelemetry semantic convention for services specifies service.name as a required field. All application metric, trace, and log data sent to New Relic with the same service.name are associated with the same entity. The specifics of how logs get annotated with the service.name resource attribute depends on the application's environment: Applications may produce structured JSON logs, which you can configure to include service.name as another field. You can deploy applications alongside a dedicated Collector Agent instance, which you can configure with a Resource Processor to annotate logs with the service.name attribute. Optionally, additional application trace context (sometimes called execution context) can be propagated to log messages. The setup and availability of this depends on the language and logging framework used by the application. The general strategy is to set up the application to write structured JSON logs and to configure it to extract trace context into specified trace context fields on available log messages. The Logs in Context with Log4j2 example in GitHub demonstrates an end-to-end working example for a simple Java application using Log4j2. View OpenTelemetry logs Here are two ways you can view logs: Look in the New Relic Logs UI. If your logs are correlated with an application, view them in the context of the application. The time field The timeUnixNano field is optional according to the OpenTelemetry specification for log data. When timeUnixNano is not present New Relic will use the time that the data was received for the New Relic log timestamp.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.94316,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Best practices for <em>OpenTelemetry</em> with New Relic",
        "sections": "Best practices for <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to analyze all your traces, make sure to verify that your <em>OpenTelemetry</em> <em>integrations</em> use the AlwaysOn sampler. Metrics <em>OpenTelemetry</em> metrics are largely compatible with New Relic dimensional metrics. We support <em>OpenTelemetry</em> metrics v0.10. All of the supported metric types include an independent set"
      },
      "id": "60f6b9b964441f5da847ac01"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/kamon/kamon-reporter": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Find your service (entity)",
        "Narrow down your data with filters",
        "Summary page",
        "Distributed tracing",
        "Find traces and their associated spans",
        "Tip",
        "View spans with errors",
        "View span events",
        "Databases",
        "Errors",
        "Externals",
        "JVMs",
        "Logs",
        "Metrics explorer",
        "Transactions",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/490779aac8c10802682d86f0b785b6a2/a4078/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-10-07T01:52:56Z",
      "updated_at": "2021-09-27T15:53:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information about your services. It offers you a way to filter services and then filter the data to show a variety of views into your data. Find your service (entity) To get started in Explorer, you need to find your service: Go to one.newrelic.com. In the left sidebar, click Services - OpenTelemetry: In the center pane, click the service you want to know more about, or find your service by entering the name in the top filter bar. Narrow down your data with filters Once you have your entity, you can then filter for data from your service. With the filter bar Narrow data to..., you can highlight a specific facet of the telemetry recorded for your service. For example, you may want to see the error rate for a particular version of the service that you've deployed in a canary instance, so you add a filter for service.version='1.2.3'. Filters are preserved when navigating between different views of your data for a service. For example, the filter for service.version='1.2.3' carries over to the Transactions view, so that you would see telemetry on requests to the endpoints (transactions) that are running version 1.2.3 of your service, and not any other versions. Filters are preserved when navigating between the Summary, Transactions, Databases, Externals, Errors, and JVMs views. Filters are also preserved when navigating to the Distributed tracing view, but with limitations. Only filter conditions that use the equals operator (\"=\") are currently supported when navigating to Distributed tracing. If you navigate back from the Distributed tracing page, the filters you selected on the previous view will come back. Once you filter your data, the UI has various views in the left-navigation pane. See our descriptions below for view details. Summary page After you click on a service in the Explorer tab, you see the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. For your data to appear in this section, make sure it has the following: UI area Attribute Response time span.kind = server or consumer Throughput span.kind = server or consumer Error rate span.kind = server or consumer otel.status_code = ERROR Service instances pane service.instance.id (usually set via the OTel resource API) Distributed tracing In Distributed tracing, you can locate traces and examine span details. For your trace data to appear in the New Relic UI, it needs to conform to the OpenTelemetry trace semantic conventions. Also, in the Traces section of our best practices guide, you can find some tips about making sure your traces and spans appear in New Relic. The following attributes are typically not added explicitly to spans. Rather, they are usually set when creating a span or performing operations (for example, recording an error) on a span using the OpenTelemetry SDK. Attribute Description name A span’s name is generally set when starting a span. The name can generally be anything, but the OpenTelemetry specification provides guidance for certain types of spans like Database spans or HTTP spans. span.kind A span’s kind is generally set when starting a span. New Relic uses span.kind to infer that a span is an entry point to a given service. When span.kind = server or consumer it is considered an entry point. When span.kind = client or producer, it is considered a call to an external service or database system. otel.status_code A span’s status is set using the span API. The otel.status_code attribute is how it is manifested by New Relic. The UI primarily uses otel.status_code for the purpose of identifying errors. Find traces and their associated spans Here are some ways to target your searches: To find the traces you want, you can run queries like these in the filter bar: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID When you find an interesting trace, click on it to display a waterfall diagram showing the spans of the trace. Click on specific spans within the trace to display span details in the right panel. Tip For more ways to filter traces, see our distributed tracing UI page. View spans with errors After you click on a span in the trace waterfall view, you can see span errors in the right panel under Error details. The error details are populated by spans containing otel.status_code = ERROR and display the content of otel.status_description. To narrow your search for spans with errors, you can enter otel.status_code = ERROR directly in the distributed tracing filter bar. View span events If you send span events as described by the OpenTelemetry specification, you can view them in the New Relic UI. Span events have two general types: Exceptions Non-exceptions (for example, logs) If you have span events, links for these appear in the right pane: Click on a span in the waterfall view. In the right pane, click View span events, or you can expand Error details and click the link at the bottom of the errors. When you're in span events and only want to view exceptions, slide the toggle Only show exceptions. Tip OpenTelemetry exceptions handled by the app/service are displayed independently of span error status and are not necessarily associated with a span error status. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. For your data to appear in this section, make sure it has the following: UI area Attribute Top database calls span.kind = client or producer db.system Facets by span name Top databases (by query time) span.kind = client or producer db.system Facets by db.system Top databases (by throughput) span.kind = client or producer db.system Facets by db.system Errors On the Errors page, you can see total errors as well as charts showing error count and error rate. For your data to appear in this section, make sure it has the following: span.kind = server or consumer otel.status_code = ERROR Facets by span name Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. For your data to appear in this section, make sure it has the following: span.kind = client or producer db.system is not present JVMs When you drill into a specific JVM, the UI display charts driven by JVM metric data: JVM metrics follow the general semantic conventions for runtime environment metrics. The Java specific runtime metrics are not well documented. The implementation is effectively the documentation and may be subject to change. For your data to appear in this section, make sure it has the following: Requires a unique service.instance.id attribute for rendering the list of JVMs Service.instance.id is an OpenTelemetry resource attribute Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. For your data to appear in this section, make sure it has the following: service.name To correlate with trace data, the logs should contain the trace.id and span.id attributes. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. For your data to appear in this section, make sure it has the following: UI area Description Top Transactions span.kind = server or consumer Facets by span name Throughput span.kind = server or consumer Facets by span name Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.9393,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "Introduction to OpenTelemetry with New Relic",
        "Benefits of OpenTelemetry",
        "Should I use OpenTelemetry instrumentation or New Relic agents?",
        "OpenTelemetry: A work in progress",
        "New Relic's APM agents",
        "How OpenTelemetry works with New Relic",
        "Important",
        "Traces",
        "Metrics",
        "Logs",
        "Next steps"
      ],
      "title": "Introduction to OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "c87898d2d5835c00930c173eabd1bf93040badea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/introduction-opentelemetry-new-relic/",
      "published_at": "2021-10-07T00:14:26Z",
      "updated_at": "2021-09-27T15:16:23Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Are you already familiar with OpenTelemetry and want to begin the setup? Check out our quick start: Quick start If you don't have one already, create a New Relic account. It's free, forever. If you're just getting acquainted with OpenTelemetry, this is what we'll explore here: Benefits of OpenTelemetry Should I use OpenTelemetry or New Relic agents? How OpenTelemetry works with New Relic Benefits of OpenTelemetry OpenTelemetry provides a secure, vendor-neutral specification for service instrumentation so that you can export data to distinct backends of your choice, such as New Relic. OpenTelemetry offers a single set of APIs and libraries that standardize how you collect and transfer telemetry data for your services. The following components make up the OpenTelemetry project: Specifications for the core pillars of observability to drive consistency across all projects. New Relic supports all of these signals: traces, metrics, and logs (see details below) APIs that contain interfaces and implementations based on the specifications SDKs (reference implementations of the APIs) created specifically for languages like Java, Python, Go, Erlang, and more Collectors that offer a vendor-agnostic implementation for processing and exporting Exporters that enable you to send data to a backend of your choice The components of OpenTelemetry work together to create some distinct advantages for capturing telemetry data: Feature Description Ubiquitous instrumentation A single, open standard of instrumentation provides better coverage and flexibility as engineers from all over the world contribute to the instrumentation. Future proof As the instrumentation gets built into libraries and frameworks, and as more vendors move to support this open standard, you can be confident that you won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build integrations into OpenTelemetry or add instrumentation directly to source code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need to decide which instrumentation option to use (a proprietary option or one of the other open standards). Cross-platform compatibility OpenTelemetry supports a variety of languages and backends. It represents a vendor-neutral path for capturing and transmitting telemetry to backends without altering existing instrumentation. Streamlined observability It is easier for vendors to support and test against a single standard as they don’t need to develop their own agents or collectors. High dimensionality OpenTelemetry uses dimensional metrics, so you can filter and facet on more aspects of the data, such as AWS regions, Kubernetes clusters, or service versions. Dimensional metrics also lead to less time between occurrence and reporting. Efficiency OpenTelemetry’s fire-and-forget trace-centric approach to instrumentation often has lower overhead than New Relic agents, especially for asynchronous workloads. It will also result in better handling of trace data for asynchronous requests. Should I use OpenTelemetry instrumentation or New Relic agents? As you consider OpenTelemetry, you may also be looking at New Relic APM agents that also capture telemetry data. As you'd expect, there is a lot of overlap between features available from OpenTelemetry agents and SDKs versus those available from New Relic APM agents. This is especially true if you're interested in distributed tracing telemetry data. The choice you make depends on what you need. We recommend that you explore both New Relic and OpenTelemetry instrumentation or discuss this with us in our CNCF Slack channel to decide what works best for you. OpenTelemetry: A work in progress OpenTelemetry is still an emerging standard, so your choices may be affected by what's available. You can check on the current state of the specification at the OpenTelemetry site. The current state of language-specific OpenTelemetry APIs and SDKs varies: some languages are still pre-alpha and may be missing instructions on how to instrument your service. Most languages have some implementation of traces that is sufficient to start exporting data to New Relic. Check out this table in GitHub that provides an overview of the state of OpenTelemetry specification compliance for each language. For languages that New Relic does not currently provide an agent or SDK, OpenTelemetry may offer you a good alternative. Also, in cases where you want explicit control over sampling of your telemetry data, OpenTelemetry provides a lot of flexibility. As OpenTelemetry matures, New Relic will continue to support new OpenTelemetry data models and to provide a curated UI experience for our Full Stack Observability customers. New Relic's APM agents In general, New Relic's APM agents will collect more telemetry data for your services, and they offer a wide range of configuration options and an extensive set of auto-instrumentation capabilities. New Relic's APM agents offer detailed transaction trace visibility for individual services. They also offer predefined sampling to balance the performance impact of your instrumentation against the need to capture enough data to gain helpful insights. How OpenTelemetry works with New Relic New Relic supports the native OpenTelemetry Protocol (OTLP) for exporting telemetry data. This allows you to use the vendor neutral components developed by the OpenTelemetry community to export your data to New Relic. The following tables show the supported features for each telemetry signal. If you have questions about these or have an unsupported use case, please contact us in our CNCF Slack channel, and watch this page for future updates. Important New Relic's exporters for OpenTelemetry are now deprecated in favor of exporting data to New Relic using OTLP. Traces New Relic offers support for the OTLP ingest of trace signals. The maturity of the upstream specification is stable. OpenTelemetry traces and spans are compatible with New Relic traces and spans. OpenTelemetry spans optionally include attributes (name-value pairs) and resource attributes which map directly to dimensions that can be used to facet or filter span data at query time. OpenTelemetry span metadata (for example, name, kind, and trace_id) also map directly to dimensions on NewRelic spans. At this time, New Relic does not support span links or array attributes. For details, see the Traces section of our best practices guide. Feature Supported Span events ✅ Span linking ❌ Array of primitives (homogeneous) ❌ Metrics New Relic offers support for the OTLP ingest of metric signals. Note that the maturity of the upstream specification is experimental. We intend to follow potentially breaking upstream changes. Here are the OpenTelemetry data types we support and their associated mappings. For details, see the Metrics section of our best practices guide. Metric Type Supported Delta sums ✅ Cumulative sums ✅ Gauges ✅ Delta histograms ✅ Summary ✅ Cumulative histograms ❌ Exemplars ❌ Array of primitives (homogeneous) ❌ Logs New Relic offers support for the OTLP ingest of log signals. Note that the maturity of the upstream specification is experimental. We intend to follow potentially breaking upstream changes. OpenTelemetry logs are compatible with New Relic logs. OpenTelemetry logs optionally include attributes (name-value pairs) and resource attributes which map directly to dimensions that can be used to facet or filter log data at query time. OpenTelemetry log metadata (for example, name, severity_text, and trace_id) also map directly to dimensions on New Relic logs. NewRelic currently supports all OpenTelemetry log message types except for arrays. For more details, see the Logs section of our best practices guide. Feature Supported Description LogRecord body ✅ Supported types: string, boolean, int, double, bytes LogRecord attributes ✅ Supported types: string, boolean, int, double, bytes LogRecord fields ✅ Examples: name, severity_text, trace_id Array messages ❌ Array attributes ❌ Next steps Follow the OpenTelemetry quick start to help you get started. You'll also want to review the best practices guide for getting the most out of the data you export to New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.85797,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "sections": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build <em>integrations</em> into <em>OpenTelemetry</em> or add instrumentation directly to <em>source</em> code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need"
      },
      "id": "603e81ba196a67304da83dab"
    },
    {
      "sections": [
        "Best practices for OpenTelemetry with New Relic",
        "Resources",
        "Batching",
        "Caution",
        "Compression",
        "Traces",
        "Required fields",
        "Sampling",
        "OpenTelemetry built-in samplers",
        "OpenTelemetry tail-based samplers",
        "New Relic tail-based sampling with Infinite Tracing",
        "Important",
        "Metrics",
        "Sum metrics",
        "Delta sums",
        "Cumulative sums",
        "Sum configuration examples",
        "Gauge metrics",
        "Histogram metrics",
        "Summary metrics",
        "Start time",
        "Array values for attributes",
        "Exemplars",
        "How to query metrics",
        "Query cumulative sums stored as gauges",
        "Example: Raw gauge value for cumulative sums",
        "Example: Rate of change with cumulative sums as gauges",
        "Query gauge metrics",
        "Query histogram metrics",
        "Example: Normal distribution",
        "Example: Heat map",
        "Logs",
        "Send logs to New Relic",
        "Application log correlation",
        "View OpenTelemetry logs",
        "The time field"
      ],
      "title": "Best practices for OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "2b624c5862a5a48d088268eccb310510f372b125",
      "image": "https://docs.newrelic.com/static/764474b6404e7a5c65e55d2eb985e93c/c1b63/sum-derivative-function.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-concepts/",
      "published_at": "2021-10-07T01:54:01Z",
      "updated_at": "2021-09-27T14:48:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Here are some best practices based on how OpenTelemetry works with New Relic: Resources Batching Compression Traces Metrics Logs Resources A resource in OpenTelemetry represents information about an entity generating telemetry data. All telemetry data sent to New Relic is expected to be associated with a resource so that it can be linked with the appropriate entity in New Relic. The OpenTelemetry Resource SDK specification defines the functionality implemented by all language SDKs for defining a resource. The following suites of attributes are defined by the OpenTelemetry resource semantic conventions. These attributes are usually set by creating a resource using the OpenTelemetry SDK. service.* attributes service.name attribute is required to associate your resource with an entity in the UI service.instance.id is required for certain panes to light up telemetry.sdk.language=java is required to see data in the JVM section Batching Caution Avoid getting rate limited! You should batch requests sent to the OTLP endpoint as described in this section. By default, the OpenTelemetry SDKs and Collector send one (1) data point per request. Using these defaults, it is likely your account will be rate limited. All OpenTelemetry SDKs and Collectors provide a BatchProcessor, which batches data points in memory. This batching allows requests to be sent with more than one (1) data point. Component Batch Processor Collector Batch Processor Go SDK BatchSpanProcessor JS SDK BatchSpanProcessor Python SDK BatchExportSpanProcessor Compression New Relic supports gzip compression for OTLP payloads exported over gRPC. To maximize the amount of data you can send per request, we recommend enabling compression in all OTLP exporters. If there are other compression formats you'd like to see us support, please let us know in the CNCF Slack channel. Traces Familiarize yourself with these trace topics to ensure your traces and spans appear in New Relic. Required fields The startTimeUnixNano and endTimeUnixNano fields on spans are required according to the OpenTelemetry protocol for trace data. When startTimeUnixNano is not present, the span is dropped and a NrIntegrationError is created. When endTimeUnixNano is not present, the duration of your span is large and negative. The timeUnixNano field on span events is required. When timeUnixNano is not present, the span event is dropped and a NrIntegrationError is created. The traceId and spanId fields on spans are required according to the OpenTelemetry protocol for trace data. When traceId or spanId are not present, the span is dropped and a NrIntegrationError is created. Sampling Trace data is the most mature OpenTelemetry data type. Because of this, New Relic's OpenTelemetry user experience is largely based on trace data and is therefore influenced by your sampling strategy. You can configure sampling in a number of places: Service: Use the OpenTelemetry SDK for your language. Collector: If you're running your own instance of the OpenTelemetry collector, you can configure it to do more sophisticated forms of sampling, such as tail-based sampling (see below). Check out this documentation about how to configure different types of sampling: OpenTelemetry built-in samplers Built-in samplers implemented by the OpenTelemetry SDK for each language. OpenTelemetry tail-based samplers The OpenTelemetry collector has a tail-based sampling processor. We have an example demonstrating the use of the tail-based sampling processor. New Relic tail-based sampling with Infinite Tracing Infinite Tracing is New Relic's tail-based sampling option. You can use this in conjunction with your OpenTelemetry instrumented services. In setting up Infinite Tracing, you need to override the default span endpoint and send telemetry data to the New Relic trace observer: Important Currently, Infinite Tracing does not support OTLP ingest. You must run your own instance of the OpenTelemetry Collector and configure it to use the New Relic exporter. Follow the steps in Set up the trace observer to get the value for YOUR_TRACE_OBSERVER_URL. Use the value of YOUR_TRACE_OBSERVER_URL to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your OpenTelemetry integrations use the AlwaysOn sampler. Metrics OpenTelemetry metrics are largely compatible with New Relic dimensional metrics. We support OpenTelemetry metrics v0.10. All of the supported metric types include an independent set of associated attributes (name-value pairs) which map directly to dimensions you can use to facet or filter metric data at query time. OpenTelemetry metrics are accompanied by a set of resource attributes that identify the originating entity that produced them and map to dimensions for faceting and filtering. The OpenTelemetry data model for metrics defines a number of different metric types: sum, gauge, histogram, and summary. Sum metrics OpenTelemetry sums are a scalar metric that is the sum of all data points over a given time window. Sums have a notion of temporality indicating whether reported values incorporate previous measurements (cumulative temporality) or not (delta temporality). In addition, sums can either be monotonic (only go up or only go down) or non-monotonic (go up and down). Delta sums In New Relic, delta metrics are handled differently depending on whether they are monotonic or non-monotonic: Monotonic delta sums are mapped to the count metric type. Non-monotonic delta sums are mapped to the gauge metric type. Cumulative sums Monotonic and non-monotonic cumulative sums are mapped to the New Relic gauge metric type. Sum configuration examples To understand how to configure aggregation temporality, see these examples using the Java and Go OpenTelemetry SDKs. Gauge metrics OpenTelemetry gauge metric data points represent a sampled value at a given time. These values are converted to the New Relic gauge metric type. OpenTelemetry gauges do not have an aggregation temporality, but the sampled values can be aggregated at query time. Histogram metrics OpenTelemetry histograms compactly represent a population of recorded values along with a total count and sum. Optionally, histograms may include a series of buckets with explicit bounds and a count value for that bucket’s population. OpenTelemetry histograms are converted to New Relic’s distribution metric type, which is backed by a scaled exponential base 2 histogram (see NrSketch for a more thorough explanation). Counts from OpenTelemetry histogram buckets are assigned to New Relic’s distribution metric buckets using linear interpolation. Also, OpenTelemetry has negative and positive infinity bound buckets which we represent in New Relic as zero-width buckets. We do this because we do not have a representation for negative and positive infinity. For example, an OpenTelemetry bucket with bounds [-∞, 10) will be represented by a [10,10) zero width New Relic bucket. You may see exaggerated bucket counts at the endpoints of your distribution due to this translation. Summary metrics OpenTelemetry summary metric data points are used to represent quantile summaries (for example, P99 latency). These map directly to the New Relic summary metric type. Summary metric data points include count, sum, and quantile values, with 0.0 as min and 1.0 as max. OpenTelemetry provides summary metrics for compatibility with other formats. Start time The startTimeUnixNano field is optional according to the OpenTelemetry specification. When this field is provided, it is used for the timestamp on the resulting NewRelic metric, and the duration is calculated as timeUnixNano - startTimeUnixNano. The duration field is used to calculate the queryable endTimeStamp attribute on the New Relic metric, but it serves no other semantic purpose. If startTimeUnixNano is not provided, then timeUnixNano is used for the timestamp field on the resulting NewRelic metric, and the duration field is set to zero. Array values for attributes OpenTelemetry metrics and other signals may include attributes that consist of a homogenous array of primitive types. These attributes are not supported by New Relic. Exemplars OpenTelemetry defines exemplar values that allow other signals, like traces, to be connected to a metric event and provide context. Exemplars are not supported by New Relic. How to query metrics Consider these tips for building metric NRQL queries in New Relic. Query cumulative sums stored as gauges Since cumulative sums are converted to gauges, here are some ways to query your data: Example: Raw gauge value for cumulative sums To view the raw gauge value for cumulative sums, you can use the latest() NRQL function: SELECT latest(totalApiBytesSent) FROM Metric Copy Example: Rate of change with cumulative sums as gauges To see the rate of change over a given time interval for a cumulative sum stored as a gauge, you can use the derivative() NRQL function: SELECT derivative(totalApiBytesSent, 1 minute) FROM Metric Copy New Relic does not currently support either reporting on resets and gaps or accounting for them with cumulative counters. Query gauge metrics When New Relic converts cumulative sums to gauges, you can query them using either the latest() or derivative() NRQL functions. The function you choose depends on whether you want to see the raw value or compute the rate of change. Query histogram metrics New Relic histograms translated from OpenTelemetry metrics have the same query semantics as other New Relic histograms. Namely, the histogram() NRQL function can be used to represent the histogram with a configurable number of buckets and bucket width. Note that you may see larger bucket counts at the endpoint buckets. This is because we are adding negative and positive infinity bound OpenTelemetry buckets into a zero width New Relic bucket. Example: Normal distribution FROM Metric SELECT histogram(test.histogram, buckets: 100, width: 1000) WHERE distributionType = 'Normal Distribution' SINCE 1 day ago Copy Example: Heat map The FACET keyword is also available to create heat map charts. FROM Metric SELECT histogram(test.histogram, buckets: 100, width: 1000) FACET distributionType SINCE 1 day ago Copy Important The TIMESERIES keyword is not supported for New Relic histograms. Logs Logs generated from your applications and environment are an important piece of telemetry. They may represent application logs, machine generated events, or system logs. OpenTelemetry has defined a log data model for representing log data. You can send logs using OpenTelemetry tooling, correlate them with applications, and view them in New Relic. Send logs to New Relic The OpenTelemetry Collector and OpenTelemetry Collector Contrib repositories contain a number of components for consuming log data. The general pattern is to configure the collector to: Receive logs from any of the log receivers. Some of the receiver options include Filelog Receiver, Fluent Forward Receiver, and Syslog Receiver. Process logs, potentially annotating them with resource information. Some of the processor options include Resource Detection Processor and Resource Processor. Export logs to New Relic via the OTLP exporter. Application log correlation Application logs are more useful if they're correlated with other telemetry data produced by the application. The OpenTelemetry semantic convention for services specifies service.name as a required field. All application metric, trace, and log data sent to New Relic with the same service.name are associated with the same entity. The specifics of how logs get annotated with the service.name resource attribute depends on the application's environment: Applications may produce structured JSON logs, which you can configure to include service.name as another field. You can deploy applications alongside a dedicated Collector Agent instance, which you can configure with a Resource Processor to annotate logs with the service.name attribute. Optionally, additional application trace context (sometimes called execution context) can be propagated to log messages. The setup and availability of this depends on the language and logging framework used by the application. The general strategy is to set up the application to write structured JSON logs and to configure it to extract trace context into specified trace context fields on available log messages. The Logs in Context with Log4j2 example in GitHub demonstrates an end-to-end working example for a simple Java application using Log4j2. View OpenTelemetry logs Here are two ways you can view logs: Look in the New Relic Logs UI. If your logs are correlated with an application, view them in the context of the application. The time field The timeUnixNano field is optional according to the OpenTelemetry specification for log data. When timeUnixNano is not present New Relic will use the time that the data was received for the New Relic log timestamp.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.79697,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Best practices for <em>OpenTelemetry</em> with New Relic",
        "sections": "Best practices for <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to analyze all your traces, make sure to verify that your <em>OpenTelemetry</em> <em>integrations</em> use the AlwaysOn sampler. Metrics <em>OpenTelemetry</em> metrics are largely compatible with New Relic dimensional metrics. We support <em>OpenTelemetry</em> metrics v0.10. All of the supported metric types include an independent set"
      },
      "id": "60f6b9b964441f5da847ac01"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/micrometer/micrometer-metrics-registry": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Find your service (entity)",
        "Narrow down your data with filters",
        "Summary page",
        "Distributed tracing",
        "Find traces and their associated spans",
        "Tip",
        "View spans with errors",
        "View span events",
        "Databases",
        "Errors",
        "Externals",
        "JVMs",
        "Logs",
        "Metrics explorer",
        "Transactions",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/490779aac8c10802682d86f0b785b6a2/a4078/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-10-07T01:52:56Z",
      "updated_at": "2021-09-27T15:53:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information about your services. It offers you a way to filter services and then filter the data to show a variety of views into your data. Find your service (entity) To get started in Explorer, you need to find your service: Go to one.newrelic.com. In the left sidebar, click Services - OpenTelemetry: In the center pane, click the service you want to know more about, or find your service by entering the name in the top filter bar. Narrow down your data with filters Once you have your entity, you can then filter for data from your service. With the filter bar Narrow data to..., you can highlight a specific facet of the telemetry recorded for your service. For example, you may want to see the error rate for a particular version of the service that you've deployed in a canary instance, so you add a filter for service.version='1.2.3'. Filters are preserved when navigating between different views of your data for a service. For example, the filter for service.version='1.2.3' carries over to the Transactions view, so that you would see telemetry on requests to the endpoints (transactions) that are running version 1.2.3 of your service, and not any other versions. Filters are preserved when navigating between the Summary, Transactions, Databases, Externals, Errors, and JVMs views. Filters are also preserved when navigating to the Distributed tracing view, but with limitations. Only filter conditions that use the equals operator (\"=\") are currently supported when navigating to Distributed tracing. If you navigate back from the Distributed tracing page, the filters you selected on the previous view will come back. Once you filter your data, the UI has various views in the left-navigation pane. See our descriptions below for view details. Summary page After you click on a service in the Explorer tab, you see the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. For your data to appear in this section, make sure it has the following: UI area Attribute Response time span.kind = server or consumer Throughput span.kind = server or consumer Error rate span.kind = server or consumer otel.status_code = ERROR Service instances pane service.instance.id (usually set via the OTel resource API) Distributed tracing In Distributed tracing, you can locate traces and examine span details. For your trace data to appear in the New Relic UI, it needs to conform to the OpenTelemetry trace semantic conventions. Also, in the Traces section of our best practices guide, you can find some tips about making sure your traces and spans appear in New Relic. The following attributes are typically not added explicitly to spans. Rather, they are usually set when creating a span or performing operations (for example, recording an error) on a span using the OpenTelemetry SDK. Attribute Description name A span’s name is generally set when starting a span. The name can generally be anything, but the OpenTelemetry specification provides guidance for certain types of spans like Database spans or HTTP spans. span.kind A span’s kind is generally set when starting a span. New Relic uses span.kind to infer that a span is an entry point to a given service. When span.kind = server or consumer it is considered an entry point. When span.kind = client or producer, it is considered a call to an external service or database system. otel.status_code A span’s status is set using the span API. The otel.status_code attribute is how it is manifested by New Relic. The UI primarily uses otel.status_code for the purpose of identifying errors. Find traces and their associated spans Here are some ways to target your searches: To find the traces you want, you can run queries like these in the filter bar: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID When you find an interesting trace, click on it to display a waterfall diagram showing the spans of the trace. Click on specific spans within the trace to display span details in the right panel. Tip For more ways to filter traces, see our distributed tracing UI page. View spans with errors After you click on a span in the trace waterfall view, you can see span errors in the right panel under Error details. The error details are populated by spans containing otel.status_code = ERROR and display the content of otel.status_description. To narrow your search for spans with errors, you can enter otel.status_code = ERROR directly in the distributed tracing filter bar. View span events If you send span events as described by the OpenTelemetry specification, you can view them in the New Relic UI. Span events have two general types: Exceptions Non-exceptions (for example, logs) If you have span events, links for these appear in the right pane: Click on a span in the waterfall view. In the right pane, click View span events, or you can expand Error details and click the link at the bottom of the errors. When you're in span events and only want to view exceptions, slide the toggle Only show exceptions. Tip OpenTelemetry exceptions handled by the app/service are displayed independently of span error status and are not necessarily associated with a span error status. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. For your data to appear in this section, make sure it has the following: UI area Attribute Top database calls span.kind = client or producer db.system Facets by span name Top databases (by query time) span.kind = client or producer db.system Facets by db.system Top databases (by throughput) span.kind = client or producer db.system Facets by db.system Errors On the Errors page, you can see total errors as well as charts showing error count and error rate. For your data to appear in this section, make sure it has the following: span.kind = server or consumer otel.status_code = ERROR Facets by span name Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. For your data to appear in this section, make sure it has the following: span.kind = client or producer db.system is not present JVMs When you drill into a specific JVM, the UI display charts driven by JVM metric data: JVM metrics follow the general semantic conventions for runtime environment metrics. The Java specific runtime metrics are not well documented. The implementation is effectively the documentation and may be subject to change. For your data to appear in this section, make sure it has the following: Requires a unique service.instance.id attribute for rendering the list of JVMs Service.instance.id is an OpenTelemetry resource attribute Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. For your data to appear in this section, make sure it has the following: service.name To correlate with trace data, the logs should contain the trace.id and span.id attributes. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. For your data to appear in this section, make sure it has the following: UI area Description Top Transactions span.kind = server or consumer Facets by span name Throughput span.kind = server or consumer Facets by span name Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 195.09009,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "Introduction to OpenTelemetry with New Relic",
        "Benefits of OpenTelemetry",
        "Should I use OpenTelemetry instrumentation or New Relic agents?",
        "OpenTelemetry: A work in progress",
        "New Relic's APM agents",
        "How OpenTelemetry works with New Relic",
        "Important",
        "Traces",
        "Metrics",
        "Logs",
        "Next steps"
      ],
      "title": "Introduction to OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "c87898d2d5835c00930c173eabd1bf93040badea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/introduction-opentelemetry-new-relic/",
      "published_at": "2021-10-07T00:14:26Z",
      "updated_at": "2021-09-27T15:16:23Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Are you already familiar with OpenTelemetry and want to begin the setup? Check out our quick start: Quick start If you don't have one already, create a New Relic account. It's free, forever. If you're just getting acquainted with OpenTelemetry, this is what we'll explore here: Benefits of OpenTelemetry Should I use OpenTelemetry or New Relic agents? How OpenTelemetry works with New Relic Benefits of OpenTelemetry OpenTelemetry provides a secure, vendor-neutral specification for service instrumentation so that you can export data to distinct backends of your choice, such as New Relic. OpenTelemetry offers a single set of APIs and libraries that standardize how you collect and transfer telemetry data for your services. The following components make up the OpenTelemetry project: Specifications for the core pillars of observability to drive consistency across all projects. New Relic supports all of these signals: traces, metrics, and logs (see details below) APIs that contain interfaces and implementations based on the specifications SDKs (reference implementations of the APIs) created specifically for languages like Java, Python, Go, Erlang, and more Collectors that offer a vendor-agnostic implementation for processing and exporting Exporters that enable you to send data to a backend of your choice The components of OpenTelemetry work together to create some distinct advantages for capturing telemetry data: Feature Description Ubiquitous instrumentation A single, open standard of instrumentation provides better coverage and flexibility as engineers from all over the world contribute to the instrumentation. Future proof As the instrumentation gets built into libraries and frameworks, and as more vendors move to support this open standard, you can be confident that you won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build integrations into OpenTelemetry or add instrumentation directly to source code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need to decide which instrumentation option to use (a proprietary option or one of the other open standards). Cross-platform compatibility OpenTelemetry supports a variety of languages and backends. It represents a vendor-neutral path for capturing and transmitting telemetry to backends without altering existing instrumentation. Streamlined observability It is easier for vendors to support and test against a single standard as they don’t need to develop their own agents or collectors. High dimensionality OpenTelemetry uses dimensional metrics, so you can filter and facet on more aspects of the data, such as AWS regions, Kubernetes clusters, or service versions. Dimensional metrics also lead to less time between occurrence and reporting. Efficiency OpenTelemetry’s fire-and-forget trace-centric approach to instrumentation often has lower overhead than New Relic agents, especially for asynchronous workloads. It will also result in better handling of trace data for asynchronous requests. Should I use OpenTelemetry instrumentation or New Relic agents? As you consider OpenTelemetry, you may also be looking at New Relic APM agents that also capture telemetry data. As you'd expect, there is a lot of overlap between features available from OpenTelemetry agents and SDKs versus those available from New Relic APM agents. This is especially true if you're interested in distributed tracing telemetry data. The choice you make depends on what you need. We recommend that you explore both New Relic and OpenTelemetry instrumentation or discuss this with us in our CNCF Slack channel to decide what works best for you. OpenTelemetry: A work in progress OpenTelemetry is still an emerging standard, so your choices may be affected by what's available. You can check on the current state of the specification at the OpenTelemetry site. The current state of language-specific OpenTelemetry APIs and SDKs varies: some languages are still pre-alpha and may be missing instructions on how to instrument your service. Most languages have some implementation of traces that is sufficient to start exporting data to New Relic. Check out this table in GitHub that provides an overview of the state of OpenTelemetry specification compliance for each language. For languages that New Relic does not currently provide an agent or SDK, OpenTelemetry may offer you a good alternative. Also, in cases where you want explicit control over sampling of your telemetry data, OpenTelemetry provides a lot of flexibility. As OpenTelemetry matures, New Relic will continue to support new OpenTelemetry data models and to provide a curated UI experience for our Full Stack Observability customers. New Relic's APM agents In general, New Relic's APM agents will collect more telemetry data for your services, and they offer a wide range of configuration options and an extensive set of auto-instrumentation capabilities. New Relic's APM agents offer detailed transaction trace visibility for individual services. They also offer predefined sampling to balance the performance impact of your instrumentation against the need to capture enough data to gain helpful insights. How OpenTelemetry works with New Relic New Relic supports the native OpenTelemetry Protocol (OTLP) for exporting telemetry data. This allows you to use the vendor neutral components developed by the OpenTelemetry community to export your data to New Relic. The following tables show the supported features for each telemetry signal. If you have questions about these or have an unsupported use case, please contact us in our CNCF Slack channel, and watch this page for future updates. Important New Relic's exporters for OpenTelemetry are now deprecated in favor of exporting data to New Relic using OTLP. Traces New Relic offers support for the OTLP ingest of trace signals. The maturity of the upstream specification is stable. OpenTelemetry traces and spans are compatible with New Relic traces and spans. OpenTelemetry spans optionally include attributes (name-value pairs) and resource attributes which map directly to dimensions that can be used to facet or filter span data at query time. OpenTelemetry span metadata (for example, name, kind, and trace_id) also map directly to dimensions on NewRelic spans. At this time, New Relic does not support span links or array attributes. For details, see the Traces section of our best practices guide. Feature Supported Span events ✅ Span linking ❌ Array of primitives (homogeneous) ❌ Metrics New Relic offers support for the OTLP ingest of metric signals. Note that the maturity of the upstream specification is experimental. We intend to follow potentially breaking upstream changes. Here are the OpenTelemetry data types we support and their associated mappings. For details, see the Metrics section of our best practices guide. Metric Type Supported Delta sums ✅ Cumulative sums ✅ Gauges ✅ Delta histograms ✅ Summary ✅ Cumulative histograms ❌ Exemplars ❌ Array of primitives (homogeneous) ❌ Logs New Relic offers support for the OTLP ingest of log signals. Note that the maturity of the upstream specification is experimental. We intend to follow potentially breaking upstream changes. OpenTelemetry logs are compatible with New Relic logs. OpenTelemetry logs optionally include attributes (name-value pairs) and resource attributes which map directly to dimensions that can be used to facet or filter log data at query time. OpenTelemetry log metadata (for example, name, severity_text, and trace_id) also map directly to dimensions on New Relic logs. NewRelic currently supports all OpenTelemetry log message types except for arrays. For more details, see the Logs section of our best practices guide. Feature Supported Description LogRecord body ✅ Supported types: string, boolean, int, double, bytes LogRecord attributes ✅ Supported types: string, boolean, int, double, bytes LogRecord fields ✅ Examples: name, severity_text, trace_id Array messages ❌ Array attributes ❌ Next steps Follow the OpenTelemetry quick start to help you get started. You'll also want to review the best practices guide for getting the most out of the data you export to New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 195.0061,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "sections": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build <em>integrations</em> into <em>OpenTelemetry</em> or add instrumentation directly to <em>source</em> code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need"
      },
      "id": "603e81ba196a67304da83dab"
    },
    {
      "sections": [
        "Best practices for OpenTelemetry with New Relic",
        "Resources",
        "Batching",
        "Caution",
        "Compression",
        "Traces",
        "Required fields",
        "Sampling",
        "OpenTelemetry built-in samplers",
        "OpenTelemetry tail-based samplers",
        "New Relic tail-based sampling with Infinite Tracing",
        "Important",
        "Metrics",
        "Sum metrics",
        "Delta sums",
        "Cumulative sums",
        "Sum configuration examples",
        "Gauge metrics",
        "Histogram metrics",
        "Summary metrics",
        "Start time",
        "Array values for attributes",
        "Exemplars",
        "How to query metrics",
        "Query cumulative sums stored as gauges",
        "Example: Raw gauge value for cumulative sums",
        "Example: Rate of change with cumulative sums as gauges",
        "Query gauge metrics",
        "Query histogram metrics",
        "Example: Normal distribution",
        "Example: Heat map",
        "Logs",
        "Send logs to New Relic",
        "Application log correlation",
        "View OpenTelemetry logs",
        "The time field"
      ],
      "title": "Best practices for OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "2b624c5862a5a48d088268eccb310510f372b125",
      "image": "https://docs.newrelic.com/static/764474b6404e7a5c65e55d2eb985e93c/c1b63/sum-derivative-function.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-concepts/",
      "published_at": "2021-10-07T01:54:01Z",
      "updated_at": "2021-09-27T14:48:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Here are some best practices based on how OpenTelemetry works with New Relic: Resources Batching Compression Traces Metrics Logs Resources A resource in OpenTelemetry represents information about an entity generating telemetry data. All telemetry data sent to New Relic is expected to be associated with a resource so that it can be linked with the appropriate entity in New Relic. The OpenTelemetry Resource SDK specification defines the functionality implemented by all language SDKs for defining a resource. The following suites of attributes are defined by the OpenTelemetry resource semantic conventions. These attributes are usually set by creating a resource using the OpenTelemetry SDK. service.* attributes service.name attribute is required to associate your resource with an entity in the UI service.instance.id is required for certain panes to light up telemetry.sdk.language=java is required to see data in the JVM section Batching Caution Avoid getting rate limited! You should batch requests sent to the OTLP endpoint as described in this section. By default, the OpenTelemetry SDKs and Collector send one (1) data point per request. Using these defaults, it is likely your account will be rate limited. All OpenTelemetry SDKs and Collectors provide a BatchProcessor, which batches data points in memory. This batching allows requests to be sent with more than one (1) data point. Component Batch Processor Collector Batch Processor Go SDK BatchSpanProcessor JS SDK BatchSpanProcessor Python SDK BatchExportSpanProcessor Compression New Relic supports gzip compression for OTLP payloads exported over gRPC. To maximize the amount of data you can send per request, we recommend enabling compression in all OTLP exporters. If there are other compression formats you'd like to see us support, please let us know in the CNCF Slack channel. Traces Familiarize yourself with these trace topics to ensure your traces and spans appear in New Relic. Required fields The startTimeUnixNano and endTimeUnixNano fields on spans are required according to the OpenTelemetry protocol for trace data. When startTimeUnixNano is not present, the span is dropped and a NrIntegrationError is created. When endTimeUnixNano is not present, the duration of your span is large and negative. The timeUnixNano field on span events is required. When timeUnixNano is not present, the span event is dropped and a NrIntegrationError is created. The traceId and spanId fields on spans are required according to the OpenTelemetry protocol for trace data. When traceId or spanId are not present, the span is dropped and a NrIntegrationError is created. Sampling Trace data is the most mature OpenTelemetry data type. Because of this, New Relic's OpenTelemetry user experience is largely based on trace data and is therefore influenced by your sampling strategy. You can configure sampling in a number of places: Service: Use the OpenTelemetry SDK for your language. Collector: If you're running your own instance of the OpenTelemetry collector, you can configure it to do more sophisticated forms of sampling, such as tail-based sampling (see below). Check out this documentation about how to configure different types of sampling: OpenTelemetry built-in samplers Built-in samplers implemented by the OpenTelemetry SDK for each language. OpenTelemetry tail-based samplers The OpenTelemetry collector has a tail-based sampling processor. We have an example demonstrating the use of the tail-based sampling processor. New Relic tail-based sampling with Infinite Tracing Infinite Tracing is New Relic's tail-based sampling option. You can use this in conjunction with your OpenTelemetry instrumented services. In setting up Infinite Tracing, you need to override the default span endpoint and send telemetry data to the New Relic trace observer: Important Currently, Infinite Tracing does not support OTLP ingest. You must run your own instance of the OpenTelemetry Collector and configure it to use the New Relic exporter. Follow the steps in Set up the trace observer to get the value for YOUR_TRACE_OBSERVER_URL. Use the value of YOUR_TRACE_OBSERVER_URL to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your OpenTelemetry integrations use the AlwaysOn sampler. Metrics OpenTelemetry metrics are largely compatible with New Relic dimensional metrics. We support OpenTelemetry metrics v0.10. All of the supported metric types include an independent set of associated attributes (name-value pairs) which map directly to dimensions you can use to facet or filter metric data at query time. OpenTelemetry metrics are accompanied by a set of resource attributes that identify the originating entity that produced them and map to dimensions for faceting and filtering. The OpenTelemetry data model for metrics defines a number of different metric types: sum, gauge, histogram, and summary. Sum metrics OpenTelemetry sums are a scalar metric that is the sum of all data points over a given time window. Sums have a notion of temporality indicating whether reported values incorporate previous measurements (cumulative temporality) or not (delta temporality). In addition, sums can either be monotonic (only go up or only go down) or non-monotonic (go up and down). Delta sums In New Relic, delta metrics are handled differently depending on whether they are monotonic or non-monotonic: Monotonic delta sums are mapped to the count metric type. Non-monotonic delta sums are mapped to the gauge metric type. Cumulative sums Monotonic and non-monotonic cumulative sums are mapped to the New Relic gauge metric type. Sum configuration examples To understand how to configure aggregation temporality, see these examples using the Java and Go OpenTelemetry SDKs. Gauge metrics OpenTelemetry gauge metric data points represent a sampled value at a given time. These values are converted to the New Relic gauge metric type. OpenTelemetry gauges do not have an aggregation temporality, but the sampled values can be aggregated at query time. Histogram metrics OpenTelemetry histograms compactly represent a population of recorded values along with a total count and sum. Optionally, histograms may include a series of buckets with explicit bounds and a count value for that bucket’s population. OpenTelemetry histograms are converted to New Relic’s distribution metric type, which is backed by a scaled exponential base 2 histogram (see NrSketch for a more thorough explanation). Counts from OpenTelemetry histogram buckets are assigned to New Relic’s distribution metric buckets using linear interpolation. Also, OpenTelemetry has negative and positive infinity bound buckets which we represent in New Relic as zero-width buckets. We do this because we do not have a representation for negative and positive infinity. For example, an OpenTelemetry bucket with bounds [-∞, 10) will be represented by a [10,10) zero width New Relic bucket. You may see exaggerated bucket counts at the endpoints of your distribution due to this translation. Summary metrics OpenTelemetry summary metric data points are used to represent quantile summaries (for example, P99 latency). These map directly to the New Relic summary metric type. Summary metric data points include count, sum, and quantile values, with 0.0 as min and 1.0 as max. OpenTelemetry provides summary metrics for compatibility with other formats. Start time The startTimeUnixNano field is optional according to the OpenTelemetry specification. When this field is provided, it is used for the timestamp on the resulting NewRelic metric, and the duration is calculated as timeUnixNano - startTimeUnixNano. The duration field is used to calculate the queryable endTimeStamp attribute on the New Relic metric, but it serves no other semantic purpose. If startTimeUnixNano is not provided, then timeUnixNano is used for the timestamp field on the resulting NewRelic metric, and the duration field is set to zero. Array values for attributes OpenTelemetry metrics and other signals may include attributes that consist of a homogenous array of primitive types. These attributes are not supported by New Relic. Exemplars OpenTelemetry defines exemplar values that allow other signals, like traces, to be connected to a metric event and provide context. Exemplars are not supported by New Relic. How to query metrics Consider these tips for building metric NRQL queries in New Relic. Query cumulative sums stored as gauges Since cumulative sums are converted to gauges, here are some ways to query your data: Example: Raw gauge value for cumulative sums To view the raw gauge value for cumulative sums, you can use the latest() NRQL function: SELECT latest(totalApiBytesSent) FROM Metric Copy Example: Rate of change with cumulative sums as gauges To see the rate of change over a given time interval for a cumulative sum stored as a gauge, you can use the derivative() NRQL function: SELECT derivative(totalApiBytesSent, 1 minute) FROM Metric Copy New Relic does not currently support either reporting on resets and gaps or accounting for them with cumulative counters. Query gauge metrics When New Relic converts cumulative sums to gauges, you can query them using either the latest() or derivative() NRQL functions. The function you choose depends on whether you want to see the raw value or compute the rate of change. Query histogram metrics New Relic histograms translated from OpenTelemetry metrics have the same query semantics as other New Relic histograms. Namely, the histogram() NRQL function can be used to represent the histogram with a configurable number of buckets and bucket width. Note that you may see larger bucket counts at the endpoint buckets. This is because we are adding negative and positive infinity bound OpenTelemetry buckets into a zero width New Relic bucket. Example: Normal distribution FROM Metric SELECT histogram(test.histogram, buckets: 100, width: 1000) WHERE distributionType = 'Normal Distribution' SINCE 1 day ago Copy Example: Heat map The FACET keyword is also available to create heat map charts. FROM Metric SELECT histogram(test.histogram, buckets: 100, width: 1000) FACET distributionType SINCE 1 day ago Copy Important The TIMESERIES keyword is not supported for New Relic histograms. Logs Logs generated from your applications and environment are an important piece of telemetry. They may represent application logs, machine generated events, or system logs. OpenTelemetry has defined a log data model for representing log data. You can send logs using OpenTelemetry tooling, correlate them with applications, and view them in New Relic. Send logs to New Relic The OpenTelemetry Collector and OpenTelemetry Collector Contrib repositories contain a number of components for consuming log data. The general pattern is to configure the collector to: Receive logs from any of the log receivers. Some of the receiver options include Filelog Receiver, Fluent Forward Receiver, and Syslog Receiver. Process logs, potentially annotating them with resource information. Some of the processor options include Resource Detection Processor and Resource Processor. Export logs to New Relic via the OTLP exporter. Application log correlation Application logs are more useful if they're correlated with other telemetry data produced by the application. The OpenTelemetry semantic convention for services specifies service.name as a required field. All application metric, trace, and log data sent to New Relic with the same service.name are associated with the same entity. The specifics of how logs get annotated with the service.name resource attribute depends on the application's environment: Applications may produce structured JSON logs, which you can configure to include service.name as another field. You can deploy applications alongside a dedicated Collector Agent instance, which you can configure with a Resource Processor to annotate logs with the service.name attribute. Optionally, additional application trace context (sometimes called execution context) can be propagated to log messages. The setup and availability of this depends on the language and logging framework used by the application. The general strategy is to set up the application to write structured JSON logs and to configure it to extract trace context into specified trace context fields on available log messages. The Logs in Context with Log4j2 example in GitHub demonstrates an end-to-end working example for a simple Java application using Log4j2. View OpenTelemetry logs Here are two ways you can view logs: Look in the New Relic Logs UI. If your logs are correlated with an application, view them in the context of the application. The time field The timeUnixNano field is optional according to the OpenTelemetry specification for log data. When timeUnixNano is not present New Relic will use the time that the data was received for the New Relic log timestamp.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.94312,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Best practices for <em>OpenTelemetry</em> with New Relic",
        "sections": "Best practices for <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to analyze all your traces, make sure to verify that your <em>OpenTelemetry</em> <em>integrations</em> use the AlwaysOn sampler. Metrics <em>OpenTelemetry</em> metrics are largely compatible with New Relic dimensional metrics. We support <em>OpenTelemetry</em> metrics v0.10. All of the supported metric types include an independent set"
      },
      "id": "60f6b9b964441f5da847ac01"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/opencensus/opencensus-exporter": [
    {
      "sections": [
        "Introduction to New Relic's open source telemetry integrations",
        "Types of integrations",
        "How they work"
      ],
      "title": "Introduction to New Relic's open source telemetry integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "Get started"
      ],
      "external_id": "239889ec292525fcfd6b417d243943ea7b3e0529",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/get-started/introduction-new-relics-open-source-telemetry-integrations/",
      "published_at": "2021-10-08T06:39:21Z",
      "updated_at": "2021-07-27T16:01:31Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic provides open source integrations that report telemetry data from telemetry tools to your New Relic account. Types of integrations We have open source integrations that report data from OpenCensus, OpenTelemetry, DropWizard, Prometheus, and more. With these solutions, you can aggregate all your telemetry data in one place: the New Relic platform. See our list of open source telemetry integrations (to browse all New Relic solutions, see our integrations page). How they work These integrations were built using our Telemetry SDKs, which are open-source language-specific libraries for reporting metrics, trace data, and other telemetry data to New Relic. If our pre-built integrations don't meet your needs, you can use the Telemetry SDKs to build your own telemetry tools. Under the hood, data reported by these solutions are ingested via our data ingest APIs. For example, metrics reported by the DropWizard exporter are ingested via the Metric API, so to understand how to query and chart that type of data, you could read Query metric data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 295.973,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to New Relic&#x27;s <em>open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "sections": "Introduction to New Relic&#x27;s <em>open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "New Relic provides <em>open</em> <em>source</em> <em>integrations</em> that report <em>telemetry</em> data from <em>telemetry</em> tools to your New Relic account. Types of <em>integrations</em> We have <em>open</em> <em>source</em> <em>integrations</em> that report data from <em>OpenCensus</em>, <em>OpenTelemetry</em>, DropWizard, Prometheus, and more. With these solutions, you can aggregate"
      },
      "id": "603e95ab28ccbc036aeba789"
    },
    {
      "sections": [
        "Telemetry SDKs: Report custom telemetry data",
        "Requirements and compatibility",
        "Tip",
        "Available libraries",
        "Write your own Telemetry SDK or contribute to an existing one",
        "Integrations built with the Telemetry SDKs"
      ],
      "title": "Telemetry SDKs: Report custom telemetry data",
      "type": "docs",
      "tags": [
        "Telemetry Data Platform",
        "Ingest and manage data",
        "Ingest APIs"
      ],
      "external_id": "759fd7fa58ab2e074d0ba50b30be8c1096698304",
      "image": "",
      "url": "https://docs.newrelic.com/docs/telemetry-data-platform/ingest-apis/telemetry-sdks-report-custom-telemetry-data/",
      "published_at": "2021-10-06T23:47:51Z",
      "updated_at": "2021-08-26T14:51:45Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Telemetry SDKs are an open source set of API client libraries that send data to the New Relic platform. Under the hood, these SDKs rely on our core data ingest APIs: the Metric API, Trace API, Log API, and Event API. We offer open-source integrations for telemetry tools like Prometheus, Istio, and OpenCensus that were created using our Telemetry SDKs. If those solutions (or our other integrations) don't meet your needs, you can use the Telemetry SDKs to create your own telemetry data solutions. Requirements and compatibility The Telemetry SDKs use our Metric API, Event API, Log API, and Trace API, which all require a license key, so you'll need a license key for the account you wish to send data to. Tip New Relic has contributed the Telemetry SDK to the open source community under an Apache 2.0 license. Available libraries The Telemetry SDKs are open source software on GitHub. Use the language-specific GitHub links below to get library details, coding examples, and procedures for how to use the SDKs. We currently support the following libraries, with more to be created in the future: Language Library Supported New Relic data types Java Java library on GitHub Metrics Events Logs Traces Node/TypeScript NodeJS library on GitHub Metrics Traces Python Python library on GitHub Metrics Events Logs Traces Go Go library on Github Metrics Traces .NET .NET library on GitHub .NET package in NuGet Metrics Traces C C library on Github Traces Rust Rust library on Github Traces Ruby Ruby library on Github Gem on Rubygems Traces For more on the supported data types, see: An overview of New Relic data types Metrics: see the Metric API Logs: see the Log API Traces: see the Trace API Events: see the Event API Write your own Telemetry SDK or contribute to an existing one If you need a Telemetry SDK in a language that does not currently exist or want to contribute to an existing library, please see the Telemetry SDK specifications. Integrations built with the Telemetry SDKs To see the integrations built using our Telemetry SDKs, see Open source telemetry integrations. For all monitoring solutions, see our integrations page.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 196.62915,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Telemetry</em> SDKs: Report custom <em>telemetry</em> data",
        "sections": "<em>Integrations</em> built with the <em>Telemetry</em> SDKs",
        "tags": "<em>Telemetry</em> Data Platform",
        "body": ", and <em>OpenCensus</em> that were created using our <em>Telemetry</em> SDKs. If those solutions (or our other <em>integrations</em>) don&#x27;t meet your needs, you can use the <em>Telemetry</em> SDKs to create your own <em>telemetry</em> data solutions. Requirements and compatibility The <em>Telemetry</em> SDKs use our Metric API, Event API, Log API"
      },
      "id": "603ea196196a670192a83d83"
    },
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Find your service (entity)",
        "Narrow down your data with filters",
        "Summary page",
        "Distributed tracing",
        "Find traces and their associated spans",
        "Tip",
        "View spans with errors",
        "View span events",
        "Databases",
        "Errors",
        "Externals",
        "JVMs",
        "Logs",
        "Metrics explorer",
        "Transactions",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/490779aac8c10802682d86f0b785b6a2/a4078/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-10-07T01:52:56Z",
      "updated_at": "2021-09-27T15:53:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information about your services. It offers you a way to filter services and then filter the data to show a variety of views into your data. Find your service (entity) To get started in Explorer, you need to find your service: Go to one.newrelic.com. In the left sidebar, click Services - OpenTelemetry: In the center pane, click the service you want to know more about, or find your service by entering the name in the top filter bar. Narrow down your data with filters Once you have your entity, you can then filter for data from your service. With the filter bar Narrow data to..., you can highlight a specific facet of the telemetry recorded for your service. For example, you may want to see the error rate for a particular version of the service that you've deployed in a canary instance, so you add a filter for service.version='1.2.3'. Filters are preserved when navigating between different views of your data for a service. For example, the filter for service.version='1.2.3' carries over to the Transactions view, so that you would see telemetry on requests to the endpoints (transactions) that are running version 1.2.3 of your service, and not any other versions. Filters are preserved when navigating between the Summary, Transactions, Databases, Externals, Errors, and JVMs views. Filters are also preserved when navigating to the Distributed tracing view, but with limitations. Only filter conditions that use the equals operator (\"=\") are currently supported when navigating to Distributed tracing. If you navigate back from the Distributed tracing page, the filters you selected on the previous view will come back. Once you filter your data, the UI has various views in the left-navigation pane. See our descriptions below for view details. Summary page After you click on a service in the Explorer tab, you see the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. For your data to appear in this section, make sure it has the following: UI area Attribute Response time span.kind = server or consumer Throughput span.kind = server or consumer Error rate span.kind = server or consumer otel.status_code = ERROR Service instances pane service.instance.id (usually set via the OTel resource API) Distributed tracing In Distributed tracing, you can locate traces and examine span details. For your trace data to appear in the New Relic UI, it needs to conform to the OpenTelemetry trace semantic conventions. Also, in the Traces section of our best practices guide, you can find some tips about making sure your traces and spans appear in New Relic. The following attributes are typically not added explicitly to spans. Rather, they are usually set when creating a span or performing operations (for example, recording an error) on a span using the OpenTelemetry SDK. Attribute Description name A span’s name is generally set when starting a span. The name can generally be anything, but the OpenTelemetry specification provides guidance for certain types of spans like Database spans or HTTP spans. span.kind A span’s kind is generally set when starting a span. New Relic uses span.kind to infer that a span is an entry point to a given service. When span.kind = server or consumer it is considered an entry point. When span.kind = client or producer, it is considered a call to an external service or database system. otel.status_code A span’s status is set using the span API. The otel.status_code attribute is how it is manifested by New Relic. The UI primarily uses otel.status_code for the purpose of identifying errors. Find traces and their associated spans Here are some ways to target your searches: To find the traces you want, you can run queries like these in the filter bar: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID When you find an interesting trace, click on it to display a waterfall diagram showing the spans of the trace. Click on specific spans within the trace to display span details in the right panel. Tip For more ways to filter traces, see our distributed tracing UI page. View spans with errors After you click on a span in the trace waterfall view, you can see span errors in the right panel under Error details. The error details are populated by spans containing otel.status_code = ERROR and display the content of otel.status_description. To narrow your search for spans with errors, you can enter otel.status_code = ERROR directly in the distributed tracing filter bar. View span events If you send span events as described by the OpenTelemetry specification, you can view them in the New Relic UI. Span events have two general types: Exceptions Non-exceptions (for example, logs) If you have span events, links for these appear in the right pane: Click on a span in the waterfall view. In the right pane, click View span events, or you can expand Error details and click the link at the bottom of the errors. When you're in span events and only want to view exceptions, slide the toggle Only show exceptions. Tip OpenTelemetry exceptions handled by the app/service are displayed independently of span error status and are not necessarily associated with a span error status. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. For your data to appear in this section, make sure it has the following: UI area Attribute Top database calls span.kind = client or producer db.system Facets by span name Top databases (by query time) span.kind = client or producer db.system Facets by db.system Top databases (by throughput) span.kind = client or producer db.system Facets by db.system Errors On the Errors page, you can see total errors as well as charts showing error count and error rate. For your data to appear in this section, make sure it has the following: span.kind = server or consumer otel.status_code = ERROR Facets by span name Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. For your data to appear in this section, make sure it has the following: span.kind = client or producer db.system is not present JVMs When you drill into a specific JVM, the UI display charts driven by JVM metric data: JVM metrics follow the general semantic conventions for runtime environment metrics. The Java specific runtime metrics are not well documented. The implementation is effectively the documentation and may be subject to change. For your data to appear in this section, make sure it has the following: Requires a unique service.instance.id attribute for rendering the list of JVMs Service.instance.id is an OpenTelemetry resource attribute Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. For your data to appear in this section, make sure it has the following: service.name To correlate with trace data, the logs should contain the trace.id and span.id attributes. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. For your data to appear in this section, make sure it has the following: UI area Description Top Transactions span.kind = server or consumer Facets by span name Throughput span.kind = server or consumer Facets by span name Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.93927,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/opentelemetry/introduction-opentelemetry-new-relic": [
    {
      "sections": [
        "Best practices for OpenTelemetry with New Relic",
        "Resources",
        "Batching",
        "Caution",
        "Compression",
        "Traces",
        "Required fields",
        "Sampling",
        "OpenTelemetry built-in samplers",
        "OpenTelemetry tail-based samplers",
        "New Relic tail-based sampling with Infinite Tracing",
        "Important",
        "Metrics",
        "Sum metrics",
        "Delta sums",
        "Cumulative sums",
        "Sum configuration examples",
        "Gauge metrics",
        "Histogram metrics",
        "Summary metrics",
        "Start time",
        "Array values for attributes",
        "Exemplars",
        "How to query metrics",
        "Query cumulative sums stored as gauges",
        "Example: Raw gauge value for cumulative sums",
        "Example: Rate of change with cumulative sums as gauges",
        "Query gauge metrics",
        "Query histogram metrics",
        "Example: Normal distribution",
        "Example: Heat map",
        "Logs",
        "Send logs to New Relic",
        "Application log correlation",
        "View OpenTelemetry logs",
        "The time field"
      ],
      "title": "Best practices for OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "2b624c5862a5a48d088268eccb310510f372b125",
      "image": "https://docs.newrelic.com/static/764474b6404e7a5c65e55d2eb985e93c/c1b63/sum-derivative-function.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-concepts/",
      "published_at": "2021-10-07T01:54:01Z",
      "updated_at": "2021-09-27T14:48:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Here are some best practices based on how OpenTelemetry works with New Relic: Resources Batching Compression Traces Metrics Logs Resources A resource in OpenTelemetry represents information about an entity generating telemetry data. All telemetry data sent to New Relic is expected to be associated with a resource so that it can be linked with the appropriate entity in New Relic. The OpenTelemetry Resource SDK specification defines the functionality implemented by all language SDKs for defining a resource. The following suites of attributes are defined by the OpenTelemetry resource semantic conventions. These attributes are usually set by creating a resource using the OpenTelemetry SDK. service.* attributes service.name attribute is required to associate your resource with an entity in the UI service.instance.id is required for certain panes to light up telemetry.sdk.language=java is required to see data in the JVM section Batching Caution Avoid getting rate limited! You should batch requests sent to the OTLP endpoint as described in this section. By default, the OpenTelemetry SDKs and Collector send one (1) data point per request. Using these defaults, it is likely your account will be rate limited. All OpenTelemetry SDKs and Collectors provide a BatchProcessor, which batches data points in memory. This batching allows requests to be sent with more than one (1) data point. Component Batch Processor Collector Batch Processor Go SDK BatchSpanProcessor JS SDK BatchSpanProcessor Python SDK BatchExportSpanProcessor Compression New Relic supports gzip compression for OTLP payloads exported over gRPC. To maximize the amount of data you can send per request, we recommend enabling compression in all OTLP exporters. If there are other compression formats you'd like to see us support, please let us know in the CNCF Slack channel. Traces Familiarize yourself with these trace topics to ensure your traces and spans appear in New Relic. Required fields The startTimeUnixNano and endTimeUnixNano fields on spans are required according to the OpenTelemetry protocol for trace data. When startTimeUnixNano is not present, the span is dropped and a NrIntegrationError is created. When endTimeUnixNano is not present, the duration of your span is large and negative. The timeUnixNano field on span events is required. When timeUnixNano is not present, the span event is dropped and a NrIntegrationError is created. The traceId and spanId fields on spans are required according to the OpenTelemetry protocol for trace data. When traceId or spanId are not present, the span is dropped and a NrIntegrationError is created. Sampling Trace data is the most mature OpenTelemetry data type. Because of this, New Relic's OpenTelemetry user experience is largely based on trace data and is therefore influenced by your sampling strategy. You can configure sampling in a number of places: Service: Use the OpenTelemetry SDK for your language. Collector: If you're running your own instance of the OpenTelemetry collector, you can configure it to do more sophisticated forms of sampling, such as tail-based sampling (see below). Check out this documentation about how to configure different types of sampling: OpenTelemetry built-in samplers Built-in samplers implemented by the OpenTelemetry SDK for each language. OpenTelemetry tail-based samplers The OpenTelemetry collector has a tail-based sampling processor. We have an example demonstrating the use of the tail-based sampling processor. New Relic tail-based sampling with Infinite Tracing Infinite Tracing is New Relic's tail-based sampling option. You can use this in conjunction with your OpenTelemetry instrumented services. In setting up Infinite Tracing, you need to override the default span endpoint and send telemetry data to the New Relic trace observer: Important Currently, Infinite Tracing does not support OTLP ingest. You must run your own instance of the OpenTelemetry Collector and configure it to use the New Relic exporter. Follow the steps in Set up the trace observer to get the value for YOUR_TRACE_OBSERVER_URL. Use the value of YOUR_TRACE_OBSERVER_URL to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your OpenTelemetry integrations use the AlwaysOn sampler. Metrics OpenTelemetry metrics are largely compatible with New Relic dimensional metrics. We support OpenTelemetry metrics v0.10. All of the supported metric types include an independent set of associated attributes (name-value pairs) which map directly to dimensions you can use to facet or filter metric data at query time. OpenTelemetry metrics are accompanied by a set of resource attributes that identify the originating entity that produced them and map to dimensions for faceting and filtering. The OpenTelemetry data model for metrics defines a number of different metric types: sum, gauge, histogram, and summary. Sum metrics OpenTelemetry sums are a scalar metric that is the sum of all data points over a given time window. Sums have a notion of temporality indicating whether reported values incorporate previous measurements (cumulative temporality) or not (delta temporality). In addition, sums can either be monotonic (only go up or only go down) or non-monotonic (go up and down). Delta sums In New Relic, delta metrics are handled differently depending on whether they are monotonic or non-monotonic: Monotonic delta sums are mapped to the count metric type. Non-monotonic delta sums are mapped to the gauge metric type. Cumulative sums Monotonic and non-monotonic cumulative sums are mapped to the New Relic gauge metric type. Sum configuration examples To understand how to configure aggregation temporality, see these examples using the Java and Go OpenTelemetry SDKs. Gauge metrics OpenTelemetry gauge metric data points represent a sampled value at a given time. These values are converted to the New Relic gauge metric type. OpenTelemetry gauges do not have an aggregation temporality, but the sampled values can be aggregated at query time. Histogram metrics OpenTelemetry histograms compactly represent a population of recorded values along with a total count and sum. Optionally, histograms may include a series of buckets with explicit bounds and a count value for that bucket’s population. OpenTelemetry histograms are converted to New Relic’s distribution metric type, which is backed by a scaled exponential base 2 histogram (see NrSketch for a more thorough explanation). Counts from OpenTelemetry histogram buckets are assigned to New Relic’s distribution metric buckets using linear interpolation. Also, OpenTelemetry has negative and positive infinity bound buckets which we represent in New Relic as zero-width buckets. We do this because we do not have a representation for negative and positive infinity. For example, an OpenTelemetry bucket with bounds [-∞, 10) will be represented by a [10,10) zero width New Relic bucket. You may see exaggerated bucket counts at the endpoints of your distribution due to this translation. Summary metrics OpenTelemetry summary metric data points are used to represent quantile summaries (for example, P99 latency). These map directly to the New Relic summary metric type. Summary metric data points include count, sum, and quantile values, with 0.0 as min and 1.0 as max. OpenTelemetry provides summary metrics for compatibility with other formats. Start time The startTimeUnixNano field is optional according to the OpenTelemetry specification. When this field is provided, it is used for the timestamp on the resulting NewRelic metric, and the duration is calculated as timeUnixNano - startTimeUnixNano. The duration field is used to calculate the queryable endTimeStamp attribute on the New Relic metric, but it serves no other semantic purpose. If startTimeUnixNano is not provided, then timeUnixNano is used for the timestamp field on the resulting NewRelic metric, and the duration field is set to zero. Array values for attributes OpenTelemetry metrics and other signals may include attributes that consist of a homogenous array of primitive types. These attributes are not supported by New Relic. Exemplars OpenTelemetry defines exemplar values that allow other signals, like traces, to be connected to a metric event and provide context. Exemplars are not supported by New Relic. How to query metrics Consider these tips for building metric NRQL queries in New Relic. Query cumulative sums stored as gauges Since cumulative sums are converted to gauges, here are some ways to query your data: Example: Raw gauge value for cumulative sums To view the raw gauge value for cumulative sums, you can use the latest() NRQL function: SELECT latest(totalApiBytesSent) FROM Metric Copy Example: Rate of change with cumulative sums as gauges To see the rate of change over a given time interval for a cumulative sum stored as a gauge, you can use the derivative() NRQL function: SELECT derivative(totalApiBytesSent, 1 minute) FROM Metric Copy New Relic does not currently support either reporting on resets and gaps or accounting for them with cumulative counters. Query gauge metrics When New Relic converts cumulative sums to gauges, you can query them using either the latest() or derivative() NRQL functions. The function you choose depends on whether you want to see the raw value or compute the rate of change. Query histogram metrics New Relic histograms translated from OpenTelemetry metrics have the same query semantics as other New Relic histograms. Namely, the histogram() NRQL function can be used to represent the histogram with a configurable number of buckets and bucket width. Note that you may see larger bucket counts at the endpoint buckets. This is because we are adding negative and positive infinity bound OpenTelemetry buckets into a zero width New Relic bucket. Example: Normal distribution FROM Metric SELECT histogram(test.histogram, buckets: 100, width: 1000) WHERE distributionType = 'Normal Distribution' SINCE 1 day ago Copy Example: Heat map The FACET keyword is also available to create heat map charts. FROM Metric SELECT histogram(test.histogram, buckets: 100, width: 1000) FACET distributionType SINCE 1 day ago Copy Important The TIMESERIES keyword is not supported for New Relic histograms. Logs Logs generated from your applications and environment are an important piece of telemetry. They may represent application logs, machine generated events, or system logs. OpenTelemetry has defined a log data model for representing log data. You can send logs using OpenTelemetry tooling, correlate them with applications, and view them in New Relic. Send logs to New Relic The OpenTelemetry Collector and OpenTelemetry Collector Contrib repositories contain a number of components for consuming log data. The general pattern is to configure the collector to: Receive logs from any of the log receivers. Some of the receiver options include Filelog Receiver, Fluent Forward Receiver, and Syslog Receiver. Process logs, potentially annotating them with resource information. Some of the processor options include Resource Detection Processor and Resource Processor. Export logs to New Relic via the OTLP exporter. Application log correlation Application logs are more useful if they're correlated with other telemetry data produced by the application. The OpenTelemetry semantic convention for services specifies service.name as a required field. All application metric, trace, and log data sent to New Relic with the same service.name are associated with the same entity. The specifics of how logs get annotated with the service.name resource attribute depends on the application's environment: Applications may produce structured JSON logs, which you can configure to include service.name as another field. You can deploy applications alongside a dedicated Collector Agent instance, which you can configure with a Resource Processor to annotate logs with the service.name attribute. Optionally, additional application trace context (sometimes called execution context) can be propagated to log messages. The setup and availability of this depends on the language and logging framework used by the application. The general strategy is to set up the application to write structured JSON logs and to configure it to extract trace context into specified trace context fields on available log messages. The Logs in Context with Log4j2 example in GitHub demonstrates an end-to-end working example for a simple Java application using Log4j2. View OpenTelemetry logs Here are two ways you can view logs: Look in the New Relic Logs UI. If your logs are correlated with an application, view them in the context of the application. The time field The timeUnixNano field is optional according to the OpenTelemetry specification for log data. When timeUnixNano is not present New Relic will use the time that the data was received for the New Relic log timestamp.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 281.6755,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Best practices for <em>OpenTelemetry</em> with New Relic",
        "sections": "Best practices for <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to analyze all your traces, make sure to verify that your <em>OpenTelemetry</em> <em>integrations</em> use the AlwaysOn sampler. Metrics <em>OpenTelemetry</em> metrics are largely compatible with New Relic dimensional metrics. We support <em>OpenTelemetry</em> metrics v0.10. All of the supported metric types include an independent set"
      },
      "id": "60f6b9b964441f5da847ac01"
    },
    {
      "sections": [
        "OpenTelemetry quick start",
        "Step 1. Prerequisites",
        "Step 2. Instrument your service with OpenTelemetry",
        "Step 3. Export your telemetry data to New Relic",
        "Review New Relic settings for exports",
        "Important",
        "Complete the export configuration steps",
        "Export data to an OpenTelemetry Collector (optional)",
        "Step 4. View your data in the New Relic UI",
        "What's next?"
      ],
      "title": "OpenTelemetry quick start",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "1b846417a2958b61b047c838db49aea06f09a2a8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-quick-start/",
      "published_at": "2021-10-07T01:54:02Z",
      "updated_at": "2021-09-27T14:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "OpenTelemetry is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up OpenTelemetry with New Relic. Here's an overview of the process, followed by details for each step. Prerequisites Instrument your service with OpenTelemetry Export your telemetry data to New Relic View your data in the New Relic UI Step 1. Prerequisites First things first: If we don’t already know you, sign up for a free New Relic account. Copy your account license key. Step 2. Instrument your service with OpenTelemetry To get started, you instrument your service with OpenTelemetry. OpenTelemetry has language-specific products and SDKs to help you. Many languages offer out-the-box instrumentation for common libraries and frameworks. Each language also provides an API for further instrumenting your service manually. Go to the repository for your language and follow the instructions to instrument your service. When you're done, return here to complete Step 3. Export your telemetry data to New Relic. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...See a complete list of languages in GitHub Step 3. Export your telemetry data to New Relic The OpenTelemetry Protocol, or OTLP for short, is a general purpose telemetry data delivery protocol designed for the OpenTelemetry project. This protocol describes how to encode and transmit telemetry data, which makes it a natural choice for data transport. Each language SDK provides an OTLP exporter you can configure to export data over OTLP. In this step, we focus on how to configure an OTLP exporter in your service to export data directly to New Relic. If you prefer to export your data first to an OpenTelemetry collector, we have separate instructions. Here's an example of sending data from your service directly to New Relic. To complete this third step, first familiarize yourself with some required New Relic settings, and then complete the steps in the OTLP exporter documentation for your language. Review New Relic settings for exports Before you go to the external OTLP exporter documentation, consult the table below so you're ready to do the following: Configure the OTLP exporter to add a header ( api-key ) whose value is your account license key. Based on your region, configure the endpoint where the exporter sends data to New Relic. Region gRPC HTTP/1.1 Endpoint API Header Name API Header Value TLS encryption required US ✅ ❌ https://otlp.nr-data.net:4317 api-key License Key ✅ EU ✅ ❌ https://otlp.eu01.nr-data.net:4317 api-key License Key ✅ If you have FedRamp compliance constraints, see FedRAMP-compliant endpoints. Important In Node.js, the opentelemetry-collector-grpc library requires additional options to enable TLS. Complete the export configuration steps Click on the link below for your language and complete the configuration steps. When you're done, return here to complete Step 4. View your data in the New Relic UI. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...Find additional OTLP language support in GitHub Export data to an OpenTelemetry Collector (optional) The OpenTelemetry Collector is a configurable and extensible software component to receive, process, and export telemetry data. When you set up a collector, it can operate as a gateway or as an agent: Gateway: The collector receives data from a variety of sources and applies standard processing before exporting to some backend. Agent: The collector is deployed on each host in an environment and can collect telemetry data about the host and processes running on it. When you use a collector, you start by following the same routine as above for setting up OTLP in your service. In this case, instead of exporting data directly to New Relic, you export through a collector that you set up. In the collector, you configure the OTLP exporter to export data to New Relic. When your data goes through a collector, the transport looks like this: Here's a Docker example of how to set up and run an OpenTelemetry collector with the collector YAML: Save the following as otel-config.yaml: receivers: otlp: protocols: grpc: http: processors: batch: exporters: otlp: endpoint: ${OTEL_EXPORTER_OTLP_ENDPOINT} headers: api-key: ${NEW_RELIC_LICENSE_KEY} service: pipelines: traces: receivers: [otlp] processors: [batch] exporters: [otlp] metrics: receivers: [otlp] processors: [batch] exporters: [otlp] logs: receivers: [otlp] processors: [batch] exporters: [otlp] Copy Run the OpenTelemetry collector, making sure you replace OTLP_ENDPOINT_HERE with the appropriate endpoint and replace YOUR_KEY_HERE with your Account License Key: export OTEL_EXPORTER_OTLP_ENDPOINT=OTLP_ENDPOINT_HERE export NEW_RELIC_LICENSE_KEY=YOUR_KEY_HERE docker run --rm \\ -e OTEL_EXPORTER_OTLP_ENDPOINT \\ -e NEW_RELIC_LICENSE_KEY \\ -p 4317:4317 \\ -v \"${PWD}/otel-config.yaml\":/otel-config.yaml \\ --config otel-config.yaml \\ --name otelcol \\ otel/opentelemetry-collector Copy Step 4. View your data in the New Relic UI Once you’ve instrumented your service and configured it to export its data to New Relic, watch the New Relic One user interface for your traces, metrics, and logs! The UI for OpenTelemetry has some similarities to the APM agent UI, so if you are familiar with that, you can go right to the UI. If you need help understanding your OpenTelemetry UI options or how to make sure your data appears in the UI, see View your OpenTelemetry data in New Relic. What's next? After you do your initial setup, check out our best-practices guide for tips about various configurations to improve your use of OpenTelemetry and New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 280.8957,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> quick start",
        "sections": "<em>OpenTelemetry</em> quick start",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "<em>OpenTelemetry</em> is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up <em>OpenTelemetry</em> with New Relic. Here&#x27;s an overview of the process, followed by details for each step. Prerequisites Instrument your service with <em>OpenTelemetry</em> Export"
      },
      "id": "6044e5dfe7b9d2aadc5799d4"
    },
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Find your service (entity)",
        "Narrow down your data with filters",
        "Summary page",
        "Distributed tracing",
        "Find traces and their associated spans",
        "Tip",
        "View spans with errors",
        "View span events",
        "Databases",
        "Errors",
        "Externals",
        "JVMs",
        "Logs",
        "Metrics explorer",
        "Transactions",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/490779aac8c10802682d86f0b785b6a2/a4078/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-10-07T01:52:56Z",
      "updated_at": "2021-09-27T15:53:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information about your services. It offers you a way to filter services and then filter the data to show a variety of views into your data. Find your service (entity) To get started in Explorer, you need to find your service: Go to one.newrelic.com. In the left sidebar, click Services - OpenTelemetry: In the center pane, click the service you want to know more about, or find your service by entering the name in the top filter bar. Narrow down your data with filters Once you have your entity, you can then filter for data from your service. With the filter bar Narrow data to..., you can highlight a specific facet of the telemetry recorded for your service. For example, you may want to see the error rate for a particular version of the service that you've deployed in a canary instance, so you add a filter for service.version='1.2.3'. Filters are preserved when navigating between different views of your data for a service. For example, the filter for service.version='1.2.3' carries over to the Transactions view, so that you would see telemetry on requests to the endpoints (transactions) that are running version 1.2.3 of your service, and not any other versions. Filters are preserved when navigating between the Summary, Transactions, Databases, Externals, Errors, and JVMs views. Filters are also preserved when navigating to the Distributed tracing view, but with limitations. Only filter conditions that use the equals operator (\"=\") are currently supported when navigating to Distributed tracing. If you navigate back from the Distributed tracing page, the filters you selected on the previous view will come back. Once you filter your data, the UI has various views in the left-navigation pane. See our descriptions below for view details. Summary page After you click on a service in the Explorer tab, you see the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. For your data to appear in this section, make sure it has the following: UI area Attribute Response time span.kind = server or consumer Throughput span.kind = server or consumer Error rate span.kind = server or consumer otel.status_code = ERROR Service instances pane service.instance.id (usually set via the OTel resource API) Distributed tracing In Distributed tracing, you can locate traces and examine span details. For your trace data to appear in the New Relic UI, it needs to conform to the OpenTelemetry trace semantic conventions. Also, in the Traces section of our best practices guide, you can find some tips about making sure your traces and spans appear in New Relic. The following attributes are typically not added explicitly to spans. Rather, they are usually set when creating a span or performing operations (for example, recording an error) on a span using the OpenTelemetry SDK. Attribute Description name A span’s name is generally set when starting a span. The name can generally be anything, but the OpenTelemetry specification provides guidance for certain types of spans like Database spans or HTTP spans. span.kind A span’s kind is generally set when starting a span. New Relic uses span.kind to infer that a span is an entry point to a given service. When span.kind = server or consumer it is considered an entry point. When span.kind = client or producer, it is considered a call to an external service or database system. otel.status_code A span’s status is set using the span API. The otel.status_code attribute is how it is manifested by New Relic. The UI primarily uses otel.status_code for the purpose of identifying errors. Find traces and their associated spans Here are some ways to target your searches: To find the traces you want, you can run queries like these in the filter bar: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID When you find an interesting trace, click on it to display a waterfall diagram showing the spans of the trace. Click on specific spans within the trace to display span details in the right panel. Tip For more ways to filter traces, see our distributed tracing UI page. View spans with errors After you click on a span in the trace waterfall view, you can see span errors in the right panel under Error details. The error details are populated by spans containing otel.status_code = ERROR and display the content of otel.status_description. To narrow your search for spans with errors, you can enter otel.status_code = ERROR directly in the distributed tracing filter bar. View span events If you send span events as described by the OpenTelemetry specification, you can view them in the New Relic UI. Span events have two general types: Exceptions Non-exceptions (for example, logs) If you have span events, links for these appear in the right pane: Click on a span in the waterfall view. In the right pane, click View span events, or you can expand Error details and click the link at the bottom of the errors. When you're in span events and only want to view exceptions, slide the toggle Only show exceptions. Tip OpenTelemetry exceptions handled by the app/service are displayed independently of span error status and are not necessarily associated with a span error status. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. For your data to appear in this section, make sure it has the following: UI area Attribute Top database calls span.kind = client or producer db.system Facets by span name Top databases (by query time) span.kind = client or producer db.system Facets by db.system Top databases (by throughput) span.kind = client or producer db.system Facets by db.system Errors On the Errors page, you can see total errors as well as charts showing error count and error rate. For your data to appear in this section, make sure it has the following: span.kind = server or consumer otel.status_code = ERROR Facets by span name Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. For your data to appear in this section, make sure it has the following: span.kind = client or producer db.system is not present JVMs When you drill into a specific JVM, the UI display charts driven by JVM metric data: JVM metrics follow the general semantic conventions for runtime environment metrics. The Java specific runtime metrics are not well documented. The implementation is effectively the documentation and may be subject to change. For your data to appear in this section, make sure it has the following: Requires a unique service.instance.id attribute for rendering the list of JVMs Service.instance.id is an OpenTelemetry resource attribute Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. For your data to appear in this section, make sure it has the following: service.name To correlate with trace data, the logs should contain the trace.id and span.id attributes. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. For your data to appear in this section, make sure it has the following: UI area Description Top Transactions span.kind = server or consumer Facets by span name Throughput span.kind = server or consumer Facets by span name Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 279.01944,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-concepts": [
    {
      "sections": [
        "Introduction to OpenTelemetry with New Relic",
        "Benefits of OpenTelemetry",
        "Should I use OpenTelemetry instrumentation or New Relic agents?",
        "OpenTelemetry: A work in progress",
        "New Relic's APM agents",
        "How OpenTelemetry works with New Relic",
        "Important",
        "Traces",
        "Metrics",
        "Logs",
        "Next steps"
      ],
      "title": "Introduction to OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "c87898d2d5835c00930c173eabd1bf93040badea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/introduction-opentelemetry-new-relic/",
      "published_at": "2021-10-07T00:14:26Z",
      "updated_at": "2021-09-27T15:16:23Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Are you already familiar with OpenTelemetry and want to begin the setup? Check out our quick start: Quick start If you don't have one already, create a New Relic account. It's free, forever. If you're just getting acquainted with OpenTelemetry, this is what we'll explore here: Benefits of OpenTelemetry Should I use OpenTelemetry or New Relic agents? How OpenTelemetry works with New Relic Benefits of OpenTelemetry OpenTelemetry provides a secure, vendor-neutral specification for service instrumentation so that you can export data to distinct backends of your choice, such as New Relic. OpenTelemetry offers a single set of APIs and libraries that standardize how you collect and transfer telemetry data for your services. The following components make up the OpenTelemetry project: Specifications for the core pillars of observability to drive consistency across all projects. New Relic supports all of these signals: traces, metrics, and logs (see details below) APIs that contain interfaces and implementations based on the specifications SDKs (reference implementations of the APIs) created specifically for languages like Java, Python, Go, Erlang, and more Collectors that offer a vendor-agnostic implementation for processing and exporting Exporters that enable you to send data to a backend of your choice The components of OpenTelemetry work together to create some distinct advantages for capturing telemetry data: Feature Description Ubiquitous instrumentation A single, open standard of instrumentation provides better coverage and flexibility as engineers from all over the world contribute to the instrumentation. Future proof As the instrumentation gets built into libraries and frameworks, and as more vendors move to support this open standard, you can be confident that you won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build integrations into OpenTelemetry or add instrumentation directly to source code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need to decide which instrumentation option to use (a proprietary option or one of the other open standards). Cross-platform compatibility OpenTelemetry supports a variety of languages and backends. It represents a vendor-neutral path for capturing and transmitting telemetry to backends without altering existing instrumentation. Streamlined observability It is easier for vendors to support and test against a single standard as they don’t need to develop their own agents or collectors. High dimensionality OpenTelemetry uses dimensional metrics, so you can filter and facet on more aspects of the data, such as AWS regions, Kubernetes clusters, or service versions. Dimensional metrics also lead to less time between occurrence and reporting. Efficiency OpenTelemetry’s fire-and-forget trace-centric approach to instrumentation often has lower overhead than New Relic agents, especially for asynchronous workloads. It will also result in better handling of trace data for asynchronous requests. Should I use OpenTelemetry instrumentation or New Relic agents? As you consider OpenTelemetry, you may also be looking at New Relic APM agents that also capture telemetry data. As you'd expect, there is a lot of overlap between features available from OpenTelemetry agents and SDKs versus those available from New Relic APM agents. This is especially true if you're interested in distributed tracing telemetry data. The choice you make depends on what you need. We recommend that you explore both New Relic and OpenTelemetry instrumentation or discuss this with us in our CNCF Slack channel to decide what works best for you. OpenTelemetry: A work in progress OpenTelemetry is still an emerging standard, so your choices may be affected by what's available. You can check on the current state of the specification at the OpenTelemetry site. The current state of language-specific OpenTelemetry APIs and SDKs varies: some languages are still pre-alpha and may be missing instructions on how to instrument your service. Most languages have some implementation of traces that is sufficient to start exporting data to New Relic. Check out this table in GitHub that provides an overview of the state of OpenTelemetry specification compliance for each language. For languages that New Relic does not currently provide an agent or SDK, OpenTelemetry may offer you a good alternative. Also, in cases where you want explicit control over sampling of your telemetry data, OpenTelemetry provides a lot of flexibility. As OpenTelemetry matures, New Relic will continue to support new OpenTelemetry data models and to provide a curated UI experience for our Full Stack Observability customers. New Relic's APM agents In general, New Relic's APM agents will collect more telemetry data for your services, and they offer a wide range of configuration options and an extensive set of auto-instrumentation capabilities. New Relic's APM agents offer detailed transaction trace visibility for individual services. They also offer predefined sampling to balance the performance impact of your instrumentation against the need to capture enough data to gain helpful insights. How OpenTelemetry works with New Relic New Relic supports the native OpenTelemetry Protocol (OTLP) for exporting telemetry data. This allows you to use the vendor neutral components developed by the OpenTelemetry community to export your data to New Relic. The following tables show the supported features for each telemetry signal. If you have questions about these or have an unsupported use case, please contact us in our CNCF Slack channel, and watch this page for future updates. Important New Relic's exporters for OpenTelemetry are now deprecated in favor of exporting data to New Relic using OTLP. Traces New Relic offers support for the OTLP ingest of trace signals. The maturity of the upstream specification is stable. OpenTelemetry traces and spans are compatible with New Relic traces and spans. OpenTelemetry spans optionally include attributes (name-value pairs) and resource attributes which map directly to dimensions that can be used to facet or filter span data at query time. OpenTelemetry span metadata (for example, name, kind, and trace_id) also map directly to dimensions on NewRelic spans. At this time, New Relic does not support span links or array attributes. For details, see the Traces section of our best practices guide. Feature Supported Span events ✅ Span linking ❌ Array of primitives (homogeneous) ❌ Metrics New Relic offers support for the OTLP ingest of metric signals. Note that the maturity of the upstream specification is experimental. We intend to follow potentially breaking upstream changes. Here are the OpenTelemetry data types we support and their associated mappings. For details, see the Metrics section of our best practices guide. Metric Type Supported Delta sums ✅ Cumulative sums ✅ Gauges ✅ Delta histograms ✅ Summary ✅ Cumulative histograms ❌ Exemplars ❌ Array of primitives (homogeneous) ❌ Logs New Relic offers support for the OTLP ingest of log signals. Note that the maturity of the upstream specification is experimental. We intend to follow potentially breaking upstream changes. OpenTelemetry logs are compatible with New Relic logs. OpenTelemetry logs optionally include attributes (name-value pairs) and resource attributes which map directly to dimensions that can be used to facet or filter log data at query time. OpenTelemetry log metadata (for example, name, severity_text, and trace_id) also map directly to dimensions on New Relic logs. NewRelic currently supports all OpenTelemetry log message types except for arrays. For more details, see the Logs section of our best practices guide. Feature Supported Description LogRecord body ✅ Supported types: string, boolean, int, double, bytes LogRecord attributes ✅ Supported types: string, boolean, int, double, bytes LogRecord fields ✅ Examples: name, severity_text, trace_id Array messages ❌ Array attributes ❌ Next steps Follow the OpenTelemetry quick start to help you get started. You'll also want to review the best practices guide for getting the most out of the data you export to New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 282.1434,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "sections": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build <em>integrations</em> into <em>OpenTelemetry</em> or add instrumentation directly to <em>source</em> code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need"
      },
      "id": "603e81ba196a67304da83dab"
    },
    {
      "sections": [
        "OpenTelemetry quick start",
        "Step 1. Prerequisites",
        "Step 2. Instrument your service with OpenTelemetry",
        "Step 3. Export your telemetry data to New Relic",
        "Review New Relic settings for exports",
        "Important",
        "Complete the export configuration steps",
        "Export data to an OpenTelemetry Collector (optional)",
        "Step 4. View your data in the New Relic UI",
        "What's next?"
      ],
      "title": "OpenTelemetry quick start",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "1b846417a2958b61b047c838db49aea06f09a2a8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-quick-start/",
      "published_at": "2021-10-07T01:54:02Z",
      "updated_at": "2021-09-27T14:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "OpenTelemetry is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up OpenTelemetry with New Relic. Here's an overview of the process, followed by details for each step. Prerequisites Instrument your service with OpenTelemetry Export your telemetry data to New Relic View your data in the New Relic UI Step 1. Prerequisites First things first: If we don’t already know you, sign up for a free New Relic account. Copy your account license key. Step 2. Instrument your service with OpenTelemetry To get started, you instrument your service with OpenTelemetry. OpenTelemetry has language-specific products and SDKs to help you. Many languages offer out-the-box instrumentation for common libraries and frameworks. Each language also provides an API for further instrumenting your service manually. Go to the repository for your language and follow the instructions to instrument your service. When you're done, return here to complete Step 3. Export your telemetry data to New Relic. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...See a complete list of languages in GitHub Step 3. Export your telemetry data to New Relic The OpenTelemetry Protocol, or OTLP for short, is a general purpose telemetry data delivery protocol designed for the OpenTelemetry project. This protocol describes how to encode and transmit telemetry data, which makes it a natural choice for data transport. Each language SDK provides an OTLP exporter you can configure to export data over OTLP. In this step, we focus on how to configure an OTLP exporter in your service to export data directly to New Relic. If you prefer to export your data first to an OpenTelemetry collector, we have separate instructions. Here's an example of sending data from your service directly to New Relic. To complete this third step, first familiarize yourself with some required New Relic settings, and then complete the steps in the OTLP exporter documentation for your language. Review New Relic settings for exports Before you go to the external OTLP exporter documentation, consult the table below so you're ready to do the following: Configure the OTLP exporter to add a header ( api-key ) whose value is your account license key. Based on your region, configure the endpoint where the exporter sends data to New Relic. Region gRPC HTTP/1.1 Endpoint API Header Name API Header Value TLS encryption required US ✅ ❌ https://otlp.nr-data.net:4317 api-key License Key ✅ EU ✅ ❌ https://otlp.eu01.nr-data.net:4317 api-key License Key ✅ If you have FedRamp compliance constraints, see FedRAMP-compliant endpoints. Important In Node.js, the opentelemetry-collector-grpc library requires additional options to enable TLS. Complete the export configuration steps Click on the link below for your language and complete the configuration steps. When you're done, return here to complete Step 4. View your data in the New Relic UI. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...Find additional OTLP language support in GitHub Export data to an OpenTelemetry Collector (optional) The OpenTelemetry Collector is a configurable and extensible software component to receive, process, and export telemetry data. When you set up a collector, it can operate as a gateway or as an agent: Gateway: The collector receives data from a variety of sources and applies standard processing before exporting to some backend. Agent: The collector is deployed on each host in an environment and can collect telemetry data about the host and processes running on it. When you use a collector, you start by following the same routine as above for setting up OTLP in your service. In this case, instead of exporting data directly to New Relic, you export through a collector that you set up. In the collector, you configure the OTLP exporter to export data to New Relic. When your data goes through a collector, the transport looks like this: Here's a Docker example of how to set up and run an OpenTelemetry collector with the collector YAML: Save the following as otel-config.yaml: receivers: otlp: protocols: grpc: http: processors: batch: exporters: otlp: endpoint: ${OTEL_EXPORTER_OTLP_ENDPOINT} headers: api-key: ${NEW_RELIC_LICENSE_KEY} service: pipelines: traces: receivers: [otlp] processors: [batch] exporters: [otlp] metrics: receivers: [otlp] processors: [batch] exporters: [otlp] logs: receivers: [otlp] processors: [batch] exporters: [otlp] Copy Run the OpenTelemetry collector, making sure you replace OTLP_ENDPOINT_HERE with the appropriate endpoint and replace YOUR_KEY_HERE with your Account License Key: export OTEL_EXPORTER_OTLP_ENDPOINT=OTLP_ENDPOINT_HERE export NEW_RELIC_LICENSE_KEY=YOUR_KEY_HERE docker run --rm \\ -e OTEL_EXPORTER_OTLP_ENDPOINT \\ -e NEW_RELIC_LICENSE_KEY \\ -p 4317:4317 \\ -v \"${PWD}/otel-config.yaml\":/otel-config.yaml \\ --config otel-config.yaml \\ --name otelcol \\ otel/opentelemetry-collector Copy Step 4. View your data in the New Relic UI Once you’ve instrumented your service and configured it to export its data to New Relic, watch the New Relic One user interface for your traces, metrics, and logs! The UI for OpenTelemetry has some similarities to the APM agent UI, so if you are familiar with that, you can go right to the UI. If you need help understanding your OpenTelemetry UI options or how to make sure your data appears in the UI, see View your OpenTelemetry data in New Relic. What's next? After you do your initial setup, check out our best-practices guide for tips about various configurations to improve your use of OpenTelemetry and New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 280.8957,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> quick start",
        "sections": "<em>OpenTelemetry</em> quick start",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "<em>OpenTelemetry</em> is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up <em>OpenTelemetry</em> with New Relic. Here&#x27;s an overview of the process, followed by details for each step. Prerequisites Instrument your service with <em>OpenTelemetry</em> Export"
      },
      "id": "6044e5dfe7b9d2aadc5799d4"
    },
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Find your service (entity)",
        "Narrow down your data with filters",
        "Summary page",
        "Distributed tracing",
        "Find traces and their associated spans",
        "Tip",
        "View spans with errors",
        "View span events",
        "Databases",
        "Errors",
        "Externals",
        "JVMs",
        "Logs",
        "Metrics explorer",
        "Transactions",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/490779aac8c10802682d86f0b785b6a2/a4078/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-10-07T01:52:56Z",
      "updated_at": "2021-09-27T15:53:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information about your services. It offers you a way to filter services and then filter the data to show a variety of views into your data. Find your service (entity) To get started in Explorer, you need to find your service: Go to one.newrelic.com. In the left sidebar, click Services - OpenTelemetry: In the center pane, click the service you want to know more about, or find your service by entering the name in the top filter bar. Narrow down your data with filters Once you have your entity, you can then filter for data from your service. With the filter bar Narrow data to..., you can highlight a specific facet of the telemetry recorded for your service. For example, you may want to see the error rate for a particular version of the service that you've deployed in a canary instance, so you add a filter for service.version='1.2.3'. Filters are preserved when navigating between different views of your data for a service. For example, the filter for service.version='1.2.3' carries over to the Transactions view, so that you would see telemetry on requests to the endpoints (transactions) that are running version 1.2.3 of your service, and not any other versions. Filters are preserved when navigating between the Summary, Transactions, Databases, Externals, Errors, and JVMs views. Filters are also preserved when navigating to the Distributed tracing view, but with limitations. Only filter conditions that use the equals operator (\"=\") are currently supported when navigating to Distributed tracing. If you navigate back from the Distributed tracing page, the filters you selected on the previous view will come back. Once you filter your data, the UI has various views in the left-navigation pane. See our descriptions below for view details. Summary page After you click on a service in the Explorer tab, you see the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. For your data to appear in this section, make sure it has the following: UI area Attribute Response time span.kind = server or consumer Throughput span.kind = server or consumer Error rate span.kind = server or consumer otel.status_code = ERROR Service instances pane service.instance.id (usually set via the OTel resource API) Distributed tracing In Distributed tracing, you can locate traces and examine span details. For your trace data to appear in the New Relic UI, it needs to conform to the OpenTelemetry trace semantic conventions. Also, in the Traces section of our best practices guide, you can find some tips about making sure your traces and spans appear in New Relic. The following attributes are typically not added explicitly to spans. Rather, they are usually set when creating a span or performing operations (for example, recording an error) on a span using the OpenTelemetry SDK. Attribute Description name A span’s name is generally set when starting a span. The name can generally be anything, but the OpenTelemetry specification provides guidance for certain types of spans like Database spans or HTTP spans. span.kind A span’s kind is generally set when starting a span. New Relic uses span.kind to infer that a span is an entry point to a given service. When span.kind = server or consumer it is considered an entry point. When span.kind = client or producer, it is considered a call to an external service or database system. otel.status_code A span’s status is set using the span API. The otel.status_code attribute is how it is manifested by New Relic. The UI primarily uses otel.status_code for the purpose of identifying errors. Find traces and their associated spans Here are some ways to target your searches: To find the traces you want, you can run queries like these in the filter bar: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID When you find an interesting trace, click on it to display a waterfall diagram showing the spans of the trace. Click on specific spans within the trace to display span details in the right panel. Tip For more ways to filter traces, see our distributed tracing UI page. View spans with errors After you click on a span in the trace waterfall view, you can see span errors in the right panel under Error details. The error details are populated by spans containing otel.status_code = ERROR and display the content of otel.status_description. To narrow your search for spans with errors, you can enter otel.status_code = ERROR directly in the distributed tracing filter bar. View span events If you send span events as described by the OpenTelemetry specification, you can view them in the New Relic UI. Span events have two general types: Exceptions Non-exceptions (for example, logs) If you have span events, links for these appear in the right pane: Click on a span in the waterfall view. In the right pane, click View span events, or you can expand Error details and click the link at the bottom of the errors. When you're in span events and only want to view exceptions, slide the toggle Only show exceptions. Tip OpenTelemetry exceptions handled by the app/service are displayed independently of span error status and are not necessarily associated with a span error status. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. For your data to appear in this section, make sure it has the following: UI area Attribute Top database calls span.kind = client or producer db.system Facets by span name Top databases (by query time) span.kind = client or producer db.system Facets by db.system Top databases (by throughput) span.kind = client or producer db.system Facets by db.system Errors On the Errors page, you can see total errors as well as charts showing error count and error rate. For your data to appear in this section, make sure it has the following: span.kind = server or consumer otel.status_code = ERROR Facets by span name Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. For your data to appear in this section, make sure it has the following: span.kind = client or producer db.system is not present JVMs When you drill into a specific JVM, the UI display charts driven by JVM metric data: JVM metrics follow the general semantic conventions for runtime environment metrics. The Java specific runtime metrics are not well documented. The implementation is effectively the documentation and may be subject to change. For your data to appear in this section, make sure it has the following: Requires a unique service.instance.id attribute for rendering the list of JVMs Service.instance.id is an OpenTelemetry resource attribute Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. For your data to appear in this section, make sure it has the following: service.name To correlate with trace data, the logs should contain the trace.id and span.id attributes. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. For your data to appear in this section, make sure it has the following: UI area Description Top Transactions span.kind = server or consumer Facets by span name Throughput span.kind = server or consumer Facets by span name Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 279.01944,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-legacy-new-relic-exporters": [
    {
      "sections": [
        "Introduction to OpenTelemetry with New Relic",
        "Benefits of OpenTelemetry",
        "Should I use OpenTelemetry instrumentation or New Relic agents?",
        "OpenTelemetry: A work in progress",
        "New Relic's APM agents",
        "How OpenTelemetry works with New Relic",
        "Important",
        "Traces",
        "Metrics",
        "Logs",
        "Next steps"
      ],
      "title": "Introduction to OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "c87898d2d5835c00930c173eabd1bf93040badea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/introduction-opentelemetry-new-relic/",
      "published_at": "2021-10-07T00:14:26Z",
      "updated_at": "2021-09-27T15:16:23Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Are you already familiar with OpenTelemetry and want to begin the setup? Check out our quick start: Quick start If you don't have one already, create a New Relic account. It's free, forever. If you're just getting acquainted with OpenTelemetry, this is what we'll explore here: Benefits of OpenTelemetry Should I use OpenTelemetry or New Relic agents? How OpenTelemetry works with New Relic Benefits of OpenTelemetry OpenTelemetry provides a secure, vendor-neutral specification for service instrumentation so that you can export data to distinct backends of your choice, such as New Relic. OpenTelemetry offers a single set of APIs and libraries that standardize how you collect and transfer telemetry data for your services. The following components make up the OpenTelemetry project: Specifications for the core pillars of observability to drive consistency across all projects. New Relic supports all of these signals: traces, metrics, and logs (see details below) APIs that contain interfaces and implementations based on the specifications SDKs (reference implementations of the APIs) created specifically for languages like Java, Python, Go, Erlang, and more Collectors that offer a vendor-agnostic implementation for processing and exporting Exporters that enable you to send data to a backend of your choice The components of OpenTelemetry work together to create some distinct advantages for capturing telemetry data: Feature Description Ubiquitous instrumentation A single, open standard of instrumentation provides better coverage and flexibility as engineers from all over the world contribute to the instrumentation. Future proof As the instrumentation gets built into libraries and frameworks, and as more vendors move to support this open standard, you can be confident that you won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build integrations into OpenTelemetry or add instrumentation directly to source code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need to decide which instrumentation option to use (a proprietary option or one of the other open standards). Cross-platform compatibility OpenTelemetry supports a variety of languages and backends. It represents a vendor-neutral path for capturing and transmitting telemetry to backends without altering existing instrumentation. Streamlined observability It is easier for vendors to support and test against a single standard as they don’t need to develop their own agents or collectors. High dimensionality OpenTelemetry uses dimensional metrics, so you can filter and facet on more aspects of the data, such as AWS regions, Kubernetes clusters, or service versions. Dimensional metrics also lead to less time between occurrence and reporting. Efficiency OpenTelemetry’s fire-and-forget trace-centric approach to instrumentation often has lower overhead than New Relic agents, especially for asynchronous workloads. It will also result in better handling of trace data for asynchronous requests. Should I use OpenTelemetry instrumentation or New Relic agents? As you consider OpenTelemetry, you may also be looking at New Relic APM agents that also capture telemetry data. As you'd expect, there is a lot of overlap between features available from OpenTelemetry agents and SDKs versus those available from New Relic APM agents. This is especially true if you're interested in distributed tracing telemetry data. The choice you make depends on what you need. We recommend that you explore both New Relic and OpenTelemetry instrumentation or discuss this with us in our CNCF Slack channel to decide what works best for you. OpenTelemetry: A work in progress OpenTelemetry is still an emerging standard, so your choices may be affected by what's available. You can check on the current state of the specification at the OpenTelemetry site. The current state of language-specific OpenTelemetry APIs and SDKs varies: some languages are still pre-alpha and may be missing instructions on how to instrument your service. Most languages have some implementation of traces that is sufficient to start exporting data to New Relic. Check out this table in GitHub that provides an overview of the state of OpenTelemetry specification compliance for each language. For languages that New Relic does not currently provide an agent or SDK, OpenTelemetry may offer you a good alternative. Also, in cases where you want explicit control over sampling of your telemetry data, OpenTelemetry provides a lot of flexibility. As OpenTelemetry matures, New Relic will continue to support new OpenTelemetry data models and to provide a curated UI experience for our Full Stack Observability customers. New Relic's APM agents In general, New Relic's APM agents will collect more telemetry data for your services, and they offer a wide range of configuration options and an extensive set of auto-instrumentation capabilities. New Relic's APM agents offer detailed transaction trace visibility for individual services. They also offer predefined sampling to balance the performance impact of your instrumentation against the need to capture enough data to gain helpful insights. How OpenTelemetry works with New Relic New Relic supports the native OpenTelemetry Protocol (OTLP) for exporting telemetry data. This allows you to use the vendor neutral components developed by the OpenTelemetry community to export your data to New Relic. The following tables show the supported features for each telemetry signal. If you have questions about these or have an unsupported use case, please contact us in our CNCF Slack channel, and watch this page for future updates. Important New Relic's exporters for OpenTelemetry are now deprecated in favor of exporting data to New Relic using OTLP. Traces New Relic offers support for the OTLP ingest of trace signals. The maturity of the upstream specification is stable. OpenTelemetry traces and spans are compatible with New Relic traces and spans. OpenTelemetry spans optionally include attributes (name-value pairs) and resource attributes which map directly to dimensions that can be used to facet or filter span data at query time. OpenTelemetry span metadata (for example, name, kind, and trace_id) also map directly to dimensions on NewRelic spans. At this time, New Relic does not support span links or array attributes. For details, see the Traces section of our best practices guide. Feature Supported Span events ✅ Span linking ❌ Array of primitives (homogeneous) ❌ Metrics New Relic offers support for the OTLP ingest of metric signals. Note that the maturity of the upstream specification is experimental. We intend to follow potentially breaking upstream changes. Here are the OpenTelemetry data types we support and their associated mappings. For details, see the Metrics section of our best practices guide. Metric Type Supported Delta sums ✅ Cumulative sums ✅ Gauges ✅ Delta histograms ✅ Summary ✅ Cumulative histograms ❌ Exemplars ❌ Array of primitives (homogeneous) ❌ Logs New Relic offers support for the OTLP ingest of log signals. Note that the maturity of the upstream specification is experimental. We intend to follow potentially breaking upstream changes. OpenTelemetry logs are compatible with New Relic logs. OpenTelemetry logs optionally include attributes (name-value pairs) and resource attributes which map directly to dimensions that can be used to facet or filter log data at query time. OpenTelemetry log metadata (for example, name, severity_text, and trace_id) also map directly to dimensions on New Relic logs. NewRelic currently supports all OpenTelemetry log message types except for arrays. For more details, see the Logs section of our best practices guide. Feature Supported Description LogRecord body ✅ Supported types: string, boolean, int, double, bytes LogRecord attributes ✅ Supported types: string, boolean, int, double, bytes LogRecord fields ✅ Examples: name, severity_text, trace_id Array messages ❌ Array attributes ❌ Next steps Follow the OpenTelemetry quick start to help you get started. You'll also want to review the best practices guide for getting the most out of the data you export to New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 282.14334,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "sections": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build <em>integrations</em> into <em>OpenTelemetry</em> or add instrumentation directly to <em>source</em> code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need"
      },
      "id": "603e81ba196a67304da83dab"
    },
    {
      "sections": [
        "Best practices for OpenTelemetry with New Relic",
        "Resources",
        "Batching",
        "Caution",
        "Compression",
        "Traces",
        "Required fields",
        "Sampling",
        "OpenTelemetry built-in samplers",
        "OpenTelemetry tail-based samplers",
        "New Relic tail-based sampling with Infinite Tracing",
        "Important",
        "Metrics",
        "Sum metrics",
        "Delta sums",
        "Cumulative sums",
        "Sum configuration examples",
        "Gauge metrics",
        "Histogram metrics",
        "Summary metrics",
        "Start time",
        "Array values for attributes",
        "Exemplars",
        "How to query metrics",
        "Query cumulative sums stored as gauges",
        "Example: Raw gauge value for cumulative sums",
        "Example: Rate of change with cumulative sums as gauges",
        "Query gauge metrics",
        "Query histogram metrics",
        "Example: Normal distribution",
        "Example: Heat map",
        "Logs",
        "Send logs to New Relic",
        "Application log correlation",
        "View OpenTelemetry logs",
        "The time field"
      ],
      "title": "Best practices for OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "2b624c5862a5a48d088268eccb310510f372b125",
      "image": "https://docs.newrelic.com/static/764474b6404e7a5c65e55d2eb985e93c/c1b63/sum-derivative-function.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-concepts/",
      "published_at": "2021-10-07T01:54:01Z",
      "updated_at": "2021-09-27T14:48:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Here are some best practices based on how OpenTelemetry works with New Relic: Resources Batching Compression Traces Metrics Logs Resources A resource in OpenTelemetry represents information about an entity generating telemetry data. All telemetry data sent to New Relic is expected to be associated with a resource so that it can be linked with the appropriate entity in New Relic. The OpenTelemetry Resource SDK specification defines the functionality implemented by all language SDKs for defining a resource. The following suites of attributes are defined by the OpenTelemetry resource semantic conventions. These attributes are usually set by creating a resource using the OpenTelemetry SDK. service.* attributes service.name attribute is required to associate your resource with an entity in the UI service.instance.id is required for certain panes to light up telemetry.sdk.language=java is required to see data in the JVM section Batching Caution Avoid getting rate limited! You should batch requests sent to the OTLP endpoint as described in this section. By default, the OpenTelemetry SDKs and Collector send one (1) data point per request. Using these defaults, it is likely your account will be rate limited. All OpenTelemetry SDKs and Collectors provide a BatchProcessor, which batches data points in memory. This batching allows requests to be sent with more than one (1) data point. Component Batch Processor Collector Batch Processor Go SDK BatchSpanProcessor JS SDK BatchSpanProcessor Python SDK BatchExportSpanProcessor Compression New Relic supports gzip compression for OTLP payloads exported over gRPC. To maximize the amount of data you can send per request, we recommend enabling compression in all OTLP exporters. If there are other compression formats you'd like to see us support, please let us know in the CNCF Slack channel. Traces Familiarize yourself with these trace topics to ensure your traces and spans appear in New Relic. Required fields The startTimeUnixNano and endTimeUnixNano fields on spans are required according to the OpenTelemetry protocol for trace data. When startTimeUnixNano is not present, the span is dropped and a NrIntegrationError is created. When endTimeUnixNano is not present, the duration of your span is large and negative. The timeUnixNano field on span events is required. When timeUnixNano is not present, the span event is dropped and a NrIntegrationError is created. The traceId and spanId fields on spans are required according to the OpenTelemetry protocol for trace data. When traceId or spanId are not present, the span is dropped and a NrIntegrationError is created. Sampling Trace data is the most mature OpenTelemetry data type. Because of this, New Relic's OpenTelemetry user experience is largely based on trace data and is therefore influenced by your sampling strategy. You can configure sampling in a number of places: Service: Use the OpenTelemetry SDK for your language. Collector: If you're running your own instance of the OpenTelemetry collector, you can configure it to do more sophisticated forms of sampling, such as tail-based sampling (see below). Check out this documentation about how to configure different types of sampling: OpenTelemetry built-in samplers Built-in samplers implemented by the OpenTelemetry SDK for each language. OpenTelemetry tail-based samplers The OpenTelemetry collector has a tail-based sampling processor. We have an example demonstrating the use of the tail-based sampling processor. New Relic tail-based sampling with Infinite Tracing Infinite Tracing is New Relic's tail-based sampling option. You can use this in conjunction with your OpenTelemetry instrumented services. In setting up Infinite Tracing, you need to override the default span endpoint and send telemetry data to the New Relic trace observer: Important Currently, Infinite Tracing does not support OTLP ingest. You must run your own instance of the OpenTelemetry Collector and configure it to use the New Relic exporter. Follow the steps in Set up the trace observer to get the value for YOUR_TRACE_OBSERVER_URL. Use the value of YOUR_TRACE_OBSERVER_URL to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your OpenTelemetry integrations use the AlwaysOn sampler. Metrics OpenTelemetry metrics are largely compatible with New Relic dimensional metrics. We support OpenTelemetry metrics v0.10. All of the supported metric types include an independent set of associated attributes (name-value pairs) which map directly to dimensions you can use to facet or filter metric data at query time. OpenTelemetry metrics are accompanied by a set of resource attributes that identify the originating entity that produced them and map to dimensions for faceting and filtering. The OpenTelemetry data model for metrics defines a number of different metric types: sum, gauge, histogram, and summary. Sum metrics OpenTelemetry sums are a scalar metric that is the sum of all data points over a given time window. Sums have a notion of temporality indicating whether reported values incorporate previous measurements (cumulative temporality) or not (delta temporality). In addition, sums can either be monotonic (only go up or only go down) or non-monotonic (go up and down). Delta sums In New Relic, delta metrics are handled differently depending on whether they are monotonic or non-monotonic: Monotonic delta sums are mapped to the count metric type. Non-monotonic delta sums are mapped to the gauge metric type. Cumulative sums Monotonic and non-monotonic cumulative sums are mapped to the New Relic gauge metric type. Sum configuration examples To understand how to configure aggregation temporality, see these examples using the Java and Go OpenTelemetry SDKs. Gauge metrics OpenTelemetry gauge metric data points represent a sampled value at a given time. These values are converted to the New Relic gauge metric type. OpenTelemetry gauges do not have an aggregation temporality, but the sampled values can be aggregated at query time. Histogram metrics OpenTelemetry histograms compactly represent a population of recorded values along with a total count and sum. Optionally, histograms may include a series of buckets with explicit bounds and a count value for that bucket’s population. OpenTelemetry histograms are converted to New Relic’s distribution metric type, which is backed by a scaled exponential base 2 histogram (see NrSketch for a more thorough explanation). Counts from OpenTelemetry histogram buckets are assigned to New Relic’s distribution metric buckets using linear interpolation. Also, OpenTelemetry has negative and positive infinity bound buckets which we represent in New Relic as zero-width buckets. We do this because we do not have a representation for negative and positive infinity. For example, an OpenTelemetry bucket with bounds [-∞, 10) will be represented by a [10,10) zero width New Relic bucket. You may see exaggerated bucket counts at the endpoints of your distribution due to this translation. Summary metrics OpenTelemetry summary metric data points are used to represent quantile summaries (for example, P99 latency). These map directly to the New Relic summary metric type. Summary metric data points include count, sum, and quantile values, with 0.0 as min and 1.0 as max. OpenTelemetry provides summary metrics for compatibility with other formats. Start time The startTimeUnixNano field is optional according to the OpenTelemetry specification. When this field is provided, it is used for the timestamp on the resulting NewRelic metric, and the duration is calculated as timeUnixNano - startTimeUnixNano. The duration field is used to calculate the queryable endTimeStamp attribute on the New Relic metric, but it serves no other semantic purpose. If startTimeUnixNano is not provided, then timeUnixNano is used for the timestamp field on the resulting NewRelic metric, and the duration field is set to zero. Array values for attributes OpenTelemetry metrics and other signals may include attributes that consist of a homogenous array of primitive types. These attributes are not supported by New Relic. Exemplars OpenTelemetry defines exemplar values that allow other signals, like traces, to be connected to a metric event and provide context. Exemplars are not supported by New Relic. How to query metrics Consider these tips for building metric NRQL queries in New Relic. Query cumulative sums stored as gauges Since cumulative sums are converted to gauges, here are some ways to query your data: Example: Raw gauge value for cumulative sums To view the raw gauge value for cumulative sums, you can use the latest() NRQL function: SELECT latest(totalApiBytesSent) FROM Metric Copy Example: Rate of change with cumulative sums as gauges To see the rate of change over a given time interval for a cumulative sum stored as a gauge, you can use the derivative() NRQL function: SELECT derivative(totalApiBytesSent, 1 minute) FROM Metric Copy New Relic does not currently support either reporting on resets and gaps or accounting for them with cumulative counters. Query gauge metrics When New Relic converts cumulative sums to gauges, you can query them using either the latest() or derivative() NRQL functions. The function you choose depends on whether you want to see the raw value or compute the rate of change. Query histogram metrics New Relic histograms translated from OpenTelemetry metrics have the same query semantics as other New Relic histograms. Namely, the histogram() NRQL function can be used to represent the histogram with a configurable number of buckets and bucket width. Note that you may see larger bucket counts at the endpoint buckets. This is because we are adding negative and positive infinity bound OpenTelemetry buckets into a zero width New Relic bucket. Example: Normal distribution FROM Metric SELECT histogram(test.histogram, buckets: 100, width: 1000) WHERE distributionType = 'Normal Distribution' SINCE 1 day ago Copy Example: Heat map The FACET keyword is also available to create heat map charts. FROM Metric SELECT histogram(test.histogram, buckets: 100, width: 1000) FACET distributionType SINCE 1 day ago Copy Important The TIMESERIES keyword is not supported for New Relic histograms. Logs Logs generated from your applications and environment are an important piece of telemetry. They may represent application logs, machine generated events, or system logs. OpenTelemetry has defined a log data model for representing log data. You can send logs using OpenTelemetry tooling, correlate them with applications, and view them in New Relic. Send logs to New Relic The OpenTelemetry Collector and OpenTelemetry Collector Contrib repositories contain a number of components for consuming log data. The general pattern is to configure the collector to: Receive logs from any of the log receivers. Some of the receiver options include Filelog Receiver, Fluent Forward Receiver, and Syslog Receiver. Process logs, potentially annotating them with resource information. Some of the processor options include Resource Detection Processor and Resource Processor. Export logs to New Relic via the OTLP exporter. Application log correlation Application logs are more useful if they're correlated with other telemetry data produced by the application. The OpenTelemetry semantic convention for services specifies service.name as a required field. All application metric, trace, and log data sent to New Relic with the same service.name are associated with the same entity. The specifics of how logs get annotated with the service.name resource attribute depends on the application's environment: Applications may produce structured JSON logs, which you can configure to include service.name as another field. You can deploy applications alongside a dedicated Collector Agent instance, which you can configure with a Resource Processor to annotate logs with the service.name attribute. Optionally, additional application trace context (sometimes called execution context) can be propagated to log messages. The setup and availability of this depends on the language and logging framework used by the application. The general strategy is to set up the application to write structured JSON logs and to configure it to extract trace context into specified trace context fields on available log messages. The Logs in Context with Log4j2 example in GitHub demonstrates an end-to-end working example for a simple Java application using Log4j2. View OpenTelemetry logs Here are two ways you can view logs: Look in the New Relic Logs UI. If your logs are correlated with an application, view them in the context of the application. The time field The timeUnixNano field is optional according to the OpenTelemetry specification for log data. When timeUnixNano is not present New Relic will use the time that the data was received for the New Relic log timestamp.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 281.67545,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Best practices for <em>OpenTelemetry</em> with New Relic",
        "sections": "Best practices for <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to analyze all your traces, make sure to verify that your <em>OpenTelemetry</em> <em>integrations</em> use the AlwaysOn sampler. Metrics <em>OpenTelemetry</em> metrics are largely compatible with New Relic dimensional metrics. We support <em>OpenTelemetry</em> metrics v0.10. All of the supported metric types include an independent set"
      },
      "id": "60f6b9b964441f5da847ac01"
    },
    {
      "sections": [
        "OpenTelemetry quick start",
        "Step 1. Prerequisites",
        "Step 2. Instrument your service with OpenTelemetry",
        "Step 3. Export your telemetry data to New Relic",
        "Review New Relic settings for exports",
        "Important",
        "Complete the export configuration steps",
        "Export data to an OpenTelemetry Collector (optional)",
        "Step 4. View your data in the New Relic UI",
        "What's next?"
      ],
      "title": "OpenTelemetry quick start",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "1b846417a2958b61b047c838db49aea06f09a2a8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-quick-start/",
      "published_at": "2021-10-07T01:54:02Z",
      "updated_at": "2021-09-27T14:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "OpenTelemetry is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up OpenTelemetry with New Relic. Here's an overview of the process, followed by details for each step. Prerequisites Instrument your service with OpenTelemetry Export your telemetry data to New Relic View your data in the New Relic UI Step 1. Prerequisites First things first: If we don’t already know you, sign up for a free New Relic account. Copy your account license key. Step 2. Instrument your service with OpenTelemetry To get started, you instrument your service with OpenTelemetry. OpenTelemetry has language-specific products and SDKs to help you. Many languages offer out-the-box instrumentation for common libraries and frameworks. Each language also provides an API for further instrumenting your service manually. Go to the repository for your language and follow the instructions to instrument your service. When you're done, return here to complete Step 3. Export your telemetry data to New Relic. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...See a complete list of languages in GitHub Step 3. Export your telemetry data to New Relic The OpenTelemetry Protocol, or OTLP for short, is a general purpose telemetry data delivery protocol designed for the OpenTelemetry project. This protocol describes how to encode and transmit telemetry data, which makes it a natural choice for data transport. Each language SDK provides an OTLP exporter you can configure to export data over OTLP. In this step, we focus on how to configure an OTLP exporter in your service to export data directly to New Relic. If you prefer to export your data first to an OpenTelemetry collector, we have separate instructions. Here's an example of sending data from your service directly to New Relic. To complete this third step, first familiarize yourself with some required New Relic settings, and then complete the steps in the OTLP exporter documentation for your language. Review New Relic settings for exports Before you go to the external OTLP exporter documentation, consult the table below so you're ready to do the following: Configure the OTLP exporter to add a header ( api-key ) whose value is your account license key. Based on your region, configure the endpoint where the exporter sends data to New Relic. Region gRPC HTTP/1.1 Endpoint API Header Name API Header Value TLS encryption required US ✅ ❌ https://otlp.nr-data.net:4317 api-key License Key ✅ EU ✅ ❌ https://otlp.eu01.nr-data.net:4317 api-key License Key ✅ If you have FedRamp compliance constraints, see FedRAMP-compliant endpoints. Important In Node.js, the opentelemetry-collector-grpc library requires additional options to enable TLS. Complete the export configuration steps Click on the link below for your language and complete the configuration steps. When you're done, return here to complete Step 4. View your data in the New Relic UI. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...Find additional OTLP language support in GitHub Export data to an OpenTelemetry Collector (optional) The OpenTelemetry Collector is a configurable and extensible software component to receive, process, and export telemetry data. When you set up a collector, it can operate as a gateway or as an agent: Gateway: The collector receives data from a variety of sources and applies standard processing before exporting to some backend. Agent: The collector is deployed on each host in an environment and can collect telemetry data about the host and processes running on it. When you use a collector, you start by following the same routine as above for setting up OTLP in your service. In this case, instead of exporting data directly to New Relic, you export through a collector that you set up. In the collector, you configure the OTLP exporter to export data to New Relic. When your data goes through a collector, the transport looks like this: Here's a Docker example of how to set up and run an OpenTelemetry collector with the collector YAML: Save the following as otel-config.yaml: receivers: otlp: protocols: grpc: http: processors: batch: exporters: otlp: endpoint: ${OTEL_EXPORTER_OTLP_ENDPOINT} headers: api-key: ${NEW_RELIC_LICENSE_KEY} service: pipelines: traces: receivers: [otlp] processors: [batch] exporters: [otlp] metrics: receivers: [otlp] processors: [batch] exporters: [otlp] logs: receivers: [otlp] processors: [batch] exporters: [otlp] Copy Run the OpenTelemetry collector, making sure you replace OTLP_ENDPOINT_HERE with the appropriate endpoint and replace YOUR_KEY_HERE with your Account License Key: export OTEL_EXPORTER_OTLP_ENDPOINT=OTLP_ENDPOINT_HERE export NEW_RELIC_LICENSE_KEY=YOUR_KEY_HERE docker run --rm \\ -e OTEL_EXPORTER_OTLP_ENDPOINT \\ -e NEW_RELIC_LICENSE_KEY \\ -p 4317:4317 \\ -v \"${PWD}/otel-config.yaml\":/otel-config.yaml \\ --config otel-config.yaml \\ --name otelcol \\ otel/opentelemetry-collector Copy Step 4. View your data in the New Relic UI Once you’ve instrumented your service and configured it to export its data to New Relic, watch the New Relic One user interface for your traces, metrics, and logs! The UI for OpenTelemetry has some similarities to the APM agent UI, so if you are familiar with that, you can go right to the UI. If you need help understanding your OpenTelemetry UI options or how to make sure your data appears in the UI, see View your OpenTelemetry data in New Relic. What's next? After you do your initial setup, check out our best-practices guide for tips about various configurations to improve your use of OpenTelemetry and New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 280.89563,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> quick start",
        "sections": "<em>OpenTelemetry</em> quick start",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "<em>OpenTelemetry</em> is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up <em>OpenTelemetry</em> with New Relic. Here&#x27;s an overview of the process, followed by details for each step. Prerequisites Instrument your service with <em>OpenTelemetry</em> Export"
      },
      "id": "6044e5dfe7b9d2aadc5799d4"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-quick-start": [
    {
      "sections": [
        "Introduction to OpenTelemetry with New Relic",
        "Benefits of OpenTelemetry",
        "Should I use OpenTelemetry instrumentation or New Relic agents?",
        "OpenTelemetry: A work in progress",
        "New Relic's APM agents",
        "How OpenTelemetry works with New Relic",
        "Important",
        "Traces",
        "Metrics",
        "Logs",
        "Next steps"
      ],
      "title": "Introduction to OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "c87898d2d5835c00930c173eabd1bf93040badea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/introduction-opentelemetry-new-relic/",
      "published_at": "2021-10-07T00:14:26Z",
      "updated_at": "2021-09-27T15:16:23Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Are you already familiar with OpenTelemetry and want to begin the setup? Check out our quick start: Quick start If you don't have one already, create a New Relic account. It's free, forever. If you're just getting acquainted with OpenTelemetry, this is what we'll explore here: Benefits of OpenTelemetry Should I use OpenTelemetry or New Relic agents? How OpenTelemetry works with New Relic Benefits of OpenTelemetry OpenTelemetry provides a secure, vendor-neutral specification for service instrumentation so that you can export data to distinct backends of your choice, such as New Relic. OpenTelemetry offers a single set of APIs and libraries that standardize how you collect and transfer telemetry data for your services. The following components make up the OpenTelemetry project: Specifications for the core pillars of observability to drive consistency across all projects. New Relic supports all of these signals: traces, metrics, and logs (see details below) APIs that contain interfaces and implementations based on the specifications SDKs (reference implementations of the APIs) created specifically for languages like Java, Python, Go, Erlang, and more Collectors that offer a vendor-agnostic implementation for processing and exporting Exporters that enable you to send data to a backend of your choice The components of OpenTelemetry work together to create some distinct advantages for capturing telemetry data: Feature Description Ubiquitous instrumentation A single, open standard of instrumentation provides better coverage and flexibility as engineers from all over the world contribute to the instrumentation. Future proof As the instrumentation gets built into libraries and frameworks, and as more vendors move to support this open standard, you can be confident that you won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build integrations into OpenTelemetry or add instrumentation directly to source code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need to decide which instrumentation option to use (a proprietary option or one of the other open standards). Cross-platform compatibility OpenTelemetry supports a variety of languages and backends. It represents a vendor-neutral path for capturing and transmitting telemetry to backends without altering existing instrumentation. Streamlined observability It is easier for vendors to support and test against a single standard as they don’t need to develop their own agents or collectors. High dimensionality OpenTelemetry uses dimensional metrics, so you can filter and facet on more aspects of the data, such as AWS regions, Kubernetes clusters, or service versions. Dimensional metrics also lead to less time between occurrence and reporting. Efficiency OpenTelemetry’s fire-and-forget trace-centric approach to instrumentation often has lower overhead than New Relic agents, especially for asynchronous workloads. It will also result in better handling of trace data for asynchronous requests. Should I use OpenTelemetry instrumentation or New Relic agents? As you consider OpenTelemetry, you may also be looking at New Relic APM agents that also capture telemetry data. As you'd expect, there is a lot of overlap between features available from OpenTelemetry agents and SDKs versus those available from New Relic APM agents. This is especially true if you're interested in distributed tracing telemetry data. The choice you make depends on what you need. We recommend that you explore both New Relic and OpenTelemetry instrumentation or discuss this with us in our CNCF Slack channel to decide what works best for you. OpenTelemetry: A work in progress OpenTelemetry is still an emerging standard, so your choices may be affected by what's available. You can check on the current state of the specification at the OpenTelemetry site. The current state of language-specific OpenTelemetry APIs and SDKs varies: some languages are still pre-alpha and may be missing instructions on how to instrument your service. Most languages have some implementation of traces that is sufficient to start exporting data to New Relic. Check out this table in GitHub that provides an overview of the state of OpenTelemetry specification compliance for each language. For languages that New Relic does not currently provide an agent or SDK, OpenTelemetry may offer you a good alternative. Also, in cases where you want explicit control over sampling of your telemetry data, OpenTelemetry provides a lot of flexibility. As OpenTelemetry matures, New Relic will continue to support new OpenTelemetry data models and to provide a curated UI experience for our Full Stack Observability customers. New Relic's APM agents In general, New Relic's APM agents will collect more telemetry data for your services, and they offer a wide range of configuration options and an extensive set of auto-instrumentation capabilities. New Relic's APM agents offer detailed transaction trace visibility for individual services. They also offer predefined sampling to balance the performance impact of your instrumentation against the need to capture enough data to gain helpful insights. How OpenTelemetry works with New Relic New Relic supports the native OpenTelemetry Protocol (OTLP) for exporting telemetry data. This allows you to use the vendor neutral components developed by the OpenTelemetry community to export your data to New Relic. The following tables show the supported features for each telemetry signal. If you have questions about these or have an unsupported use case, please contact us in our CNCF Slack channel, and watch this page for future updates. Important New Relic's exporters for OpenTelemetry are now deprecated in favor of exporting data to New Relic using OTLP. Traces New Relic offers support for the OTLP ingest of trace signals. The maturity of the upstream specification is stable. OpenTelemetry traces and spans are compatible with New Relic traces and spans. OpenTelemetry spans optionally include attributes (name-value pairs) and resource attributes which map directly to dimensions that can be used to facet or filter span data at query time. OpenTelemetry span metadata (for example, name, kind, and trace_id) also map directly to dimensions on NewRelic spans. At this time, New Relic does not support span links or array attributes. For details, see the Traces section of our best practices guide. Feature Supported Span events ✅ Span linking ❌ Array of primitives (homogeneous) ❌ Metrics New Relic offers support for the OTLP ingest of metric signals. Note that the maturity of the upstream specification is experimental. We intend to follow potentially breaking upstream changes. Here are the OpenTelemetry data types we support and their associated mappings. For details, see the Metrics section of our best practices guide. Metric Type Supported Delta sums ✅ Cumulative sums ✅ Gauges ✅ Delta histograms ✅ Summary ✅ Cumulative histograms ❌ Exemplars ❌ Array of primitives (homogeneous) ❌ Logs New Relic offers support for the OTLP ingest of log signals. Note that the maturity of the upstream specification is experimental. We intend to follow potentially breaking upstream changes. OpenTelemetry logs are compatible with New Relic logs. OpenTelemetry logs optionally include attributes (name-value pairs) and resource attributes which map directly to dimensions that can be used to facet or filter log data at query time. OpenTelemetry log metadata (for example, name, severity_text, and trace_id) also map directly to dimensions on New Relic logs. NewRelic currently supports all OpenTelemetry log message types except for arrays. For more details, see the Logs section of our best practices guide. Feature Supported Description LogRecord body ✅ Supported types: string, boolean, int, double, bytes LogRecord attributes ✅ Supported types: string, boolean, int, double, bytes LogRecord fields ✅ Examples: name, severity_text, trace_id Array messages ❌ Array attributes ❌ Next steps Follow the OpenTelemetry quick start to help you get started. You'll also want to review the best practices guide for getting the most out of the data you export to New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 282.14334,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "sections": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build <em>integrations</em> into <em>OpenTelemetry</em> or add instrumentation directly to <em>source</em> code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need"
      },
      "id": "603e81ba196a67304da83dab"
    },
    {
      "sections": [
        "Best practices for OpenTelemetry with New Relic",
        "Resources",
        "Batching",
        "Caution",
        "Compression",
        "Traces",
        "Required fields",
        "Sampling",
        "OpenTelemetry built-in samplers",
        "OpenTelemetry tail-based samplers",
        "New Relic tail-based sampling with Infinite Tracing",
        "Important",
        "Metrics",
        "Sum metrics",
        "Delta sums",
        "Cumulative sums",
        "Sum configuration examples",
        "Gauge metrics",
        "Histogram metrics",
        "Summary metrics",
        "Start time",
        "Array values for attributes",
        "Exemplars",
        "How to query metrics",
        "Query cumulative sums stored as gauges",
        "Example: Raw gauge value for cumulative sums",
        "Example: Rate of change with cumulative sums as gauges",
        "Query gauge metrics",
        "Query histogram metrics",
        "Example: Normal distribution",
        "Example: Heat map",
        "Logs",
        "Send logs to New Relic",
        "Application log correlation",
        "View OpenTelemetry logs",
        "The time field"
      ],
      "title": "Best practices for OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "2b624c5862a5a48d088268eccb310510f372b125",
      "image": "https://docs.newrelic.com/static/764474b6404e7a5c65e55d2eb985e93c/c1b63/sum-derivative-function.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-concepts/",
      "published_at": "2021-10-07T01:54:01Z",
      "updated_at": "2021-09-27T14:48:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Here are some best practices based on how OpenTelemetry works with New Relic: Resources Batching Compression Traces Metrics Logs Resources A resource in OpenTelemetry represents information about an entity generating telemetry data. All telemetry data sent to New Relic is expected to be associated with a resource so that it can be linked with the appropriate entity in New Relic. The OpenTelemetry Resource SDK specification defines the functionality implemented by all language SDKs for defining a resource. The following suites of attributes are defined by the OpenTelemetry resource semantic conventions. These attributes are usually set by creating a resource using the OpenTelemetry SDK. service.* attributes service.name attribute is required to associate your resource with an entity in the UI service.instance.id is required for certain panes to light up telemetry.sdk.language=java is required to see data in the JVM section Batching Caution Avoid getting rate limited! You should batch requests sent to the OTLP endpoint as described in this section. By default, the OpenTelemetry SDKs and Collector send one (1) data point per request. Using these defaults, it is likely your account will be rate limited. All OpenTelemetry SDKs and Collectors provide a BatchProcessor, which batches data points in memory. This batching allows requests to be sent with more than one (1) data point. Component Batch Processor Collector Batch Processor Go SDK BatchSpanProcessor JS SDK BatchSpanProcessor Python SDK BatchExportSpanProcessor Compression New Relic supports gzip compression for OTLP payloads exported over gRPC. To maximize the amount of data you can send per request, we recommend enabling compression in all OTLP exporters. If there are other compression formats you'd like to see us support, please let us know in the CNCF Slack channel. Traces Familiarize yourself with these trace topics to ensure your traces and spans appear in New Relic. Required fields The startTimeUnixNano and endTimeUnixNano fields on spans are required according to the OpenTelemetry protocol for trace data. When startTimeUnixNano is not present, the span is dropped and a NrIntegrationError is created. When endTimeUnixNano is not present, the duration of your span is large and negative. The timeUnixNano field on span events is required. When timeUnixNano is not present, the span event is dropped and a NrIntegrationError is created. The traceId and spanId fields on spans are required according to the OpenTelemetry protocol for trace data. When traceId or spanId are not present, the span is dropped and a NrIntegrationError is created. Sampling Trace data is the most mature OpenTelemetry data type. Because of this, New Relic's OpenTelemetry user experience is largely based on trace data and is therefore influenced by your sampling strategy. You can configure sampling in a number of places: Service: Use the OpenTelemetry SDK for your language. Collector: If you're running your own instance of the OpenTelemetry collector, you can configure it to do more sophisticated forms of sampling, such as tail-based sampling (see below). Check out this documentation about how to configure different types of sampling: OpenTelemetry built-in samplers Built-in samplers implemented by the OpenTelemetry SDK for each language. OpenTelemetry tail-based samplers The OpenTelemetry collector has a tail-based sampling processor. We have an example demonstrating the use of the tail-based sampling processor. New Relic tail-based sampling with Infinite Tracing Infinite Tracing is New Relic's tail-based sampling option. You can use this in conjunction with your OpenTelemetry instrumented services. In setting up Infinite Tracing, you need to override the default span endpoint and send telemetry data to the New Relic trace observer: Important Currently, Infinite Tracing does not support OTLP ingest. You must run your own instance of the OpenTelemetry Collector and configure it to use the New Relic exporter. Follow the steps in Set up the trace observer to get the value for YOUR_TRACE_OBSERVER_URL. Use the value of YOUR_TRACE_OBSERVER_URL to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your OpenTelemetry integrations use the AlwaysOn sampler. Metrics OpenTelemetry metrics are largely compatible with New Relic dimensional metrics. We support OpenTelemetry metrics v0.10. All of the supported metric types include an independent set of associated attributes (name-value pairs) which map directly to dimensions you can use to facet or filter metric data at query time. OpenTelemetry metrics are accompanied by a set of resource attributes that identify the originating entity that produced them and map to dimensions for faceting and filtering. The OpenTelemetry data model for metrics defines a number of different metric types: sum, gauge, histogram, and summary. Sum metrics OpenTelemetry sums are a scalar metric that is the sum of all data points over a given time window. Sums have a notion of temporality indicating whether reported values incorporate previous measurements (cumulative temporality) or not (delta temporality). In addition, sums can either be monotonic (only go up or only go down) or non-monotonic (go up and down). Delta sums In New Relic, delta metrics are handled differently depending on whether they are monotonic or non-monotonic: Monotonic delta sums are mapped to the count metric type. Non-monotonic delta sums are mapped to the gauge metric type. Cumulative sums Monotonic and non-monotonic cumulative sums are mapped to the New Relic gauge metric type. Sum configuration examples To understand how to configure aggregation temporality, see these examples using the Java and Go OpenTelemetry SDKs. Gauge metrics OpenTelemetry gauge metric data points represent a sampled value at a given time. These values are converted to the New Relic gauge metric type. OpenTelemetry gauges do not have an aggregation temporality, but the sampled values can be aggregated at query time. Histogram metrics OpenTelemetry histograms compactly represent a population of recorded values along with a total count and sum. Optionally, histograms may include a series of buckets with explicit bounds and a count value for that bucket’s population. OpenTelemetry histograms are converted to New Relic’s distribution metric type, which is backed by a scaled exponential base 2 histogram (see NrSketch for a more thorough explanation). Counts from OpenTelemetry histogram buckets are assigned to New Relic’s distribution metric buckets using linear interpolation. Also, OpenTelemetry has negative and positive infinity bound buckets which we represent in New Relic as zero-width buckets. We do this because we do not have a representation for negative and positive infinity. For example, an OpenTelemetry bucket with bounds [-∞, 10) will be represented by a [10,10) zero width New Relic bucket. You may see exaggerated bucket counts at the endpoints of your distribution due to this translation. Summary metrics OpenTelemetry summary metric data points are used to represent quantile summaries (for example, P99 latency). These map directly to the New Relic summary metric type. Summary metric data points include count, sum, and quantile values, with 0.0 as min and 1.0 as max. OpenTelemetry provides summary metrics for compatibility with other formats. Start time The startTimeUnixNano field is optional according to the OpenTelemetry specification. When this field is provided, it is used for the timestamp on the resulting NewRelic metric, and the duration is calculated as timeUnixNano - startTimeUnixNano. The duration field is used to calculate the queryable endTimeStamp attribute on the New Relic metric, but it serves no other semantic purpose. If startTimeUnixNano is not provided, then timeUnixNano is used for the timestamp field on the resulting NewRelic metric, and the duration field is set to zero. Array values for attributes OpenTelemetry metrics and other signals may include attributes that consist of a homogenous array of primitive types. These attributes are not supported by New Relic. Exemplars OpenTelemetry defines exemplar values that allow other signals, like traces, to be connected to a metric event and provide context. Exemplars are not supported by New Relic. How to query metrics Consider these tips for building metric NRQL queries in New Relic. Query cumulative sums stored as gauges Since cumulative sums are converted to gauges, here are some ways to query your data: Example: Raw gauge value for cumulative sums To view the raw gauge value for cumulative sums, you can use the latest() NRQL function: SELECT latest(totalApiBytesSent) FROM Metric Copy Example: Rate of change with cumulative sums as gauges To see the rate of change over a given time interval for a cumulative sum stored as a gauge, you can use the derivative() NRQL function: SELECT derivative(totalApiBytesSent, 1 minute) FROM Metric Copy New Relic does not currently support either reporting on resets and gaps or accounting for them with cumulative counters. Query gauge metrics When New Relic converts cumulative sums to gauges, you can query them using either the latest() or derivative() NRQL functions. The function you choose depends on whether you want to see the raw value or compute the rate of change. Query histogram metrics New Relic histograms translated from OpenTelemetry metrics have the same query semantics as other New Relic histograms. Namely, the histogram() NRQL function can be used to represent the histogram with a configurable number of buckets and bucket width. Note that you may see larger bucket counts at the endpoint buckets. This is because we are adding negative and positive infinity bound OpenTelemetry buckets into a zero width New Relic bucket. Example: Normal distribution FROM Metric SELECT histogram(test.histogram, buckets: 100, width: 1000) WHERE distributionType = 'Normal Distribution' SINCE 1 day ago Copy Example: Heat map The FACET keyword is also available to create heat map charts. FROM Metric SELECT histogram(test.histogram, buckets: 100, width: 1000) FACET distributionType SINCE 1 day ago Copy Important The TIMESERIES keyword is not supported for New Relic histograms. Logs Logs generated from your applications and environment are an important piece of telemetry. They may represent application logs, machine generated events, or system logs. OpenTelemetry has defined a log data model for representing log data. You can send logs using OpenTelemetry tooling, correlate them with applications, and view them in New Relic. Send logs to New Relic The OpenTelemetry Collector and OpenTelemetry Collector Contrib repositories contain a number of components for consuming log data. The general pattern is to configure the collector to: Receive logs from any of the log receivers. Some of the receiver options include Filelog Receiver, Fluent Forward Receiver, and Syslog Receiver. Process logs, potentially annotating them with resource information. Some of the processor options include Resource Detection Processor and Resource Processor. Export logs to New Relic via the OTLP exporter. Application log correlation Application logs are more useful if they're correlated with other telemetry data produced by the application. The OpenTelemetry semantic convention for services specifies service.name as a required field. All application metric, trace, and log data sent to New Relic with the same service.name are associated with the same entity. The specifics of how logs get annotated with the service.name resource attribute depends on the application's environment: Applications may produce structured JSON logs, which you can configure to include service.name as another field. You can deploy applications alongside a dedicated Collector Agent instance, which you can configure with a Resource Processor to annotate logs with the service.name attribute. Optionally, additional application trace context (sometimes called execution context) can be propagated to log messages. The setup and availability of this depends on the language and logging framework used by the application. The general strategy is to set up the application to write structured JSON logs and to configure it to extract trace context into specified trace context fields on available log messages. The Logs in Context with Log4j2 example in GitHub demonstrates an end-to-end working example for a simple Java application using Log4j2. View OpenTelemetry logs Here are two ways you can view logs: Look in the New Relic Logs UI. If your logs are correlated with an application, view them in the context of the application. The time field The timeUnixNano field is optional according to the OpenTelemetry specification for log data. When timeUnixNano is not present New Relic will use the time that the data was received for the New Relic log timestamp.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 281.67545,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Best practices for <em>OpenTelemetry</em> with New Relic",
        "sections": "Best practices for <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to analyze all your traces, make sure to verify that your <em>OpenTelemetry</em> <em>integrations</em> use the AlwaysOn sampler. Metrics <em>OpenTelemetry</em> metrics are largely compatible with New Relic dimensional metrics. We support <em>OpenTelemetry</em> metrics v0.10. All of the supported metric types include an independent set"
      },
      "id": "60f6b9b964441f5da847ac01"
    },
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Find your service (entity)",
        "Narrow down your data with filters",
        "Summary page",
        "Distributed tracing",
        "Find traces and their associated spans",
        "Tip",
        "View spans with errors",
        "View span events",
        "Databases",
        "Errors",
        "Externals",
        "JVMs",
        "Logs",
        "Metrics explorer",
        "Transactions",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/490779aac8c10802682d86f0b785b6a2/a4078/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-10-07T01:52:56Z",
      "updated_at": "2021-09-27T15:53:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information about your services. It offers you a way to filter services and then filter the data to show a variety of views into your data. Find your service (entity) To get started in Explorer, you need to find your service: Go to one.newrelic.com. In the left sidebar, click Services - OpenTelemetry: In the center pane, click the service you want to know more about, or find your service by entering the name in the top filter bar. Narrow down your data with filters Once you have your entity, you can then filter for data from your service. With the filter bar Narrow data to..., you can highlight a specific facet of the telemetry recorded for your service. For example, you may want to see the error rate for a particular version of the service that you've deployed in a canary instance, so you add a filter for service.version='1.2.3'. Filters are preserved when navigating between different views of your data for a service. For example, the filter for service.version='1.2.3' carries over to the Transactions view, so that you would see telemetry on requests to the endpoints (transactions) that are running version 1.2.3 of your service, and not any other versions. Filters are preserved when navigating between the Summary, Transactions, Databases, Externals, Errors, and JVMs views. Filters are also preserved when navigating to the Distributed tracing view, but with limitations. Only filter conditions that use the equals operator (\"=\") are currently supported when navigating to Distributed tracing. If you navigate back from the Distributed tracing page, the filters you selected on the previous view will come back. Once you filter your data, the UI has various views in the left-navigation pane. See our descriptions below for view details. Summary page After you click on a service in the Explorer tab, you see the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. For your data to appear in this section, make sure it has the following: UI area Attribute Response time span.kind = server or consumer Throughput span.kind = server or consumer Error rate span.kind = server or consumer otel.status_code = ERROR Service instances pane service.instance.id (usually set via the OTel resource API) Distributed tracing In Distributed tracing, you can locate traces and examine span details. For your trace data to appear in the New Relic UI, it needs to conform to the OpenTelemetry trace semantic conventions. Also, in the Traces section of our best practices guide, you can find some tips about making sure your traces and spans appear in New Relic. The following attributes are typically not added explicitly to spans. Rather, they are usually set when creating a span or performing operations (for example, recording an error) on a span using the OpenTelemetry SDK. Attribute Description name A span’s name is generally set when starting a span. The name can generally be anything, but the OpenTelemetry specification provides guidance for certain types of spans like Database spans or HTTP spans. span.kind A span’s kind is generally set when starting a span. New Relic uses span.kind to infer that a span is an entry point to a given service. When span.kind = server or consumer it is considered an entry point. When span.kind = client or producer, it is considered a call to an external service or database system. otel.status_code A span’s status is set using the span API. The otel.status_code attribute is how it is manifested by New Relic. The UI primarily uses otel.status_code for the purpose of identifying errors. Find traces and their associated spans Here are some ways to target your searches: To find the traces you want, you can run queries like these in the filter bar: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID When you find an interesting trace, click on it to display a waterfall diagram showing the spans of the trace. Click on specific spans within the trace to display span details in the right panel. Tip For more ways to filter traces, see our distributed tracing UI page. View spans with errors After you click on a span in the trace waterfall view, you can see span errors in the right panel under Error details. The error details are populated by spans containing otel.status_code = ERROR and display the content of otel.status_description. To narrow your search for spans with errors, you can enter otel.status_code = ERROR directly in the distributed tracing filter bar. View span events If you send span events as described by the OpenTelemetry specification, you can view them in the New Relic UI. Span events have two general types: Exceptions Non-exceptions (for example, logs) If you have span events, links for these appear in the right pane: Click on a span in the waterfall view. In the right pane, click View span events, or you can expand Error details and click the link at the bottom of the errors. When you're in span events and only want to view exceptions, slide the toggle Only show exceptions. Tip OpenTelemetry exceptions handled by the app/service are displayed independently of span error status and are not necessarily associated with a span error status. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. For your data to appear in this section, make sure it has the following: UI area Attribute Top database calls span.kind = client or producer db.system Facets by span name Top databases (by query time) span.kind = client or producer db.system Facets by db.system Top databases (by throughput) span.kind = client or producer db.system Facets by db.system Errors On the Errors page, you can see total errors as well as charts showing error count and error rate. For your data to appear in this section, make sure it has the following: span.kind = server or consumer otel.status_code = ERROR Facets by span name Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. For your data to appear in this section, make sure it has the following: span.kind = client or producer db.system is not present JVMs When you drill into a specific JVM, the UI display charts driven by JVM metric data: JVM metrics follow the general semantic conventions for runtime environment metrics. The Java specific runtime metrics are not well documented. The implementation is effectively the documentation and may be subject to change. For your data to appear in this section, make sure it has the following: Requires a unique service.instance.id attribute for rendering the list of JVMs Service.instance.id is an OpenTelemetry resource attribute Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. For your data to appear in this section, make sure it has the following: service.name To correlate with trace data, the logs should contain the trace.id and span.id attributes. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. For your data to appear in this section, make sure it has the following: UI area Description Top Transactions span.kind = server or consumer Facets by span name Throughput span.kind = server or consumer Facets by span name Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 279.01938,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic Explorer tab is a good place to start gathering information"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic": [
    {
      "sections": [
        "Introduction to OpenTelemetry with New Relic",
        "Benefits of OpenTelemetry",
        "Should I use OpenTelemetry instrumentation or New Relic agents?",
        "OpenTelemetry: A work in progress",
        "New Relic's APM agents",
        "How OpenTelemetry works with New Relic",
        "Important",
        "Traces",
        "Metrics",
        "Logs",
        "Next steps"
      ],
      "title": "Introduction to OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "c87898d2d5835c00930c173eabd1bf93040badea",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/introduction-opentelemetry-new-relic/",
      "published_at": "2021-10-07T00:14:26Z",
      "updated_at": "2021-09-27T15:16:23Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Are you already familiar with OpenTelemetry and want to begin the setup? Check out our quick start: Quick start If you don't have one already, create a New Relic account. It's free, forever. If you're just getting acquainted with OpenTelemetry, this is what we'll explore here: Benefits of OpenTelemetry Should I use OpenTelemetry or New Relic agents? How OpenTelemetry works with New Relic Benefits of OpenTelemetry OpenTelemetry provides a secure, vendor-neutral specification for service instrumentation so that you can export data to distinct backends of your choice, such as New Relic. OpenTelemetry offers a single set of APIs and libraries that standardize how you collect and transfer telemetry data for your services. The following components make up the OpenTelemetry project: Specifications for the core pillars of observability to drive consistency across all projects. New Relic supports all of these signals: traces, metrics, and logs (see details below) APIs that contain interfaces and implementations based on the specifications SDKs (reference implementations of the APIs) created specifically for languages like Java, Python, Go, Erlang, and more Collectors that offer a vendor-agnostic implementation for processing and exporting Exporters that enable you to send data to a backend of your choice The components of OpenTelemetry work together to create some distinct advantages for capturing telemetry data: Feature Description Ubiquitous instrumentation A single, open standard of instrumentation provides better coverage and flexibility as engineers from all over the world contribute to the instrumentation. Future proof As the instrumentation gets built into libraries and frameworks, and as more vendors move to support this open standard, you can be confident that you won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build integrations into OpenTelemetry or add instrumentation directly to source code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need to decide which instrumentation option to use (a proprietary option or one of the other open standards). Cross-platform compatibility OpenTelemetry supports a variety of languages and backends. It represents a vendor-neutral path for capturing and transmitting telemetry to backends without altering existing instrumentation. Streamlined observability It is easier for vendors to support and test against a single standard as they don’t need to develop their own agents or collectors. High dimensionality OpenTelemetry uses dimensional metrics, so you can filter and facet on more aspects of the data, such as AWS regions, Kubernetes clusters, or service versions. Dimensional metrics also lead to less time between occurrence and reporting. Efficiency OpenTelemetry’s fire-and-forget trace-centric approach to instrumentation often has lower overhead than New Relic agents, especially for asynchronous workloads. It will also result in better handling of trace data for asynchronous requests. Should I use OpenTelemetry instrumentation or New Relic agents? As you consider OpenTelemetry, you may also be looking at New Relic APM agents that also capture telemetry data. As you'd expect, there is a lot of overlap between features available from OpenTelemetry agents and SDKs versus those available from New Relic APM agents. This is especially true if you're interested in distributed tracing telemetry data. The choice you make depends on what you need. We recommend that you explore both New Relic and OpenTelemetry instrumentation or discuss this with us in our CNCF Slack channel to decide what works best for you. OpenTelemetry: A work in progress OpenTelemetry is still an emerging standard, so your choices may be affected by what's available. You can check on the current state of the specification at the OpenTelemetry site. The current state of language-specific OpenTelemetry APIs and SDKs varies: some languages are still pre-alpha and may be missing instructions on how to instrument your service. Most languages have some implementation of traces that is sufficient to start exporting data to New Relic. Check out this table in GitHub that provides an overview of the state of OpenTelemetry specification compliance for each language. For languages that New Relic does not currently provide an agent or SDK, OpenTelemetry may offer you a good alternative. Also, in cases where you want explicit control over sampling of your telemetry data, OpenTelemetry provides a lot of flexibility. As OpenTelemetry matures, New Relic will continue to support new OpenTelemetry data models and to provide a curated UI experience for our Full Stack Observability customers. New Relic's APM agents In general, New Relic's APM agents will collect more telemetry data for your services, and they offer a wide range of configuration options and an extensive set of auto-instrumentation capabilities. New Relic's APM agents offer detailed transaction trace visibility for individual services. They also offer predefined sampling to balance the performance impact of your instrumentation against the need to capture enough data to gain helpful insights. How OpenTelemetry works with New Relic New Relic supports the native OpenTelemetry Protocol (OTLP) for exporting telemetry data. This allows you to use the vendor neutral components developed by the OpenTelemetry community to export your data to New Relic. The following tables show the supported features for each telemetry signal. If you have questions about these or have an unsupported use case, please contact us in our CNCF Slack channel, and watch this page for future updates. Important New Relic's exporters for OpenTelemetry are now deprecated in favor of exporting data to New Relic using OTLP. Traces New Relic offers support for the OTLP ingest of trace signals. The maturity of the upstream specification is stable. OpenTelemetry traces and spans are compatible with New Relic traces and spans. OpenTelemetry spans optionally include attributes (name-value pairs) and resource attributes which map directly to dimensions that can be used to facet or filter span data at query time. OpenTelemetry span metadata (for example, name, kind, and trace_id) also map directly to dimensions on NewRelic spans. At this time, New Relic does not support span links or array attributes. For details, see the Traces section of our best practices guide. Feature Supported Span events ✅ Span linking ❌ Array of primitives (homogeneous) ❌ Metrics New Relic offers support for the OTLP ingest of metric signals. Note that the maturity of the upstream specification is experimental. We intend to follow potentially breaking upstream changes. Here are the OpenTelemetry data types we support and their associated mappings. For details, see the Metrics section of our best practices guide. Metric Type Supported Delta sums ✅ Cumulative sums ✅ Gauges ✅ Delta histograms ✅ Summary ✅ Cumulative histograms ❌ Exemplars ❌ Array of primitives (homogeneous) ❌ Logs New Relic offers support for the OTLP ingest of log signals. Note that the maturity of the upstream specification is experimental. We intend to follow potentially breaking upstream changes. OpenTelemetry logs are compatible with New Relic logs. OpenTelemetry logs optionally include attributes (name-value pairs) and resource attributes which map directly to dimensions that can be used to facet or filter log data at query time. OpenTelemetry log metadata (for example, name, severity_text, and trace_id) also map directly to dimensions on New Relic logs. NewRelic currently supports all OpenTelemetry log message types except for arrays. For more details, see the Logs section of our best practices guide. Feature Supported Description LogRecord body ✅ Supported types: string, boolean, int, double, bytes LogRecord attributes ✅ Supported types: string, boolean, int, double, bytes LogRecord fields ✅ Examples: name, severity_text, trace_id Array messages ❌ Array attributes ❌ Next steps Follow the OpenTelemetry quick start to help you get started. You'll also want to review the best practices guide for getting the most out of the data you export to New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 282.14334,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "sections": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " won’t need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build <em>integrations</em> into <em>OpenTelemetry</em> or add instrumentation directly to <em>source</em> code, ensuring end users can easily monitor these new technologies. Simplified choice You don’t need"
      },
      "id": "603e81ba196a67304da83dab"
    },
    {
      "sections": [
        "Best practices for OpenTelemetry with New Relic",
        "Resources",
        "Batching",
        "Caution",
        "Compression",
        "Traces",
        "Required fields",
        "Sampling",
        "OpenTelemetry built-in samplers",
        "OpenTelemetry tail-based samplers",
        "New Relic tail-based sampling with Infinite Tracing",
        "Important",
        "Metrics",
        "Sum metrics",
        "Delta sums",
        "Cumulative sums",
        "Sum configuration examples",
        "Gauge metrics",
        "Histogram metrics",
        "Summary metrics",
        "Start time",
        "Array values for attributes",
        "Exemplars",
        "How to query metrics",
        "Query cumulative sums stored as gauges",
        "Example: Raw gauge value for cumulative sums",
        "Example: Rate of change with cumulative sums as gauges",
        "Query gauge metrics",
        "Query histogram metrics",
        "Example: Normal distribution",
        "Example: Heat map",
        "Logs",
        "Send logs to New Relic",
        "Application log correlation",
        "View OpenTelemetry logs",
        "The time field"
      ],
      "title": "Best practices for OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "2b624c5862a5a48d088268eccb310510f372b125",
      "image": "https://docs.newrelic.com/static/764474b6404e7a5c65e55d2eb985e93c/c1b63/sum-derivative-function.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-concepts/",
      "published_at": "2021-10-07T01:54:01Z",
      "updated_at": "2021-09-27T14:48:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Here are some best practices based on how OpenTelemetry works with New Relic: Resources Batching Compression Traces Metrics Logs Resources A resource in OpenTelemetry represents information about an entity generating telemetry data. All telemetry data sent to New Relic is expected to be associated with a resource so that it can be linked with the appropriate entity in New Relic. The OpenTelemetry Resource SDK specification defines the functionality implemented by all language SDKs for defining a resource. The following suites of attributes are defined by the OpenTelemetry resource semantic conventions. These attributes are usually set by creating a resource using the OpenTelemetry SDK. service.* attributes service.name attribute is required to associate your resource with an entity in the UI service.instance.id is required for certain panes to light up telemetry.sdk.language=java is required to see data in the JVM section Batching Caution Avoid getting rate limited! You should batch requests sent to the OTLP endpoint as described in this section. By default, the OpenTelemetry SDKs and Collector send one (1) data point per request. Using these defaults, it is likely your account will be rate limited. All OpenTelemetry SDKs and Collectors provide a BatchProcessor, which batches data points in memory. This batching allows requests to be sent with more than one (1) data point. Component Batch Processor Collector Batch Processor Go SDK BatchSpanProcessor JS SDK BatchSpanProcessor Python SDK BatchExportSpanProcessor Compression New Relic supports gzip compression for OTLP payloads exported over gRPC. To maximize the amount of data you can send per request, we recommend enabling compression in all OTLP exporters. If there are other compression formats you'd like to see us support, please let us know in the CNCF Slack channel. Traces Familiarize yourself with these trace topics to ensure your traces and spans appear in New Relic. Required fields The startTimeUnixNano and endTimeUnixNano fields on spans are required according to the OpenTelemetry protocol for trace data. When startTimeUnixNano is not present, the span is dropped and a NrIntegrationError is created. When endTimeUnixNano is not present, the duration of your span is large and negative. The timeUnixNano field on span events is required. When timeUnixNano is not present, the span event is dropped and a NrIntegrationError is created. The traceId and spanId fields on spans are required according to the OpenTelemetry protocol for trace data. When traceId or spanId are not present, the span is dropped and a NrIntegrationError is created. Sampling Trace data is the most mature OpenTelemetry data type. Because of this, New Relic's OpenTelemetry user experience is largely based on trace data and is therefore influenced by your sampling strategy. You can configure sampling in a number of places: Service: Use the OpenTelemetry SDK for your language. Collector: If you're running your own instance of the OpenTelemetry collector, you can configure it to do more sophisticated forms of sampling, such as tail-based sampling (see below). Check out this documentation about how to configure different types of sampling: OpenTelemetry built-in samplers Built-in samplers implemented by the OpenTelemetry SDK for each language. OpenTelemetry tail-based samplers The OpenTelemetry collector has a tail-based sampling processor. We have an example demonstrating the use of the tail-based sampling processor. New Relic tail-based sampling with Infinite Tracing Infinite Tracing is New Relic's tail-based sampling option. You can use this in conjunction with your OpenTelemetry instrumented services. In setting up Infinite Tracing, you need to override the default span endpoint and send telemetry data to the New Relic trace observer: Important Currently, Infinite Tracing does not support OTLP ingest. You must run your own instance of the OpenTelemetry Collector and configure it to use the New Relic exporter. Follow the steps in Set up the trace observer to get the value for YOUR_TRACE_OBSERVER_URL. Use the value of YOUR_TRACE_OBSERVER_URL to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your OpenTelemetry integrations use the AlwaysOn sampler. Metrics OpenTelemetry metrics are largely compatible with New Relic dimensional metrics. We support OpenTelemetry metrics v0.10. All of the supported metric types include an independent set of associated attributes (name-value pairs) which map directly to dimensions you can use to facet or filter metric data at query time. OpenTelemetry metrics are accompanied by a set of resource attributes that identify the originating entity that produced them and map to dimensions for faceting and filtering. The OpenTelemetry data model for metrics defines a number of different metric types: sum, gauge, histogram, and summary. Sum metrics OpenTelemetry sums are a scalar metric that is the sum of all data points over a given time window. Sums have a notion of temporality indicating whether reported values incorporate previous measurements (cumulative temporality) or not (delta temporality). In addition, sums can either be monotonic (only go up or only go down) or non-monotonic (go up and down). Delta sums In New Relic, delta metrics are handled differently depending on whether they are monotonic or non-monotonic: Monotonic delta sums are mapped to the count metric type. Non-monotonic delta sums are mapped to the gauge metric type. Cumulative sums Monotonic and non-monotonic cumulative sums are mapped to the New Relic gauge metric type. Sum configuration examples To understand how to configure aggregation temporality, see these examples using the Java and Go OpenTelemetry SDKs. Gauge metrics OpenTelemetry gauge metric data points represent a sampled value at a given time. These values are converted to the New Relic gauge metric type. OpenTelemetry gauges do not have an aggregation temporality, but the sampled values can be aggregated at query time. Histogram metrics OpenTelemetry histograms compactly represent a population of recorded values along with a total count and sum. Optionally, histograms may include a series of buckets with explicit bounds and a count value for that bucket’s population. OpenTelemetry histograms are converted to New Relic’s distribution metric type, which is backed by a scaled exponential base 2 histogram (see NrSketch for a more thorough explanation). Counts from OpenTelemetry histogram buckets are assigned to New Relic’s distribution metric buckets using linear interpolation. Also, OpenTelemetry has negative and positive infinity bound buckets which we represent in New Relic as zero-width buckets. We do this because we do not have a representation for negative and positive infinity. For example, an OpenTelemetry bucket with bounds [-∞, 10) will be represented by a [10,10) zero width New Relic bucket. You may see exaggerated bucket counts at the endpoints of your distribution due to this translation. Summary metrics OpenTelemetry summary metric data points are used to represent quantile summaries (for example, P99 latency). These map directly to the New Relic summary metric type. Summary metric data points include count, sum, and quantile values, with 0.0 as min and 1.0 as max. OpenTelemetry provides summary metrics for compatibility with other formats. Start time The startTimeUnixNano field is optional according to the OpenTelemetry specification. When this field is provided, it is used for the timestamp on the resulting NewRelic metric, and the duration is calculated as timeUnixNano - startTimeUnixNano. The duration field is used to calculate the queryable endTimeStamp attribute on the New Relic metric, but it serves no other semantic purpose. If startTimeUnixNano is not provided, then timeUnixNano is used for the timestamp field on the resulting NewRelic metric, and the duration field is set to zero. Array values for attributes OpenTelemetry metrics and other signals may include attributes that consist of a homogenous array of primitive types. These attributes are not supported by New Relic. Exemplars OpenTelemetry defines exemplar values that allow other signals, like traces, to be connected to a metric event and provide context. Exemplars are not supported by New Relic. How to query metrics Consider these tips for building metric NRQL queries in New Relic. Query cumulative sums stored as gauges Since cumulative sums are converted to gauges, here are some ways to query your data: Example: Raw gauge value for cumulative sums To view the raw gauge value for cumulative sums, you can use the latest() NRQL function: SELECT latest(totalApiBytesSent) FROM Metric Copy Example: Rate of change with cumulative sums as gauges To see the rate of change over a given time interval for a cumulative sum stored as a gauge, you can use the derivative() NRQL function: SELECT derivative(totalApiBytesSent, 1 minute) FROM Metric Copy New Relic does not currently support either reporting on resets and gaps or accounting for them with cumulative counters. Query gauge metrics When New Relic converts cumulative sums to gauges, you can query them using either the latest() or derivative() NRQL functions. The function you choose depends on whether you want to see the raw value or compute the rate of change. Query histogram metrics New Relic histograms translated from OpenTelemetry metrics have the same query semantics as other New Relic histograms. Namely, the histogram() NRQL function can be used to represent the histogram with a configurable number of buckets and bucket width. Note that you may see larger bucket counts at the endpoint buckets. This is because we are adding negative and positive infinity bound OpenTelemetry buckets into a zero width New Relic bucket. Example: Normal distribution FROM Metric SELECT histogram(test.histogram, buckets: 100, width: 1000) WHERE distributionType = 'Normal Distribution' SINCE 1 day ago Copy Example: Heat map The FACET keyword is also available to create heat map charts. FROM Metric SELECT histogram(test.histogram, buckets: 100, width: 1000) FACET distributionType SINCE 1 day ago Copy Important The TIMESERIES keyword is not supported for New Relic histograms. Logs Logs generated from your applications and environment are an important piece of telemetry. They may represent application logs, machine generated events, or system logs. OpenTelemetry has defined a log data model for representing log data. You can send logs using OpenTelemetry tooling, correlate them with applications, and view them in New Relic. Send logs to New Relic The OpenTelemetry Collector and OpenTelemetry Collector Contrib repositories contain a number of components for consuming log data. The general pattern is to configure the collector to: Receive logs from any of the log receivers. Some of the receiver options include Filelog Receiver, Fluent Forward Receiver, and Syslog Receiver. Process logs, potentially annotating them with resource information. Some of the processor options include Resource Detection Processor and Resource Processor. Export logs to New Relic via the OTLP exporter. Application log correlation Application logs are more useful if they're correlated with other telemetry data produced by the application. The OpenTelemetry semantic convention for services specifies service.name as a required field. All application metric, trace, and log data sent to New Relic with the same service.name are associated with the same entity. The specifics of how logs get annotated with the service.name resource attribute depends on the application's environment: Applications may produce structured JSON logs, which you can configure to include service.name as another field. You can deploy applications alongside a dedicated Collector Agent instance, which you can configure with a Resource Processor to annotate logs with the service.name attribute. Optionally, additional application trace context (sometimes called execution context) can be propagated to log messages. The setup and availability of this depends on the language and logging framework used by the application. The general strategy is to set up the application to write structured JSON logs and to configure it to extract trace context into specified trace context fields on available log messages. The Logs in Context with Log4j2 example in GitHub demonstrates an end-to-end working example for a simple Java application using Log4j2. View OpenTelemetry logs Here are two ways you can view logs: Look in the New Relic Logs UI. If your logs are correlated with an application, view them in the context of the application. The time field The timeUnixNano field is optional according to the OpenTelemetry specification for log data. When timeUnixNano is not present New Relic will use the time that the data was received for the New Relic log timestamp.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 281.67545,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Best practices for <em>OpenTelemetry</em> with New Relic",
        "sections": "Best practices for <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to analyze all your traces, make sure to verify that your <em>OpenTelemetry</em> <em>integrations</em> use the AlwaysOn sampler. Metrics <em>OpenTelemetry</em> metrics are largely compatible with New Relic dimensional metrics. We support <em>OpenTelemetry</em> metrics v0.10. All of the supported metric types include an independent set"
      },
      "id": "60f6b9b964441f5da847ac01"
    },
    {
      "sections": [
        "OpenTelemetry quick start",
        "Step 1. Prerequisites",
        "Step 2. Instrument your service with OpenTelemetry",
        "Step 3. Export your telemetry data to New Relic",
        "Review New Relic settings for exports",
        "Important",
        "Complete the export configuration steps",
        "Export data to an OpenTelemetry Collector (optional)",
        "Step 4. View your data in the New Relic UI",
        "What's next?"
      ],
      "title": "OpenTelemetry quick start",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "1b846417a2958b61b047c838db49aea06f09a2a8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-quick-start/",
      "published_at": "2021-10-07T01:54:02Z",
      "updated_at": "2021-09-27T14:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "OpenTelemetry is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up OpenTelemetry with New Relic. Here's an overview of the process, followed by details for each step. Prerequisites Instrument your service with OpenTelemetry Export your telemetry data to New Relic View your data in the New Relic UI Step 1. Prerequisites First things first: If we don’t already know you, sign up for a free New Relic account. Copy your account license key. Step 2. Instrument your service with OpenTelemetry To get started, you instrument your service with OpenTelemetry. OpenTelemetry has language-specific products and SDKs to help you. Many languages offer out-the-box instrumentation for common libraries and frameworks. Each language also provides an API for further instrumenting your service manually. Go to the repository for your language and follow the instructions to instrument your service. When you're done, return here to complete Step 3. Export your telemetry data to New Relic. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...See a complete list of languages in GitHub Step 3. Export your telemetry data to New Relic The OpenTelemetry Protocol, or OTLP for short, is a general purpose telemetry data delivery protocol designed for the OpenTelemetry project. This protocol describes how to encode and transmit telemetry data, which makes it a natural choice for data transport. Each language SDK provides an OTLP exporter you can configure to export data over OTLP. In this step, we focus on how to configure an OTLP exporter in your service to export data directly to New Relic. If you prefer to export your data first to an OpenTelemetry collector, we have separate instructions. Here's an example of sending data from your service directly to New Relic. To complete this third step, first familiarize yourself with some required New Relic settings, and then complete the steps in the OTLP exporter documentation for your language. Review New Relic settings for exports Before you go to the external OTLP exporter documentation, consult the table below so you're ready to do the following: Configure the OTLP exporter to add a header ( api-key ) whose value is your account license key. Based on your region, configure the endpoint where the exporter sends data to New Relic. Region gRPC HTTP/1.1 Endpoint API Header Name API Header Value TLS encryption required US ✅ ❌ https://otlp.nr-data.net:4317 api-key License Key ✅ EU ✅ ❌ https://otlp.eu01.nr-data.net:4317 api-key License Key ✅ If you have FedRamp compliance constraints, see FedRAMP-compliant endpoints. Important In Node.js, the opentelemetry-collector-grpc library requires additional options to enable TLS. Complete the export configuration steps Click on the link below for your language and complete the configuration steps. When you're done, return here to complete Step 4. View your data in the New Relic UI. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...Find additional OTLP language support in GitHub Export data to an OpenTelemetry Collector (optional) The OpenTelemetry Collector is a configurable and extensible software component to receive, process, and export telemetry data. When you set up a collector, it can operate as a gateway or as an agent: Gateway: The collector receives data from a variety of sources and applies standard processing before exporting to some backend. Agent: The collector is deployed on each host in an environment and can collect telemetry data about the host and processes running on it. When you use a collector, you start by following the same routine as above for setting up OTLP in your service. In this case, instead of exporting data directly to New Relic, you export through a collector that you set up. In the collector, you configure the OTLP exporter to export data to New Relic. When your data goes through a collector, the transport looks like this: Here's a Docker example of how to set up and run an OpenTelemetry collector with the collector YAML: Save the following as otel-config.yaml: receivers: otlp: protocols: grpc: http: processors: batch: exporters: otlp: endpoint: ${OTEL_EXPORTER_OTLP_ENDPOINT} headers: api-key: ${NEW_RELIC_LICENSE_KEY} service: pipelines: traces: receivers: [otlp] processors: [batch] exporters: [otlp] metrics: receivers: [otlp] processors: [batch] exporters: [otlp] logs: receivers: [otlp] processors: [batch] exporters: [otlp] Copy Run the OpenTelemetry collector, making sure you replace OTLP_ENDPOINT_HERE with the appropriate endpoint and replace YOUR_KEY_HERE with your Account License Key: export OTEL_EXPORTER_OTLP_ENDPOINT=OTLP_ENDPOINT_HERE export NEW_RELIC_LICENSE_KEY=YOUR_KEY_HERE docker run --rm \\ -e OTEL_EXPORTER_OTLP_ENDPOINT \\ -e NEW_RELIC_LICENSE_KEY \\ -p 4317:4317 \\ -v \"${PWD}/otel-config.yaml\":/otel-config.yaml \\ --config otel-config.yaml \\ --name otelcol \\ otel/opentelemetry-collector Copy Step 4. View your data in the New Relic UI Once you’ve instrumented your service and configured it to export its data to New Relic, watch the New Relic One user interface for your traces, metrics, and logs! The UI for OpenTelemetry has some similarities to the APM agent UI, so if you are familiar with that, you can go right to the UI. If you need help understanding your OpenTelemetry UI options or how to make sure your data appears in the UI, see View your OpenTelemetry data in New Relic. What's next? After you do your initial setup, check out our best-practices guide for tips about various configurations to improve your use of OpenTelemetry and New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 280.89563,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> quick start",
        "sections": "<em>OpenTelemetry</em> quick start",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "<em>OpenTelemetry</em> is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up <em>OpenTelemetry</em> with New Relic. Here&#x27;s an overview of the process, followed by details for each step. Prerequisites Instrument your service with <em>OpenTelemetry</em> Export"
      },
      "id": "6044e5dfe7b9d2aadc5799d4"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/roku/roku-open-source-video-agent": [
    {
      "sections": [
        "Elixir open-source agent",
        "Tip",
        "Get started",
        "For more help"
      ],
      "title": "Elixir open-source agent",
      "type": "docs",
      "tags": [
        "Agents",
        "Open-source licensed agents",
        "Open-source licensed agents"
      ],
      "external_id": "aa03e1693b6ecdd06fa2940ddb99187247743772",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/elixir/elixir-open-source-agent/",
      "published_at": "2021-10-08T06:38:20Z",
      "updated_at": "2021-04-27T11:09:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Monitor Elixir behavior with New Relic using the Elixir open-source agent. The agent: Helps you track transactions, distributed traces, and other parts of your application’s behavior Provides an overview of underlying BEAM activity Tip This agent is released as open source on GitHub. A change log is also available there for the latest updates. Get started For requirements, installation, and configuration information, see the Open Source Elixir Agent README on GitHub. Visit New Relic’s Elixir repository on GitHub for questions about installation, usage, or other topics. Report issues or bugs as an issue in the GitHub repository. For more help Recommendations for learning more: Browse New Relic's Explorers Hub for community discussions about the open-source Elixir agent. Review New Relic's licenses, attributions, data usage limits, and other notices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 348.995,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elixir <em>open</em>-<em>source</em> <em>agent</em>",
        "sections": "Elixir <em>open</em>-<em>source</em> <em>agent</em>",
        "tags": "<em>Open</em>-<em>source</em> <em>licensed</em> <em>agents</em>",
        "body": "Monitor Elixir behavior with New Relic using the Elixir <em>open</em>-<em>source</em> <em>agent</em>. The <em>agent</em>: Helps you track transactions, distributed traces, and other parts of your application’s behavior Provides an overview of underlying BEAM activity Tip This <em>agent</em> is released as <em>open</em> <em>source</em> on GitHub. A change log"
      },
      "id": "6087f0ff28ccbceab351c13f"
    },
    {
      "sections": [
        "IBM WebSphere Application Server"
      ],
      "title": "IBM WebSphere Application Server",
      "type": "docs",
      "tags": [
        "Agents",
        "Java agent",
        "Additional installation"
      ],
      "external_id": "e3c2ba33d026b8912f7dd2c1d8dfc00d63f25b9c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/agents/java-agent/additional-installation/ibm-websphere-application-server/",
      "published_at": "2021-10-07T11:16:32Z",
      "updated_at": "2021-10-07T11:16:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This describes how to configure New Relic's Java agent if you are using IBM WebSphere Application Server. For compatible versions, follow New Relic's procedures to install the Java agent on WebSphere. IBM WebSphere App Server Comments Compatible IBM JVM versions New Relic supports all versions of WebSphere that are compatible with the Java agent. However, certain versions of the IBM JVM are incompatible with the Java agent due to known issues in the IBM JVM. New Relic's Java agent is compatible with these major versions of the IBM JVM: 8: All versions Java 2 Security If you are using Java 2 Security and WebSphere, you must grant the Java agent additional permissions before it can execute properly. Browser monitoring To use browser monitoring when running the Java agent on WebSphere, you must manually enable browser monitoring. WebSphere PMI metrics You can configure the Java agent to capture additional WebSphere PMI metrics. These metrics will appear on the New Relic JVM metrics page. Instance and display names You can configure the Java agent to change the default behavior of instance names or display names.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 33.228363,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Agents</em>",
        "body": "This describes how to configure New Relic&#x27;s Java <em>agent</em> if you are using IBM WebSphere Application Server. For compatible versions, follow New Relic&#x27;s procedures to install the Java <em>agent</em> on WebSphere. IBM WebSphere App Server Comments Compatible IBM JVM versions New Relic supports all versions"
      },
      "id": "6043b986e7b9d279085799eb"
    },
    {
      "sections": [
        ".NET agent: compatibility and requirements for .NET Core",
        "Requirements",
        "Microsoft .NET Core version",
        "Target framework version",
        "Important",
        "App/web servers",
        "Operating system",
        "Microsoft Azure",
        "Processor architectures",
        "Permissions",
        "Security requirements",
        "Network requirements",
        "Automatic instrumentation",
        "App frameworks",
        "Datastores",
        "External call libraries",
        "Messaging",
        "Unavailable features",
        "Connect the agent to other New Relic products"
      ],
      "title": ".NET agent: compatibility and requirements for .NET Core",
      "type": "docs",
      "tags": [
        "Agents",
        "NET agent",
        "Getting started"
      ],
      "external_id": "ce9a62a0c6a2d98442118333f39152c38bf9fdda",
      "image": "",
      "url": "https://docs.newrelic.com/docs/agents/net-agent/getting-started/net-agent-compatibility-requirements-net-core/",
      "published_at": "2021-10-07T08:42:12Z",
      "updated_at": "2021-10-07T08:42:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's .NET agent supports both .NET Framework and .NET Core. This document describes compatibility and support for .NET Core applications. See Compatibility and requirements for .NET Framework for .NET Framework applications. New Relic's .NET agent includes built-in instrumentation for some of the most popular parts of the .NET Core ecosystem, including frameworks, databases, and message queuing systems. After installation, the agent runs within the monitored process; there is not a separate process or service created by the agent. For frameworks and libraries that are not automatically instrumented out of the box, you can extend the agent with .NET custom instrumentation. Want to try out our .NET agent? Create a New Relic account for free! No credit card required. Requirements Before you install the New Relic .NET agent on Windows or Linux, make sure your system meets these requirements: Microsoft .NET Core version The .NET agent supports .NET Core versions 2.0, 2.1, 2.2, 3.0, 3.1, and .NET 5.0. Table of minimum agent versions required per .NET Core version .NET Core Version Minimum Required .NET Agent Version .NET Core 2.0 > = 8.19.353.0 .NET Core 2.1 > = 8.19.353.0 .NET Core 2.2 > = 8.19.353.0 .NET Core 3.0 > = 8.21.34.0 .NET Core 3.1 > = 8.21.34.0 .NET 5.0 > = 8.35.0 The agent is not compatible with .NET Core versions 1.0 or 1.1. For .NET Core 2.1 or higher applications with tiered compilation enabled, the agent will disable tiered compilation. .NET Core 2.1 support requires .NET Core runtime 2.1.3 and .NET Core SDK 2.1.401 or higher due to a bug in the .NET Core profiling API. Target framework version The .NET agent only supports applications targeting .NET Core 2.0, 2.1, 2.2, 3.0, 3.1, and NET 5.0. You can find the target framework in your .csproj file: Supported: <TargetFramework>netcoreapp2.0</TargetFramework> Copy <TargetFramework>netcoreapp2.1</TargetFramework> Copy <TargetFramework>netcoreapp2.2</TargetFramework> Copy <TargetFramework>netcoreapp3.0</TargetFramework> Copy <TargetFramework>netcoreapp3.1</TargetFramework> Copy <TargetFramework>net5.0</TargetFramework> Copy Unsupported: <TargetFramework>net452</TargetFramework> Copy Important If you want to monitor an ASP.NET Core application targeting .NET Framework, ensure your install of the .NET agent has .NET Framework support enabled. App/web servers Ensure you use one of these app/web servers: Kestrel Kestrel with IIS reverse proxy via AspNetCoreModule Kestrel with IIS reverse proxy via AspNetCoreModuleV2 Kestrel with Nginx reverse proxy Kestrel with Apache reverse proxy Operating system The agent has been verified to work with the following operating systems: Operating system Supported versions Windows (32- and 64-bit Intel compatible architectures) Server 2008 R2 SP1 Server 2012 Server 2012 R2 Server 2016 Server 2019 Windows containers running on Server 2016 (NanoServer based images are not supported) Linux (64-bit Intel compatible only) All x64 Linux distributions supported by the .NET Core 2.0+/.NET 5 runtime are supported by the .NET agent. For a full list, refer to Microsoft's documentation for the version of the runtime you are using. Microsoft Azure For Azure-specific installation instructions, see: Install on Azure Cloud Services Install on Azure Service Fabric Install on Azure Web Apps Processor architectures The agent is available in both 32-bit (x86) and 64-bit (x64) versions on Windows as well as 64-bit (x64) on Linux. Permissions Installing and running the .NET agent requires these permissions: Component Necessary permissions Install the agent The process or user that installs the agent must have sufficient permissions to set environment variables and write access to the directory where the agent is installed. Run the agent The monitored process must have read/write access to the directory in which you installed the agent. The agent runs as a part of the monitored process and relies on those permissions to function. For applications using IIS via reverse proxy, the group IIS_IUSRS is often used. Security requirements As a standard security measure for data collection, your app server must support SHA-2 (256-bit). SHA-1 is not supported. Network requirements The agent requires your firewall to allow outgoing connections to specific networks and ports. Automatic instrumentation If your application is hosted in ASP.NET Core, the agent automatically creates and instruments transactions. The .NET agent will automatically instrument your application after install. If your app is not automatically instrumented, or if you want to add instrumentation, use custom instrumentation. App frameworks The .NET agent automatically instruments these application frameworks: ASP.NET Core MVC 2.0, 2.1, 2.2, 3.0, 3.1, and 5.0 (includes Web API) Datastores The .NET agent automatically instruments the performance of .NET application calls to these datastores: Datastore Instance details Notes Microsoft SQL Server Use System.Data.SqlClient version 4.3.1 or Microsoft.Data.SqlClient. PostgresSQL Use Npgsql 4.0. Prior versions of Npgsql may also be instrumented, but duplicate and/or missing metrics are possible. MongoDB Driver version 2.3.x - 2.13.x: The .NET agent will support instrumenting pre-exising and new API methods in 2.6.x, but not new methods introduced in 2.7.x and higher. MySQL Use MySql.Data version 6.10.4 or later, or use the MySQL Connector StackExchange.Redis The .NET agent does not directly monitor datastore processes. Also, the .NET SQL parameter capture in a query trace does not list parameters for a parameterized query or a stored procedure. Collecting instance details for supported datastores is enabled by default. To request instance-level information from datastores not currently listed, get support at support.newrelic.com. External call libraries The .NET agent automatically instruments these external call libraries : Libraries Supported methods HttpClient The agent instruments these HttpClient methods: SendAsync GetAsync PostAsync PutAsync DeleteAsync GetStringAsync GetStreamAsync GetByteArrayAsync Messaging The agent automatically instruments these message systems: RabbitMQ 5.1.0 or higher: Puts and takes on messages and queue purge. When receiving messages using an IBasicConsumer, the EventingBasicConsumer is the only implementation that is instrumented by the .NET agent. BasicGet is instrumented, but the agent does not support distributed tracing for BasicGet. The following methods are instrumented: IModel.BasicGet IModel.BasicPublish IModel.BasicComsume IModel.QueuePurge EventingBasicConsumer.HandleBasicDeliver Unavailable features The following features are not available for the .NET agent: Memory usage on Linux due to an ongoing .NET Core issue (try using .NET performance metrics to get this information) Automatic brower monitoring script injection (API or manual instrumentation is required) The .NET agent does not support trim self-contained deployments and executables, because the compiler can potentially trim assemblies that the agent depends on. Infinite Tracing is not supported on Alpine Linux due to a GRPC compatibility issue. See this agent issue for more information. Connect the agent to other New Relic products In addition to APM, the .NET agent integrates with other New Relic products to give you end-to-end visibility: Product Integration Browser monitoring The browser monitoring JavaScript agent will not be injected by the .NET agent for ASP.NET core applications. However, you can inject the browser agent by using the .NET agent API or the browser agent's copy/paste method. After enabling browser injection, you can view browser data in the APM Summary page and quickly switch between the APM and browser data for a particular app. For configuration options and manual instrumentation, see browser monitoring and the .NET agent. Infrastructure monitoring When you install the Infrastructure and APM agents on the same host, they automatically detect one another. You can then view a list of hosts in the APM UI, and filter your Infrastructure hosts by APM app in the Infrastructure UI. For more information, see APM data in Infrastructure. Dashboards The .NET agent sends default events and attributes for NRQL queries. You can also record custom events for advanced analysis.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 33.06512,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": ".NET <em>agent</em>: compatibility and requirements for .NET Core",
        "sections": ".NET <em>agent</em>: compatibility and requirements for .NET Core",
        "tags": "<em>Agents</em>",
        "body": " and manual instrumentation, see browser monitoring and the .NET <em>agent</em>. Infrastructure monitoring When you install the Infrastructure and APM <em>agents</em> on the same host, they automatically detect one another. You can then view a list of hosts in the APM UI, and filter your Infrastructure hosts by APM app"
      },
      "id": "603e8e7e196a675c41a83d9f"
    }
  ],
  "/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic": [
    {
      "sections": [
        "Requirements for the infrastructure agent",
        "Processor architectures",
        "Operating systems",
        "Unique hostname",
        "Permissions",
        "Libraries",
        "Network access",
        "Container software",
        "CPU, memory, and disk usage",
        "Configuration management tools"
      ],
      "title": "Requirements for the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Get started"
      ],
      "external_id": "517b5d94efa0139aa3ef5238569d5b04d28fb932",
      "image": "https://docs.newrelic.com/static/8de19e871ebba1c3d12258efc569dc6f/103b3/amazon-linux.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/get-started/requirements-infrastructure-agent/",
      "published_at": "2021-10-07T05:07:22Z",
      "updated_at": "2021-10-07T05:07:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before installing our infrastructure agent, make sure your system and any on-host integrations you configure meet the requirements. You also need a New Relic account. Sign up for free. No credit card required. Processor architectures The infrastructure agent supports these processor architectures: Linux: 64-bit for x86 processor architectures (also requires 64-bit package manager and dependencies) Windows: both 32 and 64-bit for x86 processor architectures ARM: arm64 architecture including AWS Graviton 2 processor is supported on compatible Linux operating sytems. On-host integrations are also supported (with the exception of the Oracle integration). Built-in log forwarding is not yet available. macOS (Beta): 64-bit x86 processor (M1 processor is not supported yet). Operating systems The infrastructure agent supports these operating systems up to their manufacturer's end-of-life. Operating system Supported by the infrastructure agent Amazon Linux All versions CentOS Version 7 or higher Debian Version 8 (\"Jessie\") or higher Docker Docker 1.12 Kubernetes Tested with versions 1.16 to 1.22 Red Hat Enterprise Linux (RHEL) Version 7 or higher Oracle Linux Version 7 or higher SUSE Linux Enterprise Server (SLES) Versions 11.4, 12.1, 12.2, 12.3, 12.4, 12.5, 15, 15.1, 15.2, 15.3 Ubuntu LTS versions 16.04.x, 18.04.x, 20.04.x Interim releases 20.10, 21.04. Windows Windows Server 2012, 2016, and 2019, and their service packs. Windows 10 and their service packs. macOS macOS 10.14 (Mohave), 10.15 (Catalina), 11 (Big Sur). You can also monitor Amazon BottleRocket workloads: When running EC2 instances, use the containerized agent. On EKS, install the Kubernetes integration. For ECS clusters, deploy the ECS integration. Unique hostname The infrastructure agent uses the hostname to uniquely identify each server. To avoid inaccurate metrics from combining multiple servers under a single hostname, make sure that each monitored server has a unique hostname. You can use the optional display_name setting to override the default hostname. Servers named localhost are not reported because this is a default name and inherently non-unique. Permissions The infrastructure agent requires these permissions: Linux: By default, the agent runs and installs as root. You can also select privileged or unprivileged run modes. Windows: The agent must be installed from an Administrator account and requires Administrator privileges to run. macOS: The agent can be installed from any user account. Libraries For agent versions 1.1.19 or higher, you need the libcap library in order to install Infrastructure. It's available in the official repositories of your distribution. Network access In order to report data to New Relic, our infrastructure agent must have outbound access to certain domains and ports. If your system needs a proxy to connect to these domains, use the proxy setting. Container software The infrastructure agent instruments Docker containers when installed on the host server. We support Docker versions 1.12 or higher. CPU, memory, and disk usage The infrastructure agent is fairly lightweight. For typical CPU, memory, and disk usage, see our page on agent performance overhead. For more information on supported file systems, see Storage sample attributes. Configuration management tools The infrastructure agent can be deployed programmatically using several config management and deploy tools: Ansible Chef Elastic Beanstalk Puppet",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 102.66864,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Get</em> <em>started</em>",
        "body": "Before installing our infrastructure agent, make sure your system and any on-host <em>integrations</em> you configure meet the requirements. You also need a New Relic account. Sign up for free. No credit card required. Processor architectures The infrastructure agent supports these processor architectures"
      },
      "id": "60440aca28ccbc8ce02c60cf"
    },
    {
      "sections": [
        "New Relic quickstarts overview",
        "Preview feature",
        "Why it matters",
        "What are quickstarts",
        "Some technical detail"
      ],
      "title": "New Relic quickstarts overview",
      "type": "docs",
      "tags": [
        "Full-Stack Observability",
        "Observe everything",
        "Get started",
        "Quickstarts",
        "Instant Observability"
      ],
      "external_id": "e12df6102b9361f953bdab2f4b49baa5756f7048",
      "image": "https://docs.newrelic.com/static/ec52d123f2853cc7fc4d3fab9f9f3be7/c1b63/quickstart-home.png",
      "url": "https://docs.newrelic.com/docs/full-stack-observability/observe-everything/get-started/new-relic-quickstarts-overview/",
      "published_at": "2021-10-06T23:40:45Z",
      "updated_at": "2021-10-06T22:17:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Monitor your tech stack without the burden of manual set up. New Relic I/O is a rich catalog of open source quickstarts - out-of-the-box bundles of integrations, dashboards, and alerts. Each quickstart is created by observability experts around the world, vetted by New Relic, and ready for you to install with one click. Leverage community expertise and get more value out of your telemetry data with New Relic I/O, your hub for instant observability. Ready to get started? Find your quickstart in New Relic I/O: New Relic I/O Preview feature Everyone has access to New Relic I/O for now. After the preview, only full users will be able to access the dashboards installed from a quickstart. Why it matters With our I/O catalog, you can choose from hundreds of quickstarts that bundle the necessary building blocks to get started with monitoring your technology stack; that includes instrumentation, integrations, dashboards, and alerts, all ready to install with a click. What are quickstarts Quickstarts are bundles of dashboards, alerts, and instrumentation that are ready to install with a single click, specific to each technology we support. Popular technologies such as Node.js, Python, and Ruby have full-featured quickstarts, while others contain a mixture of instrumentations, dashboards, and alerts. New Relic I/O is open source, which means that you can modify and improve existing quickstarts, or build new ones, to suit your needs. We thoroughly review external edit to our quickstarts for value and quality. Interested in contributing to the community? Check out our contributor guide. Some technical detail New Relic quickstarts use open source installation recipes to instrument integrations using our guided install process.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 97.91348,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Get</em> <em>started</em>",
        "body": " to install with one click. Leverage community expertise and <em>get</em> more value out of your telemetry data with New Relic I&#x2F;O, your hub for instant observability. Ready to <em>get</em> <em>started</em>? Find your quickstart in New Relic I&#x2F;O: New Relic I&#x2F;O Preview feature Everyone has access to New Relic I&#x2F;O for now. After"
      },
      "id": "6157008964441f500d099617"
    },
    {
      "sections": [
        "Kubernetes integration: compatibility and requirements",
        "Compatibility",
        "Requirements",
        "Install using Helm"
      ],
      "title": "Kubernetes integration: compatibility and requirements",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "e9bbd729904fa01739eb91e4f3c74561b51c2ba1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements/",
      "published_at": "2021-10-07T01:56:23Z",
      "updated_at": "2021-10-01T18:20:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our integration. Compatibility Our Kubernetes integration is compatible with the following versions, depending on the installation mode: Install mode or feature Kubernetes versions Kubernetes cluster Currently tested with versions 1.16 to 1.22 Kubernetes cluster GKE Currently tested with versions 1.17 to 1.19 Kubernetes cluster EKS (EC2 nodes or Fargate) Compatible with version 1.16 or higher Kubernetes cluster AKS Compatible with version 1.16 or higher Kubernetes cluster OpenShift Currently tested with versions 4.6 Kubernetes cluster VMware Tanzu Compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10 Control plane monitoring Compatible with version 1.16 or higher Service monitoring Compatible with version 1.16 or higher Requirements The New Relic Kubernetes integration has the following requirements: A New Relic account. Don't have one? Sign up for free. No credit card required. Linux distribution compatible with New Relic infrastructure agent. kube-state-metrics version 1.9.8 running on the cluster. When using CRI-O as the container runtime, the processes inside containers are not reported. Performance data is collected at the container level. Install using Helm For detailed instructions about how to install our integration using Helm, see Manual install using Helm.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 97.72237,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kubernetes <em>integration</em>: compatibility and requirements",
        "sections": "Kubernetes <em>integration</em>: compatibility and requirements",
        "tags": "<em>Get</em> <em>started</em>"
      },
      "id": "603e92dc64441f3a974e8891"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure/prometheus-high-availability-ha": [
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Set up the integration",
        "Map Prometheus and New Relic metric types",
        "Override metric type mappings",
        "Set allow or deny lists for sent metrics",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "e2a503880e8e1c38284434d5829fad3f48dc7abf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2021-10-08T06:45:33Z",
      "updated_at": "2021-09-08T01:33:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Map Prometheus and New Relic metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Set allow or deny lists for sent metrics If you need greater control over the data you send to New Relic, you can send a subset of your metrics. To do this, configure remote-write with the write_relabel_configs parameter with a subparameter action value of keep or deny. In this example, you'll only send the metrics that match the regular expression. Unmatched metrics won't be sent. Alternatively, you can use action: drop to drop all of the metrics that match the regular expression. - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy This Kubernetes example uses this Helm chart's values.yaml file. If you're using a different Helm chart, please check its remoteWrite documentation (for example, some Helm files use camelcase writeRelabelConfigs instead). remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 222.08727,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": "_server=YOUR_DATA_SOURCE_NAME Copy European Union accounts: If you&#x27;re connecting from the EU, use the following URL: https:&#x2F;&#x2F;metric-api.eu.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;<em>write</em> Copy Kubernetes and Helm <em>remote</em> <em>write</em> <em>integrations</em>: Add the <em>remote</em> <em>write</em> URL to your Helm values.yaml file. Replace <em>remoteWrite</em>: [] with two"
      },
      "id": "603e94de196a674e6ca83def"
    },
    {
      "sections": [
        "Remote write errors and error messages",
        "Common errors and issues",
        "Configuration errors",
        "400: bad request error",
        "413: request entity too large error",
        "429: rate limit error",
        "Investigate error messages"
      ],
      "title": "Remote write errors and error messages",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "0d190be5dc4fd91ce6bbcef7343d01f75670ca51",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/remote-write-errors-error-messages/",
      "published_at": "2021-10-08T06:45:33Z",
      "updated_at": "2021-08-08T19:27:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This resource contains information about common errors and error messages that may alert you to issues with data visibility and availability, as well as information about how to respond. Common errors and issues If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, there are several actions you can take to troubleshoot and get data flowing properly. Below are a few tips regarding common issues and error messages. For specific information on how to query NrIntegrationError events, see Investigate error messages below. Configuration errors Missing or incorrect characters in the remote write URL in the config file (for example the endpoint, license key, or prometheus_server name) or incorrect placement of the information in the file will result in the Prometheus server not starting, remote write not working properly, or errors appearing in Prometheus server logs. 400: bad request error If no data appears with a bad request error, check your configuration file to confirm that the placement of the remote write information is correct, and that there are no missing or incorrect characters. 413: request entity too large error This means you have sent a request in which one or more fields, or the entire payload, has exceeded our limits. 429: rate limit error This means you have hit a rate limit on the amount of data being sent at one time (for example cardinality or data points per minute). You can troubleshoot by reducing the amount of Prometheus or general metric data you are sending, or by requesting a rate-limit increase. Investigate error messages You can investigate error messages in New Relic by doing either or both of the following. Run a NRQL query of the NrIntegrationError event and examine the message attribute. Investigate individual errors in time to see when and where they occur and any simultaneously occurring issues, and perform targeted troubleshooting based on what you find out. For example: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature = 'Metrics' TIMESERIES Copy If you’ve validated that you can send data successfully but are unable to query it, you may be running into other kinds of limits, like the inspected count limit. This may manifest itself as an error message during the integration process that says: Unable to retrieve data for Prometheus data source <name>.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 201.66234,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Remote</em> <em>write</em> errors <em>and</em> error messages",
        "sections": "<em>Remote</em> <em>write</em> errors <em>and</em> error messages",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " error messages below. Configuration errors Missing or incorrect characters in the <em>remote</em> <em>write</em> URL in the config file (for example the endpoint, license key, or <em>prometheus</em>_server name) or incorrect placement of the information in the file will result in the <em>Prometheus</em> server not starting, <em>remote</em> <em>write</em>"
      },
      "id": "6044e65d196a67914a960f6b"
    },
    {
      "sections": [
        "Drop data using Prometheus remote write",
        "Tip",
        "Drop entire metric data points from remote write integration",
        "Example",
        "Drop specific labels or attributes from data points",
        "Prometheus or NerdGraph?",
        "Considerations for the Prometheus config file method",
        "Considerations the NerdGraph method",
        "Learn more"
      ],
      "title": "Drop data using Prometheus remote write",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "f3e07dd4f6bbdb65881f13035af5af172c5409e7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure/remote-write-drop-data/",
      "published_at": "2021-10-08T06:42:36Z",
      "updated_at": "2021-07-09T08:33:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can drop data you don't want to keep by changing the remote_write section of the YAML config file. Tip You can also drop remote write data using NerdGraph. For more information, see Drop data using NerdGraph. Drop entire metric data points from remote write integration If a target is sending a noisy metric that you don't want sent to New Relic, you can specify that New Relic should drop that data. Example Let's say you don't want to receive data for the metric node_memory_active_bytes from an instance running at localhost:9100. Using the write_relabel_config entry shown below, you can target the metric name using the __name__ label in combination with the instance name. remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - source_labels: ['__name__', 'instance'] regex: 'node_memory_active_bytes;localhost:9100' action: 'drop' Copy This tells Prometheus that you want to do some action against metrics with these labels. To limit which metrics with these labels are affected, you must include some value for regex. By default this value is set to .* and it will include all metrics. In this case, it will drop all metric data points coming out of Prometheus via remote write. Drop specific labels or attributes from data points If a target is sending specific labels or attributes you're not interested in receiving, you can drop these from the metrics you receive. Example Let's say one of your targets is sending a bunch of extra attributes you're not interested in receiving. These might include things like high cardinality attributes such as unique machine identifiers, JVM IDs, or similar. In this case, you need to change both the remote_write and the scrape_configs section of the YAML file. The result will look something like this: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - regex: 'extraLabelToRemove.*' action: 'labeldrop' ... scrape_configs: # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config. - job_name: 'node' # Override the global default and scrape targets from this job every 5 seconds. scrape_interval: 5s static_configs: - targets: ['localhost:9100'] labels: group: 'production' keepLabelName1: 'please-keep-me' extraLabelToRemove: 'please-remove-me' extraLabelToRemove1: 'please-remove-me' extraLabelToRemove2: 'please-remove-me' extraLabelToRemove4: 'please-remove-me' extraLabelToRemove3: 'please-remove-me' extraLabelToRemove5: 'please-remove-me' Copy Prometheus or NerdGraph? There are advantages to both dropping data using the method described on this page and using NerdGraph. This section is intended to help you figure out which method is better for your specific needs and preferences. Considerations for the Prometheus config file method With this method, your dropped data never leaves the associated Prometheus instance. This is a valuable feature if bytes transferred is a cost consideration on the app hosting side. However, this method may be less appealing than the NerdGraph option due to the following considerations: Maintained via config yaml files that need to be loaded onto each Prometheus instance (or via a shared storage mechanism) Requires access to Prometheus server, meaning that either: The server needs to be restarted Served must be be accessed at port with path /-/reload (assuming the server has lifecycle management enabled as described here in the Prometheus configuration docs. Considerations the NerdGraph method NerdGraph is a great option if you want to manage all your data dropping in a single place. It can also be updated easily via the API and requires no restart or interaction with Prometheus. However, this method applies rules to all incoming data points. This means that you should set up your rules with careful consideration using WHERE filtering. For more information, see Drop data using NerdGraph. Learn more Send Prometheus metric data to New Relic Prometheus High Availability (HA)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 193.4635,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Drop data using <em>Prometheus</em> <em>remote</em> <em>write</em>",
        "sections": "Drop data using <em>Prometheus</em> <em>remote</em> <em>write</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " target the metric name using the __name__ label in combination with the instance name. <em>remote_write</em>: - url: https:&#x2F;&#x2F;metric-api.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;<em>write</em>?<em>prometheus</em>_server=macbook-server-cluster bearer_token: &lt;redacted&gt; <em>write</em>_relabel_configs: - source_labels: [&#x27;__name__&#x27;, &#x27;instance&#x27;] regex"
      },
      "id": "60e809e4e7b9d298bafc1035"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure/remote-write-drop-data": [
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Set up the integration",
        "Map Prometheus and New Relic metric types",
        "Override metric type mappings",
        "Set allow or deny lists for sent metrics",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "e2a503880e8e1c38284434d5829fad3f48dc7abf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2021-10-08T06:45:33Z",
      "updated_at": "2021-09-08T01:33:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Map Prometheus and New Relic metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Set allow or deny lists for sent metrics If you need greater control over the data you send to New Relic, you can send a subset of your metrics. To do this, configure remote-write with the write_relabel_configs parameter with a subparameter action value of keep or deny. In this example, you'll only send the metrics that match the regular expression. Unmatched metrics won't be sent. Alternatively, you can use action: drop to drop all of the metrics that match the regular expression. - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy This Kubernetes example uses this Helm chart's values.yaml file. If you're using a different Helm chart, please check its remoteWrite documentation (for example, some Helm files use camelcase writeRelabelConfigs instead). remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 222.08727,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": "_server=YOUR_DATA_SOURCE_NAME Copy European Union accounts: If you&#x27;re connecting from the EU, use the following URL: https:&#x2F;&#x2F;metric-api.eu.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;<em>write</em> Copy Kubernetes and Helm <em>remote</em> <em>write</em> <em>integrations</em>: Add the <em>remote</em> <em>write</em> URL to your Helm values.yaml file. Replace <em>remoteWrite</em>: [] with two"
      },
      "id": "603e94de196a674e6ca83def"
    },
    {
      "sections": [
        "Remote write errors and error messages",
        "Common errors and issues",
        "Configuration errors",
        "400: bad request error",
        "413: request entity too large error",
        "429: rate limit error",
        "Investigate error messages"
      ],
      "title": "Remote write errors and error messages",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "0d190be5dc4fd91ce6bbcef7343d01f75670ca51",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/remote-write-errors-error-messages/",
      "published_at": "2021-10-08T06:45:33Z",
      "updated_at": "2021-08-08T19:27:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This resource contains information about common errors and error messages that may alert you to issues with data visibility and availability, as well as information about how to respond. Common errors and issues If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, there are several actions you can take to troubleshoot and get data flowing properly. Below are a few tips regarding common issues and error messages. For specific information on how to query NrIntegrationError events, see Investigate error messages below. Configuration errors Missing or incorrect characters in the remote write URL in the config file (for example the endpoint, license key, or prometheus_server name) or incorrect placement of the information in the file will result in the Prometheus server not starting, remote write not working properly, or errors appearing in Prometheus server logs. 400: bad request error If no data appears with a bad request error, check your configuration file to confirm that the placement of the remote write information is correct, and that there are no missing or incorrect characters. 413: request entity too large error This means you have sent a request in which one or more fields, or the entire payload, has exceeded our limits. 429: rate limit error This means you have hit a rate limit on the amount of data being sent at one time (for example cardinality or data points per minute). You can troubleshoot by reducing the amount of Prometheus or general metric data you are sending, or by requesting a rate-limit increase. Investigate error messages You can investigate error messages in New Relic by doing either or both of the following. Run a NRQL query of the NrIntegrationError event and examine the message attribute. Investigate individual errors in time to see when and where they occur and any simultaneously occurring issues, and perform targeted troubleshooting based on what you find out. For example: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature = 'Metrics' TIMESERIES Copy If you’ve validated that you can send data successfully but are unable to query it, you may be running into other kinds of limits, like the inspected count limit. This may manifest itself as an error message during the integration process that says: Unable to retrieve data for Prometheus data source <name>.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 201.66234,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Remote</em> <em>write</em> errors <em>and</em> error messages",
        "sections": "<em>Remote</em> <em>write</em> errors <em>and</em> error messages",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " error messages below. Configuration errors Missing or incorrect characters in the <em>remote</em> <em>write</em> URL in the config file (for example the endpoint, license key, or <em>prometheus</em>_server name) or incorrect placement of the information in the file will result in the <em>Prometheus</em> server not starting, <em>remote</em> <em>write</em>"
      },
      "id": "6044e65d196a67914a960f6b"
    },
    {
      "sections": [
        "Prometheus High Availability (HA)",
        "Tip",
        "External labels",
        "Prometheus Operator",
        "Standalone Prometheus"
      ],
      "title": "Prometheus High Availability (HA)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "3c0fddd6e878f30f8ba4c132f537b88cd47f2eba",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure/prometheus-high-availability-ha/",
      "published_at": "2021-10-08T06:41:31Z",
      "updated_at": "2021-03-13T02:41:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are using our Prometheus remote write integration in a high-availability (HA) configuration, you need to make sure your Prometheus servers aren't sending multiple copies of the same metrics to New Relic. This document describes how you can configure your remote write integration so that New Relic does not keep duplicated metrics. Tip For information on standard Prometheus remote write integration without using a high-availability configuration, see Set up your Prometheus remote write integration. External labels New Relic requires two external labels to deduplicate data from replicas in a high-availability configuration: Label name Description Example value prometheus A label whose value identifies the name of a high-availability cluster or group of Prometheus servers. monitoring-cluster prometheus_replica A label whose value identifies the unique replica sending this data. replica-1 The remaining sections explain how labels work with Prometheus Operator and standalone Prometheus. Prometheus Operator These external labels are added by default if you use Prometheus Operator version 0.19.0 (or higher). This applies whether you use Prometheus Operator directly or via the helm chart. The operator sets the value of the prometheus label (the one identifying a cluster) as <prometheus deployment namespace>/<prometheus deployment name>. For example, if your namespace for the Prometheus deployment is monitoring and the name of the deployment is prometheus-cluster1, the value is monitoring/prometheus-cluster1. The operator sets the value of the prometheus_replica label as the name of the pod for each replica. This follows the format replica-<replica number>, where the number is the ordinal of that replica (for example, the first replica is named replica-1). Tip If you still see duplicate copies of replica data, make sure you do not have replicaExternalLabelName or prometheusExternalLabelName in your Prometheus spec or chart configuration because these overrides change the label name. Standalone Prometheus When deploying a Prometheus server directly, you need to add the external labels to the configuration file. Here are two different example configurations for replicas within the same high-availability cluster: Replica 1 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-1 Copy Replica 2 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-2 Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 183.2226,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Prometheus</em> High Availability (HA)",
        "sections": "<em>Prometheus</em> High Availability (HA)",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": "If you are using our <em>Prometheus</em> <em>remote</em> <em>write</em> integration in a high-availability (HA) configuration, you need to make sure your <em>Prometheus</em> servers aren&#x27;t sending multiple copies of the same metrics to New Relic. This document describes how you can <em>configure</em> your <em>remote</em> <em>write</em> integration so that New"
      },
      "id": "6044e621196a67b846960f6b"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-openmetrics/add-mutual-tls-prometheus-endpoints": [
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "12be9e8bb8c03ca3f0eed948d0bc6e863b60efef",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2021-10-07T02:57:48Z",
      "updated_at": "2021-09-07T23:50:31Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data ingested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 199.26086,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Unless otherwise noted, configuration options for your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: <em>Configure</em> your New Relic license key"
      },
      "id": "603e830964441f85a04e8877"
    },
    {
      "sections": [
        "Install, update, or uninstall your Prometheus OpenMetrics integration",
        "Install the integration",
        "Docker installation",
        "Kubernetes installation",
        "Important",
        "Update the integration",
        "Docker update procedures",
        "Kubernetes update procedures",
        "Uninstall"
      ],
      "title": "Install, update, or uninstall your Prometheus OpenMetrics integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "89b53bf5ac9ce6c14663eca2ab44d96cfe897bc8",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration/",
      "published_at": "2021-10-08T06:44:34Z",
      "updated_at": "2021-03-16T06:17:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before you install New Relic's Prometheus OpenMetrics integration, review the requirements for your environment: Docker requirements Kubernetes requirements Install the integration To install the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker installation To install the New Relic Prometheus OpenMetrics integration in a Docker environment: Create a configuration file config.yaml. Use the example configuration file, or look at the nri-prometheus-latest.yaml manifest file, which includes the nri-prometheus-cfg config map and an example configuration. Required: Add your New Relic license key and a cluster name to identify your Docker container. Add the endpoints to scrape; for example, add the http://localhost:8080/metrics endpoint to collect metrics about the integration itself. Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. For more information, see the metrics filtering documentation. Start the integration in the background: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:1.5 Copy Confirm the container is running properly: docker ps -f \"name=nri-prometheus\" Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Kubernetes installation Important To prevent your data from being duplicated, configure your New Relic Prometheus OpenMetrics integration only with one replica. Running two or more replicas will result in duplicated data. For more information, see the troubleshooting procedures for restarts and gaps in data. To install the New Relic Prometheus OpenMetrics integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-prometheus-latest.yaml Copy Edit the nri-prometheus-latest.yaml manifest file: Required: Add your New Relic license key and a cluster name to identify your Kubernetes cluster. env: - name: LICENSE_KEY value: \"<YOUR_LICENSE_KEY>\" [...] config.yaml: | cluster_name: \"<YOUR_CLUSTER_NAME>\" Copy Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. By default, the New Relic Prometheus OpenMetrics integration uses the same labels as Prometheus to discover targets. For more information, see the metrics filtering documentation. Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-prometheus-latest.yaml Copy To confirm that the deployment has been created successfully, look at the CURRENT replicas in the results generated by this command: kubectl get deployments nri-prometheus Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Update the integration To update the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker update procedures Remove the Docker container. Follow standard installation procedures to start a new Docker container. The integration logs its current version when it starts up. To determine the running version: docker logs nri-prometheus 2>&1 | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Kubernetes update procedures Follow standard installation procedures. Reapply the nri-prometheus-latest.yaml manifest file. The integration logs its version when it starts up. To determine the running version: kubectl logs deploy/nri-prometheus | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Uninstall To uninstall the Prometheus OpenMetrics integration for Docker or Kubernetes, execute the following command: Docker: docker rm -f nri-prometheus Copy Kubernetes: kubectl delete -f nri-prometheus-latest.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.56776,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "sections": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": " information, see the troubleshooting procedures for restarts and gaps in data. To <em>install</em> the New Relic <em>Prometheus</em> <em>OpenMetrics</em> integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https:&#x2F;&#x2F;download.newrelic.com&#x2F;infrastructure_agent&#x2F;<em>integrations</em>&#x2F;kubernetes&#x2F;nri"
      },
      "id": "603e8309e7b9d2d69d2a07cc"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations in large Kubernetes environments",
        "Configure the integration for large environments"
      ],
      "title": "Configure Prometheus OpenMetrics integrations in large  Kubernetes environments",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "84e7d3b803e614a6362e0246a58b48e3209094ad",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations-large-kubernetes-environments/",
      "published_at": "2021-10-08T06:43:38Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of metrics exposed by each target. For example, a Prometheus OpenMetrics integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30 seconds, consumes 2.5CPU and 700MB of RAM. Configure the integration for large environments To estimate the size of the environment you are monitoring, run the following query to see how many targets are being scraped: SELECT latest(nr_stats_targets) FROM Metric where clusterName=’clusterName’ SINCE 30 MINUTES AGO TIMESERIES Copy In huge environments with hundreds of targets to be scraped, the latency on the /metrics endpoints must be below 1 second. Run this query to check the latency of the different targets. This query retrieves the data exposed by the Prometheus OpenMetrics integration, and shows the time required to fetch each endpoint. SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=’clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy In order to keep the time needed to scrape all the targets below 30 seconds, use the following configurations: Targets Configuration Targets < 400, with 1000 metrics each No modification is required. CPU ranges roughly between 0.1 and 1.5 cores, and the memory required should be no more than 256MB. 400 < targets < 1000, with 1000 metrics each The number of workers should be increased to 6-8. CPU ranges roughly between 1.5 and 3.5 cores, and the memory required is around 100MB. Targets > 1000, with 1000 metrics each The number of workers should be increased to 10 or more. CPU is over 3.5 cores, and the memory required is around 1GB or more.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.56477,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large  Kubernetes environments",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large Kubernetes environments",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of <em>metrics</em> exposed by each target. For example, a <em>Prometheus</em> <em>OpenMetrics</em> integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30"
      },
      "id": "603e9b3b196a676cd0a83d81"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-openmetrics/add-rename-or-copy-prometheus-attributes": [
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "12be9e8bb8c03ca3f0eed948d0bc6e863b60efef",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2021-10-07T02:57:48Z",
      "updated_at": "2021-09-07T23:50:31Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data ingested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 199.26086,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Unless otherwise noted, configuration options for your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: <em>Configure</em> your New Relic license key"
      },
      "id": "603e830964441f85a04e8877"
    },
    {
      "sections": [
        "Install, update, or uninstall your Prometheus OpenMetrics integration",
        "Install the integration",
        "Docker installation",
        "Kubernetes installation",
        "Important",
        "Update the integration",
        "Docker update procedures",
        "Kubernetes update procedures",
        "Uninstall"
      ],
      "title": "Install, update, or uninstall your Prometheus OpenMetrics integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "89b53bf5ac9ce6c14663eca2ab44d96cfe897bc8",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration/",
      "published_at": "2021-10-08T06:44:34Z",
      "updated_at": "2021-03-16T06:17:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before you install New Relic's Prometheus OpenMetrics integration, review the requirements for your environment: Docker requirements Kubernetes requirements Install the integration To install the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker installation To install the New Relic Prometheus OpenMetrics integration in a Docker environment: Create a configuration file config.yaml. Use the example configuration file, or look at the nri-prometheus-latest.yaml manifest file, which includes the nri-prometheus-cfg config map and an example configuration. Required: Add your New Relic license key and a cluster name to identify your Docker container. Add the endpoints to scrape; for example, add the http://localhost:8080/metrics endpoint to collect metrics about the integration itself. Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. For more information, see the metrics filtering documentation. Start the integration in the background: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:1.5 Copy Confirm the container is running properly: docker ps -f \"name=nri-prometheus\" Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Kubernetes installation Important To prevent your data from being duplicated, configure your New Relic Prometheus OpenMetrics integration only with one replica. Running two or more replicas will result in duplicated data. For more information, see the troubleshooting procedures for restarts and gaps in data. To install the New Relic Prometheus OpenMetrics integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-prometheus-latest.yaml Copy Edit the nri-prometheus-latest.yaml manifest file: Required: Add your New Relic license key and a cluster name to identify your Kubernetes cluster. env: - name: LICENSE_KEY value: \"<YOUR_LICENSE_KEY>\" [...] config.yaml: | cluster_name: \"<YOUR_CLUSTER_NAME>\" Copy Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. By default, the New Relic Prometheus OpenMetrics integration uses the same labels as Prometheus to discover targets. For more information, see the metrics filtering documentation. Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-prometheus-latest.yaml Copy To confirm that the deployment has been created successfully, look at the CURRENT replicas in the results generated by this command: kubectl get deployments nri-prometheus Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Update the integration To update the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker update procedures Remove the Docker container. Follow standard installation procedures to start a new Docker container. The integration logs its current version when it starts up. To determine the running version: docker logs nri-prometheus 2>&1 | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Kubernetes update procedures Follow standard installation procedures. Reapply the nri-prometheus-latest.yaml manifest file. The integration logs its version when it starts up. To determine the running version: kubectl logs deploy/nri-prometheus | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Uninstall To uninstall the Prometheus OpenMetrics integration for Docker or Kubernetes, execute the following command: Docker: docker rm -f nri-prometheus Copy Kubernetes: kubectl delete -f nri-prometheus-latest.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.56776,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "sections": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": " information, see the troubleshooting procedures for restarts and gaps in data. To <em>install</em> the New Relic <em>Prometheus</em> <em>OpenMetrics</em> integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https:&#x2F;&#x2F;download.newrelic.com&#x2F;infrastructure_agent&#x2F;<em>integrations</em>&#x2F;kubernetes&#x2F;nri"
      },
      "id": "603e8309e7b9d2d69d2a07cc"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations in large Kubernetes environments",
        "Configure the integration for large environments"
      ],
      "title": "Configure Prometheus OpenMetrics integrations in large  Kubernetes environments",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "84e7d3b803e614a6362e0246a58b48e3209094ad",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations-large-kubernetes-environments/",
      "published_at": "2021-10-08T06:43:38Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of metrics exposed by each target. For example, a Prometheus OpenMetrics integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30 seconds, consumes 2.5CPU and 700MB of RAM. Configure the integration for large environments To estimate the size of the environment you are monitoring, run the following query to see how many targets are being scraped: SELECT latest(nr_stats_targets) FROM Metric where clusterName=’clusterName’ SINCE 30 MINUTES AGO TIMESERIES Copy In huge environments with hundreds of targets to be scraped, the latency on the /metrics endpoints must be below 1 second. Run this query to check the latency of the different targets. This query retrieves the data exposed by the Prometheus OpenMetrics integration, and shows the time required to fetch each endpoint. SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=’clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy In order to keep the time needed to scrape all the targets below 30 seconds, use the following configurations: Targets Configuration Targets < 400, with 1000 metrics each No modification is required. CPU ranges roughly between 0.1 and 1.5 cores, and the memory required should be no more than 256MB. 400 < targets < 1000, with 1000 metrics each The number of workers should be increased to 6-8. CPU ranges roughly between 1.5 and 3.5 cores, and the memory required is around 100MB. Targets > 1000, with 1000 metrics each The number of workers should be increased to 10 or more. CPU is over 3.5 cores, and the memory required is around 1GB or more.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.56477,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large  Kubernetes environments",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large Kubernetes environments",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of <em>metrics</em> exposed by each target. For example, a <em>Prometheus</em> <em>OpenMetrics</em> integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30"
      },
      "id": "603e9b3b196a676cd0a83d81"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations-large-kubernetes-environments": [
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "12be9e8bb8c03ca3f0eed948d0bc6e863b60efef",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2021-10-07T02:57:48Z",
      "updated_at": "2021-09-07T23:50:31Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data ingested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 199.26085,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Unless otherwise noted, configuration options for your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: <em>Configure</em> your New Relic license key"
      },
      "id": "603e830964441f85a04e8877"
    },
    {
      "sections": [
        "Install, update, or uninstall your Prometheus OpenMetrics integration",
        "Install the integration",
        "Docker installation",
        "Kubernetes installation",
        "Important",
        "Update the integration",
        "Docker update procedures",
        "Kubernetes update procedures",
        "Uninstall"
      ],
      "title": "Install, update, or uninstall your Prometheus OpenMetrics integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "89b53bf5ac9ce6c14663eca2ab44d96cfe897bc8",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration/",
      "published_at": "2021-10-08T06:44:34Z",
      "updated_at": "2021-03-16T06:17:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before you install New Relic's Prometheus OpenMetrics integration, review the requirements for your environment: Docker requirements Kubernetes requirements Install the integration To install the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker installation To install the New Relic Prometheus OpenMetrics integration in a Docker environment: Create a configuration file config.yaml. Use the example configuration file, or look at the nri-prometheus-latest.yaml manifest file, which includes the nri-prometheus-cfg config map and an example configuration. Required: Add your New Relic license key and a cluster name to identify your Docker container. Add the endpoints to scrape; for example, add the http://localhost:8080/metrics endpoint to collect metrics about the integration itself. Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. For more information, see the metrics filtering documentation. Start the integration in the background: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:1.5 Copy Confirm the container is running properly: docker ps -f \"name=nri-prometheus\" Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Kubernetes installation Important To prevent your data from being duplicated, configure your New Relic Prometheus OpenMetrics integration only with one replica. Running two or more replicas will result in duplicated data. For more information, see the troubleshooting procedures for restarts and gaps in data. To install the New Relic Prometheus OpenMetrics integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-prometheus-latest.yaml Copy Edit the nri-prometheus-latest.yaml manifest file: Required: Add your New Relic license key and a cluster name to identify your Kubernetes cluster. env: - name: LICENSE_KEY value: \"<YOUR_LICENSE_KEY>\" [...] config.yaml: | cluster_name: \"<YOUR_CLUSTER_NAME>\" Copy Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. By default, the New Relic Prometheus OpenMetrics integration uses the same labels as Prometheus to discover targets. For more information, see the metrics filtering documentation. Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-prometheus-latest.yaml Copy To confirm that the deployment has been created successfully, look at the CURRENT replicas in the results generated by this command: kubectl get deployments nri-prometheus Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Update the integration To update the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker update procedures Remove the Docker container. Follow standard installation procedures to start a new Docker container. The integration logs its current version when it starts up. To determine the running version: docker logs nri-prometheus 2>&1 | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Kubernetes update procedures Follow standard installation procedures. Reapply the nri-prometheus-latest.yaml manifest file. The integration logs its version when it starts up. To determine the running version: kubectl logs deploy/nri-prometheus | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Uninstall To uninstall the Prometheus OpenMetrics integration for Docker or Kubernetes, execute the following command: Docker: docker rm -f nri-prometheus Copy Kubernetes: kubectl delete -f nri-prometheus-latest.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.56776,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "sections": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": " information, see the troubleshooting procedures for restarts and gaps in data. To <em>install</em> the New Relic <em>Prometheus</em> <em>OpenMetrics</em> integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https:&#x2F;&#x2F;download.newrelic.com&#x2F;infrastructure_agent&#x2F;<em>integrations</em>&#x2F;kubernetes&#x2F;nri"
      },
      "id": "603e8309e7b9d2d69d2a07cc"
    },
    {
      "sections": [
        "Add mutual TLS to Prometheus endpoints",
        "Add secret to config file"
      ],
      "title": "Add mutual TLS to Prometheus endpoints",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "707c96de26f106ddeaea4e18d5b71290170fea90",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/add-mutual-tls-prometheus-endpoints/",
      "published_at": "2021-10-08T06:43:38Z",
      "updated_at": "2021-03-13T03:34:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can configure mutual TLS authentication when needed for the endpoints in your Prometheus OpenMetrics integration with New Relic. Add tls_config to your configuration file for Docker or Kubernetes, as explained in this example. Add secret to config file Recommendation: Put the CA bundle, key, and cert files in a secret, and include them in the Prometheus OpenMetrics integration's container. Mutual TLS authentication is limited to a static list of URLs. To configure endpoints that require MTLS authentication, follow this example: targets: - description: \"Secure etcd example\" urls: [\"https://123.456.7.1:2379\", \"https://123.456.7.2:2379\"] tls_config: ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" cert_file_path: \"/etc/etcd/etcd-client.crt\" key_file_path: \"/etc/etcd/etcd-client.key\" transformations: ... Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.45465,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Add mutual TLS to <em>Prometheus</em> endpoints",
        "sections": "Add mutual TLS to <em>Prometheus</em> endpoints",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "You can <em>configure</em> mutual TLS authentication when needed for the endpoints in your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic. Add tls_config to your configuration file for Docker or Kubernetes, as explained in this example. Add secret to config file Recommendation: Put the CA bundle, key"
      },
      "id": "6044e621196a67efb9960f37"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations": [
    {
      "sections": [
        "Install, update, or uninstall your Prometheus OpenMetrics integration",
        "Install the integration",
        "Docker installation",
        "Kubernetes installation",
        "Important",
        "Update the integration",
        "Docker update procedures",
        "Kubernetes update procedures",
        "Uninstall"
      ],
      "title": "Install, update, or uninstall your Prometheus OpenMetrics integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "89b53bf5ac9ce6c14663eca2ab44d96cfe897bc8",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration/",
      "published_at": "2021-10-08T06:44:34Z",
      "updated_at": "2021-03-16T06:17:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before you install New Relic's Prometheus OpenMetrics integration, review the requirements for your environment: Docker requirements Kubernetes requirements Install the integration To install the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker installation To install the New Relic Prometheus OpenMetrics integration in a Docker environment: Create a configuration file config.yaml. Use the example configuration file, or look at the nri-prometheus-latest.yaml manifest file, which includes the nri-prometheus-cfg config map and an example configuration. Required: Add your New Relic license key and a cluster name to identify your Docker container. Add the endpoints to scrape; for example, add the http://localhost:8080/metrics endpoint to collect metrics about the integration itself. Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. For more information, see the metrics filtering documentation. Start the integration in the background: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:1.5 Copy Confirm the container is running properly: docker ps -f \"name=nri-prometheus\" Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Kubernetes installation Important To prevent your data from being duplicated, configure your New Relic Prometheus OpenMetrics integration only with one replica. Running two or more replicas will result in duplicated data. For more information, see the troubleshooting procedures for restarts and gaps in data. To install the New Relic Prometheus OpenMetrics integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-prometheus-latest.yaml Copy Edit the nri-prometheus-latest.yaml manifest file: Required: Add your New Relic license key and a cluster name to identify your Kubernetes cluster. env: - name: LICENSE_KEY value: \"<YOUR_LICENSE_KEY>\" [...] config.yaml: | cluster_name: \"<YOUR_CLUSTER_NAME>\" Copy Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. By default, the New Relic Prometheus OpenMetrics integration uses the same labels as Prometheus to discover targets. For more information, see the metrics filtering documentation. Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-prometheus-latest.yaml Copy To confirm that the deployment has been created successfully, look at the CURRENT replicas in the results generated by this command: kubectl get deployments nri-prometheus Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Update the integration To update the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker update procedures Remove the Docker container. Follow standard installation procedures to start a new Docker container. The integration logs its current version when it starts up. To determine the running version: docker logs nri-prometheus 2>&1 | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Kubernetes update procedures Follow standard installation procedures. Reapply the nri-prometheus-latest.yaml manifest file. The integration logs its version when it starts up. To determine the running version: kubectl logs deploy/nri-prometheus | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Uninstall To uninstall the Prometheus OpenMetrics integration for Docker or Kubernetes, execute the following command: Docker: docker rm -f nri-prometheus Copy Kubernetes: kubectl delete -f nri-prometheus-latest.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.56776,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "sections": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": " information, see the troubleshooting procedures for restarts and gaps in data. To <em>install</em> the New Relic <em>Prometheus</em> <em>OpenMetrics</em> integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https:&#x2F;&#x2F;download.newrelic.com&#x2F;infrastructure_agent&#x2F;<em>integrations</em>&#x2F;kubernetes&#x2F;nri"
      },
      "id": "603e8309e7b9d2d69d2a07cc"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations in large Kubernetes environments",
        "Configure the integration for large environments"
      ],
      "title": "Configure Prometheus OpenMetrics integrations in large  Kubernetes environments",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "84e7d3b803e614a6362e0246a58b48e3209094ad",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations-large-kubernetes-environments/",
      "published_at": "2021-10-08T06:43:38Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of metrics exposed by each target. For example, a Prometheus OpenMetrics integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30 seconds, consumes 2.5CPU and 700MB of RAM. Configure the integration for large environments To estimate the size of the environment you are monitoring, run the following query to see how many targets are being scraped: SELECT latest(nr_stats_targets) FROM Metric where clusterName=’clusterName’ SINCE 30 MINUTES AGO TIMESERIES Copy In huge environments with hundreds of targets to be scraped, the latency on the /metrics endpoints must be below 1 second. Run this query to check the latency of the different targets. This query retrieves the data exposed by the Prometheus OpenMetrics integration, and shows the time required to fetch each endpoint. SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=’clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy In order to keep the time needed to scrape all the targets below 30 seconds, use the following configurations: Targets Configuration Targets < 400, with 1000 metrics each No modification is required. CPU ranges roughly between 0.1 and 1.5 cores, and the memory required should be no more than 256MB. 400 < targets < 1000, with 1000 metrics each The number of workers should be increased to 6-8. CPU ranges roughly between 1.5 and 3.5 cores, and the memory required is around 100MB. Targets > 1000, with 1000 metrics each The number of workers should be increased to 10 or more. CPU is over 3.5 cores, and the memory required is around 1GB or more.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.56477,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large  Kubernetes environments",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large Kubernetes environments",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of <em>metrics</em> exposed by each target. For example, a <em>Prometheus</em> <em>OpenMetrics</em> integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30"
      },
      "id": "603e9b3b196a676cd0a83d81"
    },
    {
      "sections": [
        "Add mutual TLS to Prometheus endpoints",
        "Add secret to config file"
      ],
      "title": "Add mutual TLS to Prometheus endpoints",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "707c96de26f106ddeaea4e18d5b71290170fea90",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/add-mutual-tls-prometheus-endpoints/",
      "published_at": "2021-10-08T06:43:38Z",
      "updated_at": "2021-03-13T03:34:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can configure mutual TLS authentication when needed for the endpoints in your Prometheus OpenMetrics integration with New Relic. Add tls_config to your configuration file for Docker or Kubernetes, as explained in this example. Add secret to config file Recommendation: Put the CA bundle, key, and cert files in a secret, and include them in the Prometheus OpenMetrics integration's container. Mutual TLS authentication is limited to a static list of URLs. To configure endpoints that require MTLS authentication, follow this example: targets: - description: \"Secure etcd example\" urls: [\"https://123.456.7.1:2379\", \"https://123.456.7.2:2379\"] tls_config: ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" cert_file_path: \"/etc/etcd/etcd-client.crt\" key_file_path: \"/etc/etcd/etcd-client.key\" transformations: ... Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.45465,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Add mutual TLS to <em>Prometheus</em> endpoints",
        "sections": "Add mutual TLS to <em>Prometheus</em> endpoints",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "You can <em>configure</em> mutual TLS authentication when needed for the endpoints in your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic. Add tls_config to your configuration file for Docker or Kubernetes, as explained in this example. Add secret to config file Recommendation: Put the CA bundle, key"
      },
      "id": "6044e621196a67efb9960f37"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-openmetrics/ignore-or-include-prometheus-metrics": [
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "12be9e8bb8c03ca3f0eed948d0bc6e863b60efef",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2021-10-07T02:57:48Z",
      "updated_at": "2021-09-07T23:50:31Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data ingested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 199.26085,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Unless otherwise noted, configuration options for your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: <em>Configure</em> your New Relic license key"
      },
      "id": "603e830964441f85a04e8877"
    },
    {
      "sections": [
        "Install, update, or uninstall your Prometheus OpenMetrics integration",
        "Install the integration",
        "Docker installation",
        "Kubernetes installation",
        "Important",
        "Update the integration",
        "Docker update procedures",
        "Kubernetes update procedures",
        "Uninstall"
      ],
      "title": "Install, update, or uninstall your Prometheus OpenMetrics integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "89b53bf5ac9ce6c14663eca2ab44d96cfe897bc8",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration/",
      "published_at": "2021-10-08T06:44:34Z",
      "updated_at": "2021-03-16T06:17:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before you install New Relic's Prometheus OpenMetrics integration, review the requirements for your environment: Docker requirements Kubernetes requirements Install the integration To install the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker installation To install the New Relic Prometheus OpenMetrics integration in a Docker environment: Create a configuration file config.yaml. Use the example configuration file, or look at the nri-prometheus-latest.yaml manifest file, which includes the nri-prometheus-cfg config map and an example configuration. Required: Add your New Relic license key and a cluster name to identify your Docker container. Add the endpoints to scrape; for example, add the http://localhost:8080/metrics endpoint to collect metrics about the integration itself. Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. For more information, see the metrics filtering documentation. Start the integration in the background: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:1.5 Copy Confirm the container is running properly: docker ps -f \"name=nri-prometheus\" Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Kubernetes installation Important To prevent your data from being duplicated, configure your New Relic Prometheus OpenMetrics integration only with one replica. Running two or more replicas will result in duplicated data. For more information, see the troubleshooting procedures for restarts and gaps in data. To install the New Relic Prometheus OpenMetrics integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-prometheus-latest.yaml Copy Edit the nri-prometheus-latest.yaml manifest file: Required: Add your New Relic license key and a cluster name to identify your Kubernetes cluster. env: - name: LICENSE_KEY value: \"<YOUR_LICENSE_KEY>\" [...] config.yaml: | cluster_name: \"<YOUR_CLUSTER_NAME>\" Copy Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. By default, the New Relic Prometheus OpenMetrics integration uses the same labels as Prometheus to discover targets. For more information, see the metrics filtering documentation. Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-prometheus-latest.yaml Copy To confirm that the deployment has been created successfully, look at the CURRENT replicas in the results generated by this command: kubectl get deployments nri-prometheus Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Update the integration To update the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker update procedures Remove the Docker container. Follow standard installation procedures to start a new Docker container. The integration logs its current version when it starts up. To determine the running version: docker logs nri-prometheus 2>&1 | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Kubernetes update procedures Follow standard installation procedures. Reapply the nri-prometheus-latest.yaml manifest file. The integration logs its version when it starts up. To determine the running version: kubectl logs deploy/nri-prometheus | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Uninstall To uninstall the Prometheus OpenMetrics integration for Docker or Kubernetes, execute the following command: Docker: docker rm -f nri-prometheus Copy Kubernetes: kubectl delete -f nri-prometheus-latest.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.56776,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "sections": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": " information, see the troubleshooting procedures for restarts and gaps in data. To <em>install</em> the New Relic <em>Prometheus</em> <em>OpenMetrics</em> integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https:&#x2F;&#x2F;download.newrelic.com&#x2F;infrastructure_agent&#x2F;<em>integrations</em>&#x2F;kubernetes&#x2F;nri"
      },
      "id": "603e8309e7b9d2d69d2a07cc"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations in large Kubernetes environments",
        "Configure the integration for large environments"
      ],
      "title": "Configure Prometheus OpenMetrics integrations in large  Kubernetes environments",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "84e7d3b803e614a6362e0246a58b48e3209094ad",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations-large-kubernetes-environments/",
      "published_at": "2021-10-08T06:43:38Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of metrics exposed by each target. For example, a Prometheus OpenMetrics integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30 seconds, consumes 2.5CPU and 700MB of RAM. Configure the integration for large environments To estimate the size of the environment you are monitoring, run the following query to see how many targets are being scraped: SELECT latest(nr_stats_targets) FROM Metric where clusterName=’clusterName’ SINCE 30 MINUTES AGO TIMESERIES Copy In huge environments with hundreds of targets to be scraped, the latency on the /metrics endpoints must be below 1 second. Run this query to check the latency of the different targets. This query retrieves the data exposed by the Prometheus OpenMetrics integration, and shows the time required to fetch each endpoint. SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=’clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy In order to keep the time needed to scrape all the targets below 30 seconds, use the following configurations: Targets Configuration Targets < 400, with 1000 metrics each No modification is required. CPU ranges roughly between 0.1 and 1.5 cores, and the memory required should be no more than 256MB. 400 < targets < 1000, with 1000 metrics each The number of workers should be increased to 6-8. CPU ranges roughly between 1.5 and 3.5 cores, and the memory required is around 100MB. Targets > 1000, with 1000 metrics each The number of workers should be increased to 10 or more. CPU is over 3.5 cores, and the memory required is around 1GB or more.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.56477,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large  Kubernetes environments",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large Kubernetes environments",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of <em>metrics</em> exposed by each target. For example, a <em>Prometheus</em> <em>OpenMetrics</em> integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30"
      },
      "id": "603e9b3b196a676cd0a83d81"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration": [
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "12be9e8bb8c03ca3f0eed948d0bc6e863b60efef",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2021-10-07T02:57:48Z",
      "updated_at": "2021-09-07T23:50:31Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data ingested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 199.26083,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Unless otherwise noted, configuration options for your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: <em>Configure</em> your New Relic license key"
      },
      "id": "603e830964441f85a04e8877"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations in large Kubernetes environments",
        "Configure the integration for large environments"
      ],
      "title": "Configure Prometheus OpenMetrics integrations in large  Kubernetes environments",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "84e7d3b803e614a6362e0246a58b48e3209094ad",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations-large-kubernetes-environments/",
      "published_at": "2021-10-08T06:43:38Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of metrics exposed by each target. For example, a Prometheus OpenMetrics integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30 seconds, consumes 2.5CPU and 700MB of RAM. Configure the integration for large environments To estimate the size of the environment you are monitoring, run the following query to see how many targets are being scraped: SELECT latest(nr_stats_targets) FROM Metric where clusterName=’clusterName’ SINCE 30 MINUTES AGO TIMESERIES Copy In huge environments with hundreds of targets to be scraped, the latency on the /metrics endpoints must be below 1 second. Run this query to check the latency of the different targets. This query retrieves the data exposed by the Prometheus OpenMetrics integration, and shows the time required to fetch each endpoint. SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=’clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy In order to keep the time needed to scrape all the targets below 30 seconds, use the following configurations: Targets Configuration Targets < 400, with 1000 metrics each No modification is required. CPU ranges roughly between 0.1 and 1.5 cores, and the memory required should be no more than 256MB. 400 < targets < 1000, with 1000 metrics each The number of workers should be increased to 6-8. CPU ranges roughly between 1.5 and 3.5 cores, and the memory required is around 100MB. Targets > 1000, with 1000 metrics each The number of workers should be increased to 10 or more. CPU is over 3.5 cores, and the memory required is around 1GB or more.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.56477,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large  Kubernetes environments",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large Kubernetes environments",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of <em>metrics</em> exposed by each target. For example, a <em>Prometheus</em> <em>OpenMetrics</em> integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30"
      },
      "id": "603e9b3b196a676cd0a83d81"
    },
    {
      "sections": [
        "Add mutual TLS to Prometheus endpoints",
        "Add secret to config file"
      ],
      "title": "Add mutual TLS to Prometheus endpoints",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "707c96de26f106ddeaea4e18d5b71290170fea90",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/add-mutual-tls-prometheus-endpoints/",
      "published_at": "2021-10-08T06:43:38Z",
      "updated_at": "2021-03-13T03:34:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can configure mutual TLS authentication when needed for the endpoints in your Prometheus OpenMetrics integration with New Relic. Add tls_config to your configuration file for Docker or Kubernetes, as explained in this example. Add secret to config file Recommendation: Put the CA bundle, key, and cert files in a secret, and include them in the Prometheus OpenMetrics integration's container. Mutual TLS authentication is limited to a static list of URLs. To configure endpoints that require MTLS authentication, follow this example: targets: - description: \"Secure etcd example\" urls: [\"https://123.456.7.1:2379\", \"https://123.456.7.2:2379\"] tls_config: ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" cert_file_path: \"/etc/etcd/etcd-client.crt\" key_file_path: \"/etc/etcd/etcd-client.key\" transformations: ... Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.45465,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Add mutual TLS to <em>Prometheus</em> endpoints",
        "sections": "Add mutual TLS to <em>Prometheus</em> endpoints",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "You can <em>configure</em> mutual TLS authentication when needed for the endpoints in your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic. Add tls_config to your configuration file for Docker or Kubernetes, as explained in this example. Add secret to config file Recommendation: Put the CA bundle, key"
      },
      "id": "6044e621196a67efb9960f37"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-remote-write/prometheus-remote-write-integration": [
    {
      "sections": [
        "Send Prometheus metric data to New Relic",
        "Prometheus OpenMetrics or remote write integration?",
        "Prometheus remote write integration",
        "Prometheus OpenMetrics integration for Kubernetes or Docker",
        "Scale your data and get moving quickly",
        "How it works",
        "Remote write compatibility and requirements",
        "Prometheus OpenMetrics integrations",
        "Reduce overhead and scale your data",
        "Kubernetes",
        "Docker",
        "OpenMetrics integrations compatibility and requirements",
        "Important",
        "What's next"
      ],
      "title": "Send Prometheus metric data to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Get started"
      ],
      "external_id": "c43eafc49c9c82cbf8642897c868c9602cecc6b9",
      "image": "https://docs.newrelic.com/static/3b6e65cd4f0d292124399b59a6195a0a/8c557/Prometheus-remote-write-dashboard.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/",
      "published_at": "2021-10-07T00:24:02Z",
      "updated_at": "2021-07-22T05:51:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This page provides an overview of New Relic's Prometheus integration options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. Prometheus OpenMetrics or remote write integration? We currently offer two integration options: Prometheus remote write integration and Prometheus OpenMetrics integration for Kubernetes or Docker. We recommend getting started with the remote write integration if you already have a Prometheus server install base. If you find it hard to manage your Prometheus cluster, or if you are getting started with integrating Prometheus Metrics, you should use OpenMetrics. Examine the benefits, reminders, and recommendations for each option below. Prometheus remote write integration Benefits: Easy access to your combined metrics in New Relic if you already have Prometheus servers. Access only takes one line of yaml in your Prometheus configuration. Access your metrics through both New Relic and Prometheus without making additional adjustments in Prometheus. Federation: Allows you to combine data from multiple servers into a single source. Prometheus High Availability support: We de-duplicate data from HA-pairs on ingest. Reminders: You will need to manage your Prometheus servers. You can reduce your storage retention. Fewer query loads to the server. Recommendations: Evaluate your observability needs to manage your data volumes better: The scrape interval is the biggest factor influencing data volumes: select it based on your observability needs. For example, changing from 15s (default value) to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more New Relic accounts or sub-accounts to manage rate limits. Prometheus OpenMetrics integration for Kubernetes or Docker Benefits: Best for an alternative to Prometheus servers Store all your metrics directly in New Relic No need to manage any Prometheus servers yourself. No need for local storage. Reminders: Slightly more complex setup. No support for High Availability replicas. The Kubernetes operator is not available for enhanced operations automation. Regardless of the option you chose, with our Prometheus integrations: You can use Grafana or other query tools via New Relic's Prometheus' API. You benefit from more nuanced security and user management options as part of New Relic One. The New Relic Telemetry Data Platform can be the centralized long-term data store for all your Prometheus metrics, allowing you to observe all your data in one place. You can execute queries to scale, supported by New Relic. Prometheus remote write integration The Prometheus remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. Once integrated, you can leverage the full range of options for setup and management, from raw data to queries, dashboards, and more. Scale your data and get moving quickly With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes you’re monitoring Connect your Grafana dashboards (optional). Prometheus remote write dashboard How it works Signup for New Relic is fast and free — we won't even ask for a credit card number. Once logged in, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. Check for your data. Query and explore! Read the setup docs Add Prometheus data Remote write compatibility and requirements New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Prometheus OpenMetrics integrations New Relic’s Prometheus OpenMetrics integrations for Docker and Kubernetes allow you to scrape Prometheus endpoints and send the data to New Relic, so you can store and visualize crucial metrics on one platform. With these integrations, you can: Automatically identify a static list of endpoints. Collect metrics that are important to your business. Query and visualize this data in the New Relic UI. Connect your Grafana dashboards (optional). Kubernetes OpenMetrics dashboard Reduce overhead and scale your data Collect, analyze, and visualize your metrics data from any source, alongside your telemetry data, so you can correlate issues all in one place. Out-of-the-box integrations for open-source tools like Prometheus make it easy to get started, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics integrations gather all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. To learn more about how to scale your data without the hassles of managing Prometheus and a separate dashboard tool, see New Relic's Prometheus OpenMetrics integration blog post. Kubernetes In a Kubernetes environment, New Relic automatically discovers the endpoints in the same way that the Prometheus Kubernetes collector does it. The integration looks for the prometheus.io/scrape annotation or label. You can also identify additional static endpoints in the configuration. Docker The Prometheus OpenMetrics integration gathers all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. OpenMetrics integrations compatibility and requirements For Kubernetes and Docker OpenMetrics integrations, you should be aware of the following compatibility and requirements information. Kubernetes New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2 and Kubernetes versions 1.9 or higher. The integration was tested using Kubernetes 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For more details, see the metrics API documentation. Important Recommendation: Always run the scraper with one replica. Adding more replicas will result in duplicated data. Docker New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2. The integration was tested using Docker 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For details, see the metrics API documentation. What's next Ready to get moving? Here are some suggested next steps: Read the how-to for completing the remote write integration. Read the how-to for completing the Prometheus OpenMetrics integration. Both integration options generate dimensional metrics that are subject to the same rate limits described in the Metric API. Learn about Grafana support options. Explore the range of other options available as part of the Telemetry Data Platform.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1852.232,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Send <em>Prometheus</em> metric data to New Relic",
        "sections": "<em>Prometheus</em> OpenMetrics or <em>remote</em> <em>write</em> <em>integration</em>?",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "This page provides an overview of New Relic&#x27;s <em>Prometheus</em> <em>integration</em> options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. <em>Prometheus</em> OpenMetrics or <em>remote</em> <em>write</em> <em>integration</em>? We currently offer two"
      },
      "id": "603ea41964441f0d824e8874"
    },
    {
      "image": "https://docs.newrelic.com/static/d2a9c929c7541b67b6fe4c87844fc01b/ae694/prometheus_grafana_dashboard.png",
      "url": "https://docs.newrelic.com/whats-new/2020/08/create-grafana-dashboards-prometheus-data-stored-new-relic/",
      "sections": [
        "Create Grafana dashboards with Prometheus data stored in New Relic",
        "Step 1: Get data flowing into New Relic with the Prometheus remote write integration",
        "Step 2: Configure your Grafana dashboards to use Prometheus data stored in New Relic"
      ],
      "published_at": "2021-10-08T10:47:47Z",
      "title": "Create Grafana dashboards with Prometheus data stored in New Relic",
      "updated_at": "2021-03-11T00:16:19Z",
      "type": "docs",
      "external_id": "da09ab47a2ac806ad3ed1fa67e3a02dd54394383",
      "document_type": "nr1_announcement",
      "popularity": 1,
      "body": "We’ve teamed up with Grafana Labs so you can use our Telemetry Data Platform as a data source for Prometheus metrics and see them in your existing dashboards, seamlessly tapping into the reliability, scale, and security provided by New Relic. Follow the steps below or use this more detailed walkthrough to send Prometheus data to New Relic, so that Grafana can populate your existing Prometheus-specific dashboards with that data. This process requires Prometheus version 2.15.0 or higher and Grafana version 6.7.0 or higher. You’ll also need to sign up for New Relic. Here's an example of how these Grafana dashboards with Prometheus data look in our new dark mode. Step 1: Get data flowing into New Relic with the Prometheus remote write integration Go to Instrument Everything – US or Instrument Everything – EU, then click the Prometheus tile. You can also go to the Prometheus remote write setup page to get your remote_write URL. For more information on how to set up the Prometheus remote write integration, check out our docs. Step 2: Configure your Grafana dashboards to use Prometheus data stored in New Relic For more information on how to configure New Relic as a Prometheus data source for Grafana, check out our docs.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1743.4233,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create Grafana dashboards with <em>Prometheus</em> data stored in New Relic",
        "sections": "Step 1: Get data flowing into New Relic with the <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "body": " these Grafana dashboards with <em>Prometheus</em> data look in our new dark mode. Step 1: Get data flowing into New Relic with the <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em> Go to Instrument Everything – US or Instrument Everything – EU, then click the <em>Prometheus</em> tile. You can also go to the <em>Prometheus</em> <em>remote</em> <em>write</em>"
      },
      "id": "60445821e7b9d23b585799e4"
    },
    {
      "sections": [
        "Prometheus High Availability (HA)",
        "Tip",
        "External labels",
        "Prometheus Operator",
        "Standalone Prometheus"
      ],
      "title": "Prometheus High Availability (HA)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "3c0fddd6e878f30f8ba4c132f537b88cd47f2eba",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure/prometheus-high-availability-ha/",
      "published_at": "2021-10-08T06:41:31Z",
      "updated_at": "2021-03-13T02:41:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are using our Prometheus remote write integration in a high-availability (HA) configuration, you need to make sure your Prometheus servers aren't sending multiple copies of the same metrics to New Relic. This document describes how you can configure your remote write integration so that New Relic does not keep duplicated metrics. Tip For information on standard Prometheus remote write integration without using a high-availability configuration, see Set up your Prometheus remote write integration. External labels New Relic requires two external labels to deduplicate data from replicas in a high-availability configuration: Label name Description Example value prometheus A label whose value identifies the name of a high-availability cluster or group of Prometheus servers. monitoring-cluster prometheus_replica A label whose value identifies the unique replica sending this data. replica-1 The remaining sections explain how labels work with Prometheus Operator and standalone Prometheus. Prometheus Operator These external labels are added by default if you use Prometheus Operator version 0.19.0 (or higher). This applies whether you use Prometheus Operator directly or via the helm chart. The operator sets the value of the prometheus label (the one identifying a cluster) as <prometheus deployment namespace>/<prometheus deployment name>. For example, if your namespace for the Prometheus deployment is monitoring and the name of the deployment is prometheus-cluster1, the value is monitoring/prometheus-cluster1. The operator sets the value of the prometheus_replica label as the name of the pod for each replica. This follows the format replica-<replica number>, where the number is the ordinal of that replica (for example, the first replica is named replica-1). Tip If you still see duplicate copies of replica data, make sure you do not have replicaExternalLabelName or prometheusExternalLabelName in your Prometheus spec or chart configuration because these overrides change the label name. Standalone Prometheus When deploying a Prometheus server directly, you need to add the external labels to the configuration file. Here are two different example configurations for replicas within the same high-availability cluster: Replica 1 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-1 Copy Replica 2 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-2 Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1735.5201,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Prometheus</em> High Availability (HA)",
        "sections": "<em>Prometheus</em> High Availability (HA)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "If you are using our <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em> in a high-availability (HA) configuration, you need to make sure your <em>Prometheus</em> servers aren&#x27;t sending multiple copies of the same metrics to New Relic. This document describes how you can configure your <em>remote</em> <em>write</em> <em>integration</em> so that New"
      },
      "id": "6044e621196a67b846960f6b"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-remote-write/remote-write-errors-error-messages": [
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Set up the integration",
        "Map Prometheus and New Relic metric types",
        "Override metric type mappings",
        "Set allow or deny lists for sent metrics",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "e2a503880e8e1c38284434d5829fad3f48dc7abf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2021-10-08T06:45:33Z",
      "updated_at": "2021-09-08T01:33:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Map Prometheus and New Relic metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Set allow or deny lists for sent metrics If you need greater control over the data you send to New Relic, you can send a subset of your metrics. To do this, configure remote-write with the write_relabel_configs parameter with a subparameter action value of keep or deny. In this example, you'll only send the metrics that match the regular expression. Unmatched metrics won't be sent. Alternatively, you can use action: drop to drop all of the metrics that match the regular expression. - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy This Kubernetes example uses this Helm chart's values.yaml file. If you're using a different Helm chart, please check its remoteWrite documentation (for example, some Helm files use camelcase writeRelabelConfigs instead). remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: \"coredns_(.*)|etcd_(.*)\" action: keep Copy Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 222.08722,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": "_server=YOUR_DATA_SOURCE_NAME Copy European Union accounts: If you&#x27;re connecting from the EU, use the following URL: https:&#x2F;&#x2F;metric-api.eu.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;<em>write</em> Copy Kubernetes and Helm <em>remote</em> <em>write</em> <em>integrations</em>: Add the <em>remote</em> <em>write</em> URL to your Helm values.yaml file. Replace <em>remoteWrite</em>: [] with two"
      },
      "id": "603e94de196a674e6ca83def"
    },
    {
      "sections": [
        "Drop data using Prometheus remote write",
        "Tip",
        "Drop entire metric data points from remote write integration",
        "Example",
        "Drop specific labels or attributes from data points",
        "Prometheus or NerdGraph?",
        "Considerations for the Prometheus config file method",
        "Considerations the NerdGraph method",
        "Learn more"
      ],
      "title": "Drop data using Prometheus remote write",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "f3e07dd4f6bbdb65881f13035af5af172c5409e7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure/remote-write-drop-data/",
      "published_at": "2021-10-08T06:42:36Z",
      "updated_at": "2021-07-09T08:33:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can drop data you don't want to keep by changing the remote_write section of the YAML config file. Tip You can also drop remote write data using NerdGraph. For more information, see Drop data using NerdGraph. Drop entire metric data points from remote write integration If a target is sending a noisy metric that you don't want sent to New Relic, you can specify that New Relic should drop that data. Example Let's say you don't want to receive data for the metric node_memory_active_bytes from an instance running at localhost:9100. Using the write_relabel_config entry shown below, you can target the metric name using the __name__ label in combination with the instance name. remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - source_labels: ['__name__', 'instance'] regex: 'node_memory_active_bytes;localhost:9100' action: 'drop' Copy This tells Prometheus that you want to do some action against metrics with these labels. To limit which metrics with these labels are affected, you must include some value for regex. By default this value is set to .* and it will include all metrics. In this case, it will drop all metric data points coming out of Prometheus via remote write. Drop specific labels or attributes from data points If a target is sending specific labels or attributes you're not interested in receiving, you can drop these from the metrics you receive. Example Let's say one of your targets is sending a bunch of extra attributes you're not interested in receiving. These might include things like high cardinality attributes such as unique machine identifiers, JVM IDs, or similar. In this case, you need to change both the remote_write and the scrape_configs section of the YAML file. The result will look something like this: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - regex: 'extraLabelToRemove.*' action: 'labeldrop' ... scrape_configs: # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config. - job_name: 'node' # Override the global default and scrape targets from this job every 5 seconds. scrape_interval: 5s static_configs: - targets: ['localhost:9100'] labels: group: 'production' keepLabelName1: 'please-keep-me' extraLabelToRemove: 'please-remove-me' extraLabelToRemove1: 'please-remove-me' extraLabelToRemove2: 'please-remove-me' extraLabelToRemove4: 'please-remove-me' extraLabelToRemove3: 'please-remove-me' extraLabelToRemove5: 'please-remove-me' Copy Prometheus or NerdGraph? There are advantages to both dropping data using the method described on this page and using NerdGraph. This section is intended to help you figure out which method is better for your specific needs and preferences. Considerations for the Prometheus config file method With this method, your dropped data never leaves the associated Prometheus instance. This is a valuable feature if bytes transferred is a cost consideration on the app hosting side. However, this method may be less appealing than the NerdGraph option due to the following considerations: Maintained via config yaml files that need to be loaded onto each Prometheus instance (or via a shared storage mechanism) Requires access to Prometheus server, meaning that either: The server needs to be restarted Served must be be accessed at port with path /-/reload (assuming the server has lifecycle management enabled as described here in the Prometheus configuration docs. Considerations the NerdGraph method NerdGraph is a great option if you want to manage all your data dropping in a single place. It can also be updated easily via the API and requires no restart or interaction with Prometheus. However, this method applies rules to all incoming data points. This means that you should set up your rules with careful consideration using WHERE filtering. For more information, see Drop data using NerdGraph. Learn more Send Prometheus metric data to New Relic Prometheus High Availability (HA)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 193.46349,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Drop data using <em>Prometheus</em> <em>remote</em> <em>write</em>",
        "sections": "Drop data using <em>Prometheus</em> <em>remote</em> <em>write</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " target the metric name using the __name__ label in combination with the instance name. <em>remote_write</em>: - url: https:&#x2F;&#x2F;metric-api.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;<em>write</em>?<em>prometheus</em>_server=macbook-server-cluster bearer_token: &lt;redacted&gt; <em>write</em>_relabel_configs: - source_labels: [&#x27;__name__&#x27;, &#x27;instance&#x27;] regex"
      },
      "id": "60e809e4e7b9d298bafc1035"
    },
    {
      "sections": [
        "Prometheus High Availability (HA)",
        "Tip",
        "External labels",
        "Prometheus Operator",
        "Standalone Prometheus"
      ],
      "title": "Prometheus High Availability (HA)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "3c0fddd6e878f30f8ba4c132f537b88cd47f2eba",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure/prometheus-high-availability-ha/",
      "published_at": "2021-10-08T06:41:31Z",
      "updated_at": "2021-03-13T02:41:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are using our Prometheus remote write integration in a high-availability (HA) configuration, you need to make sure your Prometheus servers aren't sending multiple copies of the same metrics to New Relic. This document describes how you can configure your remote write integration so that New Relic does not keep duplicated metrics. Tip For information on standard Prometheus remote write integration without using a high-availability configuration, see Set up your Prometheus remote write integration. External labels New Relic requires two external labels to deduplicate data from replicas in a high-availability configuration: Label name Description Example value prometheus A label whose value identifies the name of a high-availability cluster or group of Prometheus servers. monitoring-cluster prometheus_replica A label whose value identifies the unique replica sending this data. replica-1 The remaining sections explain how labels work with Prometheus Operator and standalone Prometheus. Prometheus Operator These external labels are added by default if you use Prometheus Operator version 0.19.0 (or higher). This applies whether you use Prometheus Operator directly or via the helm chart. The operator sets the value of the prometheus label (the one identifying a cluster) as <prometheus deployment namespace>/<prometheus deployment name>. For example, if your namespace for the Prometheus deployment is monitoring and the name of the deployment is prometheus-cluster1, the value is monitoring/prometheus-cluster1. The operator sets the value of the prometheus_replica label as the name of the pod for each replica. This follows the format replica-<replica number>, where the number is the ordinal of that replica (for example, the first replica is named replica-1). Tip If you still see duplicate copies of replica data, make sure you do not have replicaExternalLabelName or prometheusExternalLabelName in your Prometheus spec or chart configuration because these overrides change the label name. Standalone Prometheus When deploying a Prometheus server directly, you need to add the external labels to the configuration file. Here are two different example configurations for replicas within the same high-availability cluster: Replica 1 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-1 Copy Replica 2 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-2 Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 183.22258,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Prometheus</em> High Availability (HA)",
        "sections": "<em>Prometheus</em> High Availability (HA)",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": "If you are using our <em>Prometheus</em> <em>remote</em> <em>write</em> integration in a high-availability (HA) configuration, you need to make sure your <em>Prometheus</em> servers aren&#x27;t sending multiple copies of the same metrics to New Relic. This document describes how you can <em>configure</em> your <em>remote</em> <em>write</em> integration so that New"
      },
      "id": "6044e621196a67b846960f6b"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration": [
    {
      "sections": [
        "Remote write errors and error messages",
        "Common errors and issues",
        "Configuration errors",
        "400: bad request error",
        "413: request entity too large error",
        "429: rate limit error",
        "Investigate error messages"
      ],
      "title": "Remote write errors and error messages",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "0d190be5dc4fd91ce6bbcef7343d01f75670ca51",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/remote-write-errors-error-messages/",
      "published_at": "2021-10-08T06:45:33Z",
      "updated_at": "2021-08-08T19:27:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This resource contains information about common errors and error messages that may alert you to issues with data visibility and availability, as well as information about how to respond. Common errors and issues If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, there are several actions you can take to troubleshoot and get data flowing properly. Below are a few tips regarding common issues and error messages. For specific information on how to query NrIntegrationError events, see Investigate error messages below. Configuration errors Missing or incorrect characters in the remote write URL in the config file (for example the endpoint, license key, or prometheus_server name) or incorrect placement of the information in the file will result in the Prometheus server not starting, remote write not working properly, or errors appearing in Prometheus server logs. 400: bad request error If no data appears with a bad request error, check your configuration file to confirm that the placement of the remote write information is correct, and that there are no missing or incorrect characters. 413: request entity too large error This means you have sent a request in which one or more fields, or the entire payload, has exceeded our limits. 429: rate limit error This means you have hit a rate limit on the amount of data being sent at one time (for example cardinality or data points per minute). You can troubleshoot by reducing the amount of Prometheus or general metric data you are sending, or by requesting a rate-limit increase. Investigate error messages You can investigate error messages in New Relic by doing either or both of the following. Run a NRQL query of the NrIntegrationError event and examine the message attribute. Investigate individual errors in time to see when and where they occur and any simultaneously occurring issues, and perform targeted troubleshooting based on what you find out. For example: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature = 'Metrics' TIMESERIES Copy If you’ve validated that you can send data successfully but are unable to query it, you may be running into other kinds of limits, like the inspected count limit. This may manifest itself as an error message during the integration process that says: Unable to retrieve data for Prometheus data source <name>.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 201.66231,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Remote</em> <em>write</em> errors <em>and</em> error messages",
        "sections": "<em>Remote</em> <em>write</em> errors <em>and</em> error messages",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " error messages below. Configuration errors Missing or incorrect characters in the <em>remote</em> <em>write</em> URL in the config file (for example the endpoint, license key, or <em>prometheus</em>_server name) or incorrect placement of the information in the file will result in the <em>Prometheus</em> server not starting, <em>remote</em> <em>write</em>"
      },
      "id": "6044e65d196a67914a960f6b"
    },
    {
      "sections": [
        "Drop data using Prometheus remote write",
        "Tip",
        "Drop entire metric data points from remote write integration",
        "Example",
        "Drop specific labels or attributes from data points",
        "Prometheus or NerdGraph?",
        "Considerations for the Prometheus config file method",
        "Considerations the NerdGraph method",
        "Learn more"
      ],
      "title": "Drop data using Prometheus remote write",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "f3e07dd4f6bbdb65881f13035af5af172c5409e7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure/remote-write-drop-data/",
      "published_at": "2021-10-08T06:42:36Z",
      "updated_at": "2021-07-09T08:33:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can drop data you don't want to keep by changing the remote_write section of the YAML config file. Tip You can also drop remote write data using NerdGraph. For more information, see Drop data using NerdGraph. Drop entire metric data points from remote write integration If a target is sending a noisy metric that you don't want sent to New Relic, you can specify that New Relic should drop that data. Example Let's say you don't want to receive data for the metric node_memory_active_bytes from an instance running at localhost:9100. Using the write_relabel_config entry shown below, you can target the metric name using the __name__ label in combination with the instance name. remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - source_labels: ['__name__', 'instance'] regex: 'node_memory_active_bytes;localhost:9100' action: 'drop' Copy This tells Prometheus that you want to do some action against metrics with these labels. To limit which metrics with these labels are affected, you must include some value for regex. By default this value is set to .* and it will include all metrics. In this case, it will drop all metric data points coming out of Prometheus via remote write. Drop specific labels or attributes from data points If a target is sending specific labels or attributes you're not interested in receiving, you can drop these from the metrics you receive. Example Let's say one of your targets is sending a bunch of extra attributes you're not interested in receiving. These might include things like high cardinality attributes such as unique machine identifiers, JVM IDs, or similar. In this case, you need to change both the remote_write and the scrape_configs section of the YAML file. The result will look something like this: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - regex: 'extraLabelToRemove.*' action: 'labeldrop' ... scrape_configs: # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config. - job_name: 'node' # Override the global default and scrape targets from this job every 5 seconds. scrape_interval: 5s static_configs: - targets: ['localhost:9100'] labels: group: 'production' keepLabelName1: 'please-keep-me' extraLabelToRemove: 'please-remove-me' extraLabelToRemove1: 'please-remove-me' extraLabelToRemove2: 'please-remove-me' extraLabelToRemove4: 'please-remove-me' extraLabelToRemove3: 'please-remove-me' extraLabelToRemove5: 'please-remove-me' Copy Prometheus or NerdGraph? There are advantages to both dropping data using the method described on this page and using NerdGraph. This section is intended to help you figure out which method is better for your specific needs and preferences. Considerations for the Prometheus config file method With this method, your dropped data never leaves the associated Prometheus instance. This is a valuable feature if bytes transferred is a cost consideration on the app hosting side. However, this method may be less appealing than the NerdGraph option due to the following considerations: Maintained via config yaml files that need to be loaded onto each Prometheus instance (or via a shared storage mechanism) Requires access to Prometheus server, meaning that either: The server needs to be restarted Served must be be accessed at port with path /-/reload (assuming the server has lifecycle management enabled as described here in the Prometheus configuration docs. Considerations the NerdGraph method NerdGraph is a great option if you want to manage all your data dropping in a single place. It can also be updated easily via the API and requires no restart or interaction with Prometheus. However, this method applies rules to all incoming data points. This means that you should set up your rules with careful consideration using WHERE filtering. For more information, see Drop data using NerdGraph. Learn more Send Prometheus metric data to New Relic Prometheus High Availability (HA)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 193.46349,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Drop data using <em>Prometheus</em> <em>remote</em> <em>write</em>",
        "sections": "Drop data using <em>Prometheus</em> <em>remote</em> <em>write</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " target the metric name using the __name__ label in combination with the instance name. <em>remote_write</em>: - url: https:&#x2F;&#x2F;metric-api.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;<em>write</em>?<em>prometheus</em>_server=macbook-server-cluster bearer_token: &lt;redacted&gt; <em>write</em>_relabel_configs: - source_labels: [&#x27;__name__&#x27;, &#x27;instance&#x27;] regex"
      },
      "id": "60e809e4e7b9d298bafc1035"
    },
    {
      "sections": [
        "Prometheus High Availability (HA)",
        "Tip",
        "External labels",
        "Prometheus Operator",
        "Standalone Prometheus"
      ],
      "title": "Prometheus High Availability (HA)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "3c0fddd6e878f30f8ba4c132f537b88cd47f2eba",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure/prometheus-high-availability-ha/",
      "published_at": "2021-10-08T06:41:31Z",
      "updated_at": "2021-03-13T02:41:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are using our Prometheus remote write integration in a high-availability (HA) configuration, you need to make sure your Prometheus servers aren't sending multiple copies of the same metrics to New Relic. This document describes how you can configure your remote write integration so that New Relic does not keep duplicated metrics. Tip For information on standard Prometheus remote write integration without using a high-availability configuration, see Set up your Prometheus remote write integration. External labels New Relic requires two external labels to deduplicate data from replicas in a high-availability configuration: Label name Description Example value prometheus A label whose value identifies the name of a high-availability cluster or group of Prometheus servers. monitoring-cluster prometheus_replica A label whose value identifies the unique replica sending this data. replica-1 The remaining sections explain how labels work with Prometheus Operator and standalone Prometheus. Prometheus Operator These external labels are added by default if you use Prometheus Operator version 0.19.0 (or higher). This applies whether you use Prometheus Operator directly or via the helm chart. The operator sets the value of the prometheus label (the one identifying a cluster) as <prometheus deployment namespace>/<prometheus deployment name>. For example, if your namespace for the Prometheus deployment is monitoring and the name of the deployment is prometheus-cluster1, the value is monitoring/prometheus-cluster1. The operator sets the value of the prometheus_replica label as the name of the pod for each replica. This follows the format replica-<replica number>, where the number is the ordinal of that replica (for example, the first replica is named replica-1). Tip If you still see duplicate copies of replica data, make sure you do not have replicaExternalLabelName or prometheusExternalLabelName in your Prometheus spec or chart configuration because these overrides change the label name. Standalone Prometheus When deploying a Prometheus server directly, you need to add the external labels to the configuration file. Here are two different example configurations for replicas within the same high-availability cluster: Replica 1 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-1 Copy Replica 2 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-2 Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 183.22258,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Prometheus</em> High Availability (HA)",
        "sections": "<em>Prometheus</em> High Availability (HA)",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": "If you are using our <em>Prometheus</em> <em>remote</em> <em>write</em> integration in a high-availability (HA) configuration, you need to make sure your <em>Prometheus</em> servers aren&#x27;t sending multiple copies of the same metrics to New Relic. This document describes how you can <em>configure</em> your <em>remote</em> <em>write</em> integration so that New"
      },
      "id": "6044e621196a67b846960f6b"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/debug-issues-data-sent-metric-api-prometheus-integration": [
    {
      "sections": [
        "Rate limit errors (Prometheus integration)",
        "Problem",
        "Solution",
        "Cause"
      ],
      "title": "Rate limit errors (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "3c8907d358e49c8dde5cab6dfa386deb5407d335",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/rate-limit-errors-prometheus-integration/",
      "published_at": "2021-10-08T06:48:27Z",
      "updated_at": "2021-08-08T19:29:48Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem Your Prometheus OpenMetrics integration for Docker or Kubernetes exceeded allowable metric rate limits. You want to see more details about why the NrIntegrationError event has been applied to your New Relic account. Solution To examine rate limit errors: Run a query of Prometheus metrics using the NrIntegrationError event, like this: FROM NrIntegrationError SELECT * WHERE newRelicFeature = 'Metrics' Copy Review additional troubleshooting procedures for NrIntegrationError events. To help prevent this from happening, you can use filters to control the types and amount of data that your integration sends to New Relic. For more information, see Ignore or include Prometheus metrics. Cause New Relic does a basic validation of your Prometheus OpenMetrics integration metrics when they are submitted. More extensive validation is performed asynchronously when processing the metrics. If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for Prometheus OpenMetrics integrations, New Relic will apply rate limits to your account and create an associated NrIntegrationError event.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 110.64099,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "sections": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ". If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for <em>Prometheus</em> OpenMetrics <em>integrations</em>, New Relic will apply rate limits to your account and create an associated NrIntegrationError event."
      },
      "id": "6045054528ccbc2c342c606b"
    },
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "1e21826044fef6dc088721c30a0fb6d61636919a",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2021-10-07T09:06:27Z",
      "updated_at": "2021-03-13T02:21:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 106.26464,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "6044e6a128ccbc64f22c6068"
    },
    {
      "sections": [
        "Excessive CPU or memory consumption",
        "Problem",
        "Solution"
      ],
      "title": "Excessive CPU or memory consumption",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "130c15368dcecaeb128789171b818014d112919d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/excessive-cpu-or-memory-consumption/",
      "published_at": "2021-10-08T06:46:20Z",
      "updated_at": "2021-03-16T06:18:08Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect scrape_duration. For example, a Prometheus OpenMetrics integration consumes 2.5 CPU and 700Mb of RAM because: It scrapes 800 targets, exposing 1000 timeseries each. Each one has a latency of 150ms with a scrape_duration of 30 seconds. To reduce resource consumption: Update the integration to the latest available image. Reduce harvest time by lowering emitter_harvest_period. (The default value is 1s, and the interval cannot be smaller than 200ms.) Since metrics are sent more often, memory consumption is reduced. Collect metrics less frequently by increasing scrape_duration to reduce both memory consumption and CPU usage. Reduce the number of workers to reduce both memory consumption and CPU usage. Scraping will slow down and could exceed scrape_duration. To do so: Update the integration to the latest version available of the image. Decrease worker_threads from the default value of 4 to your preferred value.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 100.59373,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect"
      },
      "id": "603e9b3b28ccbcaae3eba790"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/excessive-cpu-or-memory-consumption": [
    {
      "sections": [
        "Rate limit errors (Prometheus integration)",
        "Problem",
        "Solution",
        "Cause"
      ],
      "title": "Rate limit errors (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "3c8907d358e49c8dde5cab6dfa386deb5407d335",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/rate-limit-errors-prometheus-integration/",
      "published_at": "2021-10-08T06:48:27Z",
      "updated_at": "2021-08-08T19:29:48Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem Your Prometheus OpenMetrics integration for Docker or Kubernetes exceeded allowable metric rate limits. You want to see more details about why the NrIntegrationError event has been applied to your New Relic account. Solution To examine rate limit errors: Run a query of Prometheus metrics using the NrIntegrationError event, like this: FROM NrIntegrationError SELECT * WHERE newRelicFeature = 'Metrics' Copy Review additional troubleshooting procedures for NrIntegrationError events. To help prevent this from happening, you can use filters to control the types and amount of data that your integration sends to New Relic. For more information, see Ignore or include Prometheus metrics. Cause New Relic does a basic validation of your Prometheus OpenMetrics integration metrics when they are submitted. More extensive validation is performed asynchronously when processing the metrics. If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for Prometheus OpenMetrics integrations, New Relic will apply rate limits to your account and create an associated NrIntegrationError event.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 110.64099,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "sections": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ". If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for <em>Prometheus</em> OpenMetrics <em>integrations</em>, New Relic will apply rate limits to your account and create an associated NrIntegrationError event."
      },
      "id": "6045054528ccbc2c342c606b"
    },
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "1e21826044fef6dc088721c30a0fb6d61636919a",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2021-10-07T09:06:27Z",
      "updated_at": "2021-03-13T02:21:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 106.26464,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "6044e6a128ccbc64f22c6068"
    },
    {
      "sections": [
        "Restarts and gaps in data (Kubernetes)",
        "Problem",
        "Solution"
      ],
      "title": "Restarts and gaps in data (Kubernetes)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "215253af52dea03f816c3864e0f10911ae960897",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/restarts-gaps-data-kubernetes/",
      "published_at": "2021-10-08T06:47:19Z",
      "updated_at": "2021-03-16T04:42:39Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem When running the Prometheus OpenMetrics integration for Kubernetes, you notice restarts and gaps in data sent to New Relic. Solution When running the Prometheus OpenMetrics integration for Kubernetes with 500K data points per minute, be sure to set these limits: CPU limit: 1 core Memory limit: 1Gb Recommendation: Always run the Kubernetes scraper with one replica. Adding more replicas will result in duplicated data. If the CPU and memory limits are not sufficient, this can result in restarts and gaps in the data. To check the status and restart events for the scraper: kubectl describe pod -l \"app=nri-prometheus\" Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 100.59223,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem When running the <em>Prometheus</em> OpenMetrics integration for Kubernetes, you notice restarts and gaps in data sent to New Relic. Solution When running the <em>Prometheus</em> OpenMetrics integration for Kubernetes with 500K data points per minute, be sure to set these limits: CPU limit: 1 core Memory"
      },
      "id": "603ea39964441f1da84e884f"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/get-logs-prometheus-integration": [
    {
      "sections": [
        "Rate limit errors (Prometheus integration)",
        "Problem",
        "Solution",
        "Cause"
      ],
      "title": "Rate limit errors (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "3c8907d358e49c8dde5cab6dfa386deb5407d335",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/rate-limit-errors-prometheus-integration/",
      "published_at": "2021-10-08T06:48:27Z",
      "updated_at": "2021-08-08T19:29:48Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem Your Prometheus OpenMetrics integration for Docker or Kubernetes exceeded allowable metric rate limits. You want to see more details about why the NrIntegrationError event has been applied to your New Relic account. Solution To examine rate limit errors: Run a query of Prometheus metrics using the NrIntegrationError event, like this: FROM NrIntegrationError SELECT * WHERE newRelicFeature = 'Metrics' Copy Review additional troubleshooting procedures for NrIntegrationError events. To help prevent this from happening, you can use filters to control the types and amount of data that your integration sends to New Relic. For more information, see Ignore or include Prometheus metrics. Cause New Relic does a basic validation of your Prometheus OpenMetrics integration metrics when they are submitted. More extensive validation is performed asynchronously when processing the metrics. If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for Prometheus OpenMetrics integrations, New Relic will apply rate limits to your account and create an associated NrIntegrationError event.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 110.64098,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "sections": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ". If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for <em>Prometheus</em> OpenMetrics <em>integrations</em>, New Relic will apply rate limits to your account and create an associated NrIntegrationError event."
      },
      "id": "6045054528ccbc2c342c606b"
    },
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "1e21826044fef6dc088721c30a0fb6d61636919a",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2021-10-07T09:06:27Z",
      "updated_at": "2021-03-13T02:21:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 106.26464,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "6044e6a128ccbc64f22c6068"
    },
    {
      "sections": [
        "Excessive CPU or memory consumption",
        "Problem",
        "Solution"
      ],
      "title": "Excessive CPU or memory consumption",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "130c15368dcecaeb128789171b818014d112919d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/excessive-cpu-or-memory-consumption/",
      "published_at": "2021-10-08T06:46:20Z",
      "updated_at": "2021-03-16T06:18:08Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect scrape_duration. For example, a Prometheus OpenMetrics integration consumes 2.5 CPU and 700Mb of RAM because: It scrapes 800 targets, exposing 1000 timeseries each. Each one has a latency of 150ms with a scrape_duration of 30 seconds. To reduce resource consumption: Update the integration to the latest available image. Reduce harvest time by lowering emitter_harvest_period. (The default value is 1s, and the interval cannot be smaller than 200ms.) Since metrics are sent more often, memory consumption is reduced. Collect metrics less frequently by increasing scrape_duration to reduce both memory consumption and CPU usage. Reduce the number of workers to reduce both memory consumption and CPU usage. Scraping will slow down and could exceed scrape_duration. To do so: Update the integration to the latest version available of the image. Decrease worker_threads from the default value of 4 to your preferred value.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 100.59373,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect"
      },
      "id": "603e9b3b28ccbcaae3eba790"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/get-scraper-metrics-prometheus-integration": [
    {
      "sections": [
        "Rate limit errors (Prometheus integration)",
        "Problem",
        "Solution",
        "Cause"
      ],
      "title": "Rate limit errors (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "3c8907d358e49c8dde5cab6dfa386deb5407d335",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/rate-limit-errors-prometheus-integration/",
      "published_at": "2021-10-08T06:48:27Z",
      "updated_at": "2021-08-08T19:29:48Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem Your Prometheus OpenMetrics integration for Docker or Kubernetes exceeded allowable metric rate limits. You want to see more details about why the NrIntegrationError event has been applied to your New Relic account. Solution To examine rate limit errors: Run a query of Prometheus metrics using the NrIntegrationError event, like this: FROM NrIntegrationError SELECT * WHERE newRelicFeature = 'Metrics' Copy Review additional troubleshooting procedures for NrIntegrationError events. To help prevent this from happening, you can use filters to control the types and amount of data that your integration sends to New Relic. For more information, see Ignore or include Prometheus metrics. Cause New Relic does a basic validation of your Prometheus OpenMetrics integration metrics when they are submitted. More extensive validation is performed asynchronously when processing the metrics. If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for Prometheus OpenMetrics integrations, New Relic will apply rate limits to your account and create an associated NrIntegrationError event.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 110.64098,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "sections": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ". If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for <em>Prometheus</em> OpenMetrics <em>integrations</em>, New Relic will apply rate limits to your account and create an associated NrIntegrationError event."
      },
      "id": "6045054528ccbc2c342c606b"
    },
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "1e21826044fef6dc088721c30a0fb6d61636919a",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2021-10-07T09:06:27Z",
      "updated_at": "2021-03-13T02:21:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 106.26464,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "6044e6a128ccbc64f22c6068"
    },
    {
      "sections": [
        "Excessive CPU or memory consumption",
        "Problem",
        "Solution"
      ],
      "title": "Excessive CPU or memory consumption",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "130c15368dcecaeb128789171b818014d112919d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/excessive-cpu-or-memory-consumption/",
      "published_at": "2021-10-08T06:46:20Z",
      "updated_at": "2021-03-16T06:18:08Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect scrape_duration. For example, a Prometheus OpenMetrics integration consumes 2.5 CPU and 700Mb of RAM because: It scrapes 800 targets, exposing 1000 timeseries each. Each one has a latency of 150ms with a scrape_duration of 30 seconds. To reduce resource consumption: Update the integration to the latest available image. Reduce harvest time by lowering emitter_harvest_period. (The default value is 1s, and the interval cannot be smaller than 200ms.) Since metrics are sent more often, memory consumption is reduced. Collect metrics less frequently by increasing scrape_duration to reduce both memory consumption and CPU usage. Reduce the number of workers to reduce both memory consumption and CPU usage. Scraping will slow down and could exceed scrape_duration. To do so: Update the integration to the latest version available of the image. Decrease worker_threads from the default value of 4 to your preferred value.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 100.59373,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect"
      },
      "id": "603e9b3b28ccbcaae3eba790"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration": [
    {
      "sections": [
        "Rate limit errors (Prometheus integration)",
        "Problem",
        "Solution",
        "Cause"
      ],
      "title": "Rate limit errors (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "3c8907d358e49c8dde5cab6dfa386deb5407d335",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/rate-limit-errors-prometheus-integration/",
      "published_at": "2021-10-08T06:48:27Z",
      "updated_at": "2021-08-08T19:29:48Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem Your Prometheus OpenMetrics integration for Docker or Kubernetes exceeded allowable metric rate limits. You want to see more details about why the NrIntegrationError event has been applied to your New Relic account. Solution To examine rate limit errors: Run a query of Prometheus metrics using the NrIntegrationError event, like this: FROM NrIntegrationError SELECT * WHERE newRelicFeature = 'Metrics' Copy Review additional troubleshooting procedures for NrIntegrationError events. To help prevent this from happening, you can use filters to control the types and amount of data that your integration sends to New Relic. For more information, see Ignore or include Prometheus metrics. Cause New Relic does a basic validation of your Prometheus OpenMetrics integration metrics when they are submitted. More extensive validation is performed asynchronously when processing the metrics. If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for Prometheus OpenMetrics integrations, New Relic will apply rate limits to your account and create an associated NrIntegrationError event.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 110.64098,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "sections": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ". If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for <em>Prometheus</em> OpenMetrics <em>integrations</em>, New Relic will apply rate limits to your account and create an associated NrIntegrationError event."
      },
      "id": "6045054528ccbc2c342c606b"
    },
    {
      "sections": [
        "Excessive CPU or memory consumption",
        "Problem",
        "Solution"
      ],
      "title": "Excessive CPU or memory consumption",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "130c15368dcecaeb128789171b818014d112919d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/excessive-cpu-or-memory-consumption/",
      "published_at": "2021-10-08T06:46:20Z",
      "updated_at": "2021-03-16T06:18:08Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect scrape_duration. For example, a Prometheus OpenMetrics integration consumes 2.5 CPU and 700Mb of RAM because: It scrapes 800 targets, exposing 1000 timeseries each. Each one has a latency of 150ms with a scrape_duration of 30 seconds. To reduce resource consumption: Update the integration to the latest available image. Reduce harvest time by lowering emitter_harvest_period. (The default value is 1s, and the interval cannot be smaller than 200ms.) Since metrics are sent more often, memory consumption is reduced. Collect metrics less frequently by increasing scrape_duration to reduce both memory consumption and CPU usage. Reduce the number of workers to reduce both memory consumption and CPU usage. Scraping will slow down and could exceed scrape_duration. To do so: Update the integration to the latest version available of the image. Decrease worker_threads from the default value of 4 to your preferred value.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 100.59373,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect"
      },
      "id": "603e9b3b28ccbcaae3eba790"
    },
    {
      "sections": [
        "Restarts and gaps in data (Kubernetes)",
        "Problem",
        "Solution"
      ],
      "title": "Restarts and gaps in data (Kubernetes)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "215253af52dea03f816c3864e0f10911ae960897",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/restarts-gaps-data-kubernetes/",
      "published_at": "2021-10-08T06:47:19Z",
      "updated_at": "2021-03-16T04:42:39Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem When running the Prometheus OpenMetrics integration for Kubernetes, you notice restarts and gaps in data sent to New Relic. Solution When running the Prometheus OpenMetrics integration for Kubernetes with 500K data points per minute, be sure to set these limits: CPU limit: 1 core Memory limit: 1Gb Recommendation: Always run the Kubernetes scraper with one replica. Adding more replicas will result in duplicated data. If the CPU and memory limits are not sufficient, this can result in restarts and gaps in the data. To check the status and restart events for the scraper: kubectl describe pod -l \"app=nri-prometheus\" Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 100.59223,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem When running the <em>Prometheus</em> OpenMetrics integration for Kubernetes, you notice restarts and gaps in data sent to New Relic. Solution When running the <em>Prometheus</em> OpenMetrics integration for Kubernetes with 500K data points per minute, be sure to set these limits: CPU limit: 1 core Memory"
      },
      "id": "603ea39964441f1da84e884f"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/rate-limit-errors-prometheus-integration": [
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "1e21826044fef6dc088721c30a0fb6d61636919a",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2021-10-07T09:06:27Z",
      "updated_at": "2021-03-13T02:21:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 106.26464,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "6044e6a128ccbc64f22c6068"
    },
    {
      "sections": [
        "Excessive CPU or memory consumption",
        "Problem",
        "Solution"
      ],
      "title": "Excessive CPU or memory consumption",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "130c15368dcecaeb128789171b818014d112919d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/excessive-cpu-or-memory-consumption/",
      "published_at": "2021-10-08T06:46:20Z",
      "updated_at": "2021-03-16T06:18:08Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect scrape_duration. For example, a Prometheus OpenMetrics integration consumes 2.5 CPU and 700Mb of RAM because: It scrapes 800 targets, exposing 1000 timeseries each. Each one has a latency of 150ms with a scrape_duration of 30 seconds. To reduce resource consumption: Update the integration to the latest available image. Reduce harvest time by lowering emitter_harvest_period. (The default value is 1s, and the interval cannot be smaller than 200ms.) Since metrics are sent more often, memory consumption is reduced. Collect metrics less frequently by increasing scrape_duration to reduce both memory consumption and CPU usage. Reduce the number of workers to reduce both memory consumption and CPU usage. Scraping will slow down and could exceed scrape_duration. To do so: Update the integration to the latest version available of the image. Decrease worker_threads from the default value of 4 to your preferred value.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 100.59373,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect"
      },
      "id": "603e9b3b28ccbcaae3eba790"
    },
    {
      "sections": [
        "Restarts and gaps in data (Kubernetes)",
        "Problem",
        "Solution"
      ],
      "title": "Restarts and gaps in data (Kubernetes)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "215253af52dea03f816c3864e0f10911ae960897",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/restarts-gaps-data-kubernetes/",
      "published_at": "2021-10-08T06:47:19Z",
      "updated_at": "2021-03-16T04:42:39Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem When running the Prometheus OpenMetrics integration for Kubernetes, you notice restarts and gaps in data sent to New Relic. Solution When running the Prometheus OpenMetrics integration for Kubernetes with 500K data points per minute, be sure to set these limits: CPU limit: 1 core Memory limit: 1Gb Recommendation: Always run the Kubernetes scraper with one replica. Adding more replicas will result in duplicated data. If the CPU and memory limits are not sufficient, this can result in restarts and gaps in the data. To check the status and restart events for the scraper: kubectl describe pod -l \"app=nri-prometheus\" Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 100.59223,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem When running the <em>Prometheus</em> OpenMetrics integration for Kubernetes, you notice restarts and gaps in data sent to New Relic. Solution When running the <em>Prometheus</em> OpenMetrics integration for Kubernetes with 500K data points per minute, be sure to set these limits: CPU limit: 1 core Memory"
      },
      "id": "603ea39964441f1da84e884f"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/restarts-gaps-data-kubernetes": [
    {
      "sections": [
        "Rate limit errors (Prometheus integration)",
        "Problem",
        "Solution",
        "Cause"
      ],
      "title": "Rate limit errors (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "3c8907d358e49c8dde5cab6dfa386deb5407d335",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/rate-limit-errors-prometheus-integration/",
      "published_at": "2021-10-08T06:48:27Z",
      "updated_at": "2021-08-08T19:29:48Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem Your Prometheus OpenMetrics integration for Docker or Kubernetes exceeded allowable metric rate limits. You want to see more details about why the NrIntegrationError event has been applied to your New Relic account. Solution To examine rate limit errors: Run a query of Prometheus metrics using the NrIntegrationError event, like this: FROM NrIntegrationError SELECT * WHERE newRelicFeature = 'Metrics' Copy Review additional troubleshooting procedures for NrIntegrationError events. To help prevent this from happening, you can use filters to control the types and amount of data that your integration sends to New Relic. For more information, see Ignore or include Prometheus metrics. Cause New Relic does a basic validation of your Prometheus OpenMetrics integration metrics when they are submitted. More extensive validation is performed asynchronously when processing the metrics. If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for Prometheus OpenMetrics integrations, New Relic will apply rate limits to your account and create an associated NrIntegrationError event.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 110.64098,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "sections": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ". If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for <em>Prometheus</em> OpenMetrics <em>integrations</em>, New Relic will apply rate limits to your account and create an associated NrIntegrationError event."
      },
      "id": "6045054528ccbc2c342c606b"
    },
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "1e21826044fef6dc088721c30a0fb6d61636919a",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2021-10-07T09:06:27Z",
      "updated_at": "2021-03-13T02:21:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 106.26464,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "6044e6a128ccbc64f22c6068"
    },
    {
      "sections": [
        "Excessive CPU or memory consumption",
        "Problem",
        "Solution"
      ],
      "title": "Excessive CPU or memory consumption",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "130c15368dcecaeb128789171b818014d112919d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/excessive-cpu-or-memory-consumption/",
      "published_at": "2021-10-08T06:46:20Z",
      "updated_at": "2021-03-16T06:18:08Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect scrape_duration. For example, a Prometheus OpenMetrics integration consumes 2.5 CPU and 700Mb of RAM because: It scrapes 800 targets, exposing 1000 timeseries each. Each one has a latency of 150ms with a scrape_duration of 30 seconds. To reduce resource consumption: Update the integration to the latest available image. Reduce harvest time by lowering emitter_harvest_period. (The default value is 1s, and the interval cannot be smaller than 200ms.) Since metrics are sent more often, memory consumption is reduced. Collect metrics less frequently by increasing scrape_duration to reduce both memory consumption and CPU usage. Reduce the number of workers to reduce both memory consumption and CPU usage. Scraping will slow down and could exceed scrape_duration. To do so: Update the integration to the latest version available of the image. Decrease worker_threads from the default value of 4 to your preferred value.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 100.59373,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect"
      },
      "id": "603e9b3b28ccbcaae3eba790"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/sparse-data-missing-metrics-data-gaps": [
    {
      "sections": [
        "Rate limit errors (Prometheus integration)",
        "Problem",
        "Solution",
        "Cause"
      ],
      "title": "Rate limit errors (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "3c8907d358e49c8dde5cab6dfa386deb5407d335",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/rate-limit-errors-prometheus-integration/",
      "published_at": "2021-10-08T06:48:27Z",
      "updated_at": "2021-08-08T19:29:48Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem Your Prometheus OpenMetrics integration for Docker or Kubernetes exceeded allowable metric rate limits. You want to see more details about why the NrIntegrationError event has been applied to your New Relic account. Solution To examine rate limit errors: Run a query of Prometheus metrics using the NrIntegrationError event, like this: FROM NrIntegrationError SELECT * WHERE newRelicFeature = 'Metrics' Copy Review additional troubleshooting procedures for NrIntegrationError events. To help prevent this from happening, you can use filters to control the types and amount of data that your integration sends to New Relic. For more information, see Ignore or include Prometheus metrics. Cause New Relic does a basic validation of your Prometheus OpenMetrics integration metrics when they are submitted. More extensive validation is performed asynchronously when processing the metrics. If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for Prometheus OpenMetrics integrations, New Relic will apply rate limits to your account and create an associated NrIntegrationError event.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 110.640976,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "sections": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ". If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for <em>Prometheus</em> OpenMetrics <em>integrations</em>, New Relic will apply rate limits to your account and create an associated NrIntegrationError event."
      },
      "id": "6045054528ccbc2c342c606b"
    },
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "1e21826044fef6dc088721c30a0fb6d61636919a",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2021-10-07T09:06:27Z",
      "updated_at": "2021-03-13T02:21:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 106.26464,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "6044e6a128ccbc64f22c6068"
    },
    {
      "sections": [
        "Excessive CPU or memory consumption",
        "Problem",
        "Solution"
      ],
      "title": "Excessive CPU or memory consumption",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "130c15368dcecaeb128789171b818014d112919d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/excessive-cpu-or-memory-consumption/",
      "published_at": "2021-10-08T06:46:20Z",
      "updated_at": "2021-03-16T06:18:08Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect scrape_duration. For example, a Prometheus OpenMetrics integration consumes 2.5 CPU and 700Mb of RAM because: It scrapes 800 targets, exposing 1000 timeseries each. Each one has a latency of 150ms with a scrape_duration of 30 seconds. To reduce resource consumption: Update the integration to the latest available image. Reduce harvest time by lowering emitter_harvest_period. (The default value is 1s, and the interval cannot be smaller than 200ms.) Since metrics are sent more often, memory consumption is reduced. Collect metrics less frequently by increasing scrape_duration to reduce both memory consumption and CPU usage. Reduce the number of workers to reduce both memory consumption and CPU usage. Scraping will slow down and could exceed scrape_duration. To do so: Update the integration to the latest version available of the image. Decrease worker_threads from the default value of 4 to your preferred value.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 100.59372,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect"
      },
      "id": "603e9b3b28ccbcaae3eba790"
    }
  ],
  "/docs/integrations/prometheus-integrations/view-query-data/supported-promql-features": [
    {
      "sections": [
        "Translate PromQL queries to NRQL",
        "Tip",
        "Prometheus and New Relic metric types",
        "Mapping between NRQL and our PromQL-style queries",
        "PromQL-style query example",
        "NRQL query example",
        "Filter examples",
        "PromQL-style to NRQL query examples",
        "For more help"
      ],
      "title": "Translate PromQL queries to NRQL",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "fcf45fb8fb49f9d22f74574c2e7032533377e584",
      "image": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/translate-promql-queries-nrql/images/PROMQL-query-2.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/translate-promql-queries-nrql/",
      "published_at": "2021-10-08T06:49:32Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Do you have a PromQL query you’d like to convert to NRQL? This document provides examples that show you how to convert some common PromQL queries to NRQL queries. You can use our PromQL-style query language to explore your Prometheus OpenMetrics integration data along with other data sent to New Relic. Tip To run PromQL-style queries in New Relic One, go to the query builder advanced PromQL-style mode. Prometheus and New Relic metric types The different metric types supported by Prometheus and New Relic are related to each other: New Relic Prometheus Description Count Counter The Prometheus counter is a cumulative sum while the New Relic count is a delta sum. For example, if you see 2 requests in the first reporting period and 3 requests in the second reporting period. The Prometheus counter will report 2 and then 5, while the New Relic count will report 2 and then 3. Gauge Gauge A Prometheus gauge is similar to a New Relic gauge. Multiple counts Histogram Prometheus automatically maps a histogram to a set of counters. In New Relic, these counters should be changed to deltas and reported as counts. Gauges and counts Summary Prometheus represents a Summary with a given basename as the following time series: a basename_sum a basename_count and 0 or more of basename{quantile=\".xx\"...} metrics New Relic maps the _sum as a Summary, the _count as a Counter, and each quantile metric as a Gauge. Summary (No equivalent in Prometheus) New Relic has a distinct metric type called a summary that is different than the Prometheus summary. It is designed for reporting aggregated discrete events so that you can query the count, sum, min, max, and average values. Mapping between NRQL and our PromQL-style queries Tip To see how New Relic translates PromQL-style queries to NRQL, write a query in the query builder PromQL-style tab, then switch to the NRQL tab. This table shows the mapping between NRQL and our PromQL-style queries when exploring data. For more contextual information, see the examples. Description Mapping between NRQL and PromQL-style queries Search for attributes: Explore the attributes on the container_memory_usage_bytes metric. PromQL: container_memory_usage_bytes Copy NRQL: FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes' Copy Find attribute's value: Explore the current value of the container_memory_usage_bytes metric for unique id attributes. PromQL: sum(container_memory_usage_bytes) by (id) Copy NRQL: FROM Metric SELECT latest(container_memory_usage_bytes) FACET id Copy Visualize the attribute's value: Chart the value of the container_memory_usage_bytes metric with the given id attribute value. PromQL: container_memory_usage_bytes{id=\"/\"} Copy NRQL: FROM Metric SELECT latest(container_memory_usage_bytes) WHERE id = '/' TIMESERIES Copy PromQL-style query example 1. Start your query. When exploring your data for a particular metric in PromQL, such as memory by container usage in bytes, you can start with a query such as: container_memory_usage_bytes Copy This will chart all the unique metric timeseries for the input metric. 2. Filter the query results. Looking at the data, you can add more query parameters to filter down the number of metric timeseries. For example, if you only want timeseries where the id is /, the PromQL-style query will be: container_memory_usage_bytes{id=\"/\"} Copy PromQL-style example: To filter the data, run this PromQL-style query: container_memory_usage_bytes { id=\"/\"}. NRQL query example 1. Query available metrics. To explore your data, start by looking at all the available metrics. Use the following NRQL query: FROM Metric SELECT uniques(metricName) Copy 2. Find unique attributes. Once you have found the metric you want to review, such as container_memory_usage_bytes, you can find the unique attributes with the following query: FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes' Copy The results will show each available attribute key and the value type (string, boolean, or number). 3. Aggregate and chart the metrics. To chart metrics using NRQL, you first need an aggregation function. For example, you can use latest for gauges, sum for counts, and average for summaries. As the following chart shows, all the unique timeseries are aggregated into one unique timeseries by default: one.newrelic.com > Query your data: This example shows the data you see after running FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes'. 4. View metrics by ID. To view the unique metric timeseries with various id values, run the following query: FROM Metric SELECT latest(container_memory_usage_bytes) FACET id Copy one.newrelic.com > Query your data: This example shows the data you see after running FROM Metric SELECT latest(container_memory_usage_bytes) FACET id. 5. Add the selected ID to the query. Next you can select an id value and put it in the NRQL where clause. FROM Metric SELECT latest(container_memory_usage_bytes) WHERE id = \"/\" timeseries Copy one.newrelic.com > Query your data: This example shows the data displayed after running From Metric select latest(container_memory_usage_bytes) where id = \"/\" timeseries. Filter examples Both our PromQL-style query language and NRQL provide syntax to filter down the number of unique metric timeseries. PromQL-style uses brackets to filter. NRQL uses a WHERE clause. Here are some example queries: Description PromQL-style and NRQL queries Select data with specific values. PromQL: go_memstats_heap_alloc_bytes{job=\"apiserver\", instance=\"1234\"}) Copy NRQL: To only select data with specific values in NRQL, use the WHERE clause with =. In this example, all data must have the selected value for job and handler. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHERE job = 'apiserver' AND instance = '1234' TIMESERIES Copy Select data with multiple values. PromQL: go_memstats_heap_alloc_bytes{environment=~\"staging|testing|development\",method!=\"GET\"} Copy NRQL: In NRQL use the in clause to select multiple values for an attribute and the != sign to select all values but the one listed. In this example, the environment can be staging, testing, or development, and the method cannot be GET. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHERE environment IN ('staging', 'testing', 'development') AND method != 'GET' TIMESERIES Copy Select data using partial string values. PromQL: go_memstats_heap_alloc_bytes{job=~\"api.*\"} Copy NRQL: In NRQL use the LIKE clause to match part of a string value. In this example, all data will be returned where the job attributes start with api. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHEREe job LIKE 'api%' TIMESERIES Copy PromQL-style to NRQL query examples You can simulate the following PromQL-style queries with NRQL queries: Description PromQL-style and NRQL queries Measure the per second rate over the last minute of the http_request_total metric. PromQL: sum(rate(http_requests_total[1m])) Copy NRQL: FROM Metric SELECT rate(sum(http_request_total), 1 second) TIMESERIES 1 minute Copy Chart the difference of the two metrics, then divide by 1024. PromQL: (instance_memory_limit_bytes - instance_memory_usage_bytes) / 1024 Copy NRQL: FROM Metric SELECT (latest(instance_memory_limit_bytes) - latest(instance_memory_usage_bytes)) / 1024 TIMESERIES Copy Provide the summed rate per 30-second interval by each handler. PromQL: sum(rate(http_requests_total[30s])) by (handler) Copy NRQL: FROM Metric SELECT rate(sum(http_requests_total), 30 seconds) FACET handler TIMESERIES Copy Chart the difference in the two metrics where the instance is named foo and the fstype is either ext4 or xfs. PromQL: (node_filesystem_free_bytes{instance='foo',fstype=~\"ext4|xfs\"} / node_filesystem_size_bytes{instance='foo',fstype=~\"ext4|xfs\"}) Copy NRQL: FROM Metric SELECT latest(node_filesystem_free_bytes) / latest(node_filesystem_size_bytes) WHERE instance = 'foo' AND fstype IN ('ext4', 'xfs') Copy For more help",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 171.36046,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Translate PromQL <em>queries</em> to NRQL",
        "sections": "<em>Prometheus</em> <em>and</em> New Relic metric types",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": "Do you have a PromQL <em>query</em> you’d like to convert to NRQL? This document provides examples that show you how to convert some common PromQL queries to NRQL queries. You can use our PromQL-style <em>query</em> language to explore your <em>Prometheus</em> OpenMetrics integration <em>data</em> along with other <em>data</em> sent to New"
      },
      "id": "603ead4528ccbcbecfeba77b"
    },
    {
      "sections": [
        "View and query your Prometheus data",
        "Default attributes for the OpenMetrics integration",
        "Default attributes for the remote write integration",
        "NRQL query examples",
        "Get metric names",
        "Get the attributes for a metric",
        "Get the values for an attribute in OpenMetrics",
        "Build the query",
        "Get metric values",
        "Get a chart of the metric",
        "Query counter metrics (deltas)",
        "View connected Redis clients per pod with OpenMetrics",
        "Docker: View average memory free for scraped endpoints",
        "Kubernetes: View average memory usage for pods in a deployment",
        "View data in New Relic",
        "Create histograms and summaries"
      ],
      "title": "View and query your Prometheus data",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "91a0388492ae73a9ddb8a1701b639a6b6a71822a",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/view-query-your-prometheus-data/",
      "published_at": "2021-10-07T03:07:22Z",
      "updated_at": "2021-03-16T04:13:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To query and visualize the metrics collected for your Prometheus OpenMetrics or remote write integration with New Relic, you can use NRQL. You can also translate your PromQL-style queries to NRQL using either Grafana or the query builder. All metrics for Docker and Kubernetes are stored in the Metric type. Default attributes for the OpenMetrics integration By default, the following attributes will be added to all metrics for Docker and Kubernetes integrations: Default attributes (all integrations) Description clusterName The name of the cluster provided in the scraper configuration. integrationName The name of this integration (nri-prometheus). integrationVersion The version of the integration; for example, 0.2.0. metricName The name of the metric itself. nrMetricType The type of the New Relic Metric type; for example, Gauges. promMetricType The metric type of the Prometheus metric scrapedEndpoint The URL of the endpoint is being scraped. Kubernetes: If the scraper is running in Kubernetes, New Relic also adds the following attributes to all the metrics: Additional Kubernetes attributes Description deploymentName Name of the deployment, if scraping a pod. label The Kubernetes labels of the object being scraped, prefixed by \"label\". namespaceName Name of the namespace. nodeName Name of the node where the pod being scraped is running, if applicable. podName Name of the pod being scraped, if applicable. serviceName Name of the service being scraped, if applicable Default attributes for the remote write integration By default, the following attributes will be added to Prometheus remote write metrics: Default attributes (all integrations) Description prometheus_server A user supplied label specified as a Prometheus remote write URL parameter. The value supplied should be unique as it is intended to differentiate between source Prometheus servers at query time. Unspecified by default. newrelic.source The name of the New Relic ingest point (prometheusAPI). instrumentation.provider prometheus instrumentation.name remote-write instrumentation.source A user supplied identifier for the source of the Prometheus data that matches the value of prometheus_server. instrumentation.version Used to identify the version of the remote write API; for example, 0.0.1. NRQL query examples When you build queries, be aware that there is no linking between the metrics, entities, and attributes. Use the following NRQL queries to find out which metrics are available and which attributes are present on these metrics: Get metric names To get all metric names for OpenMetrics: FROM Metric SELECT uniques(metricName) Copy To get metric names for a remote write integration: FROM Metric SELECT uniques(metricName) WHERE instrumentation.provider='prometheus' AND instrumentation.name='remote-write' Copy To get metric names for a remote write integration from a single Prometheus source: FROM Metric SELECT uniques(metricName) WHERE instrumentation.provider='prometheus' AND instrumentation.name='remote-write' AND instrumentation.source='<ds>' Copy To get metric names for a specific OpenMetrics endpoint: FROM Metric SELECT uniques(metricName) WHERE scrapedEndpoint='<ep>' Copy To get metric names for a specific OpenMetrics cluster, namespace, or pod: FROM Metric SELECT uniques(metricName) WHERE clusterName='<cn>' Copy FROM Metric SELECT uniques(metricName) WHERE namespaceName='<ns>' Copy FROM Metric SELECT uniques(metricName) WHERE podName='<pod>' Copy Get the attributes for a metric To get all attributes for the selected metric: FROM Metric SELECT keyset() WHERE metricName='<mn>' Copy Get the values for an attribute in OpenMetrics The autocomplete will show all values of the attribute, regardless of the pod. To determine the attribute values for a specific pod: FROM Metric SELECT uniques(<attribute>) WHERE metricName='<mn>' AND podName='<pod>' Copy Build the query Using metric name and attributes, you can query your data. For more information about facets, time series, and time selection, see the NRQL documentation. To build PromQL-style queries, see our docs. Get metric values To get raw metric values: FROM Metric SELECT <metricName> WHERE <attribute>='<value>' Copy Get a chart of the metric To get a chart of the metric with an aggregator of average, min, max, or sum: FROM Metric SELECT <aggregator>(<metricname>) WHERE <attribute>='<value>' TIMESERIES Copy Query counter metrics (deltas) Currently the integration calculates the deltas for counter metrics. This is why queries on counter metrics will show the deltas of the counter instead of the absolute value of the counter. View connected Redis clients per pod with OpenMetrics Docker: This example assumes you are scraping Redis exporters. To view the number of connected Redis clients per endpoint in a cluster: FROM Metric SELECT latest(redis_connected_clients) WHERE clusterName='my-cluster' FACET scrapedEndpoint TIMESERIES Copy Kubernetes: This example assumes that you have Redis pods with the Redis exporter installed. To view the number of connected Redis clients per pod in the default namespace: FROM Metric SELECT latest(redis_connected_clients) WHERE namespaceName='default' FACET podName TIMESERIES Copy Docker: View average memory free for scraped endpoints This example assumes you are scraping node exporters for Docker and want to use OpenMetrics. To view average memory free for all scraped endpoints in a cluster: FROM Metric SELECT average(node_memory_MemFree_bytes) WHERE clusterName='my-cluster' Copy Kubernetes: View average memory usage for pods in a deployment To view average memory usage for all pods in a Kubernetes deployment using OpenMetrics: FROM Metric SELECT average(container_memory_usage_bytes) WHERE deploymentName='my-app-deployment' AND namespaceName='default' Copy View data in New Relic When you query the data, you can view the results in the New Relic UI. You can also visualize the data as charts, histograms, etc. To view the NRQL query results for your Prometheus integration's data: Go to one.newrelic.com > Query your data. For more information, see New Relic's query builder documentation. Create histograms and summaries With remote write or version 1.2.0 or higher of the Prometheus OpenMetrics integration, you can create histograms and percentiles (summaries) of your data. The OpenMetrics data is based on New Relic's guidelines in GitHub for higher level metric abstractions, while the remote write data closely matches the schema of the original Prometheus data. Data presentation Comments Histograms A bucket <basename>_bucket{le=\"42\"} will be sent as this: For OpenMetrics: <basename>_buckets For remote write: <basename>_bucket The dimension will be this: For OpenMetrics: {histogram.bucket.upperBound=\"42\"} For remote write: {histogram.bucket.le=\"42\"} Percentiles Quantiles (summaries) are transformed into percentiles. A metric <basename>{quantile=\"0.3\"} will be sent to New Relic as <basename>.percentiles. The dimension will be this: {percentile=\"30\"} NRQL has two functions that work on remote write ingested PromQL: bucketPercentile() and histogram(). The links include query examples.These two functions don't work on OpenMetrics ingested buckets. To better support visualization of histograms, percentiles are calculated based on the histogram metrics and sent to New Relic. To configure the calculated percentiles for OpenMetrics, use the percentiles configuration option.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 171.36028,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>View</em> <em>and</em> <em>query</em> your <em>Prometheus</em> <em>data</em>",
        "sections": "<em>View</em> <em>and</em> <em>query</em> your <em>Prometheus</em> <em>data</em>",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": " identifier for the source of the <em>Prometheus</em> <em>data</em> that matches the value of <em>prometheus</em>_server. instrumentation.version Used to identify the version of the remote write API; for example, 0.0.1. NRQL <em>query</em> examples When you build queries, be aware that there is no linking between the metrics, entities"
      },
      "id": "603eb04be7b9d20f9d2a07eb"
    },
    {
      "sections": [
        "Rate limit errors (Prometheus integration)",
        "Problem",
        "Solution",
        "Cause"
      ],
      "title": "Rate limit errors (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "3c8907d358e49c8dde5cab6dfa386deb5407d335",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/rate-limit-errors-prometheus-integration/",
      "published_at": "2021-10-08T06:48:27Z",
      "updated_at": "2021-08-08T19:29:48Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem Your Prometheus OpenMetrics integration for Docker or Kubernetes exceeded allowable metric rate limits. You want to see more details about why the NrIntegrationError event has been applied to your New Relic account. Solution To examine rate limit errors: Run a query of Prometheus metrics using the NrIntegrationError event, like this: FROM NrIntegrationError SELECT * WHERE newRelicFeature = 'Metrics' Copy Review additional troubleshooting procedures for NrIntegrationError events. To help prevent this from happening, you can use filters to control the types and amount of data that your integration sends to New Relic. For more information, see Ignore or include Prometheus metrics. Cause New Relic does a basic validation of your Prometheus OpenMetrics integration metrics when they are submitted. More extensive validation is performed asynchronously when processing the metrics. If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for Prometheus OpenMetrics integrations, New Relic will apply rate limits to your account and create an associated NrIntegrationError event.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 84.82488,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "sections": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem Your <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes exceeded allowable metric rate limits. You want to see more details about why the NrIntegrationError event has been applied to your New Relic account. Solution To examine rate limit errors: Run a <em>query</em> of <em>Prometheus</em> metrics"
      },
      "id": "6045054528ccbc2c342c606b"
    }
  ],
  "/docs/integrations/prometheus-integrations/view-query-data/translate-promql-queries-nrql": [
    {
      "sections": [
        "Supported PromQL Features",
        "Important",
        "Supported features",
        "Aggregation operators and functions",
        "Arithmetic binary operators",
        "Logical operators",
        "Date/time functions",
        "Mathematical functions",
        "Rate-like functions",
        "Predictive functions",
        "Time-series selectors",
        "PromQL troubleshooting",
        "Metric types",
        "Limits",
        "Range vector selectors (sliding windows and smoothing behavior)",
        "Query range and data scraping intervals"
      ],
      "title": "Supported PromQL Features",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "d4a93b9db3bfe5639ed01968b6e55a8e0aaa9389",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/supported-promql-features/",
      "published_at": "2021-10-08T06:48:27Z",
      "updated_at": "2021-03-16T04:42:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic supports PromQL-style queries, and our query builder offers a PromQL-style query mode that translates PromQL syntax queries into the closest NRQL approximation. Although the method of approximation means that a handful of edge cases are not fully supported, it provides coverage for an overwhelming majority of queries, supporting over 99.5% of queries across the 7.8 million top Grafana dashboard downloads. Read on to learn about how we work with PromQL queries, as well as differences between standard PromQL and our PromQL-like query language we want you to be aware of. Important For general information about Prometheus queries and operators, see the Prometheus.io documentation. Supported features We support the following aggregation, arithmetic, mathematical, and rate-like functions. As we continue to expand support for Prometheus and PromQL, this list will be updated. Aggregation operators and functions Aggregation operators: avg() count() min() max() quantile() stddev() stdvar() sum() topk() Aggregation functions: histogram_quantile() <aggregation>_over_time() functions: avg_over_time count_over_time min_over_time max_over_time quantile_over_time stdev_over_time stvar_over_time sum_over_time Arithmetic binary operators + (addition) - (subtraction) * (multiplication) / (division) % (percent) ^ (power/exponents) Logical operators and or Date/time functions day_of_month() day_of_week() days_in_month() hour() minute() month() time() timestamp() year() Mathematical functions abs() ceil() clamp_max() clamp_min() exp() floor() ln() log10() log2() round() sqrt() Rate-like functions delta() deriv() idelta() increase() irate() rate() Predictive functions predict_linear Time-series selectors We offer support for PromQL time-series selectors including the following: instant vector selectors range series selectors offset modifier Important We only support offset queries if every vector in the query has the same offset value. PromQL troubleshooting This section describes differences in behavior between PromQL and our PromQL-style query behavior and how to work with and around these differences. This is particularly relevant if you want to use advanced queries and our PromQL-style mode in the query builder. Metric types Prometheus recommendations note that you should only use some functions, like delta(), on gauges, and only use others like rate() and increase() on counters, but queries in Prometheus still work most of the time even if they don’t follow those instructions. However, because NRDB converts PromQL-style accumulating counters to delta counters, our implementation is unforgiving when using these functions on the wrong data type and will produce different or incorrect answers. For this reason, it's best to follow all Prometheus recommendations when working with our PromQL-style queries, even if you don't follow these recommendations in Prometheus. Limits In order to ensure the stability and performance of our system for all users, we place some limits on what queries can be run. In all cases, we enforce a limit of 366 steps in range queries. We also default to only returning 100 timeseries from queries by default. If you want to see more (or fewer), you need to explicitly add a topk() to your query. (Note that the topk() implementation in our PromQL-style query is different from that of Prometheus.) We limit the total memory a query can use. This means that requests for large numbers of time steps or large numbers of time series may be rejected, particularly if they are combined with an aggregation like unique count or quantile which require significantly more memory to compute than simple arithmetic aggregations. Range vector selectors (sliding windows and smoothing behavior) We provide support for sliding window timeseries aggregations. For more information, see our NRQL syntax, clauses, and functions resource and our sliding windows deep dive. For information on translating between NRQL and our PromQL-style language, see Translate PromQL queries to NRQL. Query range and data scraping intervals The range of your query in PromQL must be larger than the duration of the step size of the query to avoid the error \"TIMESERIES bucket size is larger than the current time window\". We inspect data up to one minute old when servicing instant queries. If your scrape interval is greater than 1 minute, some queries may result in No data found. Avoid this by sending data at least once per minute. If the timeseries unit for your NRQL query is less than the scrape interval for your application, some periods will lack data, and the resulting graph may be jagged or contain peaks and valleys. In general, set the step size to your scrape interval, or higher.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 171.36105,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Query</em> range <em>and</em> <em>data</em> scraping intervals",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": " with and around these differences. This is particularly relevant if you want to use advanced queries and our PromQL-style mode in the <em>query</em> builder. Metric types <em>Prometheus</em> recommendations note that you should only use some functions, like delta(), on gauges, and only use others like rate() and increase"
      },
      "id": "603e9523e7b9d292ec2a07ce"
    },
    {
      "sections": [
        "View and query your Prometheus data",
        "Default attributes for the OpenMetrics integration",
        "Default attributes for the remote write integration",
        "NRQL query examples",
        "Get metric names",
        "Get the attributes for a metric",
        "Get the values for an attribute in OpenMetrics",
        "Build the query",
        "Get metric values",
        "Get a chart of the metric",
        "Query counter metrics (deltas)",
        "View connected Redis clients per pod with OpenMetrics",
        "Docker: View average memory free for scraped endpoints",
        "Kubernetes: View average memory usage for pods in a deployment",
        "View data in New Relic",
        "Create histograms and summaries"
      ],
      "title": "View and query your Prometheus data",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "91a0388492ae73a9ddb8a1701b639a6b6a71822a",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/view-query-your-prometheus-data/",
      "published_at": "2021-10-07T03:07:22Z",
      "updated_at": "2021-03-16T04:13:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To query and visualize the metrics collected for your Prometheus OpenMetrics or remote write integration with New Relic, you can use NRQL. You can also translate your PromQL-style queries to NRQL using either Grafana or the query builder. All metrics for Docker and Kubernetes are stored in the Metric type. Default attributes for the OpenMetrics integration By default, the following attributes will be added to all metrics for Docker and Kubernetes integrations: Default attributes (all integrations) Description clusterName The name of the cluster provided in the scraper configuration. integrationName The name of this integration (nri-prometheus). integrationVersion The version of the integration; for example, 0.2.0. metricName The name of the metric itself. nrMetricType The type of the New Relic Metric type; for example, Gauges. promMetricType The metric type of the Prometheus metric scrapedEndpoint The URL of the endpoint is being scraped. Kubernetes: If the scraper is running in Kubernetes, New Relic also adds the following attributes to all the metrics: Additional Kubernetes attributes Description deploymentName Name of the deployment, if scraping a pod. label The Kubernetes labels of the object being scraped, prefixed by \"label\". namespaceName Name of the namespace. nodeName Name of the node where the pod being scraped is running, if applicable. podName Name of the pod being scraped, if applicable. serviceName Name of the service being scraped, if applicable Default attributes for the remote write integration By default, the following attributes will be added to Prometheus remote write metrics: Default attributes (all integrations) Description prometheus_server A user supplied label specified as a Prometheus remote write URL parameter. The value supplied should be unique as it is intended to differentiate between source Prometheus servers at query time. Unspecified by default. newrelic.source The name of the New Relic ingest point (prometheusAPI). instrumentation.provider prometheus instrumentation.name remote-write instrumentation.source A user supplied identifier for the source of the Prometheus data that matches the value of prometheus_server. instrumentation.version Used to identify the version of the remote write API; for example, 0.0.1. NRQL query examples When you build queries, be aware that there is no linking between the metrics, entities, and attributes. Use the following NRQL queries to find out which metrics are available and which attributes are present on these metrics: Get metric names To get all metric names for OpenMetrics: FROM Metric SELECT uniques(metricName) Copy To get metric names for a remote write integration: FROM Metric SELECT uniques(metricName) WHERE instrumentation.provider='prometheus' AND instrumentation.name='remote-write' Copy To get metric names for a remote write integration from a single Prometheus source: FROM Metric SELECT uniques(metricName) WHERE instrumentation.provider='prometheus' AND instrumentation.name='remote-write' AND instrumentation.source='<ds>' Copy To get metric names for a specific OpenMetrics endpoint: FROM Metric SELECT uniques(metricName) WHERE scrapedEndpoint='<ep>' Copy To get metric names for a specific OpenMetrics cluster, namespace, or pod: FROM Metric SELECT uniques(metricName) WHERE clusterName='<cn>' Copy FROM Metric SELECT uniques(metricName) WHERE namespaceName='<ns>' Copy FROM Metric SELECT uniques(metricName) WHERE podName='<pod>' Copy Get the attributes for a metric To get all attributes for the selected metric: FROM Metric SELECT keyset() WHERE metricName='<mn>' Copy Get the values for an attribute in OpenMetrics The autocomplete will show all values of the attribute, regardless of the pod. To determine the attribute values for a specific pod: FROM Metric SELECT uniques(<attribute>) WHERE metricName='<mn>' AND podName='<pod>' Copy Build the query Using metric name and attributes, you can query your data. For more information about facets, time series, and time selection, see the NRQL documentation. To build PromQL-style queries, see our docs. Get metric values To get raw metric values: FROM Metric SELECT <metricName> WHERE <attribute>='<value>' Copy Get a chart of the metric To get a chart of the metric with an aggregator of average, min, max, or sum: FROM Metric SELECT <aggregator>(<metricname>) WHERE <attribute>='<value>' TIMESERIES Copy Query counter metrics (deltas) Currently the integration calculates the deltas for counter metrics. This is why queries on counter metrics will show the deltas of the counter instead of the absolute value of the counter. View connected Redis clients per pod with OpenMetrics Docker: This example assumes you are scraping Redis exporters. To view the number of connected Redis clients per endpoint in a cluster: FROM Metric SELECT latest(redis_connected_clients) WHERE clusterName='my-cluster' FACET scrapedEndpoint TIMESERIES Copy Kubernetes: This example assumes that you have Redis pods with the Redis exporter installed. To view the number of connected Redis clients per pod in the default namespace: FROM Metric SELECT latest(redis_connected_clients) WHERE namespaceName='default' FACET podName TIMESERIES Copy Docker: View average memory free for scraped endpoints This example assumes you are scraping node exporters for Docker and want to use OpenMetrics. To view average memory free for all scraped endpoints in a cluster: FROM Metric SELECT average(node_memory_MemFree_bytes) WHERE clusterName='my-cluster' Copy Kubernetes: View average memory usage for pods in a deployment To view average memory usage for all pods in a Kubernetes deployment using OpenMetrics: FROM Metric SELECT average(container_memory_usage_bytes) WHERE deploymentName='my-app-deployment' AND namespaceName='default' Copy View data in New Relic When you query the data, you can view the results in the New Relic UI. You can also visualize the data as charts, histograms, etc. To view the NRQL query results for your Prometheus integration's data: Go to one.newrelic.com > Query your data. For more information, see New Relic's query builder documentation. Create histograms and summaries With remote write or version 1.2.0 or higher of the Prometheus OpenMetrics integration, you can create histograms and percentiles (summaries) of your data. The OpenMetrics data is based on New Relic's guidelines in GitHub for higher level metric abstractions, while the remote write data closely matches the schema of the original Prometheus data. Data presentation Comments Histograms A bucket <basename>_bucket{le=\"42\"} will be sent as this: For OpenMetrics: <basename>_buckets For remote write: <basename>_bucket The dimension will be this: For OpenMetrics: {histogram.bucket.upperBound=\"42\"} For remote write: {histogram.bucket.le=\"42\"} Percentiles Quantiles (summaries) are transformed into percentiles. A metric <basename>{quantile=\"0.3\"} will be sent to New Relic as <basename>.percentiles. The dimension will be this: {percentile=\"30\"} NRQL has two functions that work on remote write ingested PromQL: bucketPercentile() and histogram(). The links include query examples.These two functions don't work on OpenMetrics ingested buckets. To better support visualization of histograms, percentiles are calculated based on the histogram metrics and sent to New Relic. To configure the calculated percentiles for OpenMetrics, use the percentiles configuration option.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 171.36028,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>View</em> <em>and</em> <em>query</em> your <em>Prometheus</em> <em>data</em>",
        "sections": "<em>View</em> <em>and</em> <em>query</em> your <em>Prometheus</em> <em>data</em>",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": " identifier for the source of the <em>Prometheus</em> <em>data</em> that matches the value of <em>prometheus</em>_server. instrumentation.version Used to identify the version of the remote write API; for example, 0.0.1. NRQL <em>query</em> examples When you build queries, be aware that there is no linking between the metrics, entities"
      },
      "id": "603eb04be7b9d20f9d2a07eb"
    },
    {
      "sections": [
        "Rate limit errors (Prometheus integration)",
        "Problem",
        "Solution",
        "Cause"
      ],
      "title": "Rate limit errors (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "3c8907d358e49c8dde5cab6dfa386deb5407d335",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/rate-limit-errors-prometheus-integration/",
      "published_at": "2021-10-08T06:48:27Z",
      "updated_at": "2021-08-08T19:29:48Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem Your Prometheus OpenMetrics integration for Docker or Kubernetes exceeded allowable metric rate limits. You want to see more details about why the NrIntegrationError event has been applied to your New Relic account. Solution To examine rate limit errors: Run a query of Prometheus metrics using the NrIntegrationError event, like this: FROM NrIntegrationError SELECT * WHERE newRelicFeature = 'Metrics' Copy Review additional troubleshooting procedures for NrIntegrationError events. To help prevent this from happening, you can use filters to control the types and amount of data that your integration sends to New Relic. For more information, see Ignore or include Prometheus metrics. Cause New Relic does a basic validation of your Prometheus OpenMetrics integration metrics when they are submitted. More extensive validation is performed asynchronously when processing the metrics. If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for Prometheus OpenMetrics integrations, New Relic will apply rate limits to your account and create an associated NrIntegrationError event.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 84.82488,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "sections": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem Your <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes exceeded allowable metric rate limits. You want to see more details about why the NrIntegrationError event has been applied to your New Relic account. Solution To examine rate limit errors: Run a <em>query</em> of <em>Prometheus</em> metrics"
      },
      "id": "6045054528ccbc2c342c606b"
    }
  ],
  "/docs/integrations/prometheus-integrations/view-query-data/view-query-your-prometheus-data": [
    {
      "sections": [
        "Supported PromQL Features",
        "Important",
        "Supported features",
        "Aggregation operators and functions",
        "Arithmetic binary operators",
        "Logical operators",
        "Date/time functions",
        "Mathematical functions",
        "Rate-like functions",
        "Predictive functions",
        "Time-series selectors",
        "PromQL troubleshooting",
        "Metric types",
        "Limits",
        "Range vector selectors (sliding windows and smoothing behavior)",
        "Query range and data scraping intervals"
      ],
      "title": "Supported PromQL Features",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "d4a93b9db3bfe5639ed01968b6e55a8e0aaa9389",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/supported-promql-features/",
      "published_at": "2021-10-08T06:48:27Z",
      "updated_at": "2021-03-16T04:42:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic supports PromQL-style queries, and our query builder offers a PromQL-style query mode that translates PromQL syntax queries into the closest NRQL approximation. Although the method of approximation means that a handful of edge cases are not fully supported, it provides coverage for an overwhelming majority of queries, supporting over 99.5% of queries across the 7.8 million top Grafana dashboard downloads. Read on to learn about how we work with PromQL queries, as well as differences between standard PromQL and our PromQL-like query language we want you to be aware of. Important For general information about Prometheus queries and operators, see the Prometheus.io documentation. Supported features We support the following aggregation, arithmetic, mathematical, and rate-like functions. As we continue to expand support for Prometheus and PromQL, this list will be updated. Aggregation operators and functions Aggregation operators: avg() count() min() max() quantile() stddev() stdvar() sum() topk() Aggregation functions: histogram_quantile() <aggregation>_over_time() functions: avg_over_time count_over_time min_over_time max_over_time quantile_over_time stdev_over_time stvar_over_time sum_over_time Arithmetic binary operators + (addition) - (subtraction) * (multiplication) / (division) % (percent) ^ (power/exponents) Logical operators and or Date/time functions day_of_month() day_of_week() days_in_month() hour() minute() month() time() timestamp() year() Mathematical functions abs() ceil() clamp_max() clamp_min() exp() floor() ln() log10() log2() round() sqrt() Rate-like functions delta() deriv() idelta() increase() irate() rate() Predictive functions predict_linear Time-series selectors We offer support for PromQL time-series selectors including the following: instant vector selectors range series selectors offset modifier Important We only support offset queries if every vector in the query has the same offset value. PromQL troubleshooting This section describes differences in behavior between PromQL and our PromQL-style query behavior and how to work with and around these differences. This is particularly relevant if you want to use advanced queries and our PromQL-style mode in the query builder. Metric types Prometheus recommendations note that you should only use some functions, like delta(), on gauges, and only use others like rate() and increase() on counters, but queries in Prometheus still work most of the time even if they don’t follow those instructions. However, because NRDB converts PromQL-style accumulating counters to delta counters, our implementation is unforgiving when using these functions on the wrong data type and will produce different or incorrect answers. For this reason, it's best to follow all Prometheus recommendations when working with our PromQL-style queries, even if you don't follow these recommendations in Prometheus. Limits In order to ensure the stability and performance of our system for all users, we place some limits on what queries can be run. In all cases, we enforce a limit of 366 steps in range queries. We also default to only returning 100 timeseries from queries by default. If you want to see more (or fewer), you need to explicitly add a topk() to your query. (Note that the topk() implementation in our PromQL-style query is different from that of Prometheus.) We limit the total memory a query can use. This means that requests for large numbers of time steps or large numbers of time series may be rejected, particularly if they are combined with an aggregation like unique count or quantile which require significantly more memory to compute than simple arithmetic aggregations. Range vector selectors (sliding windows and smoothing behavior) We provide support for sliding window timeseries aggregations. For more information, see our NRQL syntax, clauses, and functions resource and our sliding windows deep dive. For information on translating between NRQL and our PromQL-style language, see Translate PromQL queries to NRQL. Query range and data scraping intervals The range of your query in PromQL must be larger than the duration of the step size of the query to avoid the error \"TIMESERIES bucket size is larger than the current time window\". We inspect data up to one minute old when servicing instant queries. If your scrape interval is greater than 1 minute, some queries may result in No data found. Avoid this by sending data at least once per minute. If the timeseries unit for your NRQL query is less than the scrape interval for your application, some periods will lack data, and the resulting graph may be jagged or contain peaks and valleys. In general, set the step size to your scrape interval, or higher.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 171.36105,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Query</em> range <em>and</em> <em>data</em> scraping intervals",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": " with and around these differences. This is particularly relevant if you want to use advanced queries and our PromQL-style mode in the <em>query</em> builder. Metric types <em>Prometheus</em> recommendations note that you should only use some functions, like delta(), on gauges, and only use others like rate() and increase"
      },
      "id": "603e9523e7b9d292ec2a07ce"
    },
    {
      "sections": [
        "Translate PromQL queries to NRQL",
        "Tip",
        "Prometheus and New Relic metric types",
        "Mapping between NRQL and our PromQL-style queries",
        "PromQL-style query example",
        "NRQL query example",
        "Filter examples",
        "PromQL-style to NRQL query examples",
        "For more help"
      ],
      "title": "Translate PromQL queries to NRQL",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "fcf45fb8fb49f9d22f74574c2e7032533377e584",
      "image": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/translate-promql-queries-nrql/images/PROMQL-query-2.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/translate-promql-queries-nrql/",
      "published_at": "2021-10-08T06:49:32Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Do you have a PromQL query you’d like to convert to NRQL? This document provides examples that show you how to convert some common PromQL queries to NRQL queries. You can use our PromQL-style query language to explore your Prometheus OpenMetrics integration data along with other data sent to New Relic. Tip To run PromQL-style queries in New Relic One, go to the query builder advanced PromQL-style mode. Prometheus and New Relic metric types The different metric types supported by Prometheus and New Relic are related to each other: New Relic Prometheus Description Count Counter The Prometheus counter is a cumulative sum while the New Relic count is a delta sum. For example, if you see 2 requests in the first reporting period and 3 requests in the second reporting period. The Prometheus counter will report 2 and then 5, while the New Relic count will report 2 and then 3. Gauge Gauge A Prometheus gauge is similar to a New Relic gauge. Multiple counts Histogram Prometheus automatically maps a histogram to a set of counters. In New Relic, these counters should be changed to deltas and reported as counts. Gauges and counts Summary Prometheus represents a Summary with a given basename as the following time series: a basename_sum a basename_count and 0 or more of basename{quantile=\".xx\"...} metrics New Relic maps the _sum as a Summary, the _count as a Counter, and each quantile metric as a Gauge. Summary (No equivalent in Prometheus) New Relic has a distinct metric type called a summary that is different than the Prometheus summary. It is designed for reporting aggregated discrete events so that you can query the count, sum, min, max, and average values. Mapping between NRQL and our PromQL-style queries Tip To see how New Relic translates PromQL-style queries to NRQL, write a query in the query builder PromQL-style tab, then switch to the NRQL tab. This table shows the mapping between NRQL and our PromQL-style queries when exploring data. For more contextual information, see the examples. Description Mapping between NRQL and PromQL-style queries Search for attributes: Explore the attributes on the container_memory_usage_bytes metric. PromQL: container_memory_usage_bytes Copy NRQL: FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes' Copy Find attribute's value: Explore the current value of the container_memory_usage_bytes metric for unique id attributes. PromQL: sum(container_memory_usage_bytes) by (id) Copy NRQL: FROM Metric SELECT latest(container_memory_usage_bytes) FACET id Copy Visualize the attribute's value: Chart the value of the container_memory_usage_bytes metric with the given id attribute value. PromQL: container_memory_usage_bytes{id=\"/\"} Copy NRQL: FROM Metric SELECT latest(container_memory_usage_bytes) WHERE id = '/' TIMESERIES Copy PromQL-style query example 1. Start your query. When exploring your data for a particular metric in PromQL, such as memory by container usage in bytes, you can start with a query such as: container_memory_usage_bytes Copy This will chart all the unique metric timeseries for the input metric. 2. Filter the query results. Looking at the data, you can add more query parameters to filter down the number of metric timeseries. For example, if you only want timeseries where the id is /, the PromQL-style query will be: container_memory_usage_bytes{id=\"/\"} Copy PromQL-style example: To filter the data, run this PromQL-style query: container_memory_usage_bytes { id=\"/\"}. NRQL query example 1. Query available metrics. To explore your data, start by looking at all the available metrics. Use the following NRQL query: FROM Metric SELECT uniques(metricName) Copy 2. Find unique attributes. Once you have found the metric you want to review, such as container_memory_usage_bytes, you can find the unique attributes with the following query: FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes' Copy The results will show each available attribute key and the value type (string, boolean, or number). 3. Aggregate and chart the metrics. To chart metrics using NRQL, you first need an aggregation function. For example, you can use latest for gauges, sum for counts, and average for summaries. As the following chart shows, all the unique timeseries are aggregated into one unique timeseries by default: one.newrelic.com > Query your data: This example shows the data you see after running FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes'. 4. View metrics by ID. To view the unique metric timeseries with various id values, run the following query: FROM Metric SELECT latest(container_memory_usage_bytes) FACET id Copy one.newrelic.com > Query your data: This example shows the data you see after running FROM Metric SELECT latest(container_memory_usage_bytes) FACET id. 5. Add the selected ID to the query. Next you can select an id value and put it in the NRQL where clause. FROM Metric SELECT latest(container_memory_usage_bytes) WHERE id = \"/\" timeseries Copy one.newrelic.com > Query your data: This example shows the data displayed after running From Metric select latest(container_memory_usage_bytes) where id = \"/\" timeseries. Filter examples Both our PromQL-style query language and NRQL provide syntax to filter down the number of unique metric timeseries. PromQL-style uses brackets to filter. NRQL uses a WHERE clause. Here are some example queries: Description PromQL-style and NRQL queries Select data with specific values. PromQL: go_memstats_heap_alloc_bytes{job=\"apiserver\", instance=\"1234\"}) Copy NRQL: To only select data with specific values in NRQL, use the WHERE clause with =. In this example, all data must have the selected value for job and handler. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHERE job = 'apiserver' AND instance = '1234' TIMESERIES Copy Select data with multiple values. PromQL: go_memstats_heap_alloc_bytes{environment=~\"staging|testing|development\",method!=\"GET\"} Copy NRQL: In NRQL use the in clause to select multiple values for an attribute and the != sign to select all values but the one listed. In this example, the environment can be staging, testing, or development, and the method cannot be GET. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHERE environment IN ('staging', 'testing', 'development') AND method != 'GET' TIMESERIES Copy Select data using partial string values. PromQL: go_memstats_heap_alloc_bytes{job=~\"api.*\"} Copy NRQL: In NRQL use the LIKE clause to match part of a string value. In this example, all data will be returned where the job attributes start with api. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHEREe job LIKE 'api%' TIMESERIES Copy PromQL-style to NRQL query examples You can simulate the following PromQL-style queries with NRQL queries: Description PromQL-style and NRQL queries Measure the per second rate over the last minute of the http_request_total metric. PromQL: sum(rate(http_requests_total[1m])) Copy NRQL: FROM Metric SELECT rate(sum(http_request_total), 1 second) TIMESERIES 1 minute Copy Chart the difference of the two metrics, then divide by 1024. PromQL: (instance_memory_limit_bytes - instance_memory_usage_bytes) / 1024 Copy NRQL: FROM Metric SELECT (latest(instance_memory_limit_bytes) - latest(instance_memory_usage_bytes)) / 1024 TIMESERIES Copy Provide the summed rate per 30-second interval by each handler. PromQL: sum(rate(http_requests_total[30s])) by (handler) Copy NRQL: FROM Metric SELECT rate(sum(http_requests_total), 30 seconds) FACET handler TIMESERIES Copy Chart the difference in the two metrics where the instance is named foo and the fstype is either ext4 or xfs. PromQL: (node_filesystem_free_bytes{instance='foo',fstype=~\"ext4|xfs\"} / node_filesystem_size_bytes{instance='foo',fstype=~\"ext4|xfs\"}) Copy NRQL: FROM Metric SELECT latest(node_filesystem_free_bytes) / latest(node_filesystem_size_bytes) WHERE instance = 'foo' AND fstype IN ('ext4', 'xfs') Copy For more help",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 171.36046,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Translate PromQL <em>queries</em> to NRQL",
        "sections": "<em>Prometheus</em> <em>and</em> New Relic metric types",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": "Do you have a PromQL <em>query</em> you’d like to convert to NRQL? This document provides examples that show you how to convert some common PromQL queries to NRQL queries. You can use our PromQL-style <em>query</em> language to explore your <em>Prometheus</em> OpenMetrics integration <em>data</em> along with other <em>data</em> sent to New"
      },
      "id": "603ead4528ccbcbecfeba77b"
    },
    {
      "sections": [
        "Rate limit errors (Prometheus integration)",
        "Problem",
        "Solution",
        "Cause"
      ],
      "title": "Rate limit errors (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "3c8907d358e49c8dde5cab6dfa386deb5407d335",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/rate-limit-errors-prometheus-integration/",
      "published_at": "2021-10-08T06:48:27Z",
      "updated_at": "2021-08-08T19:29:48Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem Your Prometheus OpenMetrics integration for Docker or Kubernetes exceeded allowable metric rate limits. You want to see more details about why the NrIntegrationError event has been applied to your New Relic account. Solution To examine rate limit errors: Run a query of Prometheus metrics using the NrIntegrationError event, like this: FROM NrIntegrationError SELECT * WHERE newRelicFeature = 'Metrics' Copy Review additional troubleshooting procedures for NrIntegrationError events. To help prevent this from happening, you can use filters to control the types and amount of data that your integration sends to New Relic. For more information, see Ignore or include Prometheus metrics. Cause New Relic does a basic validation of your Prometheus OpenMetrics integration metrics when they are submitted. More extensive validation is performed asynchronously when processing the metrics. If New Relic finds errors during this asynchronous validation, the errors are put into an NrIntegrationError event in your New Relic account. For example, if you exceed the metric limits defined for Prometheus OpenMetrics integrations, New Relic will apply rate limits to your account and create an associated NrIntegrationError event.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 84.824875,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "sections": "Rate limit errors (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem Your <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes exceeded allowable metric rate limits. You want to see more details about why the NrIntegrationError event has been applied to your New Relic account. Solution To examine rate limit errors: Run a <em>query</em> of <em>Prometheus</em> metrics"
      },
      "id": "6045054528ccbc2c342c606b"
    }
  ],
  "/docs/licenses/index": [
    {
      "sections": [
        "iOS application licenses"
      ],
      "title": "iOS application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "a6df56e363112e7387e6887f04381a36a5457e84",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/ios-application-licenses/",
      "published_at": "2021-10-08T06:58:59Z",
      "updated_at": "2021-09-27T15:25:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Copyright AFNetworking MIT Copyright © 2011-2020 Alamofire Software Foundation http://alamofire.org/ Ace BSD Copyright © 2010, Ajax.org B.V. ActiveLabel MIT Copyright © 2015 Optonaut Alamofire MIT Copyright © 2014-2021 Alamofire Software Foundation Analytics MIT Copyright © 2016 Segment.io, Inc. BBlock MIT Copyright © 2012 David Keegan BigNumber MIT Copyright © 2019 mkrd CDMarkdownKit MIT Copyright © 2016-2017 Christopher de Haan contact@christopherdehaan.me Charts Apache License, Version 2.0 Copyright © 2016-2019 Daniel Cohen Gindi & Philipp Jahoda ECSlidingViewController MIT Copyright © 2013 EdgeCase LoginManagerSDK New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. Mantle MIT Copyright © GitHub, Inc. All rights reserved. Copyright © 2012, Bitswift, Inc MetricMetadata New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NRCharts New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NVActivityIndicatorView MIT Copyright © 2016 Vinh Nguyen NewRelicAgent New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. ObjectMapper MIT Copyright © 2014 Hearst RadarKit New Relic License © 2010-2021 New Relic, Inc. All rights reserved. SSPullToRefresh MIT Copyright © 2012-2014 Sam Soffes, http://soff.es UICKeyChainStore MIT Copyright © 2011 kishikawa katsumi WidgetLibrary New Relic License © 2010-2021 New Relic, Inc. All rights reserved. XYPieChart MIT Copyright © 2012 Xiaoyang Feng, XYStudio.cc Yams MIT Copyright © 2016 JP Simard. jsTimezoneDetect MIT Copyright © 2012 Jon Nylander, project maintained at bitbucket.org lodash MIT Copyright © JS Foundation and other contributors js.foundation The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 58.200317,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "iOS application <em>licenses</em>",
        "sections": "iOS application <em>licenses</em>",
        "tags": "<em>Licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Copyright"
      },
      "id": "603e9db1196a670e70a83df3"
    },
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2021-10-08T07:07:37Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0 © 2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0 © 2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0 © 2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0 © 2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0 © 2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright © 2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright © 2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright © 1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright © 1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 57.30475,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Logs plugin <em>licenses</em>",
        "sections": "Logs plugin <em>licenses</em>",
        "tags": "<em>Licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    },
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2021-10-08T06:59:42Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 55.09335,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    }
  ],
  "/docs/licenses/license-information/distributed-licenses/add-end-user-license-agreement": [
    {
      "sections": [
        "Services licenses",
        "Contents",
        "Java internal services",
        "Java crash data API",
        "CoffeeScript",
        "JavaScript",
        "Crash reporting",
        "Label services",
        "Infrastructure Monitoring services",
        ".NET support uploader service",
        "Ruby gems",
        "Storage services",
        "Go packages"
      ],
      "title": "Services licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "49a2ad450d31dacfc2aae690ca947d2326d18761",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/other-licenses/services-licenses/",
      "published_at": "2021-10-08T10:17:30Z",
      "updated_at": "2021-04-06T00:26:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Contents Java internal services Java crash data API Framework License Apache TomCat Apache 2.0 Bean Validation Apache 2.0 Cassandra Driver Core Apache 2.0 ClassMate Apache 2.0 Hamcrest BSD Mockito MIT Netty Apache 2.0 Objenesis MIT SnakeYAML Apache 2.0 Spring Boot Apache 2.0 Java internal services Library License json_simple Apache 2.0 newrelic-api New Relic newrelic-api New Relic antlr BSD aopalliance Public Domain asm-analysis BSD asm-commons BSD asm-tree BSD asm-util BSD asm BSD c3p0 EPLv1.0 cglib-nodep Apache 2.0 cglib Apache 2.0 cglib Apache 2.0 cal10n-api MIT logback-classic EPLv1.0 logback-core EPLv1.0 guava-jetty-service Apache 2.0 zkclient Apache 2.0 reporter-config-base Apache 2.0 reporter-config3 Apache 2.0 reporter-config Apache 2.0 aws-java-sdk-acm Apache 2.0 aws-java-sdk-api-gateway Apache 2.0 aws-java-sdk-applicationautoscaling Apache 2.0 aws-java-sdk-autoscaling Apache 2.0 aws-java-sdk-cloudformation Apache 2.0 aws-java-sdk-cloudfront Apache 2.0 aws-java-sdk-cloudhsm Apache 2.0 aws-java-sdk-cloudsearch Apache 2.0 aws-java-sdk-cloudtrail Apache 2.0 aws-java-sdk-cloudwatch Apache 2.0 aws-java-sdk-cloudwatchmetrics Apache 2.0 aws-java-sdk-codecommit Apache 2.0 aws-java-sdk-codedeploy Apache 2.0 aws-java-sdk-codepipeline Apache 2.0 aws-java-sdk-cognitoidentity Apache 2.0 aws-java-sdk-cognitoidp Apache 2.0 aws-java-sdk-cognitosync Apache 2.0 aws-java-sdk-config Apache 2.0 aws-java-sdk-core Apache 2.0 aws-java-sdk-datapipeline Apache 2.0 aws-java-sdk-devicefarm Apache 2.0 aws-java-sdk-directconnect Apache 2.0 aws-java-sdk-directory Apache 2.0 aws-java-sdk-discovery Apache 2.0 aws-java-sdk-dms Apache 2.0 aws-java-sdk-dynamodb Apache 2.0 aws-java-sdk-ec2 Apache 2.0 aws-java-sdk-ecr Apache 2.0 aws-java-sdk-ecs Apache 2.0 aws-java-sdk-efs Apache 2.0 aws-java-sdk-elasticache Apache 2.0 aws-java-sdk-elasticbeanstalk Apache 2.0 aws-java-sdk-elasticloadbalancing Apache 2.0 aws-java-sdk-elasticloadbalancingv2 Apache 2.0 aws-java-sdk-elasticsearch Apache 2.0 aws-java-sdk-elastictranscoder Apache 2.0 aws-java-sdk-emr Apache 2.0 aws-java-sdk-events Apache 2.0 aws-java-sdk-gamelift Apache 2.0 aws-java-sdk-glacier Apache 2.0 aws-java-sdk-iam Apache 2.0 aws-java-sdk-importexport Apache 2.0 aws-java-sdk-inspector Apache 2.0 aws-java-sdk-iot Apache 2.0 aws-java-sdk-kinesis Apache 2.0 aws-java-sdk-kms Apache 2.0 aws-java-sdk-lambda Apache 2.0 aws-java-sdk-logs Apache 2.0 aws-java-sdk-machinelearning Apache 2.0 aws-java-sdk-marketplacecommerceanalytics Apache 2.0 aws-java-sdk-marketplacemeteringservice Apache 2.0 aws-java-sdk-models Apache 2.0 aws-java-sdk-opsworks Apache 2.0 aws-java-sdk-rds Apache 2.0 aws-java-sdk-redshift Apache 2.0 aws-java-sdk-route53 Apache 2.0 aws-java-sdk-s3 Apache 2.0 aws-java-sdk-servicecatalog Apache 2.0 aws-java-sdk-ses Apache 2.0 aws-java-sdk-simpledb Apache 2.0 aws-java-sdk-simpleworkflow Apache 2.0 aws-java-sdk-snowball Apache 2.0 aws-java-sdk-sns Apache 2.0 aws-java-sdk-sqs Apache 2.0 aws-java-sdk-ssm Apache 2.0 aws-java-sdk-storagegateway Apache 2.0 aws-java-sdk-sts Apache 2.0 aws-java-sdk-support Apache 2.0 aws-java-sdk-swf-libraries Apache 2.0 aws-java-sdk-waf Apache 2.0 aws-java-sdk-workspaces Apache 2.0 aws-java-sdk Apache 2.0 jmespath-java Apache 2.0 AppleJavaExtensions BSD jcommander Apache 2.0 high-scale-lib MIT hppc Apache 2.0 clover Paid stream Apache 2.0 metrics-core Apache 2.0 cassandra-driver-core Apache 2.0 cassandra-driver-core Apache 2.0 uuid MIT grabbag MIT speed4j Apache 2.0 yamlbeans BSD jackson-annotations Apache 2.0 jackson-core Apache 2.0 jackson-databind Apache 2.0 jackson-dataformat-cbor Apache 2.0 jackson-dataformat-csv Apache 2.0 jackson-dataformat-xml Apache 2.0 jackson-dataformat-yaml Apache 2.0 jackson-datatype-guava Apache 2.0 jackson-datatype-jdk7 Apache 2.0 jackson-datatype-jdk8 Apache 2.0 jackson-datatype-joda Apache 2.0 jackson-datatype-jsr310 Apache 2.0 jackson-jaxrs-base Apache 2.0 jackson-jaxrs-json-provider Apache 2.0 jackson-module-afterburner Apache 2.0 jackson-module-jaxb-annotations Apache 2.0 jackson-module-kotlin Apache 2.0 woodstox-core Apache 2.0 classmate Apache 2.0 zjsonpatch Apache 2.0 caffeine Apache 2.0 waffle-jna EPLv1.0 docker-java Apache 2.0 docker-java Apache 2.0 btf Apache 2.0 jackson-coreutils Apache 2.0 json-schema-core Apache 2.0 json-schema-validator Apache 2.0 msg-simple Apache 2.0 uri-template Apache 2.0 jamm Apache 2.0 jffi Apache 2.0 jnr-constants Apache 2.0 jnr-ffi Apache 2.0 jnr-ffi Apache 2.0 jnr-posix EPLv1.0 jnr-x86asm MIT embedded-redis Apache 2.0 memoryfilesystem MIT guava-retrying Apache 2.0 guava-retrying Apache 2.0 snowball-stemmer BSD system-rules CPL uuid Apache 2.0 wiremock Apache 2.0 auto-common Apache 2.0 annotations LGPLv2.1 annotations LGPLv2.1 bcel-findbugs LGPLv2.1 findbugs LGPLv2.1 jFormatString BSD jFormatString BSD jsr305 BSD jsr305 BSD jsr305 BSD gson Apache 2.0 spymemcached MIT guava-testlib Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guice-multibindings Apache 2.0 guice Apache 2.0 truth Apache 2.0 concurrent-trees Apache 2.0 concurrentlinkedhashmap-lru Apache 2.0 json-simple Apache 2.0 libphonenumber Apache 2.0 locality-uuid BSD h2 EPLv1.0 annotations Apache 2.0 json-path-assert Apache 2.0 json-path Apache 2.0 json-path Apache 2.0 jzlib BSD bonecp-provider Apache 2.0 bonecp Apache 2.0 junixsocket-common Apache 2.0 junixsocket-native-common Apache 2.0 disruptor Apache 2.0 geoip-api LGPLv2.1 c3p0 LGPLv2.1 mchange-commons-java LGPLv2.1 archaius-core Apache 2.0 hystrix-core Apache 2.0 hystrix-servo-metrics-publisher Apache 2.0 rxjava-core Apache 2.0 servo-core Apache 2.0 kafka-clients Apache 2.0 kafka_2.10 Apache 2.0 kafka_2.11 Apache 2.0 common-cassandra Apache 2.0 timeslice_utils New Relic mockito-kotlin MIT compress-lzf Apache 2.0 checkstyle Apache 2.0 dagger-compiler Apache 2.0 dagger Apache 2.0 okhttp Apache 2.0 okio Apache 2.0 javapoet Apache 2.0 javawriter Apache 2.0 jaxb-impl CDDLv1 thrift-server Apache 2.0 finagle-core_2.10 Apache 2.0 finagle-core_2.11 Apache 2.0 finagle-http_2.10 Apache 2.0 finagle-http_2.11 Apache 2.0 jsr166e Creative Commons util-app_2.10 Apache 2.0 util-app_2.11 Apache 2.0 util-cache_2.10 Apache 2.0 util-cache_2.11 Apache 2.0 util-codec_2.10 Apache 2.0 util-codec_2.11 Apache 2.0 util-collection_2.10 Apache 2.0 util-collection_2.11 Apache 2.0 util-core_2.10 Apache 2.0 util-core_2.11 Apache 2.0 util-function_2.10 Apache 2.0 util-function_2.11 Apache 2.0 util-hashing_2.10 Apache 2.0 util-hashing_2.11 Apache 2.0 util-jvm_2.10 Apache 2.0 util-jvm_2.11 Apache 2.0 util-lint_2.10 Apache 2.0 util-lint_2.11 Apache 2.0 util-logging_2.10 Apache 2.0 util-logging_2.11 Apache 2.0 util-registry_2.10 Apache 2.0 util-registry_2.11 Apache 2.0 util-stats_2.10 Apache 2.0 util-stats_2.11 Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 HikariCP Apache 2.0 commons-beanutils Apache 2.0 commons-cli Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-io Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 commons-pool Apache 2.0 unix-socket-factory Apache 2.0 unix-socket-factory Apache 2.0 jflex BSD dom4j BSD airline Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 metrics-healthchecks Apache 2.0 metrics-httpclient Apache 2.0 metrics-jdbi Apache 2.0 metrics-jersey2 Apache 2.0 metrics-jetty9 Apache 2.0 metrics-json Apache 2.0 metrics-jvm Apache 2.0 metrics-logback Apache 2.0 metrics-servlets Apache 2.0 dropwizard-java8-jdbi Apache 2.0 dropwizard-client Apache 2.0 dropwizard-configuration Apache 2.0 dropwizard-core Apache 2.0 dropwizard-db Apache 2.0 dropwizard-jackson Apache 2.0 dropwizard-jdbi Apache 2.0 dropwizard-jersey Apache 2.0 dropwizard-jetty Apache 2.0 dropwizard-lifecycle Apache 2.0 dropwizard-logging Apache 2.0 dropwizard-metrics Apache 2.0 dropwizard-servlets Apache 2.0 dropwizard-testing Apache 2.0 dropwizard-util Apache 2.0 dropwizard-validation Apache 2.0 netty-all Apache 2.0 netty-buffer Apache 2.0 netty-codec-http Apache 2.0 netty-codec-socks Apache 2.0 netty-codec Apache 2.0 netty-common Apache 2.0 netty-handler-proxy Apache 2.0 netty-handler Apache 2.0 netty-resolver Apache 2.0 netty-transport-native-epoll Apache 2.0 netty-transport Apache 2.0 netty Apache 2.0 ratpack-core Apache 2.0 ratpack-groovy-test Apache 2.0 ratpack-groovy Apache 2.0 ratpack-guice Apache 2.0 ratpack-test Apache 2.0 rxjava Apache 2.0 fastutil Apache 2.0 janino BSD activation Apache 2.0 javax.annotation-api CDDLv1 jsr250-api CDDLv1 javax.inject Apache 2.0 mail CDDLv1 mailapi CDDLv1 javax.servlet-api Apache 2.0 servlet-api Apache 2.0 javax.transaction-api CDDLv1 validation-api Apache 2.0 javax.websocket-api CDDLv1 javax.ws.rs-api CDDLv1 jaxb-api CDDLv1 stax-api CDDLv1 jaxen Apache 2.0 jline BSD jline BSD joda-time Apache 2.0 junit-dep CPL junit CPL junit EPLv1.0 kafka_2.10 Apache 2.0 kafka_2.9.2 Apache 2.0 apache-log4j-extras Apache 2.0 log4j Apache 2.0 mysql-connector-java GPLv2 with Classpath Exception byte-buddy Apache 2.0 byte-buddy Apache 2.0 jna-platform LGPLv2.1 jna Apache 2.0 jcip-annotations Apache 2.0 lz4 Apache 2.0 accessors-smart Apache 2.0 asm Apache 2.0 json-smart Apache 2.0 json-smart Apache 2.0 primitive GPLv2 with Classpath Exception compiler Apache 2.0 lang Apache 2.0 beaver-cc BSD ehcache-core Apache 2.0 ehcache Apache 2.0 jopt-simple MIT jopt-simple MIT opencsv Apache 2.0 quality-check Apache 2.0 scannotation Apache 2.0 super-csv Apache 2.0 uadetector-core Apache 2.0 uadetector-resources Apache 2.0 argparse4j MIT spymemcached MIT kalium Apache 2.0 ST4 BSD antlr-runtime BSD antlr-runtime BSD antlr BSD stringtemplate BSD ant-launcher Apache 2.0 ant Apache 2.0 cassandra-all Apache 2.0 cassandra-all Apache 2.0 cassandra-thrift Apache 2.0 cassandra-thrift Apache 2.0 commons-collections4 Apache 2.0 commons-compress Apache 2.0 commons-csv Apache 2.0 commons-dbcp2 Apache 2.0 commons-io Apache 2.0 commons-lang3 Apache 2.0 commons-math3 Apache 2.0 commons-math Apache 2.0 commons-pool2 Apache 2.0 curator-client Apache 2.0 curator-framework Apache 2.0 curator-recipes Apache 2.0 curator-test Apache 2.0 derby Apache 2.0 geronimo-servlet_3.0_spec Apache 2.0 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 ivy Apache 2.0 kafka-clients Apache 2.0 kafka_2.11 Apache 2.0 libthrift Apache 2.0 tomcat-embed-core Apache 2.0 tomcat-embed-el Apache 2.0 tomcat-embed-logging-juli Apache 2.0 tomcat-embed-websocket Apache 2.0 tomcat-jdbc Apache 2.0 tomcat-juli Apache 2.0 zookeeper Apache 2.0 aspectjweaver EPLv1.0 assertj-core Apache 2.0 assertj-core Apache 2.0 evo-inflector Apache 2.0 bcmail-jdk15on MIT bcpkix-jdk15on MIT bcprov-jdk15on MIT ohc-core Apache 2.0 cassandra-unit-spring LGPLv3 cassandra-unit-spring LGPLv3 cassandra-unit LGPLv3 cassandra-unit LGPLv3 groovy-all Apache 2.0 groovy-all Apache 2.0 groovy Apache 2.0 groovy Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 easymock Apache 2.0 ecj EPLv1.0 javax.activation CDDLv1 javax.annotation CDDLv1 javax.mail.glassfish CDDLv1 javax.servlet CDDLv1 javax.transaction CDDLv1 org.objectweb.asm BSD jetty-setuid-java EPLv1.0 jetty-schemas Apache 2.0 javax-websocket-client-impl Apache 2.0 javax-websocket-server-impl Apache 2.0 websocket-api Apache 2.0 websocket-client Apache 2.0 websocket-common Apache 2.0 websocket-server Apache 2.0 websocket-servlet Apache 2.0 apache-jsp Apache 2.0 jetty-annotations CDDLv1 jetty-client Apache 2.0 jetty-continuation Apache 2.0 jetty-http Apache 2.0 jetty-io Apache 2.0 jetty-jmx Apache 2.0 jetty-jndi Apache 2.0 jetty-plus Apache 2.0 jetty-proxy Apache 2.0 jetty-security Apache 2.0 jetty-server Apache 2.0 jetty-servlet Apache 2.0 jetty-servlets Apache 2.0 jetty-util Apache 2.0 jetty-webapp Apache 2.0 jetty-xml Apache 2.0 sigar Apache 2.0 aopalliance-repackaged CDDLv1 javax.inject CDDLv1 hk2-api CDDLv1 hk2-locator CDDLv1 hk2-utils CDDLv1 osgi-resource-locator CDDLv1 jersey-guava CDDLv1 jersey-client CDDLv1 jersey-apache-connector CDDLv1 jersey-container-servlet-core CDDLv1 jersey-container-servlet CDDLv1 jersey-client CDDLv1 jersey-common CDDLv1 jersey-server CDDLv1 jersey-bean-validation CDDLv1 jersey-metainf-services CDDLv1 jersey-media-jaxb CDDLv1 jersey-client CDDLv1 jersey-test-framework-provider-inmemory CDDLv1 jersey-test-framework-core CDDLv1 javax.el CDDLv1 hamcrest-all BSD hamcrest-core BSD hamcrest-integration BSD hamcrest-library BSD HdrHistogram Public Domain hector-core MIT hibernate-commons-annotations LGPLv2.1 hibernate-jpa-2.0-api LGPLv2.1 hibernate-jpa-2.1-api LGPLv2.1 hibernate-c3p0 Apache 2.0 hibernate-core Apache 2.0 hibernate-ehcache Apache 2.0 hibernate-entitymanager Apache 2.0 hibernate-jmx Apache 2.0 hibernate-validator Apache 2.0 freebuilder Apache 2.0 org.jacoco.agent EPLv1.0 org.jacoco.ant EPLv1.0 org.jacoco.core EPLv1.0 org.jacoco.report EPLv1.0 javassist Apache 2.0 jboss-logging-annotations LGPLv2.1 jboss-logging Apache 2.0 jboss-transaction-api_1.1_spec LGPLv2.1 jboss-transaction-api_1.2_spec LGPLv2.1 jandex Apache 2.0 jdbi Apache 2.0 job-dsl-core Apache 2.0 version-number MIT kotlin-annotation-processing Apache 2.0 kotlin-reflect Apache 2.0 kotlin-runtime Apache 2.0 kotlin-stdlib Apache 2.0 kotlin-test-junit Apache 2.0 kotlin-test Apache 2.0 annotations Apache 2.0 jolokia-core Apache 2.0 jruby-complete LGPLv2.1 LGPLv2.1 json Public Domain json Apache 2.0 xstream BSD annotations Apache 2.0 mapdb Apache 2.0 mapdb Apache 2.0 jbcrypt ISC mockserver-client-java Apache 2.0 mockserver-core Apache 2.0 mockserver-logging Apache 2.0 mockserver-netty Apache 2.0 mockito-all MIT mockito-core MIT mockito-core MIT apache-el Apache 2.0 apache-jsp Apache 2.0 alpn-boot Apache 2.0 etcd4j Apache 2.0 msgpack Apache 2.0 objenesis Apache 2.0 objenesis Apache 2.0 jmh-core GPLv2 with Classpath Exception jmh-generator-annprocess GPLv2 with Classpath Exception jol-core GPLv2 with Classpath Exception asm-analysis BSD asm-analysis BSD asm-commons BSD asm-debug-all BSD asm-tree BSD asm-util BSD asm-util BSD asm BSD asm Public Domain postgresql BSD powermock-api-mockito Apache 2.0 powermock-api-support Apache 2.0 powermock-core Apache 2.0 powermock-module-junit4-common Apache 2.0 powermock-module-junit4 Apache 2.0 powermock-reflect Apache 2.0 lombok MIT reactive-streams Public Domain reflections WTFPL scala-java8-compat_2.11 Scala scala-parser-combinators_2.11 Scala scala-xml_2.11 Scala scala-library Scala scala-reflect Scala scalatest_2.10 Apache 2.0 scalatest_2.11 Apache 2.0 native-lib-loader BSD jsonassert Apache 2.0 jcl-over-slf4j MIT jul-to-slf4j MIT log4j-log4j12 MIT log4j-over-slf4j MIT slf4j-api MIT slf4j-ext MIT slf4j-log4j12 MIT slf4j-simple MIT spock-core Apache 2.0 spock-core Apache 2.0 spring-boot-actuator Apache 2.0 spring-boot-autoconfigure Apache 2.0 spring-boot-configuration-processor Apache 2.0 spring-boot-starter-actuator Apache 2.0 spring-boot-starter-aop Apache 2.0 spring-boot-starter-data-jpa Apache 2.0 spring-boot-starter-data-rest Apache 2.0 spring-boot-starter-jdbc Apache 2.0 spring-boot-starter-jetty Apache 2.0 spring-boot-starter-logging Apache 2.0 spring-boot-starter-test Apache 2.0 spring-boot-starter-tomcat Apache 2.0 spring-boot-starter-web Apache 2.0 spring-boot-starter Apache 2.0 spring-boot-test-autoconfigure Apache 2.0 spring-boot-test Apache 2.0 spring-boot Apache 2.0 spring-data-commons Apache 2.0 spring-data-jpa Apache 2.0 spring-data-rest-core Apache 2.0 spring-data-rest-webmvc Apache 2.0 spring-hateoas Apache 2.0 spring-plugin-core Apache 2.0 spring-aop Apache 2.0 spring-aspects Apache 2.0 spring-beans Apache 2.0 spring-context-support Apache 2.0 spring-context Apache 2.0 spring-core Apache 2.0 spring-core Apache 2.0 spring-expression Apache 2.0 spring-jdbc Apache 2.0 spring-orm Apache 2.0 spring-test Apache 2.0 spring-test Apache 2.0 spring-tx Apache 2.0 spring-web Apache 2.0 spring-webmvc Apache 2.0 xz Public Domain wasabi Apache 2.0 snappy-java Apache 2.0 xmlunit-core Apache 2.0 xmlunit-legacy Apache 2.0 snakeyaml Apache 2.0 JUnitParams Apache 2.0 postgresql BSD postgresql BSD jedis MIT scala-library Scala ion-java Apache 2.0 timeslice_service-thrift New Relic timeslice_service New Relic xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3_min Public Domain zkclient Apache 2.0 zookeeper Apache 2.0 CoffeeScript Library License CoffeeScript MIT coffin MIT JavaScript JavaScript License ie_html5/ie_html5.js MIT Angular.js MIT Angular UI directives for Bootstrap MIT Chart.js MIT Chosen JQuery plugin MIT Fullcalendar MIT Moment MIT Crash reporting Software License Apple OS X OS X Yosemite Xcode & Apple SDK Xcode license Label services Service License BoneCP Apache 2.0 Gradle Apache 2.0 Hibernate Apache 2.0 Spring Apache 2.0 Infrastructure Monitoring services Service License antlr BSD aopalliance Public Domain asm MIT cglib Apache 2.0 ch.qos.logback EPLv1.0 com.101tec Apache 2.0 com.amazonaws Apache 2.0 com.cenqua.clover Paid com.fasterxml.jackson.core Apache 2.0 com.fasterxml Apache 2.0 com.google.code.findbugs BSD com.google.guava Apache 2.0 com.googlecode.json-simple Apache 2.0 com.intellij Apache 2.0 New Relic com.squareup.dagger Apache 2.0 com.squareup Apache 2.0 com.yammer.metrics Apache 2.0 commons-codec Apache 2.0 commons-configuration Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 dom4j BSD io.netty Apache 2.0 javax.annotation CDDLv1 javax.inject Apache 2.0 javax.servlet.jsp CDDLv1 javax.servlet Apache 2.0 javax.validation Apache 2.0 javax.websocket CDDLv1 jline BSD joda-time Apache 2.0 junit CPL log4j Apache 2.0 mysql GPLv2 with Classpath Exception net.sf.jopt-simple MIT org.apache.commons Apache 2.0 org.apache.httpcomponents Apache 2.0 org.apache.ivy Apache 2.0 org.apache.thrift Apache 2.0 org.apache.tomcat.embed Apache 2.0 org.apache.zookeeper Apache 2.0 org.codehaus.groovy Apache 2.0 org.codehaus.jackson Apache 2.0 org.easymock Apache 2.0 org.eclipse.jetty.orbit CDDLv1 org.eclipse.jetty.toolchain Apache 2.0 org.eclipse.jetty.websocket Apache 2.0 org.eclipse.jetty CDDLv1 org.glassfish.jersey.core CDDLv1 org.glassfish.web CDDLv1 org.glassfish CDDLv1 org.hamcrest BSD org.hibernate Apache 2.0 org.javassist Apache 2.0 org.jboss.logging LGPLv2.1 org.jenkins-ci.plugins Apache 2.0 org.jenkins-ci Apache 2.0 org.jetbrains Apache 2.0 org.jolokia Apache 2.0 org.jvnet.hudson BSD org.mockito MIT org.objenesis Apache 2.0 org.ow2.asm MIT org.reflections WTFPL org.scala-lang Scala org.slf4j MIT org.springframework.boot Apache 2.0 org.springframework Apache 2.0 org.xerial.snappy Apache 2.0 org.yaml Apache 2.0 xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3 Public Domain .NET support uploader service Library License Amazon AWS SDK for Java Apache 2.0 Apache Commons Codec Apache 2.0 Gson Apache 2.0 Jetty Apache 2.0 Ruby gems Ruby gems Gem License aasm MIT actionmailer MIT actionpack MIT activeadmin MIT actionview MIT activejob MIT active_model_serializers MIT activemodel MIT activerecord MIT activerecord-deprecated_finders MIT activerecord-mysql2-adapter MIT activeresource MIT activesupport MIT addressable Apache 2.0 angularjs-rails MIT arbre MIT arel MIT arel MIT atomic Apache 2.0 atomic Apache-2.0 attr_required MIT awesome_print MIT backports MIT better_errors MIT binding_of_caller MIT bourbon MIT brwsr MIT browser-timezone-rails MIT builder MIT bundler MIT byebug MIT callsite MIT capistrano MIT capistrano-bundler MIT capistrano-ext MIT capistrano-rbenv MIT capistrano-stats MIT capybara MIT celluloid MIT CFPropertyList MIT chunky_png MIT ci_reporter MIT ci_reporter_rspec MIT clockwork MIT codeclimate-test-reporter MIT coderay MIT coderay MIT coffee-rails MIT coffee-script MIT coffee-script-source MIT columnize Ruby compass MIT compass-core MIT compass-import-once MIT compass-rails MIT concurrent-ruby MIT crack MIT crypt Unlicensed daemons MIT dalli MIT database_cleaner MIT debug_inspector MIT debugger-linecache MIT diff-lcs MIT docile MIT dotenv MIT dotenv-deployment MIT erubis MIT etcd MIT ethon MIT eventmachine Ruby excon MIT execjs MIT factory_girl MIT factory_girl_rails MIT faraday MIT faraday-middleware MIT faye-websocket MIT ffi MIT ffi BSD fission MIT fog MIT fog-atmos MIT fog-aws MIT fog-brightbox MIT fog-core MIT fog-ecloud MIT fog-google MIT fog-json MIT fog-local MIT fog-powerdns MIT fog-profitbricks MIT fog-radosgw MIT fog-riakcs MIT fog-sakuracloud MIT fog-serverlove MIT fog-softlayer MIT fog-storm_on_demand MIT fog-terremark MIT fog-vmfusion MIT fog-voxel MIT fog-xml MIT font-awesome-rails MIT foreman MIT formatador MIT formtastic MIT formtastic i18n MIT fssm MIT globalid MIT guard MIT guard-rspec MIT haml MIT haml-rails MIT has scope MIT hashie MIT highline Ruby hike MIT hipchat-api MIT histogram MIT hitimes ISC hiredis BSD http_parser.rb MIT httpclient Ruby httpclient Ruby httparty MIT i18n MIT inflecto MIT inherited resources MIT intercom-rails MIT ipaddress MIT jasmine MIT jasmine-core MIT jasmine-rails MIT jbuilder MIT jira MIT jira-ruby OSL-3.0 journey MIT jquery-rails MIT jquery-turbolinks MIT jquery-ui-rails MIT json Ruby jwt MIT kaminari MIT kgio LGPLv3 kgio LGPLv3 kgio LGPL-v2.1+ libv8 MIT listen MIT logger Ruby loofah MIT lumberjack MIT macaddr MIT mail MIT mailcatcher MIT memoist MIT metaclass MIT meta_request MIT method_source MIT mime-types MIT mini_portile MIT minitest MIT minitest-rails MIT mixlib-log Apache 2.0 mocha MIT mono_logger MIT multi_json MIT multi_xml MIT multipart-post MIT mysql2 MIT net-http-persistent MIT net-scp MIT net-sftp MIT net-ssh MIT net-ssh-gateway MIT netrc MIT nokogiri MIT oauth2 MIT oj MIT omniauth MIT omniauth-oauth2 MIT pagerduty MIT papers MIT pg BSD pg Ruby phantomjs BSD phantomjs-binaries Unlicensed poltergeist MIT polyamorous MIT polyglot MIT polyglot MIT protected_attributes MIT pry MIT pry-nav MIT pry-rails MIT pry-stack_explorer MIT puma BSD quiet assets MIT rack MIT rack-cache MIT rack-contrib MIT rack-oauth2 MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-ssl MIT rack-ssl-enforcer MIT rack-test MIT rails MIT rails-deprecated_sanitizer MIT rails-dom-testing MIT rails-html-sanitizer MIT rails-observers MIT rails_12factor MIT rails_serve_static_assets MIT rails_stdout_logging MIT railties MIT raindrops LGPLv3 raindrops LGPLv3 raindrops LGPLv2.1+ rake MIT rake MIT rake MIT rake MIT ransack MIT rb-fsevent MIT rb-inotify MIT rdoc Ruby redcarpet MIT redis MIT redis-namespace MIT redis-queue MIT ref MIT remote_syslog_logger MIT request_store MIT responders MIT resque MIT resque-cleaner MIT resque-pool MIT resque-scheduler MIT resque-status MIT rest-client MIT restforce MIT rspec MIT rspec-core MIT rspec-expectations MIT rspec-mocks MIT rspec-rails MIT rspec-support MIT rspec_junit_formatter MIT rubyntlm MIT ruby-saml MIT rufus-scheduler MIT safe_yaml MIT salesforce_bulk_query BSD sass MIT sass-rails MIT sass-rails MIT sdoc MIT secure_headers Apache 2.0 sequel MIT serveza MIT settingslogic MIT shoulda MIT shoulda-context MIT shoulda-matchers MIT simplecov MIT simplecov-html MIT simplecov-rcov MIT sinatra MIT sinatra-activerecord MIT sinatra-contrib MIT skinny MIT slop MIT spring MIT sprockets MIT sprockets-rails MIT sqlite3 BSD sshkit GPL-No Distro sys-uname Ruby syslog_protocol MIT systemu BSDL term-ansicolor GPL-No Distro terminal-table MIT therubyracer MIT thin Ruby thor MIT thread_safe Apache-2.0 thrift Apache 2.0 thrift-rack MIT tilt MIT timecop MIT timers MIT tins MIT treetop MIT turbolinks MIT twitter-bootstrap-rails MIT typhoeus MIT tzinfo MIT uglifier MIT unicorn Ruby unicorn-rails MIT uuid MIT uuidtools Apache-2.0 vegas MIT webmock MIT web-console MIT xml-simple Ruby xpath MIT yard MIT Storage services Library License Amazon AWS SDK for Java Apache 2.0 Amazon AWS Command Line Interface Apache 2.0 Go packages Library License Go-Mysql-Driver Mozilla Public License 2.0 gocql BSD 3-Clause groupcache Apache 2.0 Migrate MIT go-sqlite3 MIT Revel MIT Pathtree MIT Ansicolor MIT fsnotify BSD 3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 172.90364,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Services <em>licenses</em>",
        "sections": "Services <em>licenses</em>",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Contents Java internal"
      },
      "id": "603ea24364441f91fb4e8864"
    },
    {
      "sections": [
        "FIT instrumentation end user license agreement"
      ],
      "title": "FIT instrumentation end user license agreement",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "e8e10baf120678407d08c9f78ab708d271cf0223",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/fit-instrumentation-end-user-license-agreement/",
      "published_at": "2021-10-08T06:50:31Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "NEW RELIC, INC. FIT INSTRUMENTATION END USER LICENSE AGREEMENT In connection with the work provided by New Relic’s Field Instrumentation Team, you may be provided with certain custom-created software to enable, optimize, or enhance your use of New Relic’s Services. By downloading, installing, authorizing installation, or using the FIT Instrumentation with your Underlying Software (defined below), you (“Customer”) agree to the terms and conditions herein (“Agreement”) with New Relic, Inc., a Delaware corporation with offices located at 188 Spear Street, Suite 1200 San Francisco, CA 94105 (“New Relic”), (collectively the “Parties”). Capitalized terms not defined herein shall have the meanings set forth in the New Relic Terms of Service (the “Terms of Service”) available here: http://newrelic.com/terms IF YOU DO NOT AGREE TO THIS AGREEMENT, PLEASE DO NOT USE THE FIT INSTRUMENTATION. 1. DEFINITIONS “FIT Instrumentation” means the New Relic custom-made software, including but not limited to connectors, extensions, and plugins, provided by New Relic to Customer to enable the Services to operate with certain third party or Customer software or systems (“Underlying Software”). For the purposes of the Terms of Service, the FIT Instrumentation shall be treated like an “Agent”, subject to the separate terms herein. 2. USE OF THE FIT INSTRUMENTATION 2.1 Software Evaluation License. Subject to the terms herein, New Relic grants to Customer a limited, non-exclusive, non-transferrable, non-sublicensable right to install, use, and configure the FIT Instrumentation solely as needed to enable the Services for internal evaluation purposes. New Relic reserves all rights and licenses not expressly granted herein. 2.2 Ownership. As between the Parties, New Relic owns all right, title, and interest to the FIT Instrumentation and Feedback, including but not limited to any intellectual property and proprietary rights therein. Customer retains all right, title, and interest in any Customer Data processed by the FIT Instrumentation in connection with the Services. 2.3 Feedback. Customer agrees to provide feedback, suggestions, ideas, requests or recommendations (“Feedback”) regarding the FIT Instrumentation, and hereby irrevocably assigns all intellectual property and proprietary rights it holds in the Feedback to New Relic. 2.4 Notice. Customer acknowledges and agrees that the FIT Instrumentation is made to work with the Underlying Software as configured at the time of creation and there is no guarantee that FIT Instrumentation will continue to work in the event Customer changes, replaces, upgrades versions of, updates, or otherwise changes the Underlying Software (an “Upgrade Event”). Customer shall hold New Relic harmless from any claims or damages arising from Customer’s Upgrade Event. 2.5 Restrictions. Customer will not: (i) use the FIT Instrumentation except as permitted hereunder; (ii) distribute, sell, sublicense, or otherwise transfer the FIT Instrumentation; (iii) decompile, disassemble or reverse engineer any software underlying the FIT Instrumentation; (iv) use the FIT Instrumentation to damage, disable, overburden or impair any New Relic server or network(s) connected to any New Relic server or interfere with any other party’s use and enjoyment of the Services; (v) use the FIT Instrumentation to defraud, defame, abuse, harass, stalk, threaten or infringe the rights of privacy or other intellectual property rights of others or otherwise violate any applicable law; (vi) circumvent or modify any security technologies included as part of the FIT Instrumentation; or (vii) attempt or permit any third party to do any of the foregoing. New Relic may suspend usage of the Services by any user, without notice, pending any investigation of misuse.  These restrictions may be supplemented or superceded (to the extent they conflict) by the New Relic Acceptable Use Policy as may be published and updated from time to time at https://docs.newrelic.com/docs/licenses/license-information/acceptable-use-policy/acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential Information” means: (a) the FIT Instrumentation, and any features, results or output produced by, and other information relating to the FIT Instrumentation (including, without limitation, all Feedback); and (b) any business or technical information of New Relic including but not limited to any technical information, research, development, know-how that a reasonable person would understand to be confidential. 3.2 Restrictions. Customer will not use or disclose any Confidential Information, except as necessary for the performance of this Agreement. Customer will use all reasonable efforts to protect Confidential Information from unauthorized use or disclosure, but in no event less than the efforts that it ordinarily uses with respect to its own proprietary information. Customer may disclose Confidential Information to those of its employees who have a bona fide need to know such Confidential Information for the performance of this Agreement; provided that each such employee first executes a written agreement that contains use and nondisclosure restrictions at least as protective as those set forth herein. Confidential Information shall not include any information that: (a) is or becomes generally known to the public through no fault or breach of this Agreement by Customer; (b) is rightfully known by Customer at the time of disclosure without an obligation of confidentiality; (c) is independently developed by Customer without access or use of any Confidential Information; or (d) is rightfully obtained from a third party without restriction on use or disclosure. 4. DISCLAIMER 4.1 DISCLAIMER. FIT INSTRUMENTATION IS PROVIDED AS-IS AND AS-AVAILABLE AND NEW RELIC DISCLAIMS AND MAKES NO WARRANTIES, EITHER EXPRESS OR IMPLIED, INCLUDING ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR NON-INFRINGEMENT AND WHETHER OR NOT ARISING THROUGH A COURSE OF DEALING. FIT INSTRUMENTATION IS NOT GUARANTEED TO BE ERROR-FREE, COMPATIBLE WITH THE UNDERLYING SOFTWARE. , OR THAT CUSTOMER WILL ACHIEVE ANY RESULTS FROM USE OF THE FIT INSTRUMENTATION THEREFROM. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OF IMPLIED WARRANTIES OR LIMITATIONS ON APPLICABLE STATUTORY RIGHTS OF A CONSUMER, SO THE ABOVE EXCLUSION AND LIMITATIONS MAY NOT APPLY TO THE CUSTOMER. 4.2 LIMITATION OF LIABILITY. TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE TO CUSTOMER OR ANY THIRD PARTY FOR DAMAGES OF ANY KIND, INCLUDING, WITHOUT LIMITATION, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE INSTALLATION, USE OR INABILITY TO USE THE FIT INSTRUMENTATION OR FOR ANY ERROR OR DEFECT IN THE FIT INSTRUMENTATION OR THE SERVICES, WHETHER SUCH LIABILITY ARISES FROM ANY CLAIM BASED UPON CONTRACT, WARRANTY, TORT, STRICT LIABILITY, OR OTHERWISE, WHETHER OR NOT NEW RELIC HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSS OR DAMAGE. NEW RELIC'S AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS AGREEMENT WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE PARTIES HAVE AGREED THAT THESE LIMITATIONS WILL SURVIVE AND APPLY EVEN IF ANY LIMITED REMEDY SPECIFIED IN THIS AGREEMENT IS FOUND TO HAVE FAILED OF ITS ESSENTIAL PURPOSE. 4.3 EVALUATION VERSION. Customer agrees and acknowledges that: (a) the FIT Instrumentation is not an official product and has not been commercially released for sale by New Relic; (b) the FIT Instrumentation may not operate properly, being in final form, or fully functional; (c) the FIT Instrumentation may contain errors, security vulnerabilities, design flaws, or other problems; (d) it may not be possible to make the FIT Instrumentation fully functional; (e) the information obtained using the FIT Instrumentation may not be accurate; (f) use of the FIT Instrumentation may result in unexpected results, loss of data, delays or other unpredictable damages or loss; (g) New Relic is under no obligation to release a commercial version of the FIT Instrumentation; and (h) New Relic has the right unilaterally to abandon development of the FIT Instrumentation, at any time and without any obligation or liability to Customer. 5. GENERAL PROVISIONS 5.1 Terms of Service; Documentation. This Agreement shall be considered a part of the New Relic documentation, located at: https://docs.newrelic.com (the \"Documentation\").This Agreement and the Terms of Service constitute the entire and exclusive agreement between New Relic and Customer with respect to the FIT Instrumentation. In the absence of a separate agreement between New Relic and Customer with respect to the FIT Instrumentation, the terms and conditions of this Agreement shall govern the relationship between New Relic and Customer with respect to such FIT Instrumentation.  To the extent of a conflict between the Agreement and the Terms of Service, this Agreement shall govern with respect to the FIT Instrumentation only.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 172.01585,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "FIT instrumentation end user <em>license</em> agreement",
        "sections": "FIT instrumentation end user <em>license</em> agreement",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": " Acceptable Use Policy as may be published and updated from time to time at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;acceptable-use-policy&#x2F;acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential <em>Information</em>” means: (a) the FIT Instrumentation, and any features, results"
      },
      "id": "603e9f3ee7b9d206e32a0800"
    },
    {
      "sections": [
        "New Relic Agent Software Notice"
      ],
      "title": "New Relic Agent Software Notice",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "2bf9501c2767105130d3808f1bf3a91a032d903e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/new-relic-agent-software-notice/",
      "published_at": "2021-10-08T06:50:31Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This software is © 2008-2021 New Relic, Inc. and its licensors. This software is solely for use with New Relic’s proprietary SaaS service (“New Relic Service”), so to use the software you must have a valid account for the New Relic Service under a separate agreement with New Relic (“Subscription Agreement”). You may only use the software to support your use of the New Relic Service as permitted in the Subscription Agreement. Without a Subscription Agreement, you may not use the software. All other use is prohibited. New Relic and its suppliers retain all right, title and interest (including intellectual property rights) in the software. The Subscription Agreement will control in event of a conflict with this notice. Unless otherwise agreed by New Relic in your Subscription Agreement: You may not use, copy, distribute or sublicense the software, use the software on behalf of third parties, reverse engineer or decompile the software, modify or create derivative works of the software, use the software for competitive analysis or benchmarking, or remove or obscure any proprietary notices in the software. The software is provided “AS IS” and New Relic disclaims all warranties, whether express, implied, statutory or otherwise, including warranties of merchantability, fitness for a particular purpose, title or noninfringement. To the full extent permitted by law, New Relic will have no liability arising from or related to the software or under this notice for any direct, indirect, special, incidental, or consequential damages of any kind, even if advised of their possibility in advance, and regardless of legal theory (whether contract, tort, negligence, strict liability or otherwise). The software may contain third-party open source software (“OSS”) as described here and at https://github.com/newrelic. To the extent required by the OSS license, that license will apply to the OSS when used on a stand-alone basis. For avoidance of doubt, you may copy and distribute New Relic agents pursuant to your Subscription Agreement for New Relic Browser and New Relic Mobile and as set forth: https://docs.newrelic.com/docs/mobile-monitoring/new-relic-mobile/get-started and https://docs.newrelic.com/docs/browser/new-relic-browser/installation/install-new-relic-browser-agent Software versions New Relic makes available under an OSS license (such as Apache 2.0) are governed by the terms of the applicable OSS license. For a current list of New Relic software versions released as OSS please visit https://opensource.newrelic.com/.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 172.01585,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": " and at https:&#x2F;&#x2F;github.com&#x2F;newrelic. To the extent required by the OSS <em>license</em>, that <em>license</em> will apply to the OSS when used on a stand-alone basis. For avoidance of doubt, you may copy and distribute New Relic agents pursuant to your Subscription Agreement for New Relic Browser and New Relic Mobile"
      },
      "id": "603eb73828ccbc1f99eba74a"
    }
  ],
  "/docs/licenses/license-information/distributed-licenses/fit-instrumentation-end-user-license-agreement": [
    {
      "sections": [
        "Services licenses",
        "Contents",
        "Java internal services",
        "Java crash data API",
        "CoffeeScript",
        "JavaScript",
        "Crash reporting",
        "Label services",
        "Infrastructure Monitoring services",
        ".NET support uploader service",
        "Ruby gems",
        "Storage services",
        "Go packages"
      ],
      "title": "Services licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "49a2ad450d31dacfc2aae690ca947d2326d18761",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/other-licenses/services-licenses/",
      "published_at": "2021-10-08T10:17:30Z",
      "updated_at": "2021-04-06T00:26:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Contents Java internal services Java crash data API Framework License Apache TomCat Apache 2.0 Bean Validation Apache 2.0 Cassandra Driver Core Apache 2.0 ClassMate Apache 2.0 Hamcrest BSD Mockito MIT Netty Apache 2.0 Objenesis MIT SnakeYAML Apache 2.0 Spring Boot Apache 2.0 Java internal services Library License json_simple Apache 2.0 newrelic-api New Relic newrelic-api New Relic antlr BSD aopalliance Public Domain asm-analysis BSD asm-commons BSD asm-tree BSD asm-util BSD asm BSD c3p0 EPLv1.0 cglib-nodep Apache 2.0 cglib Apache 2.0 cglib Apache 2.0 cal10n-api MIT logback-classic EPLv1.0 logback-core EPLv1.0 guava-jetty-service Apache 2.0 zkclient Apache 2.0 reporter-config-base Apache 2.0 reporter-config3 Apache 2.0 reporter-config Apache 2.0 aws-java-sdk-acm Apache 2.0 aws-java-sdk-api-gateway Apache 2.0 aws-java-sdk-applicationautoscaling Apache 2.0 aws-java-sdk-autoscaling Apache 2.0 aws-java-sdk-cloudformation Apache 2.0 aws-java-sdk-cloudfront Apache 2.0 aws-java-sdk-cloudhsm Apache 2.0 aws-java-sdk-cloudsearch Apache 2.0 aws-java-sdk-cloudtrail Apache 2.0 aws-java-sdk-cloudwatch Apache 2.0 aws-java-sdk-cloudwatchmetrics Apache 2.0 aws-java-sdk-codecommit Apache 2.0 aws-java-sdk-codedeploy Apache 2.0 aws-java-sdk-codepipeline Apache 2.0 aws-java-sdk-cognitoidentity Apache 2.0 aws-java-sdk-cognitoidp Apache 2.0 aws-java-sdk-cognitosync Apache 2.0 aws-java-sdk-config Apache 2.0 aws-java-sdk-core Apache 2.0 aws-java-sdk-datapipeline Apache 2.0 aws-java-sdk-devicefarm Apache 2.0 aws-java-sdk-directconnect Apache 2.0 aws-java-sdk-directory Apache 2.0 aws-java-sdk-discovery Apache 2.0 aws-java-sdk-dms Apache 2.0 aws-java-sdk-dynamodb Apache 2.0 aws-java-sdk-ec2 Apache 2.0 aws-java-sdk-ecr Apache 2.0 aws-java-sdk-ecs Apache 2.0 aws-java-sdk-efs Apache 2.0 aws-java-sdk-elasticache Apache 2.0 aws-java-sdk-elasticbeanstalk Apache 2.0 aws-java-sdk-elasticloadbalancing Apache 2.0 aws-java-sdk-elasticloadbalancingv2 Apache 2.0 aws-java-sdk-elasticsearch Apache 2.0 aws-java-sdk-elastictranscoder Apache 2.0 aws-java-sdk-emr Apache 2.0 aws-java-sdk-events Apache 2.0 aws-java-sdk-gamelift Apache 2.0 aws-java-sdk-glacier Apache 2.0 aws-java-sdk-iam Apache 2.0 aws-java-sdk-importexport Apache 2.0 aws-java-sdk-inspector Apache 2.0 aws-java-sdk-iot Apache 2.0 aws-java-sdk-kinesis Apache 2.0 aws-java-sdk-kms Apache 2.0 aws-java-sdk-lambda Apache 2.0 aws-java-sdk-logs Apache 2.0 aws-java-sdk-machinelearning Apache 2.0 aws-java-sdk-marketplacecommerceanalytics Apache 2.0 aws-java-sdk-marketplacemeteringservice Apache 2.0 aws-java-sdk-models Apache 2.0 aws-java-sdk-opsworks Apache 2.0 aws-java-sdk-rds Apache 2.0 aws-java-sdk-redshift Apache 2.0 aws-java-sdk-route53 Apache 2.0 aws-java-sdk-s3 Apache 2.0 aws-java-sdk-servicecatalog Apache 2.0 aws-java-sdk-ses Apache 2.0 aws-java-sdk-simpledb Apache 2.0 aws-java-sdk-simpleworkflow Apache 2.0 aws-java-sdk-snowball Apache 2.0 aws-java-sdk-sns Apache 2.0 aws-java-sdk-sqs Apache 2.0 aws-java-sdk-ssm Apache 2.0 aws-java-sdk-storagegateway Apache 2.0 aws-java-sdk-sts Apache 2.0 aws-java-sdk-support Apache 2.0 aws-java-sdk-swf-libraries Apache 2.0 aws-java-sdk-waf Apache 2.0 aws-java-sdk-workspaces Apache 2.0 aws-java-sdk Apache 2.0 jmespath-java Apache 2.0 AppleJavaExtensions BSD jcommander Apache 2.0 high-scale-lib MIT hppc Apache 2.0 clover Paid stream Apache 2.0 metrics-core Apache 2.0 cassandra-driver-core Apache 2.0 cassandra-driver-core Apache 2.0 uuid MIT grabbag MIT speed4j Apache 2.0 yamlbeans BSD jackson-annotations Apache 2.0 jackson-core Apache 2.0 jackson-databind Apache 2.0 jackson-dataformat-cbor Apache 2.0 jackson-dataformat-csv Apache 2.0 jackson-dataformat-xml Apache 2.0 jackson-dataformat-yaml Apache 2.0 jackson-datatype-guava Apache 2.0 jackson-datatype-jdk7 Apache 2.0 jackson-datatype-jdk8 Apache 2.0 jackson-datatype-joda Apache 2.0 jackson-datatype-jsr310 Apache 2.0 jackson-jaxrs-base Apache 2.0 jackson-jaxrs-json-provider Apache 2.0 jackson-module-afterburner Apache 2.0 jackson-module-jaxb-annotations Apache 2.0 jackson-module-kotlin Apache 2.0 woodstox-core Apache 2.0 classmate Apache 2.0 zjsonpatch Apache 2.0 caffeine Apache 2.0 waffle-jna EPLv1.0 docker-java Apache 2.0 docker-java Apache 2.0 btf Apache 2.0 jackson-coreutils Apache 2.0 json-schema-core Apache 2.0 json-schema-validator Apache 2.0 msg-simple Apache 2.0 uri-template Apache 2.0 jamm Apache 2.0 jffi Apache 2.0 jnr-constants Apache 2.0 jnr-ffi Apache 2.0 jnr-ffi Apache 2.0 jnr-posix EPLv1.0 jnr-x86asm MIT embedded-redis Apache 2.0 memoryfilesystem MIT guava-retrying Apache 2.0 guava-retrying Apache 2.0 snowball-stemmer BSD system-rules CPL uuid Apache 2.0 wiremock Apache 2.0 auto-common Apache 2.0 annotations LGPLv2.1 annotations LGPLv2.1 bcel-findbugs LGPLv2.1 findbugs LGPLv2.1 jFormatString BSD jFormatString BSD jsr305 BSD jsr305 BSD jsr305 BSD gson Apache 2.0 spymemcached MIT guava-testlib Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guice-multibindings Apache 2.0 guice Apache 2.0 truth Apache 2.0 concurrent-trees Apache 2.0 concurrentlinkedhashmap-lru Apache 2.0 json-simple Apache 2.0 libphonenumber Apache 2.0 locality-uuid BSD h2 EPLv1.0 annotations Apache 2.0 json-path-assert Apache 2.0 json-path Apache 2.0 json-path Apache 2.0 jzlib BSD bonecp-provider Apache 2.0 bonecp Apache 2.0 junixsocket-common Apache 2.0 junixsocket-native-common Apache 2.0 disruptor Apache 2.0 geoip-api LGPLv2.1 c3p0 LGPLv2.1 mchange-commons-java LGPLv2.1 archaius-core Apache 2.0 hystrix-core Apache 2.0 hystrix-servo-metrics-publisher Apache 2.0 rxjava-core Apache 2.0 servo-core Apache 2.0 kafka-clients Apache 2.0 kafka_2.10 Apache 2.0 kafka_2.11 Apache 2.0 common-cassandra Apache 2.0 timeslice_utils New Relic mockito-kotlin MIT compress-lzf Apache 2.0 checkstyle Apache 2.0 dagger-compiler Apache 2.0 dagger Apache 2.0 okhttp Apache 2.0 okio Apache 2.0 javapoet Apache 2.0 javawriter Apache 2.0 jaxb-impl CDDLv1 thrift-server Apache 2.0 finagle-core_2.10 Apache 2.0 finagle-core_2.11 Apache 2.0 finagle-http_2.10 Apache 2.0 finagle-http_2.11 Apache 2.0 jsr166e Creative Commons util-app_2.10 Apache 2.0 util-app_2.11 Apache 2.0 util-cache_2.10 Apache 2.0 util-cache_2.11 Apache 2.0 util-codec_2.10 Apache 2.0 util-codec_2.11 Apache 2.0 util-collection_2.10 Apache 2.0 util-collection_2.11 Apache 2.0 util-core_2.10 Apache 2.0 util-core_2.11 Apache 2.0 util-function_2.10 Apache 2.0 util-function_2.11 Apache 2.0 util-hashing_2.10 Apache 2.0 util-hashing_2.11 Apache 2.0 util-jvm_2.10 Apache 2.0 util-jvm_2.11 Apache 2.0 util-lint_2.10 Apache 2.0 util-lint_2.11 Apache 2.0 util-logging_2.10 Apache 2.0 util-logging_2.11 Apache 2.0 util-registry_2.10 Apache 2.0 util-registry_2.11 Apache 2.0 util-stats_2.10 Apache 2.0 util-stats_2.11 Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 HikariCP Apache 2.0 commons-beanutils Apache 2.0 commons-cli Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-io Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 commons-pool Apache 2.0 unix-socket-factory Apache 2.0 unix-socket-factory Apache 2.0 jflex BSD dom4j BSD airline Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 metrics-healthchecks Apache 2.0 metrics-httpclient Apache 2.0 metrics-jdbi Apache 2.0 metrics-jersey2 Apache 2.0 metrics-jetty9 Apache 2.0 metrics-json Apache 2.0 metrics-jvm Apache 2.0 metrics-logback Apache 2.0 metrics-servlets Apache 2.0 dropwizard-java8-jdbi Apache 2.0 dropwizard-client Apache 2.0 dropwizard-configuration Apache 2.0 dropwizard-core Apache 2.0 dropwizard-db Apache 2.0 dropwizard-jackson Apache 2.0 dropwizard-jdbi Apache 2.0 dropwizard-jersey Apache 2.0 dropwizard-jetty Apache 2.0 dropwizard-lifecycle Apache 2.0 dropwizard-logging Apache 2.0 dropwizard-metrics Apache 2.0 dropwizard-servlets Apache 2.0 dropwizard-testing Apache 2.0 dropwizard-util Apache 2.0 dropwizard-validation Apache 2.0 netty-all Apache 2.0 netty-buffer Apache 2.0 netty-codec-http Apache 2.0 netty-codec-socks Apache 2.0 netty-codec Apache 2.0 netty-common Apache 2.0 netty-handler-proxy Apache 2.0 netty-handler Apache 2.0 netty-resolver Apache 2.0 netty-transport-native-epoll Apache 2.0 netty-transport Apache 2.0 netty Apache 2.0 ratpack-core Apache 2.0 ratpack-groovy-test Apache 2.0 ratpack-groovy Apache 2.0 ratpack-guice Apache 2.0 ratpack-test Apache 2.0 rxjava Apache 2.0 fastutil Apache 2.0 janino BSD activation Apache 2.0 javax.annotation-api CDDLv1 jsr250-api CDDLv1 javax.inject Apache 2.0 mail CDDLv1 mailapi CDDLv1 javax.servlet-api Apache 2.0 servlet-api Apache 2.0 javax.transaction-api CDDLv1 validation-api Apache 2.0 javax.websocket-api CDDLv1 javax.ws.rs-api CDDLv1 jaxb-api CDDLv1 stax-api CDDLv1 jaxen Apache 2.0 jline BSD jline BSD joda-time Apache 2.0 junit-dep CPL junit CPL junit EPLv1.0 kafka_2.10 Apache 2.0 kafka_2.9.2 Apache 2.0 apache-log4j-extras Apache 2.0 log4j Apache 2.0 mysql-connector-java GPLv2 with Classpath Exception byte-buddy Apache 2.0 byte-buddy Apache 2.0 jna-platform LGPLv2.1 jna Apache 2.0 jcip-annotations Apache 2.0 lz4 Apache 2.0 accessors-smart Apache 2.0 asm Apache 2.0 json-smart Apache 2.0 json-smart Apache 2.0 primitive GPLv2 with Classpath Exception compiler Apache 2.0 lang Apache 2.0 beaver-cc BSD ehcache-core Apache 2.0 ehcache Apache 2.0 jopt-simple MIT jopt-simple MIT opencsv Apache 2.0 quality-check Apache 2.0 scannotation Apache 2.0 super-csv Apache 2.0 uadetector-core Apache 2.0 uadetector-resources Apache 2.0 argparse4j MIT spymemcached MIT kalium Apache 2.0 ST4 BSD antlr-runtime BSD antlr-runtime BSD antlr BSD stringtemplate BSD ant-launcher Apache 2.0 ant Apache 2.0 cassandra-all Apache 2.0 cassandra-all Apache 2.0 cassandra-thrift Apache 2.0 cassandra-thrift Apache 2.0 commons-collections4 Apache 2.0 commons-compress Apache 2.0 commons-csv Apache 2.0 commons-dbcp2 Apache 2.0 commons-io Apache 2.0 commons-lang3 Apache 2.0 commons-math3 Apache 2.0 commons-math Apache 2.0 commons-pool2 Apache 2.0 curator-client Apache 2.0 curator-framework Apache 2.0 curator-recipes Apache 2.0 curator-test Apache 2.0 derby Apache 2.0 geronimo-servlet_3.0_spec Apache 2.0 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 ivy Apache 2.0 kafka-clients Apache 2.0 kafka_2.11 Apache 2.0 libthrift Apache 2.0 tomcat-embed-core Apache 2.0 tomcat-embed-el Apache 2.0 tomcat-embed-logging-juli Apache 2.0 tomcat-embed-websocket Apache 2.0 tomcat-jdbc Apache 2.0 tomcat-juli Apache 2.0 zookeeper Apache 2.0 aspectjweaver EPLv1.0 assertj-core Apache 2.0 assertj-core Apache 2.0 evo-inflector Apache 2.0 bcmail-jdk15on MIT bcpkix-jdk15on MIT bcprov-jdk15on MIT ohc-core Apache 2.0 cassandra-unit-spring LGPLv3 cassandra-unit-spring LGPLv3 cassandra-unit LGPLv3 cassandra-unit LGPLv3 groovy-all Apache 2.0 groovy-all Apache 2.0 groovy Apache 2.0 groovy Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 easymock Apache 2.0 ecj EPLv1.0 javax.activation CDDLv1 javax.annotation CDDLv1 javax.mail.glassfish CDDLv1 javax.servlet CDDLv1 javax.transaction CDDLv1 org.objectweb.asm BSD jetty-setuid-java EPLv1.0 jetty-schemas Apache 2.0 javax-websocket-client-impl Apache 2.0 javax-websocket-server-impl Apache 2.0 websocket-api Apache 2.0 websocket-client Apache 2.0 websocket-common Apache 2.0 websocket-server Apache 2.0 websocket-servlet Apache 2.0 apache-jsp Apache 2.0 jetty-annotations CDDLv1 jetty-client Apache 2.0 jetty-continuation Apache 2.0 jetty-http Apache 2.0 jetty-io Apache 2.0 jetty-jmx Apache 2.0 jetty-jndi Apache 2.0 jetty-plus Apache 2.0 jetty-proxy Apache 2.0 jetty-security Apache 2.0 jetty-server Apache 2.0 jetty-servlet Apache 2.0 jetty-servlets Apache 2.0 jetty-util Apache 2.0 jetty-webapp Apache 2.0 jetty-xml Apache 2.0 sigar Apache 2.0 aopalliance-repackaged CDDLv1 javax.inject CDDLv1 hk2-api CDDLv1 hk2-locator CDDLv1 hk2-utils CDDLv1 osgi-resource-locator CDDLv1 jersey-guava CDDLv1 jersey-client CDDLv1 jersey-apache-connector CDDLv1 jersey-container-servlet-core CDDLv1 jersey-container-servlet CDDLv1 jersey-client CDDLv1 jersey-common CDDLv1 jersey-server CDDLv1 jersey-bean-validation CDDLv1 jersey-metainf-services CDDLv1 jersey-media-jaxb CDDLv1 jersey-client CDDLv1 jersey-test-framework-provider-inmemory CDDLv1 jersey-test-framework-core CDDLv1 javax.el CDDLv1 hamcrest-all BSD hamcrest-core BSD hamcrest-integration BSD hamcrest-library BSD HdrHistogram Public Domain hector-core MIT hibernate-commons-annotations LGPLv2.1 hibernate-jpa-2.0-api LGPLv2.1 hibernate-jpa-2.1-api LGPLv2.1 hibernate-c3p0 Apache 2.0 hibernate-core Apache 2.0 hibernate-ehcache Apache 2.0 hibernate-entitymanager Apache 2.0 hibernate-jmx Apache 2.0 hibernate-validator Apache 2.0 freebuilder Apache 2.0 org.jacoco.agent EPLv1.0 org.jacoco.ant EPLv1.0 org.jacoco.core EPLv1.0 org.jacoco.report EPLv1.0 javassist Apache 2.0 jboss-logging-annotations LGPLv2.1 jboss-logging Apache 2.0 jboss-transaction-api_1.1_spec LGPLv2.1 jboss-transaction-api_1.2_spec LGPLv2.1 jandex Apache 2.0 jdbi Apache 2.0 job-dsl-core Apache 2.0 version-number MIT kotlin-annotation-processing Apache 2.0 kotlin-reflect Apache 2.0 kotlin-runtime Apache 2.0 kotlin-stdlib Apache 2.0 kotlin-test-junit Apache 2.0 kotlin-test Apache 2.0 annotations Apache 2.0 jolokia-core Apache 2.0 jruby-complete LGPLv2.1 LGPLv2.1 json Public Domain json Apache 2.0 xstream BSD annotations Apache 2.0 mapdb Apache 2.0 mapdb Apache 2.0 jbcrypt ISC mockserver-client-java Apache 2.0 mockserver-core Apache 2.0 mockserver-logging Apache 2.0 mockserver-netty Apache 2.0 mockito-all MIT mockito-core MIT mockito-core MIT apache-el Apache 2.0 apache-jsp Apache 2.0 alpn-boot Apache 2.0 etcd4j Apache 2.0 msgpack Apache 2.0 objenesis Apache 2.0 objenesis Apache 2.0 jmh-core GPLv2 with Classpath Exception jmh-generator-annprocess GPLv2 with Classpath Exception jol-core GPLv2 with Classpath Exception asm-analysis BSD asm-analysis BSD asm-commons BSD asm-debug-all BSD asm-tree BSD asm-util BSD asm-util BSD asm BSD asm Public Domain postgresql BSD powermock-api-mockito Apache 2.0 powermock-api-support Apache 2.0 powermock-core Apache 2.0 powermock-module-junit4-common Apache 2.0 powermock-module-junit4 Apache 2.0 powermock-reflect Apache 2.0 lombok MIT reactive-streams Public Domain reflections WTFPL scala-java8-compat_2.11 Scala scala-parser-combinators_2.11 Scala scala-xml_2.11 Scala scala-library Scala scala-reflect Scala scalatest_2.10 Apache 2.0 scalatest_2.11 Apache 2.0 native-lib-loader BSD jsonassert Apache 2.0 jcl-over-slf4j MIT jul-to-slf4j MIT log4j-log4j12 MIT log4j-over-slf4j MIT slf4j-api MIT slf4j-ext MIT slf4j-log4j12 MIT slf4j-simple MIT spock-core Apache 2.0 spock-core Apache 2.0 spring-boot-actuator Apache 2.0 spring-boot-autoconfigure Apache 2.0 spring-boot-configuration-processor Apache 2.0 spring-boot-starter-actuator Apache 2.0 spring-boot-starter-aop Apache 2.0 spring-boot-starter-data-jpa Apache 2.0 spring-boot-starter-data-rest Apache 2.0 spring-boot-starter-jdbc Apache 2.0 spring-boot-starter-jetty Apache 2.0 spring-boot-starter-logging Apache 2.0 spring-boot-starter-test Apache 2.0 spring-boot-starter-tomcat Apache 2.0 spring-boot-starter-web Apache 2.0 spring-boot-starter Apache 2.0 spring-boot-test-autoconfigure Apache 2.0 spring-boot-test Apache 2.0 spring-boot Apache 2.0 spring-data-commons Apache 2.0 spring-data-jpa Apache 2.0 spring-data-rest-core Apache 2.0 spring-data-rest-webmvc Apache 2.0 spring-hateoas Apache 2.0 spring-plugin-core Apache 2.0 spring-aop Apache 2.0 spring-aspects Apache 2.0 spring-beans Apache 2.0 spring-context-support Apache 2.0 spring-context Apache 2.0 spring-core Apache 2.0 spring-core Apache 2.0 spring-expression Apache 2.0 spring-jdbc Apache 2.0 spring-orm Apache 2.0 spring-test Apache 2.0 spring-test Apache 2.0 spring-tx Apache 2.0 spring-web Apache 2.0 spring-webmvc Apache 2.0 xz Public Domain wasabi Apache 2.0 snappy-java Apache 2.0 xmlunit-core Apache 2.0 xmlunit-legacy Apache 2.0 snakeyaml Apache 2.0 JUnitParams Apache 2.0 postgresql BSD postgresql BSD jedis MIT scala-library Scala ion-java Apache 2.0 timeslice_service-thrift New Relic timeslice_service New Relic xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3_min Public Domain zkclient Apache 2.0 zookeeper Apache 2.0 CoffeeScript Library License CoffeeScript MIT coffin MIT JavaScript JavaScript License ie_html5/ie_html5.js MIT Angular.js MIT Angular UI directives for Bootstrap MIT Chart.js MIT Chosen JQuery plugin MIT Fullcalendar MIT Moment MIT Crash reporting Software License Apple OS X OS X Yosemite Xcode & Apple SDK Xcode license Label services Service License BoneCP Apache 2.0 Gradle Apache 2.0 Hibernate Apache 2.0 Spring Apache 2.0 Infrastructure Monitoring services Service License antlr BSD aopalliance Public Domain asm MIT cglib Apache 2.0 ch.qos.logback EPLv1.0 com.101tec Apache 2.0 com.amazonaws Apache 2.0 com.cenqua.clover Paid com.fasterxml.jackson.core Apache 2.0 com.fasterxml Apache 2.0 com.google.code.findbugs BSD com.google.guava Apache 2.0 com.googlecode.json-simple Apache 2.0 com.intellij Apache 2.0 New Relic com.squareup.dagger Apache 2.0 com.squareup Apache 2.0 com.yammer.metrics Apache 2.0 commons-codec Apache 2.0 commons-configuration Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 dom4j BSD io.netty Apache 2.0 javax.annotation CDDLv1 javax.inject Apache 2.0 javax.servlet.jsp CDDLv1 javax.servlet Apache 2.0 javax.validation Apache 2.0 javax.websocket CDDLv1 jline BSD joda-time Apache 2.0 junit CPL log4j Apache 2.0 mysql GPLv2 with Classpath Exception net.sf.jopt-simple MIT org.apache.commons Apache 2.0 org.apache.httpcomponents Apache 2.0 org.apache.ivy Apache 2.0 org.apache.thrift Apache 2.0 org.apache.tomcat.embed Apache 2.0 org.apache.zookeeper Apache 2.0 org.codehaus.groovy Apache 2.0 org.codehaus.jackson Apache 2.0 org.easymock Apache 2.0 org.eclipse.jetty.orbit CDDLv1 org.eclipse.jetty.toolchain Apache 2.0 org.eclipse.jetty.websocket Apache 2.0 org.eclipse.jetty CDDLv1 org.glassfish.jersey.core CDDLv1 org.glassfish.web CDDLv1 org.glassfish CDDLv1 org.hamcrest BSD org.hibernate Apache 2.0 org.javassist Apache 2.0 org.jboss.logging LGPLv2.1 org.jenkins-ci.plugins Apache 2.0 org.jenkins-ci Apache 2.0 org.jetbrains Apache 2.0 org.jolokia Apache 2.0 org.jvnet.hudson BSD org.mockito MIT org.objenesis Apache 2.0 org.ow2.asm MIT org.reflections WTFPL org.scala-lang Scala org.slf4j MIT org.springframework.boot Apache 2.0 org.springframework Apache 2.0 org.xerial.snappy Apache 2.0 org.yaml Apache 2.0 xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3 Public Domain .NET support uploader service Library License Amazon AWS SDK for Java Apache 2.0 Apache Commons Codec Apache 2.0 Gson Apache 2.0 Jetty Apache 2.0 Ruby gems Ruby gems Gem License aasm MIT actionmailer MIT actionpack MIT activeadmin MIT actionview MIT activejob MIT active_model_serializers MIT activemodel MIT activerecord MIT activerecord-deprecated_finders MIT activerecord-mysql2-adapter MIT activeresource MIT activesupport MIT addressable Apache 2.0 angularjs-rails MIT arbre MIT arel MIT arel MIT atomic Apache 2.0 atomic Apache-2.0 attr_required MIT awesome_print MIT backports MIT better_errors MIT binding_of_caller MIT bourbon MIT brwsr MIT browser-timezone-rails MIT builder MIT bundler MIT byebug MIT callsite MIT capistrano MIT capistrano-bundler MIT capistrano-ext MIT capistrano-rbenv MIT capistrano-stats MIT capybara MIT celluloid MIT CFPropertyList MIT chunky_png MIT ci_reporter MIT ci_reporter_rspec MIT clockwork MIT codeclimate-test-reporter MIT coderay MIT coderay MIT coffee-rails MIT coffee-script MIT coffee-script-source MIT columnize Ruby compass MIT compass-core MIT compass-import-once MIT compass-rails MIT concurrent-ruby MIT crack MIT crypt Unlicensed daemons MIT dalli MIT database_cleaner MIT debug_inspector MIT debugger-linecache MIT diff-lcs MIT docile MIT dotenv MIT dotenv-deployment MIT erubis MIT etcd MIT ethon MIT eventmachine Ruby excon MIT execjs MIT factory_girl MIT factory_girl_rails MIT faraday MIT faraday-middleware MIT faye-websocket MIT ffi MIT ffi BSD fission MIT fog MIT fog-atmos MIT fog-aws MIT fog-brightbox MIT fog-core MIT fog-ecloud MIT fog-google MIT fog-json MIT fog-local MIT fog-powerdns MIT fog-profitbricks MIT fog-radosgw MIT fog-riakcs MIT fog-sakuracloud MIT fog-serverlove MIT fog-softlayer MIT fog-storm_on_demand MIT fog-terremark MIT fog-vmfusion MIT fog-voxel MIT fog-xml MIT font-awesome-rails MIT foreman MIT formatador MIT formtastic MIT formtastic i18n MIT fssm MIT globalid MIT guard MIT guard-rspec MIT haml MIT haml-rails MIT has scope MIT hashie MIT highline Ruby hike MIT hipchat-api MIT histogram MIT hitimes ISC hiredis BSD http_parser.rb MIT httpclient Ruby httpclient Ruby httparty MIT i18n MIT inflecto MIT inherited resources MIT intercom-rails MIT ipaddress MIT jasmine MIT jasmine-core MIT jasmine-rails MIT jbuilder MIT jira MIT jira-ruby OSL-3.0 journey MIT jquery-rails MIT jquery-turbolinks MIT jquery-ui-rails MIT json Ruby jwt MIT kaminari MIT kgio LGPLv3 kgio LGPLv3 kgio LGPL-v2.1+ libv8 MIT listen MIT logger Ruby loofah MIT lumberjack MIT macaddr MIT mail MIT mailcatcher MIT memoist MIT metaclass MIT meta_request MIT method_source MIT mime-types MIT mini_portile MIT minitest MIT minitest-rails MIT mixlib-log Apache 2.0 mocha MIT mono_logger MIT multi_json MIT multi_xml MIT multipart-post MIT mysql2 MIT net-http-persistent MIT net-scp MIT net-sftp MIT net-ssh MIT net-ssh-gateway MIT netrc MIT nokogiri MIT oauth2 MIT oj MIT omniauth MIT omniauth-oauth2 MIT pagerduty MIT papers MIT pg BSD pg Ruby phantomjs BSD phantomjs-binaries Unlicensed poltergeist MIT polyamorous MIT polyglot MIT polyglot MIT protected_attributes MIT pry MIT pry-nav MIT pry-rails MIT pry-stack_explorer MIT puma BSD quiet assets MIT rack MIT rack-cache MIT rack-contrib MIT rack-oauth2 MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-ssl MIT rack-ssl-enforcer MIT rack-test MIT rails MIT rails-deprecated_sanitizer MIT rails-dom-testing MIT rails-html-sanitizer MIT rails-observers MIT rails_12factor MIT rails_serve_static_assets MIT rails_stdout_logging MIT railties MIT raindrops LGPLv3 raindrops LGPLv3 raindrops LGPLv2.1+ rake MIT rake MIT rake MIT rake MIT ransack MIT rb-fsevent MIT rb-inotify MIT rdoc Ruby redcarpet MIT redis MIT redis-namespace MIT redis-queue MIT ref MIT remote_syslog_logger MIT request_store MIT responders MIT resque MIT resque-cleaner MIT resque-pool MIT resque-scheduler MIT resque-status MIT rest-client MIT restforce MIT rspec MIT rspec-core MIT rspec-expectations MIT rspec-mocks MIT rspec-rails MIT rspec-support MIT rspec_junit_formatter MIT rubyntlm MIT ruby-saml MIT rufus-scheduler MIT safe_yaml MIT salesforce_bulk_query BSD sass MIT sass-rails MIT sass-rails MIT sdoc MIT secure_headers Apache 2.0 sequel MIT serveza MIT settingslogic MIT shoulda MIT shoulda-context MIT shoulda-matchers MIT simplecov MIT simplecov-html MIT simplecov-rcov MIT sinatra MIT sinatra-activerecord MIT sinatra-contrib MIT skinny MIT slop MIT spring MIT sprockets MIT sprockets-rails MIT sqlite3 BSD sshkit GPL-No Distro sys-uname Ruby syslog_protocol MIT systemu BSDL term-ansicolor GPL-No Distro terminal-table MIT therubyracer MIT thin Ruby thor MIT thread_safe Apache-2.0 thrift Apache 2.0 thrift-rack MIT tilt MIT timecop MIT timers MIT tins MIT treetop MIT turbolinks MIT twitter-bootstrap-rails MIT typhoeus MIT tzinfo MIT uglifier MIT unicorn Ruby unicorn-rails MIT uuid MIT uuidtools Apache-2.0 vegas MIT webmock MIT web-console MIT xml-simple Ruby xpath MIT yard MIT Storage services Library License Amazon AWS SDK for Java Apache 2.0 Amazon AWS Command Line Interface Apache 2.0 Go packages Library License Go-Mysql-Driver Mozilla Public License 2.0 gocql BSD 3-Clause groupcache Apache 2.0 Migrate MIT go-sqlite3 MIT Revel MIT Pathtree MIT Ansicolor MIT fsnotify BSD 3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 172.90364,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Services <em>licenses</em>",
        "sections": "Services <em>licenses</em>",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Contents Java internal"
      },
      "id": "603ea24364441f91fb4e8864"
    },
    {
      "sections": [
        "Add-on end user license agreement",
        "New Relic, Inc. Add-on End User License Agreement"
      ],
      "title": "Add-on end user license agreement",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "a01c225ca30f95dab7db856cd946c76de557c31f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/add-end-user-license-agreement/",
      "published_at": "2021-10-08T06:49:32Z",
      "updated_at": "2021-03-16T06:19:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic, Inc. Add-on End User License Agreement In connection with the work provided by New Relic's Expert Services, Field Instrumentation Team, or Sales team, you may be provided with certain custom-created software to visualize, enable, optimize, or enhance your use of New Relic's Services. By downloading, installing, authorizing installation, or using the Add-on (defined below) with your Underlying Software (defined below), you (“Customer”) agree to the terms and conditions herein (“Agreement”) with New Relic, Inc., a Delaware corporation with offices located at 188 Spear Street, Suite 1200 San Francisco, CA 94105 (“New Relic\"), (collectively the “Parties”). Capitalized terms not defined herein shall have the meanings set forth in the New Relic Terms of Service (the “Terms of Service”) available here: newrelic.com/termsandconditions/terms. IF YOU DO NOT AGREE TO THIS AGREEMENT, YOUR SOLE REMEDY IS TO NOT USE THE ADD-ON. 1. DEFINITIONS “Add-on” means the New Relic applications and/or software, including but not limited to connectors, extensions, UI extension, and plugins, provided by New Relic to Customer to enable the Services to operate with certain third party or Customer software or systems (“Underlying Software”). For the purposes of the Terms of Service, the Add-on shall be treated like an Agent, subject to the separate terms herein. 2. USE OF THE PROGRAMMABILITY ADD-ON 2.1 Software Evaluation License. Subject to the terms herein, New Relic grants to Customer a limited, non-exclusive, non-transferrable, non-sublicensable right to install, use, and configure the Add-on solely as needed to enable the Services for internal evaluation purposes. New Relic reserves all rights and licenses not expressly granted herein. 2.2 Ownership. As between the Parties, New Relic owns all right, title, and interest to the Add-on and Feedback, including but not limited to any intellectual property and proprietary rights therein. Customer retains all right, title, and interest in any Customer Data processed by the Add-on in connection with the Services. 2.3 Feedback. Customer agrees to provide feedback, suggestions, ideas, requests or recommendations (“Feedback”) regarding the Add-on, and hereby irrevocably assigns all intellectual property and proprietary rights it holds in the Feedback to New Relic. 2.4 Notice. Customer acknowledges and agrees that the Add-on is made to work with the Underlying Software as configured at the time of creation and there is no guarantee that Add-on will continue to work in the event Customer changes, replaces, upgrades versions of, updates, or otherwise changes the Underlying Software (an “Upgrade Event”). Customer shall hold New Relic harmless from any claims or damages arising from Customer’s Upgrade Event. 2.5 Restrictions. Customer will not: (i) use the Add-on except as permitted hereunder; (ii) distribute, sell, sublicense, or otherwise transfer the Add-on; (iii) decompile, disassemble or reverse engineer any software underlying the Add-on; (iv) use the Add-on to damage, disable, overburden or impair any New Relic server or network(s) connected to any New Relic server or interfere with any other party’s use and enjoyment of the Services; (v) use the Add-on to defraud, defame, abuse, harass, stalk, threaten or infringe the rights of privacy or other intellectual property rights of others or otherwise violate any applicable law; (vi) circumvent or modify any security technologies included as part of the Add-on; or (vii) attempt or permit any third party to do any of the foregoing. New Relic may suspend Customer usage of the Services, without notice, pending any investigation of misuse. These restrictions may be supplemented or superceded (to the extent they conflict) by the New Relic Acceptable Use Policy as may be published and updated from time to time on the New Relic web site at docs.newrelic.com/docs/licenses/license-information/acceptable-use-policy/acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential Information” means: (a) the Programmability Add-on, and any features, results or output produced by, and other information relating to the Add-on (including, without limitation, all Feedback); and (b) any business or technical information of New Relic including but not limited to any technical information, research, development, know-how that a reasonable person would understand to be confidential. 3.2 Restrictions. Customer will not use or disclose any Confidential Information, except as necessary for the performance of this Agreement. Customer will use all reasonable efforts to protect Confidential Information from unauthorized use or disclosure, but in no event less than the efforts that it ordinarily uses with respect to its own proprietary information. Customer may disclose Confidential Information to those of its employees who have a bona fide need to know such Confidential Information for the performance of this Agreement; provided that each such employee first executes a written agreement that contains use and nondisclosure restrictions at least as protective as those set forth herein. Confidential Information shall not include any information that: (a) is or becomes generally known to the public through no fault or breach of this Agreement by Customer; (b) is rightfully known by Customer at the time of disclosure without an obligation of confidentiality; (c) is independently developed by Customer without access or use of any Confidential Information; or (d) is rightfully obtained from a third party without restriction on use or disclosure. 4. DISCLAIMER 4.1 DISCLAIMER. THE ADD-ON IS PROVIDED AS-IS AND AS-AVAILABLE AND NEW RELIC DISCLAIMS AND MAKES NO WARRANTIES, EITHER EXPRESS OR IMPLIED, INCLUDING ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR NON-INFRINGEMENT AND WHETHER OR NOT ARISING THROUGH A COURSE OF DEALING. THE ADD-ON IS NOT GUARANTEED TO BE ERROR-FREE, COMPATIBLE WITH THE UNDERLYING SOFTWARE, OR THAT CUSTOMER WILL ACHIEVE ANY RESULTS FROM USE OF THE ADD-ON THEREFROM. 4.2 LIMITATION OF LIABILITY. TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC, ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE TO CUSTOMER OR ANY THIRD PARTY FOR DAMAGES OF ANY KIND, INCLUDING, WITHOUT LIMITATION, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE INSTALLATION, USE OR INABILITY TO USE THE ADD-ON OR FOR ANY ERROR OR DEFECT IN THE ADD-ON OR THE SERVICES, REGARDLESS OF LEGAL THEORY, WHETHER OR NOT NEW RELIC HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSS OR DAMAGE. NEW RELIC’S AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS AGREEMENT WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE PARTIES HAVE AGREED THAT THESE LIMITATIONS WILL SURVIVE AND APPLY EVEN IF ANY LIMITED REMEDY SPECIFIED IN THIS AGREEMENT IS FOUND TO HAVE FAILED OF ITS ESSENTIAL PURPOSE. THE LIMITATIONS OF DAMAGES SET FORTH ABOVE ARE FUNDAMENTAL ELEMENTS OF THE BASIS OF THE BARGAIN BETWEEN NEW RELIC AND CUSTOMER. 4.3 EVALUATION VERSION. Customer agrees and acknowledges that: (a) the Add-on is not an official product and has not been commercially released for sale by New Relic; (b) the Add-on may not operate properly, being in final form, or fully functional; (c) the Add-on may contain errors, security vulnerabilities, design flaws, or other problems; (d) it may not be possible to make the Add-on fully functional; (e) the information obtained using the Add-on may not be accurate; (f) use of the Add-on may result in unexpected results, loss of data, delays or other unpredictable damages or loss; (g) New Relic is under no obligation to release a commercial version of the Add-on; and (h) New Relic has the right unilaterally to abandon development of the Add-on, at any time and without any obligation or liability to Customer. 5. GENERAL PROVISIONS 5.1 Terms of Service; Documentation. This Agreement shall be considered a part of the New Relic documentation, located at: https://docs.newrelic.com (the “Documentation). This Agreement and the Terms of Service constitute the entire and exclusive agreement between New Relic and Customer with respect to the Add-on. In the absence of a separate agreement, upon agreement between New Relic and Customer with respect to the Add-On, the terms and conditions of this Agreement shall govern the relationship between New Relic and Customer with respect to such Add-On. To the extent of a conflict between the Agreement and the Terms of Service, this Agreement shall govern with respect to the Add-on only.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 172.01904,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Add-on end user <em>license</em> agreement",
        "sections": "Add-on end user <em>license</em> agreement",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": "&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;acceptable-use-policy&#x2F;acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential <em>Information</em>” means: (a) the Programmability Add-on, and any features, results or output produced by, and other <em>information</em> relating to the Add-on (including, without limitation"
      },
      "id": "603ec23328ccbccf1beba79a"
    },
    {
      "sections": [
        "New Relic Agent Software Notice"
      ],
      "title": "New Relic Agent Software Notice",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "2bf9501c2767105130d3808f1bf3a91a032d903e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/new-relic-agent-software-notice/",
      "published_at": "2021-10-08T06:50:31Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This software is © 2008-2021 New Relic, Inc. and its licensors. This software is solely for use with New Relic’s proprietary SaaS service (“New Relic Service”), so to use the software you must have a valid account for the New Relic Service under a separate agreement with New Relic (“Subscription Agreement”). You may only use the software to support your use of the New Relic Service as permitted in the Subscription Agreement. Without a Subscription Agreement, you may not use the software. All other use is prohibited. New Relic and its suppliers retain all right, title and interest (including intellectual property rights) in the software. The Subscription Agreement will control in event of a conflict with this notice. Unless otherwise agreed by New Relic in your Subscription Agreement: You may not use, copy, distribute or sublicense the software, use the software on behalf of third parties, reverse engineer or decompile the software, modify or create derivative works of the software, use the software for competitive analysis or benchmarking, or remove or obscure any proprietary notices in the software. The software is provided “AS IS” and New Relic disclaims all warranties, whether express, implied, statutory or otherwise, including warranties of merchantability, fitness for a particular purpose, title or noninfringement. To the full extent permitted by law, New Relic will have no liability arising from or related to the software or under this notice for any direct, indirect, special, incidental, or consequential damages of any kind, even if advised of their possibility in advance, and regardless of legal theory (whether contract, tort, negligence, strict liability or otherwise). The software may contain third-party open source software (“OSS”) as described here and at https://github.com/newrelic. To the extent required by the OSS license, that license will apply to the OSS when used on a stand-alone basis. For avoidance of doubt, you may copy and distribute New Relic agents pursuant to your Subscription Agreement for New Relic Browser and New Relic Mobile and as set forth: https://docs.newrelic.com/docs/mobile-monitoring/new-relic-mobile/get-started and https://docs.newrelic.com/docs/browser/new-relic-browser/installation/install-new-relic-browser-agent Software versions New Relic makes available under an OSS license (such as Apache 2.0) are governed by the terms of the applicable OSS license. For a current list of New Relic software versions released as OSS please visit https://opensource.newrelic.com/.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 172.01585,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": " and at https:&#x2F;&#x2F;github.com&#x2F;newrelic. To the extent required by the OSS <em>license</em>, that <em>license</em> will apply to the OSS when used on a stand-alone basis. For avoidance of doubt, you may copy and distribute New Relic agents pursuant to your Subscription Agreement for New Relic Browser and New Relic Mobile"
      },
      "id": "603eb73828ccbc1f99eba74a"
    }
  ],
  "/docs/licenses/license-information/distributed-licenses/new-relic-agent-software-notice": [
    {
      "sections": [
        "Services licenses",
        "Contents",
        "Java internal services",
        "Java crash data API",
        "CoffeeScript",
        "JavaScript",
        "Crash reporting",
        "Label services",
        "Infrastructure Monitoring services",
        ".NET support uploader service",
        "Ruby gems",
        "Storage services",
        "Go packages"
      ],
      "title": "Services licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "49a2ad450d31dacfc2aae690ca947d2326d18761",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/other-licenses/services-licenses/",
      "published_at": "2021-10-08T10:17:30Z",
      "updated_at": "2021-04-06T00:26:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Contents Java internal services Java crash data API Framework License Apache TomCat Apache 2.0 Bean Validation Apache 2.0 Cassandra Driver Core Apache 2.0 ClassMate Apache 2.0 Hamcrest BSD Mockito MIT Netty Apache 2.0 Objenesis MIT SnakeYAML Apache 2.0 Spring Boot Apache 2.0 Java internal services Library License json_simple Apache 2.0 newrelic-api New Relic newrelic-api New Relic antlr BSD aopalliance Public Domain asm-analysis BSD asm-commons BSD asm-tree BSD asm-util BSD asm BSD c3p0 EPLv1.0 cglib-nodep Apache 2.0 cglib Apache 2.0 cglib Apache 2.0 cal10n-api MIT logback-classic EPLv1.0 logback-core EPLv1.0 guava-jetty-service Apache 2.0 zkclient Apache 2.0 reporter-config-base Apache 2.0 reporter-config3 Apache 2.0 reporter-config Apache 2.0 aws-java-sdk-acm Apache 2.0 aws-java-sdk-api-gateway Apache 2.0 aws-java-sdk-applicationautoscaling Apache 2.0 aws-java-sdk-autoscaling Apache 2.0 aws-java-sdk-cloudformation Apache 2.0 aws-java-sdk-cloudfront Apache 2.0 aws-java-sdk-cloudhsm Apache 2.0 aws-java-sdk-cloudsearch Apache 2.0 aws-java-sdk-cloudtrail Apache 2.0 aws-java-sdk-cloudwatch Apache 2.0 aws-java-sdk-cloudwatchmetrics Apache 2.0 aws-java-sdk-codecommit Apache 2.0 aws-java-sdk-codedeploy Apache 2.0 aws-java-sdk-codepipeline Apache 2.0 aws-java-sdk-cognitoidentity Apache 2.0 aws-java-sdk-cognitoidp Apache 2.0 aws-java-sdk-cognitosync Apache 2.0 aws-java-sdk-config Apache 2.0 aws-java-sdk-core Apache 2.0 aws-java-sdk-datapipeline Apache 2.0 aws-java-sdk-devicefarm Apache 2.0 aws-java-sdk-directconnect Apache 2.0 aws-java-sdk-directory Apache 2.0 aws-java-sdk-discovery Apache 2.0 aws-java-sdk-dms Apache 2.0 aws-java-sdk-dynamodb Apache 2.0 aws-java-sdk-ec2 Apache 2.0 aws-java-sdk-ecr Apache 2.0 aws-java-sdk-ecs Apache 2.0 aws-java-sdk-efs Apache 2.0 aws-java-sdk-elasticache Apache 2.0 aws-java-sdk-elasticbeanstalk Apache 2.0 aws-java-sdk-elasticloadbalancing Apache 2.0 aws-java-sdk-elasticloadbalancingv2 Apache 2.0 aws-java-sdk-elasticsearch Apache 2.0 aws-java-sdk-elastictranscoder Apache 2.0 aws-java-sdk-emr Apache 2.0 aws-java-sdk-events Apache 2.0 aws-java-sdk-gamelift Apache 2.0 aws-java-sdk-glacier Apache 2.0 aws-java-sdk-iam Apache 2.0 aws-java-sdk-importexport Apache 2.0 aws-java-sdk-inspector Apache 2.0 aws-java-sdk-iot Apache 2.0 aws-java-sdk-kinesis Apache 2.0 aws-java-sdk-kms Apache 2.0 aws-java-sdk-lambda Apache 2.0 aws-java-sdk-logs Apache 2.0 aws-java-sdk-machinelearning Apache 2.0 aws-java-sdk-marketplacecommerceanalytics Apache 2.0 aws-java-sdk-marketplacemeteringservice Apache 2.0 aws-java-sdk-models Apache 2.0 aws-java-sdk-opsworks Apache 2.0 aws-java-sdk-rds Apache 2.0 aws-java-sdk-redshift Apache 2.0 aws-java-sdk-route53 Apache 2.0 aws-java-sdk-s3 Apache 2.0 aws-java-sdk-servicecatalog Apache 2.0 aws-java-sdk-ses Apache 2.0 aws-java-sdk-simpledb Apache 2.0 aws-java-sdk-simpleworkflow Apache 2.0 aws-java-sdk-snowball Apache 2.0 aws-java-sdk-sns Apache 2.0 aws-java-sdk-sqs Apache 2.0 aws-java-sdk-ssm Apache 2.0 aws-java-sdk-storagegateway Apache 2.0 aws-java-sdk-sts Apache 2.0 aws-java-sdk-support Apache 2.0 aws-java-sdk-swf-libraries Apache 2.0 aws-java-sdk-waf Apache 2.0 aws-java-sdk-workspaces Apache 2.0 aws-java-sdk Apache 2.0 jmespath-java Apache 2.0 AppleJavaExtensions BSD jcommander Apache 2.0 high-scale-lib MIT hppc Apache 2.0 clover Paid stream Apache 2.0 metrics-core Apache 2.0 cassandra-driver-core Apache 2.0 cassandra-driver-core Apache 2.0 uuid MIT grabbag MIT speed4j Apache 2.0 yamlbeans BSD jackson-annotations Apache 2.0 jackson-core Apache 2.0 jackson-databind Apache 2.0 jackson-dataformat-cbor Apache 2.0 jackson-dataformat-csv Apache 2.0 jackson-dataformat-xml Apache 2.0 jackson-dataformat-yaml Apache 2.0 jackson-datatype-guava Apache 2.0 jackson-datatype-jdk7 Apache 2.0 jackson-datatype-jdk8 Apache 2.0 jackson-datatype-joda Apache 2.0 jackson-datatype-jsr310 Apache 2.0 jackson-jaxrs-base Apache 2.0 jackson-jaxrs-json-provider Apache 2.0 jackson-module-afterburner Apache 2.0 jackson-module-jaxb-annotations Apache 2.0 jackson-module-kotlin Apache 2.0 woodstox-core Apache 2.0 classmate Apache 2.0 zjsonpatch Apache 2.0 caffeine Apache 2.0 waffle-jna EPLv1.0 docker-java Apache 2.0 docker-java Apache 2.0 btf Apache 2.0 jackson-coreutils Apache 2.0 json-schema-core Apache 2.0 json-schema-validator Apache 2.0 msg-simple Apache 2.0 uri-template Apache 2.0 jamm Apache 2.0 jffi Apache 2.0 jnr-constants Apache 2.0 jnr-ffi Apache 2.0 jnr-ffi Apache 2.0 jnr-posix EPLv1.0 jnr-x86asm MIT embedded-redis Apache 2.0 memoryfilesystem MIT guava-retrying Apache 2.0 guava-retrying Apache 2.0 snowball-stemmer BSD system-rules CPL uuid Apache 2.0 wiremock Apache 2.0 auto-common Apache 2.0 annotations LGPLv2.1 annotations LGPLv2.1 bcel-findbugs LGPLv2.1 findbugs LGPLv2.1 jFormatString BSD jFormatString BSD jsr305 BSD jsr305 BSD jsr305 BSD gson Apache 2.0 spymemcached MIT guava-testlib Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guice-multibindings Apache 2.0 guice Apache 2.0 truth Apache 2.0 concurrent-trees Apache 2.0 concurrentlinkedhashmap-lru Apache 2.0 json-simple Apache 2.0 libphonenumber Apache 2.0 locality-uuid BSD h2 EPLv1.0 annotations Apache 2.0 json-path-assert Apache 2.0 json-path Apache 2.0 json-path Apache 2.0 jzlib BSD bonecp-provider Apache 2.0 bonecp Apache 2.0 junixsocket-common Apache 2.0 junixsocket-native-common Apache 2.0 disruptor Apache 2.0 geoip-api LGPLv2.1 c3p0 LGPLv2.1 mchange-commons-java LGPLv2.1 archaius-core Apache 2.0 hystrix-core Apache 2.0 hystrix-servo-metrics-publisher Apache 2.0 rxjava-core Apache 2.0 servo-core Apache 2.0 kafka-clients Apache 2.0 kafka_2.10 Apache 2.0 kafka_2.11 Apache 2.0 common-cassandra Apache 2.0 timeslice_utils New Relic mockito-kotlin MIT compress-lzf Apache 2.0 checkstyle Apache 2.0 dagger-compiler Apache 2.0 dagger Apache 2.0 okhttp Apache 2.0 okio Apache 2.0 javapoet Apache 2.0 javawriter Apache 2.0 jaxb-impl CDDLv1 thrift-server Apache 2.0 finagle-core_2.10 Apache 2.0 finagle-core_2.11 Apache 2.0 finagle-http_2.10 Apache 2.0 finagle-http_2.11 Apache 2.0 jsr166e Creative Commons util-app_2.10 Apache 2.0 util-app_2.11 Apache 2.0 util-cache_2.10 Apache 2.0 util-cache_2.11 Apache 2.0 util-codec_2.10 Apache 2.0 util-codec_2.11 Apache 2.0 util-collection_2.10 Apache 2.0 util-collection_2.11 Apache 2.0 util-core_2.10 Apache 2.0 util-core_2.11 Apache 2.0 util-function_2.10 Apache 2.0 util-function_2.11 Apache 2.0 util-hashing_2.10 Apache 2.0 util-hashing_2.11 Apache 2.0 util-jvm_2.10 Apache 2.0 util-jvm_2.11 Apache 2.0 util-lint_2.10 Apache 2.0 util-lint_2.11 Apache 2.0 util-logging_2.10 Apache 2.0 util-logging_2.11 Apache 2.0 util-registry_2.10 Apache 2.0 util-registry_2.11 Apache 2.0 util-stats_2.10 Apache 2.0 util-stats_2.11 Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 HikariCP Apache 2.0 commons-beanutils Apache 2.0 commons-cli Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-io Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 commons-pool Apache 2.0 unix-socket-factory Apache 2.0 unix-socket-factory Apache 2.0 jflex BSD dom4j BSD airline Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 metrics-healthchecks Apache 2.0 metrics-httpclient Apache 2.0 metrics-jdbi Apache 2.0 metrics-jersey2 Apache 2.0 metrics-jetty9 Apache 2.0 metrics-json Apache 2.0 metrics-jvm Apache 2.0 metrics-logback Apache 2.0 metrics-servlets Apache 2.0 dropwizard-java8-jdbi Apache 2.0 dropwizard-client Apache 2.0 dropwizard-configuration Apache 2.0 dropwizard-core Apache 2.0 dropwizard-db Apache 2.0 dropwizard-jackson Apache 2.0 dropwizard-jdbi Apache 2.0 dropwizard-jersey Apache 2.0 dropwizard-jetty Apache 2.0 dropwizard-lifecycle Apache 2.0 dropwizard-logging Apache 2.0 dropwizard-metrics Apache 2.0 dropwizard-servlets Apache 2.0 dropwizard-testing Apache 2.0 dropwizard-util Apache 2.0 dropwizard-validation Apache 2.0 netty-all Apache 2.0 netty-buffer Apache 2.0 netty-codec-http Apache 2.0 netty-codec-socks Apache 2.0 netty-codec Apache 2.0 netty-common Apache 2.0 netty-handler-proxy Apache 2.0 netty-handler Apache 2.0 netty-resolver Apache 2.0 netty-transport-native-epoll Apache 2.0 netty-transport Apache 2.0 netty Apache 2.0 ratpack-core Apache 2.0 ratpack-groovy-test Apache 2.0 ratpack-groovy Apache 2.0 ratpack-guice Apache 2.0 ratpack-test Apache 2.0 rxjava Apache 2.0 fastutil Apache 2.0 janino BSD activation Apache 2.0 javax.annotation-api CDDLv1 jsr250-api CDDLv1 javax.inject Apache 2.0 mail CDDLv1 mailapi CDDLv1 javax.servlet-api Apache 2.0 servlet-api Apache 2.0 javax.transaction-api CDDLv1 validation-api Apache 2.0 javax.websocket-api CDDLv1 javax.ws.rs-api CDDLv1 jaxb-api CDDLv1 stax-api CDDLv1 jaxen Apache 2.0 jline BSD jline BSD joda-time Apache 2.0 junit-dep CPL junit CPL junit EPLv1.0 kafka_2.10 Apache 2.0 kafka_2.9.2 Apache 2.0 apache-log4j-extras Apache 2.0 log4j Apache 2.0 mysql-connector-java GPLv2 with Classpath Exception byte-buddy Apache 2.0 byte-buddy Apache 2.0 jna-platform LGPLv2.1 jna Apache 2.0 jcip-annotations Apache 2.0 lz4 Apache 2.0 accessors-smart Apache 2.0 asm Apache 2.0 json-smart Apache 2.0 json-smart Apache 2.0 primitive GPLv2 with Classpath Exception compiler Apache 2.0 lang Apache 2.0 beaver-cc BSD ehcache-core Apache 2.0 ehcache Apache 2.0 jopt-simple MIT jopt-simple MIT opencsv Apache 2.0 quality-check Apache 2.0 scannotation Apache 2.0 super-csv Apache 2.0 uadetector-core Apache 2.0 uadetector-resources Apache 2.0 argparse4j MIT spymemcached MIT kalium Apache 2.0 ST4 BSD antlr-runtime BSD antlr-runtime BSD antlr BSD stringtemplate BSD ant-launcher Apache 2.0 ant Apache 2.0 cassandra-all Apache 2.0 cassandra-all Apache 2.0 cassandra-thrift Apache 2.0 cassandra-thrift Apache 2.0 commons-collections4 Apache 2.0 commons-compress Apache 2.0 commons-csv Apache 2.0 commons-dbcp2 Apache 2.0 commons-io Apache 2.0 commons-lang3 Apache 2.0 commons-math3 Apache 2.0 commons-math Apache 2.0 commons-pool2 Apache 2.0 curator-client Apache 2.0 curator-framework Apache 2.0 curator-recipes Apache 2.0 curator-test Apache 2.0 derby Apache 2.0 geronimo-servlet_3.0_spec Apache 2.0 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 ivy Apache 2.0 kafka-clients Apache 2.0 kafka_2.11 Apache 2.0 libthrift Apache 2.0 tomcat-embed-core Apache 2.0 tomcat-embed-el Apache 2.0 tomcat-embed-logging-juli Apache 2.0 tomcat-embed-websocket Apache 2.0 tomcat-jdbc Apache 2.0 tomcat-juli Apache 2.0 zookeeper Apache 2.0 aspectjweaver EPLv1.0 assertj-core Apache 2.0 assertj-core Apache 2.0 evo-inflector Apache 2.0 bcmail-jdk15on MIT bcpkix-jdk15on MIT bcprov-jdk15on MIT ohc-core Apache 2.0 cassandra-unit-spring LGPLv3 cassandra-unit-spring LGPLv3 cassandra-unit LGPLv3 cassandra-unit LGPLv3 groovy-all Apache 2.0 groovy-all Apache 2.0 groovy Apache 2.0 groovy Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 easymock Apache 2.0 ecj EPLv1.0 javax.activation CDDLv1 javax.annotation CDDLv1 javax.mail.glassfish CDDLv1 javax.servlet CDDLv1 javax.transaction CDDLv1 org.objectweb.asm BSD jetty-setuid-java EPLv1.0 jetty-schemas Apache 2.0 javax-websocket-client-impl Apache 2.0 javax-websocket-server-impl Apache 2.0 websocket-api Apache 2.0 websocket-client Apache 2.0 websocket-common Apache 2.0 websocket-server Apache 2.0 websocket-servlet Apache 2.0 apache-jsp Apache 2.0 jetty-annotations CDDLv1 jetty-client Apache 2.0 jetty-continuation Apache 2.0 jetty-http Apache 2.0 jetty-io Apache 2.0 jetty-jmx Apache 2.0 jetty-jndi Apache 2.0 jetty-plus Apache 2.0 jetty-proxy Apache 2.0 jetty-security Apache 2.0 jetty-server Apache 2.0 jetty-servlet Apache 2.0 jetty-servlets Apache 2.0 jetty-util Apache 2.0 jetty-webapp Apache 2.0 jetty-xml Apache 2.0 sigar Apache 2.0 aopalliance-repackaged CDDLv1 javax.inject CDDLv1 hk2-api CDDLv1 hk2-locator CDDLv1 hk2-utils CDDLv1 osgi-resource-locator CDDLv1 jersey-guava CDDLv1 jersey-client CDDLv1 jersey-apache-connector CDDLv1 jersey-container-servlet-core CDDLv1 jersey-container-servlet CDDLv1 jersey-client CDDLv1 jersey-common CDDLv1 jersey-server CDDLv1 jersey-bean-validation CDDLv1 jersey-metainf-services CDDLv1 jersey-media-jaxb CDDLv1 jersey-client CDDLv1 jersey-test-framework-provider-inmemory CDDLv1 jersey-test-framework-core CDDLv1 javax.el CDDLv1 hamcrest-all BSD hamcrest-core BSD hamcrest-integration BSD hamcrest-library BSD HdrHistogram Public Domain hector-core MIT hibernate-commons-annotations LGPLv2.1 hibernate-jpa-2.0-api LGPLv2.1 hibernate-jpa-2.1-api LGPLv2.1 hibernate-c3p0 Apache 2.0 hibernate-core Apache 2.0 hibernate-ehcache Apache 2.0 hibernate-entitymanager Apache 2.0 hibernate-jmx Apache 2.0 hibernate-validator Apache 2.0 freebuilder Apache 2.0 org.jacoco.agent EPLv1.0 org.jacoco.ant EPLv1.0 org.jacoco.core EPLv1.0 org.jacoco.report EPLv1.0 javassist Apache 2.0 jboss-logging-annotations LGPLv2.1 jboss-logging Apache 2.0 jboss-transaction-api_1.1_spec LGPLv2.1 jboss-transaction-api_1.2_spec LGPLv2.1 jandex Apache 2.0 jdbi Apache 2.0 job-dsl-core Apache 2.0 version-number MIT kotlin-annotation-processing Apache 2.0 kotlin-reflect Apache 2.0 kotlin-runtime Apache 2.0 kotlin-stdlib Apache 2.0 kotlin-test-junit Apache 2.0 kotlin-test Apache 2.0 annotations Apache 2.0 jolokia-core Apache 2.0 jruby-complete LGPLv2.1 LGPLv2.1 json Public Domain json Apache 2.0 xstream BSD annotations Apache 2.0 mapdb Apache 2.0 mapdb Apache 2.0 jbcrypt ISC mockserver-client-java Apache 2.0 mockserver-core Apache 2.0 mockserver-logging Apache 2.0 mockserver-netty Apache 2.0 mockito-all MIT mockito-core MIT mockito-core MIT apache-el Apache 2.0 apache-jsp Apache 2.0 alpn-boot Apache 2.0 etcd4j Apache 2.0 msgpack Apache 2.0 objenesis Apache 2.0 objenesis Apache 2.0 jmh-core GPLv2 with Classpath Exception jmh-generator-annprocess GPLv2 with Classpath Exception jol-core GPLv2 with Classpath Exception asm-analysis BSD asm-analysis BSD asm-commons BSD asm-debug-all BSD asm-tree BSD asm-util BSD asm-util BSD asm BSD asm Public Domain postgresql BSD powermock-api-mockito Apache 2.0 powermock-api-support Apache 2.0 powermock-core Apache 2.0 powermock-module-junit4-common Apache 2.0 powermock-module-junit4 Apache 2.0 powermock-reflect Apache 2.0 lombok MIT reactive-streams Public Domain reflections WTFPL scala-java8-compat_2.11 Scala scala-parser-combinators_2.11 Scala scala-xml_2.11 Scala scala-library Scala scala-reflect Scala scalatest_2.10 Apache 2.0 scalatest_2.11 Apache 2.0 native-lib-loader BSD jsonassert Apache 2.0 jcl-over-slf4j MIT jul-to-slf4j MIT log4j-log4j12 MIT log4j-over-slf4j MIT slf4j-api MIT slf4j-ext MIT slf4j-log4j12 MIT slf4j-simple MIT spock-core Apache 2.0 spock-core Apache 2.0 spring-boot-actuator Apache 2.0 spring-boot-autoconfigure Apache 2.0 spring-boot-configuration-processor Apache 2.0 spring-boot-starter-actuator Apache 2.0 spring-boot-starter-aop Apache 2.0 spring-boot-starter-data-jpa Apache 2.0 spring-boot-starter-data-rest Apache 2.0 spring-boot-starter-jdbc Apache 2.0 spring-boot-starter-jetty Apache 2.0 spring-boot-starter-logging Apache 2.0 spring-boot-starter-test Apache 2.0 spring-boot-starter-tomcat Apache 2.0 spring-boot-starter-web Apache 2.0 spring-boot-starter Apache 2.0 spring-boot-test-autoconfigure Apache 2.0 spring-boot-test Apache 2.0 spring-boot Apache 2.0 spring-data-commons Apache 2.0 spring-data-jpa Apache 2.0 spring-data-rest-core Apache 2.0 spring-data-rest-webmvc Apache 2.0 spring-hateoas Apache 2.0 spring-plugin-core Apache 2.0 spring-aop Apache 2.0 spring-aspects Apache 2.0 spring-beans Apache 2.0 spring-context-support Apache 2.0 spring-context Apache 2.0 spring-core Apache 2.0 spring-core Apache 2.0 spring-expression Apache 2.0 spring-jdbc Apache 2.0 spring-orm Apache 2.0 spring-test Apache 2.0 spring-test Apache 2.0 spring-tx Apache 2.0 spring-web Apache 2.0 spring-webmvc Apache 2.0 xz Public Domain wasabi Apache 2.0 snappy-java Apache 2.0 xmlunit-core Apache 2.0 xmlunit-legacy Apache 2.0 snakeyaml Apache 2.0 JUnitParams Apache 2.0 postgresql BSD postgresql BSD jedis MIT scala-library Scala ion-java Apache 2.0 timeslice_service-thrift New Relic timeslice_service New Relic xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3_min Public Domain zkclient Apache 2.0 zookeeper Apache 2.0 CoffeeScript Library License CoffeeScript MIT coffin MIT JavaScript JavaScript License ie_html5/ie_html5.js MIT Angular.js MIT Angular UI directives for Bootstrap MIT Chart.js MIT Chosen JQuery plugin MIT Fullcalendar MIT Moment MIT Crash reporting Software License Apple OS X OS X Yosemite Xcode & Apple SDK Xcode license Label services Service License BoneCP Apache 2.0 Gradle Apache 2.0 Hibernate Apache 2.0 Spring Apache 2.0 Infrastructure Monitoring services Service License antlr BSD aopalliance Public Domain asm MIT cglib Apache 2.0 ch.qos.logback EPLv1.0 com.101tec Apache 2.0 com.amazonaws Apache 2.0 com.cenqua.clover Paid com.fasterxml.jackson.core Apache 2.0 com.fasterxml Apache 2.0 com.google.code.findbugs BSD com.google.guava Apache 2.0 com.googlecode.json-simple Apache 2.0 com.intellij Apache 2.0 New Relic com.squareup.dagger Apache 2.0 com.squareup Apache 2.0 com.yammer.metrics Apache 2.0 commons-codec Apache 2.0 commons-configuration Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 dom4j BSD io.netty Apache 2.0 javax.annotation CDDLv1 javax.inject Apache 2.0 javax.servlet.jsp CDDLv1 javax.servlet Apache 2.0 javax.validation Apache 2.0 javax.websocket CDDLv1 jline BSD joda-time Apache 2.0 junit CPL log4j Apache 2.0 mysql GPLv2 with Classpath Exception net.sf.jopt-simple MIT org.apache.commons Apache 2.0 org.apache.httpcomponents Apache 2.0 org.apache.ivy Apache 2.0 org.apache.thrift Apache 2.0 org.apache.tomcat.embed Apache 2.0 org.apache.zookeeper Apache 2.0 org.codehaus.groovy Apache 2.0 org.codehaus.jackson Apache 2.0 org.easymock Apache 2.0 org.eclipse.jetty.orbit CDDLv1 org.eclipse.jetty.toolchain Apache 2.0 org.eclipse.jetty.websocket Apache 2.0 org.eclipse.jetty CDDLv1 org.glassfish.jersey.core CDDLv1 org.glassfish.web CDDLv1 org.glassfish CDDLv1 org.hamcrest BSD org.hibernate Apache 2.0 org.javassist Apache 2.0 org.jboss.logging LGPLv2.1 org.jenkins-ci.plugins Apache 2.0 org.jenkins-ci Apache 2.0 org.jetbrains Apache 2.0 org.jolokia Apache 2.0 org.jvnet.hudson BSD org.mockito MIT org.objenesis Apache 2.0 org.ow2.asm MIT org.reflections WTFPL org.scala-lang Scala org.slf4j MIT org.springframework.boot Apache 2.0 org.springframework Apache 2.0 org.xerial.snappy Apache 2.0 org.yaml Apache 2.0 xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3 Public Domain .NET support uploader service Library License Amazon AWS SDK for Java Apache 2.0 Apache Commons Codec Apache 2.0 Gson Apache 2.0 Jetty Apache 2.0 Ruby gems Ruby gems Gem License aasm MIT actionmailer MIT actionpack MIT activeadmin MIT actionview MIT activejob MIT active_model_serializers MIT activemodel MIT activerecord MIT activerecord-deprecated_finders MIT activerecord-mysql2-adapter MIT activeresource MIT activesupport MIT addressable Apache 2.0 angularjs-rails MIT arbre MIT arel MIT arel MIT atomic Apache 2.0 atomic Apache-2.0 attr_required MIT awesome_print MIT backports MIT better_errors MIT binding_of_caller MIT bourbon MIT brwsr MIT browser-timezone-rails MIT builder MIT bundler MIT byebug MIT callsite MIT capistrano MIT capistrano-bundler MIT capistrano-ext MIT capistrano-rbenv MIT capistrano-stats MIT capybara MIT celluloid MIT CFPropertyList MIT chunky_png MIT ci_reporter MIT ci_reporter_rspec MIT clockwork MIT codeclimate-test-reporter MIT coderay MIT coderay MIT coffee-rails MIT coffee-script MIT coffee-script-source MIT columnize Ruby compass MIT compass-core MIT compass-import-once MIT compass-rails MIT concurrent-ruby MIT crack MIT crypt Unlicensed daemons MIT dalli MIT database_cleaner MIT debug_inspector MIT debugger-linecache MIT diff-lcs MIT docile MIT dotenv MIT dotenv-deployment MIT erubis MIT etcd MIT ethon MIT eventmachine Ruby excon MIT execjs MIT factory_girl MIT factory_girl_rails MIT faraday MIT faraday-middleware MIT faye-websocket MIT ffi MIT ffi BSD fission MIT fog MIT fog-atmos MIT fog-aws MIT fog-brightbox MIT fog-core MIT fog-ecloud MIT fog-google MIT fog-json MIT fog-local MIT fog-powerdns MIT fog-profitbricks MIT fog-radosgw MIT fog-riakcs MIT fog-sakuracloud MIT fog-serverlove MIT fog-softlayer MIT fog-storm_on_demand MIT fog-terremark MIT fog-vmfusion MIT fog-voxel MIT fog-xml MIT font-awesome-rails MIT foreman MIT formatador MIT formtastic MIT formtastic i18n MIT fssm MIT globalid MIT guard MIT guard-rspec MIT haml MIT haml-rails MIT has scope MIT hashie MIT highline Ruby hike MIT hipchat-api MIT histogram MIT hitimes ISC hiredis BSD http_parser.rb MIT httpclient Ruby httpclient Ruby httparty MIT i18n MIT inflecto MIT inherited resources MIT intercom-rails MIT ipaddress MIT jasmine MIT jasmine-core MIT jasmine-rails MIT jbuilder MIT jira MIT jira-ruby OSL-3.0 journey MIT jquery-rails MIT jquery-turbolinks MIT jquery-ui-rails MIT json Ruby jwt MIT kaminari MIT kgio LGPLv3 kgio LGPLv3 kgio LGPL-v2.1+ libv8 MIT listen MIT logger Ruby loofah MIT lumberjack MIT macaddr MIT mail MIT mailcatcher MIT memoist MIT metaclass MIT meta_request MIT method_source MIT mime-types MIT mini_portile MIT minitest MIT minitest-rails MIT mixlib-log Apache 2.0 mocha MIT mono_logger MIT multi_json MIT multi_xml MIT multipart-post MIT mysql2 MIT net-http-persistent MIT net-scp MIT net-sftp MIT net-ssh MIT net-ssh-gateway MIT netrc MIT nokogiri MIT oauth2 MIT oj MIT omniauth MIT omniauth-oauth2 MIT pagerduty MIT papers MIT pg BSD pg Ruby phantomjs BSD phantomjs-binaries Unlicensed poltergeist MIT polyamorous MIT polyglot MIT polyglot MIT protected_attributes MIT pry MIT pry-nav MIT pry-rails MIT pry-stack_explorer MIT puma BSD quiet assets MIT rack MIT rack-cache MIT rack-contrib MIT rack-oauth2 MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-ssl MIT rack-ssl-enforcer MIT rack-test MIT rails MIT rails-deprecated_sanitizer MIT rails-dom-testing MIT rails-html-sanitizer MIT rails-observers MIT rails_12factor MIT rails_serve_static_assets MIT rails_stdout_logging MIT railties MIT raindrops LGPLv3 raindrops LGPLv3 raindrops LGPLv2.1+ rake MIT rake MIT rake MIT rake MIT ransack MIT rb-fsevent MIT rb-inotify MIT rdoc Ruby redcarpet MIT redis MIT redis-namespace MIT redis-queue MIT ref MIT remote_syslog_logger MIT request_store MIT responders MIT resque MIT resque-cleaner MIT resque-pool MIT resque-scheduler MIT resque-status MIT rest-client MIT restforce MIT rspec MIT rspec-core MIT rspec-expectations MIT rspec-mocks MIT rspec-rails MIT rspec-support MIT rspec_junit_formatter MIT rubyntlm MIT ruby-saml MIT rufus-scheduler MIT safe_yaml MIT salesforce_bulk_query BSD sass MIT sass-rails MIT sass-rails MIT sdoc MIT secure_headers Apache 2.0 sequel MIT serveza MIT settingslogic MIT shoulda MIT shoulda-context MIT shoulda-matchers MIT simplecov MIT simplecov-html MIT simplecov-rcov MIT sinatra MIT sinatra-activerecord MIT sinatra-contrib MIT skinny MIT slop MIT spring MIT sprockets MIT sprockets-rails MIT sqlite3 BSD sshkit GPL-No Distro sys-uname Ruby syslog_protocol MIT systemu BSDL term-ansicolor GPL-No Distro terminal-table MIT therubyracer MIT thin Ruby thor MIT thread_safe Apache-2.0 thrift Apache 2.0 thrift-rack MIT tilt MIT timecop MIT timers MIT tins MIT treetop MIT turbolinks MIT twitter-bootstrap-rails MIT typhoeus MIT tzinfo MIT uglifier MIT unicorn Ruby unicorn-rails MIT uuid MIT uuidtools Apache-2.0 vegas MIT webmock MIT web-console MIT xml-simple Ruby xpath MIT yard MIT Storage services Library License Amazon AWS SDK for Java Apache 2.0 Amazon AWS Command Line Interface Apache 2.0 Go packages Library License Go-Mysql-Driver Mozilla Public License 2.0 gocql BSD 3-Clause groupcache Apache 2.0 Migrate MIT go-sqlite3 MIT Revel MIT Pathtree MIT Ansicolor MIT fsnotify BSD 3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 172.90364,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Services <em>licenses</em>",
        "sections": "Services <em>licenses</em>",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Contents Java internal"
      },
      "id": "603ea24364441f91fb4e8864"
    },
    {
      "sections": [
        "Add-on end user license agreement",
        "New Relic, Inc. Add-on End User License Agreement"
      ],
      "title": "Add-on end user license agreement",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "a01c225ca30f95dab7db856cd946c76de557c31f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/add-end-user-license-agreement/",
      "published_at": "2021-10-08T06:49:32Z",
      "updated_at": "2021-03-16T06:19:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic, Inc. Add-on End User License Agreement In connection with the work provided by New Relic's Expert Services, Field Instrumentation Team, or Sales team, you may be provided with certain custom-created software to visualize, enable, optimize, or enhance your use of New Relic's Services. By downloading, installing, authorizing installation, or using the Add-on (defined below) with your Underlying Software (defined below), you (“Customer”) agree to the terms and conditions herein (“Agreement”) with New Relic, Inc., a Delaware corporation with offices located at 188 Spear Street, Suite 1200 San Francisco, CA 94105 (“New Relic\"), (collectively the “Parties”). Capitalized terms not defined herein shall have the meanings set forth in the New Relic Terms of Service (the “Terms of Service”) available here: newrelic.com/termsandconditions/terms. IF YOU DO NOT AGREE TO THIS AGREEMENT, YOUR SOLE REMEDY IS TO NOT USE THE ADD-ON. 1. DEFINITIONS “Add-on” means the New Relic applications and/or software, including but not limited to connectors, extensions, UI extension, and plugins, provided by New Relic to Customer to enable the Services to operate with certain third party or Customer software or systems (“Underlying Software”). For the purposes of the Terms of Service, the Add-on shall be treated like an Agent, subject to the separate terms herein. 2. USE OF THE PROGRAMMABILITY ADD-ON 2.1 Software Evaluation License. Subject to the terms herein, New Relic grants to Customer a limited, non-exclusive, non-transferrable, non-sublicensable right to install, use, and configure the Add-on solely as needed to enable the Services for internal evaluation purposes. New Relic reserves all rights and licenses not expressly granted herein. 2.2 Ownership. As between the Parties, New Relic owns all right, title, and interest to the Add-on and Feedback, including but not limited to any intellectual property and proprietary rights therein. Customer retains all right, title, and interest in any Customer Data processed by the Add-on in connection with the Services. 2.3 Feedback. Customer agrees to provide feedback, suggestions, ideas, requests or recommendations (“Feedback”) regarding the Add-on, and hereby irrevocably assigns all intellectual property and proprietary rights it holds in the Feedback to New Relic. 2.4 Notice. Customer acknowledges and agrees that the Add-on is made to work with the Underlying Software as configured at the time of creation and there is no guarantee that Add-on will continue to work in the event Customer changes, replaces, upgrades versions of, updates, or otherwise changes the Underlying Software (an “Upgrade Event”). Customer shall hold New Relic harmless from any claims or damages arising from Customer’s Upgrade Event. 2.5 Restrictions. Customer will not: (i) use the Add-on except as permitted hereunder; (ii) distribute, sell, sublicense, or otherwise transfer the Add-on; (iii) decompile, disassemble or reverse engineer any software underlying the Add-on; (iv) use the Add-on to damage, disable, overburden or impair any New Relic server or network(s) connected to any New Relic server or interfere with any other party’s use and enjoyment of the Services; (v) use the Add-on to defraud, defame, abuse, harass, stalk, threaten or infringe the rights of privacy or other intellectual property rights of others or otherwise violate any applicable law; (vi) circumvent or modify any security technologies included as part of the Add-on; or (vii) attempt or permit any third party to do any of the foregoing. New Relic may suspend Customer usage of the Services, without notice, pending any investigation of misuse. These restrictions may be supplemented or superceded (to the extent they conflict) by the New Relic Acceptable Use Policy as may be published and updated from time to time on the New Relic web site at docs.newrelic.com/docs/licenses/license-information/acceptable-use-policy/acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential Information” means: (a) the Programmability Add-on, and any features, results or output produced by, and other information relating to the Add-on (including, without limitation, all Feedback); and (b) any business or technical information of New Relic including but not limited to any technical information, research, development, know-how that a reasonable person would understand to be confidential. 3.2 Restrictions. Customer will not use or disclose any Confidential Information, except as necessary for the performance of this Agreement. Customer will use all reasonable efforts to protect Confidential Information from unauthorized use or disclosure, but in no event less than the efforts that it ordinarily uses with respect to its own proprietary information. Customer may disclose Confidential Information to those of its employees who have a bona fide need to know such Confidential Information for the performance of this Agreement; provided that each such employee first executes a written agreement that contains use and nondisclosure restrictions at least as protective as those set forth herein. Confidential Information shall not include any information that: (a) is or becomes generally known to the public through no fault or breach of this Agreement by Customer; (b) is rightfully known by Customer at the time of disclosure without an obligation of confidentiality; (c) is independently developed by Customer without access or use of any Confidential Information; or (d) is rightfully obtained from a third party without restriction on use or disclosure. 4. DISCLAIMER 4.1 DISCLAIMER. THE ADD-ON IS PROVIDED AS-IS AND AS-AVAILABLE AND NEW RELIC DISCLAIMS AND MAKES NO WARRANTIES, EITHER EXPRESS OR IMPLIED, INCLUDING ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR NON-INFRINGEMENT AND WHETHER OR NOT ARISING THROUGH A COURSE OF DEALING. THE ADD-ON IS NOT GUARANTEED TO BE ERROR-FREE, COMPATIBLE WITH THE UNDERLYING SOFTWARE, OR THAT CUSTOMER WILL ACHIEVE ANY RESULTS FROM USE OF THE ADD-ON THEREFROM. 4.2 LIMITATION OF LIABILITY. TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC, ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE TO CUSTOMER OR ANY THIRD PARTY FOR DAMAGES OF ANY KIND, INCLUDING, WITHOUT LIMITATION, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE INSTALLATION, USE OR INABILITY TO USE THE ADD-ON OR FOR ANY ERROR OR DEFECT IN THE ADD-ON OR THE SERVICES, REGARDLESS OF LEGAL THEORY, WHETHER OR NOT NEW RELIC HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSS OR DAMAGE. NEW RELIC’S AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS AGREEMENT WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE PARTIES HAVE AGREED THAT THESE LIMITATIONS WILL SURVIVE AND APPLY EVEN IF ANY LIMITED REMEDY SPECIFIED IN THIS AGREEMENT IS FOUND TO HAVE FAILED OF ITS ESSENTIAL PURPOSE. THE LIMITATIONS OF DAMAGES SET FORTH ABOVE ARE FUNDAMENTAL ELEMENTS OF THE BASIS OF THE BARGAIN BETWEEN NEW RELIC AND CUSTOMER. 4.3 EVALUATION VERSION. Customer agrees and acknowledges that: (a) the Add-on is not an official product and has not been commercially released for sale by New Relic; (b) the Add-on may not operate properly, being in final form, or fully functional; (c) the Add-on may contain errors, security vulnerabilities, design flaws, or other problems; (d) it may not be possible to make the Add-on fully functional; (e) the information obtained using the Add-on may not be accurate; (f) use of the Add-on may result in unexpected results, loss of data, delays or other unpredictable damages or loss; (g) New Relic is under no obligation to release a commercial version of the Add-on; and (h) New Relic has the right unilaterally to abandon development of the Add-on, at any time and without any obligation or liability to Customer. 5. GENERAL PROVISIONS 5.1 Terms of Service; Documentation. This Agreement shall be considered a part of the New Relic documentation, located at: https://docs.newrelic.com (the “Documentation). This Agreement and the Terms of Service constitute the entire and exclusive agreement between New Relic and Customer with respect to the Add-on. In the absence of a separate agreement, upon agreement between New Relic and Customer with respect to the Add-On, the terms and conditions of this Agreement shall govern the relationship between New Relic and Customer with respect to such Add-On. To the extent of a conflict between the Agreement and the Terms of Service, this Agreement shall govern with respect to the Add-on only.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 172.01904,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Add-on end user <em>license</em> agreement",
        "sections": "Add-on end user <em>license</em> agreement",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": "&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;acceptable-use-policy&#x2F;acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential <em>Information</em>” means: (a) the Programmability Add-on, and any features, results or output produced by, and other <em>information</em> relating to the Add-on (including, without limitation"
      },
      "id": "603ec23328ccbccf1beba79a"
    },
    {
      "sections": [
        "FIT instrumentation end user license agreement"
      ],
      "title": "FIT instrumentation end user license agreement",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "e8e10baf120678407d08c9f78ab708d271cf0223",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/fit-instrumentation-end-user-license-agreement/",
      "published_at": "2021-10-08T06:50:31Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "NEW RELIC, INC. FIT INSTRUMENTATION END USER LICENSE AGREEMENT In connection with the work provided by New Relic’s Field Instrumentation Team, you may be provided with certain custom-created software to enable, optimize, or enhance your use of New Relic’s Services. By downloading, installing, authorizing installation, or using the FIT Instrumentation with your Underlying Software (defined below), you (“Customer”) agree to the terms and conditions herein (“Agreement”) with New Relic, Inc., a Delaware corporation with offices located at 188 Spear Street, Suite 1200 San Francisco, CA 94105 (“New Relic”), (collectively the “Parties”). Capitalized terms not defined herein shall have the meanings set forth in the New Relic Terms of Service (the “Terms of Service”) available here: http://newrelic.com/terms IF YOU DO NOT AGREE TO THIS AGREEMENT, PLEASE DO NOT USE THE FIT INSTRUMENTATION. 1. DEFINITIONS “FIT Instrumentation” means the New Relic custom-made software, including but not limited to connectors, extensions, and plugins, provided by New Relic to Customer to enable the Services to operate with certain third party or Customer software or systems (“Underlying Software”). For the purposes of the Terms of Service, the FIT Instrumentation shall be treated like an “Agent”, subject to the separate terms herein. 2. USE OF THE FIT INSTRUMENTATION 2.1 Software Evaluation License. Subject to the terms herein, New Relic grants to Customer a limited, non-exclusive, non-transferrable, non-sublicensable right to install, use, and configure the FIT Instrumentation solely as needed to enable the Services for internal evaluation purposes. New Relic reserves all rights and licenses not expressly granted herein. 2.2 Ownership. As between the Parties, New Relic owns all right, title, and interest to the FIT Instrumentation and Feedback, including but not limited to any intellectual property and proprietary rights therein. Customer retains all right, title, and interest in any Customer Data processed by the FIT Instrumentation in connection with the Services. 2.3 Feedback. Customer agrees to provide feedback, suggestions, ideas, requests or recommendations (“Feedback”) regarding the FIT Instrumentation, and hereby irrevocably assigns all intellectual property and proprietary rights it holds in the Feedback to New Relic. 2.4 Notice. Customer acknowledges and agrees that the FIT Instrumentation is made to work with the Underlying Software as configured at the time of creation and there is no guarantee that FIT Instrumentation will continue to work in the event Customer changes, replaces, upgrades versions of, updates, or otherwise changes the Underlying Software (an “Upgrade Event”). Customer shall hold New Relic harmless from any claims or damages arising from Customer’s Upgrade Event. 2.5 Restrictions. Customer will not: (i) use the FIT Instrumentation except as permitted hereunder; (ii) distribute, sell, sublicense, or otherwise transfer the FIT Instrumentation; (iii) decompile, disassemble or reverse engineer any software underlying the FIT Instrumentation; (iv) use the FIT Instrumentation to damage, disable, overburden or impair any New Relic server or network(s) connected to any New Relic server or interfere with any other party’s use and enjoyment of the Services; (v) use the FIT Instrumentation to defraud, defame, abuse, harass, stalk, threaten or infringe the rights of privacy or other intellectual property rights of others or otherwise violate any applicable law; (vi) circumvent or modify any security technologies included as part of the FIT Instrumentation; or (vii) attempt or permit any third party to do any of the foregoing. New Relic may suspend usage of the Services by any user, without notice, pending any investigation of misuse.  These restrictions may be supplemented or superceded (to the extent they conflict) by the New Relic Acceptable Use Policy as may be published and updated from time to time at https://docs.newrelic.com/docs/licenses/license-information/acceptable-use-policy/acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential Information” means: (a) the FIT Instrumentation, and any features, results or output produced by, and other information relating to the FIT Instrumentation (including, without limitation, all Feedback); and (b) any business or technical information of New Relic including but not limited to any technical information, research, development, know-how that a reasonable person would understand to be confidential. 3.2 Restrictions. Customer will not use or disclose any Confidential Information, except as necessary for the performance of this Agreement. Customer will use all reasonable efforts to protect Confidential Information from unauthorized use or disclosure, but in no event less than the efforts that it ordinarily uses with respect to its own proprietary information. Customer may disclose Confidential Information to those of its employees who have a bona fide need to know such Confidential Information for the performance of this Agreement; provided that each such employee first executes a written agreement that contains use and nondisclosure restrictions at least as protective as those set forth herein. Confidential Information shall not include any information that: (a) is or becomes generally known to the public through no fault or breach of this Agreement by Customer; (b) is rightfully known by Customer at the time of disclosure without an obligation of confidentiality; (c) is independently developed by Customer without access or use of any Confidential Information; or (d) is rightfully obtained from a third party without restriction on use or disclosure. 4. DISCLAIMER 4.1 DISCLAIMER. FIT INSTRUMENTATION IS PROVIDED AS-IS AND AS-AVAILABLE AND NEW RELIC DISCLAIMS AND MAKES NO WARRANTIES, EITHER EXPRESS OR IMPLIED, INCLUDING ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR NON-INFRINGEMENT AND WHETHER OR NOT ARISING THROUGH A COURSE OF DEALING. FIT INSTRUMENTATION IS NOT GUARANTEED TO BE ERROR-FREE, COMPATIBLE WITH THE UNDERLYING SOFTWARE. , OR THAT CUSTOMER WILL ACHIEVE ANY RESULTS FROM USE OF THE FIT INSTRUMENTATION THEREFROM. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OF IMPLIED WARRANTIES OR LIMITATIONS ON APPLICABLE STATUTORY RIGHTS OF A CONSUMER, SO THE ABOVE EXCLUSION AND LIMITATIONS MAY NOT APPLY TO THE CUSTOMER. 4.2 LIMITATION OF LIABILITY. TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE TO CUSTOMER OR ANY THIRD PARTY FOR DAMAGES OF ANY KIND, INCLUDING, WITHOUT LIMITATION, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE INSTALLATION, USE OR INABILITY TO USE THE FIT INSTRUMENTATION OR FOR ANY ERROR OR DEFECT IN THE FIT INSTRUMENTATION OR THE SERVICES, WHETHER SUCH LIABILITY ARISES FROM ANY CLAIM BASED UPON CONTRACT, WARRANTY, TORT, STRICT LIABILITY, OR OTHERWISE, WHETHER OR NOT NEW RELIC HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSS OR DAMAGE. NEW RELIC'S AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS AGREEMENT WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE PARTIES HAVE AGREED THAT THESE LIMITATIONS WILL SURVIVE AND APPLY EVEN IF ANY LIMITED REMEDY SPECIFIED IN THIS AGREEMENT IS FOUND TO HAVE FAILED OF ITS ESSENTIAL PURPOSE. 4.3 EVALUATION VERSION. Customer agrees and acknowledges that: (a) the FIT Instrumentation is not an official product and has not been commercially released for sale by New Relic; (b) the FIT Instrumentation may not operate properly, being in final form, or fully functional; (c) the FIT Instrumentation may contain errors, security vulnerabilities, design flaws, or other problems; (d) it may not be possible to make the FIT Instrumentation fully functional; (e) the information obtained using the FIT Instrumentation may not be accurate; (f) use of the FIT Instrumentation may result in unexpected results, loss of data, delays or other unpredictable damages or loss; (g) New Relic is under no obligation to release a commercial version of the FIT Instrumentation; and (h) New Relic has the right unilaterally to abandon development of the FIT Instrumentation, at any time and without any obligation or liability to Customer. 5. GENERAL PROVISIONS 5.1 Terms of Service; Documentation. This Agreement shall be considered a part of the New Relic documentation, located at: https://docs.newrelic.com (the \"Documentation\").This Agreement and the Terms of Service constitute the entire and exclusive agreement between New Relic and Customer with respect to the FIT Instrumentation. In the absence of a separate agreement between New Relic and Customer with respect to the FIT Instrumentation, the terms and conditions of this Agreement shall govern the relationship between New Relic and Customer with respect to such FIT Instrumentation.  To the extent of a conflict between the Agreement and the Terms of Service, this Agreement shall govern with respect to the FIT Instrumentation only.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 172.01585,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "FIT instrumentation end user <em>license</em> agreement",
        "sections": "FIT instrumentation end user <em>license</em> agreement",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": " Acceptable Use Policy as may be published and updated from time to time at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;acceptable-use-policy&#x2F;acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential <em>Information</em>” means: (a) the FIT Instrumentation, and any features, results"
      },
      "id": "603e9f3ee7b9d206e32a0800"
    }
  ],
  "/docs/licenses/license-information/faq/new-relic-one-pricing-plan-frequently-asked-questions": [
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Purpose",
        "2. Security Overview",
        "3.Security Certifications",
        "4. Data Control and Encryption",
        "5. Facilities",
        "6. Employee Access, Screening and Controls.",
        "7. Security Incident and Data Breach Response",
        "8. Network and Systems Security",
        "9. Authentication and Access Management",
        "10. Vulnerability Management",
        "11. System Access and Logging",
        "12. Disaster Recovery and Data Backup",
        "13. Copies and Removal",
        "14. Third Party Vendor Management",
        "15. Disclosure by Law",
        "16. Updates"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2021-10-08T06:55:04Z",
      "updated_at": "2021-09-27T15:05:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Updated 23 September 2021. The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Purpose This Policy describes New Relic's security program and the technical and organizational security controls to protect New Relic’s systems as shown in New Relic’s third-party audits and certifications. New Relic may update this Policy from time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/. 2. Security Overview 2.1. New Relic maintains a comprehensive Information Protection Program to manage information security within New Relic that includes administrative, technical, and physical safeguards designed to protect the confidentiality, integrity, and availability of Customer Data and that is appropriate to the nature, size, and complexity of New Relic's business operations. New Relic’s Information Protection Program includes: 2.1.1. executive review, support and accountability for all security related policies and practices; 2.1.2. formal written policies and procedures that are designed to protect against the loss, theft, or other unauthorized access or alteration of Customer Data, meet or exceed applicable industry standards, outlines a definition of information security and its overall objectives, include defined information security roles and responsibilities, a framework for setting control objectives and controls, a formal and effective risk mitigation program and a service provider security management program; 2.1.3. periodic risk assessments, including internal audits and/or independent 3rd party audits, to measure the effectiveness and appropriateness of controls for all systems processing Customer Data; 2.1.4. in-depth review of security incidents, including effective determination of root cause and corrective action; formal, industry-recognized controls frameworks based on an external audit standard; 2.1.5. employee screening and access management; and 2.1.6. comprehensive security testing, vulnerability identification, and mitigation methodology that consists of a variety of independent approaches that, when combined, are designed to maximize coverage for a diverse set of attack vectors. 2.2. New Relic's Chief Information Security Officer (CISO) leads New Relic's Information Security Program and develops, reviews, and approves, with appropriate stakeholders, New Relic's security policies and procedures. New Relic has appointed data protection officer(s) as described in New Relic’s Privacy Notices who are consulted as necessary under applicable laws. 2.3. Information security objectives, approach, scope, importance, goals, and principles for the organization’s security program are formally identified, communicated throughout the organization to users in a form that is relevant, accessible, and understandable to the intended reader; and supported by a controls framework that considers legislative, regulatory, contractual requirements, and other policy-related requirements. 2.4. New Relic will maintain written policies and procedures to review, test, and approve (as appropriate) changes affecting New Relic infrastructure and systems that process Customer Data including, but not limited to, peer reviews prior to introducing new code into production. New Relic will establish acceptance criteria for new information systems, upgrades, and new versions and will carry out suitable tests of the system(s) during development and prior to acceptance. Any changes or updates will not materially decrease the security of the systems. 3.Security Certifications 3.1. New Relic regularly tests, assesses, and evaluates its security measures for protecting Customer Data using industry-recognized standards and uses independent third-party auditors to verify such controls. 3.2. New Relic agrees to provide Customer, upon request, with applicable certifications or reports about New Relic systems. All information exchanged in connection with the audit activities described in this section is deemed to be the Confidential Information of New Relic. 3.3. Additional information about New Relic’s security certifications are available on New Relic’s Security Guide. 4. Data Control and Encryption 4.1. The Service and related features are designed to provide Customer with control over Customer’s data sources and Customer’s environments that are monitored and sending data to New Relic. 4.2. New Relic will have established methods to: (i) enable Customer to use encryption on Customer Data in transit, and (ii) securely hash passwords in storage following industry standard practices (e.g. scrypt) as described in the Documentation. Server certificate-based authentication is used as part of the TLS encryption with a trusted certificate authority. 4.3. New Relic receives and processes data in accordance with the Agreement and the Services as described in the Documentation. New Relic permits customers to delete personal data in accordance with applicable privacy laws as further described in the Documentation. In the event of error in personal data sent in Customer Data Customers may request personal data deletion and re-send data that is accurate. 4.4. Additional information for Customer data control and encryption, including encryption of data at rest, is available on New Relic’s Security Guide. 5. Facilities 5.1. All physical locations that process Customer Data are co-locations and third party data centers. All physical locations have security measures in place that are designed to prevent unauthorized physical access to data processing facilities and unlawful access, modification, or destruction of Customer Data. 5.2. New Relic will seek assurances (e.g., in the form of an independent 3rd party audit report such as the SOC 2 Type 2, ISO 27001, and vendor security evaluations) from its data processing facilities vendors that store or process Customer Data: 5.2.1. secure its data process facilities in an access-controlled location and have protections in place to prevent unauthorized access, damage, and interference; 5.2.2 employ physical security appropriate to the classification of the assets and information being managed which may include card key access, security cameras, and solid wall construction for all exterior walls; 5.2.3 limit and screen all entrants, which may include measures such as on-site security guard, badge reader, electronic lock, or a monitored closed caption television (CCTV); and 5.2.4 maintain relevant access logs. 5.3. Additional information about New Relic’s third party data centers are available on New Relic’s Security Guide. 6. Employee Access, Screening and Controls. New Relic will have and maintain policies and practices that include, at a minimum, the following controls and safeguards applied to New Relic employees and contractors who may access Customer Data: 6.1. For U.S. New Relic employees and subject to applicable law, a criminal background screening of each of its new employees to whom it gives access to Customer Data at the federal, state, and county levels. For non-U.S. New Relic employees, New Relic will use commercially reasonable efforts to meet the same criteria as established for U.S.-based New Relic employees, subject to general business practices in the respective country and in compliance with applicable local law requirements; 6.2. All New Relic employees with access to Customer Data will undergo adequate training, such as annual security awareness training, in the care, protection and handling of Customer Data, and will align with the privacy and security measures set out in this Addendum by following the guidance provided in their welcome package, which includes New Relic’s security policies and a security acknowledgement; 6.3. A disciplinary policy and process, to be used when New Relic employees violate New Relic security or privacy policies or access Customer Data without prior authorization; 6.4. Administrative or remote access to New Relic systems that process Customer Data will align with industry standard practices, including multi-factor authentication; 6.5. Restricted access to New Relic proprietary source code to prevent unauthorized access; 6.6. Controls designed to ensure that only those New Relic employees with an actual need-to-know will have access to New Relic systems including, but not limited to, the use of a formal access management process for the request, review, approval, and provisioning; 6.7. Controls designed to ensure that New Relic employees are granted access to New Relic systems based on least-privilege principles; and 6.8. Revoke access to New Relic employees no longer requiring access. 7. Security Incident and Data Breach Response 7.1. New Relic will take appropriate physical, technical, and administrative security measures that are commercially reasonable and consistent with industry standards to prevent a Data Breach, and as required by any applicable law or regulation. “Data Breach” means the theft, loss, or unauthorized access of Customer Data. Without limiting the foregoing, New Relic will implement security measures at least as stringent as those set out in this Addendum. New Relic will designate a senior representative to provide incident briefings, as needed in case of a Data Breach, and to respond to reasonable requests by Customer pertaining to privacy and data security issues within a commercially reasonable time frame. 7.2. New Relic will: 7.2.1. Notify Customer without undue delay if New Relic becomes aware of a Data Breach; 7.2.2. Maintain a security incident response plan, including procedures and means to respond in a manner consistent with industry standards; 7.2.3. Reasonably cooperate with Customer to investigate and remediate a Data Breach and mitigate any further risk to the Customer Data, or risk to data subjects and/or Customer’s reputation or brand; 7.2.4. Provide reasonable assistance to Customer at New Relic’s sole cost and expense; and 7.2.5. Make commercially reasonable efforts to preserve evidence and reasonably cooperate with Customer and legal authorities (as applicable and legally permissible) during an investigation of a Data Breach. 8. Network and Systems Security 8.1. All extranet connectivity by New Relic personnel to systems processing Customer Data will be through secure remote connections. 8.2. Network segments connected to the Internet will be protected by secure access control mechanisms, such as a firewall configured to secure all devices behind it and properly addresses security concerns according to industry standard practices. 8.3. New Relic will have industry standard measures in place to actively monitor its systems and help detect a potential hostile attack, such as Network Intrusion Detection (NID) or Host Intrusion Detection (HID)/Prevention. 8.4. Applications, ports, services, and similar access points installed on a computer or network facility, which are not specifically required for business functionality, will be disabled or removed. 8.5. New Relic maintains configuration standards for authorized operating systems and software for systems that support processing of Customer Data. New Relic will establish and follow server configuration guidelines and processes for preventing unauthorized access to Customer Data. New Relic maintains secure images or templates for systems based on the organization’s approved configuration standards. 8.6. Development, test, and operational environments will be logically separated to reduce the risks of unauthorized access or changes to the operational system. 8.7. New Relic network architecture will be designed to limit site access and restrict the availability of information systems that are considered to be vulnerable to attack. 8.8. New Relic provides appropriate TLS certificates when users access and view Customer Data in the Service. New Relic’s software for sending Customer Data to New Relic will encrypt Customer Data in transit by default. 9. Authentication and Access Management If you subscribe to the requisite New Relic One Service, New Relic will provide industry standard authentication and access controls to protect Customer Data, including industry standard authentication methods utilized to help prevent unauthorized access to the Service. New Relic’s access control methods will clearly state the rules and rights for each user or group of users including applications and information sharing and will include a process for granting and removing access to all information systems processing Customer Data. A record of all privileges allocated will be maintained pursuant to the requirements herein. 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. 10. Vulnerability Management New Relic will have and maintain the following vulnerability management processes for all devices used to connect to the New Relic network and Services. 10.1. New Relic will align to industry standard practices for build out, minimization of services and secure configuration, in accordance with, industry-recognized minimum security baselines for the New Relic platform and subcontractor systems connected to the New Relic platform in relation to the provision of the Services. 10.2. New Relic will employ industry-recognized standards and tools to conduct frequent infrastructure vulnerability scanning to test New Relic’s network and infrastructure and application penetration testing to test the New Relic Services. New Relic applies “Medium”, “High” and “Critical” security patches for all components in production and development environments as soon as commercially possible in accordance with its vulnerability management protocol, and consistent with industry standard practices and standards; 10.3. New Relic will have processes in place designed to ensure adherence to industry standard security development practices for development and testing for all code, APIs, and applications deployed and implemented in support of the Service; 10.4. New Relic will have and maintain solutions to identify and prevent malicious attackers or code from accessing or compromising Customer Data or systems that process Customer Data. These include, but are not limited to, software that identifies and removes malware and detects attempted intrusions. New Relic will have a security event and incident monitoring system and supporting processes to notify appropriate personnel in response to threats. 11. System Access and Logging 11.1. Access to New Relic’s systems will not be granted to employees of New Relic unless they have been uniquely identified and have sufficient credentials. 11.2. Access to New Relic’s systems will be logged and retained for no less than 6 months to assist in investigations and access control monitoring, including, but not limited to, end user access and activities, and information security events. 11.3. New Relic agrees to provide Customer the capability to access log records relating to Customer Accounts and the New Relic systems that process Customer Data in the event of a Data Breach or if required in connection with a law enforcement request. 12. Disaster Recovery and Data Backup 12.1. New Relic will have plans designed to respond to loss of services, which are tested and reviewed at least annually. This plan will include documented policies and procedures to restore service in the event of a service failure. 12.2. New Relic will establish and follow backup and restore procedures for Customer Data. 12.3. New Relic business continuity plan identifies critical systems. Annual disaster recovery tests are performed to check and restore customer data in the event of an incident. 12.4. New Relic will provide Customer with redacted copies of its plan(s) and evidence of tests/reviews upon request, but not more frequently than once annually, and subject to confidentiality requirements. 12.5. Additional information for Customer Data control and encryption are available on New Relic’s Security Guide. 13. Copies and Removal 13.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of the Agreement for any reason: (a) New Relic will, and will cause its personnel, to cease all access and use of any Customer Data, and (b) New Relic will delete all copies of Customer Data within ninety (90) days. 13.2. New Relic will maintain a process of ensuring secure destruction and deletion of all Customer Data, when reasonably requested by Customer or as otherwise provided in the Agreement. The process will include industry standard processes so that: (i) Customer Data cannot be practicably read or reconstructed, and (ii) the systems that store Customer Data are securely erased and/or decommissioned disks are destroyed. 14. Third Party Vendor Management New Relic may use third party vendors to provide the Services. New Relic performs a security risk-based assessment of prospective vendors before working with them to validate they meet New Relic’s security and business continuity standards, including the type of access and classification of data being accessed (if any), controls necessary to protect data, and legal/regulatory requirements. New Relic enters into written agreements with its vendors that process Customer Data which include confidentiality, privacy, and security obligations that provide an appropriate level of protection for Customer Data that these vendors may process for New Relic to maintain the security posture in this Policy, including following industry security standards. 15. Disclosure by Law In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic will (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. New Relic publishes its law enforcement requests report on New Relic’s Security Guide. 16. Updates As New Relic releases new products, services, functionality, and features, New Relic may update this Policy to account for such products, services, functionality, and features. For additional information, see our Security Guide.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 149.96324,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;referenced-policies&#x2F;security-policy&#x2F;. 2. Security Overview 2.1. New Relic"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    },
    {
      "sections": [
        "Security guide",
        "Tip",
        "Security Program",
        "Security Domains",
        "Security Certifications",
        "Data Control, Facilities, and Encryption",
        "Law Enforcement Request Report"
      ],
      "title": "Security guide",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "356f0d11ffcb62208a743a0a7c127f5f6da9c940",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-guide/",
      "published_at": "2021-10-07T02:08:45Z",
      "updated_at": "2021-09-19T15:21:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Last updated September 17, 2021. This is supplement to our security policy and serves as a guide to New Relic’s description of its Services, functionalities, and features. Tip We may update the URLs in this document without notice. Security Program New Relic follows \"privacy by design\" principles as described here: https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Security Domains New Relic’s policies and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless Security; Configuration Management; Vulnerability Management; Network Protection; Transmission Protection; Password Management; Access Control, Audit Logging & Monitoring; Education, Training, and Awareness; Third Party Assurance; Incident Management; Business Continuity and Disaster Recover; Risk Management; Data Protection & Privacy; and Service Management Systems. Security Certifications New Relic audits its Services against industry standards as described at https://docs.newrelic.com/docs/security/security-privacy/compliance/regulatory-audits-new-relic-services/. Data Control, Facilities, and Encryption New Relic's customers can send data to New Relic's APIs by (1) using New Relic's software, (2) using vendor-neutral software that is managed and maintained by a third-party such as via OpenTelemetry instrumentation provided by opentelemetry.io, or (3) from third-party systems that customer's manage and/or control. New Relic's customers can use New Relic's Services such as NerdGraph to filter out and drop data. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/drop-data-using-nerdgraph/. New Relic's customers can adjust their data retention periods as appropriate for their needs. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/manage-data-retention/#adjust-retention. New Relic Logs obfuscates numbers that match known patterns, such as bank card and social security numbers as described here: https://docs.newrelic.com/docs/logs/log-management/get-started/new-relics-log-management-security-privacy/. New Relic honors requests to delete personal data in accordance with applicable privacy laws. Please see https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Customers may use New Relic's APIs to query data, such as NerdGraph described here, and New Relic Services to export the data to other cloud providers. Customers can configure its log forwarder [https://docs.newrelic.com/docs/logs/enable-log-management-new-relic/enable-log-monitoring-new-relic/forward-your-logs-using-infrastructure-agent/] before sending infrastructure logs to New Relic. For New Relic Customers in New Relic US, FedRAMP and HIPAA-enabled environments, Customer Data is replicated to the off-site backup system via Amazon Simple Storage Service (S3). Category of Customer Description FedRAMP HIPAA-enabled US Gen Pop EU Gen Pop Data is stored in Amazon Web Services (“AWS”). Limited Data is stored in IBM Data for New Relic Incident Intelligence is stored in Google Cloud New Relic regularly tests, assess, and evaluates its measures to ensure the security of processing using industry-recognized standards and uses independent third-party auditors as provided below: Annual SOC 2 Type 2 Annual FedRAMP assessment by an independent third-party pursuant to NIST 800-53 rev 4 Moderate authorization. Annual HITRUST-validated assessment by an independent third-party *Pursuing CY2021 Q4 ISO 27001 *Pursuing CY2021 TISAX *Pursuing CY2021 The Services that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed information about AWS security is available at https://aws.amazon.com/security/ and http://aws.amazon.com/security/sharing-the-security-responsibility/. Data encryption at rest utilizes FIPS 140-2 compliant encryption methodology. For AWS SOC Reports, please see https://aws.amazon.com/compliance/soc-faqs/. The Services that operate on Google Cloud Platform (\"GCP\") are protected by the security and environmental controls of GCP. Detailed information about GCP security is available at https://cloud.google.com/docs/tutorials#security. For GCP reports, please see https://cloud.google.com/security/compliance/. IBM Deft Zayo QTS Law Enforcement Request Report New Relic has not to date received any request for customer data from a law enforcement or other government agency (including under any national security process), and has not made any corresponding disclosures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 135.48743,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " *Pursuing CY2021 The Services that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed <em>information</em> about AWS security is available at https:&#x2F;&#x2F;aws.amazon.com&#x2F;security&#x2F; and http:&#x2F;&#x2F;aws.amazon.com&#x2F;security&#x2F;sharing-the-security-responsibility"
      },
      "id": "6147558128ccbc973a56a863"
    },
    {
      "sections": [
        "Original product-based pricing definitions",
        "App",
        "App transaction",
        "AWS Lambda event",
        "Check",
        "Compute unit",
        "Datapoints per minute",
        "Event",
        "Host",
        "Incident event",
        "Page view",
        "Per GB daily",
        "Span",
        "User"
      ],
      "title": "Original product-based pricing definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "42087e53167736831855bf9a4c2967c465677b45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/legacy-product-definitions/",
      "published_at": "2021-10-08T06:53:20Z",
      "updated_at": "2021-09-14T07:27:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our original product-based pricing. For New Relic One pricing plan terms, see New Relic One pricing definitions. App An app means the application software designed to run on smartphones, tablet computers, and other mobile devices. Each app includes a predefined number of users. App transaction An app transaction is an APM application's attempt to process a web or non-web request. In New Relic APM these manifest as throughput TIMESERIES or individually as events in the Transaction event type. AWS Lambda event An AWS Lambda event means the row of data collected from the customer's AWS Lambda function by the New Relic agent or sent from an external service into the New Relic platform. It consists of the AwsLambdaInvocation, AwsLambdaInvocationError, or custom event types. Check A check means the single instance of a Synthetics monitor running in New Relic's monitoring network and reporting back response time, and whether the check was a success or failure. Compute unit A compute unit means the measure of resources associated with a unit of computation on a physical or virtual host. Datapoints per minute Datapoints per minute (DPM) refers to the per-minute rate at which individual metric values are sent to the New Relic Metric Ingest API. For billing purposes, datapoints per minute are calculated as a monthly average value by summing the datapoints ingested during a 30 day period and dividing by the number of minutes in that period (43,200). Event An event means the row of data collected from the customer's application by the New Relic agent or sent from an external service into our database. Host A host means the physical computer or virtual machine instance running a single copy of an operating system. Host usage is tracked monthly by summing the hours that every host in the account is connected to New Relic and dividing by 750. A host is counted if it is connected any time during an hour. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Page view A page view means the full page load (triggering an onLoad event) or a recorded URL change (state change). Per GB daily Per GB daily represents a daily average of Log data sent to New Relic over a 30 day period. Span A span represents an operation summary collected from the customer's application via the New Relic APM agent or New Relic Serverless for AWS Lambda agent, or sent from other tracing tools to the New Relic Trace API. User A user means the individual that connects to your app from a single device. Each unique device is considered as a unique user.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 129.61038,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>"
      },
      "id": "603ebacc64441f77774e8872"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/acceptable-use-policy": [
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Important",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-10-07T03:33:01Z",
      "updated_at": "2021-09-07T23:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Important NOTE: If you are a New Relic HIPAA customer, please be advised that you must follow the requirements specified in the Global Technical Support Section of HIPAA enablement - what you need to know and do when requesting support and engaging with the New Relic Global Technical Support team for assistance. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customer’s paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customer’s paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relic’s distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relic’s Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and we’re here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relic’s Open Source Community Plus Projects. New Relic’s Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relic’s Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relic’s Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered “generally available.” If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we can’t help with things we didn’t build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and we’ll help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. We’ll do our best to determine whether an issue is with New Relic’s Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the “account owner,” such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relic’s Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Important As of March 10, 2021, you're no longer able to create a New Relic support ticket by emailing support@newrelic.com. If you have ticketed support as part of your subscription, when logged into your New Relic account, go to one.newrelic.com to create a support ticket. Once you're there, follow these steps: Click the ? icon in the upper right hand corner. Click I need more help. Select Create a Support Ticket. By creating a support ticket directly within your account, it will speed up the support process and improve our team’s ability to troubleshoot your issue more effectively. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relic’s Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 233.08478,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " environments Troubleshooting problems with ingesting data into New Relic <em>General</em> <em>usage</em> and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments &amp; frameworks"
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "New Relic data usage limits and policies",
        "Overview of limits",
        "View limits and manage data",
        "Account-level limits",
        "Data ingest APIs",
        "Other agent and integration limits",
        "Manage data"
      ],
      "title": "New Relic data usage limits and policies",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "fc32c25b40a030ffa0fad6bfc95be7fca1360ee1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies/",
      "published_at": "2021-10-08T06:52:18Z",
      "updated_at": "2021-07-21T21:09:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document lists some important account-level limits and links to other limit-related docs. Overview of limits We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers' accounts, we have various data volume and rate limits in place. We reserve the right to enforce these limits to protect our system and to avoid issues for you and other customers. If your New Relic account, whether by configuration or by error, exceeds one of these limits, it or its child accounts might experience one or both of the following: Sampling of data Temporary pause or cessation of data collection To learn more about how hitting a limit can affect your data, see View limits. If you have further questions about these limits, your contract, or a limit you've reached, contact your New Relic account representative. We can work with you to adjust any rate limits to meet your needs. View limits and manage data Want to understand your account’s limit violations using the UI? See View limits. Want to manage your data for organization or billing purposes? See Manage data. Account-level limits The following table includes general max limits that apply across all New Relic accounts. Specific New Relic tools, like agents and integrations, have their own limits and configurations, and might be lower than these theoretical maximum limits. Limited condition Limit Rate of NRDB record * ingest 55 million per account per minute Max NRDB records * ingested per API call 1MB (10^6 bytes) Max attribute value size 1KB (10^3 bytes) Max payload size 1MB (10^6 bytes) Max total attributes per data type (including default attributes) 254 (less for some tools; for example, 64 for agents) Number of unique custom data types 250 per account per day (applies to custom events because that's source of new data types) APM limits Agent instances: 50K per account Agent instances per app: 10K APM apps/services: 10K per second Browser: number of page views 1M per minute per app Distributed tracing: Max age of span timestamp values 20 minutes. Timestamp must be within 20 minutes of current time at ingest or within 20 minutes from the time the last span with the same trace.id was received by New Relic. Distributed tracing: Max spans per minute per account Dependent on agreement. Max limit: 2M. Distributed tracing: Max spans per trace 50K Distributed tracing: Max attributes per span 200 Rate of metric timeslice data (used by APM, browser, mobile) Ingest: 2 million per minute Rate of names: 4 million per minute per account Number per monitored app: 300K Mobile monitoring: number of crashes reported 10K per hour Infrastructure agents, integrations Number of infrastructure agents and/or integrations: 5K per account Gross number of new monitored containers: 5K per hour per account Query limits NRDB records * inspected: 100 billion per account per hour Rate of queries: 20 per account per second See other query limits * NRDB records refers to database records for our core data types, which includes events, metrics (dimensional), logs, and distributed tracing (span) data, all stored in the New Relic database (NRDB). This does not include metric timeslice data. Data ingest APIs Our ingest APIs have additional limits that may override the more general account-level limits. Note that these limits also apply to our tools that use these APIs (like our Telemetry SDKs or our open source telemetry integrations). Metric API (dimensional metrics) Event API Log API Trace API Other agent and integration limits To find limits for our other agents and integrations, which will override more general account-level limits, see the docs for those tools: you can search our solutions here. Some default reporting limits are located in these tools' configuration docs. Manage data Want to manage your New Relic data ingest and storage to improve data organization or reduce billing? See Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 205.9395,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic data <em>usage</em> limits and policies",
        "sections": "New Relic data <em>usage</em> limits and policies",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "This document lists some important account-level limits and links to other limit-related docs. Overview of limits We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data <em>usage</em> spikes in one New Relic account from impacting other"
      },
      "id": "603eb1c528ccbc0311eba7c7"
    },
    {
      "sections": [
        "The People's Republic of China"
      ],
      "title": "The People's Republic of China",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "c3c078b8139d695b928d2001cd6c6c9318c43599",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/peoples-republic-china/",
      "published_at": "2021-10-08T06:53:20Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important Information Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory requirements in the PRC. New Relic does not provide support for, including but not limited to, the deployment, access, or use of the Service(s) or Software in the PRC, or otherwise in respect of assets in the PRC (“PRC Use”). Without overriding any express prohibitions that you may have agreed to as part of your agreement with New Relic, you (and you as an agent of the respective New Relic Customer (\"Customer\")) acknowledge that any PRC use is subject to the PRC national firewall system and may be subject to outages and other interference outside of the control of New Relic. Accordingly, you acknowledge and agree (on behalf of yourself and including any Customer on whose behalf you use the Service(s) or Software) that any PRC Use is at your sole risk and is fully excepted from all representations and warranties, including, but not limited to, any terms that assure the confidentiality, integrity, availability or privacy of your data. New Relic makes no other representations and hereby expressly disclaims any and all warranties in respect of PRC Use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 192.49605,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "Important <em>Information</em> Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory"
      },
      "id": "603eb3a328ccbcb488eba76b"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings": [
    {
      "sections": [
        "New Relic data usage limits and policies",
        "Overview of limits",
        "View limits and manage data",
        "Account-level limits",
        "Data ingest APIs",
        "Other agent and integration limits",
        "Manage data"
      ],
      "title": "New Relic data usage limits and policies",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "fc32c25b40a030ffa0fad6bfc95be7fca1360ee1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies/",
      "published_at": "2021-10-08T06:52:18Z",
      "updated_at": "2021-07-21T21:09:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document lists some important account-level limits and links to other limit-related docs. Overview of limits We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers' accounts, we have various data volume and rate limits in place. We reserve the right to enforce these limits to protect our system and to avoid issues for you and other customers. If your New Relic account, whether by configuration or by error, exceeds one of these limits, it or its child accounts might experience one or both of the following: Sampling of data Temporary pause or cessation of data collection To learn more about how hitting a limit can affect your data, see View limits. If you have further questions about these limits, your contract, or a limit you've reached, contact your New Relic account representative. We can work with you to adjust any rate limits to meet your needs. View limits and manage data Want to understand your account’s limit violations using the UI? See View limits. Want to manage your data for organization or billing purposes? See Manage data. Account-level limits The following table includes general max limits that apply across all New Relic accounts. Specific New Relic tools, like agents and integrations, have their own limits and configurations, and might be lower than these theoretical maximum limits. Limited condition Limit Rate of NRDB record * ingest 55 million per account per minute Max NRDB records * ingested per API call 1MB (10^6 bytes) Max attribute value size 1KB (10^3 bytes) Max payload size 1MB (10^6 bytes) Max total attributes per data type (including default attributes) 254 (less for some tools; for example, 64 for agents) Number of unique custom data types 250 per account per day (applies to custom events because that's source of new data types) APM limits Agent instances: 50K per account Agent instances per app: 10K APM apps/services: 10K per second Browser: number of page views 1M per minute per app Distributed tracing: Max age of span timestamp values 20 minutes. Timestamp must be within 20 minutes of current time at ingest or within 20 minutes from the time the last span with the same trace.id was received by New Relic. Distributed tracing: Max spans per minute per account Dependent on agreement. Max limit: 2M. Distributed tracing: Max spans per trace 50K Distributed tracing: Max attributes per span 200 Rate of metric timeslice data (used by APM, browser, mobile) Ingest: 2 million per minute Rate of names: 4 million per minute per account Number per monitored app: 300K Mobile monitoring: number of crashes reported 10K per hour Infrastructure agents, integrations Number of infrastructure agents and/or integrations: 5K per account Gross number of new monitored containers: 5K per hour per account Query limits NRDB records * inspected: 100 billion per account per hour Rate of queries: 20 per account per second See other query limits * NRDB records refers to database records for our core data types, which includes events, metrics (dimensional), logs, and distributed tracing (span) data, all stored in the New Relic database (NRDB). This does not include metric timeslice data. Data ingest APIs Our ingest APIs have additional limits that may override the more general account-level limits. Note that these limits also apply to our tools that use these APIs (like our Telemetry SDKs or our open source telemetry integrations). Metric API (dimensional metrics) Event API Log API Trace API Other agent and integration limits To find limits for our other agents and integrations, which will override more general account-level limits, see the docs for those tools: you can search our solutions here. Some default reporting limits are located in these tools' configuration docs. Manage data Want to manage your New Relic data ingest and storage to improve data organization or reduce billing? See Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 205.9395,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic data <em>usage</em> limits and policies",
        "sections": "New Relic data <em>usage</em> limits and policies",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "This document lists some important account-level limits and links to other limit-related docs. Overview of limits We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data <em>usage</em> spikes in one New Relic account from impacting other"
      },
      "id": "603eb1c528ccbc0311eba7c7"
    },
    {
      "sections": [
        "The People's Republic of China"
      ],
      "title": "The People's Republic of China",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "c3c078b8139d695b928d2001cd6c6c9318c43599",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/peoples-republic-china/",
      "published_at": "2021-10-08T06:53:20Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important Information Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory requirements in the PRC. New Relic does not provide support for, including but not limited to, the deployment, access, or use of the Service(s) or Software in the PRC, or otherwise in respect of assets in the PRC (“PRC Use”). Without overriding any express prohibitions that you may have agreed to as part of your agreement with New Relic, you (and you as an agent of the respective New Relic Customer (\"Customer\")) acknowledge that any PRC use is subject to the PRC national firewall system and may be subject to outages and other interference outside of the control of New Relic. Accordingly, you acknowledge and agree (on behalf of yourself and including any Customer on whose behalf you use the Service(s) or Software) that any PRC Use is at your sole risk and is fully excepted from all representations and warranties, including, but not limited to, any terms that assure the confidentiality, integrity, availability or privacy of your data. New Relic makes no other representations and hereby expressly disclaims any and all warranties in respect of PRC Use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 192.49605,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "Important <em>Information</em> Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory"
      },
      "id": "603eb3a328ccbcb488eba76b"
    },
    {
      "sections": [
        "Government addendum",
        "New Relic, Inc.Government Addendum"
      ],
      "title": "Government addendum",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "cfed51b7f4f7583476f56b2b574204d38493e882",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/government-addendum/",
      "published_at": "2021-10-08T06:51:37Z",
      "updated_at": "2021-03-16T04:21:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The below Government Addendum applies only to customers that are United States federal, state, or local government customers with an existing New Relic agreement in place that explicitly references this Government Addendum applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic, Inc.Government Addendum For purposes of this Agreement and to the extent applicable, the Service is \"commercial computer software\" and a \"commercially available off-the-shelf (COTS) item\" as defined at FAR 2.101 developed at the private expense of New Relic. If acquired by or on behalf of a civilian agency, the U.S. Government acquires this commercial computer software and/or commercial computer software documentation and other technical data subject to the terms of the Agreement as specified in 48 C.F.R. 12.212 (Computer Software) and 12.211 (Technical Data) of the Federal Acquisition Regulation (\"FAR\") and its successors. If acquired by or on behalf of any agency within the Department of Defense (\"DOD\"), the U.S. Government acquires this commercial computer software and/or commercial computer software documentation subject to the terms of the Agreement as specified in 48 C.F.R. 227.7202-3 of the DOD FAR Supplement (\"DFARS\") and its successors. This addendum is in lieu of and supersedes any other FAR, DFARS, or other clause or provision that addresses government rights in computer software or technical data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 192.49538,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>"
      },
      "id": "603ea47ee7b9d2dd0d2a07ef"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/government-addendum": [
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Important",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-10-07T03:33:01Z",
      "updated_at": "2021-09-07T23:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Important NOTE: If you are a New Relic HIPAA customer, please be advised that you must follow the requirements specified in the Global Technical Support Section of HIPAA enablement - what you need to know and do when requesting support and engaging with the New Relic Global Technical Support team for assistance. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customer’s paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customer’s paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relic’s distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relic’s Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and we’re here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relic’s Open Source Community Plus Projects. New Relic’s Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relic’s Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relic’s Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered “generally available.” If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we can’t help with things we didn’t build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and we’ll help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. We’ll do our best to determine whether an issue is with New Relic’s Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the “account owner,” such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relic’s Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Important As of March 10, 2021, you're no longer able to create a New Relic support ticket by emailing support@newrelic.com. If you have ticketed support as part of your subscription, when logged into your New Relic account, go to one.newrelic.com to create a support ticket. Once you're there, follow these steps: Click the ? icon in the upper right hand corner. Click I need more help. Select Create a Support Ticket. By creating a support ticket directly within your account, it will speed up the support process and improve our team’s ability to troubleshoot your issue more effectively. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relic’s Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 233.08478,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " environments Troubleshooting problems with ingesting data into New Relic <em>General</em> <em>usage</em> and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments &amp; frameworks"
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "New Relic data usage limits and policies",
        "Overview of limits",
        "View limits and manage data",
        "Account-level limits",
        "Data ingest APIs",
        "Other agent and integration limits",
        "Manage data"
      ],
      "title": "New Relic data usage limits and policies",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "fc32c25b40a030ffa0fad6bfc95be7fca1360ee1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies/",
      "published_at": "2021-10-08T06:52:18Z",
      "updated_at": "2021-07-21T21:09:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document lists some important account-level limits and links to other limit-related docs. Overview of limits We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers' accounts, we have various data volume and rate limits in place. We reserve the right to enforce these limits to protect our system and to avoid issues for you and other customers. If your New Relic account, whether by configuration or by error, exceeds one of these limits, it or its child accounts might experience one or both of the following: Sampling of data Temporary pause or cessation of data collection To learn more about how hitting a limit can affect your data, see View limits. If you have further questions about these limits, your contract, or a limit you've reached, contact your New Relic account representative. We can work with you to adjust any rate limits to meet your needs. View limits and manage data Want to understand your account’s limit violations using the UI? See View limits. Want to manage your data for organization or billing purposes? See Manage data. Account-level limits The following table includes general max limits that apply across all New Relic accounts. Specific New Relic tools, like agents and integrations, have their own limits and configurations, and might be lower than these theoretical maximum limits. Limited condition Limit Rate of NRDB record * ingest 55 million per account per minute Max NRDB records * ingested per API call 1MB (10^6 bytes) Max attribute value size 1KB (10^3 bytes) Max payload size 1MB (10^6 bytes) Max total attributes per data type (including default attributes) 254 (less for some tools; for example, 64 for agents) Number of unique custom data types 250 per account per day (applies to custom events because that's source of new data types) APM limits Agent instances: 50K per account Agent instances per app: 10K APM apps/services: 10K per second Browser: number of page views 1M per minute per app Distributed tracing: Max age of span timestamp values 20 minutes. Timestamp must be within 20 minutes of current time at ingest or within 20 minutes from the time the last span with the same trace.id was received by New Relic. Distributed tracing: Max spans per minute per account Dependent on agreement. Max limit: 2M. Distributed tracing: Max spans per trace 50K Distributed tracing: Max attributes per span 200 Rate of metric timeslice data (used by APM, browser, mobile) Ingest: 2 million per minute Rate of names: 4 million per minute per account Number per monitored app: 300K Mobile monitoring: number of crashes reported 10K per hour Infrastructure agents, integrations Number of infrastructure agents and/or integrations: 5K per account Gross number of new monitored containers: 5K per hour per account Query limits NRDB records * inspected: 100 billion per account per hour Rate of queries: 20 per account per second See other query limits * NRDB records refers to database records for our core data types, which includes events, metrics (dimensional), logs, and distributed tracing (span) data, all stored in the New Relic database (NRDB). This does not include metric timeslice data. Data ingest APIs Our ingest APIs have additional limits that may override the more general account-level limits. Note that these limits also apply to our tools that use these APIs (like our Telemetry SDKs or our open source telemetry integrations). Metric API (dimensional metrics) Event API Log API Trace API Other agent and integration limits To find limits for our other agents and integrations, which will override more general account-level limits, see the docs for those tools: you can search our solutions here. Some default reporting limits are located in these tools' configuration docs. Manage data Want to manage your New Relic data ingest and storage to improve data organization or reduce billing? See Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 205.9395,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic data <em>usage</em> limits and policies",
        "sections": "New Relic data <em>usage</em> limits and policies",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "This document lists some important account-level limits and links to other limit-related docs. Overview of limits We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data <em>usage</em> spikes in one New Relic account from impacting other"
      },
      "id": "603eb1c528ccbc0311eba7c7"
    },
    {
      "sections": [
        "The People's Republic of China"
      ],
      "title": "The People's Republic of China",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "c3c078b8139d695b928d2001cd6c6c9318c43599",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/peoples-republic-china/",
      "published_at": "2021-10-08T06:53:20Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important Information Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory requirements in the PRC. New Relic does not provide support for, including but not limited to, the deployment, access, or use of the Service(s) or Software in the PRC, or otherwise in respect of assets in the PRC (“PRC Use”). Without overriding any express prohibitions that you may have agreed to as part of your agreement with New Relic, you (and you as an agent of the respective New Relic Customer (\"Customer\")) acknowledge that any PRC use is subject to the PRC national firewall system and may be subject to outages and other interference outside of the control of New Relic. Accordingly, you acknowledge and agree (on behalf of yourself and including any Customer on whose behalf you use the Service(s) or Software) that any PRC Use is at your sole risk and is fully excepted from all representations and warranties, including, but not limited to, any terms that assure the confidentiality, integrity, availability or privacy of your data. New Relic makes no other representations and hereby expressly disclaims any and all warranties in respect of PRC Use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 192.49605,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "Important <em>Information</em> Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory"
      },
      "id": "603eb3a328ccbcb488eba76b"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies": [
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Important",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-10-07T03:33:01Z",
      "updated_at": "2021-09-07T23:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Important NOTE: If you are a New Relic HIPAA customer, please be advised that you must follow the requirements specified in the Global Technical Support Section of HIPAA enablement - what you need to know and do when requesting support and engaging with the New Relic Global Technical Support team for assistance. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customer’s paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customer’s paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relic’s distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relic’s Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and we’re here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relic’s Open Source Community Plus Projects. New Relic’s Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relic’s Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relic’s Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered “generally available.” If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we can’t help with things we didn’t build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and we’ll help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. We’ll do our best to determine whether an issue is with New Relic’s Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the “account owner,” such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relic’s Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Important As of March 10, 2021, you're no longer able to create a New Relic support ticket by emailing support@newrelic.com. If you have ticketed support as part of your subscription, when logged into your New Relic account, go to one.newrelic.com to create a support ticket. Once you're there, follow these steps: Click the ? icon in the upper right hand corner. Click I need more help. Select Create a Support Ticket. By creating a support ticket directly within your account, it will speed up the support process and improve our team’s ability to troubleshoot your issue more effectively. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relic’s Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 233.08476,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " environments Troubleshooting problems with ingesting data into New Relic <em>General</em> <em>usage</em> and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments &amp; frameworks"
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "The People's Republic of China"
      ],
      "title": "The People's Republic of China",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "c3c078b8139d695b928d2001cd6c6c9318c43599",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/peoples-republic-china/",
      "published_at": "2021-10-08T06:53:20Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important Information Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory requirements in the PRC. New Relic does not provide support for, including but not limited to, the deployment, access, or use of the Service(s) or Software in the PRC, or otherwise in respect of assets in the PRC (“PRC Use”). Without overriding any express prohibitions that you may have agreed to as part of your agreement with New Relic, you (and you as an agent of the respective New Relic Customer (\"Customer\")) acknowledge that any PRC use is subject to the PRC national firewall system and may be subject to outages and other interference outside of the control of New Relic. Accordingly, you acknowledge and agree (on behalf of yourself and including any Customer on whose behalf you use the Service(s) or Software) that any PRC Use is at your sole risk and is fully excepted from all representations and warranties, including, but not limited to, any terms that assure the confidentiality, integrity, availability or privacy of your data. New Relic makes no other representations and hereby expressly disclaims any and all warranties in respect of PRC Use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 192.49605,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "Important <em>Information</em> Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory"
      },
      "id": "603eb3a328ccbcb488eba76b"
    },
    {
      "sections": [
        "Government addendum",
        "New Relic, Inc.Government Addendum"
      ],
      "title": "Government addendum",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "cfed51b7f4f7583476f56b2b574204d38493e882",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/government-addendum/",
      "published_at": "2021-10-08T06:51:37Z",
      "updated_at": "2021-03-16T04:21:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The below Government Addendum applies only to customers that are United States federal, state, or local government customers with an existing New Relic agreement in place that explicitly references this Government Addendum applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic, Inc.Government Addendum For purposes of this Agreement and to the extent applicable, the Service is \"commercial computer software\" and a \"commercially available off-the-shelf (COTS) item\" as defined at FAR 2.101 developed at the private expense of New Relic. If acquired by or on behalf of a civilian agency, the U.S. Government acquires this commercial computer software and/or commercial computer software documentation and other technical data subject to the terms of the Agreement as specified in 48 C.F.R. 12.212 (Computer Software) and 12.211 (Technical Data) of the Federal Acquisition Regulation (\"FAR\") and its successors. If acquired by or on behalf of any agency within the Department of Defense (\"DOD\"), the U.S. Government acquires this commercial computer software and/or commercial computer software documentation subject to the terms of the Agreement as specified in 48 C.F.R. 227.7202-3 of the DOD FAR Supplement (\"DFARS\") and its successors. This addendum is in lieu of and supersedes any other FAR, DFARS, or other clause or provision that addresses government rights in computer software or technical data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 192.49538,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>"
      },
      "id": "603ea47ee7b9d2dd0d2a07ef"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/new-relics-provision-services": [
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Important",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-10-07T03:33:01Z",
      "updated_at": "2021-09-07T23:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Important NOTE: If you are a New Relic HIPAA customer, please be advised that you must follow the requirements specified in the Global Technical Support Section of HIPAA enablement - what you need to know and do when requesting support and engaging with the New Relic Global Technical Support team for assistance. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customer’s paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customer’s paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relic’s distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relic’s Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and we’re here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relic’s Open Source Community Plus Projects. New Relic’s Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relic’s Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relic’s Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered “generally available.” If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we can’t help with things we didn’t build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and we’ll help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. We’ll do our best to determine whether an issue is with New Relic’s Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the “account owner,” such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relic’s Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Important As of March 10, 2021, you're no longer able to create a New Relic support ticket by emailing support@newrelic.com. If you have ticketed support as part of your subscription, when logged into your New Relic account, go to one.newrelic.com to create a support ticket. Once you're there, follow these steps: Click the ? icon in the upper right hand corner. Click I need more help. Select Create a Support Ticket. By creating a support ticket directly within your account, it will speed up the support process and improve our team’s ability to troubleshoot your issue more effectively. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relic’s Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 233.08476,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " environments Troubleshooting problems with ingesting data into New Relic <em>General</em> <em>usage</em> and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments &amp; frameworks"
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "New Relic data usage limits and policies",
        "Overview of limits",
        "View limits and manage data",
        "Account-level limits",
        "Data ingest APIs",
        "Other agent and integration limits",
        "Manage data"
      ],
      "title": "New Relic data usage limits and policies",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "fc32c25b40a030ffa0fad6bfc95be7fca1360ee1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies/",
      "published_at": "2021-10-08T06:52:18Z",
      "updated_at": "2021-07-21T21:09:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document lists some important account-level limits and links to other limit-related docs. Overview of limits We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers' accounts, we have various data volume and rate limits in place. We reserve the right to enforce these limits to protect our system and to avoid issues for you and other customers. If your New Relic account, whether by configuration or by error, exceeds one of these limits, it or its child accounts might experience one or both of the following: Sampling of data Temporary pause or cessation of data collection To learn more about how hitting a limit can affect your data, see View limits. If you have further questions about these limits, your contract, or a limit you've reached, contact your New Relic account representative. We can work with you to adjust any rate limits to meet your needs. View limits and manage data Want to understand your account’s limit violations using the UI? See View limits. Want to manage your data for organization or billing purposes? See Manage data. Account-level limits The following table includes general max limits that apply across all New Relic accounts. Specific New Relic tools, like agents and integrations, have their own limits and configurations, and might be lower than these theoretical maximum limits. Limited condition Limit Rate of NRDB record * ingest 55 million per account per minute Max NRDB records * ingested per API call 1MB (10^6 bytes) Max attribute value size 1KB (10^3 bytes) Max payload size 1MB (10^6 bytes) Max total attributes per data type (including default attributes) 254 (less for some tools; for example, 64 for agents) Number of unique custom data types 250 per account per day (applies to custom events because that's source of new data types) APM limits Agent instances: 50K per account Agent instances per app: 10K APM apps/services: 10K per second Browser: number of page views 1M per minute per app Distributed tracing: Max age of span timestamp values 20 minutes. Timestamp must be within 20 minutes of current time at ingest or within 20 minutes from the time the last span with the same trace.id was received by New Relic. Distributed tracing: Max spans per minute per account Dependent on agreement. Max limit: 2M. Distributed tracing: Max spans per trace 50K Distributed tracing: Max attributes per span 200 Rate of metric timeslice data (used by APM, browser, mobile) Ingest: 2 million per minute Rate of names: 4 million per minute per account Number per monitored app: 300K Mobile monitoring: number of crashes reported 10K per hour Infrastructure agents, integrations Number of infrastructure agents and/or integrations: 5K per account Gross number of new monitored containers: 5K per hour per account Query limits NRDB records * inspected: 100 billion per account per hour Rate of queries: 20 per account per second See other query limits * NRDB records refers to database records for our core data types, which includes events, metrics (dimensional), logs, and distributed tracing (span) data, all stored in the New Relic database (NRDB). This does not include metric timeslice data. Data ingest APIs Our ingest APIs have additional limits that may override the more general account-level limits. Note that these limits also apply to our tools that use these APIs (like our Telemetry SDKs or our open source telemetry integrations). Metric API (dimensional metrics) Event API Log API Trace API Other agent and integration limits To find limits for our other agents and integrations, which will override more general account-level limits, see the docs for those tools: you can search our solutions here. Some default reporting limits are located in these tools' configuration docs. Manage data Want to manage your New Relic data ingest and storage to improve data organization or reduce billing? See Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 205.9395,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic data <em>usage</em> limits and policies",
        "sections": "New Relic data <em>usage</em> limits and policies",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "This document lists some important account-level limits and links to other limit-related docs. Overview of limits We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data <em>usage</em> spikes in one New Relic account from impacting other"
      },
      "id": "603eb1c528ccbc0311eba7c7"
    },
    {
      "sections": [
        "The People's Republic of China"
      ],
      "title": "The People's Republic of China",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "c3c078b8139d695b928d2001cd6c6c9318c43599",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/peoples-republic-china/",
      "published_at": "2021-10-08T06:53:20Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important Information Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory requirements in the PRC. New Relic does not provide support for, including but not limited to, the deployment, access, or use of the Service(s) or Software in the PRC, or otherwise in respect of assets in the PRC (“PRC Use”). Without overriding any express prohibitions that you may have agreed to as part of your agreement with New Relic, you (and you as an agent of the respective New Relic Customer (\"Customer\")) acknowledge that any PRC use is subject to the PRC national firewall system and may be subject to outages and other interference outside of the control of New Relic. Accordingly, you acknowledge and agree (on behalf of yourself and including any Customer on whose behalf you use the Service(s) or Software) that any PRC Use is at your sole risk and is fully excepted from all representations and warranties, including, but not limited to, any terms that assure the confidentiality, integrity, availability or privacy of your data. New Relic makes no other representations and hereby expressly disclaims any and all warranties in respect of PRC Use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 192.49605,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "Important <em>Information</em> Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory"
      },
      "id": "603eb3a328ccbcb488eba76b"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/peoples-republic-china": [
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Important",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-10-07T03:33:01Z",
      "updated_at": "2021-09-07T23:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Important NOTE: If you are a New Relic HIPAA customer, please be advised that you must follow the requirements specified in the Global Technical Support Section of HIPAA enablement - what you need to know and do when requesting support and engaging with the New Relic Global Technical Support team for assistance. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customer’s paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customer’s paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relic’s distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relic’s Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and we’re here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relic’s Open Source Community Plus Projects. New Relic’s Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relic’s Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relic’s Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered “generally available.” If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we can’t help with things we didn’t build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and we’ll help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. We’ll do our best to determine whether an issue is with New Relic’s Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the “account owner,” such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relic’s Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Important As of March 10, 2021, you're no longer able to create a New Relic support ticket by emailing support@newrelic.com. If you have ticketed support as part of your subscription, when logged into your New Relic account, go to one.newrelic.com to create a support ticket. Once you're there, follow these steps: Click the ? icon in the upper right hand corner. Click I need more help. Select Create a Support Ticket. By creating a support ticket directly within your account, it will speed up the support process and improve our team’s ability to troubleshoot your issue more effectively. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relic’s Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 233.08475,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " environments Troubleshooting problems with ingesting data into New Relic <em>General</em> <em>usage</em> and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments &amp; frameworks"
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "New Relic data usage limits and policies",
        "Overview of limits",
        "View limits and manage data",
        "Account-level limits",
        "Data ingest APIs",
        "Other agent and integration limits",
        "Manage data"
      ],
      "title": "New Relic data usage limits and policies",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "fc32c25b40a030ffa0fad6bfc95be7fca1360ee1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies/",
      "published_at": "2021-10-08T06:52:18Z",
      "updated_at": "2021-07-21T21:09:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document lists some important account-level limits and links to other limit-related docs. Overview of limits We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers' accounts, we have various data volume and rate limits in place. We reserve the right to enforce these limits to protect our system and to avoid issues for you and other customers. If your New Relic account, whether by configuration or by error, exceeds one of these limits, it or its child accounts might experience one or both of the following: Sampling of data Temporary pause or cessation of data collection To learn more about how hitting a limit can affect your data, see View limits. If you have further questions about these limits, your contract, or a limit you've reached, contact your New Relic account representative. We can work with you to adjust any rate limits to meet your needs. View limits and manage data Want to understand your account’s limit violations using the UI? See View limits. Want to manage your data for organization or billing purposes? See Manage data. Account-level limits The following table includes general max limits that apply across all New Relic accounts. Specific New Relic tools, like agents and integrations, have their own limits and configurations, and might be lower than these theoretical maximum limits. Limited condition Limit Rate of NRDB record * ingest 55 million per account per minute Max NRDB records * ingested per API call 1MB (10^6 bytes) Max attribute value size 1KB (10^3 bytes) Max payload size 1MB (10^6 bytes) Max total attributes per data type (including default attributes) 254 (less for some tools; for example, 64 for agents) Number of unique custom data types 250 per account per day (applies to custom events because that's source of new data types) APM limits Agent instances: 50K per account Agent instances per app: 10K APM apps/services: 10K per second Browser: number of page views 1M per minute per app Distributed tracing: Max age of span timestamp values 20 minutes. Timestamp must be within 20 minutes of current time at ingest or within 20 minutes from the time the last span with the same trace.id was received by New Relic. Distributed tracing: Max spans per minute per account Dependent on agreement. Max limit: 2M. Distributed tracing: Max spans per trace 50K Distributed tracing: Max attributes per span 200 Rate of metric timeslice data (used by APM, browser, mobile) Ingest: 2 million per minute Rate of names: 4 million per minute per account Number per monitored app: 300K Mobile monitoring: number of crashes reported 10K per hour Infrastructure agents, integrations Number of infrastructure agents and/or integrations: 5K per account Gross number of new monitored containers: 5K per hour per account Query limits NRDB records * inspected: 100 billion per account per hour Rate of queries: 20 per account per second See other query limits * NRDB records refers to database records for our core data types, which includes events, metrics (dimensional), logs, and distributed tracing (span) data, all stored in the New Relic database (NRDB). This does not include metric timeslice data. Data ingest APIs Our ingest APIs have additional limits that may override the more general account-level limits. Note that these limits also apply to our tools that use these APIs (like our Telemetry SDKs or our open source telemetry integrations). Metric API (dimensional metrics) Event API Log API Trace API Other agent and integration limits To find limits for our other agents and integrations, which will override more general account-level limits, see the docs for those tools: you can search our solutions here. Some default reporting limits are located in these tools' configuration docs. Manage data Want to manage your New Relic data ingest and storage to improve data organization or reduce billing? See Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 205.9395,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic data <em>usage</em> limits and policies",
        "sections": "New Relic data <em>usage</em> limits and policies",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "This document lists some important account-level limits and links to other limit-related docs. Overview of limits We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data <em>usage</em> spikes in one New Relic account from impacting other"
      },
      "id": "603eb1c528ccbc0311eba7c7"
    },
    {
      "sections": [
        "Government addendum",
        "New Relic, Inc.Government Addendum"
      ],
      "title": "Government addendum",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "cfed51b7f4f7583476f56b2b574204d38493e882",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/government-addendum/",
      "published_at": "2021-10-08T06:51:37Z",
      "updated_at": "2021-03-16T04:21:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The below Government Addendum applies only to customers that are United States federal, state, or local government customers with an existing New Relic agreement in place that explicitly references this Government Addendum applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic, Inc.Government Addendum For purposes of this Agreement and to the extent applicable, the Service is \"commercial computer software\" and a \"commercially available off-the-shelf (COTS) item\" as defined at FAR 2.101 developed at the private expense of New Relic. If acquired by or on behalf of a civilian agency, the U.S. Government acquires this commercial computer software and/or commercial computer software documentation and other technical data subject to the terms of the Agreement as specified in 48 C.F.R. 12.212 (Computer Software) and 12.211 (Technical Data) of the Federal Acquisition Regulation (\"FAR\") and its successors. If acquired by or on behalf of any agency within the Department of Defense (\"DOD\"), the U.S. Government acquires this commercial computer software and/or commercial computer software documentation subject to the terms of the Agreement as specified in 48 C.F.R. 227.7202-3 of the DOD FAR Supplement (\"DFARS\") and its successors. This addendum is in lieu of and supersedes any other FAR, DFARS, or other clause or provision that addresses government rights in computer software or technical data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 192.49538,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>"
      },
      "id": "603ea47ee7b9d2dd0d2a07ef"
    }
  ],
  "/docs/licenses/license-information/other-licenses/services-licenses": [
    {
      "sections": [
        "Add-on end user license agreement",
        "New Relic, Inc. Add-on End User License Agreement"
      ],
      "title": "Add-on end user license agreement",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "a01c225ca30f95dab7db856cd946c76de557c31f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/add-end-user-license-agreement/",
      "published_at": "2021-10-08T06:49:32Z",
      "updated_at": "2021-03-16T06:19:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic, Inc. Add-on End User License Agreement In connection with the work provided by New Relic's Expert Services, Field Instrumentation Team, or Sales team, you may be provided with certain custom-created software to visualize, enable, optimize, or enhance your use of New Relic's Services. By downloading, installing, authorizing installation, or using the Add-on (defined below) with your Underlying Software (defined below), you (“Customer”) agree to the terms and conditions herein (“Agreement”) with New Relic, Inc., a Delaware corporation with offices located at 188 Spear Street, Suite 1200 San Francisco, CA 94105 (“New Relic\"), (collectively the “Parties”). Capitalized terms not defined herein shall have the meanings set forth in the New Relic Terms of Service (the “Terms of Service”) available here: newrelic.com/termsandconditions/terms. IF YOU DO NOT AGREE TO THIS AGREEMENT, YOUR SOLE REMEDY IS TO NOT USE THE ADD-ON. 1. DEFINITIONS “Add-on” means the New Relic applications and/or software, including but not limited to connectors, extensions, UI extension, and plugins, provided by New Relic to Customer to enable the Services to operate with certain third party or Customer software or systems (“Underlying Software”). For the purposes of the Terms of Service, the Add-on shall be treated like an Agent, subject to the separate terms herein. 2. USE OF THE PROGRAMMABILITY ADD-ON 2.1 Software Evaluation License. Subject to the terms herein, New Relic grants to Customer a limited, non-exclusive, non-transferrable, non-sublicensable right to install, use, and configure the Add-on solely as needed to enable the Services for internal evaluation purposes. New Relic reserves all rights and licenses not expressly granted herein. 2.2 Ownership. As between the Parties, New Relic owns all right, title, and interest to the Add-on and Feedback, including but not limited to any intellectual property and proprietary rights therein. Customer retains all right, title, and interest in any Customer Data processed by the Add-on in connection with the Services. 2.3 Feedback. Customer agrees to provide feedback, suggestions, ideas, requests or recommendations (“Feedback”) regarding the Add-on, and hereby irrevocably assigns all intellectual property and proprietary rights it holds in the Feedback to New Relic. 2.4 Notice. Customer acknowledges and agrees that the Add-on is made to work with the Underlying Software as configured at the time of creation and there is no guarantee that Add-on will continue to work in the event Customer changes, replaces, upgrades versions of, updates, or otherwise changes the Underlying Software (an “Upgrade Event”). Customer shall hold New Relic harmless from any claims or damages arising from Customer’s Upgrade Event. 2.5 Restrictions. Customer will not: (i) use the Add-on except as permitted hereunder; (ii) distribute, sell, sublicense, or otherwise transfer the Add-on; (iii) decompile, disassemble or reverse engineer any software underlying the Add-on; (iv) use the Add-on to damage, disable, overburden or impair any New Relic server or network(s) connected to any New Relic server or interfere with any other party’s use and enjoyment of the Services; (v) use the Add-on to defraud, defame, abuse, harass, stalk, threaten or infringe the rights of privacy or other intellectual property rights of others or otherwise violate any applicable law; (vi) circumvent or modify any security technologies included as part of the Add-on; or (vii) attempt or permit any third party to do any of the foregoing. New Relic may suspend Customer usage of the Services, without notice, pending any investigation of misuse. These restrictions may be supplemented or superceded (to the extent they conflict) by the New Relic Acceptable Use Policy as may be published and updated from time to time on the New Relic web site at docs.newrelic.com/docs/licenses/license-information/acceptable-use-policy/acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential Information” means: (a) the Programmability Add-on, and any features, results or output produced by, and other information relating to the Add-on (including, without limitation, all Feedback); and (b) any business or technical information of New Relic including but not limited to any technical information, research, development, know-how that a reasonable person would understand to be confidential. 3.2 Restrictions. Customer will not use or disclose any Confidential Information, except as necessary for the performance of this Agreement. Customer will use all reasonable efforts to protect Confidential Information from unauthorized use or disclosure, but in no event less than the efforts that it ordinarily uses with respect to its own proprietary information. Customer may disclose Confidential Information to those of its employees who have a bona fide need to know such Confidential Information for the performance of this Agreement; provided that each such employee first executes a written agreement that contains use and nondisclosure restrictions at least as protective as those set forth herein. Confidential Information shall not include any information that: (a) is or becomes generally known to the public through no fault or breach of this Agreement by Customer; (b) is rightfully known by Customer at the time of disclosure without an obligation of confidentiality; (c) is independently developed by Customer without access or use of any Confidential Information; or (d) is rightfully obtained from a third party without restriction on use or disclosure. 4. DISCLAIMER 4.1 DISCLAIMER. THE ADD-ON IS PROVIDED AS-IS AND AS-AVAILABLE AND NEW RELIC DISCLAIMS AND MAKES NO WARRANTIES, EITHER EXPRESS OR IMPLIED, INCLUDING ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR NON-INFRINGEMENT AND WHETHER OR NOT ARISING THROUGH A COURSE OF DEALING. THE ADD-ON IS NOT GUARANTEED TO BE ERROR-FREE, COMPATIBLE WITH THE UNDERLYING SOFTWARE, OR THAT CUSTOMER WILL ACHIEVE ANY RESULTS FROM USE OF THE ADD-ON THEREFROM. 4.2 LIMITATION OF LIABILITY. TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC, ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE TO CUSTOMER OR ANY THIRD PARTY FOR DAMAGES OF ANY KIND, INCLUDING, WITHOUT LIMITATION, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE INSTALLATION, USE OR INABILITY TO USE THE ADD-ON OR FOR ANY ERROR OR DEFECT IN THE ADD-ON OR THE SERVICES, REGARDLESS OF LEGAL THEORY, WHETHER OR NOT NEW RELIC HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSS OR DAMAGE. NEW RELIC’S AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS AGREEMENT WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE PARTIES HAVE AGREED THAT THESE LIMITATIONS WILL SURVIVE AND APPLY EVEN IF ANY LIMITED REMEDY SPECIFIED IN THIS AGREEMENT IS FOUND TO HAVE FAILED OF ITS ESSENTIAL PURPOSE. THE LIMITATIONS OF DAMAGES SET FORTH ABOVE ARE FUNDAMENTAL ELEMENTS OF THE BASIS OF THE BARGAIN BETWEEN NEW RELIC AND CUSTOMER. 4.3 EVALUATION VERSION. Customer agrees and acknowledges that: (a) the Add-on is not an official product and has not been commercially released for sale by New Relic; (b) the Add-on may not operate properly, being in final form, or fully functional; (c) the Add-on may contain errors, security vulnerabilities, design flaws, or other problems; (d) it may not be possible to make the Add-on fully functional; (e) the information obtained using the Add-on may not be accurate; (f) use of the Add-on may result in unexpected results, loss of data, delays or other unpredictable damages or loss; (g) New Relic is under no obligation to release a commercial version of the Add-on; and (h) New Relic has the right unilaterally to abandon development of the Add-on, at any time and without any obligation or liability to Customer. 5. GENERAL PROVISIONS 5.1 Terms of Service; Documentation. This Agreement shall be considered a part of the New Relic documentation, located at: https://docs.newrelic.com (the “Documentation). This Agreement and the Terms of Service constitute the entire and exclusive agreement between New Relic and Customer with respect to the Add-on. In the absence of a separate agreement, upon agreement between New Relic and Customer with respect to the Add-On, the terms and conditions of this Agreement shall govern the relationship between New Relic and Customer with respect to such Add-On. To the extent of a conflict between the Agreement and the Terms of Service, this Agreement shall govern with respect to the Add-on only.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 172.01904,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Add-on end user <em>license</em> agreement",
        "sections": "Add-on end user <em>license</em> agreement",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": "&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;acceptable-use-policy&#x2F;acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential <em>Information</em>” means: (a) the Programmability Add-on, and any features, results or output produced by, and other <em>information</em> relating to the Add-on (including, without limitation"
      },
      "id": "603ec23328ccbccf1beba79a"
    },
    {
      "sections": [
        "FIT instrumentation end user license agreement"
      ],
      "title": "FIT instrumentation end user license agreement",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "e8e10baf120678407d08c9f78ab708d271cf0223",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/fit-instrumentation-end-user-license-agreement/",
      "published_at": "2021-10-08T06:50:31Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "NEW RELIC, INC. FIT INSTRUMENTATION END USER LICENSE AGREEMENT In connection with the work provided by New Relic’s Field Instrumentation Team, you may be provided with certain custom-created software to enable, optimize, or enhance your use of New Relic’s Services. By downloading, installing, authorizing installation, or using the FIT Instrumentation with your Underlying Software (defined below), you (“Customer”) agree to the terms and conditions herein (“Agreement”) with New Relic, Inc., a Delaware corporation with offices located at 188 Spear Street, Suite 1200 San Francisco, CA 94105 (“New Relic”), (collectively the “Parties”). Capitalized terms not defined herein shall have the meanings set forth in the New Relic Terms of Service (the “Terms of Service”) available here: http://newrelic.com/terms IF YOU DO NOT AGREE TO THIS AGREEMENT, PLEASE DO NOT USE THE FIT INSTRUMENTATION. 1. DEFINITIONS “FIT Instrumentation” means the New Relic custom-made software, including but not limited to connectors, extensions, and plugins, provided by New Relic to Customer to enable the Services to operate with certain third party or Customer software or systems (“Underlying Software”). For the purposes of the Terms of Service, the FIT Instrumentation shall be treated like an “Agent”, subject to the separate terms herein. 2. USE OF THE FIT INSTRUMENTATION 2.1 Software Evaluation License. Subject to the terms herein, New Relic grants to Customer a limited, non-exclusive, non-transferrable, non-sublicensable right to install, use, and configure the FIT Instrumentation solely as needed to enable the Services for internal evaluation purposes. New Relic reserves all rights and licenses not expressly granted herein. 2.2 Ownership. As between the Parties, New Relic owns all right, title, and interest to the FIT Instrumentation and Feedback, including but not limited to any intellectual property and proprietary rights therein. Customer retains all right, title, and interest in any Customer Data processed by the FIT Instrumentation in connection with the Services. 2.3 Feedback. Customer agrees to provide feedback, suggestions, ideas, requests or recommendations (“Feedback”) regarding the FIT Instrumentation, and hereby irrevocably assigns all intellectual property and proprietary rights it holds in the Feedback to New Relic. 2.4 Notice. Customer acknowledges and agrees that the FIT Instrumentation is made to work with the Underlying Software as configured at the time of creation and there is no guarantee that FIT Instrumentation will continue to work in the event Customer changes, replaces, upgrades versions of, updates, or otherwise changes the Underlying Software (an “Upgrade Event”). Customer shall hold New Relic harmless from any claims or damages arising from Customer’s Upgrade Event. 2.5 Restrictions. Customer will not: (i) use the FIT Instrumentation except as permitted hereunder; (ii) distribute, sell, sublicense, or otherwise transfer the FIT Instrumentation; (iii) decompile, disassemble or reverse engineer any software underlying the FIT Instrumentation; (iv) use the FIT Instrumentation to damage, disable, overburden or impair any New Relic server or network(s) connected to any New Relic server or interfere with any other party’s use and enjoyment of the Services; (v) use the FIT Instrumentation to defraud, defame, abuse, harass, stalk, threaten or infringe the rights of privacy or other intellectual property rights of others or otherwise violate any applicable law; (vi) circumvent or modify any security technologies included as part of the FIT Instrumentation; or (vii) attempt or permit any third party to do any of the foregoing. New Relic may suspend usage of the Services by any user, without notice, pending any investigation of misuse.  These restrictions may be supplemented or superceded (to the extent they conflict) by the New Relic Acceptable Use Policy as may be published and updated from time to time at https://docs.newrelic.com/docs/licenses/license-information/acceptable-use-policy/acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential Information” means: (a) the FIT Instrumentation, and any features, results or output produced by, and other information relating to the FIT Instrumentation (including, without limitation, all Feedback); and (b) any business or technical information of New Relic including but not limited to any technical information, research, development, know-how that a reasonable person would understand to be confidential. 3.2 Restrictions. Customer will not use or disclose any Confidential Information, except as necessary for the performance of this Agreement. Customer will use all reasonable efforts to protect Confidential Information from unauthorized use or disclosure, but in no event less than the efforts that it ordinarily uses with respect to its own proprietary information. Customer may disclose Confidential Information to those of its employees who have a bona fide need to know such Confidential Information for the performance of this Agreement; provided that each such employee first executes a written agreement that contains use and nondisclosure restrictions at least as protective as those set forth herein. Confidential Information shall not include any information that: (a) is or becomes generally known to the public through no fault or breach of this Agreement by Customer; (b) is rightfully known by Customer at the time of disclosure without an obligation of confidentiality; (c) is independently developed by Customer without access or use of any Confidential Information; or (d) is rightfully obtained from a third party without restriction on use or disclosure. 4. DISCLAIMER 4.1 DISCLAIMER. FIT INSTRUMENTATION IS PROVIDED AS-IS AND AS-AVAILABLE AND NEW RELIC DISCLAIMS AND MAKES NO WARRANTIES, EITHER EXPRESS OR IMPLIED, INCLUDING ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR NON-INFRINGEMENT AND WHETHER OR NOT ARISING THROUGH A COURSE OF DEALING. FIT INSTRUMENTATION IS NOT GUARANTEED TO BE ERROR-FREE, COMPATIBLE WITH THE UNDERLYING SOFTWARE. , OR THAT CUSTOMER WILL ACHIEVE ANY RESULTS FROM USE OF THE FIT INSTRUMENTATION THEREFROM. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OF IMPLIED WARRANTIES OR LIMITATIONS ON APPLICABLE STATUTORY RIGHTS OF A CONSUMER, SO THE ABOVE EXCLUSION AND LIMITATIONS MAY NOT APPLY TO THE CUSTOMER. 4.2 LIMITATION OF LIABILITY. TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE TO CUSTOMER OR ANY THIRD PARTY FOR DAMAGES OF ANY KIND, INCLUDING, WITHOUT LIMITATION, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE INSTALLATION, USE OR INABILITY TO USE THE FIT INSTRUMENTATION OR FOR ANY ERROR OR DEFECT IN THE FIT INSTRUMENTATION OR THE SERVICES, WHETHER SUCH LIABILITY ARISES FROM ANY CLAIM BASED UPON CONTRACT, WARRANTY, TORT, STRICT LIABILITY, OR OTHERWISE, WHETHER OR NOT NEW RELIC HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSS OR DAMAGE. NEW RELIC'S AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS AGREEMENT WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE PARTIES HAVE AGREED THAT THESE LIMITATIONS WILL SURVIVE AND APPLY EVEN IF ANY LIMITED REMEDY SPECIFIED IN THIS AGREEMENT IS FOUND TO HAVE FAILED OF ITS ESSENTIAL PURPOSE. 4.3 EVALUATION VERSION. Customer agrees and acknowledges that: (a) the FIT Instrumentation is not an official product and has not been commercially released for sale by New Relic; (b) the FIT Instrumentation may not operate properly, being in final form, or fully functional; (c) the FIT Instrumentation may contain errors, security vulnerabilities, design flaws, or other problems; (d) it may not be possible to make the FIT Instrumentation fully functional; (e) the information obtained using the FIT Instrumentation may not be accurate; (f) use of the FIT Instrumentation may result in unexpected results, loss of data, delays or other unpredictable damages or loss; (g) New Relic is under no obligation to release a commercial version of the FIT Instrumentation; and (h) New Relic has the right unilaterally to abandon development of the FIT Instrumentation, at any time and without any obligation or liability to Customer. 5. GENERAL PROVISIONS 5.1 Terms of Service; Documentation. This Agreement shall be considered a part of the New Relic documentation, located at: https://docs.newrelic.com (the \"Documentation\").This Agreement and the Terms of Service constitute the entire and exclusive agreement between New Relic and Customer with respect to the FIT Instrumentation. In the absence of a separate agreement between New Relic and Customer with respect to the FIT Instrumentation, the terms and conditions of this Agreement shall govern the relationship between New Relic and Customer with respect to such FIT Instrumentation.  To the extent of a conflict between the Agreement and the Terms of Service, this Agreement shall govern with respect to the FIT Instrumentation only.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 172.01585,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "FIT instrumentation end user <em>license</em> agreement",
        "sections": "FIT instrumentation end user <em>license</em> agreement",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": " Acceptable Use Policy as may be published and updated from time to time at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;acceptable-use-policy&#x2F;acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential <em>Information</em>” means: (a) the FIT Instrumentation, and any features, results"
      },
      "id": "603e9f3ee7b9d206e32a0800"
    },
    {
      "sections": [
        "New Relic Agent Software Notice"
      ],
      "title": "New Relic Agent Software Notice",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "2bf9501c2767105130d3808f1bf3a91a032d903e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/new-relic-agent-software-notice/",
      "published_at": "2021-10-08T06:50:31Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This software is © 2008-2021 New Relic, Inc. and its licensors. This software is solely for use with New Relic’s proprietary SaaS service (“New Relic Service”), so to use the software you must have a valid account for the New Relic Service under a separate agreement with New Relic (“Subscription Agreement”). You may only use the software to support your use of the New Relic Service as permitted in the Subscription Agreement. Without a Subscription Agreement, you may not use the software. All other use is prohibited. New Relic and its suppliers retain all right, title and interest (including intellectual property rights) in the software. The Subscription Agreement will control in event of a conflict with this notice. Unless otherwise agreed by New Relic in your Subscription Agreement: You may not use, copy, distribute or sublicense the software, use the software on behalf of third parties, reverse engineer or decompile the software, modify or create derivative works of the software, use the software for competitive analysis or benchmarking, or remove or obscure any proprietary notices in the software. The software is provided “AS IS” and New Relic disclaims all warranties, whether express, implied, statutory or otherwise, including warranties of merchantability, fitness for a particular purpose, title or noninfringement. To the full extent permitted by law, New Relic will have no liability arising from or related to the software or under this notice for any direct, indirect, special, incidental, or consequential damages of any kind, even if advised of their possibility in advance, and regardless of legal theory (whether contract, tort, negligence, strict liability or otherwise). The software may contain third-party open source software (“OSS”) as described here and at https://github.com/newrelic. To the extent required by the OSS license, that license will apply to the OSS when used on a stand-alone basis. For avoidance of doubt, you may copy and distribute New Relic agents pursuant to your Subscription Agreement for New Relic Browser and New Relic Mobile and as set forth: https://docs.newrelic.com/docs/mobile-monitoring/new-relic-mobile/get-started and https://docs.newrelic.com/docs/browser/new-relic-browser/installation/install-new-relic-browser-agent Software versions New Relic makes available under an OSS license (such as Apache 2.0) are governed by the terms of the applicable OSS license. For a current list of New Relic software versions released as OSS please visit https://opensource.newrelic.com/.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 172.01585,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": " and at https:&#x2F;&#x2F;github.com&#x2F;newrelic. To the extent required by the OSS <em>license</em>, that <em>license</em> will apply to the OSS when used on a stand-alone basis. For avoidance of doubt, you may copy and distribute New Relic agents pursuant to your Subscription Agreement for New Relic Browser and New Relic Mobile"
      },
      "id": "603eb73828ccbc1f99eba74a"
    }
  ],
  "/docs/licenses/license-information/product-definitions/legacy-product-definitions": [
    {
      "sections": [
        "New Relic One pricing: Definitions",
        "Account",
        "Commitment Term",
        "Customer Data",
        "Customer Properties",
        "Documentation",
        "GB Ingested",
        "Incident event",
        "Login Credentials",
        "Monthly Provisioned User",
        "Order",
        "Paid Terms of Service",
        "Product(s)",
        "Software",
        "Terms",
        "Third-Party Services",
        "Unpaid Terms of Service",
        "Usage Plan"
      ],
      "title": "New Relic One pricing: Definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "609575acd671fecf7899378157eabc57bc8d68e2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/new-relic-one-pricing-definitions/",
      "published_at": "2021-10-07T01:58:44Z",
      "updated_at": "2021-05-22T17:25:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our New Relic One pricing plan (for a glossary for our original pricing, see Original pricing definitions. Account Account refers to the online account or subaccounts that New Relic provides for customers to manage their use of the Products. Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, information, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer Properties Customer Properties means Customer’s websites, infrastructure, networks, mobile applications, or other systems, as well as Customer accounts on Third-Party Services. Documentation Documentation means the New Relic technical guides and documentation made available from the dedicated ‘Documentation’ page of the New Relic website. GB Ingested A GB Ingested is a measurement of the volume of metrics, events, logs, traces, or other telemetry data sent to or generated by the Products for the benefit of the Customer, including from the Software, the Customer Properties, or Third-Party Services. In this context, a GB is defined as 1 billion bytes. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Login Credentials Login Credentials means the username, email address, password, or other personal information that is provided by a Customer user in order to manage an Account. Monthly Provisioned User A Monthly Provisioned User is any user who can log into Customer’s Account(s) and access the New Relic One Product functionality as specified in an Order and the Documentation. In our public docs, this is referred to as a full user. Order Order means the purchasing order for access to the Service or related services that: (1) is either executed by the Parties or entered into by you via self-service, and references this Agreement, or (2) is entered into by you and a Channel Partner. Paid Terms of Service Paid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/paid. Product(s) Product(s) mean the purchase of the New Relic subscription products described in the applicable Order and any updates, corrections, bug fixes, modifications, improvements, related services, new features, and functionality (made generally available to New Relic’s customer base) thereto. Software Software means the distributed software, APIs, scripts, or other code proprietary to New Relic provided with the Products. Terms Terms means the underlying Customer-New Relic agreement and the Order. Third-Party Services Third-Party Services means any third party platform, add-on, service, or product not provided by New Relic and that a user integrates or enables for use with the Products, including third-party applications and plug-ins. Unpaid Terms of Service Unpaid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/unpaid. Usage Plan Usage Plan refers to the Service or Product pricing, invoicing related information, and product-specific terms (e.g. concurrent user account sessions) contained within the Documentation. To learn more about this pricing plan, see New Relic One pricing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 191.93542,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic One pricing: <em>Definitions</em>",
        "sections": "New Relic One pricing: <em>Definitions</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " pricing, invoicing related <em>information</em>, and <em>product</em>-specific terms (e.g. concurrent user account sessions) contained within the Documentation. To learn more about this pricing plan, see New Relic One pricing."
      },
      "id": "6044e6e528ccbc26f22c6084"
    },
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Purpose",
        "2. Security Overview",
        "3.Security Certifications",
        "4. Data Control and Encryption",
        "5. Facilities",
        "6. Employee Access, Screening and Controls.",
        "7. Security Incident and Data Breach Response",
        "8. Network and Systems Security",
        "9. Authentication and Access Management",
        "10. Vulnerability Management",
        "11. System Access and Logging",
        "12. Disaster Recovery and Data Backup",
        "13. Copies and Removal",
        "14. Third Party Vendor Management",
        "15. Disclosure by Law",
        "16. Updates"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2021-10-08T06:55:04Z",
      "updated_at": "2021-09-27T15:05:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Updated 23 September 2021. The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Purpose This Policy describes New Relic's security program and the technical and organizational security controls to protect New Relic’s systems as shown in New Relic’s third-party audits and certifications. New Relic may update this Policy from time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/. 2. Security Overview 2.1. New Relic maintains a comprehensive Information Protection Program to manage information security within New Relic that includes administrative, technical, and physical safeguards designed to protect the confidentiality, integrity, and availability of Customer Data and that is appropriate to the nature, size, and complexity of New Relic's business operations. New Relic’s Information Protection Program includes: 2.1.1. executive review, support and accountability for all security related policies and practices; 2.1.2. formal written policies and procedures that are designed to protect against the loss, theft, or other unauthorized access or alteration of Customer Data, meet or exceed applicable industry standards, outlines a definition of information security and its overall objectives, include defined information security roles and responsibilities, a framework for setting control objectives and controls, a formal and effective risk mitigation program and a service provider security management program; 2.1.3. periodic risk assessments, including internal audits and/or independent 3rd party audits, to measure the effectiveness and appropriateness of controls for all systems processing Customer Data; 2.1.4. in-depth review of security incidents, including effective determination of root cause and corrective action; formal, industry-recognized controls frameworks based on an external audit standard; 2.1.5. employee screening and access management; and 2.1.6. comprehensive security testing, vulnerability identification, and mitigation methodology that consists of a variety of independent approaches that, when combined, are designed to maximize coverage for a diverse set of attack vectors. 2.2. New Relic's Chief Information Security Officer (CISO) leads New Relic's Information Security Program and develops, reviews, and approves, with appropriate stakeholders, New Relic's security policies and procedures. New Relic has appointed data protection officer(s) as described in New Relic’s Privacy Notices who are consulted as necessary under applicable laws. 2.3. Information security objectives, approach, scope, importance, goals, and principles for the organization’s security program are formally identified, communicated throughout the organization to users in a form that is relevant, accessible, and understandable to the intended reader; and supported by a controls framework that considers legislative, regulatory, contractual requirements, and other policy-related requirements. 2.4. New Relic will maintain written policies and procedures to review, test, and approve (as appropriate) changes affecting New Relic infrastructure and systems that process Customer Data including, but not limited to, peer reviews prior to introducing new code into production. New Relic will establish acceptance criteria for new information systems, upgrades, and new versions and will carry out suitable tests of the system(s) during development and prior to acceptance. Any changes or updates will not materially decrease the security of the systems. 3.Security Certifications 3.1. New Relic regularly tests, assesses, and evaluates its security measures for protecting Customer Data using industry-recognized standards and uses independent third-party auditors to verify such controls. 3.2. New Relic agrees to provide Customer, upon request, with applicable certifications or reports about New Relic systems. All information exchanged in connection with the audit activities described in this section is deemed to be the Confidential Information of New Relic. 3.3. Additional information about New Relic’s security certifications are available on New Relic’s Security Guide. 4. Data Control and Encryption 4.1. The Service and related features are designed to provide Customer with control over Customer’s data sources and Customer’s environments that are monitored and sending data to New Relic. 4.2. New Relic will have established methods to: (i) enable Customer to use encryption on Customer Data in transit, and (ii) securely hash passwords in storage following industry standard practices (e.g. scrypt) as described in the Documentation. Server certificate-based authentication is used as part of the TLS encryption with a trusted certificate authority. 4.3. New Relic receives and processes data in accordance with the Agreement and the Services as described in the Documentation. New Relic permits customers to delete personal data in accordance with applicable privacy laws as further described in the Documentation. In the event of error in personal data sent in Customer Data Customers may request personal data deletion and re-send data that is accurate. 4.4. Additional information for Customer data control and encryption, including encryption of data at rest, is available on New Relic’s Security Guide. 5. Facilities 5.1. All physical locations that process Customer Data are co-locations and third party data centers. All physical locations have security measures in place that are designed to prevent unauthorized physical access to data processing facilities and unlawful access, modification, or destruction of Customer Data. 5.2. New Relic will seek assurances (e.g., in the form of an independent 3rd party audit report such as the SOC 2 Type 2, ISO 27001, and vendor security evaluations) from its data processing facilities vendors that store or process Customer Data: 5.2.1. secure its data process facilities in an access-controlled location and have protections in place to prevent unauthorized access, damage, and interference; 5.2.2 employ physical security appropriate to the classification of the assets and information being managed which may include card key access, security cameras, and solid wall construction for all exterior walls; 5.2.3 limit and screen all entrants, which may include measures such as on-site security guard, badge reader, electronic lock, or a monitored closed caption television (CCTV); and 5.2.4 maintain relevant access logs. 5.3. Additional information about New Relic’s third party data centers are available on New Relic’s Security Guide. 6. Employee Access, Screening and Controls. New Relic will have and maintain policies and practices that include, at a minimum, the following controls and safeguards applied to New Relic employees and contractors who may access Customer Data: 6.1. For U.S. New Relic employees and subject to applicable law, a criminal background screening of each of its new employees to whom it gives access to Customer Data at the federal, state, and county levels. For non-U.S. New Relic employees, New Relic will use commercially reasonable efforts to meet the same criteria as established for U.S.-based New Relic employees, subject to general business practices in the respective country and in compliance with applicable local law requirements; 6.2. All New Relic employees with access to Customer Data will undergo adequate training, such as annual security awareness training, in the care, protection and handling of Customer Data, and will align with the privacy and security measures set out in this Addendum by following the guidance provided in their welcome package, which includes New Relic’s security policies and a security acknowledgement; 6.3. A disciplinary policy and process, to be used when New Relic employees violate New Relic security or privacy policies or access Customer Data without prior authorization; 6.4. Administrative or remote access to New Relic systems that process Customer Data will align with industry standard practices, including multi-factor authentication; 6.5. Restricted access to New Relic proprietary source code to prevent unauthorized access; 6.6. Controls designed to ensure that only those New Relic employees with an actual need-to-know will have access to New Relic systems including, but not limited to, the use of a formal access management process for the request, review, approval, and provisioning; 6.7. Controls designed to ensure that New Relic employees are granted access to New Relic systems based on least-privilege principles; and 6.8. Revoke access to New Relic employees no longer requiring access. 7. Security Incident and Data Breach Response 7.1. New Relic will take appropriate physical, technical, and administrative security measures that are commercially reasonable and consistent with industry standards to prevent a Data Breach, and as required by any applicable law or regulation. “Data Breach” means the theft, loss, or unauthorized access of Customer Data. Without limiting the foregoing, New Relic will implement security measures at least as stringent as those set out in this Addendum. New Relic will designate a senior representative to provide incident briefings, as needed in case of a Data Breach, and to respond to reasonable requests by Customer pertaining to privacy and data security issues within a commercially reasonable time frame. 7.2. New Relic will: 7.2.1. Notify Customer without undue delay if New Relic becomes aware of a Data Breach; 7.2.2. Maintain a security incident response plan, including procedures and means to respond in a manner consistent with industry standards; 7.2.3. Reasonably cooperate with Customer to investigate and remediate a Data Breach and mitigate any further risk to the Customer Data, or risk to data subjects and/or Customer’s reputation or brand; 7.2.4. Provide reasonable assistance to Customer at New Relic’s sole cost and expense; and 7.2.5. Make commercially reasonable efforts to preserve evidence and reasonably cooperate with Customer and legal authorities (as applicable and legally permissible) during an investigation of a Data Breach. 8. Network and Systems Security 8.1. All extranet connectivity by New Relic personnel to systems processing Customer Data will be through secure remote connections. 8.2. Network segments connected to the Internet will be protected by secure access control mechanisms, such as a firewall configured to secure all devices behind it and properly addresses security concerns according to industry standard practices. 8.3. New Relic will have industry standard measures in place to actively monitor its systems and help detect a potential hostile attack, such as Network Intrusion Detection (NID) or Host Intrusion Detection (HID)/Prevention. 8.4. Applications, ports, services, and similar access points installed on a computer or network facility, which are not specifically required for business functionality, will be disabled or removed. 8.5. New Relic maintains configuration standards for authorized operating systems and software for systems that support processing of Customer Data. New Relic will establish and follow server configuration guidelines and processes for preventing unauthorized access to Customer Data. New Relic maintains secure images or templates for systems based on the organization’s approved configuration standards. 8.6. Development, test, and operational environments will be logically separated to reduce the risks of unauthorized access or changes to the operational system. 8.7. New Relic network architecture will be designed to limit site access and restrict the availability of information systems that are considered to be vulnerable to attack. 8.8. New Relic provides appropriate TLS certificates when users access and view Customer Data in the Service. New Relic’s software for sending Customer Data to New Relic will encrypt Customer Data in transit by default. 9. Authentication and Access Management If you subscribe to the requisite New Relic One Service, New Relic will provide industry standard authentication and access controls to protect Customer Data, including industry standard authentication methods utilized to help prevent unauthorized access to the Service. New Relic’s access control methods will clearly state the rules and rights for each user or group of users including applications and information sharing and will include a process for granting and removing access to all information systems processing Customer Data. A record of all privileges allocated will be maintained pursuant to the requirements herein. 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. 10. Vulnerability Management New Relic will have and maintain the following vulnerability management processes for all devices used to connect to the New Relic network and Services. 10.1. New Relic will align to industry standard practices for build out, minimization of services and secure configuration, in accordance with, industry-recognized minimum security baselines for the New Relic platform and subcontractor systems connected to the New Relic platform in relation to the provision of the Services. 10.2. New Relic will employ industry-recognized standards and tools to conduct frequent infrastructure vulnerability scanning to test New Relic’s network and infrastructure and application penetration testing to test the New Relic Services. New Relic applies “Medium”, “High” and “Critical” security patches for all components in production and development environments as soon as commercially possible in accordance with its vulnerability management protocol, and consistent with industry standard practices and standards; 10.3. New Relic will have processes in place designed to ensure adherence to industry standard security development practices for development and testing for all code, APIs, and applications deployed and implemented in support of the Service; 10.4. New Relic will have and maintain solutions to identify and prevent malicious attackers or code from accessing or compromising Customer Data or systems that process Customer Data. These include, but are not limited to, software that identifies and removes malware and detects attempted intrusions. New Relic will have a security event and incident monitoring system and supporting processes to notify appropriate personnel in response to threats. 11. System Access and Logging 11.1. Access to New Relic’s systems will not be granted to employees of New Relic unless they have been uniquely identified and have sufficient credentials. 11.2. Access to New Relic’s systems will be logged and retained for no less than 6 months to assist in investigations and access control monitoring, including, but not limited to, end user access and activities, and information security events. 11.3. New Relic agrees to provide Customer the capability to access log records relating to Customer Accounts and the New Relic systems that process Customer Data in the event of a Data Breach or if required in connection with a law enforcement request. 12. Disaster Recovery and Data Backup 12.1. New Relic will have plans designed to respond to loss of services, which are tested and reviewed at least annually. This plan will include documented policies and procedures to restore service in the event of a service failure. 12.2. New Relic will establish and follow backup and restore procedures for Customer Data. 12.3. New Relic business continuity plan identifies critical systems. Annual disaster recovery tests are performed to check and restore customer data in the event of an incident. 12.4. New Relic will provide Customer with redacted copies of its plan(s) and evidence of tests/reviews upon request, but not more frequently than once annually, and subject to confidentiality requirements. 12.5. Additional information for Customer Data control and encryption are available on New Relic’s Security Guide. 13. Copies and Removal 13.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of the Agreement for any reason: (a) New Relic will, and will cause its personnel, to cease all access and use of any Customer Data, and (b) New Relic will delete all copies of Customer Data within ninety (90) days. 13.2. New Relic will maintain a process of ensuring secure destruction and deletion of all Customer Data, when reasonably requested by Customer or as otherwise provided in the Agreement. The process will include industry standard processes so that: (i) Customer Data cannot be practicably read or reconstructed, and (ii) the systems that store Customer Data are securely erased and/or decommissioned disks are destroyed. 14. Third Party Vendor Management New Relic may use third party vendors to provide the Services. New Relic performs a security risk-based assessment of prospective vendors before working with them to validate they meet New Relic’s security and business continuity standards, including the type of access and classification of data being accessed (if any), controls necessary to protect data, and legal/regulatory requirements. New Relic enters into written agreements with its vendors that process Customer Data which include confidentiality, privacy, and security obligations that provide an appropriate level of protection for Customer Data that these vendors may process for New Relic to maintain the security posture in this Policy, including following industry security standards. 15. Disclosure by Law In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic will (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. New Relic publishes its law enforcement requests report on New Relic’s Security Guide. 16. Updates As New Relic releases new products, services, functionality, and features, New Relic may update this Policy to account for such products, services, functionality, and features. For additional information, see our Security Guide.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 149.96313,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;referenced-policies&#x2F;security-policy&#x2F;. 2. Security Overview 2.1. New Relic"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    },
    {
      "sections": [
        "Security guide",
        "Tip",
        "Security Program",
        "Security Domains",
        "Security Certifications",
        "Data Control, Facilities, and Encryption",
        "Law Enforcement Request Report"
      ],
      "title": "Security guide",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "356f0d11ffcb62208a743a0a7c127f5f6da9c940",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-guide/",
      "published_at": "2021-10-07T02:08:45Z",
      "updated_at": "2021-09-19T15:21:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Last updated September 17, 2021. This is supplement to our security policy and serves as a guide to New Relic’s description of its Services, functionalities, and features. Tip We may update the URLs in this document without notice. Security Program New Relic follows \"privacy by design\" principles as described here: https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Security Domains New Relic’s policies and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless Security; Configuration Management; Vulnerability Management; Network Protection; Transmission Protection; Password Management; Access Control, Audit Logging & Monitoring; Education, Training, and Awareness; Third Party Assurance; Incident Management; Business Continuity and Disaster Recover; Risk Management; Data Protection & Privacy; and Service Management Systems. Security Certifications New Relic audits its Services against industry standards as described at https://docs.newrelic.com/docs/security/security-privacy/compliance/regulatory-audits-new-relic-services/. Data Control, Facilities, and Encryption New Relic's customers can send data to New Relic's APIs by (1) using New Relic's software, (2) using vendor-neutral software that is managed and maintained by a third-party such as via OpenTelemetry instrumentation provided by opentelemetry.io, or (3) from third-party systems that customer's manage and/or control. New Relic's customers can use New Relic's Services such as NerdGraph to filter out and drop data. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/drop-data-using-nerdgraph/. New Relic's customers can adjust their data retention periods as appropriate for their needs. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/manage-data-retention/#adjust-retention. New Relic Logs obfuscates numbers that match known patterns, such as bank card and social security numbers as described here: https://docs.newrelic.com/docs/logs/log-management/get-started/new-relics-log-management-security-privacy/. New Relic honors requests to delete personal data in accordance with applicable privacy laws. Please see https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Customers may use New Relic's APIs to query data, such as NerdGraph described here, and New Relic Services to export the data to other cloud providers. Customers can configure its log forwarder [https://docs.newrelic.com/docs/logs/enable-log-management-new-relic/enable-log-monitoring-new-relic/forward-your-logs-using-infrastructure-agent/] before sending infrastructure logs to New Relic. For New Relic Customers in New Relic US, FedRAMP and HIPAA-enabled environments, Customer Data is replicated to the off-site backup system via Amazon Simple Storage Service (S3). Category of Customer Description FedRAMP HIPAA-enabled US Gen Pop EU Gen Pop Data is stored in Amazon Web Services (“AWS”). Limited Data is stored in IBM Data for New Relic Incident Intelligence is stored in Google Cloud New Relic regularly tests, assess, and evaluates its measures to ensure the security of processing using industry-recognized standards and uses independent third-party auditors as provided below: Annual SOC 2 Type 2 Annual FedRAMP assessment by an independent third-party pursuant to NIST 800-53 rev 4 Moderate authorization. Annual HITRUST-validated assessment by an independent third-party *Pursuing CY2021 Q4 ISO 27001 *Pursuing CY2021 TISAX *Pursuing CY2021 The Services that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed information about AWS security is available at https://aws.amazon.com/security/ and http://aws.amazon.com/security/sharing-the-security-responsibility/. Data encryption at rest utilizes FIPS 140-2 compliant encryption methodology. For AWS SOC Reports, please see https://aws.amazon.com/compliance/soc-faqs/. The Services that operate on Google Cloud Platform (\"GCP\") are protected by the security and environmental controls of GCP. Detailed information about GCP security is available at https://cloud.google.com/docs/tutorials#security. For GCP reports, please see https://cloud.google.com/security/compliance/. IBM Deft Zayo QTS Law Enforcement Request Report New Relic has not to date received any request for customer data from a law enforcement or other government agency (including under any national security process), and has not made any corresponding disclosures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 135.48737,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " *Pursuing CY2021 The Services that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed <em>information</em> about AWS security is available at https:&#x2F;&#x2F;aws.amazon.com&#x2F;security&#x2F; and http:&#x2F;&#x2F;aws.amazon.com&#x2F;security&#x2F;sharing-the-security-responsibility"
      },
      "id": "6147558128ccbc973a56a863"
    }
  ],
  "/docs/licenses/license-information/product-definitions/new-relic-one-pricing-definitions": [
    {
      "sections": [
        "Original product-based pricing definitions",
        "App",
        "App transaction",
        "AWS Lambda event",
        "Check",
        "Compute unit",
        "Datapoints per minute",
        "Event",
        "Host",
        "Incident event",
        "Page view",
        "Per GB daily",
        "Span",
        "User"
      ],
      "title": "Original product-based pricing definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "42087e53167736831855bf9a4c2967c465677b45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/legacy-product-definitions/",
      "published_at": "2021-10-08T06:53:20Z",
      "updated_at": "2021-09-14T07:27:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our original product-based pricing. For New Relic One pricing plan terms, see New Relic One pricing definitions. App An app means the application software designed to run on smartphones, tablet computers, and other mobile devices. Each app includes a predefined number of users. App transaction An app transaction is an APM application's attempt to process a web or non-web request. In New Relic APM these manifest as throughput TIMESERIES or individually as events in the Transaction event type. AWS Lambda event An AWS Lambda event means the row of data collected from the customer's AWS Lambda function by the New Relic agent or sent from an external service into the New Relic platform. It consists of the AwsLambdaInvocation, AwsLambdaInvocationError, or custom event types. Check A check means the single instance of a Synthetics monitor running in New Relic's monitoring network and reporting back response time, and whether the check was a success or failure. Compute unit A compute unit means the measure of resources associated with a unit of computation on a physical or virtual host. Datapoints per minute Datapoints per minute (DPM) refers to the per-minute rate at which individual metric values are sent to the New Relic Metric Ingest API. For billing purposes, datapoints per minute are calculated as a monthly average value by summing the datapoints ingested during a 30 day period and dividing by the number of minutes in that period (43,200). Event An event means the row of data collected from the customer's application by the New Relic agent or sent from an external service into our database. Host A host means the physical computer or virtual machine instance running a single copy of an operating system. Host usage is tracked monthly by summing the hours that every host in the account is connected to New Relic and dividing by 750. A host is counted if it is connected any time during an hour. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Page view A page view means the full page load (triggering an onLoad event) or a recorded URL change (state change). Per GB daily Per GB daily represents a daily average of Log data sent to New Relic over a 30 day period. Span A span represents an operation summary collected from the customer's application via the New Relic APM agent or New Relic Serverless for AWS Lambda agent, or sent from other tracing tools to the New Relic Trace API. User A user means the individual that connects to your app from a single device. Each unique device is considered as a unique user.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 236.44061,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Original <em>product</em>-based pricing <em>definitions</em>",
        "sections": "Original <em>product</em>-based pricing <em>definitions</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": "This is a glossary of terms that appear in contracts for our original <em>product</em>-based pricing. For New Relic One pricing plan terms, see New Relic One pricing <em>definitions</em>. App An app means the application software designed to run on smartphones, tablet computers, and other mobile devices. Each app"
      },
      "id": "603ebacc64441f77774e8872"
    },
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Purpose",
        "2. Security Overview",
        "3.Security Certifications",
        "4. Data Control and Encryption",
        "5. Facilities",
        "6. Employee Access, Screening and Controls.",
        "7. Security Incident and Data Breach Response",
        "8. Network and Systems Security",
        "9. Authentication and Access Management",
        "10. Vulnerability Management",
        "11. System Access and Logging",
        "12. Disaster Recovery and Data Backup",
        "13. Copies and Removal",
        "14. Third Party Vendor Management",
        "15. Disclosure by Law",
        "16. Updates"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2021-10-08T06:55:04Z",
      "updated_at": "2021-09-27T15:05:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Updated 23 September 2021. The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Purpose This Policy describes New Relic's security program and the technical and organizational security controls to protect New Relic’s systems as shown in New Relic’s third-party audits and certifications. New Relic may update this Policy from time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/. 2. Security Overview 2.1. New Relic maintains a comprehensive Information Protection Program to manage information security within New Relic that includes administrative, technical, and physical safeguards designed to protect the confidentiality, integrity, and availability of Customer Data and that is appropriate to the nature, size, and complexity of New Relic's business operations. New Relic’s Information Protection Program includes: 2.1.1. executive review, support and accountability for all security related policies and practices; 2.1.2. formal written policies and procedures that are designed to protect against the loss, theft, or other unauthorized access or alteration of Customer Data, meet or exceed applicable industry standards, outlines a definition of information security and its overall objectives, include defined information security roles and responsibilities, a framework for setting control objectives and controls, a formal and effective risk mitigation program and a service provider security management program; 2.1.3. periodic risk assessments, including internal audits and/or independent 3rd party audits, to measure the effectiveness and appropriateness of controls for all systems processing Customer Data; 2.1.4. in-depth review of security incidents, including effective determination of root cause and corrective action; formal, industry-recognized controls frameworks based on an external audit standard; 2.1.5. employee screening and access management; and 2.1.6. comprehensive security testing, vulnerability identification, and mitigation methodology that consists of a variety of independent approaches that, when combined, are designed to maximize coverage for a diverse set of attack vectors. 2.2. New Relic's Chief Information Security Officer (CISO) leads New Relic's Information Security Program and develops, reviews, and approves, with appropriate stakeholders, New Relic's security policies and procedures. New Relic has appointed data protection officer(s) as described in New Relic’s Privacy Notices who are consulted as necessary under applicable laws. 2.3. Information security objectives, approach, scope, importance, goals, and principles for the organization’s security program are formally identified, communicated throughout the organization to users in a form that is relevant, accessible, and understandable to the intended reader; and supported by a controls framework that considers legislative, regulatory, contractual requirements, and other policy-related requirements. 2.4. New Relic will maintain written policies and procedures to review, test, and approve (as appropriate) changes affecting New Relic infrastructure and systems that process Customer Data including, but not limited to, peer reviews prior to introducing new code into production. New Relic will establish acceptance criteria for new information systems, upgrades, and new versions and will carry out suitable tests of the system(s) during development and prior to acceptance. Any changes or updates will not materially decrease the security of the systems. 3.Security Certifications 3.1. New Relic regularly tests, assesses, and evaluates its security measures for protecting Customer Data using industry-recognized standards and uses independent third-party auditors to verify such controls. 3.2. New Relic agrees to provide Customer, upon request, with applicable certifications or reports about New Relic systems. All information exchanged in connection with the audit activities described in this section is deemed to be the Confidential Information of New Relic. 3.3. Additional information about New Relic’s security certifications are available on New Relic’s Security Guide. 4. Data Control and Encryption 4.1. The Service and related features are designed to provide Customer with control over Customer’s data sources and Customer’s environments that are monitored and sending data to New Relic. 4.2. New Relic will have established methods to: (i) enable Customer to use encryption on Customer Data in transit, and (ii) securely hash passwords in storage following industry standard practices (e.g. scrypt) as described in the Documentation. Server certificate-based authentication is used as part of the TLS encryption with a trusted certificate authority. 4.3. New Relic receives and processes data in accordance with the Agreement and the Services as described in the Documentation. New Relic permits customers to delete personal data in accordance with applicable privacy laws as further described in the Documentation. In the event of error in personal data sent in Customer Data Customers may request personal data deletion and re-send data that is accurate. 4.4. Additional information for Customer data control and encryption, including encryption of data at rest, is available on New Relic’s Security Guide. 5. Facilities 5.1. All physical locations that process Customer Data are co-locations and third party data centers. All physical locations have security measures in place that are designed to prevent unauthorized physical access to data processing facilities and unlawful access, modification, or destruction of Customer Data. 5.2. New Relic will seek assurances (e.g., in the form of an independent 3rd party audit report such as the SOC 2 Type 2, ISO 27001, and vendor security evaluations) from its data processing facilities vendors that store or process Customer Data: 5.2.1. secure its data process facilities in an access-controlled location and have protections in place to prevent unauthorized access, damage, and interference; 5.2.2 employ physical security appropriate to the classification of the assets and information being managed which may include card key access, security cameras, and solid wall construction for all exterior walls; 5.2.3 limit and screen all entrants, which may include measures such as on-site security guard, badge reader, electronic lock, or a monitored closed caption television (CCTV); and 5.2.4 maintain relevant access logs. 5.3. Additional information about New Relic’s third party data centers are available on New Relic’s Security Guide. 6. Employee Access, Screening and Controls. New Relic will have and maintain policies and practices that include, at a minimum, the following controls and safeguards applied to New Relic employees and contractors who may access Customer Data: 6.1. For U.S. New Relic employees and subject to applicable law, a criminal background screening of each of its new employees to whom it gives access to Customer Data at the federal, state, and county levels. For non-U.S. New Relic employees, New Relic will use commercially reasonable efforts to meet the same criteria as established for U.S.-based New Relic employees, subject to general business practices in the respective country and in compliance with applicable local law requirements; 6.2. All New Relic employees with access to Customer Data will undergo adequate training, such as annual security awareness training, in the care, protection and handling of Customer Data, and will align with the privacy and security measures set out in this Addendum by following the guidance provided in their welcome package, which includes New Relic’s security policies and a security acknowledgement; 6.3. A disciplinary policy and process, to be used when New Relic employees violate New Relic security or privacy policies or access Customer Data without prior authorization; 6.4. Administrative or remote access to New Relic systems that process Customer Data will align with industry standard practices, including multi-factor authentication; 6.5. Restricted access to New Relic proprietary source code to prevent unauthorized access; 6.6. Controls designed to ensure that only those New Relic employees with an actual need-to-know will have access to New Relic systems including, but not limited to, the use of a formal access management process for the request, review, approval, and provisioning; 6.7. Controls designed to ensure that New Relic employees are granted access to New Relic systems based on least-privilege principles; and 6.8. Revoke access to New Relic employees no longer requiring access. 7. Security Incident and Data Breach Response 7.1. New Relic will take appropriate physical, technical, and administrative security measures that are commercially reasonable and consistent with industry standards to prevent a Data Breach, and as required by any applicable law or regulation. “Data Breach” means the theft, loss, or unauthorized access of Customer Data. Without limiting the foregoing, New Relic will implement security measures at least as stringent as those set out in this Addendum. New Relic will designate a senior representative to provide incident briefings, as needed in case of a Data Breach, and to respond to reasonable requests by Customer pertaining to privacy and data security issues within a commercially reasonable time frame. 7.2. New Relic will: 7.2.1. Notify Customer without undue delay if New Relic becomes aware of a Data Breach; 7.2.2. Maintain a security incident response plan, including procedures and means to respond in a manner consistent with industry standards; 7.2.3. Reasonably cooperate with Customer to investigate and remediate a Data Breach and mitigate any further risk to the Customer Data, or risk to data subjects and/or Customer’s reputation or brand; 7.2.4. Provide reasonable assistance to Customer at New Relic’s sole cost and expense; and 7.2.5. Make commercially reasonable efforts to preserve evidence and reasonably cooperate with Customer and legal authorities (as applicable and legally permissible) during an investigation of a Data Breach. 8. Network and Systems Security 8.1. All extranet connectivity by New Relic personnel to systems processing Customer Data will be through secure remote connections. 8.2. Network segments connected to the Internet will be protected by secure access control mechanisms, such as a firewall configured to secure all devices behind it and properly addresses security concerns according to industry standard practices. 8.3. New Relic will have industry standard measures in place to actively monitor its systems and help detect a potential hostile attack, such as Network Intrusion Detection (NID) or Host Intrusion Detection (HID)/Prevention. 8.4. Applications, ports, services, and similar access points installed on a computer or network facility, which are not specifically required for business functionality, will be disabled or removed. 8.5. New Relic maintains configuration standards for authorized operating systems and software for systems that support processing of Customer Data. New Relic will establish and follow server configuration guidelines and processes for preventing unauthorized access to Customer Data. New Relic maintains secure images or templates for systems based on the organization’s approved configuration standards. 8.6. Development, test, and operational environments will be logically separated to reduce the risks of unauthorized access or changes to the operational system. 8.7. New Relic network architecture will be designed to limit site access and restrict the availability of information systems that are considered to be vulnerable to attack. 8.8. New Relic provides appropriate TLS certificates when users access and view Customer Data in the Service. New Relic’s software for sending Customer Data to New Relic will encrypt Customer Data in transit by default. 9. Authentication and Access Management If you subscribe to the requisite New Relic One Service, New Relic will provide industry standard authentication and access controls to protect Customer Data, including industry standard authentication methods utilized to help prevent unauthorized access to the Service. New Relic’s access control methods will clearly state the rules and rights for each user or group of users including applications and information sharing and will include a process for granting and removing access to all information systems processing Customer Data. A record of all privileges allocated will be maintained pursuant to the requirements herein. 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. 10. Vulnerability Management New Relic will have and maintain the following vulnerability management processes for all devices used to connect to the New Relic network and Services. 10.1. New Relic will align to industry standard practices for build out, minimization of services and secure configuration, in accordance with, industry-recognized minimum security baselines for the New Relic platform and subcontractor systems connected to the New Relic platform in relation to the provision of the Services. 10.2. New Relic will employ industry-recognized standards and tools to conduct frequent infrastructure vulnerability scanning to test New Relic’s network and infrastructure and application penetration testing to test the New Relic Services. New Relic applies “Medium”, “High” and “Critical” security patches for all components in production and development environments as soon as commercially possible in accordance with its vulnerability management protocol, and consistent with industry standard practices and standards; 10.3. New Relic will have processes in place designed to ensure adherence to industry standard security development practices for development and testing for all code, APIs, and applications deployed and implemented in support of the Service; 10.4. New Relic will have and maintain solutions to identify and prevent malicious attackers or code from accessing or compromising Customer Data or systems that process Customer Data. These include, but are not limited to, software that identifies and removes malware and detects attempted intrusions. New Relic will have a security event and incident monitoring system and supporting processes to notify appropriate personnel in response to threats. 11. System Access and Logging 11.1. Access to New Relic’s systems will not be granted to employees of New Relic unless they have been uniquely identified and have sufficient credentials. 11.2. Access to New Relic’s systems will be logged and retained for no less than 6 months to assist in investigations and access control monitoring, including, but not limited to, end user access and activities, and information security events. 11.3. New Relic agrees to provide Customer the capability to access log records relating to Customer Accounts and the New Relic systems that process Customer Data in the event of a Data Breach or if required in connection with a law enforcement request. 12. Disaster Recovery and Data Backup 12.1. New Relic will have plans designed to respond to loss of services, which are tested and reviewed at least annually. This plan will include documented policies and procedures to restore service in the event of a service failure. 12.2. New Relic will establish and follow backup and restore procedures for Customer Data. 12.3. New Relic business continuity plan identifies critical systems. Annual disaster recovery tests are performed to check and restore customer data in the event of an incident. 12.4. New Relic will provide Customer with redacted copies of its plan(s) and evidence of tests/reviews upon request, but not more frequently than once annually, and subject to confidentiality requirements. 12.5. Additional information for Customer Data control and encryption are available on New Relic’s Security Guide. 13. Copies and Removal 13.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of the Agreement for any reason: (a) New Relic will, and will cause its personnel, to cease all access and use of any Customer Data, and (b) New Relic will delete all copies of Customer Data within ninety (90) days. 13.2. New Relic will maintain a process of ensuring secure destruction and deletion of all Customer Data, when reasonably requested by Customer or as otherwise provided in the Agreement. The process will include industry standard processes so that: (i) Customer Data cannot be practicably read or reconstructed, and (ii) the systems that store Customer Data are securely erased and/or decommissioned disks are destroyed. 14. Third Party Vendor Management New Relic may use third party vendors to provide the Services. New Relic performs a security risk-based assessment of prospective vendors before working with them to validate they meet New Relic’s security and business continuity standards, including the type of access and classification of data being accessed (if any), controls necessary to protect data, and legal/regulatory requirements. New Relic enters into written agreements with its vendors that process Customer Data which include confidentiality, privacy, and security obligations that provide an appropriate level of protection for Customer Data that these vendors may process for New Relic to maintain the security posture in this Policy, including following industry security standards. 15. Disclosure by Law In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic will (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. New Relic publishes its law enforcement requests report on New Relic’s Security Guide. 16. Updates As New Relic releases new products, services, functionality, and features, New Relic may update this Policy to account for such products, services, functionality, and features. For additional information, see our Security Guide.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 149.96313,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;referenced-policies&#x2F;security-policy&#x2F;. 2. Security Overview 2.1. New Relic"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    },
    {
      "sections": [
        "Security guide",
        "Tip",
        "Security Program",
        "Security Domains",
        "Security Certifications",
        "Data Control, Facilities, and Encryption",
        "Law Enforcement Request Report"
      ],
      "title": "Security guide",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "356f0d11ffcb62208a743a0a7c127f5f6da9c940",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-guide/",
      "published_at": "2021-10-07T02:08:45Z",
      "updated_at": "2021-09-19T15:21:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Last updated September 17, 2021. This is supplement to our security policy and serves as a guide to New Relic’s description of its Services, functionalities, and features. Tip We may update the URLs in this document without notice. Security Program New Relic follows \"privacy by design\" principles as described here: https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Security Domains New Relic’s policies and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless Security; Configuration Management; Vulnerability Management; Network Protection; Transmission Protection; Password Management; Access Control, Audit Logging & Monitoring; Education, Training, and Awareness; Third Party Assurance; Incident Management; Business Continuity and Disaster Recover; Risk Management; Data Protection & Privacy; and Service Management Systems. Security Certifications New Relic audits its Services against industry standards as described at https://docs.newrelic.com/docs/security/security-privacy/compliance/regulatory-audits-new-relic-services/. Data Control, Facilities, and Encryption New Relic's customers can send data to New Relic's APIs by (1) using New Relic's software, (2) using vendor-neutral software that is managed and maintained by a third-party such as via OpenTelemetry instrumentation provided by opentelemetry.io, or (3) from third-party systems that customer's manage and/or control. New Relic's customers can use New Relic's Services such as NerdGraph to filter out and drop data. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/drop-data-using-nerdgraph/. New Relic's customers can adjust their data retention periods as appropriate for their needs. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/manage-data-retention/#adjust-retention. New Relic Logs obfuscates numbers that match known patterns, such as bank card and social security numbers as described here: https://docs.newrelic.com/docs/logs/log-management/get-started/new-relics-log-management-security-privacy/. New Relic honors requests to delete personal data in accordance with applicable privacy laws. Please see https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Customers may use New Relic's APIs to query data, such as NerdGraph described here, and New Relic Services to export the data to other cloud providers. Customers can configure its log forwarder [https://docs.newrelic.com/docs/logs/enable-log-management-new-relic/enable-log-monitoring-new-relic/forward-your-logs-using-infrastructure-agent/] before sending infrastructure logs to New Relic. For New Relic Customers in New Relic US, FedRAMP and HIPAA-enabled environments, Customer Data is replicated to the off-site backup system via Amazon Simple Storage Service (S3). Category of Customer Description FedRAMP HIPAA-enabled US Gen Pop EU Gen Pop Data is stored in Amazon Web Services (“AWS”). Limited Data is stored in IBM Data for New Relic Incident Intelligence is stored in Google Cloud New Relic regularly tests, assess, and evaluates its measures to ensure the security of processing using industry-recognized standards and uses independent third-party auditors as provided below: Annual SOC 2 Type 2 Annual FedRAMP assessment by an independent third-party pursuant to NIST 800-53 rev 4 Moderate authorization. Annual HITRUST-validated assessment by an independent third-party *Pursuing CY2021 Q4 ISO 27001 *Pursuing CY2021 TISAX *Pursuing CY2021 The Services that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed information about AWS security is available at https://aws.amazon.com/security/ and http://aws.amazon.com/security/sharing-the-security-responsibility/. Data encryption at rest utilizes FIPS 140-2 compliant encryption methodology. For AWS SOC Reports, please see https://aws.amazon.com/compliance/soc-faqs/. The Services that operate on Google Cloud Platform (\"GCP\") are protected by the security and environmental controls of GCP. Detailed information about GCP security is available at https://cloud.google.com/docs/tutorials#security. For GCP reports, please see https://cloud.google.com/security/compliance/. IBM Deft Zayo QTS Law Enforcement Request Report New Relic has not to date received any request for customer data from a law enforcement or other government agency (including under any national security process), and has not made any corresponding disclosures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 135.48737,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " *Pursuing CY2021 The Services that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed <em>information</em> about AWS security is available at https:&#x2F;&#x2F;aws.amazon.com&#x2F;security&#x2F; and http:&#x2F;&#x2F;aws.amazon.com&#x2F;security&#x2F;sharing-the-security-responsibility"
      },
      "id": "6147558128ccbc973a56a863"
    }
  ],
  "/docs/licenses/license-information/referenced-policies/new-relic-pre-release-policy": [
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Purpose",
        "2. Security Overview",
        "3.Security Certifications",
        "4. Data Control and Encryption",
        "5. Facilities",
        "6. Employee Access, Screening and Controls.",
        "7. Security Incident and Data Breach Response",
        "8. Network and Systems Security",
        "9. Authentication and Access Management",
        "10. Vulnerability Management",
        "11. System Access and Logging",
        "12. Disaster Recovery and Data Backup",
        "13. Copies and Removal",
        "14. Third Party Vendor Management",
        "15. Disclosure by Law",
        "16. Updates"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2021-10-08T06:55:04Z",
      "updated_at": "2021-09-27T15:05:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Updated 23 September 2021. The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Purpose This Policy describes New Relic's security program and the technical and organizational security controls to protect New Relic’s systems as shown in New Relic’s third-party audits and certifications. New Relic may update this Policy from time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/. 2. Security Overview 2.1. New Relic maintains a comprehensive Information Protection Program to manage information security within New Relic that includes administrative, technical, and physical safeguards designed to protect the confidentiality, integrity, and availability of Customer Data and that is appropriate to the nature, size, and complexity of New Relic's business operations. New Relic’s Information Protection Program includes: 2.1.1. executive review, support and accountability for all security related policies and practices; 2.1.2. formal written policies and procedures that are designed to protect against the loss, theft, or other unauthorized access or alteration of Customer Data, meet or exceed applicable industry standards, outlines a definition of information security and its overall objectives, include defined information security roles and responsibilities, a framework for setting control objectives and controls, a formal and effective risk mitigation program and a service provider security management program; 2.1.3. periodic risk assessments, including internal audits and/or independent 3rd party audits, to measure the effectiveness and appropriateness of controls for all systems processing Customer Data; 2.1.4. in-depth review of security incidents, including effective determination of root cause and corrective action; formal, industry-recognized controls frameworks based on an external audit standard; 2.1.5. employee screening and access management; and 2.1.6. comprehensive security testing, vulnerability identification, and mitigation methodology that consists of a variety of independent approaches that, when combined, are designed to maximize coverage for a diverse set of attack vectors. 2.2. New Relic's Chief Information Security Officer (CISO) leads New Relic's Information Security Program and develops, reviews, and approves, with appropriate stakeholders, New Relic's security policies and procedures. New Relic has appointed data protection officer(s) as described in New Relic’s Privacy Notices who are consulted as necessary under applicable laws. 2.3. Information security objectives, approach, scope, importance, goals, and principles for the organization’s security program are formally identified, communicated throughout the organization to users in a form that is relevant, accessible, and understandable to the intended reader; and supported by a controls framework that considers legislative, regulatory, contractual requirements, and other policy-related requirements. 2.4. New Relic will maintain written policies and procedures to review, test, and approve (as appropriate) changes affecting New Relic infrastructure and systems that process Customer Data including, but not limited to, peer reviews prior to introducing new code into production. New Relic will establish acceptance criteria for new information systems, upgrades, and new versions and will carry out suitable tests of the system(s) during development and prior to acceptance. Any changes or updates will not materially decrease the security of the systems. 3.Security Certifications 3.1. New Relic regularly tests, assesses, and evaluates its security measures for protecting Customer Data using industry-recognized standards and uses independent third-party auditors to verify such controls. 3.2. New Relic agrees to provide Customer, upon request, with applicable certifications or reports about New Relic systems. All information exchanged in connection with the audit activities described in this section is deemed to be the Confidential Information of New Relic. 3.3. Additional information about New Relic’s security certifications are available on New Relic’s Security Guide. 4. Data Control and Encryption 4.1. The Service and related features are designed to provide Customer with control over Customer’s data sources and Customer’s environments that are monitored and sending data to New Relic. 4.2. New Relic will have established methods to: (i) enable Customer to use encryption on Customer Data in transit, and (ii) securely hash passwords in storage following industry standard practices (e.g. scrypt) as described in the Documentation. Server certificate-based authentication is used as part of the TLS encryption with a trusted certificate authority. 4.3. New Relic receives and processes data in accordance with the Agreement and the Services as described in the Documentation. New Relic permits customers to delete personal data in accordance with applicable privacy laws as further described in the Documentation. In the event of error in personal data sent in Customer Data Customers may request personal data deletion and re-send data that is accurate. 4.4. Additional information for Customer data control and encryption, including encryption of data at rest, is available on New Relic’s Security Guide. 5. Facilities 5.1. All physical locations that process Customer Data are co-locations and third party data centers. All physical locations have security measures in place that are designed to prevent unauthorized physical access to data processing facilities and unlawful access, modification, or destruction of Customer Data. 5.2. New Relic will seek assurances (e.g., in the form of an independent 3rd party audit report such as the SOC 2 Type 2, ISO 27001, and vendor security evaluations) from its data processing facilities vendors that store or process Customer Data: 5.2.1. secure its data process facilities in an access-controlled location and have protections in place to prevent unauthorized access, damage, and interference; 5.2.2 employ physical security appropriate to the classification of the assets and information being managed which may include card key access, security cameras, and solid wall construction for all exterior walls; 5.2.3 limit and screen all entrants, which may include measures such as on-site security guard, badge reader, electronic lock, or a monitored closed caption television (CCTV); and 5.2.4 maintain relevant access logs. 5.3. Additional information about New Relic’s third party data centers are available on New Relic’s Security Guide. 6. Employee Access, Screening and Controls. New Relic will have and maintain policies and practices that include, at a minimum, the following controls and safeguards applied to New Relic employees and contractors who may access Customer Data: 6.1. For U.S. New Relic employees and subject to applicable law, a criminal background screening of each of its new employees to whom it gives access to Customer Data at the federal, state, and county levels. For non-U.S. New Relic employees, New Relic will use commercially reasonable efforts to meet the same criteria as established for U.S.-based New Relic employees, subject to general business practices in the respective country and in compliance with applicable local law requirements; 6.2. All New Relic employees with access to Customer Data will undergo adequate training, such as annual security awareness training, in the care, protection and handling of Customer Data, and will align with the privacy and security measures set out in this Addendum by following the guidance provided in their welcome package, which includes New Relic’s security policies and a security acknowledgement; 6.3. A disciplinary policy and process, to be used when New Relic employees violate New Relic security or privacy policies or access Customer Data without prior authorization; 6.4. Administrative or remote access to New Relic systems that process Customer Data will align with industry standard practices, including multi-factor authentication; 6.5. Restricted access to New Relic proprietary source code to prevent unauthorized access; 6.6. Controls designed to ensure that only those New Relic employees with an actual need-to-know will have access to New Relic systems including, but not limited to, the use of a formal access management process for the request, review, approval, and provisioning; 6.7. Controls designed to ensure that New Relic employees are granted access to New Relic systems based on least-privilege principles; and 6.8. Revoke access to New Relic employees no longer requiring access. 7. Security Incident and Data Breach Response 7.1. New Relic will take appropriate physical, technical, and administrative security measures that are commercially reasonable and consistent with industry standards to prevent a Data Breach, and as required by any applicable law or regulation. “Data Breach” means the theft, loss, or unauthorized access of Customer Data. Without limiting the foregoing, New Relic will implement security measures at least as stringent as those set out in this Addendum. New Relic will designate a senior representative to provide incident briefings, as needed in case of a Data Breach, and to respond to reasonable requests by Customer pertaining to privacy and data security issues within a commercially reasonable time frame. 7.2. New Relic will: 7.2.1. Notify Customer without undue delay if New Relic becomes aware of a Data Breach; 7.2.2. Maintain a security incident response plan, including procedures and means to respond in a manner consistent with industry standards; 7.2.3. Reasonably cooperate with Customer to investigate and remediate a Data Breach and mitigate any further risk to the Customer Data, or risk to data subjects and/or Customer’s reputation or brand; 7.2.4. Provide reasonable assistance to Customer at New Relic’s sole cost and expense; and 7.2.5. Make commercially reasonable efforts to preserve evidence and reasonably cooperate with Customer and legal authorities (as applicable and legally permissible) during an investigation of a Data Breach. 8. Network and Systems Security 8.1. All extranet connectivity by New Relic personnel to systems processing Customer Data will be through secure remote connections. 8.2. Network segments connected to the Internet will be protected by secure access control mechanisms, such as a firewall configured to secure all devices behind it and properly addresses security concerns according to industry standard practices. 8.3. New Relic will have industry standard measures in place to actively monitor its systems and help detect a potential hostile attack, such as Network Intrusion Detection (NID) or Host Intrusion Detection (HID)/Prevention. 8.4. Applications, ports, services, and similar access points installed on a computer or network facility, which are not specifically required for business functionality, will be disabled or removed. 8.5. New Relic maintains configuration standards for authorized operating systems and software for systems that support processing of Customer Data. New Relic will establish and follow server configuration guidelines and processes for preventing unauthorized access to Customer Data. New Relic maintains secure images or templates for systems based on the organization’s approved configuration standards. 8.6. Development, test, and operational environments will be logically separated to reduce the risks of unauthorized access or changes to the operational system. 8.7. New Relic network architecture will be designed to limit site access and restrict the availability of information systems that are considered to be vulnerable to attack. 8.8. New Relic provides appropriate TLS certificates when users access and view Customer Data in the Service. New Relic’s software for sending Customer Data to New Relic will encrypt Customer Data in transit by default. 9. Authentication and Access Management If you subscribe to the requisite New Relic One Service, New Relic will provide industry standard authentication and access controls to protect Customer Data, including industry standard authentication methods utilized to help prevent unauthorized access to the Service. New Relic’s access control methods will clearly state the rules and rights for each user or group of users including applications and information sharing and will include a process for granting and removing access to all information systems processing Customer Data. A record of all privileges allocated will be maintained pursuant to the requirements herein. 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. 10. Vulnerability Management New Relic will have and maintain the following vulnerability management processes for all devices used to connect to the New Relic network and Services. 10.1. New Relic will align to industry standard practices for build out, minimization of services and secure configuration, in accordance with, industry-recognized minimum security baselines for the New Relic platform and subcontractor systems connected to the New Relic platform in relation to the provision of the Services. 10.2. New Relic will employ industry-recognized standards and tools to conduct frequent infrastructure vulnerability scanning to test New Relic’s network and infrastructure and application penetration testing to test the New Relic Services. New Relic applies “Medium”, “High” and “Critical” security patches for all components in production and development environments as soon as commercially possible in accordance with its vulnerability management protocol, and consistent with industry standard practices and standards; 10.3. New Relic will have processes in place designed to ensure adherence to industry standard security development practices for development and testing for all code, APIs, and applications deployed and implemented in support of the Service; 10.4. New Relic will have and maintain solutions to identify and prevent malicious attackers or code from accessing or compromising Customer Data or systems that process Customer Data. These include, but are not limited to, software that identifies and removes malware and detects attempted intrusions. New Relic will have a security event and incident monitoring system and supporting processes to notify appropriate personnel in response to threats. 11. System Access and Logging 11.1. Access to New Relic’s systems will not be granted to employees of New Relic unless they have been uniquely identified and have sufficient credentials. 11.2. Access to New Relic’s systems will be logged and retained for no less than 6 months to assist in investigations and access control monitoring, including, but not limited to, end user access and activities, and information security events. 11.3. New Relic agrees to provide Customer the capability to access log records relating to Customer Accounts and the New Relic systems that process Customer Data in the event of a Data Breach or if required in connection with a law enforcement request. 12. Disaster Recovery and Data Backup 12.1. New Relic will have plans designed to respond to loss of services, which are tested and reviewed at least annually. This plan will include documented policies and procedures to restore service in the event of a service failure. 12.2. New Relic will establish and follow backup and restore procedures for Customer Data. 12.3. New Relic business continuity plan identifies critical systems. Annual disaster recovery tests are performed to check and restore customer data in the event of an incident. 12.4. New Relic will provide Customer with redacted copies of its plan(s) and evidence of tests/reviews upon request, but not more frequently than once annually, and subject to confidentiality requirements. 12.5. Additional information for Customer Data control and encryption are available on New Relic’s Security Guide. 13. Copies and Removal 13.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of the Agreement for any reason: (a) New Relic will, and will cause its personnel, to cease all access and use of any Customer Data, and (b) New Relic will delete all copies of Customer Data within ninety (90) days. 13.2. New Relic will maintain a process of ensuring secure destruction and deletion of all Customer Data, when reasonably requested by Customer or as otherwise provided in the Agreement. The process will include industry standard processes so that: (i) Customer Data cannot be practicably read or reconstructed, and (ii) the systems that store Customer Data are securely erased and/or decommissioned disks are destroyed. 14. Third Party Vendor Management New Relic may use third party vendors to provide the Services. New Relic performs a security risk-based assessment of prospective vendors before working with them to validate they meet New Relic’s security and business continuity standards, including the type of access and classification of data being accessed (if any), controls necessary to protect data, and legal/regulatory requirements. New Relic enters into written agreements with its vendors that process Customer Data which include confidentiality, privacy, and security obligations that provide an appropriate level of protection for Customer Data that these vendors may process for New Relic to maintain the security posture in this Policy, including following industry security standards. 15. Disclosure by Law In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic will (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. New Relic publishes its law enforcement requests report on New Relic’s Security Guide. 16. Updates As New Relic releases new products, services, functionality, and features, New Relic may update this Policy to account for such products, services, functionality, and features. For additional information, see our Security Guide.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 280.13876,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Security <em>policy</em>",
        "sections": "Security <em>policy</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;<em>referenced</em>-<em>policies</em>&#x2F;security-policy&#x2F;. 2. Security Overview 2.1. New Relic"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    },
    {
      "sections": [
        "Security guide",
        "Tip",
        "Security Program",
        "Security Domains",
        "Security Certifications",
        "Data Control, Facilities, and Encryption",
        "Law Enforcement Request Report"
      ],
      "title": "Security guide",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "356f0d11ffcb62208a743a0a7c127f5f6da9c940",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-guide/",
      "published_at": "2021-10-07T02:08:45Z",
      "updated_at": "2021-09-19T15:21:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Last updated September 17, 2021. This is supplement to our security policy and serves as a guide to New Relic’s description of its Services, functionalities, and features. Tip We may update the URLs in this document without notice. Security Program New Relic follows \"privacy by design\" principles as described here: https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Security Domains New Relic’s policies and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless Security; Configuration Management; Vulnerability Management; Network Protection; Transmission Protection; Password Management; Access Control, Audit Logging & Monitoring; Education, Training, and Awareness; Third Party Assurance; Incident Management; Business Continuity and Disaster Recover; Risk Management; Data Protection & Privacy; and Service Management Systems. Security Certifications New Relic audits its Services against industry standards as described at https://docs.newrelic.com/docs/security/security-privacy/compliance/regulatory-audits-new-relic-services/. Data Control, Facilities, and Encryption New Relic's customers can send data to New Relic's APIs by (1) using New Relic's software, (2) using vendor-neutral software that is managed and maintained by a third-party such as via OpenTelemetry instrumentation provided by opentelemetry.io, or (3) from third-party systems that customer's manage and/or control. New Relic's customers can use New Relic's Services such as NerdGraph to filter out and drop data. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/drop-data-using-nerdgraph/. New Relic's customers can adjust their data retention periods as appropriate for their needs. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/manage-data-retention/#adjust-retention. New Relic Logs obfuscates numbers that match known patterns, such as bank card and social security numbers as described here: https://docs.newrelic.com/docs/logs/log-management/get-started/new-relics-log-management-security-privacy/. New Relic honors requests to delete personal data in accordance with applicable privacy laws. Please see https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Customers may use New Relic's APIs to query data, such as NerdGraph described here, and New Relic Services to export the data to other cloud providers. Customers can configure its log forwarder [https://docs.newrelic.com/docs/logs/enable-log-management-new-relic/enable-log-monitoring-new-relic/forward-your-logs-using-infrastructure-agent/] before sending infrastructure logs to New Relic. For New Relic Customers in New Relic US, FedRAMP and HIPAA-enabled environments, Customer Data is replicated to the off-site backup system via Amazon Simple Storage Service (S3). Category of Customer Description FedRAMP HIPAA-enabled US Gen Pop EU Gen Pop Data is stored in Amazon Web Services (“AWS”). Limited Data is stored in IBM Data for New Relic Incident Intelligence is stored in Google Cloud New Relic regularly tests, assess, and evaluates its measures to ensure the security of processing using industry-recognized standards and uses independent third-party auditors as provided below: Annual SOC 2 Type 2 Annual FedRAMP assessment by an independent third-party pursuant to NIST 800-53 rev 4 Moderate authorization. Annual HITRUST-validated assessment by an independent third-party *Pursuing CY2021 Q4 ISO 27001 *Pursuing CY2021 TISAX *Pursuing CY2021 The Services that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed information about AWS security is available at https://aws.amazon.com/security/ and http://aws.amazon.com/security/sharing-the-security-responsibility/. Data encryption at rest utilizes FIPS 140-2 compliant encryption methodology. For AWS SOC Reports, please see https://aws.amazon.com/compliance/soc-faqs/. The Services that operate on Google Cloud Platform (\"GCP\") are protected by the security and environmental controls of GCP. Detailed information about GCP security is available at https://cloud.google.com/docs/tutorials#security. For GCP reports, please see https://cloud.google.com/security/compliance/. IBM Deft Zayo QTS Law Enforcement Request Report New Relic has not to date received any request for customer data from a law enforcement or other government agency (including under any national security process), and has not made any corresponding disclosures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 253.09732,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " as described here: https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;security&#x2F;security-privacy&#x2F;data-privacy&#x2F;data-privacy-new-relic&#x2F;. Security Domains New Relic’s <em>policies</em> and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless"
      },
      "id": "6147558128ccbc973a56a863"
    },
    {
      "sections": [
        "Service level availability commitment",
        "New Relic Service Level Availability"
      ],
      "title": "Service level availability commitment",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "5489f268010887c72446f12059a1a0e4279ad960",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/service-level-availability-commitment/",
      "published_at": "2021-10-08T06:54:27Z",
      "updated_at": "2021-03-16T04:22:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you subscribe to New Relic Full Stack Observability Pro or Enterprise Products, the service level availability commitments available to you are those set forth (i) in the Terms, or (ii) as set forth on this page. If you subscribe to New Relic Full Stack Observability Standard, any service level availability commitment or related remedies contained in your Terms are vacated and nullified, and New Relic will use commercially reasonable efforts to make New Relic Full Stack Observability Standard and Telemetry Data Platform available in line with industry standards. If you subscribe to any other New Relic Products on a product-based pricing basis, any service level availability commitment are contained in (i) your Terms, or (ii) if your Terms explicitly references this Service Level Availability commitment applying to the Service purchased in an Order, this page for the Services you use. Capitalized terms not defined below shall take on the meaning set forth in your Terms. New Relic Service Level Availability The Service will be considered available so long as Customer is able to log in to its interface and view Customer Data (\"Service Availability\"). The applicable Service Availability will be calculated as a percentage of: (1) the total number of minutes in a month after (2) subtracting any periods of unavailability during such month from the total number of minutes in a month. New Relic will use commercially reasonable efforts to maintain Service Availability of at least 99.8% during any calendar month. In the event the Service Availability drops below: (i) 98.5% for two consecutive calendar months during the Subscription Term, or (ii) 96.5% in any single calendar month, Customer may request to terminate the relevant Service with no penalty. Such termination will be effective as of the end of the then-current billing period and no additional fees will be charged. The service level within New Relic's control is the Service Availability, not, for example, the transmission of data over the public Internet. Service Availability calculations will exclude unavailability arising from any: (a) planned maintenance periods; (b) emergency maintenance that is necessary to prevent imminent harm to the Service; (c) force majeure events; (d) Third-Party Services, Customer application, equipment, software or other technology, or Customer or its User's use of the Service, in violation of the Agreement or not in accordance with the Documentation; or (e) suspension, limitation, and/or termination of Customer’s access or use of the Service in accordance with this Agreement. This describes Customer’s sole and exclusive remedy for failures of Service Availability. Customer may request the Service Availability attainment for the previous month by filing a support ticket on the New Relic support site.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 192.31628,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>"
      },
      "id": "603ea44828ccbcd9c8eba7b7"
    }
  ],
  "/docs/licenses/license-information/referenced-policies/security-guide": [
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Purpose",
        "2. Security Overview",
        "3.Security Certifications",
        "4. Data Control and Encryption",
        "5. Facilities",
        "6. Employee Access, Screening and Controls.",
        "7. Security Incident and Data Breach Response",
        "8. Network and Systems Security",
        "9. Authentication and Access Management",
        "10. Vulnerability Management",
        "11. System Access and Logging",
        "12. Disaster Recovery and Data Backup",
        "13. Copies and Removal",
        "14. Third Party Vendor Management",
        "15. Disclosure by Law",
        "16. Updates"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2021-10-08T06:55:04Z",
      "updated_at": "2021-09-27T15:05:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Updated 23 September 2021. The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Purpose This Policy describes New Relic's security program and the technical and organizational security controls to protect New Relic’s systems as shown in New Relic’s third-party audits and certifications. New Relic may update this Policy from time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/. 2. Security Overview 2.1. New Relic maintains a comprehensive Information Protection Program to manage information security within New Relic that includes administrative, technical, and physical safeguards designed to protect the confidentiality, integrity, and availability of Customer Data and that is appropriate to the nature, size, and complexity of New Relic's business operations. New Relic’s Information Protection Program includes: 2.1.1. executive review, support and accountability for all security related policies and practices; 2.1.2. formal written policies and procedures that are designed to protect against the loss, theft, or other unauthorized access or alteration of Customer Data, meet or exceed applicable industry standards, outlines a definition of information security and its overall objectives, include defined information security roles and responsibilities, a framework for setting control objectives and controls, a formal and effective risk mitigation program and a service provider security management program; 2.1.3. periodic risk assessments, including internal audits and/or independent 3rd party audits, to measure the effectiveness and appropriateness of controls for all systems processing Customer Data; 2.1.4. in-depth review of security incidents, including effective determination of root cause and corrective action; formal, industry-recognized controls frameworks based on an external audit standard; 2.1.5. employee screening and access management; and 2.1.6. comprehensive security testing, vulnerability identification, and mitigation methodology that consists of a variety of independent approaches that, when combined, are designed to maximize coverage for a diverse set of attack vectors. 2.2. New Relic's Chief Information Security Officer (CISO) leads New Relic's Information Security Program and develops, reviews, and approves, with appropriate stakeholders, New Relic's security policies and procedures. New Relic has appointed data protection officer(s) as described in New Relic’s Privacy Notices who are consulted as necessary under applicable laws. 2.3. Information security objectives, approach, scope, importance, goals, and principles for the organization’s security program are formally identified, communicated throughout the organization to users in a form that is relevant, accessible, and understandable to the intended reader; and supported by a controls framework that considers legislative, regulatory, contractual requirements, and other policy-related requirements. 2.4. New Relic will maintain written policies and procedures to review, test, and approve (as appropriate) changes affecting New Relic infrastructure and systems that process Customer Data including, but not limited to, peer reviews prior to introducing new code into production. New Relic will establish acceptance criteria for new information systems, upgrades, and new versions and will carry out suitable tests of the system(s) during development and prior to acceptance. Any changes or updates will not materially decrease the security of the systems. 3.Security Certifications 3.1. New Relic regularly tests, assesses, and evaluates its security measures for protecting Customer Data using industry-recognized standards and uses independent third-party auditors to verify such controls. 3.2. New Relic agrees to provide Customer, upon request, with applicable certifications or reports about New Relic systems. All information exchanged in connection with the audit activities described in this section is deemed to be the Confidential Information of New Relic. 3.3. Additional information about New Relic’s security certifications are available on New Relic’s Security Guide. 4. Data Control and Encryption 4.1. The Service and related features are designed to provide Customer with control over Customer’s data sources and Customer’s environments that are monitored and sending data to New Relic. 4.2. New Relic will have established methods to: (i) enable Customer to use encryption on Customer Data in transit, and (ii) securely hash passwords in storage following industry standard practices (e.g. scrypt) as described in the Documentation. Server certificate-based authentication is used as part of the TLS encryption with a trusted certificate authority. 4.3. New Relic receives and processes data in accordance with the Agreement and the Services as described in the Documentation. New Relic permits customers to delete personal data in accordance with applicable privacy laws as further described in the Documentation. In the event of error in personal data sent in Customer Data Customers may request personal data deletion and re-send data that is accurate. 4.4. Additional information for Customer data control and encryption, including encryption of data at rest, is available on New Relic’s Security Guide. 5. Facilities 5.1. All physical locations that process Customer Data are co-locations and third party data centers. All physical locations have security measures in place that are designed to prevent unauthorized physical access to data processing facilities and unlawful access, modification, or destruction of Customer Data. 5.2. New Relic will seek assurances (e.g., in the form of an independent 3rd party audit report such as the SOC 2 Type 2, ISO 27001, and vendor security evaluations) from its data processing facilities vendors that store or process Customer Data: 5.2.1. secure its data process facilities in an access-controlled location and have protections in place to prevent unauthorized access, damage, and interference; 5.2.2 employ physical security appropriate to the classification of the assets and information being managed which may include card key access, security cameras, and solid wall construction for all exterior walls; 5.2.3 limit and screen all entrants, which may include measures such as on-site security guard, badge reader, electronic lock, or a monitored closed caption television (CCTV); and 5.2.4 maintain relevant access logs. 5.3. Additional information about New Relic’s third party data centers are available on New Relic’s Security Guide. 6. Employee Access, Screening and Controls. New Relic will have and maintain policies and practices that include, at a minimum, the following controls and safeguards applied to New Relic employees and contractors who may access Customer Data: 6.1. For U.S. New Relic employees and subject to applicable law, a criminal background screening of each of its new employees to whom it gives access to Customer Data at the federal, state, and county levels. For non-U.S. New Relic employees, New Relic will use commercially reasonable efforts to meet the same criteria as established for U.S.-based New Relic employees, subject to general business practices in the respective country and in compliance with applicable local law requirements; 6.2. All New Relic employees with access to Customer Data will undergo adequate training, such as annual security awareness training, in the care, protection and handling of Customer Data, and will align with the privacy and security measures set out in this Addendum by following the guidance provided in their welcome package, which includes New Relic’s security policies and a security acknowledgement; 6.3. A disciplinary policy and process, to be used when New Relic employees violate New Relic security or privacy policies or access Customer Data without prior authorization; 6.4. Administrative or remote access to New Relic systems that process Customer Data will align with industry standard practices, including multi-factor authentication; 6.5. Restricted access to New Relic proprietary source code to prevent unauthorized access; 6.6. Controls designed to ensure that only those New Relic employees with an actual need-to-know will have access to New Relic systems including, but not limited to, the use of a formal access management process for the request, review, approval, and provisioning; 6.7. Controls designed to ensure that New Relic employees are granted access to New Relic systems based on least-privilege principles; and 6.8. Revoke access to New Relic employees no longer requiring access. 7. Security Incident and Data Breach Response 7.1. New Relic will take appropriate physical, technical, and administrative security measures that are commercially reasonable and consistent with industry standards to prevent a Data Breach, and as required by any applicable law or regulation. “Data Breach” means the theft, loss, or unauthorized access of Customer Data. Without limiting the foregoing, New Relic will implement security measures at least as stringent as those set out in this Addendum. New Relic will designate a senior representative to provide incident briefings, as needed in case of a Data Breach, and to respond to reasonable requests by Customer pertaining to privacy and data security issues within a commercially reasonable time frame. 7.2. New Relic will: 7.2.1. Notify Customer without undue delay if New Relic becomes aware of a Data Breach; 7.2.2. Maintain a security incident response plan, including procedures and means to respond in a manner consistent with industry standards; 7.2.3. Reasonably cooperate with Customer to investigate and remediate a Data Breach and mitigate any further risk to the Customer Data, or risk to data subjects and/or Customer’s reputation or brand; 7.2.4. Provide reasonable assistance to Customer at New Relic’s sole cost and expense; and 7.2.5. Make commercially reasonable efforts to preserve evidence and reasonably cooperate with Customer and legal authorities (as applicable and legally permissible) during an investigation of a Data Breach. 8. Network and Systems Security 8.1. All extranet connectivity by New Relic personnel to systems processing Customer Data will be through secure remote connections. 8.2. Network segments connected to the Internet will be protected by secure access control mechanisms, such as a firewall configured to secure all devices behind it and properly addresses security concerns according to industry standard practices. 8.3. New Relic will have industry standard measures in place to actively monitor its systems and help detect a potential hostile attack, such as Network Intrusion Detection (NID) or Host Intrusion Detection (HID)/Prevention. 8.4. Applications, ports, services, and similar access points installed on a computer or network facility, which are not specifically required for business functionality, will be disabled or removed. 8.5. New Relic maintains configuration standards for authorized operating systems and software for systems that support processing of Customer Data. New Relic will establish and follow server configuration guidelines and processes for preventing unauthorized access to Customer Data. New Relic maintains secure images or templates for systems based on the organization’s approved configuration standards. 8.6. Development, test, and operational environments will be logically separated to reduce the risks of unauthorized access or changes to the operational system. 8.7. New Relic network architecture will be designed to limit site access and restrict the availability of information systems that are considered to be vulnerable to attack. 8.8. New Relic provides appropriate TLS certificates when users access and view Customer Data in the Service. New Relic’s software for sending Customer Data to New Relic will encrypt Customer Data in transit by default. 9. Authentication and Access Management If you subscribe to the requisite New Relic One Service, New Relic will provide industry standard authentication and access controls to protect Customer Data, including industry standard authentication methods utilized to help prevent unauthorized access to the Service. New Relic’s access control methods will clearly state the rules and rights for each user or group of users including applications and information sharing and will include a process for granting and removing access to all information systems processing Customer Data. A record of all privileges allocated will be maintained pursuant to the requirements herein. 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. 10. Vulnerability Management New Relic will have and maintain the following vulnerability management processes for all devices used to connect to the New Relic network and Services. 10.1. New Relic will align to industry standard practices for build out, minimization of services and secure configuration, in accordance with, industry-recognized minimum security baselines for the New Relic platform and subcontractor systems connected to the New Relic platform in relation to the provision of the Services. 10.2. New Relic will employ industry-recognized standards and tools to conduct frequent infrastructure vulnerability scanning to test New Relic’s network and infrastructure and application penetration testing to test the New Relic Services. New Relic applies “Medium”, “High” and “Critical” security patches for all components in production and development environments as soon as commercially possible in accordance with its vulnerability management protocol, and consistent with industry standard practices and standards; 10.3. New Relic will have processes in place designed to ensure adherence to industry standard security development practices for development and testing for all code, APIs, and applications deployed and implemented in support of the Service; 10.4. New Relic will have and maintain solutions to identify and prevent malicious attackers or code from accessing or compromising Customer Data or systems that process Customer Data. These include, but are not limited to, software that identifies and removes malware and detects attempted intrusions. New Relic will have a security event and incident monitoring system and supporting processes to notify appropriate personnel in response to threats. 11. System Access and Logging 11.1. Access to New Relic’s systems will not be granted to employees of New Relic unless they have been uniquely identified and have sufficient credentials. 11.2. Access to New Relic’s systems will be logged and retained for no less than 6 months to assist in investigations and access control monitoring, including, but not limited to, end user access and activities, and information security events. 11.3. New Relic agrees to provide Customer the capability to access log records relating to Customer Accounts and the New Relic systems that process Customer Data in the event of a Data Breach or if required in connection with a law enforcement request. 12. Disaster Recovery and Data Backup 12.1. New Relic will have plans designed to respond to loss of services, which are tested and reviewed at least annually. This plan will include documented policies and procedures to restore service in the event of a service failure. 12.2. New Relic will establish and follow backup and restore procedures for Customer Data. 12.3. New Relic business continuity plan identifies critical systems. Annual disaster recovery tests are performed to check and restore customer data in the event of an incident. 12.4. New Relic will provide Customer with redacted copies of its plan(s) and evidence of tests/reviews upon request, but not more frequently than once annually, and subject to confidentiality requirements. 12.5. Additional information for Customer Data control and encryption are available on New Relic’s Security Guide. 13. Copies and Removal 13.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of the Agreement for any reason: (a) New Relic will, and will cause its personnel, to cease all access and use of any Customer Data, and (b) New Relic will delete all copies of Customer Data within ninety (90) days. 13.2. New Relic will maintain a process of ensuring secure destruction and deletion of all Customer Data, when reasonably requested by Customer or as otherwise provided in the Agreement. The process will include industry standard processes so that: (i) Customer Data cannot be practicably read or reconstructed, and (ii) the systems that store Customer Data are securely erased and/or decommissioned disks are destroyed. 14. Third Party Vendor Management New Relic may use third party vendors to provide the Services. New Relic performs a security risk-based assessment of prospective vendors before working with them to validate they meet New Relic’s security and business continuity standards, including the type of access and classification of data being accessed (if any), controls necessary to protect data, and legal/regulatory requirements. New Relic enters into written agreements with its vendors that process Customer Data which include confidentiality, privacy, and security obligations that provide an appropriate level of protection for Customer Data that these vendors may process for New Relic to maintain the security posture in this Policy, including following industry security standards. 15. Disclosure by Law In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic will (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. New Relic publishes its law enforcement requests report on New Relic’s Security Guide. 16. Updates As New Relic releases new products, services, functionality, and features, New Relic may update this Policy to account for such products, services, functionality, and features. For additional information, see our Security Guide.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 280.13876,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Security <em>policy</em>",
        "sections": "Security <em>policy</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;<em>referenced</em>-<em>policies</em>&#x2F;security-policy&#x2F;. 2. Security Overview 2.1. New Relic"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    },
    {
      "sections": [
        "Service level availability commitment",
        "New Relic Service Level Availability"
      ],
      "title": "Service level availability commitment",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "5489f268010887c72446f12059a1a0e4279ad960",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/service-level-availability-commitment/",
      "published_at": "2021-10-08T06:54:27Z",
      "updated_at": "2021-03-16T04:22:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you subscribe to New Relic Full Stack Observability Pro or Enterprise Products, the service level availability commitments available to you are those set forth (i) in the Terms, or (ii) as set forth on this page. If you subscribe to New Relic Full Stack Observability Standard, any service level availability commitment or related remedies contained in your Terms are vacated and nullified, and New Relic will use commercially reasonable efforts to make New Relic Full Stack Observability Standard and Telemetry Data Platform available in line with industry standards. If you subscribe to any other New Relic Products on a product-based pricing basis, any service level availability commitment are contained in (i) your Terms, or (ii) if your Terms explicitly references this Service Level Availability commitment applying to the Service purchased in an Order, this page for the Services you use. Capitalized terms not defined below shall take on the meaning set forth in your Terms. New Relic Service Level Availability The Service will be considered available so long as Customer is able to log in to its interface and view Customer Data (\"Service Availability\"). The applicable Service Availability will be calculated as a percentage of: (1) the total number of minutes in a month after (2) subtracting any periods of unavailability during such month from the total number of minutes in a month. New Relic will use commercially reasonable efforts to maintain Service Availability of at least 99.8% during any calendar month. In the event the Service Availability drops below: (i) 98.5% for two consecutive calendar months during the Subscription Term, or (ii) 96.5% in any single calendar month, Customer may request to terminate the relevant Service with no penalty. Such termination will be effective as of the end of the then-current billing period and no additional fees will be charged. The service level within New Relic's control is the Service Availability, not, for example, the transmission of data over the public Internet. Service Availability calculations will exclude unavailability arising from any: (a) planned maintenance periods; (b) emergency maintenance that is necessary to prevent imminent harm to the Service; (c) force majeure events; (d) Third-Party Services, Customer application, equipment, software or other technology, or Customer or its User's use of the Service, in violation of the Agreement or not in accordance with the Documentation; or (e) suspension, limitation, and/or termination of Customer’s access or use of the Service in accordance with this Agreement. This describes Customer’s sole and exclusive remedy for failures of Service Availability. Customer may request the Service Availability attainment for the previous month by filing a support ticket on the New Relic support site.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 192.31628,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>"
      },
      "id": "603ea44828ccbcd9c8eba7b7"
    },
    {
      "sections": [
        "New Relic Pre-release policy",
        "New Relic Pre-Release Policy",
        "1. Introduction",
        "2. License and Restrictions",
        "3. Confidential Information",
        "4. Disclaimers and Acknowledgement",
        "5. Limitation of Liability",
        "6. Indemnity",
        "7. Export Restrictions",
        "8. Termination"
      ],
      "title": "New Relic Pre-release policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "5ca324e942906913fc346245cfd80d405e90ad1f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/new-relic-pre-release-policy/",
      "published_at": "2021-10-08T06:54:27Z",
      "updated_at": "2021-03-13T01:17:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Without overriding any express provisions that you may have agreed to in a separate written beta or pre-release agreement executed by you and New Relic, the following terms apply to your use of any New Relic Pre-Release Service(s). New Relic Pre-Release Policy Through your use of any New Relic, Inc. (“New Relic”) Pre-Release Services (as further defined), you signify that you have read, understood, and accept the terms of this Pre-Release Policy (the “Policy”) governing your use and participation in New Relic’s program for making beta, alpha and pre-release versions of New Relic software (i.e. agents, SDKs, APIs, integrations, private locations, code, etc.), services, features, user-interfaces, and platforms made available for evaluation purposes (the “Pre-Release Service(s)”). This Policy is effective on behalf of yourself or the entity(ies) associated with your New Relic account (collectively referred to herein as “you” or “your”) as of your date of first use of the Pre-Release Service(s) (the “Effective Date”), which means that your choice to use the Pre-Release Service(s) means that you have the legal authority to use the Pre-Release Service(s) personally or on behalf of the company or organization associated with your New Relic account in accordance with this Policy. Please read this Policy carefully as your use of Pre-Release Service(s) may have unintended consequences and materially impact your data or use of the New Relic services and products. If you do not agree to this Policy, your sole remedy is to not make use of the Pre-Release Services. 1. Introduction This Policy and any New Relic technical guides and documentation made available with the Pre-Release Service(s) or from the dedicated ‘Documentation’ page of the New Relic website (the “Documentation”) enables you to test experimental features and products before they are made generally available. New Relic reserves the right to withhold or discontinue any Pre-Release Service(s), which may be designated as an alpha, beta, pilot, limited release, developer preview, technology preview, non-production, evaluation, or by a similar description, in its sole discretion and provides no guarantee that any Pre-Release Service(s) will eventually be made commercially or generally available. Pre-Release Service(s) are versions provided before they are generally available and may have bugs, stability issues, or other problems. Use of the Pre-Release Service(s) may result in unexpected results, loss of data, outages, or other damage to your systems or networks. New Relic advises against using the Pre-Release Service(s) in production. No support is offered with or for Pre-Release Service(s). You hereby release New Relic from any liability arising from or related to your use of the Pre-Release Service(s). In connection with your use of the Pre-Release Services, you may provide feedback, comments, and/or suggestions based on or relating to the Pre-Release Services (“Feedback”). The Pre-Release Services and any related information, including without limitation features, functionality, designs, user interface, capabilities, specifications, architectural diagrams, usage data, APIs, deployment schedules, email lists, bug databases, know how, source code, potential features, Feedback, and information related to any business or technical information of New Relic, including without limitation product plans, costs, prices, finances, marketing plans, business opportunities, research, development, and negotiations are considered confidential information of New Relic (“Confidential Information”). 2. License and Restrictions 2.1 Evaluation License. Subject to your compliance with the Policy, New Relic hereby grants you a limited, non-exclusive, non-transferable, revocable license, during the term of this Policy, to use the Pre-Release Service(s) in accordance with the Documentation solely for the purpose of internal evaluation and supplying Feedback to New Relic. 2.2 License Restrictions. You may not (and may not authorize or enable a third party to): (i) reverse engineer, decompile, disassemble, or otherwise attempt to discover the source code or other trade secrets in the Pre-Release Service(s); (ii) access the Pre-Release Service(s) in order to build a similar or competitive application, service, feature or other competitive purpose; (iii) remove or destroy any copyright notices or other proprietary marks contained on or in the Pre-Release Service(s); (iv) use the Pre-Release Service(s) for any benchmarking purposes or in connection with a service bureau, timeshare, service provider, or like activity where you operate the Pre-Release Service(s) on behalf of a third party; (v) mirror, frame, copy, modify, host, rent, lease, sell, commercialize, sublicense, assign or otherwise transfer the Pre-Release Service(s), or the access or use of the Pre-Release Service(s); (vi) use the Pre-Release Service(s) in a manner that may violate or infringe the intellectual property, data protection, or other proprietary right of a third party; (vii) use or access the Pre-Release Service(s) in an unauthorized manner and/or in a manner that violates any applicable law, rule, contract, or guideline (including but not limited to our Documentation or Community Guidelines); (viii) use the Pre-Release Service(s) to transmit worms, viruses, malicious code, security vulnerabilities, or otherwise negatively impact network operations, third parties, or New Relic; (ix) use any data mining or similar data gathering and extraction methods in connection with the Pre-Release Service(s); or (x) use the Pre-Release Service(s) to process sensitive personal information, e.g., “personal information” of children as defined by the Children’s Online Privacy Protection Act, “protected health information” as defined by the Health Insurance Portability and Accountability Act of 1996, government issued identification numbers, financial account information, payment card data, “special categories of data” as described in the EU General Data Protection Regulation (GDPR), or other information subject to regulatory, statutory, or contractual restrictions. New Relic reserves the right, but not the obligation, to monitor or review your use of the Pre-Release Service(s) at any time and may investigate any suspected violations of this Policy. 2.3 Intellectual Property Rights. You acknowledge and agree that, as between you and New Relic, New Relic owns all right, title, and interest in and to the Pre-Release Service(s), Confidential Information, and Feedback, including but not limited to all intellectual property and proprietary rights therein. New Relic and its licensors reserve all rights and licenses not expressly granted herein. 2.4 Feedback. You acknowledge and agree that all Feedback will be the sole and exclusive property of New Relic. You hereby irrevocably transfer and assign to New Relic all right, title, and interest in all Feedback, including but not limited to all intellectual property and proprietary rights therein. 3. Confidential Information You will not use or disclose any Confidential Information, except as necessary for the performance of this Policy, and you will use reasonable efforts to protect Confidential Information from unauthorized use or disclosure. You may disclose Confidential Information only to those employees with a bona fide need to know, provided that each employee has signed a written agreement with nondisclosure restrictions at least as protective of the Confidential Information as those set forth herein. For the purposes of this section, information will not be deemed Confidential Information if it: (a) is or becomes generally known to the public through no fault or breach of this Policy; (b) is rightfully known by you at the time of disclosure without an obligation of confidentiality; (c) is independently developed by you without access to or use of the Confidential Information; or (d) is rightfully obtained by you from a third party without restriction on use or disclosure. Upon New Relic’s request, you agree to destroy all Confidential Information in your possession within 30 days of termination of this Policy. 4. Disclaimers and Acknowledgement 4.1 Acknowledgement of Pre-Release Service(s). You acknowledge and agree that: (a) the Pre-Release Service(s) is not an official product and has not been commercially released for sale by New Relic; (b) the Pre-Release Service(s) may not operate properly, be in final form, or fully function; (c) the Pre-Release Service(s) may contain errors, design flaws, security vulnerabilities, or other problems; (d) it may not be possible to make the Pre-Release Service(s) fully functional or secure; (e) the information obtained using the Pre-Release Service(s) may not be accurate; (f) use of the Pre-Release Service(s) may result in unexpected results, vulnerabilities, loss of data, project delays, or other unpredictable damage or loss, including without limitation to your use of New Relic services and products not governed by this Policy (collectively “Unintended Effects”); (g) New Relic is under no obligation to release a commercial or generally available version of the Pre-Release Service(s); and (h) New Relic has the right unilaterally to abandon development of the Pre-Release Service(s) at any time and without any obligation or liability to you. 4.2 Your Data. You acknowledge and agree that you should not rely on the Pre-Release Service(s) for any reason. You further acknowledge and agree that you are solely responsible for maintaining and protecting all data and information that is stored, retrieved, or otherwise processed by the Pre-Release Service(s) in accordance with your contractual obligations and requirements under applicable law. Without limiting the foregoing, you are responsible for all costs and expenses required to backup and restore any data and information that is lost or corrupted as a result of your use of the Pre-Release Service(s) or any Unintended Effects. You understand and agree that your Data may be transferred to the United States for storage, processing, and use by New Relic to provide the Pre-Release Service(s). For any data that you process using the Pre-Release Service(s), you represent and warrant that you have all necessary rights and consent to do so. New Relic has no obligation to store any data you process using the Pre-Release Service(s) and shall have no liability for the deletion or accuracy of such data. 4.3 Warranty Disclaimers. YOU ACKNOWLEDGE THAT PRE-RELEASE SERVICE(S) ARE PROVIDED “AS IS.” TO THE EXTENT PERMITTED BY APPLICABLE LAW, NEW RELIC DISCLAIMS ALL WARRANTIES, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO ANY WARRANTIES REGARDING MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, SECURITY, AND ANY WARRANTIES ARISING OUT OF THE COURSE OF DEALING OR USAGE OF TRADE. NEW RELIC MAKES NO WARRANTY THAT PRE-RELEASE SERVICE(S) WILL MEET YOUR REQUIREMENTS OR WILL BE ACCURATE, RELIABLE, ERROR-FREE, UNINTERRUPTED, TIMELY, OR SECURE. YOUR USE OF PRE-RELEASE SERVICE(S) IS AT YOUR OWN RISK. 4.4 Modifications. New Relic may change the Policy from time to time as our business changes and technology evolves, and future versions of our generally available services may not be compatible with Pre-Release Service(s) built using previous versions. The most current version of this Policy will be posted in the Documentation. Any changes to this Policy will be effective immediately for all users in instances to comply with applicable law, for new users of a Pre-Release Service(s) and, for all other users, any changes to this Policy will be effective as of fifteen (15) days after posting notice of such changes. If we determine in our sole discretion that an update is material, we will provide notice of such material change to you through your New Relic account, the Pre-Release Service(s), the Documentation, our blogs or community forums, and/or by email to the email address of your account administrator. We may require you to provide consent to the updated Policy in a specified manner before further use of the Pre-Release Service(s) is permitted. If you do not agree to any change(s), your sole remedy is to stop using the Pre-Release Service(s) and any such termination by you shall be without penalty except as specified in this Policy. Otherwise, your use of any Pre-Release Service(s) after any such update shall constitute acceptance of the then-current Policy. 5. Limitation of Liability TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC, ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE FOR ANY INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, EXEMPLARY OR CONSEQUENTIAL (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) DAMAGES, REGARDLESS OF LEGAL THEORY, WHETHER OR NOT NEW RELIC HAS BEEN WARNED OF THE POSSIBILITY OF SUCH DAMAGES, AND EVEN IF A REMEDY FAILS OF ITS ESSENTIAL PURPOSE. NEW RELIC’S AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS POLICY, THE PRE-RELEASE SERVICES, OR ANY UNINTENDED EFFECTS WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE LIMITATIONS OF DAMAGES SET FORTH ABOVE ARE FUNDAMENTAL ELEMENTS OF THE BASIS OF THE BARGAIN BETWEEN NEW RELIC AND YOU. 6. Indemnity You agree to indemnify and hold New Relic, its parents, subsidiaries, affiliates, officers, agents, employees, and licensors harmless from any claims, fees, fines, demands, losses, liabilities, damages, and costs, including reasonable attorney’s fees, arising from or related to your use of the Pre-Release Service(s), including but not limited to allegations arising from your breach of any terms herein and/or allegations that data processed by the Pre-Release Service(s) violates or infringes the privacy, data protection, or intellectual property rights of a third party. 7. Export Restrictions You acknowledge that the software in the Pre-Release Service(s) licensed hereunder may be subject to the export control laws and regulations of the U.S. and other countries. You agree that you will not export or re-export the Pre-Release Service(s), any part thereof, or any process or service that is the direct product of the Pre-Release Service(s) to any country, person or entity subject to U.S. export restrictions. 8. Termination Your access to the Pre-Release Service(s) will terminate upon the earliest of: (i) New Relic making the Pre-Release Service(s) or a successor version of the Pre-Release Service(s) generally available or available for commercial release; (ii) New Relic ceasing to make the Pre-Release Service(s) available to you; or (iii) New Relic terminating this Policy. New Relic may terminate this Policy for any reason upon notice to you. Sections 2.2, 2.3, 2.4, 3, 4, 5, 6, and 7 will survive your use of the Pre-Release Service(s).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 192.18352,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic Pre-release <em>policy</em>",
        "sections": "2. <em>License</em> and Restrictions",
        "tags": "<em>License</em> <em>information</em>",
        "body": " Services and any related <em>information</em>, including without limitation features, functionality, designs, user interface, capabilities, specifications, architectural diagrams, usage data, APIs, deployment schedules, email lists, bug databases, know how, source code, potential features, Feedback"
      },
      "id": "6044e71b28ccbc984d2c60c8"
    }
  ],
  "/docs/licenses/license-information/referenced-policies/security-policy": [
    {
      "sections": [
        "Security guide",
        "Tip",
        "Security Program",
        "Security Domains",
        "Security Certifications",
        "Data Control, Facilities, and Encryption",
        "Law Enforcement Request Report"
      ],
      "title": "Security guide",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "356f0d11ffcb62208a743a0a7c127f5f6da9c940",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-guide/",
      "published_at": "2021-10-07T02:08:45Z",
      "updated_at": "2021-09-19T15:21:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Last updated September 17, 2021. This is supplement to our security policy and serves as a guide to New Relic’s description of its Services, functionalities, and features. Tip We may update the URLs in this document without notice. Security Program New Relic follows \"privacy by design\" principles as described here: https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Security Domains New Relic’s policies and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless Security; Configuration Management; Vulnerability Management; Network Protection; Transmission Protection; Password Management; Access Control, Audit Logging & Monitoring; Education, Training, and Awareness; Third Party Assurance; Incident Management; Business Continuity and Disaster Recover; Risk Management; Data Protection & Privacy; and Service Management Systems. Security Certifications New Relic audits its Services against industry standards as described at https://docs.newrelic.com/docs/security/security-privacy/compliance/regulatory-audits-new-relic-services/. Data Control, Facilities, and Encryption New Relic's customers can send data to New Relic's APIs by (1) using New Relic's software, (2) using vendor-neutral software that is managed and maintained by a third-party such as via OpenTelemetry instrumentation provided by opentelemetry.io, or (3) from third-party systems that customer's manage and/or control. New Relic's customers can use New Relic's Services such as NerdGraph to filter out and drop data. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/drop-data-using-nerdgraph/. New Relic's customers can adjust their data retention periods as appropriate for their needs. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/manage-data-retention/#adjust-retention. New Relic Logs obfuscates numbers that match known patterns, such as bank card and social security numbers as described here: https://docs.newrelic.com/docs/logs/log-management/get-started/new-relics-log-management-security-privacy/. New Relic honors requests to delete personal data in accordance with applicable privacy laws. Please see https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Customers may use New Relic's APIs to query data, such as NerdGraph described here, and New Relic Services to export the data to other cloud providers. Customers can configure its log forwarder [https://docs.newrelic.com/docs/logs/enable-log-management-new-relic/enable-log-monitoring-new-relic/forward-your-logs-using-infrastructure-agent/] before sending infrastructure logs to New Relic. For New Relic Customers in New Relic US, FedRAMP and HIPAA-enabled environments, Customer Data is replicated to the off-site backup system via Amazon Simple Storage Service (S3). Category of Customer Description FedRAMP HIPAA-enabled US Gen Pop EU Gen Pop Data is stored in Amazon Web Services (“AWS”). Limited Data is stored in IBM Data for New Relic Incident Intelligence is stored in Google Cloud New Relic regularly tests, assess, and evaluates its measures to ensure the security of processing using industry-recognized standards and uses independent third-party auditors as provided below: Annual SOC 2 Type 2 Annual FedRAMP assessment by an independent third-party pursuant to NIST 800-53 rev 4 Moderate authorization. Annual HITRUST-validated assessment by an independent third-party *Pursuing CY2021 Q4 ISO 27001 *Pursuing CY2021 TISAX *Pursuing CY2021 The Services that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed information about AWS security is available at https://aws.amazon.com/security/ and http://aws.amazon.com/security/sharing-the-security-responsibility/. Data encryption at rest utilizes FIPS 140-2 compliant encryption methodology. For AWS SOC Reports, please see https://aws.amazon.com/compliance/soc-faqs/. The Services that operate on Google Cloud Platform (\"GCP\") are protected by the security and environmental controls of GCP. Detailed information about GCP security is available at https://cloud.google.com/docs/tutorials#security. For GCP reports, please see https://cloud.google.com/security/compliance/. IBM Deft Zayo QTS Law Enforcement Request Report New Relic has not to date received any request for customer data from a law enforcement or other government agency (including under any national security process), and has not made any corresponding disclosures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 253.09729,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " as described here: https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;security&#x2F;security-privacy&#x2F;data-privacy&#x2F;data-privacy-new-relic&#x2F;. Security Domains New Relic’s <em>policies</em> and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless"
      },
      "id": "6147558128ccbc973a56a863"
    },
    {
      "sections": [
        "Service level availability commitment",
        "New Relic Service Level Availability"
      ],
      "title": "Service level availability commitment",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "5489f268010887c72446f12059a1a0e4279ad960",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/service-level-availability-commitment/",
      "published_at": "2021-10-08T06:54:27Z",
      "updated_at": "2021-03-16T04:22:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you subscribe to New Relic Full Stack Observability Pro or Enterprise Products, the service level availability commitments available to you are those set forth (i) in the Terms, or (ii) as set forth on this page. If you subscribe to New Relic Full Stack Observability Standard, any service level availability commitment or related remedies contained in your Terms are vacated and nullified, and New Relic will use commercially reasonable efforts to make New Relic Full Stack Observability Standard and Telemetry Data Platform available in line with industry standards. If you subscribe to any other New Relic Products on a product-based pricing basis, any service level availability commitment are contained in (i) your Terms, or (ii) if your Terms explicitly references this Service Level Availability commitment applying to the Service purchased in an Order, this page for the Services you use. Capitalized terms not defined below shall take on the meaning set forth in your Terms. New Relic Service Level Availability The Service will be considered available so long as Customer is able to log in to its interface and view Customer Data (\"Service Availability\"). The applicable Service Availability will be calculated as a percentage of: (1) the total number of minutes in a month after (2) subtracting any periods of unavailability during such month from the total number of minutes in a month. New Relic will use commercially reasonable efforts to maintain Service Availability of at least 99.8% during any calendar month. In the event the Service Availability drops below: (i) 98.5% for two consecutive calendar months during the Subscription Term, or (ii) 96.5% in any single calendar month, Customer may request to terminate the relevant Service with no penalty. Such termination will be effective as of the end of the then-current billing period and no additional fees will be charged. The service level within New Relic's control is the Service Availability, not, for example, the transmission of data over the public Internet. Service Availability calculations will exclude unavailability arising from any: (a) planned maintenance periods; (b) emergency maintenance that is necessary to prevent imminent harm to the Service; (c) force majeure events; (d) Third-Party Services, Customer application, equipment, software or other technology, or Customer or its User's use of the Service, in violation of the Agreement or not in accordance with the Documentation; or (e) suspension, limitation, and/or termination of Customer’s access or use of the Service in accordance with this Agreement. This describes Customer’s sole and exclusive remedy for failures of Service Availability. Customer may request the Service Availability attainment for the previous month by filing a support ticket on the New Relic support site.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 192.31628,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>"
      },
      "id": "603ea44828ccbcd9c8eba7b7"
    },
    {
      "sections": [
        "New Relic Pre-release policy",
        "New Relic Pre-Release Policy",
        "1. Introduction",
        "2. License and Restrictions",
        "3. Confidential Information",
        "4. Disclaimers and Acknowledgement",
        "5. Limitation of Liability",
        "6. Indemnity",
        "7. Export Restrictions",
        "8. Termination"
      ],
      "title": "New Relic Pre-release policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "5ca324e942906913fc346245cfd80d405e90ad1f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/new-relic-pre-release-policy/",
      "published_at": "2021-10-08T06:54:27Z",
      "updated_at": "2021-03-13T01:17:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Without overriding any express provisions that you may have agreed to in a separate written beta or pre-release agreement executed by you and New Relic, the following terms apply to your use of any New Relic Pre-Release Service(s). New Relic Pre-Release Policy Through your use of any New Relic, Inc. (“New Relic”) Pre-Release Services (as further defined), you signify that you have read, understood, and accept the terms of this Pre-Release Policy (the “Policy”) governing your use and participation in New Relic’s program for making beta, alpha and pre-release versions of New Relic software (i.e. agents, SDKs, APIs, integrations, private locations, code, etc.), services, features, user-interfaces, and platforms made available for evaluation purposes (the “Pre-Release Service(s)”). This Policy is effective on behalf of yourself or the entity(ies) associated with your New Relic account (collectively referred to herein as “you” or “your”) as of your date of first use of the Pre-Release Service(s) (the “Effective Date”), which means that your choice to use the Pre-Release Service(s) means that you have the legal authority to use the Pre-Release Service(s) personally or on behalf of the company or organization associated with your New Relic account in accordance with this Policy. Please read this Policy carefully as your use of Pre-Release Service(s) may have unintended consequences and materially impact your data or use of the New Relic services and products. If you do not agree to this Policy, your sole remedy is to not make use of the Pre-Release Services. 1. Introduction This Policy and any New Relic technical guides and documentation made available with the Pre-Release Service(s) or from the dedicated ‘Documentation’ page of the New Relic website (the “Documentation”) enables you to test experimental features and products before they are made generally available. New Relic reserves the right to withhold or discontinue any Pre-Release Service(s), which may be designated as an alpha, beta, pilot, limited release, developer preview, technology preview, non-production, evaluation, or by a similar description, in its sole discretion and provides no guarantee that any Pre-Release Service(s) will eventually be made commercially or generally available. Pre-Release Service(s) are versions provided before they are generally available and may have bugs, stability issues, or other problems. Use of the Pre-Release Service(s) may result in unexpected results, loss of data, outages, or other damage to your systems or networks. New Relic advises against using the Pre-Release Service(s) in production. No support is offered with or for Pre-Release Service(s). You hereby release New Relic from any liability arising from or related to your use of the Pre-Release Service(s). In connection with your use of the Pre-Release Services, you may provide feedback, comments, and/or suggestions based on or relating to the Pre-Release Services (“Feedback”). The Pre-Release Services and any related information, including without limitation features, functionality, designs, user interface, capabilities, specifications, architectural diagrams, usage data, APIs, deployment schedules, email lists, bug databases, know how, source code, potential features, Feedback, and information related to any business or technical information of New Relic, including without limitation product plans, costs, prices, finances, marketing plans, business opportunities, research, development, and negotiations are considered confidential information of New Relic (“Confidential Information”). 2. License and Restrictions 2.1 Evaluation License. Subject to your compliance with the Policy, New Relic hereby grants you a limited, non-exclusive, non-transferable, revocable license, during the term of this Policy, to use the Pre-Release Service(s) in accordance with the Documentation solely for the purpose of internal evaluation and supplying Feedback to New Relic. 2.2 License Restrictions. You may not (and may not authorize or enable a third party to): (i) reverse engineer, decompile, disassemble, or otherwise attempt to discover the source code or other trade secrets in the Pre-Release Service(s); (ii) access the Pre-Release Service(s) in order to build a similar or competitive application, service, feature or other competitive purpose; (iii) remove or destroy any copyright notices or other proprietary marks contained on or in the Pre-Release Service(s); (iv) use the Pre-Release Service(s) for any benchmarking purposes or in connection with a service bureau, timeshare, service provider, or like activity where you operate the Pre-Release Service(s) on behalf of a third party; (v) mirror, frame, copy, modify, host, rent, lease, sell, commercialize, sublicense, assign or otherwise transfer the Pre-Release Service(s), or the access or use of the Pre-Release Service(s); (vi) use the Pre-Release Service(s) in a manner that may violate or infringe the intellectual property, data protection, or other proprietary right of a third party; (vii) use or access the Pre-Release Service(s) in an unauthorized manner and/or in a manner that violates any applicable law, rule, contract, or guideline (including but not limited to our Documentation or Community Guidelines); (viii) use the Pre-Release Service(s) to transmit worms, viruses, malicious code, security vulnerabilities, or otherwise negatively impact network operations, third parties, or New Relic; (ix) use any data mining or similar data gathering and extraction methods in connection with the Pre-Release Service(s); or (x) use the Pre-Release Service(s) to process sensitive personal information, e.g., “personal information” of children as defined by the Children’s Online Privacy Protection Act, “protected health information” as defined by the Health Insurance Portability and Accountability Act of 1996, government issued identification numbers, financial account information, payment card data, “special categories of data” as described in the EU General Data Protection Regulation (GDPR), or other information subject to regulatory, statutory, or contractual restrictions. New Relic reserves the right, but not the obligation, to monitor or review your use of the Pre-Release Service(s) at any time and may investigate any suspected violations of this Policy. 2.3 Intellectual Property Rights. You acknowledge and agree that, as between you and New Relic, New Relic owns all right, title, and interest in and to the Pre-Release Service(s), Confidential Information, and Feedback, including but not limited to all intellectual property and proprietary rights therein. New Relic and its licensors reserve all rights and licenses not expressly granted herein. 2.4 Feedback. You acknowledge and agree that all Feedback will be the sole and exclusive property of New Relic. You hereby irrevocably transfer and assign to New Relic all right, title, and interest in all Feedback, including but not limited to all intellectual property and proprietary rights therein. 3. Confidential Information You will not use or disclose any Confidential Information, except as necessary for the performance of this Policy, and you will use reasonable efforts to protect Confidential Information from unauthorized use or disclosure. You may disclose Confidential Information only to those employees with a bona fide need to know, provided that each employee has signed a written agreement with nondisclosure restrictions at least as protective of the Confidential Information as those set forth herein. For the purposes of this section, information will not be deemed Confidential Information if it: (a) is or becomes generally known to the public through no fault or breach of this Policy; (b) is rightfully known by you at the time of disclosure without an obligation of confidentiality; (c) is independently developed by you without access to or use of the Confidential Information; or (d) is rightfully obtained by you from a third party without restriction on use or disclosure. Upon New Relic’s request, you agree to destroy all Confidential Information in your possession within 30 days of termination of this Policy. 4. Disclaimers and Acknowledgement 4.1 Acknowledgement of Pre-Release Service(s). You acknowledge and agree that: (a) the Pre-Release Service(s) is not an official product and has not been commercially released for sale by New Relic; (b) the Pre-Release Service(s) may not operate properly, be in final form, or fully function; (c) the Pre-Release Service(s) may contain errors, design flaws, security vulnerabilities, or other problems; (d) it may not be possible to make the Pre-Release Service(s) fully functional or secure; (e) the information obtained using the Pre-Release Service(s) may not be accurate; (f) use of the Pre-Release Service(s) may result in unexpected results, vulnerabilities, loss of data, project delays, or other unpredictable damage or loss, including without limitation to your use of New Relic services and products not governed by this Policy (collectively “Unintended Effects”); (g) New Relic is under no obligation to release a commercial or generally available version of the Pre-Release Service(s); and (h) New Relic has the right unilaterally to abandon development of the Pre-Release Service(s) at any time and without any obligation or liability to you. 4.2 Your Data. You acknowledge and agree that you should not rely on the Pre-Release Service(s) for any reason. You further acknowledge and agree that you are solely responsible for maintaining and protecting all data and information that is stored, retrieved, or otherwise processed by the Pre-Release Service(s) in accordance with your contractual obligations and requirements under applicable law. Without limiting the foregoing, you are responsible for all costs and expenses required to backup and restore any data and information that is lost or corrupted as a result of your use of the Pre-Release Service(s) or any Unintended Effects. You understand and agree that your Data may be transferred to the United States for storage, processing, and use by New Relic to provide the Pre-Release Service(s). For any data that you process using the Pre-Release Service(s), you represent and warrant that you have all necessary rights and consent to do so. New Relic has no obligation to store any data you process using the Pre-Release Service(s) and shall have no liability for the deletion or accuracy of such data. 4.3 Warranty Disclaimers. YOU ACKNOWLEDGE THAT PRE-RELEASE SERVICE(S) ARE PROVIDED “AS IS.” TO THE EXTENT PERMITTED BY APPLICABLE LAW, NEW RELIC DISCLAIMS ALL WARRANTIES, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO ANY WARRANTIES REGARDING MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, SECURITY, AND ANY WARRANTIES ARISING OUT OF THE COURSE OF DEALING OR USAGE OF TRADE. NEW RELIC MAKES NO WARRANTY THAT PRE-RELEASE SERVICE(S) WILL MEET YOUR REQUIREMENTS OR WILL BE ACCURATE, RELIABLE, ERROR-FREE, UNINTERRUPTED, TIMELY, OR SECURE. YOUR USE OF PRE-RELEASE SERVICE(S) IS AT YOUR OWN RISK. 4.4 Modifications. New Relic may change the Policy from time to time as our business changes and technology evolves, and future versions of our generally available services may not be compatible with Pre-Release Service(s) built using previous versions. The most current version of this Policy will be posted in the Documentation. Any changes to this Policy will be effective immediately for all users in instances to comply with applicable law, for new users of a Pre-Release Service(s) and, for all other users, any changes to this Policy will be effective as of fifteen (15) days after posting notice of such changes. If we determine in our sole discretion that an update is material, we will provide notice of such material change to you through your New Relic account, the Pre-Release Service(s), the Documentation, our blogs or community forums, and/or by email to the email address of your account administrator. We may require you to provide consent to the updated Policy in a specified manner before further use of the Pre-Release Service(s) is permitted. If you do not agree to any change(s), your sole remedy is to stop using the Pre-Release Service(s) and any such termination by you shall be without penalty except as specified in this Policy. Otherwise, your use of any Pre-Release Service(s) after any such update shall constitute acceptance of the then-current Policy. 5. Limitation of Liability TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC, ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE FOR ANY INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, EXEMPLARY OR CONSEQUENTIAL (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) DAMAGES, REGARDLESS OF LEGAL THEORY, WHETHER OR NOT NEW RELIC HAS BEEN WARNED OF THE POSSIBILITY OF SUCH DAMAGES, AND EVEN IF A REMEDY FAILS OF ITS ESSENTIAL PURPOSE. NEW RELIC’S AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS POLICY, THE PRE-RELEASE SERVICES, OR ANY UNINTENDED EFFECTS WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE LIMITATIONS OF DAMAGES SET FORTH ABOVE ARE FUNDAMENTAL ELEMENTS OF THE BASIS OF THE BARGAIN BETWEEN NEW RELIC AND YOU. 6. Indemnity You agree to indemnify and hold New Relic, its parents, subsidiaries, affiliates, officers, agents, employees, and licensors harmless from any claims, fees, fines, demands, losses, liabilities, damages, and costs, including reasonable attorney’s fees, arising from or related to your use of the Pre-Release Service(s), including but not limited to allegations arising from your breach of any terms herein and/or allegations that data processed by the Pre-Release Service(s) violates or infringes the privacy, data protection, or intellectual property rights of a third party. 7. Export Restrictions You acknowledge that the software in the Pre-Release Service(s) licensed hereunder may be subject to the export control laws and regulations of the U.S. and other countries. You agree that you will not export or re-export the Pre-Release Service(s), any part thereof, or any process or service that is the direct product of the Pre-Release Service(s) to any country, person or entity subject to U.S. export restrictions. 8. Termination Your access to the Pre-Release Service(s) will terminate upon the earliest of: (i) New Relic making the Pre-Release Service(s) or a successor version of the Pre-Release Service(s) generally available or available for commercial release; (ii) New Relic ceasing to make the Pre-Release Service(s) available to you; or (iii) New Relic terminating this Policy. New Relic may terminate this Policy for any reason upon notice to you. Sections 2.2, 2.3, 2.4, 3, 4, 5, 6, and 7 will survive your use of the Pre-Release Service(s).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 192.18352,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic Pre-release <em>policy</em>",
        "sections": "2. <em>License</em> and Restrictions",
        "tags": "<em>License</em> <em>information</em>",
        "body": " Services and any related <em>information</em>, including without limitation features, functionality, designs, user interface, capabilities, specifications, architectural diagrams, usage data, APIs, deployment schedules, email lists, bug databases, know how, source code, potential features, Feedback"
      },
      "id": "6044e71b28ccbc984d2c60c8"
    }
  ],
  "/docs/licenses/license-information/referenced-policies/service-level-availability-commitment": [
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Purpose",
        "2. Security Overview",
        "3.Security Certifications",
        "4. Data Control and Encryption",
        "5. Facilities",
        "6. Employee Access, Screening and Controls.",
        "7. Security Incident and Data Breach Response",
        "8. Network and Systems Security",
        "9. Authentication and Access Management",
        "10. Vulnerability Management",
        "11. System Access and Logging",
        "12. Disaster Recovery and Data Backup",
        "13. Copies and Removal",
        "14. Third Party Vendor Management",
        "15. Disclosure by Law",
        "16. Updates"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2021-10-08T06:55:04Z",
      "updated_at": "2021-09-27T15:05:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Updated 23 September 2021. The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Purpose This Policy describes New Relic's security program and the technical and organizational security controls to protect New Relic’s systems as shown in New Relic’s third-party audits and certifications. New Relic may update this Policy from time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/. 2. Security Overview 2.1. New Relic maintains a comprehensive Information Protection Program to manage information security within New Relic that includes administrative, technical, and physical safeguards designed to protect the confidentiality, integrity, and availability of Customer Data and that is appropriate to the nature, size, and complexity of New Relic's business operations. New Relic’s Information Protection Program includes: 2.1.1. executive review, support and accountability for all security related policies and practices; 2.1.2. formal written policies and procedures that are designed to protect against the loss, theft, or other unauthorized access or alteration of Customer Data, meet or exceed applicable industry standards, outlines a definition of information security and its overall objectives, include defined information security roles and responsibilities, a framework for setting control objectives and controls, a formal and effective risk mitigation program and a service provider security management program; 2.1.3. periodic risk assessments, including internal audits and/or independent 3rd party audits, to measure the effectiveness and appropriateness of controls for all systems processing Customer Data; 2.1.4. in-depth review of security incidents, including effective determination of root cause and corrective action; formal, industry-recognized controls frameworks based on an external audit standard; 2.1.5. employee screening and access management; and 2.1.6. comprehensive security testing, vulnerability identification, and mitigation methodology that consists of a variety of independent approaches that, when combined, are designed to maximize coverage for a diverse set of attack vectors. 2.2. New Relic's Chief Information Security Officer (CISO) leads New Relic's Information Security Program and develops, reviews, and approves, with appropriate stakeholders, New Relic's security policies and procedures. New Relic has appointed data protection officer(s) as described in New Relic’s Privacy Notices who are consulted as necessary under applicable laws. 2.3. Information security objectives, approach, scope, importance, goals, and principles for the organization’s security program are formally identified, communicated throughout the organization to users in a form that is relevant, accessible, and understandable to the intended reader; and supported by a controls framework that considers legislative, regulatory, contractual requirements, and other policy-related requirements. 2.4. New Relic will maintain written policies and procedures to review, test, and approve (as appropriate) changes affecting New Relic infrastructure and systems that process Customer Data including, but not limited to, peer reviews prior to introducing new code into production. New Relic will establish acceptance criteria for new information systems, upgrades, and new versions and will carry out suitable tests of the system(s) during development and prior to acceptance. Any changes or updates will not materially decrease the security of the systems. 3.Security Certifications 3.1. New Relic regularly tests, assesses, and evaluates its security measures for protecting Customer Data using industry-recognized standards and uses independent third-party auditors to verify such controls. 3.2. New Relic agrees to provide Customer, upon request, with applicable certifications or reports about New Relic systems. All information exchanged in connection with the audit activities described in this section is deemed to be the Confidential Information of New Relic. 3.3. Additional information about New Relic’s security certifications are available on New Relic’s Security Guide. 4. Data Control and Encryption 4.1. The Service and related features are designed to provide Customer with control over Customer’s data sources and Customer’s environments that are monitored and sending data to New Relic. 4.2. New Relic will have established methods to: (i) enable Customer to use encryption on Customer Data in transit, and (ii) securely hash passwords in storage following industry standard practices (e.g. scrypt) as described in the Documentation. Server certificate-based authentication is used as part of the TLS encryption with a trusted certificate authority. 4.3. New Relic receives and processes data in accordance with the Agreement and the Services as described in the Documentation. New Relic permits customers to delete personal data in accordance with applicable privacy laws as further described in the Documentation. In the event of error in personal data sent in Customer Data Customers may request personal data deletion and re-send data that is accurate. 4.4. Additional information for Customer data control and encryption, including encryption of data at rest, is available on New Relic’s Security Guide. 5. Facilities 5.1. All physical locations that process Customer Data are co-locations and third party data centers. All physical locations have security measures in place that are designed to prevent unauthorized physical access to data processing facilities and unlawful access, modification, or destruction of Customer Data. 5.2. New Relic will seek assurances (e.g., in the form of an independent 3rd party audit report such as the SOC 2 Type 2, ISO 27001, and vendor security evaluations) from its data processing facilities vendors that store or process Customer Data: 5.2.1. secure its data process facilities in an access-controlled location and have protections in place to prevent unauthorized access, damage, and interference; 5.2.2 employ physical security appropriate to the classification of the assets and information being managed which may include card key access, security cameras, and solid wall construction for all exterior walls; 5.2.3 limit and screen all entrants, which may include measures such as on-site security guard, badge reader, electronic lock, or a monitored closed caption television (CCTV); and 5.2.4 maintain relevant access logs. 5.3. Additional information about New Relic’s third party data centers are available on New Relic’s Security Guide. 6. Employee Access, Screening and Controls. New Relic will have and maintain policies and practices that include, at a minimum, the following controls and safeguards applied to New Relic employees and contractors who may access Customer Data: 6.1. For U.S. New Relic employees and subject to applicable law, a criminal background screening of each of its new employees to whom it gives access to Customer Data at the federal, state, and county levels. For non-U.S. New Relic employees, New Relic will use commercially reasonable efforts to meet the same criteria as established for U.S.-based New Relic employees, subject to general business practices in the respective country and in compliance with applicable local law requirements; 6.2. All New Relic employees with access to Customer Data will undergo adequate training, such as annual security awareness training, in the care, protection and handling of Customer Data, and will align with the privacy and security measures set out in this Addendum by following the guidance provided in their welcome package, which includes New Relic’s security policies and a security acknowledgement; 6.3. A disciplinary policy and process, to be used when New Relic employees violate New Relic security or privacy policies or access Customer Data without prior authorization; 6.4. Administrative or remote access to New Relic systems that process Customer Data will align with industry standard practices, including multi-factor authentication; 6.5. Restricted access to New Relic proprietary source code to prevent unauthorized access; 6.6. Controls designed to ensure that only those New Relic employees with an actual need-to-know will have access to New Relic systems including, but not limited to, the use of a formal access management process for the request, review, approval, and provisioning; 6.7. Controls designed to ensure that New Relic employees are granted access to New Relic systems based on least-privilege principles; and 6.8. Revoke access to New Relic employees no longer requiring access. 7. Security Incident and Data Breach Response 7.1. New Relic will take appropriate physical, technical, and administrative security measures that are commercially reasonable and consistent with industry standards to prevent a Data Breach, and as required by any applicable law or regulation. “Data Breach” means the theft, loss, or unauthorized access of Customer Data. Without limiting the foregoing, New Relic will implement security measures at least as stringent as those set out in this Addendum. New Relic will designate a senior representative to provide incident briefings, as needed in case of a Data Breach, and to respond to reasonable requests by Customer pertaining to privacy and data security issues within a commercially reasonable time frame. 7.2. New Relic will: 7.2.1. Notify Customer without undue delay if New Relic becomes aware of a Data Breach; 7.2.2. Maintain a security incident response plan, including procedures and means to respond in a manner consistent with industry standards; 7.2.3. Reasonably cooperate with Customer to investigate and remediate a Data Breach and mitigate any further risk to the Customer Data, or risk to data subjects and/or Customer’s reputation or brand; 7.2.4. Provide reasonable assistance to Customer at New Relic’s sole cost and expense; and 7.2.5. Make commercially reasonable efforts to preserve evidence and reasonably cooperate with Customer and legal authorities (as applicable and legally permissible) during an investigation of a Data Breach. 8. Network and Systems Security 8.1. All extranet connectivity by New Relic personnel to systems processing Customer Data will be through secure remote connections. 8.2. Network segments connected to the Internet will be protected by secure access control mechanisms, such as a firewall configured to secure all devices behind it and properly addresses security concerns according to industry standard practices. 8.3. New Relic will have industry standard measures in place to actively monitor its systems and help detect a potential hostile attack, such as Network Intrusion Detection (NID) or Host Intrusion Detection (HID)/Prevention. 8.4. Applications, ports, services, and similar access points installed on a computer or network facility, which are not specifically required for business functionality, will be disabled or removed. 8.5. New Relic maintains configuration standards for authorized operating systems and software for systems that support processing of Customer Data. New Relic will establish and follow server configuration guidelines and processes for preventing unauthorized access to Customer Data. New Relic maintains secure images or templates for systems based on the organization’s approved configuration standards. 8.6. Development, test, and operational environments will be logically separated to reduce the risks of unauthorized access or changes to the operational system. 8.7. New Relic network architecture will be designed to limit site access and restrict the availability of information systems that are considered to be vulnerable to attack. 8.8. New Relic provides appropriate TLS certificates when users access and view Customer Data in the Service. New Relic’s software for sending Customer Data to New Relic will encrypt Customer Data in transit by default. 9. Authentication and Access Management If you subscribe to the requisite New Relic One Service, New Relic will provide industry standard authentication and access controls to protect Customer Data, including industry standard authentication methods utilized to help prevent unauthorized access to the Service. New Relic’s access control methods will clearly state the rules and rights for each user or group of users including applications and information sharing and will include a process for granting and removing access to all information systems processing Customer Data. A record of all privileges allocated will be maintained pursuant to the requirements herein. 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. 10. Vulnerability Management New Relic will have and maintain the following vulnerability management processes for all devices used to connect to the New Relic network and Services. 10.1. New Relic will align to industry standard practices for build out, minimization of services and secure configuration, in accordance with, industry-recognized minimum security baselines for the New Relic platform and subcontractor systems connected to the New Relic platform in relation to the provision of the Services. 10.2. New Relic will employ industry-recognized standards and tools to conduct frequent infrastructure vulnerability scanning to test New Relic’s network and infrastructure and application penetration testing to test the New Relic Services. New Relic applies “Medium”, “High” and “Critical” security patches for all components in production and development environments as soon as commercially possible in accordance with its vulnerability management protocol, and consistent with industry standard practices and standards; 10.3. New Relic will have processes in place designed to ensure adherence to industry standard security development practices for development and testing for all code, APIs, and applications deployed and implemented in support of the Service; 10.4. New Relic will have and maintain solutions to identify and prevent malicious attackers or code from accessing or compromising Customer Data or systems that process Customer Data. These include, but are not limited to, software that identifies and removes malware and detects attempted intrusions. New Relic will have a security event and incident monitoring system and supporting processes to notify appropriate personnel in response to threats. 11. System Access and Logging 11.1. Access to New Relic’s systems will not be granted to employees of New Relic unless they have been uniquely identified and have sufficient credentials. 11.2. Access to New Relic’s systems will be logged and retained for no less than 6 months to assist in investigations and access control monitoring, including, but not limited to, end user access and activities, and information security events. 11.3. New Relic agrees to provide Customer the capability to access log records relating to Customer Accounts and the New Relic systems that process Customer Data in the event of a Data Breach or if required in connection with a law enforcement request. 12. Disaster Recovery and Data Backup 12.1. New Relic will have plans designed to respond to loss of services, which are tested and reviewed at least annually. This plan will include documented policies and procedures to restore service in the event of a service failure. 12.2. New Relic will establish and follow backup and restore procedures for Customer Data. 12.3. New Relic business continuity plan identifies critical systems. Annual disaster recovery tests are performed to check and restore customer data in the event of an incident. 12.4. New Relic will provide Customer with redacted copies of its plan(s) and evidence of tests/reviews upon request, but not more frequently than once annually, and subject to confidentiality requirements. 12.5. Additional information for Customer Data control and encryption are available on New Relic’s Security Guide. 13. Copies and Removal 13.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of the Agreement for any reason: (a) New Relic will, and will cause its personnel, to cease all access and use of any Customer Data, and (b) New Relic will delete all copies of Customer Data within ninety (90) days. 13.2. New Relic will maintain a process of ensuring secure destruction and deletion of all Customer Data, when reasonably requested by Customer or as otherwise provided in the Agreement. The process will include industry standard processes so that: (i) Customer Data cannot be practicably read or reconstructed, and (ii) the systems that store Customer Data are securely erased and/or decommissioned disks are destroyed. 14. Third Party Vendor Management New Relic may use third party vendors to provide the Services. New Relic performs a security risk-based assessment of prospective vendors before working with them to validate they meet New Relic’s security and business continuity standards, including the type of access and classification of data being accessed (if any), controls necessary to protect data, and legal/regulatory requirements. New Relic enters into written agreements with its vendors that process Customer Data which include confidentiality, privacy, and security obligations that provide an appropriate level of protection for Customer Data that these vendors may process for New Relic to maintain the security posture in this Policy, including following industry security standards. 15. Disclosure by Law In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic will (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. New Relic publishes its law enforcement requests report on New Relic’s Security Guide. 16. Updates As New Relic releases new products, services, functionality, and features, New Relic may update this Policy to account for such products, services, functionality, and features. For additional information, see our Security Guide.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 280.1387,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Security <em>policy</em>",
        "sections": "Security <em>policy</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;<em>referenced</em>-<em>policies</em>&#x2F;security-policy&#x2F;. 2. Security Overview 2.1. New Relic"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    },
    {
      "sections": [
        "Security guide",
        "Tip",
        "Security Program",
        "Security Domains",
        "Security Certifications",
        "Data Control, Facilities, and Encryption",
        "Law Enforcement Request Report"
      ],
      "title": "Security guide",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "356f0d11ffcb62208a743a0a7c127f5f6da9c940",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-guide/",
      "published_at": "2021-10-07T02:08:45Z",
      "updated_at": "2021-09-19T15:21:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Last updated September 17, 2021. This is supplement to our security policy and serves as a guide to New Relic’s description of its Services, functionalities, and features. Tip We may update the URLs in this document without notice. Security Program New Relic follows \"privacy by design\" principles as described here: https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Security Domains New Relic’s policies and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless Security; Configuration Management; Vulnerability Management; Network Protection; Transmission Protection; Password Management; Access Control, Audit Logging & Monitoring; Education, Training, and Awareness; Third Party Assurance; Incident Management; Business Continuity and Disaster Recover; Risk Management; Data Protection & Privacy; and Service Management Systems. Security Certifications New Relic audits its Services against industry standards as described at https://docs.newrelic.com/docs/security/security-privacy/compliance/regulatory-audits-new-relic-services/. Data Control, Facilities, and Encryption New Relic's customers can send data to New Relic's APIs by (1) using New Relic's software, (2) using vendor-neutral software that is managed and maintained by a third-party such as via OpenTelemetry instrumentation provided by opentelemetry.io, or (3) from third-party systems that customer's manage and/or control. New Relic's customers can use New Relic's Services such as NerdGraph to filter out and drop data. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/drop-data-using-nerdgraph/. New Relic's customers can adjust their data retention periods as appropriate for their needs. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/manage-data-retention/#adjust-retention. New Relic Logs obfuscates numbers that match known patterns, such as bank card and social security numbers as described here: https://docs.newrelic.com/docs/logs/log-management/get-started/new-relics-log-management-security-privacy/. New Relic honors requests to delete personal data in accordance with applicable privacy laws. Please see https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Customers may use New Relic's APIs to query data, such as NerdGraph described here, and New Relic Services to export the data to other cloud providers. Customers can configure its log forwarder [https://docs.newrelic.com/docs/logs/enable-log-management-new-relic/enable-log-monitoring-new-relic/forward-your-logs-using-infrastructure-agent/] before sending infrastructure logs to New Relic. For New Relic Customers in New Relic US, FedRAMP and HIPAA-enabled environments, Customer Data is replicated to the off-site backup system via Amazon Simple Storage Service (S3). Category of Customer Description FedRAMP HIPAA-enabled US Gen Pop EU Gen Pop Data is stored in Amazon Web Services (“AWS”). Limited Data is stored in IBM Data for New Relic Incident Intelligence is stored in Google Cloud New Relic regularly tests, assess, and evaluates its measures to ensure the security of processing using industry-recognized standards and uses independent third-party auditors as provided below: Annual SOC 2 Type 2 Annual FedRAMP assessment by an independent third-party pursuant to NIST 800-53 rev 4 Moderate authorization. Annual HITRUST-validated assessment by an independent third-party *Pursuing CY2021 Q4 ISO 27001 *Pursuing CY2021 TISAX *Pursuing CY2021 The Services that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed information about AWS security is available at https://aws.amazon.com/security/ and http://aws.amazon.com/security/sharing-the-security-responsibility/. Data encryption at rest utilizes FIPS 140-2 compliant encryption methodology. For AWS SOC Reports, please see https://aws.amazon.com/compliance/soc-faqs/. The Services that operate on Google Cloud Platform (\"GCP\") are protected by the security and environmental controls of GCP. Detailed information about GCP security is available at https://cloud.google.com/docs/tutorials#security. For GCP reports, please see https://cloud.google.com/security/compliance/. IBM Deft Zayo QTS Law Enforcement Request Report New Relic has not to date received any request for customer data from a law enforcement or other government agency (including under any national security process), and has not made any corresponding disclosures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 253.09729,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " as described here: https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;security&#x2F;security-privacy&#x2F;data-privacy&#x2F;data-privacy-new-relic&#x2F;. Security Domains New Relic’s <em>policies</em> and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless"
      },
      "id": "6147558128ccbc973a56a863"
    },
    {
      "sections": [
        "New Relic Pre-release policy",
        "New Relic Pre-Release Policy",
        "1. Introduction",
        "2. License and Restrictions",
        "3. Confidential Information",
        "4. Disclaimers and Acknowledgement",
        "5. Limitation of Liability",
        "6. Indemnity",
        "7. Export Restrictions",
        "8. Termination"
      ],
      "title": "New Relic Pre-release policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "5ca324e942906913fc346245cfd80d405e90ad1f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/new-relic-pre-release-policy/",
      "published_at": "2021-10-08T06:54:27Z",
      "updated_at": "2021-03-13T01:17:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Without overriding any express provisions that you may have agreed to in a separate written beta or pre-release agreement executed by you and New Relic, the following terms apply to your use of any New Relic Pre-Release Service(s). New Relic Pre-Release Policy Through your use of any New Relic, Inc. (“New Relic”) Pre-Release Services (as further defined), you signify that you have read, understood, and accept the terms of this Pre-Release Policy (the “Policy”) governing your use and participation in New Relic’s program for making beta, alpha and pre-release versions of New Relic software (i.e. agents, SDKs, APIs, integrations, private locations, code, etc.), services, features, user-interfaces, and platforms made available for evaluation purposes (the “Pre-Release Service(s)”). This Policy is effective on behalf of yourself or the entity(ies) associated with your New Relic account (collectively referred to herein as “you” or “your”) as of your date of first use of the Pre-Release Service(s) (the “Effective Date”), which means that your choice to use the Pre-Release Service(s) means that you have the legal authority to use the Pre-Release Service(s) personally or on behalf of the company or organization associated with your New Relic account in accordance with this Policy. Please read this Policy carefully as your use of Pre-Release Service(s) may have unintended consequences and materially impact your data or use of the New Relic services and products. If you do not agree to this Policy, your sole remedy is to not make use of the Pre-Release Services. 1. Introduction This Policy and any New Relic technical guides and documentation made available with the Pre-Release Service(s) or from the dedicated ‘Documentation’ page of the New Relic website (the “Documentation”) enables you to test experimental features and products before they are made generally available. New Relic reserves the right to withhold or discontinue any Pre-Release Service(s), which may be designated as an alpha, beta, pilot, limited release, developer preview, technology preview, non-production, evaluation, or by a similar description, in its sole discretion and provides no guarantee that any Pre-Release Service(s) will eventually be made commercially or generally available. Pre-Release Service(s) are versions provided before they are generally available and may have bugs, stability issues, or other problems. Use of the Pre-Release Service(s) may result in unexpected results, loss of data, outages, or other damage to your systems or networks. New Relic advises against using the Pre-Release Service(s) in production. No support is offered with or for Pre-Release Service(s). You hereby release New Relic from any liability arising from or related to your use of the Pre-Release Service(s). In connection with your use of the Pre-Release Services, you may provide feedback, comments, and/or suggestions based on or relating to the Pre-Release Services (“Feedback”). The Pre-Release Services and any related information, including without limitation features, functionality, designs, user interface, capabilities, specifications, architectural diagrams, usage data, APIs, deployment schedules, email lists, bug databases, know how, source code, potential features, Feedback, and information related to any business or technical information of New Relic, including without limitation product plans, costs, prices, finances, marketing plans, business opportunities, research, development, and negotiations are considered confidential information of New Relic (“Confidential Information”). 2. License and Restrictions 2.1 Evaluation License. Subject to your compliance with the Policy, New Relic hereby grants you a limited, non-exclusive, non-transferable, revocable license, during the term of this Policy, to use the Pre-Release Service(s) in accordance with the Documentation solely for the purpose of internal evaluation and supplying Feedback to New Relic. 2.2 License Restrictions. You may not (and may not authorize or enable a third party to): (i) reverse engineer, decompile, disassemble, or otherwise attempt to discover the source code or other trade secrets in the Pre-Release Service(s); (ii) access the Pre-Release Service(s) in order to build a similar or competitive application, service, feature or other competitive purpose; (iii) remove or destroy any copyright notices or other proprietary marks contained on or in the Pre-Release Service(s); (iv) use the Pre-Release Service(s) for any benchmarking purposes or in connection with a service bureau, timeshare, service provider, or like activity where you operate the Pre-Release Service(s) on behalf of a third party; (v) mirror, frame, copy, modify, host, rent, lease, sell, commercialize, sublicense, assign or otherwise transfer the Pre-Release Service(s), or the access or use of the Pre-Release Service(s); (vi) use the Pre-Release Service(s) in a manner that may violate or infringe the intellectual property, data protection, or other proprietary right of a third party; (vii) use or access the Pre-Release Service(s) in an unauthorized manner and/or in a manner that violates any applicable law, rule, contract, or guideline (including but not limited to our Documentation or Community Guidelines); (viii) use the Pre-Release Service(s) to transmit worms, viruses, malicious code, security vulnerabilities, or otherwise negatively impact network operations, third parties, or New Relic; (ix) use any data mining or similar data gathering and extraction methods in connection with the Pre-Release Service(s); or (x) use the Pre-Release Service(s) to process sensitive personal information, e.g., “personal information” of children as defined by the Children’s Online Privacy Protection Act, “protected health information” as defined by the Health Insurance Portability and Accountability Act of 1996, government issued identification numbers, financial account information, payment card data, “special categories of data” as described in the EU General Data Protection Regulation (GDPR), or other information subject to regulatory, statutory, or contractual restrictions. New Relic reserves the right, but not the obligation, to monitor or review your use of the Pre-Release Service(s) at any time and may investigate any suspected violations of this Policy. 2.3 Intellectual Property Rights. You acknowledge and agree that, as between you and New Relic, New Relic owns all right, title, and interest in and to the Pre-Release Service(s), Confidential Information, and Feedback, including but not limited to all intellectual property and proprietary rights therein. New Relic and its licensors reserve all rights and licenses not expressly granted herein. 2.4 Feedback. You acknowledge and agree that all Feedback will be the sole and exclusive property of New Relic. You hereby irrevocably transfer and assign to New Relic all right, title, and interest in all Feedback, including but not limited to all intellectual property and proprietary rights therein. 3. Confidential Information You will not use or disclose any Confidential Information, except as necessary for the performance of this Policy, and you will use reasonable efforts to protect Confidential Information from unauthorized use or disclosure. You may disclose Confidential Information only to those employees with a bona fide need to know, provided that each employee has signed a written agreement with nondisclosure restrictions at least as protective of the Confidential Information as those set forth herein. For the purposes of this section, information will not be deemed Confidential Information if it: (a) is or becomes generally known to the public through no fault or breach of this Policy; (b) is rightfully known by you at the time of disclosure without an obligation of confidentiality; (c) is independently developed by you without access to or use of the Confidential Information; or (d) is rightfully obtained by you from a third party without restriction on use or disclosure. Upon New Relic’s request, you agree to destroy all Confidential Information in your possession within 30 days of termination of this Policy. 4. Disclaimers and Acknowledgement 4.1 Acknowledgement of Pre-Release Service(s). You acknowledge and agree that: (a) the Pre-Release Service(s) is not an official product and has not been commercially released for sale by New Relic; (b) the Pre-Release Service(s) may not operate properly, be in final form, or fully function; (c) the Pre-Release Service(s) may contain errors, design flaws, security vulnerabilities, or other problems; (d) it may not be possible to make the Pre-Release Service(s) fully functional or secure; (e) the information obtained using the Pre-Release Service(s) may not be accurate; (f) use of the Pre-Release Service(s) may result in unexpected results, vulnerabilities, loss of data, project delays, or other unpredictable damage or loss, including without limitation to your use of New Relic services and products not governed by this Policy (collectively “Unintended Effects”); (g) New Relic is under no obligation to release a commercial or generally available version of the Pre-Release Service(s); and (h) New Relic has the right unilaterally to abandon development of the Pre-Release Service(s) at any time and without any obligation or liability to you. 4.2 Your Data. You acknowledge and agree that you should not rely on the Pre-Release Service(s) for any reason. You further acknowledge and agree that you are solely responsible for maintaining and protecting all data and information that is stored, retrieved, or otherwise processed by the Pre-Release Service(s) in accordance with your contractual obligations and requirements under applicable law. Without limiting the foregoing, you are responsible for all costs and expenses required to backup and restore any data and information that is lost or corrupted as a result of your use of the Pre-Release Service(s) or any Unintended Effects. You understand and agree that your Data may be transferred to the United States for storage, processing, and use by New Relic to provide the Pre-Release Service(s). For any data that you process using the Pre-Release Service(s), you represent and warrant that you have all necessary rights and consent to do so. New Relic has no obligation to store any data you process using the Pre-Release Service(s) and shall have no liability for the deletion or accuracy of such data. 4.3 Warranty Disclaimers. YOU ACKNOWLEDGE THAT PRE-RELEASE SERVICE(S) ARE PROVIDED “AS IS.” TO THE EXTENT PERMITTED BY APPLICABLE LAW, NEW RELIC DISCLAIMS ALL WARRANTIES, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO ANY WARRANTIES REGARDING MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, SECURITY, AND ANY WARRANTIES ARISING OUT OF THE COURSE OF DEALING OR USAGE OF TRADE. NEW RELIC MAKES NO WARRANTY THAT PRE-RELEASE SERVICE(S) WILL MEET YOUR REQUIREMENTS OR WILL BE ACCURATE, RELIABLE, ERROR-FREE, UNINTERRUPTED, TIMELY, OR SECURE. YOUR USE OF PRE-RELEASE SERVICE(S) IS AT YOUR OWN RISK. 4.4 Modifications. New Relic may change the Policy from time to time as our business changes and technology evolves, and future versions of our generally available services may not be compatible with Pre-Release Service(s) built using previous versions. The most current version of this Policy will be posted in the Documentation. Any changes to this Policy will be effective immediately for all users in instances to comply with applicable law, for new users of a Pre-Release Service(s) and, for all other users, any changes to this Policy will be effective as of fifteen (15) days after posting notice of such changes. If we determine in our sole discretion that an update is material, we will provide notice of such material change to you through your New Relic account, the Pre-Release Service(s), the Documentation, our blogs or community forums, and/or by email to the email address of your account administrator. We may require you to provide consent to the updated Policy in a specified manner before further use of the Pre-Release Service(s) is permitted. If you do not agree to any change(s), your sole remedy is to stop using the Pre-Release Service(s) and any such termination by you shall be without penalty except as specified in this Policy. Otherwise, your use of any Pre-Release Service(s) after any such update shall constitute acceptance of the then-current Policy. 5. Limitation of Liability TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC, ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE FOR ANY INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, EXEMPLARY OR CONSEQUENTIAL (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) DAMAGES, REGARDLESS OF LEGAL THEORY, WHETHER OR NOT NEW RELIC HAS BEEN WARNED OF THE POSSIBILITY OF SUCH DAMAGES, AND EVEN IF A REMEDY FAILS OF ITS ESSENTIAL PURPOSE. NEW RELIC’S AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS POLICY, THE PRE-RELEASE SERVICES, OR ANY UNINTENDED EFFECTS WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE LIMITATIONS OF DAMAGES SET FORTH ABOVE ARE FUNDAMENTAL ELEMENTS OF THE BASIS OF THE BARGAIN BETWEEN NEW RELIC AND YOU. 6. Indemnity You agree to indemnify and hold New Relic, its parents, subsidiaries, affiliates, officers, agents, employees, and licensors harmless from any claims, fees, fines, demands, losses, liabilities, damages, and costs, including reasonable attorney’s fees, arising from or related to your use of the Pre-Release Service(s), including but not limited to allegations arising from your breach of any terms herein and/or allegations that data processed by the Pre-Release Service(s) violates or infringes the privacy, data protection, or intellectual property rights of a third party. 7. Export Restrictions You acknowledge that the software in the Pre-Release Service(s) licensed hereunder may be subject to the export control laws and regulations of the U.S. and other countries. You agree that you will not export or re-export the Pre-Release Service(s), any part thereof, or any process or service that is the direct product of the Pre-Release Service(s) to any country, person or entity subject to U.S. export restrictions. 8. Termination Your access to the Pre-Release Service(s) will terminate upon the earliest of: (i) New Relic making the Pre-Release Service(s) or a successor version of the Pre-Release Service(s) generally available or available for commercial release; (ii) New Relic ceasing to make the Pre-Release Service(s) available to you; or (iii) New Relic terminating this Policy. New Relic may terminate this Policy for any reason upon notice to you. Sections 2.2, 2.3, 2.4, 3, 4, 5, 6, and 7 will survive your use of the Pre-Release Service(s).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 192.18352,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic Pre-release <em>policy</em>",
        "sections": "2. <em>License</em> and Restrictions",
        "tags": "<em>License</em> <em>information</em>",
        "body": " Services and any related <em>information</em>, including without limitation features, functionality, designs, user interface, capabilities, specifications, architectural diagrams, usage data, APIs, deployment schedules, email lists, bug databases, know how, source code, potential features, Feedback"
      },
      "id": "6044e71b28ccbc984d2c60c8"
    }
  ],
  "/docs/licenses/license-information/special-services-licenses/data-collector-licenses": [
    {
      "sections": [
        "New Relic Priority Support"
      ],
      "title": "New Relic Priority Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "f700b8b349627e66540b23ed020c7d9a46e19580",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-priority-support/",
      "published_at": "2021-10-08T06:56:02Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Priority Support document as a PDF (172 KB). The services described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.57979,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Priority Support document as a PDF (172 KB). The <em>services</em> described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative."
      },
      "id": "603ea90ae7b9d2e9d42a07d1"
    },
    {
      "sections": [
        "New Relic Premium Support"
      ],
      "title": "New Relic Premium Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "db095214148ca053cdabd9815dc74b04299dbe7b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-premium-support/",
      "published_at": "2021-10-08T06:56:02Z",
      "updated_at": "2021-03-16T04:14:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Premium Support document as a PDF (248 KB). The services described in this document only apply to initial orders entered after April 1, 2018. If you have questions about New Relic Premium Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.5789,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Premium Support document as a PDF (248 KB). The <em>services</em> described in this document only apply to initial orders entered after April 1, 2018. If you have questions about New Relic Premium Support, contact your New Relic account representative."
      },
      "id": "603ea8c964441fe48b4e88af"
    },
    {
      "sections": [
        "New Relic Diagnostics licenses",
        "Proprietary license",
        "New Relic Diagnostics license terms",
        "Open-source licenses"
      ],
      "title": "New Relic Diagnostics licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "6d86f8f63d372be98270fadc8634f9da8da2a893",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-diagnostics-licenses/",
      "published_at": "2021-10-08T06:55:04Z",
      "updated_at": "2021-03-13T03:25:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Proprietary license New Relic Diagnostics license terms These New Relic Diagnostics License Terms (“Terms”) set forth the terms and conditions under which you (“Customer”) may use New Relic Diagnostics (the “Software”), as made available by New Relic, Inc. (“New Relic”). By clicking “accept” or downloading or using the Software, you agree to be bound by these Terms. If you are agreeing to these Terms on behalf of your company, then “Customer” means your company and you are binding your company to these Terms. 1. License Grant. Subject to all of the terms and conditions of these Terms, New Relic grants Customer a limited, non-exclusive, non-transferable, non-sublicensable license to use the Software in accordance with its documentation in support of Customer’s use of New Relic products to which Customer has a separate subscription (“New Relic Products”). Unless otherwise specified, there is no fee for use of the Software. 2. License Restrictions. Customer will not (a) sell, rent, sublicense, transfer, time-share or otherwise provide access to any copies of the Software, or portions thereof, to a third party; (b) modify, decompile, disassemble or reverse engineer the Software; (c) use the Software to develop services or products for sale or include any components of the Software in any product; (d) remove any product identification, proprietary, copyright or other notices in the Software; or (e) publicly disseminate performance information or analysis (including benchmarks) relating to the Software. 3. Ownership. Except for the limited license rights expressly provided herein, New Relic and its suppliers have and will retain all right, title and interest in and to the Software, and all copies, updates, modifications and derivative works thereof. Customer acknowledges that it is obtaining only a limited license right to the Software and no ownership rights are being conveyed to Customer under these Terms or otherwise. 4. Usage Data. Customer agrees that New Relic and its affiliates have the right to collect Usage Data from Customer through the Software and use Usage Data to support, operate and improve New Relic products and services and for other lawful business purposes. “Usage Data” means diagnostics data related to the use of the Software with New Relic Products, including, without limitation, configuration information, details on diagnostics tasks, New Relic account and application ID numbers and New Relic license keys. For clarity, Usage Data is not considered “Customer Data”, “your Data” or any other similar term as used in any applicable subscription or license agreement for New Relic Products. At Customer’s election, Customer may disable the collection of Usage Data as described in the Software documentation. 5. Disclaimers. New Relic is not obligated to provide any support or maintenance for the Software. ALL USE OF SOFTWARE IS AT CUSTOMER’S OWN RISK. SOFTWARE IS PROVIDED “AS IS,” WITH ALL FAULTS AND WITHOUT WARRANTY OF ANY KIND, EXPRESS, IMPLIED OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, WARRANTIES OF TITLE OR NON-INFRINGEMENT. CUSTOMER MAY HAVE OTHER STATUTORY RIGHTS, HOWEVER, THE DURATION OF STATUTORILY REQUIRED WARRANTIES, IF ANY, WILL BE LIMITED TO THE FULLEST EXTENT PERMITTED BY LAW. 6. Limitation of Liability. TO THE FULLEST EXTENT PERMITTED BY LAW, IN NO EVENT WILL NEW RELIC BE LIABLE FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES OF ANY KIND, REGARDLESS OF THE FORM OF ACTION, WHETHER IN CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY OR OTHERWISE, EVEN IF INFORMED OF THE POSSIBILITY OF SUCH DAMAGES IN ADVANCE. TO THE EXTENT ANY OF THE ABOVE LIMITATIONS ARE NOT ENFORCEABLE AT APPLICABLE LAW, NEW RELIC’S ENTIRE LIABILITY TO CUSTOMER UNDER THESE TERMS WILL NOT EXCEED $50. THESE LIMITATIONS ON LIABILITY ARE A FUNDAMENTAL BASIS OF THE BARGAIN AND NEW RELIC WOULD NOT BE ABLE TO PROVIDE THE SOFTWARE WITHOUT SUCH LIMITATIONS. THESE LIMITATIONS ON LIABILITY WILL APPLY NOTWITHSTANDING ANY FAILURE OF ESSENTIAL PURPOSE OF ANY LIMITED REMEDY. 7. Changes and Termination. At its discretion, without notice or liability to Customer, New Relic may (a) update, modify or discontinue the Software; (b) modify these Terms, effective upon posting; or (c) terminate or suspend these Terms or Customer’s access to the Software. If Customer does not agree with any modification, its sole remedy is to terminate its use of the Software. Upon any termination or suspension, Customer must stop using the Software. 8. Third Party Code. The Software may contain or be provided with third party code (including code which may be made available to Customer in source code form). A list of third party code and ownership, use, warranty and modification rights with respect to such code may be identified in the documentation or provided by New Relic upon Customer’s written request. New Relic is not responsible for applications and services not licensed by New Relic. 9. Export Compliance. Customer acknowledges that the Software is subject to export restrictions by the U.S. government and import restrictions by certain foreign governments. Customer shall not remove or export from the U.S. or allow the export or re-export of any part of the Software or any direct product thereof: (a) into (or to a national or resident of) any embargoed or terrorist-supporting country; (b) to anyone on the U.S. Commerce Department’s Table of Denial Orders or U.S. Treasury Department’s list of Specially Designated Nationals; (c) to any country to which such export or re-export is restricted or prohibited, or as to which the U.S. government or any agency thereof requires an export license or other governmental approval at the time of export or re-export without first obtaining such license or approval; or (d) otherwise in violation of any export or import restrictions, laws or regulations of any U.S. or foreign agency or authority. Customer agrees to the foregoing and warrants that Customer is not located in, under the control of, or a national or resident of any such prohibited country or on any such prohibited party list. 10. Government End-Users. The Software is commercial computer software. If Customer is an entity of the U.S. government, the use, duplication, reproduction, release, modification, disclosure or transfer of the Software, or any related documentation of any kind, is restricted by a license agreement or by these Terms in accordance with Federal Acquisition Regulation 12.212 for civilian purposes and Defense Federal Acquisition Regulation Supplement 227.7202 for military purposes. The Software was developed fully at private expense. All other use is prohibited. 11. General. These Terms will be governed by and construed under the laws of the State of California and the U.S. without regard to conflicts of law provisions thereof, and without regard to the United Nations Convention on the International Sale of Goods. The jurisdiction and venue for actions arising out of or relating to these Terms shall be in the state and federal courts in San Francisco, California. The parties are independent contractors. Customer may not assign these Terms without New Relic’s prior written consent and any attempt to do so will be void; New Relic may assign these Terms freely to any party without Customer’s consent. If any provision of these Terms is held by a court of competent jurisdiction to be unenforceable or invalid for any reason, that provision shall be limited to the minimum extent necessary so that these Terms shall otherwise remain in effect. These Terms are the entire agreement between the parties relating to the Software, and supersede all prior or contemporaneous agreements (oral or written) relating to the Software. Any separate agreement Customer has for New Relic Products does not apply to the Software. Open-source licenses We love open-source software, and use the following in New Relic Diagnostics. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License clbanning/mxj MIT go-yaml/yaml Apache 2.0 StackExchange/wmi MIT go-ole/go-ole MIT shirou/gopsutil BSD-3-Clause shirou/w32 BSD-3-Clause cheggaaa/pb BSD-3-Clause google/uuid BSD-3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.44862,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic Diagnostics <em>licenses</em>",
        "sections": "New Relic Diagnostics <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": ") use the Software to develop <em>services</em> or products for sale or include any components of the Software in any product; (d) remove any product identification, proprietary, copyright or other notices in the Software; or (e) publicly disseminate performance <em>information</em> or analysis (including benchmarks"
      },
      "id": "604505ad28ccbc457e2c60b4"
    }
  ],
  "/docs/licenses/license-information/special-services-licenses/new-relic-diagnostics-licenses": [
    {
      "sections": [
        "Data collector licenses"
      ],
      "title": "Data collector licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "8eeb19ae388c1f4fd6856084084467a037861675",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/data-collector-licenses/",
      "published_at": "2021-10-08T06:55:04Z",
      "updated_at": "2021-03-16T06:20:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Package Licenses annotations Apache 2.0 antlr BSD apache-log4j-extras Apache 2.0 apache-mime4j Apache 2.0 bonecp Apache 2.0 bonecp-provider Apache 2.0 bytelist MIT c3p0 Eclipse 1.0 commons-beanutils Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-digester Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-lang3 Apache 2.0 commons-logging Apache 2.0 commons-math Apache 2.0 commons-pool Apache 2.0 constantine MIT dom4j BSD ehcache Apache 2.0 gson Apache 2.0 guava Apache 2.0 hamcrest-core BSD-3-Clause hibernate-commons-annotations GNU LGPL 2.1 hibernate-core GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 hibernate-c3p0 GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 hystrix Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 jaffl GNU LGPL 3.0 java-driver Apache 2.0 javassist MPL 1.1, GNU GPL 2.1, Apache 2.0 javax.servlet Apache 2.0, Eclipse 1.0 javax.servlet-api CDDL + GPLv2 with classpath exception jboss-logging GNU LGPL 2.1 jboss-transaction-api_1.1_spec GNU LGPL 2.1 jcodings Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 jedis MIT jetty Apache 2.0, Eclipse 1.0 jffi GNU LGPL 3.0 jline BSD 2-Clause joni MIT jnr-netdb GNU LGPL 3.0 jnr-posix Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 joda-time Apache 2.0 jopt-simple MIT json-simple Apache 2.0 jsr305 Apache 2.0 junit Common Public License 1.0 kafka Apache 2.0 libthrift Apache 2.0 log4j Apache 2.0 logback-classic Eclipse 1.0 logback-core Eclipse 1.0 metrics-annotation Apache 2.0 mockito-all MIT msgpack Apache 2.0 mysql-connector-java GNU GPL 2 reflections WTFPL scala-library Apache 2.0 scannotation Apache 2.0 slf4j-api MIT slf4j-over-log4j MIT snakeyaml Apache 2.0 snappy Apache 2.0 syslog4j LGPL xml-apis Apache 2.0 yamlbeans MIT zkclient Apache 2.0 zookeeper Apache 2.0",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.58272,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Data collector <em>licenses</em>",
        "sections": "Data collector <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Package <em>Licenses</em> annotations"
      },
      "id": "603eb41d64441fd7f74e8893"
    },
    {
      "sections": [
        "New Relic Priority Support"
      ],
      "title": "New Relic Priority Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "f700b8b349627e66540b23ed020c7d9a46e19580",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-priority-support/",
      "published_at": "2021-10-08T06:56:02Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Priority Support document as a PDF (172 KB). The services described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.57979,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Priority Support document as a PDF (172 KB). The <em>services</em> described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative."
      },
      "id": "603ea90ae7b9d2e9d42a07d1"
    },
    {
      "sections": [
        "New Relic Premium Support"
      ],
      "title": "New Relic Premium Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "db095214148ca053cdabd9815dc74b04299dbe7b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-premium-support/",
      "published_at": "2021-10-08T06:56:02Z",
      "updated_at": "2021-03-16T04:14:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Premium Support document as a PDF (248 KB). The services described in this document only apply to initial orders entered after April 1, 2018. If you have questions about New Relic Premium Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.5789,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Premium Support document as a PDF (248 KB). The <em>services</em> described in this document only apply to initial orders entered after April 1, 2018. If you have questions about New Relic Premium Support, contact your New Relic account representative."
      },
      "id": "603ea8c964441fe48b4e88af"
    }
  ],
  "/docs/licenses/license-information/special-services-licenses/new-relic-premium-support": [
    {
      "sections": [
        "Data collector licenses"
      ],
      "title": "Data collector licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "8eeb19ae388c1f4fd6856084084467a037861675",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/data-collector-licenses/",
      "published_at": "2021-10-08T06:55:04Z",
      "updated_at": "2021-03-16T06:20:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Package Licenses annotations Apache 2.0 antlr BSD apache-log4j-extras Apache 2.0 apache-mime4j Apache 2.0 bonecp Apache 2.0 bonecp-provider Apache 2.0 bytelist MIT c3p0 Eclipse 1.0 commons-beanutils Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-digester Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-lang3 Apache 2.0 commons-logging Apache 2.0 commons-math Apache 2.0 commons-pool Apache 2.0 constantine MIT dom4j BSD ehcache Apache 2.0 gson Apache 2.0 guava Apache 2.0 hamcrest-core BSD-3-Clause hibernate-commons-annotations GNU LGPL 2.1 hibernate-core GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 hibernate-c3p0 GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 hystrix Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 jaffl GNU LGPL 3.0 java-driver Apache 2.0 javassist MPL 1.1, GNU GPL 2.1, Apache 2.0 javax.servlet Apache 2.0, Eclipse 1.0 javax.servlet-api CDDL + GPLv2 with classpath exception jboss-logging GNU LGPL 2.1 jboss-transaction-api_1.1_spec GNU LGPL 2.1 jcodings Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 jedis MIT jetty Apache 2.0, Eclipse 1.0 jffi GNU LGPL 3.0 jline BSD 2-Clause joni MIT jnr-netdb GNU LGPL 3.0 jnr-posix Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 joda-time Apache 2.0 jopt-simple MIT json-simple Apache 2.0 jsr305 Apache 2.0 junit Common Public License 1.0 kafka Apache 2.0 libthrift Apache 2.0 log4j Apache 2.0 logback-classic Eclipse 1.0 logback-core Eclipse 1.0 metrics-annotation Apache 2.0 mockito-all MIT msgpack Apache 2.0 mysql-connector-java GNU GPL 2 reflections WTFPL scala-library Apache 2.0 scannotation Apache 2.0 slf4j-api MIT slf4j-over-log4j MIT snakeyaml Apache 2.0 snappy Apache 2.0 syslog4j LGPL xml-apis Apache 2.0 yamlbeans MIT zkclient Apache 2.0 zookeeper Apache 2.0",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.58272,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Data collector <em>licenses</em>",
        "sections": "Data collector <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Package <em>Licenses</em> annotations"
      },
      "id": "603eb41d64441fd7f74e8893"
    },
    {
      "sections": [
        "New Relic Priority Support"
      ],
      "title": "New Relic Priority Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "f700b8b349627e66540b23ed020c7d9a46e19580",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-priority-support/",
      "published_at": "2021-10-08T06:56:02Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Priority Support document as a PDF (172 KB). The services described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.57979,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Priority Support document as a PDF (172 KB). The <em>services</em> described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative."
      },
      "id": "603ea90ae7b9d2e9d42a07d1"
    },
    {
      "sections": [
        "New Relic Diagnostics licenses",
        "Proprietary license",
        "New Relic Diagnostics license terms",
        "Open-source licenses"
      ],
      "title": "New Relic Diagnostics licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "6d86f8f63d372be98270fadc8634f9da8da2a893",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-diagnostics-licenses/",
      "published_at": "2021-10-08T06:55:04Z",
      "updated_at": "2021-03-13T03:25:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Proprietary license New Relic Diagnostics license terms These New Relic Diagnostics License Terms (“Terms”) set forth the terms and conditions under which you (“Customer”) may use New Relic Diagnostics (the “Software”), as made available by New Relic, Inc. (“New Relic”). By clicking “accept” or downloading or using the Software, you agree to be bound by these Terms. If you are agreeing to these Terms on behalf of your company, then “Customer” means your company and you are binding your company to these Terms. 1. License Grant. Subject to all of the terms and conditions of these Terms, New Relic grants Customer a limited, non-exclusive, non-transferable, non-sublicensable license to use the Software in accordance with its documentation in support of Customer’s use of New Relic products to which Customer has a separate subscription (“New Relic Products”). Unless otherwise specified, there is no fee for use of the Software. 2. License Restrictions. Customer will not (a) sell, rent, sublicense, transfer, time-share or otherwise provide access to any copies of the Software, or portions thereof, to a third party; (b) modify, decompile, disassemble or reverse engineer the Software; (c) use the Software to develop services or products for sale or include any components of the Software in any product; (d) remove any product identification, proprietary, copyright or other notices in the Software; or (e) publicly disseminate performance information or analysis (including benchmarks) relating to the Software. 3. Ownership. Except for the limited license rights expressly provided herein, New Relic and its suppliers have and will retain all right, title and interest in and to the Software, and all copies, updates, modifications and derivative works thereof. Customer acknowledges that it is obtaining only a limited license right to the Software and no ownership rights are being conveyed to Customer under these Terms or otherwise. 4. Usage Data. Customer agrees that New Relic and its affiliates have the right to collect Usage Data from Customer through the Software and use Usage Data to support, operate and improve New Relic products and services and for other lawful business purposes. “Usage Data” means diagnostics data related to the use of the Software with New Relic Products, including, without limitation, configuration information, details on diagnostics tasks, New Relic account and application ID numbers and New Relic license keys. For clarity, Usage Data is not considered “Customer Data”, “your Data” or any other similar term as used in any applicable subscription or license agreement for New Relic Products. At Customer’s election, Customer may disable the collection of Usage Data as described in the Software documentation. 5. Disclaimers. New Relic is not obligated to provide any support or maintenance for the Software. ALL USE OF SOFTWARE IS AT CUSTOMER’S OWN RISK. SOFTWARE IS PROVIDED “AS IS,” WITH ALL FAULTS AND WITHOUT WARRANTY OF ANY KIND, EXPRESS, IMPLIED OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, WARRANTIES OF TITLE OR NON-INFRINGEMENT. CUSTOMER MAY HAVE OTHER STATUTORY RIGHTS, HOWEVER, THE DURATION OF STATUTORILY REQUIRED WARRANTIES, IF ANY, WILL BE LIMITED TO THE FULLEST EXTENT PERMITTED BY LAW. 6. Limitation of Liability. TO THE FULLEST EXTENT PERMITTED BY LAW, IN NO EVENT WILL NEW RELIC BE LIABLE FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES OF ANY KIND, REGARDLESS OF THE FORM OF ACTION, WHETHER IN CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY OR OTHERWISE, EVEN IF INFORMED OF THE POSSIBILITY OF SUCH DAMAGES IN ADVANCE. TO THE EXTENT ANY OF THE ABOVE LIMITATIONS ARE NOT ENFORCEABLE AT APPLICABLE LAW, NEW RELIC’S ENTIRE LIABILITY TO CUSTOMER UNDER THESE TERMS WILL NOT EXCEED $50. THESE LIMITATIONS ON LIABILITY ARE A FUNDAMENTAL BASIS OF THE BARGAIN AND NEW RELIC WOULD NOT BE ABLE TO PROVIDE THE SOFTWARE WITHOUT SUCH LIMITATIONS. THESE LIMITATIONS ON LIABILITY WILL APPLY NOTWITHSTANDING ANY FAILURE OF ESSENTIAL PURPOSE OF ANY LIMITED REMEDY. 7. Changes and Termination. At its discretion, without notice or liability to Customer, New Relic may (a) update, modify or discontinue the Software; (b) modify these Terms, effective upon posting; or (c) terminate or suspend these Terms or Customer’s access to the Software. If Customer does not agree with any modification, its sole remedy is to terminate its use of the Software. Upon any termination or suspension, Customer must stop using the Software. 8. Third Party Code. The Software may contain or be provided with third party code (including code which may be made available to Customer in source code form). A list of third party code and ownership, use, warranty and modification rights with respect to such code may be identified in the documentation or provided by New Relic upon Customer’s written request. New Relic is not responsible for applications and services not licensed by New Relic. 9. Export Compliance. Customer acknowledges that the Software is subject to export restrictions by the U.S. government and import restrictions by certain foreign governments. Customer shall not remove or export from the U.S. or allow the export or re-export of any part of the Software or any direct product thereof: (a) into (or to a national or resident of) any embargoed or terrorist-supporting country; (b) to anyone on the U.S. Commerce Department’s Table of Denial Orders or U.S. Treasury Department’s list of Specially Designated Nationals; (c) to any country to which such export or re-export is restricted or prohibited, or as to which the U.S. government or any agency thereof requires an export license or other governmental approval at the time of export or re-export without first obtaining such license or approval; or (d) otherwise in violation of any export or import restrictions, laws or regulations of any U.S. or foreign agency or authority. Customer agrees to the foregoing and warrants that Customer is not located in, under the control of, or a national or resident of any such prohibited country or on any such prohibited party list. 10. Government End-Users. The Software is commercial computer software. If Customer is an entity of the U.S. government, the use, duplication, reproduction, release, modification, disclosure or transfer of the Software, or any related documentation of any kind, is restricted by a license agreement or by these Terms in accordance with Federal Acquisition Regulation 12.212 for civilian purposes and Defense Federal Acquisition Regulation Supplement 227.7202 for military purposes. The Software was developed fully at private expense. All other use is prohibited. 11. General. These Terms will be governed by and construed under the laws of the State of California and the U.S. without regard to conflicts of law provisions thereof, and without regard to the United Nations Convention on the International Sale of Goods. The jurisdiction and venue for actions arising out of or relating to these Terms shall be in the state and federal courts in San Francisco, California. The parties are independent contractors. Customer may not assign these Terms without New Relic’s prior written consent and any attempt to do so will be void; New Relic may assign these Terms freely to any party without Customer’s consent. If any provision of these Terms is held by a court of competent jurisdiction to be unenforceable or invalid for any reason, that provision shall be limited to the minimum extent necessary so that these Terms shall otherwise remain in effect. These Terms are the entire agreement between the parties relating to the Software, and supersede all prior or contemporaneous agreements (oral or written) relating to the Software. Any separate agreement Customer has for New Relic Products does not apply to the Software. Open-source licenses We love open-source software, and use the following in New Relic Diagnostics. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License clbanning/mxj MIT go-yaml/yaml Apache 2.0 StackExchange/wmi MIT go-ole/go-ole MIT shirou/gopsutil BSD-3-Clause shirou/w32 BSD-3-Clause cheggaaa/pb BSD-3-Clause google/uuid BSD-3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.44862,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic Diagnostics <em>licenses</em>",
        "sections": "New Relic Diagnostics <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": ") use the Software to develop <em>services</em> or products for sale or include any components of the Software in any product; (d) remove any product identification, proprietary, copyright or other notices in the Software; or (e) publicly disseminate performance <em>information</em> or analysis (including benchmarks"
      },
      "id": "604505ad28ccbc457e2c60b4"
    }
  ],
  "/docs/licenses/license-information/special-services-licenses/new-relic-priority-support": [
    {
      "sections": [
        "Data collector licenses"
      ],
      "title": "Data collector licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "8eeb19ae388c1f4fd6856084084467a037861675",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/data-collector-licenses/",
      "published_at": "2021-10-08T06:55:04Z",
      "updated_at": "2021-03-16T06:20:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Package Licenses annotations Apache 2.0 antlr BSD apache-log4j-extras Apache 2.0 apache-mime4j Apache 2.0 bonecp Apache 2.0 bonecp-provider Apache 2.0 bytelist MIT c3p0 Eclipse 1.0 commons-beanutils Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-digester Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-lang3 Apache 2.0 commons-logging Apache 2.0 commons-math Apache 2.0 commons-pool Apache 2.0 constantine MIT dom4j BSD ehcache Apache 2.0 gson Apache 2.0 guava Apache 2.0 hamcrest-core BSD-3-Clause hibernate-commons-annotations GNU LGPL 2.1 hibernate-core GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 hibernate-c3p0 GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 hystrix Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 jaffl GNU LGPL 3.0 java-driver Apache 2.0 javassist MPL 1.1, GNU GPL 2.1, Apache 2.0 javax.servlet Apache 2.0, Eclipse 1.0 javax.servlet-api CDDL + GPLv2 with classpath exception jboss-logging GNU LGPL 2.1 jboss-transaction-api_1.1_spec GNU LGPL 2.1 jcodings Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 jedis MIT jetty Apache 2.0, Eclipse 1.0 jffi GNU LGPL 3.0 jline BSD 2-Clause joni MIT jnr-netdb GNU LGPL 3.0 jnr-posix Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 joda-time Apache 2.0 jopt-simple MIT json-simple Apache 2.0 jsr305 Apache 2.0 junit Common Public License 1.0 kafka Apache 2.0 libthrift Apache 2.0 log4j Apache 2.0 logback-classic Eclipse 1.0 logback-core Eclipse 1.0 metrics-annotation Apache 2.0 mockito-all MIT msgpack Apache 2.0 mysql-connector-java GNU GPL 2 reflections WTFPL scala-library Apache 2.0 scannotation Apache 2.0 slf4j-api MIT slf4j-over-log4j MIT snakeyaml Apache 2.0 snappy Apache 2.0 syslog4j LGPL xml-apis Apache 2.0 yamlbeans MIT zkclient Apache 2.0 zookeeper Apache 2.0",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.58272,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Data collector <em>licenses</em>",
        "sections": "Data collector <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Package <em>Licenses</em> annotations"
      },
      "id": "603eb41d64441fd7f74e8893"
    },
    {
      "sections": [
        "New Relic Premium Support"
      ],
      "title": "New Relic Premium Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "db095214148ca053cdabd9815dc74b04299dbe7b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-premium-support/",
      "published_at": "2021-10-08T06:56:02Z",
      "updated_at": "2021-03-16T04:14:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Premium Support document as a PDF (248 KB). The services described in this document only apply to initial orders entered after April 1, 2018. If you have questions about New Relic Premium Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.5789,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Premium Support document as a PDF (248 KB). The <em>services</em> described in this document only apply to initial orders entered after April 1, 2018. If you have questions about New Relic Premium Support, contact your New Relic account representative."
      },
      "id": "603ea8c964441fe48b4e88af"
    },
    {
      "sections": [
        "New Relic Diagnostics licenses",
        "Proprietary license",
        "New Relic Diagnostics license terms",
        "Open-source licenses"
      ],
      "title": "New Relic Diagnostics licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "6d86f8f63d372be98270fadc8634f9da8da2a893",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-diagnostics-licenses/",
      "published_at": "2021-10-08T06:55:04Z",
      "updated_at": "2021-03-13T03:25:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Proprietary license New Relic Diagnostics license terms These New Relic Diagnostics License Terms (“Terms”) set forth the terms and conditions under which you (“Customer”) may use New Relic Diagnostics (the “Software”), as made available by New Relic, Inc. (“New Relic”). By clicking “accept” or downloading or using the Software, you agree to be bound by these Terms. If you are agreeing to these Terms on behalf of your company, then “Customer” means your company and you are binding your company to these Terms. 1. License Grant. Subject to all of the terms and conditions of these Terms, New Relic grants Customer a limited, non-exclusive, non-transferable, non-sublicensable license to use the Software in accordance with its documentation in support of Customer’s use of New Relic products to which Customer has a separate subscription (“New Relic Products”). Unless otherwise specified, there is no fee for use of the Software. 2. License Restrictions. Customer will not (a) sell, rent, sublicense, transfer, time-share or otherwise provide access to any copies of the Software, or portions thereof, to a third party; (b) modify, decompile, disassemble or reverse engineer the Software; (c) use the Software to develop services or products for sale or include any components of the Software in any product; (d) remove any product identification, proprietary, copyright or other notices in the Software; or (e) publicly disseminate performance information or analysis (including benchmarks) relating to the Software. 3. Ownership. Except for the limited license rights expressly provided herein, New Relic and its suppliers have and will retain all right, title and interest in and to the Software, and all copies, updates, modifications and derivative works thereof. Customer acknowledges that it is obtaining only a limited license right to the Software and no ownership rights are being conveyed to Customer under these Terms or otherwise. 4. Usage Data. Customer agrees that New Relic and its affiliates have the right to collect Usage Data from Customer through the Software and use Usage Data to support, operate and improve New Relic products and services and for other lawful business purposes. “Usage Data” means diagnostics data related to the use of the Software with New Relic Products, including, without limitation, configuration information, details on diagnostics tasks, New Relic account and application ID numbers and New Relic license keys. For clarity, Usage Data is not considered “Customer Data”, “your Data” or any other similar term as used in any applicable subscription or license agreement for New Relic Products. At Customer’s election, Customer may disable the collection of Usage Data as described in the Software documentation. 5. Disclaimers. New Relic is not obligated to provide any support or maintenance for the Software. ALL USE OF SOFTWARE IS AT CUSTOMER’S OWN RISK. SOFTWARE IS PROVIDED “AS IS,” WITH ALL FAULTS AND WITHOUT WARRANTY OF ANY KIND, EXPRESS, IMPLIED OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, WARRANTIES OF TITLE OR NON-INFRINGEMENT. CUSTOMER MAY HAVE OTHER STATUTORY RIGHTS, HOWEVER, THE DURATION OF STATUTORILY REQUIRED WARRANTIES, IF ANY, WILL BE LIMITED TO THE FULLEST EXTENT PERMITTED BY LAW. 6. Limitation of Liability. TO THE FULLEST EXTENT PERMITTED BY LAW, IN NO EVENT WILL NEW RELIC BE LIABLE FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES OF ANY KIND, REGARDLESS OF THE FORM OF ACTION, WHETHER IN CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY OR OTHERWISE, EVEN IF INFORMED OF THE POSSIBILITY OF SUCH DAMAGES IN ADVANCE. TO THE EXTENT ANY OF THE ABOVE LIMITATIONS ARE NOT ENFORCEABLE AT APPLICABLE LAW, NEW RELIC’S ENTIRE LIABILITY TO CUSTOMER UNDER THESE TERMS WILL NOT EXCEED $50. THESE LIMITATIONS ON LIABILITY ARE A FUNDAMENTAL BASIS OF THE BARGAIN AND NEW RELIC WOULD NOT BE ABLE TO PROVIDE THE SOFTWARE WITHOUT SUCH LIMITATIONS. THESE LIMITATIONS ON LIABILITY WILL APPLY NOTWITHSTANDING ANY FAILURE OF ESSENTIAL PURPOSE OF ANY LIMITED REMEDY. 7. Changes and Termination. At its discretion, without notice or liability to Customer, New Relic may (a) update, modify or discontinue the Software; (b) modify these Terms, effective upon posting; or (c) terminate or suspend these Terms or Customer’s access to the Software. If Customer does not agree with any modification, its sole remedy is to terminate its use of the Software. Upon any termination or suspension, Customer must stop using the Software. 8. Third Party Code. The Software may contain or be provided with third party code (including code which may be made available to Customer in source code form). A list of third party code and ownership, use, warranty and modification rights with respect to such code may be identified in the documentation or provided by New Relic upon Customer’s written request. New Relic is not responsible for applications and services not licensed by New Relic. 9. Export Compliance. Customer acknowledges that the Software is subject to export restrictions by the U.S. government and import restrictions by certain foreign governments. Customer shall not remove or export from the U.S. or allow the export or re-export of any part of the Software or any direct product thereof: (a) into (or to a national or resident of) any embargoed or terrorist-supporting country; (b) to anyone on the U.S. Commerce Department’s Table of Denial Orders or U.S. Treasury Department’s list of Specially Designated Nationals; (c) to any country to which such export or re-export is restricted or prohibited, or as to which the U.S. government or any agency thereof requires an export license or other governmental approval at the time of export or re-export without first obtaining such license or approval; or (d) otherwise in violation of any export or import restrictions, laws or regulations of any U.S. or foreign agency or authority. Customer agrees to the foregoing and warrants that Customer is not located in, under the control of, or a national or resident of any such prohibited country or on any such prohibited party list. 10. Government End-Users. The Software is commercial computer software. If Customer is an entity of the U.S. government, the use, duplication, reproduction, release, modification, disclosure or transfer of the Software, or any related documentation of any kind, is restricted by a license agreement or by these Terms in accordance with Federal Acquisition Regulation 12.212 for civilian purposes and Defense Federal Acquisition Regulation Supplement 227.7202 for military purposes. The Software was developed fully at private expense. All other use is prohibited. 11. General. These Terms will be governed by and construed under the laws of the State of California and the U.S. without regard to conflicts of law provisions thereof, and without regard to the United Nations Convention on the International Sale of Goods. The jurisdiction and venue for actions arising out of or relating to these Terms shall be in the state and federal courts in San Francisco, California. The parties are independent contractors. Customer may not assign these Terms without New Relic’s prior written consent and any attempt to do so will be void; New Relic may assign these Terms freely to any party without Customer’s consent. If any provision of these Terms is held by a court of competent jurisdiction to be unenforceable or invalid for any reason, that provision shall be limited to the minimum extent necessary so that these Terms shall otherwise remain in effect. These Terms are the entire agreement between the parties relating to the Software, and supersede all prior or contemporaneous agreements (oral or written) relating to the Software. Any separate agreement Customer has for New Relic Products does not apply to the Software. Open-source licenses We love open-source software, and use the following in New Relic Diagnostics. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License clbanning/mxj MIT go-yaml/yaml Apache 2.0 StackExchange/wmi MIT go-ole/go-ole MIT shirou/gopsutil BSD-3-Clause shirou/w32 BSD-3-Clause cheggaaa/pb BSD-3-Clause google/uuid BSD-3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.44862,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic Diagnostics <em>licenses</em>",
        "sections": "New Relic Diagnostics <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": ") use the Software to develop <em>services</em> or products for sale or include any components of the Software in any product; (d) remove any product identification, proprietary, copyright or other notices in the Software; or (e) publicly disseminate performance <em>information</em> or analysis (including benchmarks"
      },
      "id": "604505ad28ccbc457e2c60b4"
    }
  ],
  "/docs/licenses/license-information/usage-plans/new-relic-one-usage-plan-descriptions": [
    {
      "sections": [
        "Product-based pricing usage and New Relic Platform Pricing Usage Plan",
        "Important",
        "Usage Plan: New Relic Platform Pricing",
        "Eligible Services",
        "Eligible Services (Partners)",
        "Subscriptions with indeterminate pricing or usage quantities",
        "Product Usage Ratio"
      ],
      "title": "Product-based pricing usage and New Relic Platform Pricing Usage Plan",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Usage plans"
      ],
      "external_id": "e2686dc773c4e844544ce633a9a41a16f15edf5a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/usage-plans/product-based-pricing-usage-new-relic-platform-pricing-usage-plan/",
      "published_at": "2021-10-08T06:57:03Z",
      "updated_at": "2021-03-16T04:22:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This doc explains our original product-based pricing plan. For more about pricing changes, see Overview of pricing changes. The following provisions are applicable to existing customers (i) that have existing Terms, and (ii) that to the extent its subscription to the New Relic products reference the usage plans set forth below or where a subscription has indeterminate product pricing or usage quantities. New customers are eligible for New Relic One pricing as described here. Capitalized terms not defined below shall take on the meaning set forth in such New Relic order form. Usage Plan: New Relic Platform Pricing The following Eligible Services with New Relic are products that are referenced in specific order forms for New Relic Platform Pricing. For more information, contact your New Relic account representative. Eligible Services Eligible Services Per Unit Unit of Measure Monthly Standard Fee Rate New Relic APM Pro CU Annual* 10,000 Compute Units $166.70 New Relic APM Pro Host Annual* 1 Hosts $149 New Relic Insights Pro Annual 50,000,000 Events $165 New Relic Infrastructure Pro Annual 10,000 Compute Units $12 New Relic Browser Pro Annual 500,000 Page Views $149 New Relic Logs Annual - 8 Days* * 1 Per GB Daily $55 New Relic Logs Annual - 15 Days* * 1 Per GB Daily $65 New Relic Logs Annual - 30 Days* * 1 Per GB Daily $75 New Relic Metrics Annual 1,000 Data Points per Minute $25 New Relic Mobile Enterprise Annual 50,000 Total Users $499 New Relic Serverless for AWS Lambda Annual 1,000,000 AWS Lambda Events $15 New Relic Synthetics Pro Annual 10,000 Checks $69 New Relic Synthetics private locations (for New Relic Synthetics Pro Annual product) Fixed fee N/A $1,000 New Relic Traces Annual 1,000,000 Spans $1 New Relic AI Incident Intelligence Annual 1,000 Incident Events $500 New Relic AI Proactive Detection Annual 1,000,000,000 App Transactions $250 * Customer may utilize either New Relic APM Pro Annual or New Relic APM Pro CUs Annual, but not both. Customer will be provisioned New Relic APM Pro CUs Annual by default, unless otherwise stated in the relevant Order Form. * * Customer may utilize only one of New Relic Logs Annual - 8 Days, New Relic Logs Annual - 15 Days, or New Relic Logs Annual - 30 days. For the avoidance of doubt, Customer may not utilize New Relic Logs Annual with varying numbers of days retention concurrently. Customer will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. For more information about units of measures, see Product definitions. Eligible Services (Partners) Eligible services for New Relic Partners Measurement New Relic APM Pro CU Annual* Sold in Units of 8,900 Compute Units New Relic APM Pro Host Annual* Sold in Units of 1 Host New Relic Browser Pro Annual Sold in Units of 500,000 Page Views New Relic Infrastructure Pro Annual Sold in Units of 125,000 Compute Units New Relic Insights Pro Annual Sold in Units of 45,000,000 Events New Relic Logs Annual - 8 Days* * Sold in Units of 3 GB Daily New Relic Logs Annual - 15 Days* * Sold in Units of 2.5 GB Daily New Relic Logs Annual - 30 Days* * Sold in Units of 2 GB Daily New Relic Metrics Annual Sold in Units of 6,000 Data Points per Minute New Relic Mobile Enterprise Annual Sold in Units of 15,000 Total Users New Relic Serverless for AWS Lambda Annual Sold in Units of 10,000,000 AWS Lambda Events New Relic Synthetics Pro with Private Locations Annual Sold in Units of 22,000 Checks New Relic Traces Annual Sold in Units of 149,000,000 Spans * Customer and/or Partner may utilize either New Relic APM Pro Annual or New Relic APM Pro CUs Annual, but not both. Customer and/or Partner will be provisioned New Relic APM Pro CUs Annual by default, unless otherwise stated in the relevant Order Form. * * Customer and/or Partner may utilize only one of New Relic Logs Annual - 8 Days, New Relic Logs Annual - 15 Days, or New Relic Logs Annual - 30 days. For the avoidance of doubt, Customer may not utilize New Relic Logs Annual with varying numbers of days retention concurrently. Customer and/or Partner will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. Subscriptions with indeterminate pricing or usage quantities Product Usage Ratio Where a Customer subscription to the Products contain no specific pricing or quantities, Product-specific usage for a specific calendar month shall be determined through the calculation of the following ratio (the “Product Usage Ratio”) where: (1) the numerator shall be the Product specific usage for such calendar month multiplied by such Product’s list price, and (2) the denominator shall be the the aggregate Customer usage of all Products during such calendar month multiplied by all such Product(s) list price. The Product Usage Ratio shall be the percentage of usage for a specific Product for a calendar month period, or if usage cannot be measured for a period, the last Product Usage Ratio that can be calculated shall be assumed constant.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 185.20029,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Product-based pricing <em>usage</em> and New Relic Platform Pricing <em>Usage</em> <em>Plan</em>",
        "sections": "Product-based pricing <em>usage</em> and New Relic Platform Pricing <em>Usage</em> <em>Plan</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " reference the <em>usage</em> <em>plans</em> set forth below or where a subscription has indeterminate product pricing or <em>usage</em> quantities. New customers are eligible for New Relic One pricing as described here. Capitalized terms not defined below shall take on the meaning set forth in such New Relic order form. <em>Usage</em>"
      },
      "id": "603ea32a28ccbc7e22eba768"
    },
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Purpose",
        "2. Security Overview",
        "3.Security Certifications",
        "4. Data Control and Encryption",
        "5. Facilities",
        "6. Employee Access, Screening and Controls.",
        "7. Security Incident and Data Breach Response",
        "8. Network and Systems Security",
        "9. Authentication and Access Management",
        "10. Vulnerability Management",
        "11. System Access and Logging",
        "12. Disaster Recovery and Data Backup",
        "13. Copies and Removal",
        "14. Third Party Vendor Management",
        "15. Disclosure by Law",
        "16. Updates"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2021-10-08T06:55:04Z",
      "updated_at": "2021-09-27T15:05:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Updated 23 September 2021. The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Purpose This Policy describes New Relic's security program and the technical and organizational security controls to protect New Relic’s systems as shown in New Relic’s third-party audits and certifications. New Relic may update this Policy from time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/. 2. Security Overview 2.1. New Relic maintains a comprehensive Information Protection Program to manage information security within New Relic that includes administrative, technical, and physical safeguards designed to protect the confidentiality, integrity, and availability of Customer Data and that is appropriate to the nature, size, and complexity of New Relic's business operations. New Relic’s Information Protection Program includes: 2.1.1. executive review, support and accountability for all security related policies and practices; 2.1.2. formal written policies and procedures that are designed to protect against the loss, theft, or other unauthorized access or alteration of Customer Data, meet or exceed applicable industry standards, outlines a definition of information security and its overall objectives, include defined information security roles and responsibilities, a framework for setting control objectives and controls, a formal and effective risk mitigation program and a service provider security management program; 2.1.3. periodic risk assessments, including internal audits and/or independent 3rd party audits, to measure the effectiveness and appropriateness of controls for all systems processing Customer Data; 2.1.4. in-depth review of security incidents, including effective determination of root cause and corrective action; formal, industry-recognized controls frameworks based on an external audit standard; 2.1.5. employee screening and access management; and 2.1.6. comprehensive security testing, vulnerability identification, and mitigation methodology that consists of a variety of independent approaches that, when combined, are designed to maximize coverage for a diverse set of attack vectors. 2.2. New Relic's Chief Information Security Officer (CISO) leads New Relic's Information Security Program and develops, reviews, and approves, with appropriate stakeholders, New Relic's security policies and procedures. New Relic has appointed data protection officer(s) as described in New Relic’s Privacy Notices who are consulted as necessary under applicable laws. 2.3. Information security objectives, approach, scope, importance, goals, and principles for the organization’s security program are formally identified, communicated throughout the organization to users in a form that is relevant, accessible, and understandable to the intended reader; and supported by a controls framework that considers legislative, regulatory, contractual requirements, and other policy-related requirements. 2.4. New Relic will maintain written policies and procedures to review, test, and approve (as appropriate) changes affecting New Relic infrastructure and systems that process Customer Data including, but not limited to, peer reviews prior to introducing new code into production. New Relic will establish acceptance criteria for new information systems, upgrades, and new versions and will carry out suitable tests of the system(s) during development and prior to acceptance. Any changes or updates will not materially decrease the security of the systems. 3.Security Certifications 3.1. New Relic regularly tests, assesses, and evaluates its security measures for protecting Customer Data using industry-recognized standards and uses independent third-party auditors to verify such controls. 3.2. New Relic agrees to provide Customer, upon request, with applicable certifications or reports about New Relic systems. All information exchanged in connection with the audit activities described in this section is deemed to be the Confidential Information of New Relic. 3.3. Additional information about New Relic’s security certifications are available on New Relic’s Security Guide. 4. Data Control and Encryption 4.1. The Service and related features are designed to provide Customer with control over Customer’s data sources and Customer’s environments that are monitored and sending data to New Relic. 4.2. New Relic will have established methods to: (i) enable Customer to use encryption on Customer Data in transit, and (ii) securely hash passwords in storage following industry standard practices (e.g. scrypt) as described in the Documentation. Server certificate-based authentication is used as part of the TLS encryption with a trusted certificate authority. 4.3. New Relic receives and processes data in accordance with the Agreement and the Services as described in the Documentation. New Relic permits customers to delete personal data in accordance with applicable privacy laws as further described in the Documentation. In the event of error in personal data sent in Customer Data Customers may request personal data deletion and re-send data that is accurate. 4.4. Additional information for Customer data control and encryption, including encryption of data at rest, is available on New Relic’s Security Guide. 5. Facilities 5.1. All physical locations that process Customer Data are co-locations and third party data centers. All physical locations have security measures in place that are designed to prevent unauthorized physical access to data processing facilities and unlawful access, modification, or destruction of Customer Data. 5.2. New Relic will seek assurances (e.g., in the form of an independent 3rd party audit report such as the SOC 2 Type 2, ISO 27001, and vendor security evaluations) from its data processing facilities vendors that store or process Customer Data: 5.2.1. secure its data process facilities in an access-controlled location and have protections in place to prevent unauthorized access, damage, and interference; 5.2.2 employ physical security appropriate to the classification of the assets and information being managed which may include card key access, security cameras, and solid wall construction for all exterior walls; 5.2.3 limit and screen all entrants, which may include measures such as on-site security guard, badge reader, electronic lock, or a monitored closed caption television (CCTV); and 5.2.4 maintain relevant access logs. 5.3. Additional information about New Relic’s third party data centers are available on New Relic’s Security Guide. 6. Employee Access, Screening and Controls. New Relic will have and maintain policies and practices that include, at a minimum, the following controls and safeguards applied to New Relic employees and contractors who may access Customer Data: 6.1. For U.S. New Relic employees and subject to applicable law, a criminal background screening of each of its new employees to whom it gives access to Customer Data at the federal, state, and county levels. For non-U.S. New Relic employees, New Relic will use commercially reasonable efforts to meet the same criteria as established for U.S.-based New Relic employees, subject to general business practices in the respective country and in compliance with applicable local law requirements; 6.2. All New Relic employees with access to Customer Data will undergo adequate training, such as annual security awareness training, in the care, protection and handling of Customer Data, and will align with the privacy and security measures set out in this Addendum by following the guidance provided in their welcome package, which includes New Relic’s security policies and a security acknowledgement; 6.3. A disciplinary policy and process, to be used when New Relic employees violate New Relic security or privacy policies or access Customer Data without prior authorization; 6.4. Administrative or remote access to New Relic systems that process Customer Data will align with industry standard practices, including multi-factor authentication; 6.5. Restricted access to New Relic proprietary source code to prevent unauthorized access; 6.6. Controls designed to ensure that only those New Relic employees with an actual need-to-know will have access to New Relic systems including, but not limited to, the use of a formal access management process for the request, review, approval, and provisioning; 6.7. Controls designed to ensure that New Relic employees are granted access to New Relic systems based on least-privilege principles; and 6.8. Revoke access to New Relic employees no longer requiring access. 7. Security Incident and Data Breach Response 7.1. New Relic will take appropriate physical, technical, and administrative security measures that are commercially reasonable and consistent with industry standards to prevent a Data Breach, and as required by any applicable law or regulation. “Data Breach” means the theft, loss, or unauthorized access of Customer Data. Without limiting the foregoing, New Relic will implement security measures at least as stringent as those set out in this Addendum. New Relic will designate a senior representative to provide incident briefings, as needed in case of a Data Breach, and to respond to reasonable requests by Customer pertaining to privacy and data security issues within a commercially reasonable time frame. 7.2. New Relic will: 7.2.1. Notify Customer without undue delay if New Relic becomes aware of a Data Breach; 7.2.2. Maintain a security incident response plan, including procedures and means to respond in a manner consistent with industry standards; 7.2.3. Reasonably cooperate with Customer to investigate and remediate a Data Breach and mitigate any further risk to the Customer Data, or risk to data subjects and/or Customer’s reputation or brand; 7.2.4. Provide reasonable assistance to Customer at New Relic’s sole cost and expense; and 7.2.5. Make commercially reasonable efforts to preserve evidence and reasonably cooperate with Customer and legal authorities (as applicable and legally permissible) during an investigation of a Data Breach. 8. Network and Systems Security 8.1. All extranet connectivity by New Relic personnel to systems processing Customer Data will be through secure remote connections. 8.2. Network segments connected to the Internet will be protected by secure access control mechanisms, such as a firewall configured to secure all devices behind it and properly addresses security concerns according to industry standard practices. 8.3. New Relic will have industry standard measures in place to actively monitor its systems and help detect a potential hostile attack, such as Network Intrusion Detection (NID) or Host Intrusion Detection (HID)/Prevention. 8.4. Applications, ports, services, and similar access points installed on a computer or network facility, which are not specifically required for business functionality, will be disabled or removed. 8.5. New Relic maintains configuration standards for authorized operating systems and software for systems that support processing of Customer Data. New Relic will establish and follow server configuration guidelines and processes for preventing unauthorized access to Customer Data. New Relic maintains secure images or templates for systems based on the organization’s approved configuration standards. 8.6. Development, test, and operational environments will be logically separated to reduce the risks of unauthorized access or changes to the operational system. 8.7. New Relic network architecture will be designed to limit site access and restrict the availability of information systems that are considered to be vulnerable to attack. 8.8. New Relic provides appropriate TLS certificates when users access and view Customer Data in the Service. New Relic’s software for sending Customer Data to New Relic will encrypt Customer Data in transit by default. 9. Authentication and Access Management If you subscribe to the requisite New Relic One Service, New Relic will provide industry standard authentication and access controls to protect Customer Data, including industry standard authentication methods utilized to help prevent unauthorized access to the Service. New Relic’s access control methods will clearly state the rules and rights for each user or group of users including applications and information sharing and will include a process for granting and removing access to all information systems processing Customer Data. A record of all privileges allocated will be maintained pursuant to the requirements herein. 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. 10. Vulnerability Management New Relic will have and maintain the following vulnerability management processes for all devices used to connect to the New Relic network and Services. 10.1. New Relic will align to industry standard practices for build out, minimization of services and secure configuration, in accordance with, industry-recognized minimum security baselines for the New Relic platform and subcontractor systems connected to the New Relic platform in relation to the provision of the Services. 10.2. New Relic will employ industry-recognized standards and tools to conduct frequent infrastructure vulnerability scanning to test New Relic’s network and infrastructure and application penetration testing to test the New Relic Services. New Relic applies “Medium”, “High” and “Critical” security patches for all components in production and development environments as soon as commercially possible in accordance with its vulnerability management protocol, and consistent with industry standard practices and standards; 10.3. New Relic will have processes in place designed to ensure adherence to industry standard security development practices for development and testing for all code, APIs, and applications deployed and implemented in support of the Service; 10.4. New Relic will have and maintain solutions to identify and prevent malicious attackers or code from accessing or compromising Customer Data or systems that process Customer Data. These include, but are not limited to, software that identifies and removes malware and detects attempted intrusions. New Relic will have a security event and incident monitoring system and supporting processes to notify appropriate personnel in response to threats. 11. System Access and Logging 11.1. Access to New Relic’s systems will not be granted to employees of New Relic unless they have been uniquely identified and have sufficient credentials. 11.2. Access to New Relic’s systems will be logged and retained for no less than 6 months to assist in investigations and access control monitoring, including, but not limited to, end user access and activities, and information security events. 11.3. New Relic agrees to provide Customer the capability to access log records relating to Customer Accounts and the New Relic systems that process Customer Data in the event of a Data Breach or if required in connection with a law enforcement request. 12. Disaster Recovery and Data Backup 12.1. New Relic will have plans designed to respond to loss of services, which are tested and reviewed at least annually. This plan will include documented policies and procedures to restore service in the event of a service failure. 12.2. New Relic will establish and follow backup and restore procedures for Customer Data. 12.3. New Relic business continuity plan identifies critical systems. Annual disaster recovery tests are performed to check and restore customer data in the event of an incident. 12.4. New Relic will provide Customer with redacted copies of its plan(s) and evidence of tests/reviews upon request, but not more frequently than once annually, and subject to confidentiality requirements. 12.5. Additional information for Customer Data control and encryption are available on New Relic’s Security Guide. 13. Copies and Removal 13.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of the Agreement for any reason: (a) New Relic will, and will cause its personnel, to cease all access and use of any Customer Data, and (b) New Relic will delete all copies of Customer Data within ninety (90) days. 13.2. New Relic will maintain a process of ensuring secure destruction and deletion of all Customer Data, when reasonably requested by Customer or as otherwise provided in the Agreement. The process will include industry standard processes so that: (i) Customer Data cannot be practicably read or reconstructed, and (ii) the systems that store Customer Data are securely erased and/or decommissioned disks are destroyed. 14. Third Party Vendor Management New Relic may use third party vendors to provide the Services. New Relic performs a security risk-based assessment of prospective vendors before working with them to validate they meet New Relic’s security and business continuity standards, including the type of access and classification of data being accessed (if any), controls necessary to protect data, and legal/regulatory requirements. New Relic enters into written agreements with its vendors that process Customer Data which include confidentiality, privacy, and security obligations that provide an appropriate level of protection for Customer Data that these vendors may process for New Relic to maintain the security posture in this Policy, including following industry security standards. 15. Disclosure by Law In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic will (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. New Relic publishes its law enforcement requests report on New Relic’s Security Guide. 16. Updates As New Relic releases new products, services, functionality, and features, New Relic may update this Policy to account for such products, services, functionality, and features. For additional information, see our Security Guide.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 149.96301,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;referenced-policies&#x2F;security-policy&#x2F;. 2. Security Overview 2.1. New Relic"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    },
    {
      "sections": [
        "Security guide",
        "Tip",
        "Security Program",
        "Security Domains",
        "Security Certifications",
        "Data Control, Facilities, and Encryption",
        "Law Enforcement Request Report"
      ],
      "title": "Security guide",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "356f0d11ffcb62208a743a0a7c127f5f6da9c940",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-guide/",
      "published_at": "2021-10-07T02:08:45Z",
      "updated_at": "2021-09-19T15:21:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Last updated September 17, 2021. This is supplement to our security policy and serves as a guide to New Relic’s description of its Services, functionalities, and features. Tip We may update the URLs in this document without notice. Security Program New Relic follows \"privacy by design\" principles as described here: https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Security Domains New Relic’s policies and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless Security; Configuration Management; Vulnerability Management; Network Protection; Transmission Protection; Password Management; Access Control, Audit Logging & Monitoring; Education, Training, and Awareness; Third Party Assurance; Incident Management; Business Continuity and Disaster Recover; Risk Management; Data Protection & Privacy; and Service Management Systems. Security Certifications New Relic audits its Services against industry standards as described at https://docs.newrelic.com/docs/security/security-privacy/compliance/regulatory-audits-new-relic-services/. Data Control, Facilities, and Encryption New Relic's customers can send data to New Relic's APIs by (1) using New Relic's software, (2) using vendor-neutral software that is managed and maintained by a third-party such as via OpenTelemetry instrumentation provided by opentelemetry.io, or (3) from third-party systems that customer's manage and/or control. New Relic's customers can use New Relic's Services such as NerdGraph to filter out and drop data. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/drop-data-using-nerdgraph/. New Relic's customers can adjust their data retention periods as appropriate for their needs. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/manage-data-retention/#adjust-retention. New Relic Logs obfuscates numbers that match known patterns, such as bank card and social security numbers as described here: https://docs.newrelic.com/docs/logs/log-management/get-started/new-relics-log-management-security-privacy/. New Relic honors requests to delete personal data in accordance with applicable privacy laws. Please see https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Customers may use New Relic's APIs to query data, such as NerdGraph described here, and New Relic Services to export the data to other cloud providers. Customers can configure its log forwarder [https://docs.newrelic.com/docs/logs/enable-log-management-new-relic/enable-log-monitoring-new-relic/forward-your-logs-using-infrastructure-agent/] before sending infrastructure logs to New Relic. For New Relic Customers in New Relic US, FedRAMP and HIPAA-enabled environments, Customer Data is replicated to the off-site backup system via Amazon Simple Storage Service (S3). Category of Customer Description FedRAMP HIPAA-enabled US Gen Pop EU Gen Pop Data is stored in Amazon Web Services (“AWS”). Limited Data is stored in IBM Data for New Relic Incident Intelligence is stored in Google Cloud New Relic regularly tests, assess, and evaluates its measures to ensure the security of processing using industry-recognized standards and uses independent third-party auditors as provided below: Annual SOC 2 Type 2 Annual FedRAMP assessment by an independent third-party pursuant to NIST 800-53 rev 4 Moderate authorization. Annual HITRUST-validated assessment by an independent third-party *Pursuing CY2021 Q4 ISO 27001 *Pursuing CY2021 TISAX *Pursuing CY2021 The Services that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed information about AWS security is available at https://aws.amazon.com/security/ and http://aws.amazon.com/security/sharing-the-security-responsibility/. Data encryption at rest utilizes FIPS 140-2 compliant encryption methodology. For AWS SOC Reports, please see https://aws.amazon.com/compliance/soc-faqs/. The Services that operate on Google Cloud Platform (\"GCP\") are protected by the security and environmental controls of GCP. Detailed information about GCP security is available at https://cloud.google.com/docs/tutorials#security. For GCP reports, please see https://cloud.google.com/security/compliance/. IBM Deft Zayo QTS Law Enforcement Request Report New Relic has not to date received any request for customer data from a law enforcement or other government agency (including under any national security process), and has not made any corresponding disclosures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 135.4873,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " *Pursuing CY2021 The Services that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed <em>information</em> about AWS security is available at https:&#x2F;&#x2F;aws.amazon.com&#x2F;security&#x2F; and http:&#x2F;&#x2F;aws.amazon.com&#x2F;security&#x2F;sharing-the-security-responsibility"
      },
      "id": "6147558128ccbc973a56a863"
    }
  ],
  "/docs/licenses/license-information/usage-plans/product-based-pricing-usage-new-relic-platform-pricing-usage-plan": [
    {
      "sections": [
        "New Relic One usage plan descriptions",
        "Pay As You Go",
        "Annual Pool of Funds",
        "Applicable Invoicing and Order Terms",
        "User Accounts",
        "New Relic One Pro and Enterprise Service Level Availability Commitment",
        "New Relic One Pro and Enterprise Support Plans",
        "Free tier, ‘lite’, no-charge, preview access, New Relic One - Data, New Relic One - Standard User subscriptions",
        "Separate Platforms"
      ],
      "title": "New Relic One usage plan descriptions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Usage plans"
      ],
      "external_id": "c18e1c6c294914c28bba48f9de025333210ed254",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/usage-plans/new-relic-one-usage-plan-descriptions/",
      "published_at": "2021-10-07T05:17:22Z",
      "updated_at": "2021-08-08T23:13:31Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document is about the New Relic One pricing plan. For an explanation of how that plan works, see New Relic One pricing. The document below goes into license-level details. The Usage Plan applies to (i) your Pay As You Go subscription, or (ii) your Commitment Term for the Annual Pool of Funds subscription (see a description of these two plans). New Relic may modify the Usage Plan from time to time. Any changes to the Usage Plan will become effective immediately for changes that provide a benefit or right to the Customer, all other changes will become effective if Customer assents or upon any new or renewal Commitment Term. Usage Plan - Effective as February 19, 2021: The Order and Usage Plan may contain defined terms that are denoted by capitalization. In the event that a capitalized term is not defined in either the Order or the Usage Plan, such terms shall have the meaning set forth in the New Relic One pricing definitions page. Pay As You Go By electing and subscribing to the Pay As You Go subscription model (“Pay As You Go” or “PAYG”), Customer commits to paying for the New Relic Products on a month-to-month consumption basis. Monthly Product Usage will be invoiced regardless if a PO is required or not. Any Customer dispute to Monthly Product Usage from the prior month must be in good-faith and received by New Relic in writing within three (3) business days of the start of the next month or such dispute notice will be considered invalid. The dispute notice will set forth in reasonable detail the information concerning the disputed charges. The parties will use good-faith efforts to promptly resolve any disputed charges. Customer’s usage of the Products in excess of the Free Tier each month shall be billed in arrears on the first business day of the following month based on the Customer’s Per Unit usage of each Product each month multiplied by the corresponding rates set forth in an Order and summed (“Monthly Product Usage”). Annual Pool of Funds By electing and subscribing to the Annual Pool of Funds subscription model (“Annual Pool of Funds” or “APoF”) for the Commitment Term, Customer commits in an Order to: (i) paying the Commitment Fee amounts described in the Subscription table and any additional commitment fees set forth; and (ii) the Monthly Discounted Fee Rates applying to Customer’s Monthly Product Usage. New Relic will invoice the Commitment Fee as per the ‘Billing Terms’ described in an Order. On a monthly cadence during the Commitment Term, Customer’s Per Unit usage of the Products will be multiplied by the corresponding Monthly Discounted Rate and summed (“Monthly Product Usage”). Monthly Product Usage will be deducted from the Commitment Fee amounts that are paid in advance. If Monthly Product Usage exceeds any such remaining unconsumed amounts, Customer will be invoiced for the difference (“Additional Usage”), including for any Monthly Product Usage during the last month of the Commitment Term. Payment of such invoices will be governed as set forth in the Terms. Any Customer dispute to Monthly Product Usage from the prior month must be in good-faith and received by New Relic in writing within three (3) business days of the start of the next month (including for any Monthly Product Usage for the last month of the Commitment Term) or such dispute notice will be considered invalid. The dispute notice will set forth in reasonable detail the information concerning the disputed charges. The parties will use good-faith efforts to promptly resolve any disputed charges. Any unconsumed balances from the Customer payment of each annual Commitment Fee and any additional payments, if applicable, as set out in an Order will expire and lapse at the end of each year of the Commitment Term. Applicable Invoicing and Order Terms All amounts stated in an Order are non-cancelable payment obligations of the Customer for the Commitment Term regardless of usage. Any fees paid are non-refundable and do not represent a deposit for, or a credit towards, the purchase of other products not specified in an Order or for any purchase after the Commitment Term. Customer acknowledges that a final payment for the full outstanding amount of any remaining unpaid Commitment Fee and/or Monthly Product Usage Fee may be invoiced upon each anniversary date of the term start date. Tax will be added where applicable. New Relic may review Customer's use of the Products at any time. If New Relic identifies any Customer usage of the Product(s) that is not in accordance with the Terms or Documentation, New Relic may suspend such unauthorized usage. All existing purchases and related pricing in effect prior to the execution of an Order shall remain in force. Unless otherwise stated in an Order, an Order does not modify or amend any existing purchases. Additional future products or quantities are not subject to promotional pricing unless otherwise stated in an Order. In the event that a Customer indicates in an Order that it requires a purchase order (“PO”) for its subscription, Customer agrees to provide the required PO prior to the provisioning of the Products. If a Customer does not indicate a PO is required, Customer agrees that New Relic may issue invoice(s) and is entitled to such payment without a PO reference. All Additional Usage fees will be invoiced regardless of a PO requirement. The Product(s) are deemed accepted upon its provisioning. User Accounts Use of the Products require Customer users to create Login Credentials. Customer user Login Credential information must be accurate, current, and complete. New Relic’s use and collection of Login Credentials (in accordance with its General Data Privacy Notice) is for account and product management and support of its customers. Customer and Customer users must abide by the New Relic Acceptable Use Policy (AUP) and Login Credentials may not be shared. Each Customer user must have their own user account. Entry into an Order indicates your agreement that the amount of provisioned users (at the rate specified in the Order) applies in lieu of and supersedes any other amount of users of the Products that may be specified in the agreement between Customer and New Relic. New Relic One Pro and Enterprise Service Level Availability Commitment With a subscription to New Relic Full Stack Observability Pro or Enterprise Products, you agree that during the Commitment Term the applicable service level availability commitment set forth on the ‘Service level availability commitment’ page in the Documentation shall apply to the Products. For clarity, if your agreement with New Relic contains a different service level availability commitment or remedies, the above does not apply to your subscription to the New Relic Full Stack Observability Pro or Enterprise Products. If you subscribe to any other New Relic Products with New Relic One pricing, any service level availability commitment or related remedies contained within your agreement with New Relic are vacated and nullified, and New Relic will use commercially reasonable efforts to make New Relic Full Stack Observability Standard and Telemetry Data Platform available in line with industry standards. New Relic One Pro and Enterprise Support Plans With a subscription to New Relic One Users (Full Stack Observability), New Relic provides an updated support plan commitment. By subscribing to New Relic One Users (Full Stack Observability), you agree that during the Commitment Term the applicable Support Plan set forth on the ‘Support plan’ page in the Documentation shall apply in lieu of, and supersedes and replaces, any other support related commitments that may be contained within your agreement with New Relic. For New Relic K.K. customers (Japan), the above does not currently apply to the support offerings provided by New Relic to you. Free tier, ‘lite’, no-charge, preview access, New Relic One - Data, New Relic One - Standard User subscriptions If you are using New Relic’s Products in the free tier only, or on a no-charge, or a ‘lite’ or ‘preview access’ basis you agree that the Unpaid Terms of Service will apply to such Product usage and replace and supersede any other terms. In addition, if your subscription contains the New Relic One - Standard User (Full Stack Observability Standard) Product, you agree that the Paid Terms of Service will apply to your subscription to the Products set forth in an Order and replace and supersede any other terms. Separate Platforms Customer access to any separate platforms for purposes of applicability of governing terms, such as Community Cloud for Pixie, is subject to their own terms of service. For clarity, such other platforms do not form part of the New Relic One offering and New Relic One warranties, indemnities, etc., do not apply to use of any other platforms for purposes of applicability of governing terms.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.76224,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic One <em>usage</em> <em>plan</em> descriptions",
        "sections": "New Relic One <em>usage</em> <em>plan</em> descriptions",
        "tags": "<em>License</em> <em>information</em>",
        "body": "This document is about the New Relic One pricing <em>plan</em>. For an explanation of how that <em>plan</em> works, see New Relic One pricing. The document below goes into <em>license</em>-level details. The <em>Usage</em> <em>Plan</em> applies to (i) your Pay As You Go subscription, or (ii) your Commitment Term for the Annual Pool of Funds"
      },
      "id": "6044e74ee7b9d2a4515799c8"
    },
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Purpose",
        "2. Security Overview",
        "3.Security Certifications",
        "4. Data Control and Encryption",
        "5. Facilities",
        "6. Employee Access, Screening and Controls.",
        "7. Security Incident and Data Breach Response",
        "8. Network and Systems Security",
        "9. Authentication and Access Management",
        "10. Vulnerability Management",
        "11. System Access and Logging",
        "12. Disaster Recovery and Data Backup",
        "13. Copies and Removal",
        "14. Third Party Vendor Management",
        "15. Disclosure by Law",
        "16. Updates"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2021-10-08T06:55:04Z",
      "updated_at": "2021-09-27T15:05:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Updated 23 September 2021. The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Purpose This Policy describes New Relic's security program and the technical and organizational security controls to protect New Relic’s systems as shown in New Relic’s third-party audits and certifications. New Relic may update this Policy from time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/. 2. Security Overview 2.1. New Relic maintains a comprehensive Information Protection Program to manage information security within New Relic that includes administrative, technical, and physical safeguards designed to protect the confidentiality, integrity, and availability of Customer Data and that is appropriate to the nature, size, and complexity of New Relic's business operations. New Relic’s Information Protection Program includes: 2.1.1. executive review, support and accountability for all security related policies and practices; 2.1.2. formal written policies and procedures that are designed to protect against the loss, theft, or other unauthorized access or alteration of Customer Data, meet or exceed applicable industry standards, outlines a definition of information security and its overall objectives, include defined information security roles and responsibilities, a framework for setting control objectives and controls, a formal and effective risk mitigation program and a service provider security management program; 2.1.3. periodic risk assessments, including internal audits and/or independent 3rd party audits, to measure the effectiveness and appropriateness of controls for all systems processing Customer Data; 2.1.4. in-depth review of security incidents, including effective determination of root cause and corrective action; formal, industry-recognized controls frameworks based on an external audit standard; 2.1.5. employee screening and access management; and 2.1.6. comprehensive security testing, vulnerability identification, and mitigation methodology that consists of a variety of independent approaches that, when combined, are designed to maximize coverage for a diverse set of attack vectors. 2.2. New Relic's Chief Information Security Officer (CISO) leads New Relic's Information Security Program and develops, reviews, and approves, with appropriate stakeholders, New Relic's security policies and procedures. New Relic has appointed data protection officer(s) as described in New Relic’s Privacy Notices who are consulted as necessary under applicable laws. 2.3. Information security objectives, approach, scope, importance, goals, and principles for the organization’s security program are formally identified, communicated throughout the organization to users in a form that is relevant, accessible, and understandable to the intended reader; and supported by a controls framework that considers legislative, regulatory, contractual requirements, and other policy-related requirements. 2.4. New Relic will maintain written policies and procedures to review, test, and approve (as appropriate) changes affecting New Relic infrastructure and systems that process Customer Data including, but not limited to, peer reviews prior to introducing new code into production. New Relic will establish acceptance criteria for new information systems, upgrades, and new versions and will carry out suitable tests of the system(s) during development and prior to acceptance. Any changes or updates will not materially decrease the security of the systems. 3.Security Certifications 3.1. New Relic regularly tests, assesses, and evaluates its security measures for protecting Customer Data using industry-recognized standards and uses independent third-party auditors to verify such controls. 3.2. New Relic agrees to provide Customer, upon request, with applicable certifications or reports about New Relic systems. All information exchanged in connection with the audit activities described in this section is deemed to be the Confidential Information of New Relic. 3.3. Additional information about New Relic’s security certifications are available on New Relic’s Security Guide. 4. Data Control and Encryption 4.1. The Service and related features are designed to provide Customer with control over Customer’s data sources and Customer’s environments that are monitored and sending data to New Relic. 4.2. New Relic will have established methods to: (i) enable Customer to use encryption on Customer Data in transit, and (ii) securely hash passwords in storage following industry standard practices (e.g. scrypt) as described in the Documentation. Server certificate-based authentication is used as part of the TLS encryption with a trusted certificate authority. 4.3. New Relic receives and processes data in accordance with the Agreement and the Services as described in the Documentation. New Relic permits customers to delete personal data in accordance with applicable privacy laws as further described in the Documentation. In the event of error in personal data sent in Customer Data Customers may request personal data deletion and re-send data that is accurate. 4.4. Additional information for Customer data control and encryption, including encryption of data at rest, is available on New Relic’s Security Guide. 5. Facilities 5.1. All physical locations that process Customer Data are co-locations and third party data centers. All physical locations have security measures in place that are designed to prevent unauthorized physical access to data processing facilities and unlawful access, modification, or destruction of Customer Data. 5.2. New Relic will seek assurances (e.g., in the form of an independent 3rd party audit report such as the SOC 2 Type 2, ISO 27001, and vendor security evaluations) from its data processing facilities vendors that store or process Customer Data: 5.2.1. secure its data process facilities in an access-controlled location and have protections in place to prevent unauthorized access, damage, and interference; 5.2.2 employ physical security appropriate to the classification of the assets and information being managed which may include card key access, security cameras, and solid wall construction for all exterior walls; 5.2.3 limit and screen all entrants, which may include measures such as on-site security guard, badge reader, electronic lock, or a monitored closed caption television (CCTV); and 5.2.4 maintain relevant access logs. 5.3. Additional information about New Relic’s third party data centers are available on New Relic’s Security Guide. 6. Employee Access, Screening and Controls. New Relic will have and maintain policies and practices that include, at a minimum, the following controls and safeguards applied to New Relic employees and contractors who may access Customer Data: 6.1. For U.S. New Relic employees and subject to applicable law, a criminal background screening of each of its new employees to whom it gives access to Customer Data at the federal, state, and county levels. For non-U.S. New Relic employees, New Relic will use commercially reasonable efforts to meet the same criteria as established for U.S.-based New Relic employees, subject to general business practices in the respective country and in compliance with applicable local law requirements; 6.2. All New Relic employees with access to Customer Data will undergo adequate training, such as annual security awareness training, in the care, protection and handling of Customer Data, and will align with the privacy and security measures set out in this Addendum by following the guidance provided in their welcome package, which includes New Relic’s security policies and a security acknowledgement; 6.3. A disciplinary policy and process, to be used when New Relic employees violate New Relic security or privacy policies or access Customer Data without prior authorization; 6.4. Administrative or remote access to New Relic systems that process Customer Data will align with industry standard practices, including multi-factor authentication; 6.5. Restricted access to New Relic proprietary source code to prevent unauthorized access; 6.6. Controls designed to ensure that only those New Relic employees with an actual need-to-know will have access to New Relic systems including, but not limited to, the use of a formal access management process for the request, review, approval, and provisioning; 6.7. Controls designed to ensure that New Relic employees are granted access to New Relic systems based on least-privilege principles; and 6.8. Revoke access to New Relic employees no longer requiring access. 7. Security Incident and Data Breach Response 7.1. New Relic will take appropriate physical, technical, and administrative security measures that are commercially reasonable and consistent with industry standards to prevent a Data Breach, and as required by any applicable law or regulation. “Data Breach” means the theft, loss, or unauthorized access of Customer Data. Without limiting the foregoing, New Relic will implement security measures at least as stringent as those set out in this Addendum. New Relic will designate a senior representative to provide incident briefings, as needed in case of a Data Breach, and to respond to reasonable requests by Customer pertaining to privacy and data security issues within a commercially reasonable time frame. 7.2. New Relic will: 7.2.1. Notify Customer without undue delay if New Relic becomes aware of a Data Breach; 7.2.2. Maintain a security incident response plan, including procedures and means to respond in a manner consistent with industry standards; 7.2.3. Reasonably cooperate with Customer to investigate and remediate a Data Breach and mitigate any further risk to the Customer Data, or risk to data subjects and/or Customer’s reputation or brand; 7.2.4. Provide reasonable assistance to Customer at New Relic’s sole cost and expense; and 7.2.5. Make commercially reasonable efforts to preserve evidence and reasonably cooperate with Customer and legal authorities (as applicable and legally permissible) during an investigation of a Data Breach. 8. Network and Systems Security 8.1. All extranet connectivity by New Relic personnel to systems processing Customer Data will be through secure remote connections. 8.2. Network segments connected to the Internet will be protected by secure access control mechanisms, such as a firewall configured to secure all devices behind it and properly addresses security concerns according to industry standard practices. 8.3. New Relic will have industry standard measures in place to actively monitor its systems and help detect a potential hostile attack, such as Network Intrusion Detection (NID) or Host Intrusion Detection (HID)/Prevention. 8.4. Applications, ports, services, and similar access points installed on a computer or network facility, which are not specifically required for business functionality, will be disabled or removed. 8.5. New Relic maintains configuration standards for authorized operating systems and software for systems that support processing of Customer Data. New Relic will establish and follow server configuration guidelines and processes for preventing unauthorized access to Customer Data. New Relic maintains secure images or templates for systems based on the organization’s approved configuration standards. 8.6. Development, test, and operational environments will be logically separated to reduce the risks of unauthorized access or changes to the operational system. 8.7. New Relic network architecture will be designed to limit site access and restrict the availability of information systems that are considered to be vulnerable to attack. 8.8. New Relic provides appropriate TLS certificates when users access and view Customer Data in the Service. New Relic’s software for sending Customer Data to New Relic will encrypt Customer Data in transit by default. 9. Authentication and Access Management If you subscribe to the requisite New Relic One Service, New Relic will provide industry standard authentication and access controls to protect Customer Data, including industry standard authentication methods utilized to help prevent unauthorized access to the Service. New Relic’s access control methods will clearly state the rules and rights for each user or group of users including applications and information sharing and will include a process for granting and removing access to all information systems processing Customer Data. A record of all privileges allocated will be maintained pursuant to the requirements herein. 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. 10. Vulnerability Management New Relic will have and maintain the following vulnerability management processes for all devices used to connect to the New Relic network and Services. 10.1. New Relic will align to industry standard practices for build out, minimization of services and secure configuration, in accordance with, industry-recognized minimum security baselines for the New Relic platform and subcontractor systems connected to the New Relic platform in relation to the provision of the Services. 10.2. New Relic will employ industry-recognized standards and tools to conduct frequent infrastructure vulnerability scanning to test New Relic’s network and infrastructure and application penetration testing to test the New Relic Services. New Relic applies “Medium”, “High” and “Critical” security patches for all components in production and development environments as soon as commercially possible in accordance with its vulnerability management protocol, and consistent with industry standard practices and standards; 10.3. New Relic will have processes in place designed to ensure adherence to industry standard security development practices for development and testing for all code, APIs, and applications deployed and implemented in support of the Service; 10.4. New Relic will have and maintain solutions to identify and prevent malicious attackers or code from accessing or compromising Customer Data or systems that process Customer Data. These include, but are not limited to, software that identifies and removes malware and detects attempted intrusions. New Relic will have a security event and incident monitoring system and supporting processes to notify appropriate personnel in response to threats. 11. System Access and Logging 11.1. Access to New Relic’s systems will not be granted to employees of New Relic unless they have been uniquely identified and have sufficient credentials. 11.2. Access to New Relic’s systems will be logged and retained for no less than 6 months to assist in investigations and access control monitoring, including, but not limited to, end user access and activities, and information security events. 11.3. New Relic agrees to provide Customer the capability to access log records relating to Customer Accounts and the New Relic systems that process Customer Data in the event of a Data Breach or if required in connection with a law enforcement request. 12. Disaster Recovery and Data Backup 12.1. New Relic will have plans designed to respond to loss of services, which are tested and reviewed at least annually. This plan will include documented policies and procedures to restore service in the event of a service failure. 12.2. New Relic will establish and follow backup and restore procedures for Customer Data. 12.3. New Relic business continuity plan identifies critical systems. Annual disaster recovery tests are performed to check and restore customer data in the event of an incident. 12.4. New Relic will provide Customer with redacted copies of its plan(s) and evidence of tests/reviews upon request, but not more frequently than once annually, and subject to confidentiality requirements. 12.5. Additional information for Customer Data control and encryption are available on New Relic’s Security Guide. 13. Copies and Removal 13.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of the Agreement for any reason: (a) New Relic will, and will cause its personnel, to cease all access and use of any Customer Data, and (b) New Relic will delete all copies of Customer Data within ninety (90) days. 13.2. New Relic will maintain a process of ensuring secure destruction and deletion of all Customer Data, when reasonably requested by Customer or as otherwise provided in the Agreement. The process will include industry standard processes so that: (i) Customer Data cannot be practicably read or reconstructed, and (ii) the systems that store Customer Data are securely erased and/or decommissioned disks are destroyed. 14. Third Party Vendor Management New Relic may use third party vendors to provide the Services. New Relic performs a security risk-based assessment of prospective vendors before working with them to validate they meet New Relic’s security and business continuity standards, including the type of access and classification of data being accessed (if any), controls necessary to protect data, and legal/regulatory requirements. New Relic enters into written agreements with its vendors that process Customer Data which include confidentiality, privacy, and security obligations that provide an appropriate level of protection for Customer Data that these vendors may process for New Relic to maintain the security posture in this Policy, including following industry security standards. 15. Disclosure by Law In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic will (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. New Relic publishes its law enforcement requests report on New Relic’s Security Guide. 16. Updates As New Relic releases new products, services, functionality, and features, New Relic may update this Policy to account for such products, services, functionality, and features. For additional information, see our Security Guide.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 149.96298,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;referenced-policies&#x2F;security-policy&#x2F;. 2. Security Overview 2.1. New Relic"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    },
    {
      "sections": [
        "Security guide",
        "Tip",
        "Security Program",
        "Security Domains",
        "Security Certifications",
        "Data Control, Facilities, and Encryption",
        "Law Enforcement Request Report"
      ],
      "title": "Security guide",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "356f0d11ffcb62208a743a0a7c127f5f6da9c940",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-guide/",
      "published_at": "2021-10-07T02:08:45Z",
      "updated_at": "2021-09-19T15:21:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Last updated September 17, 2021. This is supplement to our security policy and serves as a guide to New Relic’s description of its Services, functionalities, and features. Tip We may update the URLs in this document without notice. Security Program New Relic follows \"privacy by design\" principles as described here: https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Security Domains New Relic’s policies and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless Security; Configuration Management; Vulnerability Management; Network Protection; Transmission Protection; Password Management; Access Control, Audit Logging & Monitoring; Education, Training, and Awareness; Third Party Assurance; Incident Management; Business Continuity and Disaster Recover; Risk Management; Data Protection & Privacy; and Service Management Systems. Security Certifications New Relic audits its Services against industry standards as described at https://docs.newrelic.com/docs/security/security-privacy/compliance/regulatory-audits-new-relic-services/. Data Control, Facilities, and Encryption New Relic's customers can send data to New Relic's APIs by (1) using New Relic's software, (2) using vendor-neutral software that is managed and maintained by a third-party such as via OpenTelemetry instrumentation provided by opentelemetry.io, or (3) from third-party systems that customer's manage and/or control. New Relic's customers can use New Relic's Services such as NerdGraph to filter out and drop data. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/drop-data-using-nerdgraph/. New Relic's customers can adjust their data retention periods as appropriate for their needs. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/manage-data-retention/#adjust-retention. New Relic Logs obfuscates numbers that match known patterns, such as bank card and social security numbers as described here: https://docs.newrelic.com/docs/logs/log-management/get-started/new-relics-log-management-security-privacy/. New Relic honors requests to delete personal data in accordance with applicable privacy laws. Please see https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Customers may use New Relic's APIs to query data, such as NerdGraph described here, and New Relic Services to export the data to other cloud providers. Customers can configure its log forwarder [https://docs.newrelic.com/docs/logs/enable-log-management-new-relic/enable-log-monitoring-new-relic/forward-your-logs-using-infrastructure-agent/] before sending infrastructure logs to New Relic. For New Relic Customers in New Relic US, FedRAMP and HIPAA-enabled environments, Customer Data is replicated to the off-site backup system via Amazon Simple Storage Service (S3). Category of Customer Description FedRAMP HIPAA-enabled US Gen Pop EU Gen Pop Data is stored in Amazon Web Services (“AWS”). Limited Data is stored in IBM Data for New Relic Incident Intelligence is stored in Google Cloud New Relic regularly tests, assess, and evaluates its measures to ensure the security of processing using industry-recognized standards and uses independent third-party auditors as provided below: Annual SOC 2 Type 2 Annual FedRAMP assessment by an independent third-party pursuant to NIST 800-53 rev 4 Moderate authorization. Annual HITRUST-validated assessment by an independent third-party *Pursuing CY2021 Q4 ISO 27001 *Pursuing CY2021 TISAX *Pursuing CY2021 The Services that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed information about AWS security is available at https://aws.amazon.com/security/ and http://aws.amazon.com/security/sharing-the-security-responsibility/. Data encryption at rest utilizes FIPS 140-2 compliant encryption methodology. For AWS SOC Reports, please see https://aws.amazon.com/compliance/soc-faqs/. The Services that operate on Google Cloud Platform (\"GCP\") are protected by the security and environmental controls of GCP. Detailed information about GCP security is available at https://cloud.google.com/docs/tutorials#security. For GCP reports, please see https://cloud.google.com/security/compliance/. IBM Deft Zayo QTS Law Enforcement Request Report New Relic has not to date received any request for customer data from a law enforcement or other government agency (including under any national security process), and has not made any corresponding disclosures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 135.48729,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " *Pursuing CY2021 The Services that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed <em>information</em> about AWS security is available at https:&#x2F;&#x2F;aws.amazon.com&#x2F;security&#x2F; and http:&#x2F;&#x2F;aws.amazon.com&#x2F;security&#x2F;sharing-the-security-responsibility"
      },
      "id": "6147558128ccbc973a56a863"
    }
  ],
  "/docs/licenses/product-or-service-licenses/miscellaneous/help-center-documentation-licenses": [
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2021-10-08T06:59:42Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 177.99402,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    },
    {
      "sections": [
        "iOS application licenses"
      ],
      "title": "iOS application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "a6df56e363112e7387e6887f04381a36a5457e84",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/ios-application-licenses/",
      "published_at": "2021-10-08T06:58:59Z",
      "updated_at": "2021-09-27T15:25:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Copyright AFNetworking MIT Copyright © 2011-2020 Alamofire Software Foundation http://alamofire.org/ Ace BSD Copyright © 2010, Ajax.org B.V. ActiveLabel MIT Copyright © 2015 Optonaut Alamofire MIT Copyright © 2014-2021 Alamofire Software Foundation Analytics MIT Copyright © 2016 Segment.io, Inc. BBlock MIT Copyright © 2012 David Keegan BigNumber MIT Copyright © 2019 mkrd CDMarkdownKit MIT Copyright © 2016-2017 Christopher de Haan contact@christopherdehaan.me Charts Apache License, Version 2.0 Copyright © 2016-2019 Daniel Cohen Gindi & Philipp Jahoda ECSlidingViewController MIT Copyright © 2013 EdgeCase LoginManagerSDK New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. Mantle MIT Copyright © GitHub, Inc. All rights reserved. Copyright © 2012, Bitswift, Inc MetricMetadata New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NRCharts New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NVActivityIndicatorView MIT Copyright © 2016 Vinh Nguyen NewRelicAgent New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. ObjectMapper MIT Copyright © 2014 Hearst RadarKit New Relic License © 2010-2021 New Relic, Inc. All rights reserved. SSPullToRefresh MIT Copyright © 2012-2014 Sam Soffes, http://soff.es UICKeyChainStore MIT Copyright © 2011 kishikawa katsumi WidgetLibrary New Relic License © 2010-2021 New Relic, Inc. All rights reserved. XYPieChart MIT Copyright © 2012 Xiaoyang Feng, XYStudio.cc Yams MIT Copyright © 2016 JP Simard. jsTimezoneDetect MIT Copyright © 2012 Jon Nylander, project maintained at bitbucket.org lodash MIT Copyright © JS Foundation and other contributors js.foundation The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 171.91861,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "iOS application <em>licenses</em>",
        "sections": "iOS application <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Copyright"
      },
      "id": "603e9db1196a670e70a83df3"
    },
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2021-10-08T07:07:37Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0 © 2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0 © 2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0 © 2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0 © 2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0 © 2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright © 2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright © 2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright © 1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright © 1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 134.86768,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Logs plugin <em>licenses</em>",
        "sections": "Logs plugin <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    }
  ]
}