{
  "/docs/kubernetes-pixie/auto-telemetry-pixie/get-started-auto-telemetry-pixie": [
    {
      "sections": [
        "Install Auto-telemetry with Pixie",
        "General prerequisites for using Pixie",
        "Setup steps depend on your account status",
        "Install from the beginning of the guided install process",
        "Install from the Configure the HELM command/manifest (yaml) file",
        "Important",
        "Helm method",
        "manifest method",
        "If you link the wrong Pixie and New Relic account"
      ],
      "title": "Install Auto-telemetry with Pixie",
      "type": "docs",
      "tags": [
        "Pixie Auto-telemetry",
        "Service monitoring",
        "Kubernetes",
        "eBPF"
      ],
      "external_id": "bb957763e579e39ef2bbeafeebc46ab3a111bca2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/auto-telemetry-pixie/install-auto-telemetry-pixie/",
      "published_at": "2022-01-08T03:49:26Z",
      "updated_at": "2022-01-08T03:49:26Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To get up and running with Auto-telemetry with Pixie, you start with our guided installation. The guided installation deploys Pixie with New Relic's Kubernetes integration on your cluster. You don't need to do any further configuration or installation to start using Pixie. If you want to install Auto-telemetry with Pixie on multiple clusters, re-run the guided install for each additional cluster. General prerequisites for using Pixie Ready to get started? If you don't already have one, sign up for a New Relic account. It's free, forever! You must be a New Relic full platform user. Other user-related requirements: Users on our New Relic One user model must be assigned to a group that has a role with Pixie-related capabilities. Users on our original user model cannot be Restricted. In addition: Review this Pixie data security overview for actions to take to secure your data. Make sure you have sufficient memory. Pixie requires at least 2Gb of memory on each node in your cluster. More memory may be required for larger clusters. Pixie needs to run in privileged mode. Review the other Pixie technical requirements. Setup steps depend on your account status Use the following table to find out where to start installing Auto-telemetry with Pixie. Where you start the installation depends on whether you already have a New Relic or Pixie account, or both. New Relic Pixie Next steps Start the guided install at the beginning of the process. If you already have both types of accounts, and used the same email address for each of them, click the New Relic icon in the Pixie UI. This brings you to the Configure the HELM command/manifest (yaml) file section of the guided installation. Then, follow the steps. If you're using different email addresses in Pixie and New Relic, create a new account for either Pixie or New Relic to match email addresses across both products. You can also contact New Relic support to manually link your existing New Relic account with your Pixie account. If you follow a link to New Relic from the Pixie UI and do not have a New Relic account, you must first create one. Click the New Relic icon in the Pixie UI, and follow the steps to create a New Relic account. When you do so, your Pixie account is linked to it. Then, continue the guided install process with these steps. Sign up for a free New Relic account. Then, start the guided install at the beginning of the process. Install from the beginning of the guided install process Open our New Relic One guided install. Select the account you want to use for the guided install, and click Continue. Note: if you have a single account, you won't see this option. Select Kubernetes and then continue with step one in the next section. Install from the Configure the HELM command/manifest (yaml) file If you arrived in the guided installation process by following a link from Pixie or from within New Relic, your steps begin here. Select the account and cluster for the install. If needed, select a namespace. Important Currently, Pixie performs best on clusters with up to 100 nodes (exceeding 100 nodes can lead to excessive memory usage and scripts failing to run). Friendly reminder: autoscaling can quickly drive up your node numbers. Click Continue. Select the data you want to gather, observe, and debug, and click Continue. On the Choose install method page, select either Helm or manifest. Helm method Run the provided Helm command on your command line. If you're concerned about the amount of Pixie data you'll ingest, check out strategies for reducing ingest. Helm installs a bundle containing the New Relic infrastructure agent, an integration to gather Prometheus metrics and Kubernetes events, and the Pixie integration. The deployment takes a few minutes to complete. To see the status of the install to the cluster, run kubectl get pods -n newrelic. For general information about installing a Kubernetes integration, see this Helm install info. manifest method Run the provided command in your console, and insert the path to your downloaded manifest. If you're running your Kubernetes cluster in the cloud, see the additional steps in the Kubernetes docs. Click Continue to open the Listening for data page. When you get the message, See your data, click Kubernetes Cluster Explorer to see your cluster. Auto-telemetry with Pixie might restart after installation. This is caused by the auto update feature. If you link the wrong Pixie and New Relic account Contact support to unlink a Pixie account from your New Relic account. Be aware that if you unlink a Pixie account that was created automatically through the guided install, you'll lose access to that Pixie account.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 491.92688,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install <em>Auto</em>-<em>telemetry</em> with <em>Pixie</em>",
        "sections": "Install <em>Auto</em>-<em>telemetry</em> with <em>Pixie</em>",
        "tags": "<em>Pixie</em> <em>Auto</em>-<em>telemetry</em>",
        "body": "To get up and running with <em>Auto</em>-<em>telemetry</em> with <em>Pixie</em>, you start with our guided installation. The guided installation deploys <em>Pixie</em> with New Relic&#x27;s <em>Kubernetes</em> integration on your cluster. You don&#x27;t need to do any further configuration or installation to start using <em>Pixie</em>. If you want to install"
      },
      "id": "6174ca14e7b9d26ba513cd12"
    },
    {
      "sections": [
        "Query Pixie data",
        "Metrics and specifications",
        "HTTP metrics",
        "JVM metrics",
        "HTTP server span"
      ],
      "title": "Query Pixie data",
      "type": "docs",
      "tags": [
        "Pixie Auto-telemetry",
        "Service monitoring",
        "Kubernetes",
        "eBPF"
      ],
      "external_id": "22fa4b0cc62f42db920a5ab3ac7b334dca0e2193",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/auto-telemetry-pixie/auto-telemetry-pixie-data-model/",
      "published_at": "2022-01-08T01:57:23Z",
      "updated_at": "2021-10-24T02:51:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Auto-telemetry with Pixie pulls data from the Pixie Cloud API and sends it to the New Relic OpenTelemetry endpoint. You can build your own charts and query your Auto-telemetry with Pixie data using the query builder and the NerdGraph API. Find out more about getting started with Auto-telemetry with Pixie here. Metrics and specifications HTTP metrics Query for duration of inbound HTTP request. For example: FROM Metric SELECT average(http.server.duration) FACET service.name WHERE instrumentation.provider='pixie' Copy Event type Metric Metric name http.server.duration Spec OpenTelemetry HTTP metric spec Description Measures the duration of the inbound HTTP request. OTEL data type MetricDataTypeDoubleSummary with min(quantile=0) and max(quantile=1) Unit milliseconds Required attributes service.name Static attributes instrumentation.provider = pixie HTTP attributes http.status_code Entity attributes service.instance.id k8s.cluster.name k8s.namespace.name k8s.pod.name k8s.container.name JVM metrics Query to measure the time spent in a given JVM garbage collectors in milliseconds. For example: FROM Metric SELECT average(runtime.jvm.gc.collection) FACET service.name, gc WHERE instrumentation.provider='pixie' Copy Event type Metric Metric name runtime.jvm.gc.collection Spec opentelemetry.jvm.gc.collection Description Time spent in a given JVM garbage collector in milliseconds. Unit milliseconds Required attributes service.name Static attributes instrumentation.provider = pixie JVM attributes gc = young|full Entity attributes service.instance.id k8s.cluster.name k8s.namespace.name k8s.pod.name k8s.container.name Query to find out the number of bytes in a given JVM memory area. For example: FROM Metric SELECT average(runtime.jvm.memory.area) FACET service.name WHERE type='used' AND instrumentation.provider='pixie' Copy Event type Metric Metric name runtime.jvm.memory.area Spec opentelemetry-java-instrumentation Description Bytes of a given JVM memory area. Unit bytes Required attributes service.name Static attributes instrumentation.provider = pixie JVM attributes type = used|total|max area = heap Entity attributes service.instance.id k8s.cluster.name k8s.namespace.name k8s.pod.name k8s.container.name HTTP server span Example query: FROM Span SELECT uniques(name) WHERE span.kind='server' AND instrumentation.provider='pixie' AND service.name='orders' Copy Spec Semantic conventions for HTTP spans Event type Span Required attributes name = normalized HTTP path service.name trace.id span.id Static attributes span.kind = server instrumentation.provider = pixie HTTP attributes http.host http.method http.path http.status_code http.url http.user_agent",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 252.664,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Query <em>Pixie</em> data",
        "sections": "Query <em>Pixie</em> data",
        "tags": "<em>Pixie</em> <em>Auto</em>-<em>telemetry</em>",
        "body": "<em>Auto</em>-<em>telemetry</em> with <em>Pixie</em> pulls data from the <em>Pixie</em> Cloud API and sends it to the New Relic Open<em>Telemetry</em> endpoint. You can build your own charts and query your <em>Auto</em>-<em>telemetry</em> with <em>Pixie</em> data using the query builder and the NerdGraph API. Find out more about getting started with <em>Auto</em>-<em>telemetry</em>"
      },
      "id": "617e08a264441f3985fbecaf"
    },
    {
      "sections": [
        "Auto-telemetry with Pixie data and security",
        "Control who has access to Pixie data",
        "Manage auto-update and two-way communication",
        "Helm option",
        "newrelic-manifest.yaml option"
      ],
      "title": "Auto-telemetry with Pixie data and security",
      "type": "docs",
      "tags": [
        "Pixie Auto-telemetry data",
        "Service monitoring",
        "Kubernetes",
        "eBPF",
        "Pixie data"
      ],
      "external_id": "acec5042b8735d73fa255829401e8708bd6d0595",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/auto-telemetry-pixie/pixie-data-security-overview/",
      "published_at": "2022-01-08T07:41:45Z",
      "updated_at": "2021-10-24T02:51:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Auto-telemetry with Pixie is New Relic One's integration of Community Cloud for Pixie, a managed version of Pixie open source software. Auto-telemetry with Pixie therefore benefits from Pixie's approach to keeping data secure. The data that Pixie collects is stored entirely within your Kubernetes cluster. This data does not persist outside of your environment, and will never be stored by Community Cloud for Pixie. This means that your sensitive data remains within your environment and control. Community Cloud for Pixie makes queries directly to your Kubernetes cluster to access the data. In order for the query results to be shown in the Community Cloud for Pixie UI, CLI, and API, the data is sent to the client from your cluster using a reverse proxy. Community Cloud for Pixie’s reverse proxy is designed to ensure: Data is ephemeral. It only passes through the Community Cloud for Pixie's cloud proxy in transit. This ensures data locality. Data is encrypted while in transit. Only you are able to read your data. New Relic One fetches and stores data that related to an application's performance. With Auto-telemetry with Pixie, a predefined subset of data persists outside of your cluster. This data is stored in our database, in your selected region. This data persists in order to give you long-term storage, alerting, correlation with additional data, and the ability to use advanced New Relic platform capabilities, such as anomaly detection. The persisted performance metrics include, but are not limited to: Golden metrics (throughput, latency, error rate) for HTTP-based services HTTP transaction data Database transaction data (for MySQL & PostgreSQL) Distributed tracing JVM metrics The data you view on the Live debugging tab comes through Community Cloud for Pixie, and is therefore potentially sensitive. It is not stored by New Relic and is ephemeral and queryable for less than 24 hours. Control who has access to Pixie data If you want to manage which members of your organization can view Pixie data in New Relic One, as well as install and delete Pixie links, you can create a custom role. Note that this option is available only to Enterprise and Pro level customers. For more information, see New Relic's user model. Manage auto-update and two-way communication Pixie maintains an active two-way communication channel from your host system to Community Cloud with Pixie at withpixie.ai. Pixie uses this communication channel to query data, push updates, and retrieve metadata and health checks about Pixie and your Kubernetes cluster. By default, Pixie queries withpixie.ai to check if new updates have been pushed and then automatically installs them if they’re present. To disable auto updates, you must set a flag prior to the install process using either Helm or in the newrelic-manifest.yaml file. To disable automatic updates, choose one: Helm option Add --set pixie-chart.disableAutoUpdate=true to your Helm command. newrelic-manifest.yaml option in your newrelic-manifest.yaml file under the pl-cluster-config section, add PL_DISABLE_AUTO_UPDATE: \"true\" to the data directive. Example: --- apiVersion: v1 data: PL_CUSTOM_ANNOTATIONS: \"\" PL_CUSTOM_LABELS: \"\" PL_DISABLE_AUTO_UPDATE: \"true\" PL_ETCD_OPERATOR_ENABLED: \"false\" PL_MD_ETCD_SERVER: \"https://etcd.newrelic.svc:2379\" PX_MEMORY_LIMIT: \"\" kind: ConfigMap metadata: annotations: creationTimestamp: null labels: name: pl-cluster-config namespace: newrelic --- Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 213.02283,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Auto</em>-<em>telemetry</em> with <em>Pixie</em> data and security",
        "sections": "<em>Auto</em>-<em>telemetry</em> with <em>Pixie</em> data and security",
        "tags": "<em>Pixie</em> <em>Auto</em>-<em>telemetry</em> data",
        "body": "<em>Auto</em>-<em>telemetry</em> with <em>Pixie</em> is New Relic One&#x27;s integration of Community Cloud for <em>Pixie</em>, a managed version of <em>Pixie</em> open source software. <em>Auto</em>-<em>telemetry</em> with <em>Pixie</em> therefore benefits from <em>Pixie</em>&#x27;s approach to keeping data secure. The data that <em>Pixie</em> collects is stored entirely within your <em>Kubernetes</em>"
      },
      "id": "6174ca40e7b9d203d413d744"
    }
  ],
  "/docs/kubernetes-pixie/auto-telemetry-pixie/install-auto-telemetry-pixie": [
    {
      "sections": [
        "Auto-telemetry with Pixie for instant Kubernetes observability",
        "Quickly start observing and debugging Kubernetes clusters",
        "Important",
        "Explore your cluster",
        "Tip",
        "Investigate usage spikes using the flame graph",
        "Debug live"
      ],
      "title": "Auto-telemetry with Pixie for instant Kubernetes observability",
      "type": "docs",
      "tags": [
        "Pixie Auto-telemetry",
        "Service monitoring",
        "Kubernetes",
        "eBPF"
      ],
      "external_id": "1dc7474aedbf84c51593c152fc39fda0a46b4f48",
      "image": "https://docs.newrelic.com/static/521c1e907520f4bc8da33ad27ab47289/c1b63/flamegraph.png",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/auto-telemetry-pixie/get-started-auto-telemetry-pixie/",
      "published_at": "2022-01-08T03:10:54Z",
      "updated_at": "2021-12-30T09:39:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "When we say auto-telemetry, we’re not talking about cars — we're talking about instant baseline visibility into your Kubernetes clusters. With the New Relic One integration with Pixie, you get similar data to traditional language agents, but without manually instrumenting your code or redeploying your application. Pixie auto-telemetry is powered by eBPF, a virtual machine-like construct that enables Pixie to seamlessly collect fine-grained telemetry data — service-level metrics, unsampled requests, and more. With one install command, you get deeper insight into your Kubernetes clusters and workloads. No language agents required. Live debugging with Pixie shows a service graph listing the namespaces and the node that are available on the current cluster. Simply put, Auto-telemetry with Pixie offers the quickest option for getting observability into your Kubernetes services. Ready to get started? You'll be able to configure Pixie to suit your environnment after you create a New Relic account (it's free, forever!) and install our Pixie integration. Quickly start observing and debugging Kubernetes clusters Our Pixie integration gives you the best of both worlds: Pixie’s fast and simple Kubernetes observability coupled with New Relic One’s incident correlation, intelligent alerting, and long-term retention. You’ll get visibility into HTTP services using golden signals, HTTP transactions, database transactions, distributed tracing, and JVM metrics. You can operate, debug, and scale your Kubernetes clusters based on the information you learn about how your clusters and services are running. Using the New Relic Explorer, you can see key metrics and events at every level, starting with the cluster, and diving down into namespaces, deployments, and pods. You can quickly spot anomalous behavior, and where it’s happening. And then dive deeper using embedded visualizations of your Pixie data. Quickly identify hot spots with a flame graph display. On the Live debugging with Pixie tab, answer questions like what SQL requests your app is making or which services are talking to each other. This short video (approx. 6:20 minutes) shows how Pixie and Kubernetes work together in New Relic One so you can debug faster with code-level insights: Important Auto-Telemetry with Pixie leverages Community Cloud with Pixie, a separate platform from New Relic One. Use of Community Cloud with Pixie is subject to separate terms of service. Explore your cluster Access the Pixie UI via New Relic's Live debugging with Pixie area of your Kubernetes clusters. The cluster explorer provides a quick overview of the nodes in your cluster, including CPU, memory, and storage, as well as the status of each pod (healthy, warning, or critical). You can also find out what services are running in each container, their latency, throughput, and error rate. For more information about using the cluster explorer, see Navigate the Kubernetes cluster explorer. Note that you cannot log directly into the Pixie UI unless you have created a separate Pixie login. Tip Containers might be listed for up to four hours after they get decommissioned. You can query the Pixie data in New Relic One and create dashboards for at-a-glance monitoring. For more information, see our documentation about the data model and sample queries. Investigate usage spikes using the flame graph Debugging is orders of magnitude easier when you can quickly see what your application is doing. The Pixie flame graph display requires no instrumentation, redeploying, or recompiling. It works for compiled languages like Go, C+, Rust, to name a few. And at a glance, the flame graph tells you what functions your application is spending time on and where you have hot spots. Flame graphs are especially useful for hierarchical resource use, like disk usage and CPU utilization. For more information on how to read a flame graph, see the Pixie flame graph docs. Debug live On the Live debugging with Pixie tab, run PxL scripts — scripts written in Pixie's PxL language — to view live data captured through eBPF. Select the script drop-down and then select a script to run in the tab. (For best results, select a time range that is recent in the time picker.) Scripts enable you to debug: Traffic in multiple formats: HTTP and HTTPs (including encrypted), DNS, Postgres, MySQL, Cassandra, Redis (currently supporting SQL and HTTP in beta). Learn more: Request tracing tutorial. Database request performance. Learn more: Database Query Profiling tutorial. Service maps to learn which services are talking to each other. Learn more: Service Performance tutorial. Network traffic maps to learn which nodes are talking to each other. Learn more: Network Monitoring tutorial. Monitor resource usage by Node and Pod. Learn more: Infra health tutorial.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 508.02463,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Auto</em>-<em>telemetry</em> with <em>Pixie</em> for instant <em>Kubernetes</em> observability",
        "sections": "<em>Auto</em>-<em>telemetry</em> with <em>Pixie</em> for instant <em>Kubernetes</em> observability",
        "tags": "<em>Pixie</em> <em>Auto</em>-<em>telemetry</em>",
        "body": " your application. <em>Pixie</em> <em>auto</em>-<em>telemetry</em> is powered by <em>eBPF</em>, a virtual machine-like construct that enables <em>Pixie</em> to seamlessly collect fine-grained <em>telemetry</em> data — <em>service</em>-level metrics, unsampled requests, and more. With one install command, you get deeper insight into your <em>Kubernetes</em> clusters"
      },
      "id": "6174ca7364441f0e385fdea5"
    },
    {
      "sections": [
        "Query Pixie data",
        "Metrics and specifications",
        "HTTP metrics",
        "JVM metrics",
        "HTTP server span"
      ],
      "title": "Query Pixie data",
      "type": "docs",
      "tags": [
        "Pixie Auto-telemetry",
        "Service monitoring",
        "Kubernetes",
        "eBPF"
      ],
      "external_id": "22fa4b0cc62f42db920a5ab3ac7b334dca0e2193",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/auto-telemetry-pixie/auto-telemetry-pixie-data-model/",
      "published_at": "2022-01-08T01:57:23Z",
      "updated_at": "2021-10-24T02:51:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Auto-telemetry with Pixie pulls data from the Pixie Cloud API and sends it to the New Relic OpenTelemetry endpoint. You can build your own charts and query your Auto-telemetry with Pixie data using the query builder and the NerdGraph API. Find out more about getting started with Auto-telemetry with Pixie here. Metrics and specifications HTTP metrics Query for duration of inbound HTTP request. For example: FROM Metric SELECT average(http.server.duration) FACET service.name WHERE instrumentation.provider='pixie' Copy Event type Metric Metric name http.server.duration Spec OpenTelemetry HTTP metric spec Description Measures the duration of the inbound HTTP request. OTEL data type MetricDataTypeDoubleSummary with min(quantile=0) and max(quantile=1) Unit milliseconds Required attributes service.name Static attributes instrumentation.provider = pixie HTTP attributes http.status_code Entity attributes service.instance.id k8s.cluster.name k8s.namespace.name k8s.pod.name k8s.container.name JVM metrics Query to measure the time spent in a given JVM garbage collectors in milliseconds. For example: FROM Metric SELECT average(runtime.jvm.gc.collection) FACET service.name, gc WHERE instrumentation.provider='pixie' Copy Event type Metric Metric name runtime.jvm.gc.collection Spec opentelemetry.jvm.gc.collection Description Time spent in a given JVM garbage collector in milliseconds. Unit milliseconds Required attributes service.name Static attributes instrumentation.provider = pixie JVM attributes gc = young|full Entity attributes service.instance.id k8s.cluster.name k8s.namespace.name k8s.pod.name k8s.container.name Query to find out the number of bytes in a given JVM memory area. For example: FROM Metric SELECT average(runtime.jvm.memory.area) FACET service.name WHERE type='used' AND instrumentation.provider='pixie' Copy Event type Metric Metric name runtime.jvm.memory.area Spec opentelemetry-java-instrumentation Description Bytes of a given JVM memory area. Unit bytes Required attributes service.name Static attributes instrumentation.provider = pixie JVM attributes type = used|total|max area = heap Entity attributes service.instance.id k8s.cluster.name k8s.namespace.name k8s.pod.name k8s.container.name HTTP server span Example query: FROM Span SELECT uniques(name) WHERE span.kind='server' AND instrumentation.provider='pixie' AND service.name='orders' Copy Spec Semantic conventions for HTTP spans Event type Span Required attributes name = normalized HTTP path service.name trace.id span.id Static attributes span.kind = server instrumentation.provider = pixie HTTP attributes http.host http.method http.path http.status_code http.url http.user_agent",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 252.664,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Query <em>Pixie</em> data",
        "sections": "Query <em>Pixie</em> data",
        "tags": "<em>Pixie</em> <em>Auto</em>-<em>telemetry</em>",
        "body": "<em>Auto</em>-<em>telemetry</em> with <em>Pixie</em> pulls data from the <em>Pixie</em> Cloud API and sends it to the New Relic Open<em>Telemetry</em> endpoint. You can build your own charts and query your <em>Auto</em>-<em>telemetry</em> with <em>Pixie</em> data using the query builder and the NerdGraph API. Find out more about getting started with <em>Auto</em>-<em>telemetry</em>"
      },
      "id": "617e08a264441f3985fbecaf"
    },
    {
      "sections": [
        "Auto-telemetry with Pixie data and security",
        "Control who has access to Pixie data",
        "Manage auto-update and two-way communication",
        "Helm option",
        "newrelic-manifest.yaml option"
      ],
      "title": "Auto-telemetry with Pixie data and security",
      "type": "docs",
      "tags": [
        "Pixie Auto-telemetry data",
        "Service monitoring",
        "Kubernetes",
        "eBPF",
        "Pixie data"
      ],
      "external_id": "acec5042b8735d73fa255829401e8708bd6d0595",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/auto-telemetry-pixie/pixie-data-security-overview/",
      "published_at": "2022-01-08T07:41:45Z",
      "updated_at": "2021-10-24T02:51:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Auto-telemetry with Pixie is New Relic One's integration of Community Cloud for Pixie, a managed version of Pixie open source software. Auto-telemetry with Pixie therefore benefits from Pixie's approach to keeping data secure. The data that Pixie collects is stored entirely within your Kubernetes cluster. This data does not persist outside of your environment, and will never be stored by Community Cloud for Pixie. This means that your sensitive data remains within your environment and control. Community Cloud for Pixie makes queries directly to your Kubernetes cluster to access the data. In order for the query results to be shown in the Community Cloud for Pixie UI, CLI, and API, the data is sent to the client from your cluster using a reverse proxy. Community Cloud for Pixie’s reverse proxy is designed to ensure: Data is ephemeral. It only passes through the Community Cloud for Pixie's cloud proxy in transit. This ensures data locality. Data is encrypted while in transit. Only you are able to read your data. New Relic One fetches and stores data that related to an application's performance. With Auto-telemetry with Pixie, a predefined subset of data persists outside of your cluster. This data is stored in our database, in your selected region. This data persists in order to give you long-term storage, alerting, correlation with additional data, and the ability to use advanced New Relic platform capabilities, such as anomaly detection. The persisted performance metrics include, but are not limited to: Golden metrics (throughput, latency, error rate) for HTTP-based services HTTP transaction data Database transaction data (for MySQL & PostgreSQL) Distributed tracing JVM metrics The data you view on the Live debugging tab comes through Community Cloud for Pixie, and is therefore potentially sensitive. It is not stored by New Relic and is ephemeral and queryable for less than 24 hours. Control who has access to Pixie data If you want to manage which members of your organization can view Pixie data in New Relic One, as well as install and delete Pixie links, you can create a custom role. Note that this option is available only to Enterprise and Pro level customers. For more information, see New Relic's user model. Manage auto-update and two-way communication Pixie maintains an active two-way communication channel from your host system to Community Cloud with Pixie at withpixie.ai. Pixie uses this communication channel to query data, push updates, and retrieve metadata and health checks about Pixie and your Kubernetes cluster. By default, Pixie queries withpixie.ai to check if new updates have been pushed and then automatically installs them if they’re present. To disable auto updates, you must set a flag prior to the install process using either Helm or in the newrelic-manifest.yaml file. To disable automatic updates, choose one: Helm option Add --set pixie-chart.disableAutoUpdate=true to your Helm command. newrelic-manifest.yaml option in your newrelic-manifest.yaml file under the pl-cluster-config section, add PL_DISABLE_AUTO_UPDATE: \"true\" to the data directive. Example: --- apiVersion: v1 data: PL_CUSTOM_ANNOTATIONS: \"\" PL_CUSTOM_LABELS: \"\" PL_DISABLE_AUTO_UPDATE: \"true\" PL_ETCD_OPERATOR_ENABLED: \"false\" PL_MD_ETCD_SERVER: \"https://etcd.newrelic.svc:2379\" PX_MEMORY_LIMIT: \"\" kind: ConfigMap metadata: annotations: creationTimestamp: null labels: name: pl-cluster-config namespace: newrelic --- Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 213.02281,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Auto</em>-<em>telemetry</em> with <em>Pixie</em> data and security",
        "sections": "<em>Auto</em>-<em>telemetry</em> with <em>Pixie</em> data and security",
        "tags": "<em>Pixie</em> <em>Auto</em>-<em>telemetry</em> data",
        "body": "<em>Auto</em>-<em>telemetry</em> with <em>Pixie</em> is New Relic One&#x27;s integration of Community Cloud for <em>Pixie</em>, a managed version of <em>Pixie</em> open source software. <em>Auto</em>-<em>telemetry</em> with <em>Pixie</em> therefore benefits from <em>Pixie</em>&#x27;s approach to keeping data secure. The data that <em>Pixie</em> collects is stored entirely within your <em>Kubernetes</em>"
      },
      "id": "6174ca40e7b9d203d413d744"
    }
  ],
  "/docs/kubernetes-pixie/auto-telemetry-pixie/manage-pixie-data": [
    {
      "sections": [
        "Install the Kubernetes integration manually using Helm",
        "Compatibility and requirements",
        "Install Kubernetes integration with Helm",
        "Install with Helm 3 and nri-bundle (recommended)",
        "Installing and configuring nri-bundle with Helm",
        "Install with Helm 2 and nri-bundle (legacy)",
        "Installation instructions for Helm 2",
        "Important",
        "Tip",
        "Helm configuration options",
        "Upgrade using Helm",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data",
        "Reduce data ingest",
        "Prometheus OpenMetrics Integration",
        "New Relic Logging",
        "New Relic Pixie Integration",
        "Uninstall Kubernetes integration"
      ],
      "title": "Install the Kubernetes integration manually using Helm",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "29d42af98d41e7e4e7be86f0150254e132cb6b6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/install-kubernetes-integration-using-helm/",
      "published_at": "2022-01-08T09:49:44Z",
      "updated_at": "2022-01-08T09:49:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Helm is a package manager on top of Kubernetes. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in Kubernetes. To install the integration using Helm, we recommend our Kubernetes automated installer, which will prompt for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our integration as plain manifests rather than a Helm release. See Kubernetes integration: install and configure for more details about how to use our automated installer. Start the installer This page describes in more depth how to install and configure the New Relic integration without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the Kubernetes integration. Our charts are also compatible with Helm 2, but support for it might be removed in the future. To install the Kubernetes integration using Helm, you will need your New Relic account license key and your Kubernetes cluster's name: Find and copy your New Relic license key. Find the name of your cluster with this command: kubectl config current-context Copy Note these values somewhere safe, as you will need them later during the installation process. Install Kubernetes integration with Helm Install with Helm 3 and nri-bundle (recommended) New Relic has several charts for the different components which offer different features for the platform: newrelic-infrastructure: Contains the main Kubernetes integration and the infrastructure agent. This is the core component for the New Relic Kubernetes experience, responsible of reporting most of the data that is surfaced in dashboard and the Kubernetes Cluster Explorer. newrelic-logging: Provides a DaemonSet with New Relic's Fluent Bit output plugin to easily forward your logs to New Relic. nri-kube-events: Collects and reports cluster events (such as kubectl get events) to New Relic. nri-prometheus: New Relic's Prometheus OpenMetrics Integration, automatically scrapes Prometheus endpoints present in the cluser and reports metrics to New Relic. nri-metadata-injection: Sets up a minimal MutatingAdmissionWebhook that injects a couple of environment to containers. These contain metadata about the cluster and New Relic installation and will be later picked up by applications instrumented using APM, allowing to correlate APM and infrastructure data. nri-statsd: New Relic StatsD integration. Additionally, New Relic provides nri-bundle, a chart which pulls a selectable set of the charts mentioned above. nri-bundle can also install Kube State Metrics and Pixie for you if needed. While it is possible to install those charts separately, we strongly recommend using the nri-bundle chart for Kubernetes deployments, as it ensures that values across all the charts are consistent and provides full control over which components are installed, as well as the possibility to configure all of them as Helm dependencies. This is the same chart that is used and referenced by our automated installer. Installing and configuring nri-bundle with Helm Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Please notice and adjust the following flags: global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY: Must be set to a valid License Key for your account. global.cluster=K8S_CLUSTER_NAME: Is used to identify the cluster in the New Relic UI, so should be a descriptive value not used by any other Kubernetes cluster configured in your New Relic account. ksm.enabled=true: Setting this to true will automatically install Kube State Metrics (KSM) for you, which is required for our integration to run. You can set this to false if KSM is already present in your cluster, even if it is on a different namespace. newrelic-infrastructure.privileged=true: Can be set to false to install a trimmed down version of our integration that does not require extra privileges, such as hostPath mounts or running containers as root. Please note that this will disable detailed process collection from the host. For performance reasons, our logging solution still requires hostPath mounts, regardless of the value of this flag. If this is not allowedin your cluster, you will need to disable the logging solution by specifying logging.enabled=false. prometheus.enabled=true: Will deploy our Prometheus OpenMetrics integration, which automatically collects data from prometheus endpoints present in the cluster. webhook.enabled=true: Will install our minimal webhook, which adds environment variables that, in turn, allows linking applications instrumented with APM with infrastructure data. Our chart has a comprehensive set of flags and tunables that can be edited to configure our solution to your particular needs. For a full list of all the flags that can be configured, please check the chart's README and the default values.yaml file. Install the Kubernetes integration by running the customized command without --debug and --dry-run: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl -n newrelic get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node for newrelic-infrastructure, as well as one Deployment and one pod for any other component that you have enabled. Install with Helm 2 and nri-bundle (legacy) Installation instructions for Helm 2 Important Helm 2 has been deprecated and New Relic does not recommend using it for deployments. Instructions in this section are provided for legacy systems only. To install using Helm 2: Ensure that Helm 2, including their cluster-side components (such as Tiller) are properly installed and configured. Please check the official documentation for more details. Set the cluster where you want to install the agent: kubectl config set-cluster DESIRED_CLUSTER Copy To see the available clusters, run kubectl config get-clusters Make sure that kube-state-metrics is installed on your machine: kubectl get deployment --all-namespaces | grep kube-state-metrics Copy If it's not installed, follow the instructions in the kube-state-metrics GitHub repo to install it. Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. This step uses the --dry-run and --debug switches and therefore the agent is not installed. helm install newrelic/newrelic-infrastructure \\ --dry-run \\ --debug \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Install the New Relic Kubernetes integration: helm install newrelic/newrelic-infrastructure \\ --set licenseKey=your_new_relic_license_key \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Tip Note that the --dry-run and --debug switches have been removed. Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node. Helm configuration options When you install or upgrade the Kubernetes integration with Helm using the command line, you can pass your configuration variables with the --set flag. helm install newrelic/newrelic-infrastructure \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=YOUR_CLUSTER_NAME Copy A full list of the configuration parameters can be found in the newrelic-infrastructure chart README Upgrade using Helm To update your Kubernetes integration installed via Helm: Update the local chart repository: helm repo update Copy Update the release by running again the appropriate helm upgrade --install ... command in the section above. Monitor services running on Kubernetes After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages. Reduce data ingest Our charts support setting an option to reduce the amount of data ingest at the cost of dropping detailed information. To enable it, set global.lowDataMode=true in the nri-bundle chart. lowDataMode affects three specific components of the nri-bundle chart outlined below. Prometheus OpenMetrics Integration If lowDataMode is enabled, the following metrics are excluded by default as they are already collected and used by the New Relic Kubernetes Integration. - kube_ - container_ - machine_ - cadvisor_ Copy New Relic Logging If lowDataMode is enabled, Labels and Annotations are set to Off in the Filter section of the fluent-bit.conf file. This means that this detail will be dropped from the container log files which reduces the overall data ingest into New Relic. The following fields are retained: Allowlist_key container_name Allowlist_key namespace_name Allowlist_key pod_name Allowlist_key stream Allowlist_key log Copy Low Data Mode Log Example Complete Log Record [ { \"cluster_name\": \"api-test\", \"kubernetes\": { \"annotations\": { \"kubernetes.io/psp\": \"eks.privileged\" }, \"container_hash\": \"fryckbos/test@sha256:5b098eaf3c7d5b3585eb10cebee63665b6208bea31ef31a3f0856c5ffdda644b\", \"container_image\": \"fryckbos/test:latest\", \"container_name\": \"newrelic-logging\", \"docker_id\": \"134e1daf63761baa15e035b08b7aea04518a0f0e50af4215131a50c6a379a072\", \"host\": \"ip-192-168-17-123.ec2.internal\", \"labels\": { \"app\": \"newrelic-logging\", \"app.kubernetes.io/name\": \"newrelic-logging\", \"controller-revision-hash\": \"84db95db86\", \"pod-template-generation\": \"1\", \"release\": \"nri-bundle\" }, \"namespace_name\": \"nrlogs\", \"pod_id\": \"54556e3e-719c-46b5-af69-020b75d69bf1\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\" }, \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"plugin\": { \"source\": \"kubernetes\", \"type\": \"fluent-bit\", \"version\": \"1.8.1\" }, \"stream\": \"stderr\", \"time\": \"2021-09-14T12:30:49.138824971Z\", \"timestamp\": 1631622649138 } ] Copy Log Record after enabling lowDataMode. [ { \"cluster_name\": \"api-test\", \"container_name\": \"newrelic-logging\", \"namespace_name\": \"nrlogs\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\", \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"stream\": \"stderr\", \"timestamp\": 1631622649138 } ] Copy New Relic Pixie Integration If lowDataMode is enabled, the newrelic-pixie integration performs heavier sampling on Pixie spans and reduces the collection interval from 10 seconds to 15 seconds. lowDataMode settings: HTTP_SPAN_LIMIT: 750 DB_SPAN_LIMIT: 250 COLLECT_INTERVAL_SEC: 15 Copy The default settings for these parameters and others can be found in the newrelic-pixie-integration Github repo. Uninstall Kubernetes integration To uninstall the Kubernetes integration using Helm, run the following command. Replace some-manifest-file.yml with your specific filename. helm uninstall newrelic -n newrelic if kubectl then : kubectl delete -f some-manifest-file.yml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 664.06683,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>Kubernetes</em> integration manually using Helm",
        "sections": "<em>Reduce</em> <em>data</em> <em>ingest</em>",
        "tags": "<em>Kubernetes</em> integration",
        "body": " Our charts support setting an option to <em>reduce</em> the amount of <em>data</em> <em>ingest</em> at the cost of dropping detailed information. To enable it, set global.<em>lowDataMode</em>=true in the nri-bundle chart. <em>lowDataMode</em> affects three specific components of the nri-bundle chart outlined below. Prometheus OpenMetrics"
      },
      "id": "617d5841196a67bb40f7c1de"
    },
    {
      "sections": [
        "Install Auto-telemetry with Pixie",
        "General prerequisites for using Pixie",
        "Setup steps depend on your account status",
        "Install from the beginning of the guided install process",
        "Install from the Configure the HELM command/manifest (yaml) file",
        "Important",
        "Helm method",
        "manifest method",
        "If you link the wrong Pixie and New Relic account"
      ],
      "title": "Install Auto-telemetry with Pixie",
      "type": "docs",
      "tags": [
        "Pixie Auto-telemetry",
        "Service monitoring",
        "Kubernetes",
        "eBPF"
      ],
      "external_id": "bb957763e579e39ef2bbeafeebc46ab3a111bca2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/auto-telemetry-pixie/install-auto-telemetry-pixie/",
      "published_at": "2022-01-08T03:49:26Z",
      "updated_at": "2022-01-08T03:49:26Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To get up and running with Auto-telemetry with Pixie, you start with our guided installation. The guided installation deploys Pixie with New Relic's Kubernetes integration on your cluster. You don't need to do any further configuration or installation to start using Pixie. If you want to install Auto-telemetry with Pixie on multiple clusters, re-run the guided install for each additional cluster. General prerequisites for using Pixie Ready to get started? If you don't already have one, sign up for a New Relic account. It's free, forever! You must be a New Relic full platform user. Other user-related requirements: Users on our New Relic One user model must be assigned to a group that has a role with Pixie-related capabilities. Users on our original user model cannot be Restricted. In addition: Review this Pixie data security overview for actions to take to secure your data. Make sure you have sufficient memory. Pixie requires at least 2Gb of memory on each node in your cluster. More memory may be required for larger clusters. Pixie needs to run in privileged mode. Review the other Pixie technical requirements. Setup steps depend on your account status Use the following table to find out where to start installing Auto-telemetry with Pixie. Where you start the installation depends on whether you already have a New Relic or Pixie account, or both. New Relic Pixie Next steps Start the guided install at the beginning of the process. If you already have both types of accounts, and used the same email address for each of them, click the New Relic icon in the Pixie UI. This brings you to the Configure the HELM command/manifest (yaml) file section of the guided installation. Then, follow the steps. If you're using different email addresses in Pixie and New Relic, create a new account for either Pixie or New Relic to match email addresses across both products. You can also contact New Relic support to manually link your existing New Relic account with your Pixie account. If you follow a link to New Relic from the Pixie UI and do not have a New Relic account, you must first create one. Click the New Relic icon in the Pixie UI, and follow the steps to create a New Relic account. When you do so, your Pixie account is linked to it. Then, continue the guided install process with these steps. Sign up for a free New Relic account. Then, start the guided install at the beginning of the process. Install from the beginning of the guided install process Open our New Relic One guided install. Select the account you want to use for the guided install, and click Continue. Note: if you have a single account, you won't see this option. Select Kubernetes and then continue with step one in the next section. Install from the Configure the HELM command/manifest (yaml) file If you arrived in the guided installation process by following a link from Pixie or from within New Relic, your steps begin here. Select the account and cluster for the install. If needed, select a namespace. Important Currently, Pixie performs best on clusters with up to 100 nodes (exceeding 100 nodes can lead to excessive memory usage and scripts failing to run). Friendly reminder: autoscaling can quickly drive up your node numbers. Click Continue. Select the data you want to gather, observe, and debug, and click Continue. On the Choose install method page, select either Helm or manifest. Helm method Run the provided Helm command on your command line. If you're concerned about the amount of Pixie data you'll ingest, check out strategies for reducing ingest. Helm installs a bundle containing the New Relic infrastructure agent, an integration to gather Prometheus metrics and Kubernetes events, and the Pixie integration. The deployment takes a few minutes to complete. To see the status of the install to the cluster, run kubectl get pods -n newrelic. For general information about installing a Kubernetes integration, see this Helm install info. manifest method Run the provided command in your console, and insert the path to your downloaded manifest. If you're running your Kubernetes cluster in the cloud, see the additional steps in the Kubernetes docs. Click Continue to open the Listening for data page. When you get the message, See your data, click Kubernetes Cluster Explorer to see your cluster. Auto-telemetry with Pixie might restart after installation. This is caused by the auto update feature. If you link the wrong Pixie and New Relic account Contact support to unlink a Pixie account from your New Relic account. Be aware that if you unlink a Pixie account that was created automatically through the guided install, you'll lose access to that Pixie account.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 517.22644,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install <em>Auto</em>-<em>telemetry</em> with <em>Pixie</em>",
        "sections": "Install <em>Auto</em>-<em>telemetry</em> with <em>Pixie</em>",
        "tags": "<em>Pixie</em> <em>Auto</em>-<em>telemetry</em>",
        "body": " the Listening for <em>data</em> page. When you get the message, See your <em>data</em>, click <em>Kubernetes</em> Cluster Explorer to see your cluster. <em>Auto</em>-<em>telemetry</em> with <em>Pixie</em> might restart after installation. This is caused by the <em>auto</em> update feature. If you link the wrong <em>Pixie</em> and New Relic account Contact support to unlink a <em>Pixie</em>"
      },
      "id": "6174ca14e7b9d26ba513cd12"
    },
    {
      "sections": [
        "Auto-telemetry with Pixie for instant Kubernetes observability",
        "Quickly start observing and debugging Kubernetes clusters",
        "Important",
        "Explore your cluster",
        "Tip",
        "Investigate usage spikes using the flame graph",
        "Debug live"
      ],
      "title": "Auto-telemetry with Pixie for instant Kubernetes observability",
      "type": "docs",
      "tags": [
        "Pixie Auto-telemetry",
        "Service monitoring",
        "Kubernetes",
        "eBPF"
      ],
      "external_id": "1dc7474aedbf84c51593c152fc39fda0a46b4f48",
      "image": "https://docs.newrelic.com/static/521c1e907520f4bc8da33ad27ab47289/c1b63/flamegraph.png",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/auto-telemetry-pixie/get-started-auto-telemetry-pixie/",
      "published_at": "2022-01-08T03:10:54Z",
      "updated_at": "2021-12-30T09:39:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "When we say auto-telemetry, we’re not talking about cars — we're talking about instant baseline visibility into your Kubernetes clusters. With the New Relic One integration with Pixie, you get similar data to traditional language agents, but without manually instrumenting your code or redeploying your application. Pixie auto-telemetry is powered by eBPF, a virtual machine-like construct that enables Pixie to seamlessly collect fine-grained telemetry data — service-level metrics, unsampled requests, and more. With one install command, you get deeper insight into your Kubernetes clusters and workloads. No language agents required. Live debugging with Pixie shows a service graph listing the namespaces and the node that are available on the current cluster. Simply put, Auto-telemetry with Pixie offers the quickest option for getting observability into your Kubernetes services. Ready to get started? You'll be able to configure Pixie to suit your environnment after you create a New Relic account (it's free, forever!) and install our Pixie integration. Quickly start observing and debugging Kubernetes clusters Our Pixie integration gives you the best of both worlds: Pixie’s fast and simple Kubernetes observability coupled with New Relic One’s incident correlation, intelligent alerting, and long-term retention. You’ll get visibility into HTTP services using golden signals, HTTP transactions, database transactions, distributed tracing, and JVM metrics. You can operate, debug, and scale your Kubernetes clusters based on the information you learn about how your clusters and services are running. Using the New Relic Explorer, you can see key metrics and events at every level, starting with the cluster, and diving down into namespaces, deployments, and pods. You can quickly spot anomalous behavior, and where it’s happening. And then dive deeper using embedded visualizations of your Pixie data. Quickly identify hot spots with a flame graph display. On the Live debugging with Pixie tab, answer questions like what SQL requests your app is making or which services are talking to each other. This short video (approx. 6:20 minutes) shows how Pixie and Kubernetes work together in New Relic One so you can debug faster with code-level insights: Important Auto-Telemetry with Pixie leverages Community Cloud with Pixie, a separate platform from New Relic One. Use of Community Cloud with Pixie is subject to separate terms of service. Explore your cluster Access the Pixie UI via New Relic's Live debugging with Pixie area of your Kubernetes clusters. The cluster explorer provides a quick overview of the nodes in your cluster, including CPU, memory, and storage, as well as the status of each pod (healthy, warning, or critical). You can also find out what services are running in each container, their latency, throughput, and error rate. For more information about using the cluster explorer, see Navigate the Kubernetes cluster explorer. Note that you cannot log directly into the Pixie UI unless you have created a separate Pixie login. Tip Containers might be listed for up to four hours after they get decommissioned. You can query the Pixie data in New Relic One and create dashboards for at-a-glance monitoring. For more information, see our documentation about the data model and sample queries. Investigate usage spikes using the flame graph Debugging is orders of magnitude easier when you can quickly see what your application is doing. The Pixie flame graph display requires no instrumentation, redeploying, or recompiling. It works for compiled languages like Go, C+, Rust, to name a few. And at a glance, the flame graph tells you what functions your application is spending time on and where you have hot spots. Flame graphs are especially useful for hierarchical resource use, like disk usage and CPU utilization. For more information on how to read a flame graph, see the Pixie flame graph docs. Debug live On the Live debugging with Pixie tab, run PxL scripts — scripts written in Pixie's PxL language — to view live data captured through eBPF. Select the script drop-down and then select a script to run in the tab. (For best results, select a time range that is recent in the time picker.) Scripts enable you to debug: Traffic in multiple formats: HTTP and HTTPs (including encrypted), DNS, Postgres, MySQL, Cassandra, Redis (currently supporting SQL and HTTP in beta). Learn more: Request tracing tutorial. Database request performance. Learn more: Database Query Profiling tutorial. Service maps to learn which services are talking to each other. Learn more: Service Performance tutorial. Network traffic maps to learn which nodes are talking to each other. Learn more: Network Monitoring tutorial. Monitor resource usage by Node and Pod. Learn more: Infra health tutorial.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 313.21466,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Auto</em>-<em>telemetry</em> with <em>Pixie</em> for instant <em>Kubernetes</em> observability",
        "sections": "<em>Auto</em>-<em>telemetry</em> with <em>Pixie</em> for instant <em>Kubernetes</em> observability",
        "tags": "<em>Pixie</em> <em>Auto</em>-<em>telemetry</em>",
        "body": "When we say <em>auto</em>-<em>telemetry</em>, we’re not talking about cars — we&#x27;re talking about instant baseline visibility into your <em>Kubernetes</em> clusters. With the New Relic One integration with <em>Pixie</em>, you get similar <em>data</em> to traditional language agents, but without manually instrumenting your code or redeploying"
      },
      "id": "6174ca7364441f0e385fdea5"
    }
  ],
  "/docs/kubernetes-pixie/auto-telemetry-pixie/pixie-data-security-overview": [
    {
      "sections": [
        "Auto-telemetry with Pixie for instant Kubernetes observability",
        "Quickly start observing and debugging Kubernetes clusters",
        "Important",
        "Explore your cluster",
        "Tip",
        "Investigate usage spikes using the flame graph",
        "Debug live"
      ],
      "title": "Auto-telemetry with Pixie for instant Kubernetes observability",
      "type": "docs",
      "tags": [
        "Pixie Auto-telemetry",
        "Service monitoring",
        "Kubernetes",
        "eBPF"
      ],
      "external_id": "1dc7474aedbf84c51593c152fc39fda0a46b4f48",
      "image": "https://docs.newrelic.com/static/521c1e907520f4bc8da33ad27ab47289/c1b63/flamegraph.png",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/auto-telemetry-pixie/get-started-auto-telemetry-pixie/",
      "published_at": "2022-01-08T03:10:54Z",
      "updated_at": "2021-12-30T09:39:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "When we say auto-telemetry, we’re not talking about cars — we're talking about instant baseline visibility into your Kubernetes clusters. With the New Relic One integration with Pixie, you get similar data to traditional language agents, but without manually instrumenting your code or redeploying your application. Pixie auto-telemetry is powered by eBPF, a virtual machine-like construct that enables Pixie to seamlessly collect fine-grained telemetry data — service-level metrics, unsampled requests, and more. With one install command, you get deeper insight into your Kubernetes clusters and workloads. No language agents required. Live debugging with Pixie shows a service graph listing the namespaces and the node that are available on the current cluster. Simply put, Auto-telemetry with Pixie offers the quickest option for getting observability into your Kubernetes services. Ready to get started? You'll be able to configure Pixie to suit your environnment after you create a New Relic account (it's free, forever!) and install our Pixie integration. Quickly start observing and debugging Kubernetes clusters Our Pixie integration gives you the best of both worlds: Pixie’s fast and simple Kubernetes observability coupled with New Relic One’s incident correlation, intelligent alerting, and long-term retention. You’ll get visibility into HTTP services using golden signals, HTTP transactions, database transactions, distributed tracing, and JVM metrics. You can operate, debug, and scale your Kubernetes clusters based on the information you learn about how your clusters and services are running. Using the New Relic Explorer, you can see key metrics and events at every level, starting with the cluster, and diving down into namespaces, deployments, and pods. You can quickly spot anomalous behavior, and where it’s happening. And then dive deeper using embedded visualizations of your Pixie data. Quickly identify hot spots with a flame graph display. On the Live debugging with Pixie tab, answer questions like what SQL requests your app is making or which services are talking to each other. This short video (approx. 6:20 minutes) shows how Pixie and Kubernetes work together in New Relic One so you can debug faster with code-level insights: Important Auto-Telemetry with Pixie leverages Community Cloud with Pixie, a separate platform from New Relic One. Use of Community Cloud with Pixie is subject to separate terms of service. Explore your cluster Access the Pixie UI via New Relic's Live debugging with Pixie area of your Kubernetes clusters. The cluster explorer provides a quick overview of the nodes in your cluster, including CPU, memory, and storage, as well as the status of each pod (healthy, warning, or critical). You can also find out what services are running in each container, their latency, throughput, and error rate. For more information about using the cluster explorer, see Navigate the Kubernetes cluster explorer. Note that you cannot log directly into the Pixie UI unless you have created a separate Pixie login. Tip Containers might be listed for up to four hours after they get decommissioned. You can query the Pixie data in New Relic One and create dashboards for at-a-glance monitoring. For more information, see our documentation about the data model and sample queries. Investigate usage spikes using the flame graph Debugging is orders of magnitude easier when you can quickly see what your application is doing. The Pixie flame graph display requires no instrumentation, redeploying, or recompiling. It works for compiled languages like Go, C+, Rust, to name a few. And at a glance, the flame graph tells you what functions your application is spending time on and where you have hot spots. Flame graphs are especially useful for hierarchical resource use, like disk usage and CPU utilization. For more information on how to read a flame graph, see the Pixie flame graph docs. Debug live On the Live debugging with Pixie tab, run PxL scripts — scripts written in Pixie's PxL language — to view live data captured through eBPF. Select the script drop-down and then select a script to run in the tab. (For best results, select a time range that is recent in the time picker.) Scripts enable you to debug: Traffic in multiple formats: HTTP and HTTPs (including encrypted), DNS, Postgres, MySQL, Cassandra, Redis (currently supporting SQL and HTTP in beta). Learn more: Request tracing tutorial. Database request performance. Learn more: Database Query Profiling tutorial. Service maps to learn which services are talking to each other. Learn more: Service Performance tutorial. Network traffic maps to learn which nodes are talking to each other. Learn more: Network Monitoring tutorial. Monitor resource usage by Node and Pod. Learn more: Infra health tutorial.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 364.5496,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Auto</em>-<em>telemetry</em> with <em>Pixie</em> for instant <em>Kubernetes</em> observability",
        "sections": "<em>Auto</em>-<em>telemetry</em> with <em>Pixie</em> for instant <em>Kubernetes</em> observability",
        "tags": "<em>Pixie</em> <em>Auto</em>-<em>telemetry</em>",
        "body": " your application. <em>Pixie</em> <em>auto</em>-<em>telemetry</em> is powered by <em>eBPF</em>, a virtual machine-like construct that enables <em>Pixie</em> to seamlessly collect fine-grained <em>telemetry</em> <em>data</em> — <em>service</em>-level metrics, unsampled requests, and more. With one install command, you get deeper insight into your <em>Kubernetes</em> clusters"
      },
      "id": "6174ca7364441f0e385fdea5"
    },
    {
      "sections": [
        "Install Auto-telemetry with Pixie",
        "General prerequisites for using Pixie",
        "Setup steps depend on your account status",
        "Install from the beginning of the guided install process",
        "Install from the Configure the HELM command/manifest (yaml) file",
        "Important",
        "Helm method",
        "manifest method",
        "If you link the wrong Pixie and New Relic account"
      ],
      "title": "Install Auto-telemetry with Pixie",
      "type": "docs",
      "tags": [
        "Pixie Auto-telemetry",
        "Service monitoring",
        "Kubernetes",
        "eBPF"
      ],
      "external_id": "bb957763e579e39ef2bbeafeebc46ab3a111bca2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/auto-telemetry-pixie/install-auto-telemetry-pixie/",
      "published_at": "2022-01-08T03:49:26Z",
      "updated_at": "2022-01-08T03:49:26Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To get up and running with Auto-telemetry with Pixie, you start with our guided installation. The guided installation deploys Pixie with New Relic's Kubernetes integration on your cluster. You don't need to do any further configuration or installation to start using Pixie. If you want to install Auto-telemetry with Pixie on multiple clusters, re-run the guided install for each additional cluster. General prerequisites for using Pixie Ready to get started? If you don't already have one, sign up for a New Relic account. It's free, forever! You must be a New Relic full platform user. Other user-related requirements: Users on our New Relic One user model must be assigned to a group that has a role with Pixie-related capabilities. Users on our original user model cannot be Restricted. In addition: Review this Pixie data security overview for actions to take to secure your data. Make sure you have sufficient memory. Pixie requires at least 2Gb of memory on each node in your cluster. More memory may be required for larger clusters. Pixie needs to run in privileged mode. Review the other Pixie technical requirements. Setup steps depend on your account status Use the following table to find out where to start installing Auto-telemetry with Pixie. Where you start the installation depends on whether you already have a New Relic or Pixie account, or both. New Relic Pixie Next steps Start the guided install at the beginning of the process. If you already have both types of accounts, and used the same email address for each of them, click the New Relic icon in the Pixie UI. This brings you to the Configure the HELM command/manifest (yaml) file section of the guided installation. Then, follow the steps. If you're using different email addresses in Pixie and New Relic, create a new account for either Pixie or New Relic to match email addresses across both products. You can also contact New Relic support to manually link your existing New Relic account with your Pixie account. If you follow a link to New Relic from the Pixie UI and do not have a New Relic account, you must first create one. Click the New Relic icon in the Pixie UI, and follow the steps to create a New Relic account. When you do so, your Pixie account is linked to it. Then, continue the guided install process with these steps. Sign up for a free New Relic account. Then, start the guided install at the beginning of the process. Install from the beginning of the guided install process Open our New Relic One guided install. Select the account you want to use for the guided install, and click Continue. Note: if you have a single account, you won't see this option. Select Kubernetes and then continue with step one in the next section. Install from the Configure the HELM command/manifest (yaml) file If you arrived in the guided installation process by following a link from Pixie or from within New Relic, your steps begin here. Select the account and cluster for the install. If needed, select a namespace. Important Currently, Pixie performs best on clusters with up to 100 nodes (exceeding 100 nodes can lead to excessive memory usage and scripts failing to run). Friendly reminder: autoscaling can quickly drive up your node numbers. Click Continue. Select the data you want to gather, observe, and debug, and click Continue. On the Choose install method page, select either Helm or manifest. Helm method Run the provided Helm command on your command line. If you're concerned about the amount of Pixie data you'll ingest, check out strategies for reducing ingest. Helm installs a bundle containing the New Relic infrastructure agent, an integration to gather Prometheus metrics and Kubernetes events, and the Pixie integration. The deployment takes a few minutes to complete. To see the status of the install to the cluster, run kubectl get pods -n newrelic. For general information about installing a Kubernetes integration, see this Helm install info. manifest method Run the provided command in your console, and insert the path to your downloaded manifest. If you're running your Kubernetes cluster in the cloud, see the additional steps in the Kubernetes docs. Click Continue to open the Listening for data page. When you get the message, See your data, click Kubernetes Cluster Explorer to see your cluster. Auto-telemetry with Pixie might restart after installation. This is caused by the auto update feature. If you link the wrong Pixie and New Relic account Contact support to unlink a Pixie account from your New Relic account. Be aware that if you unlink a Pixie account that was created automatically through the guided install, you'll lose access to that Pixie account.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 295.67285,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install <em>Auto</em>-<em>telemetry</em> with <em>Pixie</em>",
        "sections": "Install <em>Auto</em>-<em>telemetry</em> with <em>Pixie</em>",
        "tags": "<em>Pixie</em> <em>Auto</em>-<em>telemetry</em>",
        "body": " the Listening for <em>data</em> page. When you get the message, See your <em>data</em>, click <em>Kubernetes</em> Cluster Explorer to see your cluster. <em>Auto</em>-<em>telemetry</em> with <em>Pixie</em> might restart after installation. This is caused by the <em>auto</em> update feature. If you link the wrong <em>Pixie</em> and New Relic account Contact support to unlink a <em>Pixie</em>"
      },
      "id": "6174ca14e7b9d26ba513cd12"
    },
    {
      "image": "https://docs.newrelic.com/static/3b7d4417336ed5ef2eb3beb02d4affea/ae694/kubernetes-cluster-explorer.png",
      "url": "https://docs.newrelic.com/whats-new/2021/05/pixie-kubernetes-post-5-26/",
      "sections": [
        "Instant Kubernetes observability with Pixie",
        "Get started today"
      ],
      "published_at": "2022-01-08T07:37:42Z",
      "title": "Instant Kubernetes observability with Pixie",
      "updated_at": "2021-05-28T15:26:33Z",
      "type": "docs",
      "external_id": "f132310e72ece8cefc9e318b433c15cddfafa389",
      "document_type": "nr1_announcement",
      "popularity": 1,
      "body": "We've removed the largest barriers to Kubernetes observability: The time and expertise required to manually instrument application code, by integrating Pixie Auto-Telemetry into our Kubernetes solution. Now, you can get visibility into your Kubernetes clusters and workloads instantly without installing language agents. Pixie data helps you debug faster than ever before, giving you access to everything on-cluster without sampling, then using AI/ML models to send the most relevant subset of your data to the Telemetry Data Platform for correlation with other services, intelligent alerting, and long term storage. Pixie Auto-Telemetry uses eBPF to automatically collect metrics, events, logs, and traces for your Kubernetes clusters, applications, OS, and network layers. Start fast: No code to update, new deployments, or lengthy monitoring standardization processes. Observe everything: Analyze data on-cluster using AI/ML without sampling, storing high-value telemetry data for alerting, correlation, and long term storage. Debug faster with Pixie’s developer-focused workflows, providing code-level insights. Get started today In New Relic One, choose Add more data. Choose Guided install or EU Guided install. Choose Kubernetes, and then follow the on-screen prompts.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 172.01666,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Instant <em>Kubernetes</em> observability with <em>Pixie</em>",
        "sections": "Instant <em>Kubernetes</em> observability with <em>Pixie</em>",
        "body": ", and long term storage. <em>Pixie</em> <em>Auto</em>-<em>Telemetry</em> uses <em>eBPF</em> to automatically collect metrics, events, logs, and traces for your <em>Kubernetes</em> clusters, applications, OS, and network layers. Start fast: No code to update, new deployments, or lengthy <em>monitoring</em> standardization processes. Observe everything: Analyze"
      },
      "id": "60aeed8a28ccbc146b77a392"
    }
  ],
  "/docs/kubernetes-pixie/kubernetes-integration/get-started/introduction-kubernetes-integration": [
    {
      "sections": [
        "Install the Kubernetes integration manually using Helm",
        "Compatibility and requirements",
        "Install Kubernetes integration with Helm",
        "Install with Helm 3 and nri-bundle (recommended)",
        "Installing and configuring nri-bundle with Helm",
        "Install with Helm 2 and nri-bundle (legacy)",
        "Installation instructions for Helm 2",
        "Important",
        "Tip",
        "Helm configuration options",
        "Upgrade using Helm",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data",
        "Reduce data ingest",
        "Prometheus OpenMetrics Integration",
        "New Relic Logging",
        "New Relic Pixie Integration",
        "Uninstall Kubernetes integration"
      ],
      "title": "Install the Kubernetes integration manually using Helm",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "29d42af98d41e7e4e7be86f0150254e132cb6b6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/install-kubernetes-integration-using-helm/",
      "published_at": "2022-01-08T09:49:44Z",
      "updated_at": "2022-01-08T09:49:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Helm is a package manager on top of Kubernetes. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in Kubernetes. To install the integration using Helm, we recommend our Kubernetes automated installer, which will prompt for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our integration as plain manifests rather than a Helm release. See Kubernetes integration: install and configure for more details about how to use our automated installer. Start the installer This page describes in more depth how to install and configure the New Relic integration without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the Kubernetes integration. Our charts are also compatible with Helm 2, but support for it might be removed in the future. To install the Kubernetes integration using Helm, you will need your New Relic account license key and your Kubernetes cluster's name: Find and copy your New Relic license key. Find the name of your cluster with this command: kubectl config current-context Copy Note these values somewhere safe, as you will need them later during the installation process. Install Kubernetes integration with Helm Install with Helm 3 and nri-bundle (recommended) New Relic has several charts for the different components which offer different features for the platform: newrelic-infrastructure: Contains the main Kubernetes integration and the infrastructure agent. This is the core component for the New Relic Kubernetes experience, responsible of reporting most of the data that is surfaced in dashboard and the Kubernetes Cluster Explorer. newrelic-logging: Provides a DaemonSet with New Relic's Fluent Bit output plugin to easily forward your logs to New Relic. nri-kube-events: Collects and reports cluster events (such as kubectl get events) to New Relic. nri-prometheus: New Relic's Prometheus OpenMetrics Integration, automatically scrapes Prometheus endpoints present in the cluser and reports metrics to New Relic. nri-metadata-injection: Sets up a minimal MutatingAdmissionWebhook that injects a couple of environment to containers. These contain metadata about the cluster and New Relic installation and will be later picked up by applications instrumented using APM, allowing to correlate APM and infrastructure data. nri-statsd: New Relic StatsD integration. Additionally, New Relic provides nri-bundle, a chart which pulls a selectable set of the charts mentioned above. nri-bundle can also install Kube State Metrics and Pixie for you if needed. While it is possible to install those charts separately, we strongly recommend using the nri-bundle chart for Kubernetes deployments, as it ensures that values across all the charts are consistent and provides full control over which components are installed, as well as the possibility to configure all of them as Helm dependencies. This is the same chart that is used and referenced by our automated installer. Installing and configuring nri-bundle with Helm Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Please notice and adjust the following flags: global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY: Must be set to a valid License Key for your account. global.cluster=K8S_CLUSTER_NAME: Is used to identify the cluster in the New Relic UI, so should be a descriptive value not used by any other Kubernetes cluster configured in your New Relic account. ksm.enabled=true: Setting this to true will automatically install Kube State Metrics (KSM) for you, which is required for our integration to run. You can set this to false if KSM is already present in your cluster, even if it is on a different namespace. newrelic-infrastructure.privileged=true: Can be set to false to install a trimmed down version of our integration that does not require extra privileges, such as hostPath mounts or running containers as root. Please note that this will disable detailed process collection from the host. For performance reasons, our logging solution still requires hostPath mounts, regardless of the value of this flag. If this is not allowedin your cluster, you will need to disable the logging solution by specifying logging.enabled=false. prometheus.enabled=true: Will deploy our Prometheus OpenMetrics integration, which automatically collects data from prometheus endpoints present in the cluster. webhook.enabled=true: Will install our minimal webhook, which adds environment variables that, in turn, allows linking applications instrumented with APM with infrastructure data. Our chart has a comprehensive set of flags and tunables that can be edited to configure our solution to your particular needs. For a full list of all the flags that can be configured, please check the chart's README and the default values.yaml file. Install the Kubernetes integration by running the customized command without --debug and --dry-run: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl -n newrelic get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node for newrelic-infrastructure, as well as one Deployment and one pod for any other component that you have enabled. Install with Helm 2 and nri-bundle (legacy) Installation instructions for Helm 2 Important Helm 2 has been deprecated and New Relic does not recommend using it for deployments. Instructions in this section are provided for legacy systems only. To install using Helm 2: Ensure that Helm 2, including their cluster-side components (such as Tiller) are properly installed and configured. Please check the official documentation for more details. Set the cluster where you want to install the agent: kubectl config set-cluster DESIRED_CLUSTER Copy To see the available clusters, run kubectl config get-clusters Make sure that kube-state-metrics is installed on your machine: kubectl get deployment --all-namespaces | grep kube-state-metrics Copy If it's not installed, follow the instructions in the kube-state-metrics GitHub repo to install it. Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. This step uses the --dry-run and --debug switches and therefore the agent is not installed. helm install newrelic/newrelic-infrastructure \\ --dry-run \\ --debug \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Install the New Relic Kubernetes integration: helm install newrelic/newrelic-infrastructure \\ --set licenseKey=your_new_relic_license_key \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Tip Note that the --dry-run and --debug switches have been removed. Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node. Helm configuration options When you install or upgrade the Kubernetes integration with Helm using the command line, you can pass your configuration variables with the --set flag. helm install newrelic/newrelic-infrastructure \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=YOUR_CLUSTER_NAME Copy A full list of the configuration parameters can be found in the newrelic-infrastructure chart README Upgrade using Helm To update your Kubernetes integration installed via Helm: Update the local chart repository: helm repo update Copy Update the release by running again the appropriate helm upgrade --install ... command in the section above. Monitor services running on Kubernetes After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages. Reduce data ingest Our charts support setting an option to reduce the amount of data ingest at the cost of dropping detailed information. To enable it, set global.lowDataMode=true in the nri-bundle chart. lowDataMode affects three specific components of the nri-bundle chart outlined below. Prometheus OpenMetrics Integration If lowDataMode is enabled, the following metrics are excluded by default as they are already collected and used by the New Relic Kubernetes Integration. - kube_ - container_ - machine_ - cadvisor_ Copy New Relic Logging If lowDataMode is enabled, Labels and Annotations are set to Off in the Filter section of the fluent-bit.conf file. This means that this detail will be dropped from the container log files which reduces the overall data ingest into New Relic. The following fields are retained: Allowlist_key container_name Allowlist_key namespace_name Allowlist_key pod_name Allowlist_key stream Allowlist_key log Copy Low Data Mode Log Example Complete Log Record [ { \"cluster_name\": \"api-test\", \"kubernetes\": { \"annotations\": { \"kubernetes.io/psp\": \"eks.privileged\" }, \"container_hash\": \"fryckbos/test@sha256:5b098eaf3c7d5b3585eb10cebee63665b6208bea31ef31a3f0856c5ffdda644b\", \"container_image\": \"fryckbos/test:latest\", \"container_name\": \"newrelic-logging\", \"docker_id\": \"134e1daf63761baa15e035b08b7aea04518a0f0e50af4215131a50c6a379a072\", \"host\": \"ip-192-168-17-123.ec2.internal\", \"labels\": { \"app\": \"newrelic-logging\", \"app.kubernetes.io/name\": \"newrelic-logging\", \"controller-revision-hash\": \"84db95db86\", \"pod-template-generation\": \"1\", \"release\": \"nri-bundle\" }, \"namespace_name\": \"nrlogs\", \"pod_id\": \"54556e3e-719c-46b5-af69-020b75d69bf1\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\" }, \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"plugin\": { \"source\": \"kubernetes\", \"type\": \"fluent-bit\", \"version\": \"1.8.1\" }, \"stream\": \"stderr\", \"time\": \"2021-09-14T12:30:49.138824971Z\", \"timestamp\": 1631622649138 } ] Copy Log Record after enabling lowDataMode. [ { \"cluster_name\": \"api-test\", \"container_name\": \"newrelic-logging\", \"namespace_name\": \"nrlogs\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\", \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"stream\": \"stderr\", \"timestamp\": 1631622649138 } ] Copy New Relic Pixie Integration If lowDataMode is enabled, the newrelic-pixie integration performs heavier sampling on Pixie spans and reduces the collection interval from 10 seconds to 15 seconds. lowDataMode settings: HTTP_SPAN_LIMIT: 750 DB_SPAN_LIMIT: 250 COLLECT_INTERVAL_SEC: 15 Copy The default settings for these parameters and others can be found in the newrelic-pixie-integration Github repo. Uninstall Kubernetes integration To uninstall the Kubernetes integration using Helm, run the following command. Replace some-manifest-file.yml with your specific filename. helm uninstall newrelic -n newrelic if kubectl then : kubectl delete -f some-manifest-file.yml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 160.49814,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "sections": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": " installer. <em>Start</em> the installer This page describes in more depth how to install and configure the New Relic <em>integration</em> without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the <em>Kubernetes</em>"
      },
      "id": "617d5841196a67bb40f7c1de"
    },
    {
      "sections": [
        "Introduction to New Relic NerdGraph, our GraphQL API",
        "What is NerdGraph?",
        "Important",
        "Use the GraphiQL explorer",
        "Requirements and endpoints",
        "What can you do with NerdGraph?",
        "NerdGraph terminology",
        "Tips on using the GraphiQL explorer",
        "Query accounts a New Relic user can access",
        "Query user, account, and NRQL in one request"
      ],
      "title": "Introduction to New Relic NerdGraph, our GraphQL API",
      "type": "docs",
      "tags": [
        "APIs",
        "NerdGraph",
        "Get started"
      ],
      "external_id": "e8e96c16cd75f494ebfacb3bc53b4ee9ccf1c727",
      "image": "",
      "url": "https://docs.newrelic.com/docs/apis/nerdgraph/get-started/introduction-new-relic-nerdgraph/",
      "published_at": "2022-01-08T03:28:33Z",
      "updated_at": "2022-01-08T03:28:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "NerdGraph is our GraphQL-format API that lets you query New Relic data and configure some New Relic features. After you sign up for a free New Relic account and install any of our monitoring services, you can get started with NerdGraph. What is NerdGraph? New Relic has several APIs. NerdGraph is the API we recommend for querying New Relic data and for performing some specific configurations (learn more about features). NerdGraph provides a single API interface for returning data from New Relic’s various APIs and microservices. Over time, other configuration capabilities will be added to NerdGraph. Important NerdGraph isn’t used for data ingest. For that, you'd use our data ingest APIs. NerdGraph is built using GraphQL, which is an open source API format that allows you to request exactly the data needed, with no over-fetching or under-fetching. For a lesson in how to use NerdGraph, watch this 7-minute video: Want to watch more video tutorials? Go to the New Relic University’s Intro to NerdGraph. Or see the online course on New Relic APIs. Use the GraphiQL explorer To get started using GraphQL, we recommend playing around with our GraphiQL explorer (GraphiQL is an open source graphical interface for using GraphQL). You can use it to explore our data schema, to read built-in object definitions, and to build and execute queries. To use GraphQL, you’ll need a user-specific New Relic API key called a user key. You can generate one or find an existing one from the GraphiQL explorer’s API key dropdown. To find the GraphiQL explorer: If your New Relic account uses an EU data center, go to api.eu.newrelic.com/graphiql. Otherwise use api.newrelic.com/graphiql. For tips on how to build queries, see Build queries. Requirements and endpoints To use NerdGraph, you need a New Relic user key, which can be generated and accessed from the GraphiQL explorer. The endpoints are: Main endpoint: https://api.newrelic.com/graphql Endpoint for accounts using EU data center: https://api.eu.newrelic.com/graphql To access the endpoint, use the following cURL command: curl -X POST https://api.newrelic.com/graphql \\ -H 'Content-Type: application/json' \\ -H 'API-Key: YOUR_NEW_RELIC_USER_KEY' \\ -d '{ \"query\": \"{ requestContext { userId apiKey } }\" } ' Copy What can you do with NerdGraph? NerdGraph functionality can be broken down into two main categories: Querying New Relic data. You can fetch data for a variety of purposes, including using it in a programmatic workflow, or building a New Relic One app for custom data visualizations. Configuring New Relic features. There are a variety of configurations available and more will be added over time. You can do things like add tags, configure workloads, or customize \"golden metrics.\" You can use NerdGraph to return a wide range of New Relic data but we’ve created some tutorials for common use cases: Topic Tutorials Your monitored entities Get data about entities Understand entity relationships and dependencies (used to build service maps) Query and configure \"golden metrics\" (important entity metrics) Querying data Query using NRQL (our query language) Tags Add and manage tags APM agents APM agent configuration Dashboards Create dashboards Export dashboards to other accounts Export dashboards as files Migrate from Insights Dashboard API to NerdGraph Alerts See all alert-related tutorials Applied Intelligence View and configure topology Workloads View and configure workloads Service levels Configure and manage service levels Manage keys Create and manage keys (license keys used for data ingest, and user keys) Manage data Convert event data to metric data Drop data Distributed tracing Query distributed tracing data Configure Infinite Tracing New Relic One apps Build a New Relic One app Cloud integrations (AWS, Azure, GCP) Configure cloud integrations Partners and resellers Manage subscriptions (only for partners using original pricing model) Data partitions Manage data partitions Date retention Manage data retention NerdGraph terminology The following are terms that originate with GraphQL (the API format NerdGraph uses). Term Definition Queries and mutations There are two classes of GraphQL operations: Queries are basic requests used only to fetch data. These queries are not static, meaning that you can ask for more data or less data, depending on your needs. For each query, you can specify exactly what data you want to retrieve, as long as it is supported by the schema. Mutations are requests that perform an action, such as creating a resource or changing configuration. Mutations require the keyword mutation, as well as the name of the mutation. Type Data in GraphQL is organized into types. Types can be scalars (like strings, numbers, or booleans) or object types. An object type is a custom type made up of a collection of fields. For example, an object type called User may represent a user in a system. Field A field represents a piece of information on an object type that can be queried. Fields can be scalars, lists, or objects. For example, a User object type could have a string field called name. Interface An interface is an abstract type that represents a collection of common fields that other object types can implement. Tips on using the GraphiQL explorer You can make queries with the NerdGraph GraphiQL explorer. The explorer provides built-in schema definitions and features, including auto-complete and query validation. Query accounts a New Relic user can access You can query for the name of an account that an actor (a New Relic authorized user) has access to: query { actor { account(id: YOUR_ACCOUNT_ID) { name } } } Copy The response will mirror the query structure you defined in the request, making it easy to ask for the specific data that you want. { \"data\": { \"actor\": { \"account\": { \"name\": \"Data Nerd\" } } } } Copy Query user, account, and NRQL in one request The graph structure shows its capabilities when queries become more complex. For example, you can query for user information, account information, and make a NRQL query with one request. With REST API, this would take three different requests to three different endpoints. query { actor { account(id: YOUR_ACCOUNT_ID) { name nrql(query: \"SELECT * FROM Transaction\") { results } } user { name id } } } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 123.588165,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Get</em> <em>started</em>",
        "body": "NerdGraph is our GraphQL-format API that lets you query New Relic data and configure some New Relic features. After you sign up for a free New Relic account and install any of our monitoring services, you can <em>get</em> <em>started</em> with NerdGraph. What is NerdGraph? New Relic has several APIs. NerdGraph"
      },
      "id": "6043ff97196a67d0a0960f55"
    },
    {
      "sections": [
        "Kubernetes integration: compatibility and requirements",
        "Compatibility",
        "EOL NOTICE",
        "Requirements",
        "Install using Helm"
      ],
      "title": "Kubernetes integration: compatibility and requirements",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "5d37ab2bb0cc980c42a41a360eb61cc1e45685c4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements/",
      "published_at": "2022-01-08T09:48:25Z",
      "updated_at": "2021-11-13T22:08:46Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our integration. Compatibility EOL NOTICE We're discontinuing support for several capabilities in December 2021, including Kubernetes instrumentation support for versions v1.10 to v1.15. For more details, including how you can easily prepare for this transition, see our Explorers Hub post. Our Kubernetes integration is compatible with the following versions, depending on the installation mode: Install mode or feature Kubernetes versions Kubernetes cluster Currently tested with versions 1.16 to 1.22 Kubernetes cluster GKE Currently tested with versions 1.17 to 1.19 Kubernetes cluster EKS, EKS-Anywhere, EKS-Fargate Compatible with version 1.16 or higher Kubernetes cluster AKS Compatible with version 1.16 or higher Kubernetes cluster OpenShift Currently tested with versions 4.6 Kubernetes cluster VMware Tanzu Compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10 Control plane monitoring Compatible with version 1.16 or higher Service monitoring Compatible with version 1.16 or higher Requirements The New Relic Kubernetes integration has the following requirements: A New Relic account. Don't have one? Sign up for free. No credit card required. Linux distribution compatible with New Relic infrastructure agent. kube-state-metrics version 1.9.8 running on the cluster. When using CRI-O as the container runtime, the processes inside containers are not reported. Performance data is collected at the container level. Install using Helm For detailed instructions about how to install our integration using Helm, see Manual install using Helm.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 123.332405,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: compatibility and requirements",
        "sections": "<em>Kubernetes</em> <em>integration</em>: compatibility and requirements",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "New Relic&#x27;s <em>Kubernetes</em> <em>integration</em> can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our <em>integration</em>. Compatibility EOL NOTICE We&#x27;re discontinuing support for several capabilities in December"
      },
      "id": "617d54de64441f0879fbebed"
    }
  ],
  "/docs/kubernetes-pixie/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements": [
    {
      "sections": [
        "Install the Kubernetes integration manually using Helm",
        "Compatibility and requirements",
        "Install Kubernetes integration with Helm",
        "Install with Helm 3 and nri-bundle (recommended)",
        "Installing and configuring nri-bundle with Helm",
        "Install with Helm 2 and nri-bundle (legacy)",
        "Installation instructions for Helm 2",
        "Important",
        "Tip",
        "Helm configuration options",
        "Upgrade using Helm",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data",
        "Reduce data ingest",
        "Prometheus OpenMetrics Integration",
        "New Relic Logging",
        "New Relic Pixie Integration",
        "Uninstall Kubernetes integration"
      ],
      "title": "Install the Kubernetes integration manually using Helm",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "29d42af98d41e7e4e7be86f0150254e132cb6b6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/install-kubernetes-integration-using-helm/",
      "published_at": "2022-01-08T09:49:44Z",
      "updated_at": "2022-01-08T09:49:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Helm is a package manager on top of Kubernetes. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in Kubernetes. To install the integration using Helm, we recommend our Kubernetes automated installer, which will prompt for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our integration as plain manifests rather than a Helm release. See Kubernetes integration: install and configure for more details about how to use our automated installer. Start the installer This page describes in more depth how to install and configure the New Relic integration without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the Kubernetes integration. Our charts are also compatible with Helm 2, but support for it might be removed in the future. To install the Kubernetes integration using Helm, you will need your New Relic account license key and your Kubernetes cluster's name: Find and copy your New Relic license key. Find the name of your cluster with this command: kubectl config current-context Copy Note these values somewhere safe, as you will need them later during the installation process. Install Kubernetes integration with Helm Install with Helm 3 and nri-bundle (recommended) New Relic has several charts for the different components which offer different features for the platform: newrelic-infrastructure: Contains the main Kubernetes integration and the infrastructure agent. This is the core component for the New Relic Kubernetes experience, responsible of reporting most of the data that is surfaced in dashboard and the Kubernetes Cluster Explorer. newrelic-logging: Provides a DaemonSet with New Relic's Fluent Bit output plugin to easily forward your logs to New Relic. nri-kube-events: Collects and reports cluster events (such as kubectl get events) to New Relic. nri-prometheus: New Relic's Prometheus OpenMetrics Integration, automatically scrapes Prometheus endpoints present in the cluser and reports metrics to New Relic. nri-metadata-injection: Sets up a minimal MutatingAdmissionWebhook that injects a couple of environment to containers. These contain metadata about the cluster and New Relic installation and will be later picked up by applications instrumented using APM, allowing to correlate APM and infrastructure data. nri-statsd: New Relic StatsD integration. Additionally, New Relic provides nri-bundle, a chart which pulls a selectable set of the charts mentioned above. nri-bundle can also install Kube State Metrics and Pixie for you if needed. While it is possible to install those charts separately, we strongly recommend using the nri-bundle chart for Kubernetes deployments, as it ensures that values across all the charts are consistent and provides full control over which components are installed, as well as the possibility to configure all of them as Helm dependencies. This is the same chart that is used and referenced by our automated installer. Installing and configuring nri-bundle with Helm Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Please notice and adjust the following flags: global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY: Must be set to a valid License Key for your account. global.cluster=K8S_CLUSTER_NAME: Is used to identify the cluster in the New Relic UI, so should be a descriptive value not used by any other Kubernetes cluster configured in your New Relic account. ksm.enabled=true: Setting this to true will automatically install Kube State Metrics (KSM) for you, which is required for our integration to run. You can set this to false if KSM is already present in your cluster, even if it is on a different namespace. newrelic-infrastructure.privileged=true: Can be set to false to install a trimmed down version of our integration that does not require extra privileges, such as hostPath mounts or running containers as root. Please note that this will disable detailed process collection from the host. For performance reasons, our logging solution still requires hostPath mounts, regardless of the value of this flag. If this is not allowedin your cluster, you will need to disable the logging solution by specifying logging.enabled=false. prometheus.enabled=true: Will deploy our Prometheus OpenMetrics integration, which automatically collects data from prometheus endpoints present in the cluster. webhook.enabled=true: Will install our minimal webhook, which adds environment variables that, in turn, allows linking applications instrumented with APM with infrastructure data. Our chart has a comprehensive set of flags and tunables that can be edited to configure our solution to your particular needs. For a full list of all the flags that can be configured, please check the chart's README and the default values.yaml file. Install the Kubernetes integration by running the customized command without --debug and --dry-run: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl -n newrelic get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node for newrelic-infrastructure, as well as one Deployment and one pod for any other component that you have enabled. Install with Helm 2 and nri-bundle (legacy) Installation instructions for Helm 2 Important Helm 2 has been deprecated and New Relic does not recommend using it for deployments. Instructions in this section are provided for legacy systems only. To install using Helm 2: Ensure that Helm 2, including their cluster-side components (such as Tiller) are properly installed and configured. Please check the official documentation for more details. Set the cluster where you want to install the agent: kubectl config set-cluster DESIRED_CLUSTER Copy To see the available clusters, run kubectl config get-clusters Make sure that kube-state-metrics is installed on your machine: kubectl get deployment --all-namespaces | grep kube-state-metrics Copy If it's not installed, follow the instructions in the kube-state-metrics GitHub repo to install it. Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. This step uses the --dry-run and --debug switches and therefore the agent is not installed. helm install newrelic/newrelic-infrastructure \\ --dry-run \\ --debug \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Install the New Relic Kubernetes integration: helm install newrelic/newrelic-infrastructure \\ --set licenseKey=your_new_relic_license_key \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Tip Note that the --dry-run and --debug switches have been removed. Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node. Helm configuration options When you install or upgrade the Kubernetes integration with Helm using the command line, you can pass your configuration variables with the --set flag. helm install newrelic/newrelic-infrastructure \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=YOUR_CLUSTER_NAME Copy A full list of the configuration parameters can be found in the newrelic-infrastructure chart README Upgrade using Helm To update your Kubernetes integration installed via Helm: Update the local chart repository: helm repo update Copy Update the release by running again the appropriate helm upgrade --install ... command in the section above. Monitor services running on Kubernetes After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages. Reduce data ingest Our charts support setting an option to reduce the amount of data ingest at the cost of dropping detailed information. To enable it, set global.lowDataMode=true in the nri-bundle chart. lowDataMode affects three specific components of the nri-bundle chart outlined below. Prometheus OpenMetrics Integration If lowDataMode is enabled, the following metrics are excluded by default as they are already collected and used by the New Relic Kubernetes Integration. - kube_ - container_ - machine_ - cadvisor_ Copy New Relic Logging If lowDataMode is enabled, Labels and Annotations are set to Off in the Filter section of the fluent-bit.conf file. This means that this detail will be dropped from the container log files which reduces the overall data ingest into New Relic. The following fields are retained: Allowlist_key container_name Allowlist_key namespace_name Allowlist_key pod_name Allowlist_key stream Allowlist_key log Copy Low Data Mode Log Example Complete Log Record [ { \"cluster_name\": \"api-test\", \"kubernetes\": { \"annotations\": { \"kubernetes.io/psp\": \"eks.privileged\" }, \"container_hash\": \"fryckbos/test@sha256:5b098eaf3c7d5b3585eb10cebee63665b6208bea31ef31a3f0856c5ffdda644b\", \"container_image\": \"fryckbos/test:latest\", \"container_name\": \"newrelic-logging\", \"docker_id\": \"134e1daf63761baa15e035b08b7aea04518a0f0e50af4215131a50c6a379a072\", \"host\": \"ip-192-168-17-123.ec2.internal\", \"labels\": { \"app\": \"newrelic-logging\", \"app.kubernetes.io/name\": \"newrelic-logging\", \"controller-revision-hash\": \"84db95db86\", \"pod-template-generation\": \"1\", \"release\": \"nri-bundle\" }, \"namespace_name\": \"nrlogs\", \"pod_id\": \"54556e3e-719c-46b5-af69-020b75d69bf1\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\" }, \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"plugin\": { \"source\": \"kubernetes\", \"type\": \"fluent-bit\", \"version\": \"1.8.1\" }, \"stream\": \"stderr\", \"time\": \"2021-09-14T12:30:49.138824971Z\", \"timestamp\": 1631622649138 } ] Copy Log Record after enabling lowDataMode. [ { \"cluster_name\": \"api-test\", \"container_name\": \"newrelic-logging\", \"namespace_name\": \"nrlogs\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\", \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"stream\": \"stderr\", \"timestamp\": 1631622649138 } ] Copy New Relic Pixie Integration If lowDataMode is enabled, the newrelic-pixie integration performs heavier sampling on Pixie spans and reduces the collection interval from 10 seconds to 15 seconds. lowDataMode settings: HTTP_SPAN_LIMIT: 750 DB_SPAN_LIMIT: 250 COLLECT_INTERVAL_SEC: 15 Copy The default settings for these parameters and others can be found in the newrelic-pixie-integration Github repo. Uninstall Kubernetes integration To uninstall the Kubernetes integration using Helm, run the following command. Replace some-manifest-file.yml with your specific filename. helm uninstall newrelic -n newrelic if kubectl then : kubectl delete -f some-manifest-file.yml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 160.49814,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "sections": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": " installer. <em>Start</em> the installer This page describes in more depth how to install and configure the New Relic <em>integration</em> without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the <em>Kubernetes</em>"
      },
      "id": "617d5841196a67bb40f7c1de"
    },
    {
      "sections": [
        "Introduction to the Kubernetes integration",
        "Get started: Install the Kubernetes integration",
        "Tip",
        "Why it matters",
        "Navigate all your Kubernetes events",
        "Bring your cluster logs to New Relic",
        "Check the source code"
      ],
      "title": "Introduction to the Kubernetes integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "81c1ba10f4c79655db595bd6423a7b03720af947",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/get-started/introduction-kubernetes-integration/",
      "published_at": "2022-01-09T01:41:55Z",
      "updated_at": "2021-11-25T01:46:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration gives you full observability into the health and performance of your environment, no matter whether you run Kubernetes on-premises or in the cloud. With our cluster explorer, you can cut through layers of complexity to see how your cluster is performing, from the heights of the control plane down to applications running on a single pod. one.newrelic.com > Kubernetes cluster explorer: The cluster explorer is our powerful, fully visual answer to the challenges associated with running Kubernetes at a large scale. You can see the power of the Kubernetes integration in the cluster explorer, where the full picture of a cluster is made available on a single screen: nodes and pods are visualized according to their health and performance, with pending and alerting nodes in the innermost circles. Predefined alert conditions help you troubleshoot issues right from the start. Clicking each node reveals its status and how each app is performing. You'll be able to configure Kubernetes to suit your environnment after you create a New Relic account (it's free, forever!) and install our Kubernetes integration. Get started: Install the Kubernetes integration We have an automated installer to help you with many types of installations: servers, virtual machines, and unprivileged environments. It can also help you with installations in managed services or platforms, but you'll need to review a few preliminary notes before getting started. Our automated installer will generate either a helm command or a set of plain manifests for you to install. Our automated installer: Allows users to select the cluster name and namespace for the installation. Allows users to selectively enable or disable bundling of Kube-state-metrics, a dependency of the Kubernetes integration. Allows users to seamlessly install our other products related to Kubernetes such as: Kubernetes events monitoring In-cluster prometheus services monitoring Service instrumentation without code changes using Pixie Automatically fills the required properties with the license keys the integration needs to work. Read the install docs Start the installer Tip If your New Relic account is in the EU region, access the automated installer from one.eu.newrelic.com. Why it matters Governing the complexity of Kubernetes can be challenging; there's so much going on at any given moment, with containers being created and deleted in a matter of minutes, applications crashing, and resources being consumed unexpectedly. Our integration helps you navigate Kubernetes abstractions across on-premises, cloud, and hybrid deployments. In New Relic, you can build your own charts and query all your Kubernetes data, which our integration collects by instrumenting the container orchestration layer. This gives you additional insight into nodes, namespaces, deployments, replica sets, pods, and containers. one.newrelic.com > Dashboards: Using the query builder you can turn any query on Kubernetes data to clear visuals. With the Kubernetes integration you can also: Link your APM data to Kubernetes to measure the performance of your web and mobile applications, with metrics such as request rate, throughput, error rate, and availability. Monitor services running on Kubernetes, such as Apache, NGINX, Cassandra, and many more (see our tutorial for monitoring Redis on Kubernetes). Create new alert policies and alert conditions based on your Kubernetes data, or extend the predefined alert conditions. These features are in addition to the data New Relic already reports for containerized processes running on instrumented hosts. Navigate all your Kubernetes events The Kubernetes events integration, which is installed separately, watches for events happening in your Kubernetes clusters and sends those events to New Relic. Events data is then visualized in the cluster explorer. To set it up, check the Kubernetes events box in step 3 of our install wizard, or follow the instructions. one.newrelic.com > Kubernetes cluster explorer > Events: Browse and filter all your Kubernetes events, and dig into application logs and infrastructure data. Bring your cluster logs to New Relic Our Kubernetes plugin for log monitoring can collect all your cluster's logs and send them to our platform, so that you can set up new alerts and charts. To set it up, check the Log data box in step 3 of our install wizard. For detailed information, see our log forwarding documentation for Kubernetes. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or you can create your own fork and build it. For more information, see the README.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 127.09575,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to the <em>Kubernetes</em> <em>integration</em>",
        "sections": "<em>Get</em> <em>started</em>: Install the <em>Kubernetes</em> <em>integration</em>",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": " from the <em>start</em>. Clicking each node reveals its status and how each app is performing. You&#x27;ll be able to configure <em>Kubernetes</em> to suit your environnment after you create a New Relic account (it&#x27;s free, forever!) and install our <em>Kubernetes</em> <em>integration</em>. <em>Get</em> <em>started</em>: Install the <em>Kubernetes</em> <em>integration</em> We"
      },
      "id": "6174cb5c28ccbcde75c6c73f"
    },
    {
      "sections": [
        "Introduction to New Relic NerdGraph, our GraphQL API",
        "What is NerdGraph?",
        "Important",
        "Use the GraphiQL explorer",
        "Requirements and endpoints",
        "What can you do with NerdGraph?",
        "NerdGraph terminology",
        "Tips on using the GraphiQL explorer",
        "Query accounts a New Relic user can access",
        "Query user, account, and NRQL in one request"
      ],
      "title": "Introduction to New Relic NerdGraph, our GraphQL API",
      "type": "docs",
      "tags": [
        "APIs",
        "NerdGraph",
        "Get started"
      ],
      "external_id": "e8e96c16cd75f494ebfacb3bc53b4ee9ccf1c727",
      "image": "",
      "url": "https://docs.newrelic.com/docs/apis/nerdgraph/get-started/introduction-new-relic-nerdgraph/",
      "published_at": "2022-01-08T03:28:33Z",
      "updated_at": "2022-01-08T03:28:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "NerdGraph is our GraphQL-format API that lets you query New Relic data and configure some New Relic features. After you sign up for a free New Relic account and install any of our monitoring services, you can get started with NerdGraph. What is NerdGraph? New Relic has several APIs. NerdGraph is the API we recommend for querying New Relic data and for performing some specific configurations (learn more about features). NerdGraph provides a single API interface for returning data from New Relic’s various APIs and microservices. Over time, other configuration capabilities will be added to NerdGraph. Important NerdGraph isn’t used for data ingest. For that, you'd use our data ingest APIs. NerdGraph is built using GraphQL, which is an open source API format that allows you to request exactly the data needed, with no over-fetching or under-fetching. For a lesson in how to use NerdGraph, watch this 7-minute video: Want to watch more video tutorials? Go to the New Relic University’s Intro to NerdGraph. Or see the online course on New Relic APIs. Use the GraphiQL explorer To get started using GraphQL, we recommend playing around with our GraphiQL explorer (GraphiQL is an open source graphical interface for using GraphQL). You can use it to explore our data schema, to read built-in object definitions, and to build and execute queries. To use GraphQL, you’ll need a user-specific New Relic API key called a user key. You can generate one or find an existing one from the GraphiQL explorer’s API key dropdown. To find the GraphiQL explorer: If your New Relic account uses an EU data center, go to api.eu.newrelic.com/graphiql. Otherwise use api.newrelic.com/graphiql. For tips on how to build queries, see Build queries. Requirements and endpoints To use NerdGraph, you need a New Relic user key, which can be generated and accessed from the GraphiQL explorer. The endpoints are: Main endpoint: https://api.newrelic.com/graphql Endpoint for accounts using EU data center: https://api.eu.newrelic.com/graphql To access the endpoint, use the following cURL command: curl -X POST https://api.newrelic.com/graphql \\ -H 'Content-Type: application/json' \\ -H 'API-Key: YOUR_NEW_RELIC_USER_KEY' \\ -d '{ \"query\": \"{ requestContext { userId apiKey } }\" } ' Copy What can you do with NerdGraph? NerdGraph functionality can be broken down into two main categories: Querying New Relic data. You can fetch data for a variety of purposes, including using it in a programmatic workflow, or building a New Relic One app for custom data visualizations. Configuring New Relic features. There are a variety of configurations available and more will be added over time. You can do things like add tags, configure workloads, or customize \"golden metrics.\" You can use NerdGraph to return a wide range of New Relic data but we’ve created some tutorials for common use cases: Topic Tutorials Your monitored entities Get data about entities Understand entity relationships and dependencies (used to build service maps) Query and configure \"golden metrics\" (important entity metrics) Querying data Query using NRQL (our query language) Tags Add and manage tags APM agents APM agent configuration Dashboards Create dashboards Export dashboards to other accounts Export dashboards as files Migrate from Insights Dashboard API to NerdGraph Alerts See all alert-related tutorials Applied Intelligence View and configure topology Workloads View and configure workloads Service levels Configure and manage service levels Manage keys Create and manage keys (license keys used for data ingest, and user keys) Manage data Convert event data to metric data Drop data Distributed tracing Query distributed tracing data Configure Infinite Tracing New Relic One apps Build a New Relic One app Cloud integrations (AWS, Azure, GCP) Configure cloud integrations Partners and resellers Manage subscriptions (only for partners using original pricing model) Data partitions Manage data partitions Date retention Manage data retention NerdGraph terminology The following are terms that originate with GraphQL (the API format NerdGraph uses). Term Definition Queries and mutations There are two classes of GraphQL operations: Queries are basic requests used only to fetch data. These queries are not static, meaning that you can ask for more data or less data, depending on your needs. For each query, you can specify exactly what data you want to retrieve, as long as it is supported by the schema. Mutations are requests that perform an action, such as creating a resource or changing configuration. Mutations require the keyword mutation, as well as the name of the mutation. Type Data in GraphQL is organized into types. Types can be scalars (like strings, numbers, or booleans) or object types. An object type is a custom type made up of a collection of fields. For example, an object type called User may represent a user in a system. Field A field represents a piece of information on an object type that can be queried. Fields can be scalars, lists, or objects. For example, a User object type could have a string field called name. Interface An interface is an abstract type that represents a collection of common fields that other object types can implement. Tips on using the GraphiQL explorer You can make queries with the NerdGraph GraphiQL explorer. The explorer provides built-in schema definitions and features, including auto-complete and query validation. Query accounts a New Relic user can access You can query for the name of an account that an actor (a New Relic authorized user) has access to: query { actor { account(id: YOUR_ACCOUNT_ID) { name } } } Copy The response will mirror the query structure you defined in the request, making it easy to ask for the specific data that you want. { \"data\": { \"actor\": { \"account\": { \"name\": \"Data Nerd\" } } } } Copy Query user, account, and NRQL in one request The graph structure shows its capabilities when queries become more complex. For example, you can query for user information, account information, and make a NRQL query with one request. With REST API, this would take three different requests to three different endpoints. query { actor { account(id: YOUR_ACCOUNT_ID) { name nrql(query: \"SELECT * FROM Transaction\") { results } } user { name id } } } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 123.588165,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Get</em> <em>started</em>",
        "body": "NerdGraph is our GraphQL-format API that lets you query New Relic data and configure some New Relic features. After you sign up for a free New Relic account and install any of our monitoring services, you can <em>get</em> <em>started</em> with NerdGraph. What is NerdGraph? New Relic has several APIs. NerdGraph"
      },
      "id": "6043ff97196a67d0a0960f55"
    }
  ],
  "/docs/kubernetes-pixie/kubernetes-integration/installation/configure-control-plane-monitoring": [
    {
      "sections": [
        "Install the Kubernetes integration manually using Helm",
        "Compatibility and requirements",
        "Install Kubernetes integration with Helm",
        "Install with Helm 3 and nri-bundle (recommended)",
        "Installing and configuring nri-bundle with Helm",
        "Install with Helm 2 and nri-bundle (legacy)",
        "Installation instructions for Helm 2",
        "Important",
        "Tip",
        "Helm configuration options",
        "Upgrade using Helm",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data",
        "Reduce data ingest",
        "Prometheus OpenMetrics Integration",
        "New Relic Logging",
        "New Relic Pixie Integration",
        "Uninstall Kubernetes integration"
      ],
      "title": "Install the Kubernetes integration manually using Helm",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "29d42af98d41e7e4e7be86f0150254e132cb6b6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/install-kubernetes-integration-using-helm/",
      "published_at": "2022-01-08T09:49:44Z",
      "updated_at": "2022-01-08T09:49:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Helm is a package manager on top of Kubernetes. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in Kubernetes. To install the integration using Helm, we recommend our Kubernetes automated installer, which will prompt for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our integration as plain manifests rather than a Helm release. See Kubernetes integration: install and configure for more details about how to use our automated installer. Start the installer This page describes in more depth how to install and configure the New Relic integration without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the Kubernetes integration. Our charts are also compatible with Helm 2, but support for it might be removed in the future. To install the Kubernetes integration using Helm, you will need your New Relic account license key and your Kubernetes cluster's name: Find and copy your New Relic license key. Find the name of your cluster with this command: kubectl config current-context Copy Note these values somewhere safe, as you will need them later during the installation process. Install Kubernetes integration with Helm Install with Helm 3 and nri-bundle (recommended) New Relic has several charts for the different components which offer different features for the platform: newrelic-infrastructure: Contains the main Kubernetes integration and the infrastructure agent. This is the core component for the New Relic Kubernetes experience, responsible of reporting most of the data that is surfaced in dashboard and the Kubernetes Cluster Explorer. newrelic-logging: Provides a DaemonSet with New Relic's Fluent Bit output plugin to easily forward your logs to New Relic. nri-kube-events: Collects and reports cluster events (such as kubectl get events) to New Relic. nri-prometheus: New Relic's Prometheus OpenMetrics Integration, automatically scrapes Prometheus endpoints present in the cluser and reports metrics to New Relic. nri-metadata-injection: Sets up a minimal MutatingAdmissionWebhook that injects a couple of environment to containers. These contain metadata about the cluster and New Relic installation and will be later picked up by applications instrumented using APM, allowing to correlate APM and infrastructure data. nri-statsd: New Relic StatsD integration. Additionally, New Relic provides nri-bundle, a chart which pulls a selectable set of the charts mentioned above. nri-bundle can also install Kube State Metrics and Pixie for you if needed. While it is possible to install those charts separately, we strongly recommend using the nri-bundle chart for Kubernetes deployments, as it ensures that values across all the charts are consistent and provides full control over which components are installed, as well as the possibility to configure all of them as Helm dependencies. This is the same chart that is used and referenced by our automated installer. Installing and configuring nri-bundle with Helm Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Please notice and adjust the following flags: global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY: Must be set to a valid License Key for your account. global.cluster=K8S_CLUSTER_NAME: Is used to identify the cluster in the New Relic UI, so should be a descriptive value not used by any other Kubernetes cluster configured in your New Relic account. ksm.enabled=true: Setting this to true will automatically install Kube State Metrics (KSM) for you, which is required for our integration to run. You can set this to false if KSM is already present in your cluster, even if it is on a different namespace. newrelic-infrastructure.privileged=true: Can be set to false to install a trimmed down version of our integration that does not require extra privileges, such as hostPath mounts or running containers as root. Please note that this will disable detailed process collection from the host. For performance reasons, our logging solution still requires hostPath mounts, regardless of the value of this flag. If this is not allowedin your cluster, you will need to disable the logging solution by specifying logging.enabled=false. prometheus.enabled=true: Will deploy our Prometheus OpenMetrics integration, which automatically collects data from prometheus endpoints present in the cluster. webhook.enabled=true: Will install our minimal webhook, which adds environment variables that, in turn, allows linking applications instrumented with APM with infrastructure data. Our chart has a comprehensive set of flags and tunables that can be edited to configure our solution to your particular needs. For a full list of all the flags that can be configured, please check the chart's README and the default values.yaml file. Install the Kubernetes integration by running the customized command without --debug and --dry-run: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl -n newrelic get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node for newrelic-infrastructure, as well as one Deployment and one pod for any other component that you have enabled. Install with Helm 2 and nri-bundle (legacy) Installation instructions for Helm 2 Important Helm 2 has been deprecated and New Relic does not recommend using it for deployments. Instructions in this section are provided for legacy systems only. To install using Helm 2: Ensure that Helm 2, including their cluster-side components (such as Tiller) are properly installed and configured. Please check the official documentation for more details. Set the cluster where you want to install the agent: kubectl config set-cluster DESIRED_CLUSTER Copy To see the available clusters, run kubectl config get-clusters Make sure that kube-state-metrics is installed on your machine: kubectl get deployment --all-namespaces | grep kube-state-metrics Copy If it's not installed, follow the instructions in the kube-state-metrics GitHub repo to install it. Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. This step uses the --dry-run and --debug switches and therefore the agent is not installed. helm install newrelic/newrelic-infrastructure \\ --dry-run \\ --debug \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Install the New Relic Kubernetes integration: helm install newrelic/newrelic-infrastructure \\ --set licenseKey=your_new_relic_license_key \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Tip Note that the --dry-run and --debug switches have been removed. Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node. Helm configuration options When you install or upgrade the Kubernetes integration with Helm using the command line, you can pass your configuration variables with the --set flag. helm install newrelic/newrelic-infrastructure \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=YOUR_CLUSTER_NAME Copy A full list of the configuration parameters can be found in the newrelic-infrastructure chart README Upgrade using Helm To update your Kubernetes integration installed via Helm: Update the local chart repository: helm repo update Copy Update the release by running again the appropriate helm upgrade --install ... command in the section above. Monitor services running on Kubernetes After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages. Reduce data ingest Our charts support setting an option to reduce the amount of data ingest at the cost of dropping detailed information. To enable it, set global.lowDataMode=true in the nri-bundle chart. lowDataMode affects three specific components of the nri-bundle chart outlined below. Prometheus OpenMetrics Integration If lowDataMode is enabled, the following metrics are excluded by default as they are already collected and used by the New Relic Kubernetes Integration. - kube_ - container_ - machine_ - cadvisor_ Copy New Relic Logging If lowDataMode is enabled, Labels and Annotations are set to Off in the Filter section of the fluent-bit.conf file. This means that this detail will be dropped from the container log files which reduces the overall data ingest into New Relic. The following fields are retained: Allowlist_key container_name Allowlist_key namespace_name Allowlist_key pod_name Allowlist_key stream Allowlist_key log Copy Low Data Mode Log Example Complete Log Record [ { \"cluster_name\": \"api-test\", \"kubernetes\": { \"annotations\": { \"kubernetes.io/psp\": \"eks.privileged\" }, \"container_hash\": \"fryckbos/test@sha256:5b098eaf3c7d5b3585eb10cebee63665b6208bea31ef31a3f0856c5ffdda644b\", \"container_image\": \"fryckbos/test:latest\", \"container_name\": \"newrelic-logging\", \"docker_id\": \"134e1daf63761baa15e035b08b7aea04518a0f0e50af4215131a50c6a379a072\", \"host\": \"ip-192-168-17-123.ec2.internal\", \"labels\": { \"app\": \"newrelic-logging\", \"app.kubernetes.io/name\": \"newrelic-logging\", \"controller-revision-hash\": \"84db95db86\", \"pod-template-generation\": \"1\", \"release\": \"nri-bundle\" }, \"namespace_name\": \"nrlogs\", \"pod_id\": \"54556e3e-719c-46b5-af69-020b75d69bf1\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\" }, \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"plugin\": { \"source\": \"kubernetes\", \"type\": \"fluent-bit\", \"version\": \"1.8.1\" }, \"stream\": \"stderr\", \"time\": \"2021-09-14T12:30:49.138824971Z\", \"timestamp\": 1631622649138 } ] Copy Log Record after enabling lowDataMode. [ { \"cluster_name\": \"api-test\", \"container_name\": \"newrelic-logging\", \"namespace_name\": \"nrlogs\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\", \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"stream\": \"stderr\", \"timestamp\": 1631622649138 } ] Copy New Relic Pixie Integration If lowDataMode is enabled, the newrelic-pixie integration performs heavier sampling on Pixie spans and reduces the collection interval from 10 seconds to 15 seconds. lowDataMode settings: HTTP_SPAN_LIMIT: 750 DB_SPAN_LIMIT: 250 COLLECT_INTERVAL_SEC: 15 Copy The default settings for these parameters and others can be found in the newrelic-pixie-integration Github repo. Uninstall Kubernetes integration To uninstall the Kubernetes integration using Helm, run the following command. Replace some-manifest-file.yml with your specific filename. helm uninstall newrelic -n newrelic if kubectl then : kubectl delete -f some-manifest-file.yml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 218.82137,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "sections": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "Helm is a package manager on top of <em>Kubernetes</em>. It facilitates <em>installation</em>, upgrades, or revision tracking, and it manages dependencies for the services that you install in <em>Kubernetes</em>. To install the <em>integration</em> using Helm, we recommend our <em>Kubernetes</em> automated installer, which will prompt"
      },
      "id": "617d5841196a67bb40f7c1de"
    },
    {
      "sections": [
        "Configure Kubernetes with a proxy",
        "Install the infrastructure agent with a proxy",
        "Set logging with a proxy",
        "Install Kubernetes events with a proxy",
        "Install the Prometheus OpenMetrics integration with a proxy",
        "Set the synthetics minion to use a proxy"
      ],
      "title": "Configure Kubernetes with a proxy",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "9d7794244b451d878d00db0f84c4709e43ce138c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/configure-kubernetes-proxy/",
      "published_at": "2022-01-08T09:49:01Z",
      "updated_at": "2021-12-25T07:57:45Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are running our Kubernetes integration with a proxy, you need to configure each component (the infrastructure agent, logging, Kubernetes events, etc.) to correctly work with your proxy. Install the infrastructure agent with a proxy There are three options to install the infrastructure agent with a proxy: Installing with chart newrelic-infrastructure: The proxy can be configured setting the configuration option config.proxy as described in the values.yaml of the chart. The whole config object is used to generate a configMap that is mounted as the file /etc/newrelic-infra.yml used to configure the agent. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option newrelic-infrastructure.config.proxy. The configuration option is passed down to the dependency newrelic-infrastructure modifying the values.yaml of nri-bundle: newrelic-infrastructure.config.proxy: https://user:password@hostname:port Copy Installing with manifest: When installing through the manifest, the proxy can be configured adding the NRIA_PROXY environment variable to the environment variable section of the DaemonSet configuration. The environment variable NRIA_VERBOSE in the DaemonSet YAML can be used as an example of the environment variable configuration. Set logging with a proxy If you’re using log forwarding for the Kubernetes integration, the plugin automatically detects the HTTP_PROXY and HTTPS_PROXY environment variables, and automatically uses them to set up the proxy configuration. For more information, see how to customize the proxy, or bypass it. Install Kubernetes events with a proxy There are three options to install the Kubernetes events integration with a proxy: Installing with chart nri-kube-events: The proxy can be configured setting the configuration option proxy in the values.yaml file. If set, the variable NRIA_PROXY will be added into the deployment.yaml template. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option nri-kube-events.proxy. The configuration option is passed down to the dependency nri-kube-events modifying the values.yaml of nri-bundle: nri-kube-events.proxy: https://user:password@hostname:port Copy Installing with manifest: When installing through the manifest, to configure the proxy set the NRIA_PROXY in the environment variable section of the infra container. Check the deployment template of the chart as an example of environment variable configuration. Install the Prometheus OpenMetrics integration with a proxy There are three options to install the Prometheus OpenMetric integration with a proxy: Installing with charts nri-prometheus: The proxy can be configured setting the configuration option config.emitter_proxy in the values.yaml. The config object is used to generate a configMap that mounted into the deployment under /etc/nri-prometheus/. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option nri-prometheus.config.emitter_proxy. The configuration option is passed down to the dependency nri-prometheus modifying the values.yaml of nri-bundle: nri-prometheus.config.emitter_proxy: \"http://localhost:8888 Copy Installing with the manifest: Configure the proxy uncommenting the emitter_proxy option directly in the manifest: # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" Copy Set the synthetics minion to use a proxy Check this README on how to set the proxy for the Synthetics minions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 144.90126,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure <em>Kubernetes</em> with a proxy",
        "sections": "<em>Install</em> <em>Kubernetes</em> events with a proxy",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "If you are running our <em>Kubernetes</em> <em>integration</em> with a proxy, you need to configure each component (the infrastructure agent, logging, <em>Kubernetes</em> events, etc.) to correctly work with your proxy. Install the infrastructure agent with a proxy There are three options to install the infrastructure agent"
      },
      "id": "617d584264441f2d01fbc6d7"
    },
    {
      "sections": [
        "Kubernetes integration: install and configure",
        "Use automated installer",
        "Installs for managed services and platforms",
        "Amazon EKS / EKS-Anywhere",
        "Amazon EKS Fargate",
        "Google Kubernetes Engine (GKE)",
        "OpenShift container platform",
        "Azure Kubernetes Service (AKS)",
        "Pivotal Container Service (PKS / VMware Tanzu)",
        "Upgrading our Kubernetes integration",
        "Upgrading using the automated installer and Helm",
        "Upgrading using the automated installer and plain manifests",
        "Tip",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data"
      ],
      "title": "Kubernetes integration: install and configure",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "b1306d1b3a01711286cf78f26008571c5c3036cf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/kubernetes-integration-install-configure/",
      "published_at": "2022-01-08T08:24:04Z",
      "updated_at": "2021-11-25T03:49:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The easiest way to install the Kubernetes integration is to use our automated installer to generate a manifest. It bundles not just the integration DaemonSets, but also other New Relic Kubernetes configurations, like Kubernetes events, Prometheus OpenMetrics, and New Relic log monitoring. As a Kubernetes user, you might consider Auto-telemetry with Pixie for deeper insights into your Kubernetes clusters and workloads. No language agents required. Ready to get started? If you don't already have one, sign up for a New Relic account. It's free, forever! Use automated installer We encourage you to use our automated installer for servers, VMs, and unprivileged environments. The automated installer can provide you either a Helm command with the required values filled, or a plain manifest if you don't want to use Helm. It also features great customizability and full control over which features and dependencies are enabled. If you are installing our integration on a managed cloud, please take a look at these preliminary notes before proceeding. Before you start the install, make sure you have a New Relic account. It's free! No credit card required. If your New Relic account is in the EU region, choose the EU install option. Get an account Start the installer Eu guided install Or, for fully manual instructions for deploying our integration using Helm. Installs for managed services and platforms Before starting our automated installer, check out these notes for your managed services or platforms: Amazon EKS / EKS-Anywhere The Kubernetes integration monitors worker nodes. In Amazon EKS, master nodes are managed by Amazon and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration in Amazon EKS, make sure you are using the version of kubectl provided by AWS. Amazon EKS Fargate Installation on EKS Fargate clusters requires dedicated steps, which are detailed in our fargate installation docs. Google Kubernetes Engine (GKE) The Kubernetes integration monitors worker nodes. In GKE, master nodes are managed by Google and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration on GKE, ensure you have sufficient permissions: Go to console.cloud.google.com/iam-admin/iam and find your username. Click edit. Ensure you have permissions to create Roles and ClusterRoles: If you are not sure, add the Kubernetes Engine Cluster Admin role. If you cannot edit your user role, ask the owner of the GCP project to give you the necessary permissions. OpenShift container platform To deploy the Kubernetes integration with OpenShift: Add the <>{'<release_name>'}</>-newrelic-infrastructure service account to your privileged Security Context Constraints: oc adm policy add-scc-to-user privileged \\ system:serviceaccount:<namespace>:<release_name>-newrelic-infrastructure Copy The default <>{'<release_name>'}</> provided by the installer is nri-bundle. Complete the steps in our automated installer. If you're using signed certificates, make sure they are properly configured by using the following variables in the DaemonSet portion of your manifest to set the .pem file: Copy name: NRIA_CA_BUNDLE_DIR value: YOUR_CA_BUNDLE_DIR name: NRIA_CA_BUNDLE_FILE value: YOUR_CA_BUNDLE_NAME YAML key path: `spec.template.spec.containers.name.env` Copy Save your changes. Azure Kubernetes Service (AKS) The Kubernetes integration monitors worker nodes. In Azure Kubernetes Service, master nodes are managed by Azure and abstracted from the Kubernetes platforms. To deploy in Azure Kubernetes Service (AKS), complete the steps in our automated installer. Pivotal Container Service (PKS / VMware Tanzu) To deploy in PKS, we recommend that you use the automated installer, or you can follow the manual instructions provided in Install the Kubernetes integration using Helm. Upgrading our Kubernetes integration Our Kubernetes integration is under active development and we regularly release updates which include bug fixes, new features, and support for newer Kubernetes versions and cloud providers. We strongly recommend all our customers to regularly update the Kubernetes integration to get the best experience. Upgrading using the automated installer and Helm In order to update an installation that was deployed using Helm command provided by the automated installer, just go through the process and run the Helm command again. This will pull the new version of the chart and its dependencies and upgrade it to the latest version. Upgrading using the automated installer and plain manifests Tip We encourage you to deploy our integration using Helm, as it provides a cleaner upgrade path compared to using manifests directly. If custom manifests have been used instead of Helm, we encourage you to first remove the old installation using kubectl delete -f previous-manifest-file.yml, and then proceed through the guided installer again. This will generate an updated set of manifests that can be deployed using kubectl apply -f manifest-file.yml. We do not recommend applying a new version of the manifest file without removing the previous one first, since it might leave some leftover components in your cluster. Monitor services running on Kubernetes Tip We encourage you to deploy our integration using Helm, as it allows easier configuration on how to monitor services by just adding snippets to your values.yml file. After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 118.318436,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: <em>install</em> and configure",
        "sections": "<em>Kubernetes</em> <em>integration</em>: <em>install</em> and configure",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": " the version of kubectl provided by AWS. Amazon EKS Fargate <em>Installation</em> on EKS Fargate clusters requires dedicated steps, which are detailed in our fargate <em>installation</em> docs. Google <em>Kubernetes</em> Engine (GKE) The <em>Kubernetes</em> <em>integration</em> monitors worker nodes. In GKE, master nodes are managed by Google"
      },
      "id": "617daa4fe7b9d26e59c05d34"
    }
  ],
  "/docs/kubernetes-pixie/kubernetes-integration/installation/configure-kubernetes-proxy": [
    {
      "sections": [
        "Install the Kubernetes integration manually using Helm",
        "Compatibility and requirements",
        "Install Kubernetes integration with Helm",
        "Install with Helm 3 and nri-bundle (recommended)",
        "Installing and configuring nri-bundle with Helm",
        "Install with Helm 2 and nri-bundle (legacy)",
        "Installation instructions for Helm 2",
        "Important",
        "Tip",
        "Helm configuration options",
        "Upgrade using Helm",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data",
        "Reduce data ingest",
        "Prometheus OpenMetrics Integration",
        "New Relic Logging",
        "New Relic Pixie Integration",
        "Uninstall Kubernetes integration"
      ],
      "title": "Install the Kubernetes integration manually using Helm",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "29d42af98d41e7e4e7be86f0150254e132cb6b6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/install-kubernetes-integration-using-helm/",
      "published_at": "2022-01-08T09:49:44Z",
      "updated_at": "2022-01-08T09:49:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Helm is a package manager on top of Kubernetes. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in Kubernetes. To install the integration using Helm, we recommend our Kubernetes automated installer, which will prompt for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our integration as plain manifests rather than a Helm release. See Kubernetes integration: install and configure for more details about how to use our automated installer. Start the installer This page describes in more depth how to install and configure the New Relic integration without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the Kubernetes integration. Our charts are also compatible with Helm 2, but support for it might be removed in the future. To install the Kubernetes integration using Helm, you will need your New Relic account license key and your Kubernetes cluster's name: Find and copy your New Relic license key. Find the name of your cluster with this command: kubectl config current-context Copy Note these values somewhere safe, as you will need them later during the installation process. Install Kubernetes integration with Helm Install with Helm 3 and nri-bundle (recommended) New Relic has several charts for the different components which offer different features for the platform: newrelic-infrastructure: Contains the main Kubernetes integration and the infrastructure agent. This is the core component for the New Relic Kubernetes experience, responsible of reporting most of the data that is surfaced in dashboard and the Kubernetes Cluster Explorer. newrelic-logging: Provides a DaemonSet with New Relic's Fluent Bit output plugin to easily forward your logs to New Relic. nri-kube-events: Collects and reports cluster events (such as kubectl get events) to New Relic. nri-prometheus: New Relic's Prometheus OpenMetrics Integration, automatically scrapes Prometheus endpoints present in the cluser and reports metrics to New Relic. nri-metadata-injection: Sets up a minimal MutatingAdmissionWebhook that injects a couple of environment to containers. These contain metadata about the cluster and New Relic installation and will be later picked up by applications instrumented using APM, allowing to correlate APM and infrastructure data. nri-statsd: New Relic StatsD integration. Additionally, New Relic provides nri-bundle, a chart which pulls a selectable set of the charts mentioned above. nri-bundle can also install Kube State Metrics and Pixie for you if needed. While it is possible to install those charts separately, we strongly recommend using the nri-bundle chart for Kubernetes deployments, as it ensures that values across all the charts are consistent and provides full control over which components are installed, as well as the possibility to configure all of them as Helm dependencies. This is the same chart that is used and referenced by our automated installer. Installing and configuring nri-bundle with Helm Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Please notice and adjust the following flags: global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY: Must be set to a valid License Key for your account. global.cluster=K8S_CLUSTER_NAME: Is used to identify the cluster in the New Relic UI, so should be a descriptive value not used by any other Kubernetes cluster configured in your New Relic account. ksm.enabled=true: Setting this to true will automatically install Kube State Metrics (KSM) for you, which is required for our integration to run. You can set this to false if KSM is already present in your cluster, even if it is on a different namespace. newrelic-infrastructure.privileged=true: Can be set to false to install a trimmed down version of our integration that does not require extra privileges, such as hostPath mounts or running containers as root. Please note that this will disable detailed process collection from the host. For performance reasons, our logging solution still requires hostPath mounts, regardless of the value of this flag. If this is not allowedin your cluster, you will need to disable the logging solution by specifying logging.enabled=false. prometheus.enabled=true: Will deploy our Prometheus OpenMetrics integration, which automatically collects data from prometheus endpoints present in the cluster. webhook.enabled=true: Will install our minimal webhook, which adds environment variables that, in turn, allows linking applications instrumented with APM with infrastructure data. Our chart has a comprehensive set of flags and tunables that can be edited to configure our solution to your particular needs. For a full list of all the flags that can be configured, please check the chart's README and the default values.yaml file. Install the Kubernetes integration by running the customized command without --debug and --dry-run: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl -n newrelic get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node for newrelic-infrastructure, as well as one Deployment and one pod for any other component that you have enabled. Install with Helm 2 and nri-bundle (legacy) Installation instructions for Helm 2 Important Helm 2 has been deprecated and New Relic does not recommend using it for deployments. Instructions in this section are provided for legacy systems only. To install using Helm 2: Ensure that Helm 2, including their cluster-side components (such as Tiller) are properly installed and configured. Please check the official documentation for more details. Set the cluster where you want to install the agent: kubectl config set-cluster DESIRED_CLUSTER Copy To see the available clusters, run kubectl config get-clusters Make sure that kube-state-metrics is installed on your machine: kubectl get deployment --all-namespaces | grep kube-state-metrics Copy If it's not installed, follow the instructions in the kube-state-metrics GitHub repo to install it. Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. This step uses the --dry-run and --debug switches and therefore the agent is not installed. helm install newrelic/newrelic-infrastructure \\ --dry-run \\ --debug \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Install the New Relic Kubernetes integration: helm install newrelic/newrelic-infrastructure \\ --set licenseKey=your_new_relic_license_key \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Tip Note that the --dry-run and --debug switches have been removed. Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node. Helm configuration options When you install or upgrade the Kubernetes integration with Helm using the command line, you can pass your configuration variables with the --set flag. helm install newrelic/newrelic-infrastructure \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=YOUR_CLUSTER_NAME Copy A full list of the configuration parameters can be found in the newrelic-infrastructure chart README Upgrade using Helm To update your Kubernetes integration installed via Helm: Update the local chart repository: helm repo update Copy Update the release by running again the appropriate helm upgrade --install ... command in the section above. Monitor services running on Kubernetes After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages. Reduce data ingest Our charts support setting an option to reduce the amount of data ingest at the cost of dropping detailed information. To enable it, set global.lowDataMode=true in the nri-bundle chart. lowDataMode affects three specific components of the nri-bundle chart outlined below. Prometheus OpenMetrics Integration If lowDataMode is enabled, the following metrics are excluded by default as they are already collected and used by the New Relic Kubernetes Integration. - kube_ - container_ - machine_ - cadvisor_ Copy New Relic Logging If lowDataMode is enabled, Labels and Annotations are set to Off in the Filter section of the fluent-bit.conf file. This means that this detail will be dropped from the container log files which reduces the overall data ingest into New Relic. The following fields are retained: Allowlist_key container_name Allowlist_key namespace_name Allowlist_key pod_name Allowlist_key stream Allowlist_key log Copy Low Data Mode Log Example Complete Log Record [ { \"cluster_name\": \"api-test\", \"kubernetes\": { \"annotations\": { \"kubernetes.io/psp\": \"eks.privileged\" }, \"container_hash\": \"fryckbos/test@sha256:5b098eaf3c7d5b3585eb10cebee63665b6208bea31ef31a3f0856c5ffdda644b\", \"container_image\": \"fryckbos/test:latest\", \"container_name\": \"newrelic-logging\", \"docker_id\": \"134e1daf63761baa15e035b08b7aea04518a0f0e50af4215131a50c6a379a072\", \"host\": \"ip-192-168-17-123.ec2.internal\", \"labels\": { \"app\": \"newrelic-logging\", \"app.kubernetes.io/name\": \"newrelic-logging\", \"controller-revision-hash\": \"84db95db86\", \"pod-template-generation\": \"1\", \"release\": \"nri-bundle\" }, \"namespace_name\": \"nrlogs\", \"pod_id\": \"54556e3e-719c-46b5-af69-020b75d69bf1\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\" }, \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"plugin\": { \"source\": \"kubernetes\", \"type\": \"fluent-bit\", \"version\": \"1.8.1\" }, \"stream\": \"stderr\", \"time\": \"2021-09-14T12:30:49.138824971Z\", \"timestamp\": 1631622649138 } ] Copy Log Record after enabling lowDataMode. [ { \"cluster_name\": \"api-test\", \"container_name\": \"newrelic-logging\", \"namespace_name\": \"nrlogs\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\", \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"stream\": \"stderr\", \"timestamp\": 1631622649138 } ] Copy New Relic Pixie Integration If lowDataMode is enabled, the newrelic-pixie integration performs heavier sampling on Pixie spans and reduces the collection interval from 10 seconds to 15 seconds. lowDataMode settings: HTTP_SPAN_LIMIT: 750 DB_SPAN_LIMIT: 250 COLLECT_INTERVAL_SEC: 15 Copy The default settings for these parameters and others can be found in the newrelic-pixie-integration Github repo. Uninstall Kubernetes integration To uninstall the Kubernetes integration using Helm, run the following command. Replace some-manifest-file.yml with your specific filename. helm uninstall newrelic -n newrelic if kubectl then : kubectl delete -f some-manifest-file.yml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 218.82121,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "sections": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "Helm is a package manager on top of <em>Kubernetes</em>. It facilitates <em>installation</em>, upgrades, or revision tracking, and it manages dependencies for the services that you install in <em>Kubernetes</em>. To install the <em>integration</em> using Helm, we recommend our <em>Kubernetes</em> automated installer, which will prompt"
      },
      "id": "617d5841196a67bb40f7c1de"
    },
    {
      "sections": [
        "Configure control plane monitoring",
        "Features",
        "Compatibility and requirements",
        "Discovery of master nodes and control plane components",
        "Configuration",
        "Important",
        "ETCD",
        "API server",
        "OpenShift configuration",
        "Set up mTLS from the ETCD client CA",
        "Tip",
        "To ease future installations",
        "Set up mTLS for ETCD in OpenShift",
        "See your data"
      ],
      "title": "Configure control plane monitoring",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "d648be240fd2f1b09bc1f29ef805bc0dd5c59e7a",
      "image": "https://docs.newrelic.com/static/209f301630c770f87ea8cbb1cace8e6e/8c557/new-relic-one-k8s-cluster-explorer-control-plane-parameters.png",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/configure-control-plane-monitoring/",
      "published_at": "2022-01-08T09:49:01Z",
      "updated_at": "2021-12-09T15:16:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic provides Control Plane support for your Kubernetes integration, allowing you to monitor and collect metrics from your cluster's Control Plane components. That data can then be found in New Relic and used to create queries and charts. Features We monitor and collect metrics from the following control plane components: ETCD: leader information, resident memory size, number of OS threads, consensus proposals data, etc. For a list of supported metrics, see ETCD data. API server: rate of apiserver requests, breakdown of apiserver requests by HTTP method and response code, etc. For the complete list of supported metrics, see API server data. Scheduler: requested CPU/memory vs available on the node, tolerations to taints, any set affinity or anti-affinity, etc. For the complete list of supported metrics, see Scheduler data. Controller manager: resident memory size, number of OS threads created, goroutines currently existing, etc. For the complete list of supported metrics, see Controller manager data. Compatibility and requirements Control plane monitoring requires Kubernetes integration version 1.16.0 or higher. Control plane monitoring support is not enabled for managed clusters. This is because providers (EKS, GKE, AKS, etc.) abstract away the concept of master nodes and control plane components, so that access to them is limited or non-existent. External control planes are not supported. The unprivileged version of the Kubernetes integration does not support control plane monitoring. OpenShift 4.x uses control plane component metric endpoints that are different than the default. Discovery of master nodes and control plane components The Kubernetes integration relies on the kubeadm labeling conventions to discover the master nodes and the control plane components. This means that master nodes should be labeled with node-role.kubernetes.io/master=\"\" or kubernetes.io/role=\"master\". The control plane components should have either the k8s-app or the tier and component labels. Refer to the following table for accepted label combinations and values: Component Label Endpoint API server Kubeadm / Kops / ClusterAPI k8s-app=kube-apiserver tier=control-plane component=kube-apiserver OpenShift app=openshift-kube-apiserver apiserver=true localhost:443/metrics by default (can be configured) if the request fails falls back to localhost:8080/metrics ETCD Kubeadm / Kops / ClusterAPI k8s-app=etcd-manager-main tier=control-plane component=etcd OpenShift k8s-app=etcd localhost:4001/metrics Scheduler Kubeadm / Kops / ClusterAPI k8s-app=kube-scheduler tier=control-plane component=kube-scheduler OpenShift app=openshift-kube-scheduler scheduler=true localhost:10251/metrics Controller manager Kubeadm / Kops / ClusterAPI k8s-app=kube-controller-manager tier=control-plane component=kube-controller-manager​ OpenShift app=kube-controller-manager kube-controller-manager=true localhost:10252/metrics When the integration detects that it is running inside a master node, it tries to find which components are running on the node by looking for pods that match the labels listed in the table above. For every running component, the integration makes a request to its metrics endpoint. Configuration Control plane monitoring is automatic for agents running inside master nodes. The only component that requires an extra step to run is ETCD, because it uses mutual TLS authentication (mTLS) for client requests. The API Server can also be configured to be queried using the Secure Port. Important Control plane monitoring for OpenShift 4.x requires additional configuration. For more information, see the OpenShift 4.x Configuration section. ETCD In order to set mTLS for querying ETCD, there are two configuration options that need to be set: Option Value ETCD_TLS_SECRET_NAME Name of a Kubernetes secret that contains the mTLS configuration. The secret should contain the following keys: cert: the certificate that identifies the client making the request. It should be signed by an ETCD trusted CA. key: the private key used to generate the client certificate. cacert: the root CA used to identify the ETCD server certificate. If the ETCD_TLS_SECRET_NAME option is not set, ETCD metrics won't be fetched. For step by step instructions on how to create a certificate and sign it with the ETCD client CA, see Set up mTLS from the ETCD client CA. ETCD_TLS_SECRET_NAMESPACE The namespace where the secret specified in the ETCD_TLS_SECRET_NAME was created. If not set, the default namespace is used. API server By default, the API server metrics are queried using the localhost:8080 unsecured endpoint. If this port is disabled, you can also query these metrics over the secure port. To enable this, set the following configuration option in the Kubernetes integration manifest file: Option Value API_SERVER_ENDPOINT_URL The (secure) URL to query the metrics. The API server uses localhost:443 by default Ensure that the ClusterRole has been updated to the newest version found in the manifest Added in version 1.15.0 Important Note that the port can be different according to the secure port used by the API server. For example, in Minikube the API server secure port is 8443 and therefore API_SERVER_ENDPOINT_URL should be set to https://localhost:8443 OpenShift configuration Control plane components on OpenShift 4.x use endpoint URLs that require SSL and service account based authentication. Therefore, the default endpoint URLs can not be used. To configure control plane monitoring on OpenShift, uncomment the following environment variables in the customized manifest. URL values are pre-configured to the default base URLs for the control plane monitoring metrics endpoints in OpenShift 4.x. - name: \"SCHEDULER_ENDPOINT_URL\" value: \"https://localhost:10259 - name: \"ETCD_ENDPOINT_URL\" value: \"https://localhost:9979\" - name: \"CONTROLLER_MANAGER_ENDPOINT_URL\" value: \"https://localhost:10257\" - name: \"API_SERVER_ENDPOINT_URL\" value: \"https://localhost:6443\" Copy Important Even though the custom ETCD_ENDPOINT_URL is defined, ETCD requires HTTPS and mTLS authentication to be configured. For more on configuring mTLS for ETCD in OpenShift, see Set up mTLS for ETCD in OpenShift. Important When installing through Helm openshift, specify the config to automatically include these endpoints. Setting openshift.enabled=true and openshift.version=\"4.x\" will include the secure endpoints and enable the /var/run/crio.sock runtime. Set up mTLS from the ETCD client CA The instructions below are based on the Kubernetes documentation. For more information, see Managing TLS certificates in a cluster. For OpenShift, see Set up mTLS for ETCD in OpenShift. To set up mTLS from the ETCD client CA: Download and install the tool cfssl, selecting the correct binaries for your OS from the list. Once installed, execute the following command: cat <<EOF | cfssl genkey - | cfssljson -bare server { \"hosts\": [ \"localhost\" ], \"CN\": \"newrelic-infra.pod.cluster.local\", \"key\": { \"algo\": \"ecdsa\", \"size\": 256 } } EOF Copy This command generates two files; server.csr containing the PEM encoded pkcs#10 certification request and server-key.pem containing the PEM encoded key to the certificate to be created. Use the generated certificate authority (CA) of ETCD to sign your CSR. Depending on your cluster configuration, you may already have this information. For default install configuration, download the CA certificate and the private key directly from ETCD with the following commands: kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.crt ./cacert -n kube-system kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.key ./cacert.key -n kube-system Copy Tip This requires that the etcd-manager-main pod has the label k8s-app=etcd-manager-main, which is a requirement for control plane monitoring . If your etc-manager-main pod is located in a different namespace, change the -n kube-system flags accordingly. With those files downloaded, use the following command to sign your CSRF: cfssl sign -ca cacert -ca-key cacert.key server.csr | cfssljson -bare cert Copy Create the secret that is used to retrieve the TLS config for making requests to ETC. We recommend renaming the certificate and the private key: cp cert.pem cert && cp server-key.pem key Copy kubectl -n default create secret generic newrelic-infra-etcd-tls-secret --from-file=./cert --from-file=./key --from-file=./cacert Copy To ease future installations Use the following commands to simultaneously create the CSR, retrieve the CA, generate the certificate by signing the CSR, and create the secret with all the required fields: cat <<EOF | cfssl genkey - | cfssljson -bare server && \\ kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.crt ./cacert -n kube-system && \\ kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.key ./cacert.key -n kube-system && \\ cp server-key.pem key && \\ cfssl sign -ca cacert -ca-key cacert.key server.csr | cfssljson -bare cert && \\ cp cert.pem cert && \\ kubectl -n default create secret generic newrelic-infra-etcd-tls-secret --from-file=./cert --from-file=./key --from-file=./cacert { \"hosts\": [ \"localhost\" ], \"CN\": \"newrelic-infra.pod.cluster.local\", \"key\": { \"algo\": \"ecdsa\", \"size\": 256 } } EOF Copy The last step is to update the configuration in the manifest and apply it. In the configuration section, there are two options related to ETCD mTLS: ETCD_TLS_SECRET_NAME with the name of the secret that we just created. ETCD_TLS_SECRET_NAMESPACE with the namespace that we used to create the secret. To complete the installation, add these variables to the container spec of the integration DaemonSet and apply the changes: - name: \"ETCD_TLS_SECRET_NAME” value: \"newrelic-infra-etcd-tls-secret\" - name: \"ETCD_TLS_SECRET_NAMESPACE\" value: \"default\" Copy Set up mTLS for ETCD in OpenShift Follow these instructions to set up mutual TLS authentication for ETCD in OpenShift 4.x: Export the ETCD client certificates from the cluster to an opaque secret. In a default managed OpenShift cluster, the secret is named kube-etcd-client-certs and it is stored in the openshift-monitoring namespace. kubectl get secret/kube-etcd-client-certs -n openshift-monitoring -o yaml > etcd-secret.yaml Copy Open the secret file and change the keys: Rename the certificate authority to cacert. Rename the client certificate to cert. Rename the client key to key. Optional: change the secret name and namespace to something meaningful. Remove these unnecessary keys in the metadata section: creationTimestamp resourceVersion selfLink uid Install the manifest with its new name and namespace: kubectl apply -f etcd-secret.yaml Copy Go to Update manifest configuration (the last step under Set up MTL from ETCD client) to configure the required environment variables. See your data If the integration has been been set up correctly, the Kubernetes cluster explorer contains all the Control Plane components and their status in a dedicated section, as shown below. one.newrelic.com > Kubernetes Cluster Explorer: Use the Kubernetes cluster explorer to monitor and collect metrics from your cluster's Control Plane components You can also check for Control Plane data with this NRQL query: SELECT latest(timestamp) FROM K8sApiServerSample, K8sEtcdSample, K8sSchedulerSample, K8sControllerManagerSample FACET entityName where clusterName = 'MY_CLUSTER_NAME' Copy Tip If you still can't see Control Plane data, try the solution described in Kubernetes integration troubleshooting: Not seeing data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 125.899605,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "To ease future <em>installations</em>",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "New Relic provides Control Plane support for your <em>Kubernetes</em> <em>integration</em>, allowing you to monitor and collect metrics from your cluster&#x27;s Control Plane components. That data can then be found in New Relic and used to create queries and charts. Features We monitor and collect metrics from"
      },
      "id": "617d735f64441f9d39fbe7ee"
    },
    {
      "sections": [
        "Kubernetes integration: install and configure",
        "Use automated installer",
        "Installs for managed services and platforms",
        "Amazon EKS / EKS-Anywhere",
        "Amazon EKS Fargate",
        "Google Kubernetes Engine (GKE)",
        "OpenShift container platform",
        "Azure Kubernetes Service (AKS)",
        "Pivotal Container Service (PKS / VMware Tanzu)",
        "Upgrading our Kubernetes integration",
        "Upgrading using the automated installer and Helm",
        "Upgrading using the automated installer and plain manifests",
        "Tip",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data"
      ],
      "title": "Kubernetes integration: install and configure",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "b1306d1b3a01711286cf78f26008571c5c3036cf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/kubernetes-integration-install-configure/",
      "published_at": "2022-01-08T08:24:04Z",
      "updated_at": "2021-11-25T03:49:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The easiest way to install the Kubernetes integration is to use our automated installer to generate a manifest. It bundles not just the integration DaemonSets, but also other New Relic Kubernetes configurations, like Kubernetes events, Prometheus OpenMetrics, and New Relic log monitoring. As a Kubernetes user, you might consider Auto-telemetry with Pixie for deeper insights into your Kubernetes clusters and workloads. No language agents required. Ready to get started? If you don't already have one, sign up for a New Relic account. It's free, forever! Use automated installer We encourage you to use our automated installer for servers, VMs, and unprivileged environments. The automated installer can provide you either a Helm command with the required values filled, or a plain manifest if you don't want to use Helm. It also features great customizability and full control over which features and dependencies are enabled. If you are installing our integration on a managed cloud, please take a look at these preliminary notes before proceeding. Before you start the install, make sure you have a New Relic account. It's free! No credit card required. If your New Relic account is in the EU region, choose the EU install option. Get an account Start the installer Eu guided install Or, for fully manual instructions for deploying our integration using Helm. Installs for managed services and platforms Before starting our automated installer, check out these notes for your managed services or platforms: Amazon EKS / EKS-Anywhere The Kubernetes integration monitors worker nodes. In Amazon EKS, master nodes are managed by Amazon and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration in Amazon EKS, make sure you are using the version of kubectl provided by AWS. Amazon EKS Fargate Installation on EKS Fargate clusters requires dedicated steps, which are detailed in our fargate installation docs. Google Kubernetes Engine (GKE) The Kubernetes integration monitors worker nodes. In GKE, master nodes are managed by Google and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration on GKE, ensure you have sufficient permissions: Go to console.cloud.google.com/iam-admin/iam and find your username. Click edit. Ensure you have permissions to create Roles and ClusterRoles: If you are not sure, add the Kubernetes Engine Cluster Admin role. If you cannot edit your user role, ask the owner of the GCP project to give you the necessary permissions. OpenShift container platform To deploy the Kubernetes integration with OpenShift: Add the <>{'<release_name>'}</>-newrelic-infrastructure service account to your privileged Security Context Constraints: oc adm policy add-scc-to-user privileged \\ system:serviceaccount:<namespace>:<release_name>-newrelic-infrastructure Copy The default <>{'<release_name>'}</> provided by the installer is nri-bundle. Complete the steps in our automated installer. If you're using signed certificates, make sure they are properly configured by using the following variables in the DaemonSet portion of your manifest to set the .pem file: Copy name: NRIA_CA_BUNDLE_DIR value: YOUR_CA_BUNDLE_DIR name: NRIA_CA_BUNDLE_FILE value: YOUR_CA_BUNDLE_NAME YAML key path: `spec.template.spec.containers.name.env` Copy Save your changes. Azure Kubernetes Service (AKS) The Kubernetes integration monitors worker nodes. In Azure Kubernetes Service, master nodes are managed by Azure and abstracted from the Kubernetes platforms. To deploy in Azure Kubernetes Service (AKS), complete the steps in our automated installer. Pivotal Container Service (PKS / VMware Tanzu) To deploy in PKS, we recommend that you use the automated installer, or you can follow the manual instructions provided in Install the Kubernetes integration using Helm. Upgrading our Kubernetes integration Our Kubernetes integration is under active development and we regularly release updates which include bug fixes, new features, and support for newer Kubernetes versions and cloud providers. We strongly recommend all our customers to regularly update the Kubernetes integration to get the best experience. Upgrading using the automated installer and Helm In order to update an installation that was deployed using Helm command provided by the automated installer, just go through the process and run the Helm command again. This will pull the new version of the chart and its dependencies and upgrade it to the latest version. Upgrading using the automated installer and plain manifests Tip We encourage you to deploy our integration using Helm, as it provides a cleaner upgrade path compared to using manifests directly. If custom manifests have been used instead of Helm, we encourage you to first remove the old installation using kubectl delete -f previous-manifest-file.yml, and then proceed through the guided installer again. This will generate an updated set of manifests that can be deployed using kubectl apply -f manifest-file.yml. We do not recommend applying a new version of the manifest file without removing the previous one first, since it might leave some leftover components in your cluster. Monitor services running on Kubernetes Tip We encourage you to deploy our integration using Helm, as it allows easier configuration on how to monitor services by just adding snippets to your values.yml file. After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 118.31843,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: <em>install</em> and configure",
        "sections": "<em>Kubernetes</em> <em>integration</em>: <em>install</em> and configure",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": " the version of kubectl provided by AWS. Amazon EKS Fargate <em>Installation</em> on EKS Fargate clusters requires dedicated steps, which are detailed in our fargate <em>installation</em> docs. Google <em>Kubernetes</em> Engine (GKE) The <em>Kubernetes</em> <em>integration</em> monitors worker nodes. In GKE, master nodes are managed by Google"
      },
      "id": "617daa4fe7b9d26e59c05d34"
    }
  ],
  "/docs/kubernetes-pixie/kubernetes-integration/installation/install-fargate-integration": [
    {
      "image": "",
      "url": "https://docs.newrelic.com/whats-new/2021/06/monitoring-amazon-eks-on-aws-fargate/",
      "sections": [
        "Monitor Amazon EKS on AWS Fargate integration with our public beta"
      ],
      "published_at": "2022-01-08T05:33:33Z",
      "title": "Monitor Amazon EKS on AWS Fargate integration with our public beta",
      "updated_at": "2021-07-07T14:12:51Z",
      "type": "docs",
      "external_id": "a7899b6302aa05b943053c2230e2787799a0fef4",
      "document_type": "nr1_announcement",
      "popularity": 1,
      "body": "We are introducing New Relic's integration for Amazon EKS on AWS Fargate public beta. If you are interested in checking out the beta, please follow the steps found in the documentation. Our EKS Fargate integration supports any Fargate setup, whether the cluster is only composed of Fargate nodes or if it also coexists with EC2 nodes. The integration is also compatible with the New Relic One Kubernetes cluster explorer, providing a holistic view of a Kubernetes cluster and rapid troubleshooting. We've also improved the Kubernetes dashboard to list Fargate nodes and distinguish between standard and Fargate serverless nodes. Check it out today.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 739.8958,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor Amazon <em>EKS</em> on AWS <em>Fargate</em> integration with our public beta",
        "sections": "Monitor Amazon <em>EKS</em> on AWS <em>Fargate</em> integration with our public beta",
        "body": "We are introducing New Relic&#x27;s integration for Amazon <em>EKS</em> on AWS <em>Fargate</em> public beta. If you are interested in checking out the beta, please follow the steps found in the documentation. Our <em>EKS</em> <em>Fargate</em> integration supports any <em>Fargate</em> setup, whether the cluster is only composed of <em>Fargate</em> nodes"
      },
      "id": "60e5b66364441f2e58c42399"
    },
    {
      "sections": [
        "Kubernetes integration: compatibility and requirements",
        "Compatibility",
        "EOL NOTICE",
        "Requirements",
        "Install using Helm"
      ],
      "title": "Kubernetes integration: compatibility and requirements",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "5d37ab2bb0cc980c42a41a360eb61cc1e45685c4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements/",
      "published_at": "2022-01-08T09:48:25Z",
      "updated_at": "2021-11-13T22:08:46Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our integration. Compatibility EOL NOTICE We're discontinuing support for several capabilities in December 2021, including Kubernetes instrumentation support for versions v1.10 to v1.15. For more details, including how you can easily prepare for this transition, see our Explorers Hub post. Our Kubernetes integration is compatible with the following versions, depending on the installation mode: Install mode or feature Kubernetes versions Kubernetes cluster Currently tested with versions 1.16 to 1.22 Kubernetes cluster GKE Currently tested with versions 1.17 to 1.19 Kubernetes cluster EKS, EKS-Anywhere, EKS-Fargate Compatible with version 1.16 or higher Kubernetes cluster AKS Compatible with version 1.16 or higher Kubernetes cluster OpenShift Currently tested with versions 4.6 Kubernetes cluster VMware Tanzu Compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10 Control plane monitoring Compatible with version 1.16 or higher Service monitoring Compatible with version 1.16 or higher Requirements The New Relic Kubernetes integration has the following requirements: A New Relic account. Don't have one? Sign up for free. No credit card required. Linux distribution compatible with New Relic infrastructure agent. kube-state-metrics version 1.9.8 running on the cluster. When using CRI-O as the container runtime, the processes inside containers are not reported. Performance data is collected at the container level. Install using Helm For detailed instructions about how to install our integration using Helm, see Manual install using Helm.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 610.3855,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": " mode or feature Kubernetes versions Kubernetes cluster Currently tested with versions 1.16 to 1.22 Kubernetes cluster GKE Currently tested with versions 1.17 to 1.19 Kubernetes cluster <em>EKS</em>, <em>EKS</em>-Anywhere, <em>EKS</em>-<em>Fargate</em> Compatible with version 1.16 or higher Kubernetes cluster AKS Compatible with version"
      },
      "id": "617d54de64441f0879fbebed"
    },
    {
      "sections": [
        "Kubernetes integration: install and configure",
        "Use automated installer",
        "Installs for managed services and platforms",
        "Amazon EKS / EKS-Anywhere",
        "Amazon EKS Fargate",
        "Google Kubernetes Engine (GKE)",
        "OpenShift container platform",
        "Azure Kubernetes Service (AKS)",
        "Pivotal Container Service (PKS / VMware Tanzu)",
        "Upgrading our Kubernetes integration",
        "Upgrading using the automated installer and Helm",
        "Upgrading using the automated installer and plain manifests",
        "Tip",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data"
      ],
      "title": "Kubernetes integration: install and configure",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "b1306d1b3a01711286cf78f26008571c5c3036cf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/kubernetes-integration-install-configure/",
      "published_at": "2022-01-08T08:24:04Z",
      "updated_at": "2021-11-25T03:49:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The easiest way to install the Kubernetes integration is to use our automated installer to generate a manifest. It bundles not just the integration DaemonSets, but also other New Relic Kubernetes configurations, like Kubernetes events, Prometheus OpenMetrics, and New Relic log monitoring. As a Kubernetes user, you might consider Auto-telemetry with Pixie for deeper insights into your Kubernetes clusters and workloads. No language agents required. Ready to get started? If you don't already have one, sign up for a New Relic account. It's free, forever! Use automated installer We encourage you to use our automated installer for servers, VMs, and unprivileged environments. The automated installer can provide you either a Helm command with the required values filled, or a plain manifest if you don't want to use Helm. It also features great customizability and full control over which features and dependencies are enabled. If you are installing our integration on a managed cloud, please take a look at these preliminary notes before proceeding. Before you start the install, make sure you have a New Relic account. It's free! No credit card required. If your New Relic account is in the EU region, choose the EU install option. Get an account Start the installer Eu guided install Or, for fully manual instructions for deploying our integration using Helm. Installs for managed services and platforms Before starting our automated installer, check out these notes for your managed services or platforms: Amazon EKS / EKS-Anywhere The Kubernetes integration monitors worker nodes. In Amazon EKS, master nodes are managed by Amazon and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration in Amazon EKS, make sure you are using the version of kubectl provided by AWS. Amazon EKS Fargate Installation on EKS Fargate clusters requires dedicated steps, which are detailed in our fargate installation docs. Google Kubernetes Engine (GKE) The Kubernetes integration monitors worker nodes. In GKE, master nodes are managed by Google and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration on GKE, ensure you have sufficient permissions: Go to console.cloud.google.com/iam-admin/iam and find your username. Click edit. Ensure you have permissions to create Roles and ClusterRoles: If you are not sure, add the Kubernetes Engine Cluster Admin role. If you cannot edit your user role, ask the owner of the GCP project to give you the necessary permissions. OpenShift container platform To deploy the Kubernetes integration with OpenShift: Add the <>{'<release_name>'}</>-newrelic-infrastructure service account to your privileged Security Context Constraints: oc adm policy add-scc-to-user privileged \\ system:serviceaccount:<namespace>:<release_name>-newrelic-infrastructure Copy The default <>{'<release_name>'}</> provided by the installer is nri-bundle. Complete the steps in our automated installer. If you're using signed certificates, make sure they are properly configured by using the following variables in the DaemonSet portion of your manifest to set the .pem file: Copy name: NRIA_CA_BUNDLE_DIR value: YOUR_CA_BUNDLE_DIR name: NRIA_CA_BUNDLE_FILE value: YOUR_CA_BUNDLE_NAME YAML key path: `spec.template.spec.containers.name.env` Copy Save your changes. Azure Kubernetes Service (AKS) The Kubernetes integration monitors worker nodes. In Azure Kubernetes Service, master nodes are managed by Azure and abstracted from the Kubernetes platforms. To deploy in Azure Kubernetes Service (AKS), complete the steps in our automated installer. Pivotal Container Service (PKS / VMware Tanzu) To deploy in PKS, we recommend that you use the automated installer, or you can follow the manual instructions provided in Install the Kubernetes integration using Helm. Upgrading our Kubernetes integration Our Kubernetes integration is under active development and we regularly release updates which include bug fixes, new features, and support for newer Kubernetes versions and cloud providers. We strongly recommend all our customers to regularly update the Kubernetes integration to get the best experience. Upgrading using the automated installer and Helm In order to update an installation that was deployed using Helm command provided by the automated installer, just go through the process and run the Helm command again. This will pull the new version of the chart and its dependencies and upgrade it to the latest version. Upgrading using the automated installer and plain manifests Tip We encourage you to deploy our integration using Helm, as it provides a cleaner upgrade path compared to using manifests directly. If custom manifests have been used instead of Helm, we encourage you to first remove the old installation using kubectl delete -f previous-manifest-file.yml, and then proceed through the guided installer again. This will generate an updated set of manifests that can be deployed using kubectl apply -f manifest-file.yml. We do not recommend applying a new version of the manifest file without removing the previous one first, since it might leave some leftover components in your cluster. Monitor services running on Kubernetes Tip We encourage you to deploy our integration using Helm, as it allows easier configuration on how to monitor services by just adding snippets to your values.yml file. After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 543.35925,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Amazon <em>EKS</em> <em>Fargate</em>",
        "body": " the version of kubectl provided by AWS. Amazon <em>EKS</em> <em>Fargate</em> Installation on <em>EKS</em> <em>Fargate</em> clusters requires dedicated steps, which are detailed in our <em>fargate</em> installation docs. Google Kubernetes Engine (GKE) The Kubernetes integration monitors worker nodes. In GKE, master nodes are managed by Google"
      },
      "id": "617daa4fe7b9d26e59c05d34"
    }
  ],
  "/docs/kubernetes-pixie/kubernetes-integration/installation/install-kubernetes-integration-using-helm": [
    {
      "sections": [
        "Configure Kubernetes with a proxy",
        "Install the infrastructure agent with a proxy",
        "Set logging with a proxy",
        "Install Kubernetes events with a proxy",
        "Install the Prometheus OpenMetrics integration with a proxy",
        "Set the synthetics minion to use a proxy"
      ],
      "title": "Configure Kubernetes with a proxy",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "9d7794244b451d878d00db0f84c4709e43ce138c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/configure-kubernetes-proxy/",
      "published_at": "2022-01-08T09:49:01Z",
      "updated_at": "2021-12-25T07:57:45Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are running our Kubernetes integration with a proxy, you need to configure each component (the infrastructure agent, logging, Kubernetes events, etc.) to correctly work with your proxy. Install the infrastructure agent with a proxy There are three options to install the infrastructure agent with a proxy: Installing with chart newrelic-infrastructure: The proxy can be configured setting the configuration option config.proxy as described in the values.yaml of the chart. The whole config object is used to generate a configMap that is mounted as the file /etc/newrelic-infra.yml used to configure the agent. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option newrelic-infrastructure.config.proxy. The configuration option is passed down to the dependency newrelic-infrastructure modifying the values.yaml of nri-bundle: newrelic-infrastructure.config.proxy: https://user:password@hostname:port Copy Installing with manifest: When installing through the manifest, the proxy can be configured adding the NRIA_PROXY environment variable to the environment variable section of the DaemonSet configuration. The environment variable NRIA_VERBOSE in the DaemonSet YAML can be used as an example of the environment variable configuration. Set logging with a proxy If you’re using log forwarding for the Kubernetes integration, the plugin automatically detects the HTTP_PROXY and HTTPS_PROXY environment variables, and automatically uses them to set up the proxy configuration. For more information, see how to customize the proxy, or bypass it. Install Kubernetes events with a proxy There are three options to install the Kubernetes events integration with a proxy: Installing with chart nri-kube-events: The proxy can be configured setting the configuration option proxy in the values.yaml file. If set, the variable NRIA_PROXY will be added into the deployment.yaml template. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option nri-kube-events.proxy. The configuration option is passed down to the dependency nri-kube-events modifying the values.yaml of nri-bundle: nri-kube-events.proxy: https://user:password@hostname:port Copy Installing with manifest: When installing through the manifest, to configure the proxy set the NRIA_PROXY in the environment variable section of the infra container. Check the deployment template of the chart as an example of environment variable configuration. Install the Prometheus OpenMetrics integration with a proxy There are three options to install the Prometheus OpenMetric integration with a proxy: Installing with charts nri-prometheus: The proxy can be configured setting the configuration option config.emitter_proxy in the values.yaml. The config object is used to generate a configMap that mounted into the deployment under /etc/nri-prometheus/. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option nri-prometheus.config.emitter_proxy. The configuration option is passed down to the dependency nri-prometheus modifying the values.yaml of nri-bundle: nri-prometheus.config.emitter_proxy: \"http://localhost:8888 Copy Installing with the manifest: Configure the proxy uncommenting the emitter_proxy option directly in the manifest: # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" Copy Set the synthetics minion to use a proxy Check this README on how to set the proxy for the Synthetics minions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 144.90125,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure <em>Kubernetes</em> with a proxy",
        "sections": "<em>Install</em> <em>Kubernetes</em> events with a proxy",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "If you are running our <em>Kubernetes</em> <em>integration</em> with a proxy, you need to configure each component (the infrastructure agent, logging, <em>Kubernetes</em> events, etc.) to correctly work with your proxy. Install the infrastructure agent with a proxy There are three options to install the infrastructure agent"
      },
      "id": "617d584264441f2d01fbc6d7"
    },
    {
      "sections": [
        "Configure control plane monitoring",
        "Features",
        "Compatibility and requirements",
        "Discovery of master nodes and control plane components",
        "Configuration",
        "Important",
        "ETCD",
        "API server",
        "OpenShift configuration",
        "Set up mTLS from the ETCD client CA",
        "Tip",
        "To ease future installations",
        "Set up mTLS for ETCD in OpenShift",
        "See your data"
      ],
      "title": "Configure control plane monitoring",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "d648be240fd2f1b09bc1f29ef805bc0dd5c59e7a",
      "image": "https://docs.newrelic.com/static/209f301630c770f87ea8cbb1cace8e6e/8c557/new-relic-one-k8s-cluster-explorer-control-plane-parameters.png",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/configure-control-plane-monitoring/",
      "published_at": "2022-01-08T09:49:01Z",
      "updated_at": "2021-12-09T15:16:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic provides Control Plane support for your Kubernetes integration, allowing you to monitor and collect metrics from your cluster's Control Plane components. That data can then be found in New Relic and used to create queries and charts. Features We monitor and collect metrics from the following control plane components: ETCD: leader information, resident memory size, number of OS threads, consensus proposals data, etc. For a list of supported metrics, see ETCD data. API server: rate of apiserver requests, breakdown of apiserver requests by HTTP method and response code, etc. For the complete list of supported metrics, see API server data. Scheduler: requested CPU/memory vs available on the node, tolerations to taints, any set affinity or anti-affinity, etc. For the complete list of supported metrics, see Scheduler data. Controller manager: resident memory size, number of OS threads created, goroutines currently existing, etc. For the complete list of supported metrics, see Controller manager data. Compatibility and requirements Control plane monitoring requires Kubernetes integration version 1.16.0 or higher. Control plane monitoring support is not enabled for managed clusters. This is because providers (EKS, GKE, AKS, etc.) abstract away the concept of master nodes and control plane components, so that access to them is limited or non-existent. External control planes are not supported. The unprivileged version of the Kubernetes integration does not support control plane monitoring. OpenShift 4.x uses control plane component metric endpoints that are different than the default. Discovery of master nodes and control plane components The Kubernetes integration relies on the kubeadm labeling conventions to discover the master nodes and the control plane components. This means that master nodes should be labeled with node-role.kubernetes.io/master=\"\" or kubernetes.io/role=\"master\". The control plane components should have either the k8s-app or the tier and component labels. Refer to the following table for accepted label combinations and values: Component Label Endpoint API server Kubeadm / Kops / ClusterAPI k8s-app=kube-apiserver tier=control-plane component=kube-apiserver OpenShift app=openshift-kube-apiserver apiserver=true localhost:443/metrics by default (can be configured) if the request fails falls back to localhost:8080/metrics ETCD Kubeadm / Kops / ClusterAPI k8s-app=etcd-manager-main tier=control-plane component=etcd OpenShift k8s-app=etcd localhost:4001/metrics Scheduler Kubeadm / Kops / ClusterAPI k8s-app=kube-scheduler tier=control-plane component=kube-scheduler OpenShift app=openshift-kube-scheduler scheduler=true localhost:10251/metrics Controller manager Kubeadm / Kops / ClusterAPI k8s-app=kube-controller-manager tier=control-plane component=kube-controller-manager​ OpenShift app=kube-controller-manager kube-controller-manager=true localhost:10252/metrics When the integration detects that it is running inside a master node, it tries to find which components are running on the node by looking for pods that match the labels listed in the table above. For every running component, the integration makes a request to its metrics endpoint. Configuration Control plane monitoring is automatic for agents running inside master nodes. The only component that requires an extra step to run is ETCD, because it uses mutual TLS authentication (mTLS) for client requests. The API Server can also be configured to be queried using the Secure Port. Important Control plane monitoring for OpenShift 4.x requires additional configuration. For more information, see the OpenShift 4.x Configuration section. ETCD In order to set mTLS for querying ETCD, there are two configuration options that need to be set: Option Value ETCD_TLS_SECRET_NAME Name of a Kubernetes secret that contains the mTLS configuration. The secret should contain the following keys: cert: the certificate that identifies the client making the request. It should be signed by an ETCD trusted CA. key: the private key used to generate the client certificate. cacert: the root CA used to identify the ETCD server certificate. If the ETCD_TLS_SECRET_NAME option is not set, ETCD metrics won't be fetched. For step by step instructions on how to create a certificate and sign it with the ETCD client CA, see Set up mTLS from the ETCD client CA. ETCD_TLS_SECRET_NAMESPACE The namespace where the secret specified in the ETCD_TLS_SECRET_NAME was created. If not set, the default namespace is used. API server By default, the API server metrics are queried using the localhost:8080 unsecured endpoint. If this port is disabled, you can also query these metrics over the secure port. To enable this, set the following configuration option in the Kubernetes integration manifest file: Option Value API_SERVER_ENDPOINT_URL The (secure) URL to query the metrics. The API server uses localhost:443 by default Ensure that the ClusterRole has been updated to the newest version found in the manifest Added in version 1.15.0 Important Note that the port can be different according to the secure port used by the API server. For example, in Minikube the API server secure port is 8443 and therefore API_SERVER_ENDPOINT_URL should be set to https://localhost:8443 OpenShift configuration Control plane components on OpenShift 4.x use endpoint URLs that require SSL and service account based authentication. Therefore, the default endpoint URLs can not be used. To configure control plane monitoring on OpenShift, uncomment the following environment variables in the customized manifest. URL values are pre-configured to the default base URLs for the control plane monitoring metrics endpoints in OpenShift 4.x. - name: \"SCHEDULER_ENDPOINT_URL\" value: \"https://localhost:10259 - name: \"ETCD_ENDPOINT_URL\" value: \"https://localhost:9979\" - name: \"CONTROLLER_MANAGER_ENDPOINT_URL\" value: \"https://localhost:10257\" - name: \"API_SERVER_ENDPOINT_URL\" value: \"https://localhost:6443\" Copy Important Even though the custom ETCD_ENDPOINT_URL is defined, ETCD requires HTTPS and mTLS authentication to be configured. For more on configuring mTLS for ETCD in OpenShift, see Set up mTLS for ETCD in OpenShift. Important When installing through Helm openshift, specify the config to automatically include these endpoints. Setting openshift.enabled=true and openshift.version=\"4.x\" will include the secure endpoints and enable the /var/run/crio.sock runtime. Set up mTLS from the ETCD client CA The instructions below are based on the Kubernetes documentation. For more information, see Managing TLS certificates in a cluster. For OpenShift, see Set up mTLS for ETCD in OpenShift. To set up mTLS from the ETCD client CA: Download and install the tool cfssl, selecting the correct binaries for your OS from the list. Once installed, execute the following command: cat <<EOF | cfssl genkey - | cfssljson -bare server { \"hosts\": [ \"localhost\" ], \"CN\": \"newrelic-infra.pod.cluster.local\", \"key\": { \"algo\": \"ecdsa\", \"size\": 256 } } EOF Copy This command generates two files; server.csr containing the PEM encoded pkcs#10 certification request and server-key.pem containing the PEM encoded key to the certificate to be created. Use the generated certificate authority (CA) of ETCD to sign your CSR. Depending on your cluster configuration, you may already have this information. For default install configuration, download the CA certificate and the private key directly from ETCD with the following commands: kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.crt ./cacert -n kube-system kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.key ./cacert.key -n kube-system Copy Tip This requires that the etcd-manager-main pod has the label k8s-app=etcd-manager-main, which is a requirement for control plane monitoring . If your etc-manager-main pod is located in a different namespace, change the -n kube-system flags accordingly. With those files downloaded, use the following command to sign your CSRF: cfssl sign -ca cacert -ca-key cacert.key server.csr | cfssljson -bare cert Copy Create the secret that is used to retrieve the TLS config for making requests to ETC. We recommend renaming the certificate and the private key: cp cert.pem cert && cp server-key.pem key Copy kubectl -n default create secret generic newrelic-infra-etcd-tls-secret --from-file=./cert --from-file=./key --from-file=./cacert Copy To ease future installations Use the following commands to simultaneously create the CSR, retrieve the CA, generate the certificate by signing the CSR, and create the secret with all the required fields: cat <<EOF | cfssl genkey - | cfssljson -bare server && \\ kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.crt ./cacert -n kube-system && \\ kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.key ./cacert.key -n kube-system && \\ cp server-key.pem key && \\ cfssl sign -ca cacert -ca-key cacert.key server.csr | cfssljson -bare cert && \\ cp cert.pem cert && \\ kubectl -n default create secret generic newrelic-infra-etcd-tls-secret --from-file=./cert --from-file=./key --from-file=./cacert { \"hosts\": [ \"localhost\" ], \"CN\": \"newrelic-infra.pod.cluster.local\", \"key\": { \"algo\": \"ecdsa\", \"size\": 256 } } EOF Copy The last step is to update the configuration in the manifest and apply it. In the configuration section, there are two options related to ETCD mTLS: ETCD_TLS_SECRET_NAME with the name of the secret that we just created. ETCD_TLS_SECRET_NAMESPACE with the namespace that we used to create the secret. To complete the installation, add these variables to the container spec of the integration DaemonSet and apply the changes: - name: \"ETCD_TLS_SECRET_NAME” value: \"newrelic-infra-etcd-tls-secret\" - name: \"ETCD_TLS_SECRET_NAMESPACE\" value: \"default\" Copy Set up mTLS for ETCD in OpenShift Follow these instructions to set up mutual TLS authentication for ETCD in OpenShift 4.x: Export the ETCD client certificates from the cluster to an opaque secret. In a default managed OpenShift cluster, the secret is named kube-etcd-client-certs and it is stored in the openshift-monitoring namespace. kubectl get secret/kube-etcd-client-certs -n openshift-monitoring -o yaml > etcd-secret.yaml Copy Open the secret file and change the keys: Rename the certificate authority to cacert. Rename the client certificate to cert. Rename the client key to key. Optional: change the secret name and namespace to something meaningful. Remove these unnecessary keys in the metadata section: creationTimestamp resourceVersion selfLink uid Install the manifest with its new name and namespace: kubectl apply -f etcd-secret.yaml Copy Go to Update manifest configuration (the last step under Set up MTL from ETCD client) to configure the required environment variables. See your data If the integration has been been set up correctly, the Kubernetes cluster explorer contains all the Control Plane components and their status in a dedicated section, as shown below. one.newrelic.com > Kubernetes Cluster Explorer: Use the Kubernetes cluster explorer to monitor and collect metrics from your cluster's Control Plane components You can also check for Control Plane data with this NRQL query: SELECT latest(timestamp) FROM K8sApiServerSample, K8sEtcdSample, K8sSchedulerSample, K8sControllerManagerSample FACET entityName where clusterName = 'MY_CLUSTER_NAME' Copy Tip If you still can't see Control Plane data, try the solution described in Kubernetes integration troubleshooting: Not seeing data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 125.899605,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "To ease future <em>installations</em>",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "New Relic provides Control Plane support for your <em>Kubernetes</em> <em>integration</em>, allowing you to monitor and collect metrics from your cluster&#x27;s Control Plane components. That data can then be found in New Relic and used to create queries and charts. Features We monitor and collect metrics from"
      },
      "id": "617d735f64441f9d39fbe7ee"
    },
    {
      "sections": [
        "Kubernetes integration: install and configure",
        "Use automated installer",
        "Installs for managed services and platforms",
        "Amazon EKS / EKS-Anywhere",
        "Amazon EKS Fargate",
        "Google Kubernetes Engine (GKE)",
        "OpenShift container platform",
        "Azure Kubernetes Service (AKS)",
        "Pivotal Container Service (PKS / VMware Tanzu)",
        "Upgrading our Kubernetes integration",
        "Upgrading using the automated installer and Helm",
        "Upgrading using the automated installer and plain manifests",
        "Tip",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data"
      ],
      "title": "Kubernetes integration: install and configure",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "b1306d1b3a01711286cf78f26008571c5c3036cf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/kubernetes-integration-install-configure/",
      "published_at": "2022-01-08T08:24:04Z",
      "updated_at": "2021-11-25T03:49:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The easiest way to install the Kubernetes integration is to use our automated installer to generate a manifest. It bundles not just the integration DaemonSets, but also other New Relic Kubernetes configurations, like Kubernetes events, Prometheus OpenMetrics, and New Relic log monitoring. As a Kubernetes user, you might consider Auto-telemetry with Pixie for deeper insights into your Kubernetes clusters and workloads. No language agents required. Ready to get started? If you don't already have one, sign up for a New Relic account. It's free, forever! Use automated installer We encourage you to use our automated installer for servers, VMs, and unprivileged environments. The automated installer can provide you either a Helm command with the required values filled, or a plain manifest if you don't want to use Helm. It also features great customizability and full control over which features and dependencies are enabled. If you are installing our integration on a managed cloud, please take a look at these preliminary notes before proceeding. Before you start the install, make sure you have a New Relic account. It's free! No credit card required. If your New Relic account is in the EU region, choose the EU install option. Get an account Start the installer Eu guided install Or, for fully manual instructions for deploying our integration using Helm. Installs for managed services and platforms Before starting our automated installer, check out these notes for your managed services or platforms: Amazon EKS / EKS-Anywhere The Kubernetes integration monitors worker nodes. In Amazon EKS, master nodes are managed by Amazon and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration in Amazon EKS, make sure you are using the version of kubectl provided by AWS. Amazon EKS Fargate Installation on EKS Fargate clusters requires dedicated steps, which are detailed in our fargate installation docs. Google Kubernetes Engine (GKE) The Kubernetes integration monitors worker nodes. In GKE, master nodes are managed by Google and abstracted from the Kubernetes platforms. Before starting our automated installer to deploy the Kubernetes integration on GKE, ensure you have sufficient permissions: Go to console.cloud.google.com/iam-admin/iam and find your username. Click edit. Ensure you have permissions to create Roles and ClusterRoles: If you are not sure, add the Kubernetes Engine Cluster Admin role. If you cannot edit your user role, ask the owner of the GCP project to give you the necessary permissions. OpenShift container platform To deploy the Kubernetes integration with OpenShift: Add the <>{'<release_name>'}</>-newrelic-infrastructure service account to your privileged Security Context Constraints: oc adm policy add-scc-to-user privileged \\ system:serviceaccount:<namespace>:<release_name>-newrelic-infrastructure Copy The default <>{'<release_name>'}</> provided by the installer is nri-bundle. Complete the steps in our automated installer. If you're using signed certificates, make sure they are properly configured by using the following variables in the DaemonSet portion of your manifest to set the .pem file: Copy name: NRIA_CA_BUNDLE_DIR value: YOUR_CA_BUNDLE_DIR name: NRIA_CA_BUNDLE_FILE value: YOUR_CA_BUNDLE_NAME YAML key path: `spec.template.spec.containers.name.env` Copy Save your changes. Azure Kubernetes Service (AKS) The Kubernetes integration monitors worker nodes. In Azure Kubernetes Service, master nodes are managed by Azure and abstracted from the Kubernetes platforms. To deploy in Azure Kubernetes Service (AKS), complete the steps in our automated installer. Pivotal Container Service (PKS / VMware Tanzu) To deploy in PKS, we recommend that you use the automated installer, or you can follow the manual instructions provided in Install the Kubernetes integration using Helm. Upgrading our Kubernetes integration Our Kubernetes integration is under active development and we regularly release updates which include bug fixes, new features, and support for newer Kubernetes versions and cloud providers. We strongly recommend all our customers to regularly update the Kubernetes integration to get the best experience. Upgrading using the automated installer and Helm In order to update an installation that was deployed using Helm command provided by the automated installer, just go through the process and run the Helm command again. This will pull the new version of the chart and its dependencies and upgrade it to the latest version. Upgrading using the automated installer and plain manifests Tip We encourage you to deploy our integration using Helm, as it provides a cleaner upgrade path compared to using manifests directly. If custom manifests have been used instead of Helm, we encourage you to first remove the old installation using kubectl delete -f previous-manifest-file.yml, and then proceed through the guided installer again. This will generate an updated set of manifests that can be deployed using kubectl apply -f manifest-file.yml. We do not recommend applying a new version of the manifest file without removing the previous one first, since it might leave some leftover components in your cluster. Monitor services running on Kubernetes Tip We encourage you to deploy our integration using Helm, as it allows easier configuration on how to monitor services by just adding snippets to your values.yml file. After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 118.31843,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Kubernetes</em> <em>integration</em>: <em>install</em> and configure",
        "sections": "<em>Kubernetes</em> <em>integration</em>: <em>install</em> and configure",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": " the version of kubectl provided by AWS. Amazon EKS Fargate <em>Installation</em> on EKS Fargate clusters requires dedicated steps, which are detailed in our fargate <em>installation</em> docs. Google <em>Kubernetes</em> Engine (GKE) The <em>Kubernetes</em> <em>integration</em> monitors worker nodes. In GKE, master nodes are managed by Google"
      },
      "id": "617daa4fe7b9d26e59c05d34"
    }
  ],
  "/docs/kubernetes-pixie/kubernetes-integration/installation/install-kubernetes-integration-windows": [
    {
      "sections": [
        "Install the Kubernetes integration manually using Helm",
        "Compatibility and requirements",
        "Install Kubernetes integration with Helm",
        "Install with Helm 3 and nri-bundle (recommended)",
        "Installing and configuring nri-bundle with Helm",
        "Install with Helm 2 and nri-bundle (legacy)",
        "Installation instructions for Helm 2",
        "Important",
        "Tip",
        "Helm configuration options",
        "Upgrade using Helm",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data",
        "Reduce data ingest",
        "Prometheus OpenMetrics Integration",
        "New Relic Logging",
        "New Relic Pixie Integration",
        "Uninstall Kubernetes integration"
      ],
      "title": "Install the Kubernetes integration manually using Helm",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "29d42af98d41e7e4e7be86f0150254e132cb6b6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/install-kubernetes-integration-using-helm/",
      "published_at": "2022-01-08T09:49:44Z",
      "updated_at": "2022-01-08T09:49:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Helm is a package manager on top of Kubernetes. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in Kubernetes. To install the integration using Helm, we recommend our Kubernetes automated installer, which will prompt for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our integration as plain manifests rather than a Helm release. See Kubernetes integration: install and configure for more details about how to use our automated installer. Start the installer This page describes in more depth how to install and configure the New Relic integration without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the Kubernetes integration. Our charts are also compatible with Helm 2, but support for it might be removed in the future. To install the Kubernetes integration using Helm, you will need your New Relic account license key and your Kubernetes cluster's name: Find and copy your New Relic license key. Find the name of your cluster with this command: kubectl config current-context Copy Note these values somewhere safe, as you will need them later during the installation process. Install Kubernetes integration with Helm Install with Helm 3 and nri-bundle (recommended) New Relic has several charts for the different components which offer different features for the platform: newrelic-infrastructure: Contains the main Kubernetes integration and the infrastructure agent. This is the core component for the New Relic Kubernetes experience, responsible of reporting most of the data that is surfaced in dashboard and the Kubernetes Cluster Explorer. newrelic-logging: Provides a DaemonSet with New Relic's Fluent Bit output plugin to easily forward your logs to New Relic. nri-kube-events: Collects and reports cluster events (such as kubectl get events) to New Relic. nri-prometheus: New Relic's Prometheus OpenMetrics Integration, automatically scrapes Prometheus endpoints present in the cluser and reports metrics to New Relic. nri-metadata-injection: Sets up a minimal MutatingAdmissionWebhook that injects a couple of environment to containers. These contain metadata about the cluster and New Relic installation and will be later picked up by applications instrumented using APM, allowing to correlate APM and infrastructure data. nri-statsd: New Relic StatsD integration. Additionally, New Relic provides nri-bundle, a chart which pulls a selectable set of the charts mentioned above. nri-bundle can also install Kube State Metrics and Pixie for you if needed. While it is possible to install those charts separately, we strongly recommend using the nri-bundle chart for Kubernetes deployments, as it ensures that values across all the charts are consistent and provides full control over which components are installed, as well as the possibility to configure all of them as Helm dependencies. This is the same chart that is used and referenced by our automated installer. Installing and configuring nri-bundle with Helm Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Please notice and adjust the following flags: global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY: Must be set to a valid License Key for your account. global.cluster=K8S_CLUSTER_NAME: Is used to identify the cluster in the New Relic UI, so should be a descriptive value not used by any other Kubernetes cluster configured in your New Relic account. ksm.enabled=true: Setting this to true will automatically install Kube State Metrics (KSM) for you, which is required for our integration to run. You can set this to false if KSM is already present in your cluster, even if it is on a different namespace. newrelic-infrastructure.privileged=true: Can be set to false to install a trimmed down version of our integration that does not require extra privileges, such as hostPath mounts or running containers as root. Please note that this will disable detailed process collection from the host. For performance reasons, our logging solution still requires hostPath mounts, regardless of the value of this flag. If this is not allowedin your cluster, you will need to disable the logging solution by specifying logging.enabled=false. prometheus.enabled=true: Will deploy our Prometheus OpenMetrics integration, which automatically collects data from prometheus endpoints present in the cluster. webhook.enabled=true: Will install our minimal webhook, which adds environment variables that, in turn, allows linking applications instrumented with APM with infrastructure data. Our chart has a comprehensive set of flags and tunables that can be edited to configure our solution to your particular needs. For a full list of all the flags that can be configured, please check the chart's README and the default values.yaml file. Install the Kubernetes integration by running the customized command without --debug and --dry-run: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl -n newrelic get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node for newrelic-infrastructure, as well as one Deployment and one pod for any other component that you have enabled. Install with Helm 2 and nri-bundle (legacy) Installation instructions for Helm 2 Important Helm 2 has been deprecated and New Relic does not recommend using it for deployments. Instructions in this section are provided for legacy systems only. To install using Helm 2: Ensure that Helm 2, including their cluster-side components (such as Tiller) are properly installed and configured. Please check the official documentation for more details. Set the cluster where you want to install the agent: kubectl config set-cluster DESIRED_CLUSTER Copy To see the available clusters, run kubectl config get-clusters Make sure that kube-state-metrics is installed on your machine: kubectl get deployment --all-namespaces | grep kube-state-metrics Copy If it's not installed, follow the instructions in the kube-state-metrics GitHub repo to install it. Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. This step uses the --dry-run and --debug switches and therefore the agent is not installed. helm install newrelic/newrelic-infrastructure \\ --dry-run \\ --debug \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Install the New Relic Kubernetes integration: helm install newrelic/newrelic-infrastructure \\ --set licenseKey=your_new_relic_license_key \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Tip Note that the --dry-run and --debug switches have been removed. Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node. Helm configuration options When you install or upgrade the Kubernetes integration with Helm using the command line, you can pass your configuration variables with the --set flag. helm install newrelic/newrelic-infrastructure \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=YOUR_CLUSTER_NAME Copy A full list of the configuration parameters can be found in the newrelic-infrastructure chart README Upgrade using Helm To update your Kubernetes integration installed via Helm: Update the local chart repository: helm repo update Copy Update the release by running again the appropriate helm upgrade --install ... command in the section above. Monitor services running on Kubernetes After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages. Reduce data ingest Our charts support setting an option to reduce the amount of data ingest at the cost of dropping detailed information. To enable it, set global.lowDataMode=true in the nri-bundle chart. lowDataMode affects three specific components of the nri-bundle chart outlined below. Prometheus OpenMetrics Integration If lowDataMode is enabled, the following metrics are excluded by default as they are already collected and used by the New Relic Kubernetes Integration. - kube_ - container_ - machine_ - cadvisor_ Copy New Relic Logging If lowDataMode is enabled, Labels and Annotations are set to Off in the Filter section of the fluent-bit.conf file. This means that this detail will be dropped from the container log files which reduces the overall data ingest into New Relic. The following fields are retained: Allowlist_key container_name Allowlist_key namespace_name Allowlist_key pod_name Allowlist_key stream Allowlist_key log Copy Low Data Mode Log Example Complete Log Record [ { \"cluster_name\": \"api-test\", \"kubernetes\": { \"annotations\": { \"kubernetes.io/psp\": \"eks.privileged\" }, \"container_hash\": \"fryckbos/test@sha256:5b098eaf3c7d5b3585eb10cebee63665b6208bea31ef31a3f0856c5ffdda644b\", \"container_image\": \"fryckbos/test:latest\", \"container_name\": \"newrelic-logging\", \"docker_id\": \"134e1daf63761baa15e035b08b7aea04518a0f0e50af4215131a50c6a379a072\", \"host\": \"ip-192-168-17-123.ec2.internal\", \"labels\": { \"app\": \"newrelic-logging\", \"app.kubernetes.io/name\": \"newrelic-logging\", \"controller-revision-hash\": \"84db95db86\", \"pod-template-generation\": \"1\", \"release\": \"nri-bundle\" }, \"namespace_name\": \"nrlogs\", \"pod_id\": \"54556e3e-719c-46b5-af69-020b75d69bf1\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\" }, \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"plugin\": { \"source\": \"kubernetes\", \"type\": \"fluent-bit\", \"version\": \"1.8.1\" }, \"stream\": \"stderr\", \"time\": \"2021-09-14T12:30:49.138824971Z\", \"timestamp\": 1631622649138 } ] Copy Log Record after enabling lowDataMode. [ { \"cluster_name\": \"api-test\", \"container_name\": \"newrelic-logging\", \"namespace_name\": \"nrlogs\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\", \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"stream\": \"stderr\", \"timestamp\": 1631622649138 } ] Copy New Relic Pixie Integration If lowDataMode is enabled, the newrelic-pixie integration performs heavier sampling on Pixie spans and reduces the collection interval from 10 seconds to 15 seconds. lowDataMode settings: HTTP_SPAN_LIMIT: 750 DB_SPAN_LIMIT: 250 COLLECT_INTERVAL_SEC: 15 Copy The default settings for these parameters and others can be found in the newrelic-pixie-integration Github repo. Uninstall Kubernetes integration To uninstall the Kubernetes integration using Helm, run the following command. Replace some-manifest-file.yml with your specific filename. helm uninstall newrelic -n newrelic if kubectl then : kubectl delete -f some-manifest-file.yml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 218.82104,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "sections": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "Helm is a package manager on top of <em>Kubernetes</em>. It facilitates <em>installation</em>, upgrades, or revision tracking, and it manages dependencies for the services that you install in <em>Kubernetes</em>. To install the <em>integration</em> using Helm, we recommend our <em>Kubernetes</em> automated installer, which will prompt"
      },
      "id": "617d5841196a67bb40f7c1de"
    },
    {
      "sections": [
        "Configure Kubernetes with a proxy",
        "Install the infrastructure agent with a proxy",
        "Set logging with a proxy",
        "Install Kubernetes events with a proxy",
        "Install the Prometheus OpenMetrics integration with a proxy",
        "Set the synthetics minion to use a proxy"
      ],
      "title": "Configure Kubernetes with a proxy",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "9d7794244b451d878d00db0f84c4709e43ce138c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/configure-kubernetes-proxy/",
      "published_at": "2022-01-08T09:49:01Z",
      "updated_at": "2021-12-25T07:57:45Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are running our Kubernetes integration with a proxy, you need to configure each component (the infrastructure agent, logging, Kubernetes events, etc.) to correctly work with your proxy. Install the infrastructure agent with a proxy There are three options to install the infrastructure agent with a proxy: Installing with chart newrelic-infrastructure: The proxy can be configured setting the configuration option config.proxy as described in the values.yaml of the chart. The whole config object is used to generate a configMap that is mounted as the file /etc/newrelic-infra.yml used to configure the agent. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option newrelic-infrastructure.config.proxy. The configuration option is passed down to the dependency newrelic-infrastructure modifying the values.yaml of nri-bundle: newrelic-infrastructure.config.proxy: https://user:password@hostname:port Copy Installing with manifest: When installing through the manifest, the proxy can be configured adding the NRIA_PROXY environment variable to the environment variable section of the DaemonSet configuration. The environment variable NRIA_VERBOSE in the DaemonSet YAML can be used as an example of the environment variable configuration. Set logging with a proxy If you’re using log forwarding for the Kubernetes integration, the plugin automatically detects the HTTP_PROXY and HTTPS_PROXY environment variables, and automatically uses them to set up the proxy configuration. For more information, see how to customize the proxy, or bypass it. Install Kubernetes events with a proxy There are three options to install the Kubernetes events integration with a proxy: Installing with chart nri-kube-events: The proxy can be configured setting the configuration option proxy in the values.yaml file. If set, the variable NRIA_PROXY will be added into the deployment.yaml template. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option nri-kube-events.proxy. The configuration option is passed down to the dependency nri-kube-events modifying the values.yaml of nri-bundle: nri-kube-events.proxy: https://user:password@hostname:port Copy Installing with manifest: When installing through the manifest, to configure the proxy set the NRIA_PROXY in the environment variable section of the infra container. Check the deployment template of the chart as an example of environment variable configuration. Install the Prometheus OpenMetrics integration with a proxy There are three options to install the Prometheus OpenMetric integration with a proxy: Installing with charts nri-prometheus: The proxy can be configured setting the configuration option config.emitter_proxy in the values.yaml. The config object is used to generate a configMap that mounted into the deployment under /etc/nri-prometheus/. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option nri-prometheus.config.emitter_proxy. The configuration option is passed down to the dependency nri-prometheus modifying the values.yaml of nri-bundle: nri-prometheus.config.emitter_proxy: \"http://localhost:8888 Copy Installing with the manifest: Configure the proxy uncommenting the emitter_proxy option directly in the manifest: # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" Copy Set the synthetics minion to use a proxy Check this README on how to set the proxy for the Synthetics minions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 144.90121,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure <em>Kubernetes</em> with a proxy",
        "sections": "<em>Install</em> <em>Kubernetes</em> events with a proxy",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "If you are running our <em>Kubernetes</em> <em>integration</em> with a proxy, you need to configure each component (the infrastructure agent, logging, <em>Kubernetes</em> events, etc.) to correctly work with your proxy. Install the infrastructure agent with a proxy There are three options to install the infrastructure agent"
      },
      "id": "617d584264441f2d01fbc6d7"
    },
    {
      "sections": [
        "Configure control plane monitoring",
        "Features",
        "Compatibility and requirements",
        "Discovery of master nodes and control plane components",
        "Configuration",
        "Important",
        "ETCD",
        "API server",
        "OpenShift configuration",
        "Set up mTLS from the ETCD client CA",
        "Tip",
        "To ease future installations",
        "Set up mTLS for ETCD in OpenShift",
        "See your data"
      ],
      "title": "Configure control plane monitoring",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "d648be240fd2f1b09bc1f29ef805bc0dd5c59e7a",
      "image": "https://docs.newrelic.com/static/209f301630c770f87ea8cbb1cace8e6e/8c557/new-relic-one-k8s-cluster-explorer-control-plane-parameters.png",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/configure-control-plane-monitoring/",
      "published_at": "2022-01-08T09:49:01Z",
      "updated_at": "2021-12-09T15:16:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic provides Control Plane support for your Kubernetes integration, allowing you to monitor and collect metrics from your cluster's Control Plane components. That data can then be found in New Relic and used to create queries and charts. Features We monitor and collect metrics from the following control plane components: ETCD: leader information, resident memory size, number of OS threads, consensus proposals data, etc. For a list of supported metrics, see ETCD data. API server: rate of apiserver requests, breakdown of apiserver requests by HTTP method and response code, etc. For the complete list of supported metrics, see API server data. Scheduler: requested CPU/memory vs available on the node, tolerations to taints, any set affinity or anti-affinity, etc. For the complete list of supported metrics, see Scheduler data. Controller manager: resident memory size, number of OS threads created, goroutines currently existing, etc. For the complete list of supported metrics, see Controller manager data. Compatibility and requirements Control plane monitoring requires Kubernetes integration version 1.16.0 or higher. Control plane monitoring support is not enabled for managed clusters. This is because providers (EKS, GKE, AKS, etc.) abstract away the concept of master nodes and control plane components, so that access to them is limited or non-existent. External control planes are not supported. The unprivileged version of the Kubernetes integration does not support control plane monitoring. OpenShift 4.x uses control plane component metric endpoints that are different than the default. Discovery of master nodes and control plane components The Kubernetes integration relies on the kubeadm labeling conventions to discover the master nodes and the control plane components. This means that master nodes should be labeled with node-role.kubernetes.io/master=\"\" or kubernetes.io/role=\"master\". The control plane components should have either the k8s-app or the tier and component labels. Refer to the following table for accepted label combinations and values: Component Label Endpoint API server Kubeadm / Kops / ClusterAPI k8s-app=kube-apiserver tier=control-plane component=kube-apiserver OpenShift app=openshift-kube-apiserver apiserver=true localhost:443/metrics by default (can be configured) if the request fails falls back to localhost:8080/metrics ETCD Kubeadm / Kops / ClusterAPI k8s-app=etcd-manager-main tier=control-plane component=etcd OpenShift k8s-app=etcd localhost:4001/metrics Scheduler Kubeadm / Kops / ClusterAPI k8s-app=kube-scheduler tier=control-plane component=kube-scheduler OpenShift app=openshift-kube-scheduler scheduler=true localhost:10251/metrics Controller manager Kubeadm / Kops / ClusterAPI k8s-app=kube-controller-manager tier=control-plane component=kube-controller-manager​ OpenShift app=kube-controller-manager kube-controller-manager=true localhost:10252/metrics When the integration detects that it is running inside a master node, it tries to find which components are running on the node by looking for pods that match the labels listed in the table above. For every running component, the integration makes a request to its metrics endpoint. Configuration Control plane monitoring is automatic for agents running inside master nodes. The only component that requires an extra step to run is ETCD, because it uses mutual TLS authentication (mTLS) for client requests. The API Server can also be configured to be queried using the Secure Port. Important Control plane monitoring for OpenShift 4.x requires additional configuration. For more information, see the OpenShift 4.x Configuration section. ETCD In order to set mTLS for querying ETCD, there are two configuration options that need to be set: Option Value ETCD_TLS_SECRET_NAME Name of a Kubernetes secret that contains the mTLS configuration. The secret should contain the following keys: cert: the certificate that identifies the client making the request. It should be signed by an ETCD trusted CA. key: the private key used to generate the client certificate. cacert: the root CA used to identify the ETCD server certificate. If the ETCD_TLS_SECRET_NAME option is not set, ETCD metrics won't be fetched. For step by step instructions on how to create a certificate and sign it with the ETCD client CA, see Set up mTLS from the ETCD client CA. ETCD_TLS_SECRET_NAMESPACE The namespace where the secret specified in the ETCD_TLS_SECRET_NAME was created. If not set, the default namespace is used. API server By default, the API server metrics are queried using the localhost:8080 unsecured endpoint. If this port is disabled, you can also query these metrics over the secure port. To enable this, set the following configuration option in the Kubernetes integration manifest file: Option Value API_SERVER_ENDPOINT_URL The (secure) URL to query the metrics. The API server uses localhost:443 by default Ensure that the ClusterRole has been updated to the newest version found in the manifest Added in version 1.15.0 Important Note that the port can be different according to the secure port used by the API server. For example, in Minikube the API server secure port is 8443 and therefore API_SERVER_ENDPOINT_URL should be set to https://localhost:8443 OpenShift configuration Control plane components on OpenShift 4.x use endpoint URLs that require SSL and service account based authentication. Therefore, the default endpoint URLs can not be used. To configure control plane monitoring on OpenShift, uncomment the following environment variables in the customized manifest. URL values are pre-configured to the default base URLs for the control plane monitoring metrics endpoints in OpenShift 4.x. - name: \"SCHEDULER_ENDPOINT_URL\" value: \"https://localhost:10259 - name: \"ETCD_ENDPOINT_URL\" value: \"https://localhost:9979\" - name: \"CONTROLLER_MANAGER_ENDPOINT_URL\" value: \"https://localhost:10257\" - name: \"API_SERVER_ENDPOINT_URL\" value: \"https://localhost:6443\" Copy Important Even though the custom ETCD_ENDPOINT_URL is defined, ETCD requires HTTPS and mTLS authentication to be configured. For more on configuring mTLS for ETCD in OpenShift, see Set up mTLS for ETCD in OpenShift. Important When installing through Helm openshift, specify the config to automatically include these endpoints. Setting openshift.enabled=true and openshift.version=\"4.x\" will include the secure endpoints and enable the /var/run/crio.sock runtime. Set up mTLS from the ETCD client CA The instructions below are based on the Kubernetes documentation. For more information, see Managing TLS certificates in a cluster. For OpenShift, see Set up mTLS for ETCD in OpenShift. To set up mTLS from the ETCD client CA: Download and install the tool cfssl, selecting the correct binaries for your OS from the list. Once installed, execute the following command: cat <<EOF | cfssl genkey - | cfssljson -bare server { \"hosts\": [ \"localhost\" ], \"CN\": \"newrelic-infra.pod.cluster.local\", \"key\": { \"algo\": \"ecdsa\", \"size\": 256 } } EOF Copy This command generates two files; server.csr containing the PEM encoded pkcs#10 certification request and server-key.pem containing the PEM encoded key to the certificate to be created. Use the generated certificate authority (CA) of ETCD to sign your CSR. Depending on your cluster configuration, you may already have this information. For default install configuration, download the CA certificate and the private key directly from ETCD with the following commands: kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.crt ./cacert -n kube-system kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.key ./cacert.key -n kube-system Copy Tip This requires that the etcd-manager-main pod has the label k8s-app=etcd-manager-main, which is a requirement for control plane monitoring . If your etc-manager-main pod is located in a different namespace, change the -n kube-system flags accordingly. With those files downloaded, use the following command to sign your CSRF: cfssl sign -ca cacert -ca-key cacert.key server.csr | cfssljson -bare cert Copy Create the secret that is used to retrieve the TLS config for making requests to ETC. We recommend renaming the certificate and the private key: cp cert.pem cert && cp server-key.pem key Copy kubectl -n default create secret generic newrelic-infra-etcd-tls-secret --from-file=./cert --from-file=./key --from-file=./cacert Copy To ease future installations Use the following commands to simultaneously create the CSR, retrieve the CA, generate the certificate by signing the CSR, and create the secret with all the required fields: cat <<EOF | cfssl genkey - | cfssljson -bare server && \\ kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.crt ./cacert -n kube-system && \\ kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.key ./cacert.key -n kube-system && \\ cp server-key.pem key && \\ cfssl sign -ca cacert -ca-key cacert.key server.csr | cfssljson -bare cert && \\ cp cert.pem cert && \\ kubectl -n default create secret generic newrelic-infra-etcd-tls-secret --from-file=./cert --from-file=./key --from-file=./cacert { \"hosts\": [ \"localhost\" ], \"CN\": \"newrelic-infra.pod.cluster.local\", \"key\": { \"algo\": \"ecdsa\", \"size\": 256 } } EOF Copy The last step is to update the configuration in the manifest and apply it. In the configuration section, there are two options related to ETCD mTLS: ETCD_TLS_SECRET_NAME with the name of the secret that we just created. ETCD_TLS_SECRET_NAMESPACE with the namespace that we used to create the secret. To complete the installation, add these variables to the container spec of the integration DaemonSet and apply the changes: - name: \"ETCD_TLS_SECRET_NAME” value: \"newrelic-infra-etcd-tls-secret\" - name: \"ETCD_TLS_SECRET_NAMESPACE\" value: \"default\" Copy Set up mTLS for ETCD in OpenShift Follow these instructions to set up mutual TLS authentication for ETCD in OpenShift 4.x: Export the ETCD client certificates from the cluster to an opaque secret. In a default managed OpenShift cluster, the secret is named kube-etcd-client-certs and it is stored in the openshift-monitoring namespace. kubectl get secret/kube-etcd-client-certs -n openshift-monitoring -o yaml > etcd-secret.yaml Copy Open the secret file and change the keys: Rename the certificate authority to cacert. Rename the client certificate to cert. Rename the client key to key. Optional: change the secret name and namespace to something meaningful. Remove these unnecessary keys in the metadata section: creationTimestamp resourceVersion selfLink uid Install the manifest with its new name and namespace: kubectl apply -f etcd-secret.yaml Copy Go to Update manifest configuration (the last step under Set up MTL from ETCD client) to configure the required environment variables. See your data If the integration has been been set up correctly, the Kubernetes cluster explorer contains all the Control Plane components and their status in a dedicated section, as shown below. one.newrelic.com > Kubernetes Cluster Explorer: Use the Kubernetes cluster explorer to monitor and collect metrics from your cluster's Control Plane components You can also check for Control Plane data with this NRQL query: SELECT latest(timestamp) FROM K8sApiServerSample, K8sEtcdSample, K8sSchedulerSample, K8sControllerManagerSample FACET entityName where clusterName = 'MY_CLUSTER_NAME' Copy Tip If you still can't see Control Plane data, try the solution described in Kubernetes integration troubleshooting: Not seeing data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 125.8996,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "To ease future <em>installations</em>",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "New Relic provides Control Plane support for your <em>Kubernetes</em> <em>integration</em>, allowing you to monitor and collect metrics from your cluster&#x27;s Control Plane components. That data can then be found in New Relic and used to create queries and charts. Features We monitor and collect metrics from"
      },
      "id": "617d735f64441f9d39fbe7ee"
    }
  ],
  "/docs/kubernetes-pixie/kubernetes-integration/installation/kubernetes-integration-install-configure": [
    {
      "sections": [
        "Install the Kubernetes integration manually using Helm",
        "Compatibility and requirements",
        "Install Kubernetes integration with Helm",
        "Install with Helm 3 and nri-bundle (recommended)",
        "Installing and configuring nri-bundle with Helm",
        "Install with Helm 2 and nri-bundle (legacy)",
        "Installation instructions for Helm 2",
        "Important",
        "Tip",
        "Helm configuration options",
        "Upgrade using Helm",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data",
        "Reduce data ingest",
        "Prometheus OpenMetrics Integration",
        "New Relic Logging",
        "New Relic Pixie Integration",
        "Uninstall Kubernetes integration"
      ],
      "title": "Install the Kubernetes integration manually using Helm",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "29d42af98d41e7e4e7be86f0150254e132cb6b6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/install-kubernetes-integration-using-helm/",
      "published_at": "2022-01-08T09:49:44Z",
      "updated_at": "2022-01-08T09:49:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Helm is a package manager on top of Kubernetes. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in Kubernetes. To install the integration using Helm, we recommend our Kubernetes automated installer, which will prompt for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our integration as plain manifests rather than a Helm release. See Kubernetes integration: install and configure for more details about how to use our automated installer. Start the installer This page describes in more depth how to install and configure the New Relic integration without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the Kubernetes integration. Our charts are also compatible with Helm 2, but support for it might be removed in the future. To install the Kubernetes integration using Helm, you will need your New Relic account license key and your Kubernetes cluster's name: Find and copy your New Relic license key. Find the name of your cluster with this command: kubectl config current-context Copy Note these values somewhere safe, as you will need them later during the installation process. Install Kubernetes integration with Helm Install with Helm 3 and nri-bundle (recommended) New Relic has several charts for the different components which offer different features for the platform: newrelic-infrastructure: Contains the main Kubernetes integration and the infrastructure agent. This is the core component for the New Relic Kubernetes experience, responsible of reporting most of the data that is surfaced in dashboard and the Kubernetes Cluster Explorer. newrelic-logging: Provides a DaemonSet with New Relic's Fluent Bit output plugin to easily forward your logs to New Relic. nri-kube-events: Collects and reports cluster events (such as kubectl get events) to New Relic. nri-prometheus: New Relic's Prometheus OpenMetrics Integration, automatically scrapes Prometheus endpoints present in the cluser and reports metrics to New Relic. nri-metadata-injection: Sets up a minimal MutatingAdmissionWebhook that injects a couple of environment to containers. These contain metadata about the cluster and New Relic installation and will be later picked up by applications instrumented using APM, allowing to correlate APM and infrastructure data. nri-statsd: New Relic StatsD integration. Additionally, New Relic provides nri-bundle, a chart which pulls a selectable set of the charts mentioned above. nri-bundle can also install Kube State Metrics and Pixie for you if needed. While it is possible to install those charts separately, we strongly recommend using the nri-bundle chart for Kubernetes deployments, as it ensures that values across all the charts are consistent and provides full control over which components are installed, as well as the possibility to configure all of them as Helm dependencies. This is the same chart that is used and referenced by our automated installer. Installing and configuring nri-bundle with Helm Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Please notice and adjust the following flags: global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY: Must be set to a valid License Key for your account. global.cluster=K8S_CLUSTER_NAME: Is used to identify the cluster in the New Relic UI, so should be a descriptive value not used by any other Kubernetes cluster configured in your New Relic account. ksm.enabled=true: Setting this to true will automatically install Kube State Metrics (KSM) for you, which is required for our integration to run. You can set this to false if KSM is already present in your cluster, even if it is on a different namespace. newrelic-infrastructure.privileged=true: Can be set to false to install a trimmed down version of our integration that does not require extra privileges, such as hostPath mounts or running containers as root. Please note that this will disable detailed process collection from the host. For performance reasons, our logging solution still requires hostPath mounts, regardless of the value of this flag. If this is not allowedin your cluster, you will need to disable the logging solution by specifying logging.enabled=false. prometheus.enabled=true: Will deploy our Prometheus OpenMetrics integration, which automatically collects data from prometheus endpoints present in the cluster. webhook.enabled=true: Will install our minimal webhook, which adds environment variables that, in turn, allows linking applications instrumented with APM with infrastructure data. Our chart has a comprehensive set of flags and tunables that can be edited to configure our solution to your particular needs. For a full list of all the flags that can be configured, please check the chart's README and the default values.yaml file. Install the Kubernetes integration by running the customized command without --debug and --dry-run: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl -n newrelic get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node for newrelic-infrastructure, as well as one Deployment and one pod for any other component that you have enabled. Install with Helm 2 and nri-bundle (legacy) Installation instructions for Helm 2 Important Helm 2 has been deprecated and New Relic does not recommend using it for deployments. Instructions in this section are provided for legacy systems only. To install using Helm 2: Ensure that Helm 2, including their cluster-side components (such as Tiller) are properly installed and configured. Please check the official documentation for more details. Set the cluster where you want to install the agent: kubectl config set-cluster DESIRED_CLUSTER Copy To see the available clusters, run kubectl config get-clusters Make sure that kube-state-metrics is installed on your machine: kubectl get deployment --all-namespaces | grep kube-state-metrics Copy If it's not installed, follow the instructions in the kube-state-metrics GitHub repo to install it. Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. This step uses the --dry-run and --debug switches and therefore the agent is not installed. helm install newrelic/newrelic-infrastructure \\ --dry-run \\ --debug \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Install the New Relic Kubernetes integration: helm install newrelic/newrelic-infrastructure \\ --set licenseKey=your_new_relic_license_key \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Tip Note that the --dry-run and --debug switches have been removed. Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node. Helm configuration options When you install or upgrade the Kubernetes integration with Helm using the command line, you can pass your configuration variables with the --set flag. helm install newrelic/newrelic-infrastructure \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=YOUR_CLUSTER_NAME Copy A full list of the configuration parameters can be found in the newrelic-infrastructure chart README Upgrade using Helm To update your Kubernetes integration installed via Helm: Update the local chart repository: helm repo update Copy Update the release by running again the appropriate helm upgrade --install ... command in the section above. Monitor services running on Kubernetes After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages. Reduce data ingest Our charts support setting an option to reduce the amount of data ingest at the cost of dropping detailed information. To enable it, set global.lowDataMode=true in the nri-bundle chart. lowDataMode affects three specific components of the nri-bundle chart outlined below. Prometheus OpenMetrics Integration If lowDataMode is enabled, the following metrics are excluded by default as they are already collected and used by the New Relic Kubernetes Integration. - kube_ - container_ - machine_ - cadvisor_ Copy New Relic Logging If lowDataMode is enabled, Labels and Annotations are set to Off in the Filter section of the fluent-bit.conf file. This means that this detail will be dropped from the container log files which reduces the overall data ingest into New Relic. The following fields are retained: Allowlist_key container_name Allowlist_key namespace_name Allowlist_key pod_name Allowlist_key stream Allowlist_key log Copy Low Data Mode Log Example Complete Log Record [ { \"cluster_name\": \"api-test\", \"kubernetes\": { \"annotations\": { \"kubernetes.io/psp\": \"eks.privileged\" }, \"container_hash\": \"fryckbos/test@sha256:5b098eaf3c7d5b3585eb10cebee63665b6208bea31ef31a3f0856c5ffdda644b\", \"container_image\": \"fryckbos/test:latest\", \"container_name\": \"newrelic-logging\", \"docker_id\": \"134e1daf63761baa15e035b08b7aea04518a0f0e50af4215131a50c6a379a072\", \"host\": \"ip-192-168-17-123.ec2.internal\", \"labels\": { \"app\": \"newrelic-logging\", \"app.kubernetes.io/name\": \"newrelic-logging\", \"controller-revision-hash\": \"84db95db86\", \"pod-template-generation\": \"1\", \"release\": \"nri-bundle\" }, \"namespace_name\": \"nrlogs\", \"pod_id\": \"54556e3e-719c-46b5-af69-020b75d69bf1\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\" }, \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"plugin\": { \"source\": \"kubernetes\", \"type\": \"fluent-bit\", \"version\": \"1.8.1\" }, \"stream\": \"stderr\", \"time\": \"2021-09-14T12:30:49.138824971Z\", \"timestamp\": 1631622649138 } ] Copy Log Record after enabling lowDataMode. [ { \"cluster_name\": \"api-test\", \"container_name\": \"newrelic-logging\", \"namespace_name\": \"nrlogs\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\", \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"stream\": \"stderr\", \"timestamp\": 1631622649138 } ] Copy New Relic Pixie Integration If lowDataMode is enabled, the newrelic-pixie integration performs heavier sampling on Pixie spans and reduces the collection interval from 10 seconds to 15 seconds. lowDataMode settings: HTTP_SPAN_LIMIT: 750 DB_SPAN_LIMIT: 250 COLLECT_INTERVAL_SEC: 15 Copy The default settings for these parameters and others can be found in the newrelic-pixie-integration Github repo. Uninstall Kubernetes integration To uninstall the Kubernetes integration using Helm, run the following command. Replace some-manifest-file.yml with your specific filename. helm uninstall newrelic -n newrelic if kubectl then : kubectl delete -f some-manifest-file.yml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 218.82104,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "sections": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "Helm is a package manager on top of <em>Kubernetes</em>. It facilitates <em>installation</em>, upgrades, or revision tracking, and it manages dependencies for the services that you install in <em>Kubernetes</em>. To install the <em>integration</em> using Helm, we recommend our <em>Kubernetes</em> automated installer, which will prompt"
      },
      "id": "617d5841196a67bb40f7c1de"
    },
    {
      "sections": [
        "Configure Kubernetes with a proxy",
        "Install the infrastructure agent with a proxy",
        "Set logging with a proxy",
        "Install Kubernetes events with a proxy",
        "Install the Prometheus OpenMetrics integration with a proxy",
        "Set the synthetics minion to use a proxy"
      ],
      "title": "Configure Kubernetes with a proxy",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "9d7794244b451d878d00db0f84c4709e43ce138c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/configure-kubernetes-proxy/",
      "published_at": "2022-01-08T09:49:01Z",
      "updated_at": "2021-12-25T07:57:45Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are running our Kubernetes integration with a proxy, you need to configure each component (the infrastructure agent, logging, Kubernetes events, etc.) to correctly work with your proxy. Install the infrastructure agent with a proxy There are three options to install the infrastructure agent with a proxy: Installing with chart newrelic-infrastructure: The proxy can be configured setting the configuration option config.proxy as described in the values.yaml of the chart. The whole config object is used to generate a configMap that is mounted as the file /etc/newrelic-infra.yml used to configure the agent. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option newrelic-infrastructure.config.proxy. The configuration option is passed down to the dependency newrelic-infrastructure modifying the values.yaml of nri-bundle: newrelic-infrastructure.config.proxy: https://user:password@hostname:port Copy Installing with manifest: When installing through the manifest, the proxy can be configured adding the NRIA_PROXY environment variable to the environment variable section of the DaemonSet configuration. The environment variable NRIA_VERBOSE in the DaemonSet YAML can be used as an example of the environment variable configuration. Set logging with a proxy If you’re using log forwarding for the Kubernetes integration, the plugin automatically detects the HTTP_PROXY and HTTPS_PROXY environment variables, and automatically uses them to set up the proxy configuration. For more information, see how to customize the proxy, or bypass it. Install Kubernetes events with a proxy There are three options to install the Kubernetes events integration with a proxy: Installing with chart nri-kube-events: The proxy can be configured setting the configuration option proxy in the values.yaml file. If set, the variable NRIA_PROXY will be added into the deployment.yaml template. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option nri-kube-events.proxy. The configuration option is passed down to the dependency nri-kube-events modifying the values.yaml of nri-bundle: nri-kube-events.proxy: https://user:password@hostname:port Copy Installing with manifest: When installing through the manifest, to configure the proxy set the NRIA_PROXY in the environment variable section of the infra container. Check the deployment template of the chart as an example of environment variable configuration. Install the Prometheus OpenMetrics integration with a proxy There are three options to install the Prometheus OpenMetric integration with a proxy: Installing with charts nri-prometheus: The proxy can be configured setting the configuration option config.emitter_proxy in the values.yaml. The config object is used to generate a configMap that mounted into the deployment under /etc/nri-prometheus/. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option nri-prometheus.config.emitter_proxy. The configuration option is passed down to the dependency nri-prometheus modifying the values.yaml of nri-bundle: nri-prometheus.config.emitter_proxy: \"http://localhost:8888 Copy Installing with the manifest: Configure the proxy uncommenting the emitter_proxy option directly in the manifest: # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" Copy Set the synthetics minion to use a proxy Check this README on how to set the proxy for the Synthetics minions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 144.90121,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure <em>Kubernetes</em> with a proxy",
        "sections": "<em>Install</em> <em>Kubernetes</em> events with a proxy",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "If you are running our <em>Kubernetes</em> <em>integration</em> with a proxy, you need to configure each component (the infrastructure agent, logging, <em>Kubernetes</em> events, etc.) to correctly work with your proxy. Install the infrastructure agent with a proxy There are three options to install the infrastructure agent"
      },
      "id": "617d584264441f2d01fbc6d7"
    },
    {
      "sections": [
        "Configure control plane monitoring",
        "Features",
        "Compatibility and requirements",
        "Discovery of master nodes and control plane components",
        "Configuration",
        "Important",
        "ETCD",
        "API server",
        "OpenShift configuration",
        "Set up mTLS from the ETCD client CA",
        "Tip",
        "To ease future installations",
        "Set up mTLS for ETCD in OpenShift",
        "See your data"
      ],
      "title": "Configure control plane monitoring",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "d648be240fd2f1b09bc1f29ef805bc0dd5c59e7a",
      "image": "https://docs.newrelic.com/static/209f301630c770f87ea8cbb1cace8e6e/8c557/new-relic-one-k8s-cluster-explorer-control-plane-parameters.png",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/configure-control-plane-monitoring/",
      "published_at": "2022-01-08T09:49:01Z",
      "updated_at": "2021-12-09T15:16:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic provides Control Plane support for your Kubernetes integration, allowing you to monitor and collect metrics from your cluster's Control Plane components. That data can then be found in New Relic and used to create queries and charts. Features We monitor and collect metrics from the following control plane components: ETCD: leader information, resident memory size, number of OS threads, consensus proposals data, etc. For a list of supported metrics, see ETCD data. API server: rate of apiserver requests, breakdown of apiserver requests by HTTP method and response code, etc. For the complete list of supported metrics, see API server data. Scheduler: requested CPU/memory vs available on the node, tolerations to taints, any set affinity or anti-affinity, etc. For the complete list of supported metrics, see Scheduler data. Controller manager: resident memory size, number of OS threads created, goroutines currently existing, etc. For the complete list of supported metrics, see Controller manager data. Compatibility and requirements Control plane monitoring requires Kubernetes integration version 1.16.0 or higher. Control plane monitoring support is not enabled for managed clusters. This is because providers (EKS, GKE, AKS, etc.) abstract away the concept of master nodes and control plane components, so that access to them is limited or non-existent. External control planes are not supported. The unprivileged version of the Kubernetes integration does not support control plane monitoring. OpenShift 4.x uses control plane component metric endpoints that are different than the default. Discovery of master nodes and control plane components The Kubernetes integration relies on the kubeadm labeling conventions to discover the master nodes and the control plane components. This means that master nodes should be labeled with node-role.kubernetes.io/master=\"\" or kubernetes.io/role=\"master\". The control plane components should have either the k8s-app or the tier and component labels. Refer to the following table for accepted label combinations and values: Component Label Endpoint API server Kubeadm / Kops / ClusterAPI k8s-app=kube-apiserver tier=control-plane component=kube-apiserver OpenShift app=openshift-kube-apiserver apiserver=true localhost:443/metrics by default (can be configured) if the request fails falls back to localhost:8080/metrics ETCD Kubeadm / Kops / ClusterAPI k8s-app=etcd-manager-main tier=control-plane component=etcd OpenShift k8s-app=etcd localhost:4001/metrics Scheduler Kubeadm / Kops / ClusterAPI k8s-app=kube-scheduler tier=control-plane component=kube-scheduler OpenShift app=openshift-kube-scheduler scheduler=true localhost:10251/metrics Controller manager Kubeadm / Kops / ClusterAPI k8s-app=kube-controller-manager tier=control-plane component=kube-controller-manager​ OpenShift app=kube-controller-manager kube-controller-manager=true localhost:10252/metrics When the integration detects that it is running inside a master node, it tries to find which components are running on the node by looking for pods that match the labels listed in the table above. For every running component, the integration makes a request to its metrics endpoint. Configuration Control plane monitoring is automatic for agents running inside master nodes. The only component that requires an extra step to run is ETCD, because it uses mutual TLS authentication (mTLS) for client requests. The API Server can also be configured to be queried using the Secure Port. Important Control plane monitoring for OpenShift 4.x requires additional configuration. For more information, see the OpenShift 4.x Configuration section. ETCD In order to set mTLS for querying ETCD, there are two configuration options that need to be set: Option Value ETCD_TLS_SECRET_NAME Name of a Kubernetes secret that contains the mTLS configuration. The secret should contain the following keys: cert: the certificate that identifies the client making the request. It should be signed by an ETCD trusted CA. key: the private key used to generate the client certificate. cacert: the root CA used to identify the ETCD server certificate. If the ETCD_TLS_SECRET_NAME option is not set, ETCD metrics won't be fetched. For step by step instructions on how to create a certificate and sign it with the ETCD client CA, see Set up mTLS from the ETCD client CA. ETCD_TLS_SECRET_NAMESPACE The namespace where the secret specified in the ETCD_TLS_SECRET_NAME was created. If not set, the default namespace is used. API server By default, the API server metrics are queried using the localhost:8080 unsecured endpoint. If this port is disabled, you can also query these metrics over the secure port. To enable this, set the following configuration option in the Kubernetes integration manifest file: Option Value API_SERVER_ENDPOINT_URL The (secure) URL to query the metrics. The API server uses localhost:443 by default Ensure that the ClusterRole has been updated to the newest version found in the manifest Added in version 1.15.0 Important Note that the port can be different according to the secure port used by the API server. For example, in Minikube the API server secure port is 8443 and therefore API_SERVER_ENDPOINT_URL should be set to https://localhost:8443 OpenShift configuration Control plane components on OpenShift 4.x use endpoint URLs that require SSL and service account based authentication. Therefore, the default endpoint URLs can not be used. To configure control plane monitoring on OpenShift, uncomment the following environment variables in the customized manifest. URL values are pre-configured to the default base URLs for the control plane monitoring metrics endpoints in OpenShift 4.x. - name: \"SCHEDULER_ENDPOINT_URL\" value: \"https://localhost:10259 - name: \"ETCD_ENDPOINT_URL\" value: \"https://localhost:9979\" - name: \"CONTROLLER_MANAGER_ENDPOINT_URL\" value: \"https://localhost:10257\" - name: \"API_SERVER_ENDPOINT_URL\" value: \"https://localhost:6443\" Copy Important Even though the custom ETCD_ENDPOINT_URL is defined, ETCD requires HTTPS and mTLS authentication to be configured. For more on configuring mTLS for ETCD in OpenShift, see Set up mTLS for ETCD in OpenShift. Important When installing through Helm openshift, specify the config to automatically include these endpoints. Setting openshift.enabled=true and openshift.version=\"4.x\" will include the secure endpoints and enable the /var/run/crio.sock runtime. Set up mTLS from the ETCD client CA The instructions below are based on the Kubernetes documentation. For more information, see Managing TLS certificates in a cluster. For OpenShift, see Set up mTLS for ETCD in OpenShift. To set up mTLS from the ETCD client CA: Download and install the tool cfssl, selecting the correct binaries for your OS from the list. Once installed, execute the following command: cat <<EOF | cfssl genkey - | cfssljson -bare server { \"hosts\": [ \"localhost\" ], \"CN\": \"newrelic-infra.pod.cluster.local\", \"key\": { \"algo\": \"ecdsa\", \"size\": 256 } } EOF Copy This command generates two files; server.csr containing the PEM encoded pkcs#10 certification request and server-key.pem containing the PEM encoded key to the certificate to be created. Use the generated certificate authority (CA) of ETCD to sign your CSR. Depending on your cluster configuration, you may already have this information. For default install configuration, download the CA certificate and the private key directly from ETCD with the following commands: kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.crt ./cacert -n kube-system kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.key ./cacert.key -n kube-system Copy Tip This requires that the etcd-manager-main pod has the label k8s-app=etcd-manager-main, which is a requirement for control plane monitoring . If your etc-manager-main pod is located in a different namespace, change the -n kube-system flags accordingly. With those files downloaded, use the following command to sign your CSRF: cfssl sign -ca cacert -ca-key cacert.key server.csr | cfssljson -bare cert Copy Create the secret that is used to retrieve the TLS config for making requests to ETC. We recommend renaming the certificate and the private key: cp cert.pem cert && cp server-key.pem key Copy kubectl -n default create secret generic newrelic-infra-etcd-tls-secret --from-file=./cert --from-file=./key --from-file=./cacert Copy To ease future installations Use the following commands to simultaneously create the CSR, retrieve the CA, generate the certificate by signing the CSR, and create the secret with all the required fields: cat <<EOF | cfssl genkey - | cfssljson -bare server && \\ kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.crt ./cacert -n kube-system && \\ kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.key ./cacert.key -n kube-system && \\ cp server-key.pem key && \\ cfssl sign -ca cacert -ca-key cacert.key server.csr | cfssljson -bare cert && \\ cp cert.pem cert && \\ kubectl -n default create secret generic newrelic-infra-etcd-tls-secret --from-file=./cert --from-file=./key --from-file=./cacert { \"hosts\": [ \"localhost\" ], \"CN\": \"newrelic-infra.pod.cluster.local\", \"key\": { \"algo\": \"ecdsa\", \"size\": 256 } } EOF Copy The last step is to update the configuration in the manifest and apply it. In the configuration section, there are two options related to ETCD mTLS: ETCD_TLS_SECRET_NAME with the name of the secret that we just created. ETCD_TLS_SECRET_NAMESPACE with the namespace that we used to create the secret. To complete the installation, add these variables to the container spec of the integration DaemonSet and apply the changes: - name: \"ETCD_TLS_SECRET_NAME” value: \"newrelic-infra-etcd-tls-secret\" - name: \"ETCD_TLS_SECRET_NAMESPACE\" value: \"default\" Copy Set up mTLS for ETCD in OpenShift Follow these instructions to set up mutual TLS authentication for ETCD in OpenShift 4.x: Export the ETCD client certificates from the cluster to an opaque secret. In a default managed OpenShift cluster, the secret is named kube-etcd-client-certs and it is stored in the openshift-monitoring namespace. kubectl get secret/kube-etcd-client-certs -n openshift-monitoring -o yaml > etcd-secret.yaml Copy Open the secret file and change the keys: Rename the certificate authority to cacert. Rename the client certificate to cert. Rename the client key to key. Optional: change the secret name and namespace to something meaningful. Remove these unnecessary keys in the metadata section: creationTimestamp resourceVersion selfLink uid Install the manifest with its new name and namespace: kubectl apply -f etcd-secret.yaml Copy Go to Update manifest configuration (the last step under Set up MTL from ETCD client) to configure the required environment variables. See your data If the integration has been been set up correctly, the Kubernetes cluster explorer contains all the Control Plane components and their status in a dedicated section, as shown below. one.newrelic.com > Kubernetes Cluster Explorer: Use the Kubernetes cluster explorer to monitor and collect metrics from your cluster's Control Plane components You can also check for Control Plane data with this NRQL query: SELECT latest(timestamp) FROM K8sApiServerSample, K8sEtcdSample, K8sSchedulerSample, K8sControllerManagerSample FACET entityName where clusterName = 'MY_CLUSTER_NAME' Copy Tip If you still can't see Control Plane data, try the solution described in Kubernetes integration troubleshooting: Not seeing data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 125.8996,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "To ease future <em>installations</em>",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "New Relic provides Control Plane support for your <em>Kubernetes</em> <em>integration</em>, allowing you to monitor and collect metrics from your cluster&#x27;s Control Plane components. That data can then be found in New Relic and used to create queries and charts. Features We monitor and collect metrics from"
      },
      "id": "617d735f64441f9d39fbe7ee"
    }
  ],
  "/docs/kubernetes-pixie/kubernetes-integration/installation/kubernetes-integration-recommended-alert-policy": [
    {
      "sections": [
        "Install the Kubernetes integration manually using Helm",
        "Compatibility and requirements",
        "Install Kubernetes integration with Helm",
        "Install with Helm 3 and nri-bundle (recommended)",
        "Installing and configuring nri-bundle with Helm",
        "Install with Helm 2 and nri-bundle (legacy)",
        "Installation instructions for Helm 2",
        "Important",
        "Tip",
        "Helm configuration options",
        "Upgrade using Helm",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data",
        "Reduce data ingest",
        "Prometheus OpenMetrics Integration",
        "New Relic Logging",
        "New Relic Pixie Integration",
        "Uninstall Kubernetes integration"
      ],
      "title": "Install the Kubernetes integration manually using Helm",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "29d42af98d41e7e4e7be86f0150254e132cb6b6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/install-kubernetes-integration-using-helm/",
      "published_at": "2022-01-08T09:49:44Z",
      "updated_at": "2022-01-08T09:49:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Helm is a package manager on top of Kubernetes. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in Kubernetes. To install the integration using Helm, we recommend our Kubernetes automated installer, which will prompt for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our integration as plain manifests rather than a Helm release. See Kubernetes integration: install and configure for more details about how to use our automated installer. Start the installer This page describes in more depth how to install and configure the New Relic integration without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the Kubernetes integration. Our charts are also compatible with Helm 2, but support for it might be removed in the future. To install the Kubernetes integration using Helm, you will need your New Relic account license key and your Kubernetes cluster's name: Find and copy your New Relic license key. Find the name of your cluster with this command: kubectl config current-context Copy Note these values somewhere safe, as you will need them later during the installation process. Install Kubernetes integration with Helm Install with Helm 3 and nri-bundle (recommended) New Relic has several charts for the different components which offer different features for the platform: newrelic-infrastructure: Contains the main Kubernetes integration and the infrastructure agent. This is the core component for the New Relic Kubernetes experience, responsible of reporting most of the data that is surfaced in dashboard and the Kubernetes Cluster Explorer. newrelic-logging: Provides a DaemonSet with New Relic's Fluent Bit output plugin to easily forward your logs to New Relic. nri-kube-events: Collects and reports cluster events (such as kubectl get events) to New Relic. nri-prometheus: New Relic's Prometheus OpenMetrics Integration, automatically scrapes Prometheus endpoints present in the cluser and reports metrics to New Relic. nri-metadata-injection: Sets up a minimal MutatingAdmissionWebhook that injects a couple of environment to containers. These contain metadata about the cluster and New Relic installation and will be later picked up by applications instrumented using APM, allowing to correlate APM and infrastructure data. nri-statsd: New Relic StatsD integration. Additionally, New Relic provides nri-bundle, a chart which pulls a selectable set of the charts mentioned above. nri-bundle can also install Kube State Metrics and Pixie for you if needed. While it is possible to install those charts separately, we strongly recommend using the nri-bundle chart for Kubernetes deployments, as it ensures that values across all the charts are consistent and provides full control over which components are installed, as well as the possibility to configure all of them as Helm dependencies. This is the same chart that is used and referenced by our automated installer. Installing and configuring nri-bundle with Helm Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Please notice and adjust the following flags: global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY: Must be set to a valid License Key for your account. global.cluster=K8S_CLUSTER_NAME: Is used to identify the cluster in the New Relic UI, so should be a descriptive value not used by any other Kubernetes cluster configured in your New Relic account. ksm.enabled=true: Setting this to true will automatically install Kube State Metrics (KSM) for you, which is required for our integration to run. You can set this to false if KSM is already present in your cluster, even if it is on a different namespace. newrelic-infrastructure.privileged=true: Can be set to false to install a trimmed down version of our integration that does not require extra privileges, such as hostPath mounts or running containers as root. Please note that this will disable detailed process collection from the host. For performance reasons, our logging solution still requires hostPath mounts, regardless of the value of this flag. If this is not allowedin your cluster, you will need to disable the logging solution by specifying logging.enabled=false. prometheus.enabled=true: Will deploy our Prometheus OpenMetrics integration, which automatically collects data from prometheus endpoints present in the cluster. webhook.enabled=true: Will install our minimal webhook, which adds environment variables that, in turn, allows linking applications instrumented with APM with infrastructure data. Our chart has a comprehensive set of flags and tunables that can be edited to configure our solution to your particular needs. For a full list of all the flags that can be configured, please check the chart's README and the default values.yaml file. Install the Kubernetes integration by running the customized command without --debug and --dry-run: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl -n newrelic get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node for newrelic-infrastructure, as well as one Deployment and one pod for any other component that you have enabled. Install with Helm 2 and nri-bundle (legacy) Installation instructions for Helm 2 Important Helm 2 has been deprecated and New Relic does not recommend using it for deployments. Instructions in this section are provided for legacy systems only. To install using Helm 2: Ensure that Helm 2, including their cluster-side components (such as Tiller) are properly installed and configured. Please check the official documentation for more details. Set the cluster where you want to install the agent: kubectl config set-cluster DESIRED_CLUSTER Copy To see the available clusters, run kubectl config get-clusters Make sure that kube-state-metrics is installed on your machine: kubectl get deployment --all-namespaces | grep kube-state-metrics Copy If it's not installed, follow the instructions in the kube-state-metrics GitHub repo to install it. Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. This step uses the --dry-run and --debug switches and therefore the agent is not installed. helm install newrelic/newrelic-infrastructure \\ --dry-run \\ --debug \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Install the New Relic Kubernetes integration: helm install newrelic/newrelic-infrastructure \\ --set licenseKey=your_new_relic_license_key \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Tip Note that the --dry-run and --debug switches have been removed. Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node. Helm configuration options When you install or upgrade the Kubernetes integration with Helm using the command line, you can pass your configuration variables with the --set flag. helm install newrelic/newrelic-infrastructure \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=YOUR_CLUSTER_NAME Copy A full list of the configuration parameters can be found in the newrelic-infrastructure chart README Upgrade using Helm To update your Kubernetes integration installed via Helm: Update the local chart repository: helm repo update Copy Update the release by running again the appropriate helm upgrade --install ... command in the section above. Monitor services running on Kubernetes After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages. Reduce data ingest Our charts support setting an option to reduce the amount of data ingest at the cost of dropping detailed information. To enable it, set global.lowDataMode=true in the nri-bundle chart. lowDataMode affects three specific components of the nri-bundle chart outlined below. Prometheus OpenMetrics Integration If lowDataMode is enabled, the following metrics are excluded by default as they are already collected and used by the New Relic Kubernetes Integration. - kube_ - container_ - machine_ - cadvisor_ Copy New Relic Logging If lowDataMode is enabled, Labels and Annotations are set to Off in the Filter section of the fluent-bit.conf file. This means that this detail will be dropped from the container log files which reduces the overall data ingest into New Relic. The following fields are retained: Allowlist_key container_name Allowlist_key namespace_name Allowlist_key pod_name Allowlist_key stream Allowlist_key log Copy Low Data Mode Log Example Complete Log Record [ { \"cluster_name\": \"api-test\", \"kubernetes\": { \"annotations\": { \"kubernetes.io/psp\": \"eks.privileged\" }, \"container_hash\": \"fryckbos/test@sha256:5b098eaf3c7d5b3585eb10cebee63665b6208bea31ef31a3f0856c5ffdda644b\", \"container_image\": \"fryckbos/test:latest\", \"container_name\": \"newrelic-logging\", \"docker_id\": \"134e1daf63761baa15e035b08b7aea04518a0f0e50af4215131a50c6a379a072\", \"host\": \"ip-192-168-17-123.ec2.internal\", \"labels\": { \"app\": \"newrelic-logging\", \"app.kubernetes.io/name\": \"newrelic-logging\", \"controller-revision-hash\": \"84db95db86\", \"pod-template-generation\": \"1\", \"release\": \"nri-bundle\" }, \"namespace_name\": \"nrlogs\", \"pod_id\": \"54556e3e-719c-46b5-af69-020b75d69bf1\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\" }, \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"plugin\": { \"source\": \"kubernetes\", \"type\": \"fluent-bit\", \"version\": \"1.8.1\" }, \"stream\": \"stderr\", \"time\": \"2021-09-14T12:30:49.138824971Z\", \"timestamp\": 1631622649138 } ] Copy Log Record after enabling lowDataMode. [ { \"cluster_name\": \"api-test\", \"container_name\": \"newrelic-logging\", \"namespace_name\": \"nrlogs\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\", \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"stream\": \"stderr\", \"timestamp\": 1631622649138 } ] Copy New Relic Pixie Integration If lowDataMode is enabled, the newrelic-pixie integration performs heavier sampling on Pixie spans and reduces the collection interval from 10 seconds to 15 seconds. lowDataMode settings: HTTP_SPAN_LIMIT: 750 DB_SPAN_LIMIT: 250 COLLECT_INTERVAL_SEC: 15 Copy The default settings for these parameters and others can be found in the newrelic-pixie-integration Github repo. Uninstall Kubernetes integration To uninstall the Kubernetes integration using Helm, run the following command. Replace some-manifest-file.yml with your specific filename. helm uninstall newrelic -n newrelic if kubectl then : kubectl delete -f some-manifest-file.yml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 218.82104,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "sections": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "Helm is a package manager on top of <em>Kubernetes</em>. It facilitates <em>installation</em>, upgrades, or revision tracking, and it manages dependencies for the services that you install in <em>Kubernetes</em>. To install the <em>integration</em> using Helm, we recommend our <em>Kubernetes</em> automated installer, which will prompt"
      },
      "id": "617d5841196a67bb40f7c1de"
    },
    {
      "sections": [
        "Configure Kubernetes with a proxy",
        "Install the infrastructure agent with a proxy",
        "Set logging with a proxy",
        "Install Kubernetes events with a proxy",
        "Install the Prometheus OpenMetrics integration with a proxy",
        "Set the synthetics minion to use a proxy"
      ],
      "title": "Configure Kubernetes with a proxy",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "9d7794244b451d878d00db0f84c4709e43ce138c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/configure-kubernetes-proxy/",
      "published_at": "2022-01-08T09:49:01Z",
      "updated_at": "2021-12-25T07:57:45Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are running our Kubernetes integration with a proxy, you need to configure each component (the infrastructure agent, logging, Kubernetes events, etc.) to correctly work with your proxy. Install the infrastructure agent with a proxy There are three options to install the infrastructure agent with a proxy: Installing with chart newrelic-infrastructure: The proxy can be configured setting the configuration option config.proxy as described in the values.yaml of the chart. The whole config object is used to generate a configMap that is mounted as the file /etc/newrelic-infra.yml used to configure the agent. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option newrelic-infrastructure.config.proxy. The configuration option is passed down to the dependency newrelic-infrastructure modifying the values.yaml of nri-bundle: newrelic-infrastructure.config.proxy: https://user:password@hostname:port Copy Installing with manifest: When installing through the manifest, the proxy can be configured adding the NRIA_PROXY environment variable to the environment variable section of the DaemonSet configuration. The environment variable NRIA_VERBOSE in the DaemonSet YAML can be used as an example of the environment variable configuration. Set logging with a proxy If you’re using log forwarding for the Kubernetes integration, the plugin automatically detects the HTTP_PROXY and HTTPS_PROXY environment variables, and automatically uses them to set up the proxy configuration. For more information, see how to customize the proxy, or bypass it. Install Kubernetes events with a proxy There are three options to install the Kubernetes events integration with a proxy: Installing with chart nri-kube-events: The proxy can be configured setting the configuration option proxy in the values.yaml file. If set, the variable NRIA_PROXY will be added into the deployment.yaml template. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option nri-kube-events.proxy. The configuration option is passed down to the dependency nri-kube-events modifying the values.yaml of nri-bundle: nri-kube-events.proxy: https://user:password@hostname:port Copy Installing with manifest: When installing through the manifest, to configure the proxy set the NRIA_PROXY in the environment variable section of the infra container. Check the deployment template of the chart as an example of environment variable configuration. Install the Prometheus OpenMetrics integration with a proxy There are three options to install the Prometheus OpenMetric integration with a proxy: Installing with charts nri-prometheus: The proxy can be configured setting the configuration option config.emitter_proxy in the values.yaml. The config object is used to generate a configMap that mounted into the deployment under /etc/nri-prometheus/. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option nri-prometheus.config.emitter_proxy. The configuration option is passed down to the dependency nri-prometheus modifying the values.yaml of nri-bundle: nri-prometheus.config.emitter_proxy: \"http://localhost:8888 Copy Installing with the manifest: Configure the proxy uncommenting the emitter_proxy option directly in the manifest: # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" Copy Set the synthetics minion to use a proxy Check this README on how to set the proxy for the Synthetics minions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 144.90121,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure <em>Kubernetes</em> with a proxy",
        "sections": "<em>Install</em> <em>Kubernetes</em> events with a proxy",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "If you are running our <em>Kubernetes</em> <em>integration</em> with a proxy, you need to configure each component (the infrastructure agent, logging, <em>Kubernetes</em> events, etc.) to correctly work with your proxy. Install the infrastructure agent with a proxy There are three options to install the infrastructure agent"
      },
      "id": "617d584264441f2d01fbc6d7"
    },
    {
      "sections": [
        "Configure control plane monitoring",
        "Features",
        "Compatibility and requirements",
        "Discovery of master nodes and control plane components",
        "Configuration",
        "Important",
        "ETCD",
        "API server",
        "OpenShift configuration",
        "Set up mTLS from the ETCD client CA",
        "Tip",
        "To ease future installations",
        "Set up mTLS for ETCD in OpenShift",
        "See your data"
      ],
      "title": "Configure control plane monitoring",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "d648be240fd2f1b09bc1f29ef805bc0dd5c59e7a",
      "image": "https://docs.newrelic.com/static/209f301630c770f87ea8cbb1cace8e6e/8c557/new-relic-one-k8s-cluster-explorer-control-plane-parameters.png",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/configure-control-plane-monitoring/",
      "published_at": "2022-01-08T09:49:01Z",
      "updated_at": "2021-12-09T15:16:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic provides Control Plane support for your Kubernetes integration, allowing you to monitor and collect metrics from your cluster's Control Plane components. That data can then be found in New Relic and used to create queries and charts. Features We monitor and collect metrics from the following control plane components: ETCD: leader information, resident memory size, number of OS threads, consensus proposals data, etc. For a list of supported metrics, see ETCD data. API server: rate of apiserver requests, breakdown of apiserver requests by HTTP method and response code, etc. For the complete list of supported metrics, see API server data. Scheduler: requested CPU/memory vs available on the node, tolerations to taints, any set affinity or anti-affinity, etc. For the complete list of supported metrics, see Scheduler data. Controller manager: resident memory size, number of OS threads created, goroutines currently existing, etc. For the complete list of supported metrics, see Controller manager data. Compatibility and requirements Control plane monitoring requires Kubernetes integration version 1.16.0 or higher. Control plane monitoring support is not enabled for managed clusters. This is because providers (EKS, GKE, AKS, etc.) abstract away the concept of master nodes and control plane components, so that access to them is limited or non-existent. External control planes are not supported. The unprivileged version of the Kubernetes integration does not support control plane monitoring. OpenShift 4.x uses control plane component metric endpoints that are different than the default. Discovery of master nodes and control plane components The Kubernetes integration relies on the kubeadm labeling conventions to discover the master nodes and the control plane components. This means that master nodes should be labeled with node-role.kubernetes.io/master=\"\" or kubernetes.io/role=\"master\". The control plane components should have either the k8s-app or the tier and component labels. Refer to the following table for accepted label combinations and values: Component Label Endpoint API server Kubeadm / Kops / ClusterAPI k8s-app=kube-apiserver tier=control-plane component=kube-apiserver OpenShift app=openshift-kube-apiserver apiserver=true localhost:443/metrics by default (can be configured) if the request fails falls back to localhost:8080/metrics ETCD Kubeadm / Kops / ClusterAPI k8s-app=etcd-manager-main tier=control-plane component=etcd OpenShift k8s-app=etcd localhost:4001/metrics Scheduler Kubeadm / Kops / ClusterAPI k8s-app=kube-scheduler tier=control-plane component=kube-scheduler OpenShift app=openshift-kube-scheduler scheduler=true localhost:10251/metrics Controller manager Kubeadm / Kops / ClusterAPI k8s-app=kube-controller-manager tier=control-plane component=kube-controller-manager​ OpenShift app=kube-controller-manager kube-controller-manager=true localhost:10252/metrics When the integration detects that it is running inside a master node, it tries to find which components are running on the node by looking for pods that match the labels listed in the table above. For every running component, the integration makes a request to its metrics endpoint. Configuration Control plane monitoring is automatic for agents running inside master nodes. The only component that requires an extra step to run is ETCD, because it uses mutual TLS authentication (mTLS) for client requests. The API Server can also be configured to be queried using the Secure Port. Important Control plane monitoring for OpenShift 4.x requires additional configuration. For more information, see the OpenShift 4.x Configuration section. ETCD In order to set mTLS for querying ETCD, there are two configuration options that need to be set: Option Value ETCD_TLS_SECRET_NAME Name of a Kubernetes secret that contains the mTLS configuration. The secret should contain the following keys: cert: the certificate that identifies the client making the request. It should be signed by an ETCD trusted CA. key: the private key used to generate the client certificate. cacert: the root CA used to identify the ETCD server certificate. If the ETCD_TLS_SECRET_NAME option is not set, ETCD metrics won't be fetched. For step by step instructions on how to create a certificate and sign it with the ETCD client CA, see Set up mTLS from the ETCD client CA. ETCD_TLS_SECRET_NAMESPACE The namespace where the secret specified in the ETCD_TLS_SECRET_NAME was created. If not set, the default namespace is used. API server By default, the API server metrics are queried using the localhost:8080 unsecured endpoint. If this port is disabled, you can also query these metrics over the secure port. To enable this, set the following configuration option in the Kubernetes integration manifest file: Option Value API_SERVER_ENDPOINT_URL The (secure) URL to query the metrics. The API server uses localhost:443 by default Ensure that the ClusterRole has been updated to the newest version found in the manifest Added in version 1.15.0 Important Note that the port can be different according to the secure port used by the API server. For example, in Minikube the API server secure port is 8443 and therefore API_SERVER_ENDPOINT_URL should be set to https://localhost:8443 OpenShift configuration Control plane components on OpenShift 4.x use endpoint URLs that require SSL and service account based authentication. Therefore, the default endpoint URLs can not be used. To configure control plane monitoring on OpenShift, uncomment the following environment variables in the customized manifest. URL values are pre-configured to the default base URLs for the control plane monitoring metrics endpoints in OpenShift 4.x. - name: \"SCHEDULER_ENDPOINT_URL\" value: \"https://localhost:10259 - name: \"ETCD_ENDPOINT_URL\" value: \"https://localhost:9979\" - name: \"CONTROLLER_MANAGER_ENDPOINT_URL\" value: \"https://localhost:10257\" - name: \"API_SERVER_ENDPOINT_URL\" value: \"https://localhost:6443\" Copy Important Even though the custom ETCD_ENDPOINT_URL is defined, ETCD requires HTTPS and mTLS authentication to be configured. For more on configuring mTLS for ETCD in OpenShift, see Set up mTLS for ETCD in OpenShift. Important When installing through Helm openshift, specify the config to automatically include these endpoints. Setting openshift.enabled=true and openshift.version=\"4.x\" will include the secure endpoints and enable the /var/run/crio.sock runtime. Set up mTLS from the ETCD client CA The instructions below are based on the Kubernetes documentation. For more information, see Managing TLS certificates in a cluster. For OpenShift, see Set up mTLS for ETCD in OpenShift. To set up mTLS from the ETCD client CA: Download and install the tool cfssl, selecting the correct binaries for your OS from the list. Once installed, execute the following command: cat <<EOF | cfssl genkey - | cfssljson -bare server { \"hosts\": [ \"localhost\" ], \"CN\": \"newrelic-infra.pod.cluster.local\", \"key\": { \"algo\": \"ecdsa\", \"size\": 256 } } EOF Copy This command generates two files; server.csr containing the PEM encoded pkcs#10 certification request and server-key.pem containing the PEM encoded key to the certificate to be created. Use the generated certificate authority (CA) of ETCD to sign your CSR. Depending on your cluster configuration, you may already have this information. For default install configuration, download the CA certificate and the private key directly from ETCD with the following commands: kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.crt ./cacert -n kube-system kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.key ./cacert.key -n kube-system Copy Tip This requires that the etcd-manager-main pod has the label k8s-app=etcd-manager-main, which is a requirement for control plane monitoring . If your etc-manager-main pod is located in a different namespace, change the -n kube-system flags accordingly. With those files downloaded, use the following command to sign your CSRF: cfssl sign -ca cacert -ca-key cacert.key server.csr | cfssljson -bare cert Copy Create the secret that is used to retrieve the TLS config for making requests to ETC. We recommend renaming the certificate and the private key: cp cert.pem cert && cp server-key.pem key Copy kubectl -n default create secret generic newrelic-infra-etcd-tls-secret --from-file=./cert --from-file=./key --from-file=./cacert Copy To ease future installations Use the following commands to simultaneously create the CSR, retrieve the CA, generate the certificate by signing the CSR, and create the secret with all the required fields: cat <<EOF | cfssl genkey - | cfssljson -bare server && \\ kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.crt ./cacert -n kube-system && \\ kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.key ./cacert.key -n kube-system && \\ cp server-key.pem key && \\ cfssl sign -ca cacert -ca-key cacert.key server.csr | cfssljson -bare cert && \\ cp cert.pem cert && \\ kubectl -n default create secret generic newrelic-infra-etcd-tls-secret --from-file=./cert --from-file=./key --from-file=./cacert { \"hosts\": [ \"localhost\" ], \"CN\": \"newrelic-infra.pod.cluster.local\", \"key\": { \"algo\": \"ecdsa\", \"size\": 256 } } EOF Copy The last step is to update the configuration in the manifest and apply it. In the configuration section, there are two options related to ETCD mTLS: ETCD_TLS_SECRET_NAME with the name of the secret that we just created. ETCD_TLS_SECRET_NAMESPACE with the namespace that we used to create the secret. To complete the installation, add these variables to the container spec of the integration DaemonSet and apply the changes: - name: \"ETCD_TLS_SECRET_NAME” value: \"newrelic-infra-etcd-tls-secret\" - name: \"ETCD_TLS_SECRET_NAMESPACE\" value: \"default\" Copy Set up mTLS for ETCD in OpenShift Follow these instructions to set up mutual TLS authentication for ETCD in OpenShift 4.x: Export the ETCD client certificates from the cluster to an opaque secret. In a default managed OpenShift cluster, the secret is named kube-etcd-client-certs and it is stored in the openshift-monitoring namespace. kubectl get secret/kube-etcd-client-certs -n openshift-monitoring -o yaml > etcd-secret.yaml Copy Open the secret file and change the keys: Rename the certificate authority to cacert. Rename the client certificate to cert. Rename the client key to key. Optional: change the secret name and namespace to something meaningful. Remove these unnecessary keys in the metadata section: creationTimestamp resourceVersion selfLink uid Install the manifest with its new name and namespace: kubectl apply -f etcd-secret.yaml Copy Go to Update manifest configuration (the last step under Set up MTL from ETCD client) to configure the required environment variables. See your data If the integration has been been set up correctly, the Kubernetes cluster explorer contains all the Control Plane components and their status in a dedicated section, as shown below. one.newrelic.com > Kubernetes Cluster Explorer: Use the Kubernetes cluster explorer to monitor and collect metrics from your cluster's Control Plane components You can also check for Control Plane data with this NRQL query: SELECT latest(timestamp) FROM K8sApiServerSample, K8sEtcdSample, K8sSchedulerSample, K8sControllerManagerSample FACET entityName where clusterName = 'MY_CLUSTER_NAME' Copy Tip If you still can't see Control Plane data, try the solution described in Kubernetes integration troubleshooting: Not seeing data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 125.8996,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "To ease future <em>installations</em>",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "New Relic provides Control Plane support for your <em>Kubernetes</em> <em>integration</em>, allowing you to monitor and collect metrics from your cluster&#x27;s Control Plane components. That data can then be found in New Relic and used to create queries and charts. Features We monitor and collect metrics from"
      },
      "id": "617d735f64441f9d39fbe7ee"
    }
  ],
  "/docs/kubernetes-pixie/kubernetes-integration/kubernetes-events/install-kubernetes-events-integration": [
    {
      "sections": [
        "Install the Kubernetes integration manually using Helm",
        "Compatibility and requirements",
        "Install Kubernetes integration with Helm",
        "Install with Helm 3 and nri-bundle (recommended)",
        "Installing and configuring nri-bundle with Helm",
        "Install with Helm 2 and nri-bundle (legacy)",
        "Installation instructions for Helm 2",
        "Important",
        "Tip",
        "Helm configuration options",
        "Upgrade using Helm",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data",
        "Reduce data ingest",
        "Prometheus OpenMetrics Integration",
        "New Relic Logging",
        "New Relic Pixie Integration",
        "Uninstall Kubernetes integration"
      ],
      "title": "Install the Kubernetes integration manually using Helm",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "29d42af98d41e7e4e7be86f0150254e132cb6b6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/install-kubernetes-integration-using-helm/",
      "published_at": "2022-01-08T09:49:44Z",
      "updated_at": "2022-01-08T09:49:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Helm is a package manager on top of Kubernetes. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in Kubernetes. To install the integration using Helm, we recommend our Kubernetes automated installer, which will prompt for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our integration as plain manifests rather than a Helm release. See Kubernetes integration: install and configure for more details about how to use our automated installer. Start the installer This page describes in more depth how to install and configure the New Relic integration without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the Kubernetes integration. Our charts are also compatible with Helm 2, but support for it might be removed in the future. To install the Kubernetes integration using Helm, you will need your New Relic account license key and your Kubernetes cluster's name: Find and copy your New Relic license key. Find the name of your cluster with this command: kubectl config current-context Copy Note these values somewhere safe, as you will need them later during the installation process. Install Kubernetes integration with Helm Install with Helm 3 and nri-bundle (recommended) New Relic has several charts for the different components which offer different features for the platform: newrelic-infrastructure: Contains the main Kubernetes integration and the infrastructure agent. This is the core component for the New Relic Kubernetes experience, responsible of reporting most of the data that is surfaced in dashboard and the Kubernetes Cluster Explorer. newrelic-logging: Provides a DaemonSet with New Relic's Fluent Bit output plugin to easily forward your logs to New Relic. nri-kube-events: Collects and reports cluster events (such as kubectl get events) to New Relic. nri-prometheus: New Relic's Prometheus OpenMetrics Integration, automatically scrapes Prometheus endpoints present in the cluser and reports metrics to New Relic. nri-metadata-injection: Sets up a minimal MutatingAdmissionWebhook that injects a couple of environment to containers. These contain metadata about the cluster and New Relic installation and will be later picked up by applications instrumented using APM, allowing to correlate APM and infrastructure data. nri-statsd: New Relic StatsD integration. Additionally, New Relic provides nri-bundle, a chart which pulls a selectable set of the charts mentioned above. nri-bundle can also install Kube State Metrics and Pixie for you if needed. While it is possible to install those charts separately, we strongly recommend using the nri-bundle chart for Kubernetes deployments, as it ensures that values across all the charts are consistent and provides full control over which components are installed, as well as the possibility to configure all of them as Helm dependencies. This is the same chart that is used and referenced by our automated installer. Installing and configuring nri-bundle with Helm Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Please notice and adjust the following flags: global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY: Must be set to a valid License Key for your account. global.cluster=K8S_CLUSTER_NAME: Is used to identify the cluster in the New Relic UI, so should be a descriptive value not used by any other Kubernetes cluster configured in your New Relic account. ksm.enabled=true: Setting this to true will automatically install Kube State Metrics (KSM) for you, which is required for our integration to run. You can set this to false if KSM is already present in your cluster, even if it is on a different namespace. newrelic-infrastructure.privileged=true: Can be set to false to install a trimmed down version of our integration that does not require extra privileges, such as hostPath mounts or running containers as root. Please note that this will disable detailed process collection from the host. For performance reasons, our logging solution still requires hostPath mounts, regardless of the value of this flag. If this is not allowedin your cluster, you will need to disable the logging solution by specifying logging.enabled=false. prometheus.enabled=true: Will deploy our Prometheus OpenMetrics integration, which automatically collects data from prometheus endpoints present in the cluster. webhook.enabled=true: Will install our minimal webhook, which adds environment variables that, in turn, allows linking applications instrumented with APM with infrastructure data. Our chart has a comprehensive set of flags and tunables that can be edited to configure our solution to your particular needs. For a full list of all the flags that can be configured, please check the chart's README and the default values.yaml file. Install the Kubernetes integration by running the customized command without --debug and --dry-run: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl -n newrelic get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node for newrelic-infrastructure, as well as one Deployment and one pod for any other component that you have enabled. Install with Helm 2 and nri-bundle (legacy) Installation instructions for Helm 2 Important Helm 2 has been deprecated and New Relic does not recommend using it for deployments. Instructions in this section are provided for legacy systems only. To install using Helm 2: Ensure that Helm 2, including their cluster-side components (such as Tiller) are properly installed and configured. Please check the official documentation for more details. Set the cluster where you want to install the agent: kubectl config set-cluster DESIRED_CLUSTER Copy To see the available clusters, run kubectl config get-clusters Make sure that kube-state-metrics is installed on your machine: kubectl get deployment --all-namespaces | grep kube-state-metrics Copy If it's not installed, follow the instructions in the kube-state-metrics GitHub repo to install it. Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. This step uses the --dry-run and --debug switches and therefore the agent is not installed. helm install newrelic/newrelic-infrastructure \\ --dry-run \\ --debug \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Install the New Relic Kubernetes integration: helm install newrelic/newrelic-infrastructure \\ --set licenseKey=your_new_relic_license_key \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Tip Note that the --dry-run and --debug switches have been removed. Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node. Helm configuration options When you install or upgrade the Kubernetes integration with Helm using the command line, you can pass your configuration variables with the --set flag. helm install newrelic/newrelic-infrastructure \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=YOUR_CLUSTER_NAME Copy A full list of the configuration parameters can be found in the newrelic-infrastructure chart README Upgrade using Helm To update your Kubernetes integration installed via Helm: Update the local chart repository: helm repo update Copy Update the release by running again the appropriate helm upgrade --install ... command in the section above. Monitor services running on Kubernetes After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages. Reduce data ingest Our charts support setting an option to reduce the amount of data ingest at the cost of dropping detailed information. To enable it, set global.lowDataMode=true in the nri-bundle chart. lowDataMode affects three specific components of the nri-bundle chart outlined below. Prometheus OpenMetrics Integration If lowDataMode is enabled, the following metrics are excluded by default as they are already collected and used by the New Relic Kubernetes Integration. - kube_ - container_ - machine_ - cadvisor_ Copy New Relic Logging If lowDataMode is enabled, Labels and Annotations are set to Off in the Filter section of the fluent-bit.conf file. This means that this detail will be dropped from the container log files which reduces the overall data ingest into New Relic. The following fields are retained: Allowlist_key container_name Allowlist_key namespace_name Allowlist_key pod_name Allowlist_key stream Allowlist_key log Copy Low Data Mode Log Example Complete Log Record [ { \"cluster_name\": \"api-test\", \"kubernetes\": { \"annotations\": { \"kubernetes.io/psp\": \"eks.privileged\" }, \"container_hash\": \"fryckbos/test@sha256:5b098eaf3c7d5b3585eb10cebee63665b6208bea31ef31a3f0856c5ffdda644b\", \"container_image\": \"fryckbos/test:latest\", \"container_name\": \"newrelic-logging\", \"docker_id\": \"134e1daf63761baa15e035b08b7aea04518a0f0e50af4215131a50c6a379a072\", \"host\": \"ip-192-168-17-123.ec2.internal\", \"labels\": { \"app\": \"newrelic-logging\", \"app.kubernetes.io/name\": \"newrelic-logging\", \"controller-revision-hash\": \"84db95db86\", \"pod-template-generation\": \"1\", \"release\": \"nri-bundle\" }, \"namespace_name\": \"nrlogs\", \"pod_id\": \"54556e3e-719c-46b5-af69-020b75d69bf1\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\" }, \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"plugin\": { \"source\": \"kubernetes\", \"type\": \"fluent-bit\", \"version\": \"1.8.1\" }, \"stream\": \"stderr\", \"time\": \"2021-09-14T12:30:49.138824971Z\", \"timestamp\": 1631622649138 } ] Copy Log Record after enabling lowDataMode. [ { \"cluster_name\": \"api-test\", \"container_name\": \"newrelic-logging\", \"namespace_name\": \"nrlogs\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\", \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"stream\": \"stderr\", \"timestamp\": 1631622649138 } ] Copy New Relic Pixie Integration If lowDataMode is enabled, the newrelic-pixie integration performs heavier sampling on Pixie spans and reduces the collection interval from 10 seconds to 15 seconds. lowDataMode settings: HTTP_SPAN_LIMIT: 750 DB_SPAN_LIMIT: 250 COLLECT_INTERVAL_SEC: 15 Copy The default settings for these parameters and others can be found in the newrelic-pixie-integration Github repo. Uninstall Kubernetes integration To uninstall the Kubernetes integration using Helm, run the following command. Replace some-manifest-file.yml with your specific filename. helm uninstall newrelic -n newrelic if kubectl then : kubectl delete -f some-manifest-file.yml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 160.4978,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "sections": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "Helm is a package manager on top of <em>Kubernetes</em>. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in <em>Kubernetes</em>. To install the <em>integration</em> using Helm, we recommend our <em>Kubernetes</em> automated installer, which will prompt"
      },
      "id": "617d5841196a67bb40f7c1de"
    },
    {
      "sections": [
        "Configure Kubernetes with a proxy",
        "Install the infrastructure agent with a proxy",
        "Set logging with a proxy",
        "Install Kubernetes events with a proxy",
        "Install the Prometheus OpenMetrics integration with a proxy",
        "Set the synthetics minion to use a proxy"
      ],
      "title": "Configure Kubernetes with a proxy",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "9d7794244b451d878d00db0f84c4709e43ce138c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/configure-kubernetes-proxy/",
      "published_at": "2022-01-08T09:49:01Z",
      "updated_at": "2021-12-25T07:57:45Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are running our Kubernetes integration with a proxy, you need to configure each component (the infrastructure agent, logging, Kubernetes events, etc.) to correctly work with your proxy. Install the infrastructure agent with a proxy There are three options to install the infrastructure agent with a proxy: Installing with chart newrelic-infrastructure: The proxy can be configured setting the configuration option config.proxy as described in the values.yaml of the chart. The whole config object is used to generate a configMap that is mounted as the file /etc/newrelic-infra.yml used to configure the agent. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option newrelic-infrastructure.config.proxy. The configuration option is passed down to the dependency newrelic-infrastructure modifying the values.yaml of nri-bundle: newrelic-infrastructure.config.proxy: https://user:password@hostname:port Copy Installing with manifest: When installing through the manifest, the proxy can be configured adding the NRIA_PROXY environment variable to the environment variable section of the DaemonSet configuration. The environment variable NRIA_VERBOSE in the DaemonSet YAML can be used as an example of the environment variable configuration. Set logging with a proxy If you’re using log forwarding for the Kubernetes integration, the plugin automatically detects the HTTP_PROXY and HTTPS_PROXY environment variables, and automatically uses them to set up the proxy configuration. For more information, see how to customize the proxy, or bypass it. Install Kubernetes events with a proxy There are three options to install the Kubernetes events integration with a proxy: Installing with chart nri-kube-events: The proxy can be configured setting the configuration option proxy in the values.yaml file. If set, the variable NRIA_PROXY will be added into the deployment.yaml template. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option nri-kube-events.proxy. The configuration option is passed down to the dependency nri-kube-events modifying the values.yaml of nri-bundle: nri-kube-events.proxy: https://user:password@hostname:port Copy Installing with manifest: When installing through the manifest, to configure the proxy set the NRIA_PROXY in the environment variable section of the infra container. Check the deployment template of the chart as an example of environment variable configuration. Install the Prometheus OpenMetrics integration with a proxy There are three options to install the Prometheus OpenMetric integration with a proxy: Installing with charts nri-prometheus: The proxy can be configured setting the configuration option config.emitter_proxy in the values.yaml. The config object is used to generate a configMap that mounted into the deployment under /etc/nri-prometheus/. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option nri-prometheus.config.emitter_proxy. The configuration option is passed down to the dependency nri-prometheus modifying the values.yaml of nri-bundle: nri-prometheus.config.emitter_proxy: \"http://localhost:8888 Copy Installing with the manifest: Configure the proxy uncommenting the emitter_proxy option directly in the manifest: # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" Copy Set the synthetics minion to use a proxy Check this README on how to set the proxy for the Synthetics minions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 108.12324,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure <em>Kubernetes</em> with a proxy",
        "sections": "Install <em>Kubernetes</em> <em>events</em> with a proxy",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "If you are running our <em>Kubernetes</em> <em>integration</em> with a proxy, you need to configure each component (the infrastructure agent, logging, <em>Kubernetes</em> <em>events</em>, etc.) to correctly work with your proxy. Install the infrastructure agent with a proxy There are three options to install the infrastructure agent"
      },
      "id": "617d584264441f2d01fbc6d7"
    },
    {
      "sections": [
        "Configure control plane monitoring",
        "Features",
        "Compatibility and requirements",
        "Discovery of master nodes and control plane components",
        "Configuration",
        "Important",
        "ETCD",
        "API server",
        "OpenShift configuration",
        "Set up mTLS from the ETCD client CA",
        "Tip",
        "To ease future installations",
        "Set up mTLS for ETCD in OpenShift",
        "See your data"
      ],
      "title": "Configure control plane monitoring",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "d648be240fd2f1b09bc1f29ef805bc0dd5c59e7a",
      "image": "https://docs.newrelic.com/static/209f301630c770f87ea8cbb1cace8e6e/8c557/new-relic-one-k8s-cluster-explorer-control-plane-parameters.png",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/configure-control-plane-monitoring/",
      "published_at": "2022-01-08T09:49:01Z",
      "updated_at": "2021-12-09T15:16:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic provides Control Plane support for your Kubernetes integration, allowing you to monitor and collect metrics from your cluster's Control Plane components. That data can then be found in New Relic and used to create queries and charts. Features We monitor and collect metrics from the following control plane components: ETCD: leader information, resident memory size, number of OS threads, consensus proposals data, etc. For a list of supported metrics, see ETCD data. API server: rate of apiserver requests, breakdown of apiserver requests by HTTP method and response code, etc. For the complete list of supported metrics, see API server data. Scheduler: requested CPU/memory vs available on the node, tolerations to taints, any set affinity or anti-affinity, etc. For the complete list of supported metrics, see Scheduler data. Controller manager: resident memory size, number of OS threads created, goroutines currently existing, etc. For the complete list of supported metrics, see Controller manager data. Compatibility and requirements Control plane monitoring requires Kubernetes integration version 1.16.0 or higher. Control plane monitoring support is not enabled for managed clusters. This is because providers (EKS, GKE, AKS, etc.) abstract away the concept of master nodes and control plane components, so that access to them is limited or non-existent. External control planes are not supported. The unprivileged version of the Kubernetes integration does not support control plane monitoring. OpenShift 4.x uses control plane component metric endpoints that are different than the default. Discovery of master nodes and control plane components The Kubernetes integration relies on the kubeadm labeling conventions to discover the master nodes and the control plane components. This means that master nodes should be labeled with node-role.kubernetes.io/master=\"\" or kubernetes.io/role=\"master\". The control plane components should have either the k8s-app or the tier and component labels. Refer to the following table for accepted label combinations and values: Component Label Endpoint API server Kubeadm / Kops / ClusterAPI k8s-app=kube-apiserver tier=control-plane component=kube-apiserver OpenShift app=openshift-kube-apiserver apiserver=true localhost:443/metrics by default (can be configured) if the request fails falls back to localhost:8080/metrics ETCD Kubeadm / Kops / ClusterAPI k8s-app=etcd-manager-main tier=control-plane component=etcd OpenShift k8s-app=etcd localhost:4001/metrics Scheduler Kubeadm / Kops / ClusterAPI k8s-app=kube-scheduler tier=control-plane component=kube-scheduler OpenShift app=openshift-kube-scheduler scheduler=true localhost:10251/metrics Controller manager Kubeadm / Kops / ClusterAPI k8s-app=kube-controller-manager tier=control-plane component=kube-controller-manager​ OpenShift app=kube-controller-manager kube-controller-manager=true localhost:10252/metrics When the integration detects that it is running inside a master node, it tries to find which components are running on the node by looking for pods that match the labels listed in the table above. For every running component, the integration makes a request to its metrics endpoint. Configuration Control plane monitoring is automatic for agents running inside master nodes. The only component that requires an extra step to run is ETCD, because it uses mutual TLS authentication (mTLS) for client requests. The API Server can also be configured to be queried using the Secure Port. Important Control plane monitoring for OpenShift 4.x requires additional configuration. For more information, see the OpenShift 4.x Configuration section. ETCD In order to set mTLS for querying ETCD, there are two configuration options that need to be set: Option Value ETCD_TLS_SECRET_NAME Name of a Kubernetes secret that contains the mTLS configuration. The secret should contain the following keys: cert: the certificate that identifies the client making the request. It should be signed by an ETCD trusted CA. key: the private key used to generate the client certificate. cacert: the root CA used to identify the ETCD server certificate. If the ETCD_TLS_SECRET_NAME option is not set, ETCD metrics won't be fetched. For step by step instructions on how to create a certificate and sign it with the ETCD client CA, see Set up mTLS from the ETCD client CA. ETCD_TLS_SECRET_NAMESPACE The namespace where the secret specified in the ETCD_TLS_SECRET_NAME was created. If not set, the default namespace is used. API server By default, the API server metrics are queried using the localhost:8080 unsecured endpoint. If this port is disabled, you can also query these metrics over the secure port. To enable this, set the following configuration option in the Kubernetes integration manifest file: Option Value API_SERVER_ENDPOINT_URL The (secure) URL to query the metrics. The API server uses localhost:443 by default Ensure that the ClusterRole has been updated to the newest version found in the manifest Added in version 1.15.0 Important Note that the port can be different according to the secure port used by the API server. For example, in Minikube the API server secure port is 8443 and therefore API_SERVER_ENDPOINT_URL should be set to https://localhost:8443 OpenShift configuration Control plane components on OpenShift 4.x use endpoint URLs that require SSL and service account based authentication. Therefore, the default endpoint URLs can not be used. To configure control plane monitoring on OpenShift, uncomment the following environment variables in the customized manifest. URL values are pre-configured to the default base URLs for the control plane monitoring metrics endpoints in OpenShift 4.x. - name: \"SCHEDULER_ENDPOINT_URL\" value: \"https://localhost:10259 - name: \"ETCD_ENDPOINT_URL\" value: \"https://localhost:9979\" - name: \"CONTROLLER_MANAGER_ENDPOINT_URL\" value: \"https://localhost:10257\" - name: \"API_SERVER_ENDPOINT_URL\" value: \"https://localhost:6443\" Copy Important Even though the custom ETCD_ENDPOINT_URL is defined, ETCD requires HTTPS and mTLS authentication to be configured. For more on configuring mTLS for ETCD in OpenShift, see Set up mTLS for ETCD in OpenShift. Important When installing through Helm openshift, specify the config to automatically include these endpoints. Setting openshift.enabled=true and openshift.version=\"4.x\" will include the secure endpoints and enable the /var/run/crio.sock runtime. Set up mTLS from the ETCD client CA The instructions below are based on the Kubernetes documentation. For more information, see Managing TLS certificates in a cluster. For OpenShift, see Set up mTLS for ETCD in OpenShift. To set up mTLS from the ETCD client CA: Download and install the tool cfssl, selecting the correct binaries for your OS from the list. Once installed, execute the following command: cat <<EOF | cfssl genkey - | cfssljson -bare server { \"hosts\": [ \"localhost\" ], \"CN\": \"newrelic-infra.pod.cluster.local\", \"key\": { \"algo\": \"ecdsa\", \"size\": 256 } } EOF Copy This command generates two files; server.csr containing the PEM encoded pkcs#10 certification request and server-key.pem containing the PEM encoded key to the certificate to be created. Use the generated certificate authority (CA) of ETCD to sign your CSR. Depending on your cluster configuration, you may already have this information. For default install configuration, download the CA certificate and the private key directly from ETCD with the following commands: kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.crt ./cacert -n kube-system kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.key ./cacert.key -n kube-system Copy Tip This requires that the etcd-manager-main pod has the label k8s-app=etcd-manager-main, which is a requirement for control plane monitoring . If your etc-manager-main pod is located in a different namespace, change the -n kube-system flags accordingly. With those files downloaded, use the following command to sign your CSRF: cfssl sign -ca cacert -ca-key cacert.key server.csr | cfssljson -bare cert Copy Create the secret that is used to retrieve the TLS config for making requests to ETC. We recommend renaming the certificate and the private key: cp cert.pem cert && cp server-key.pem key Copy kubectl -n default create secret generic newrelic-infra-etcd-tls-secret --from-file=./cert --from-file=./key --from-file=./cacert Copy To ease future installations Use the following commands to simultaneously create the CSR, retrieve the CA, generate the certificate by signing the CSR, and create the secret with all the required fields: cat <<EOF | cfssl genkey - | cfssljson -bare server && \\ kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.crt ./cacert -n kube-system && \\ kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.key ./cacert.key -n kube-system && \\ cp server-key.pem key && \\ cfssl sign -ca cacert -ca-key cacert.key server.csr | cfssljson -bare cert && \\ cp cert.pem cert && \\ kubectl -n default create secret generic newrelic-infra-etcd-tls-secret --from-file=./cert --from-file=./key --from-file=./cacert { \"hosts\": [ \"localhost\" ], \"CN\": \"newrelic-infra.pod.cluster.local\", \"key\": { \"algo\": \"ecdsa\", \"size\": 256 } } EOF Copy The last step is to update the configuration in the manifest and apply it. In the configuration section, there are two options related to ETCD mTLS: ETCD_TLS_SECRET_NAME with the name of the secret that we just created. ETCD_TLS_SECRET_NAMESPACE with the namespace that we used to create the secret. To complete the installation, add these variables to the container spec of the integration DaemonSet and apply the changes: - name: \"ETCD_TLS_SECRET_NAME” value: \"newrelic-infra-etcd-tls-secret\" - name: \"ETCD_TLS_SECRET_NAMESPACE\" value: \"default\" Copy Set up mTLS for ETCD in OpenShift Follow these instructions to set up mutual TLS authentication for ETCD in OpenShift 4.x: Export the ETCD client certificates from the cluster to an opaque secret. In a default managed OpenShift cluster, the secret is named kube-etcd-client-certs and it is stored in the openshift-monitoring namespace. kubectl get secret/kube-etcd-client-certs -n openshift-monitoring -o yaml > etcd-secret.yaml Copy Open the secret file and change the keys: Rename the certificate authority to cacert. Rename the client certificate to cert. Rename the client key to key. Optional: change the secret name and namespace to something meaningful. Remove these unnecessary keys in the metadata section: creationTimestamp resourceVersion selfLink uid Install the manifest with its new name and namespace: kubectl apply -f etcd-secret.yaml Copy Go to Update manifest configuration (the last step under Set up MTL from ETCD client) to configure the required environment variables. See your data If the integration has been been set up correctly, the Kubernetes cluster explorer contains all the Control Plane components and their status in a dedicated section, as shown below. one.newrelic.com > Kubernetes Cluster Explorer: Use the Kubernetes cluster explorer to monitor and collect metrics from your cluster's Control Plane components You can also check for Control Plane data with this NRQL query: SELECT latest(timestamp) FROM K8sApiServerSample, K8sEtcdSample, K8sSchedulerSample, K8sControllerManagerSample FACET entityName where clusterName = 'MY_CLUSTER_NAME' Copy Tip If you still can't see Control Plane data, try the solution described in Kubernetes integration troubleshooting: Not seeing data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 92.34313,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "New Relic provides Control Plane support for your <em>Kubernetes</em> <em>integration</em>, allowing you to monitor and collect metrics from your cluster&#x27;s Control Plane components. That data can then be found in New Relic and used to create queries and charts. Features We monitor and collect metrics from"
      },
      "id": "617d735f64441f9d39fbe7ee"
    }
  ],
  "/docs/kubernetes-pixie/kubernetes-integration/kubernetes-events/kubernetes-integration-predefined-alert-policy": [
    {
      "sections": [
        "Install the Kubernetes integration manually using Helm",
        "Compatibility and requirements",
        "Install Kubernetes integration with Helm",
        "Install with Helm 3 and nri-bundle (recommended)",
        "Installing and configuring nri-bundle with Helm",
        "Install with Helm 2 and nri-bundle (legacy)",
        "Installation instructions for Helm 2",
        "Important",
        "Tip",
        "Helm configuration options",
        "Upgrade using Helm",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data",
        "Reduce data ingest",
        "Prometheus OpenMetrics Integration",
        "New Relic Logging",
        "New Relic Pixie Integration",
        "Uninstall Kubernetes integration"
      ],
      "title": "Install the Kubernetes integration manually using Helm",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "29d42af98d41e7e4e7be86f0150254e132cb6b6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/install-kubernetes-integration-using-helm/",
      "published_at": "2022-01-08T09:49:44Z",
      "updated_at": "2022-01-08T09:49:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Helm is a package manager on top of Kubernetes. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in Kubernetes. To install the integration using Helm, we recommend our Kubernetes automated installer, which will prompt for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our integration as plain manifests rather than a Helm release. See Kubernetes integration: install and configure for more details about how to use our automated installer. Start the installer This page describes in more depth how to install and configure the New Relic integration without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the Kubernetes integration. Our charts are also compatible with Helm 2, but support for it might be removed in the future. To install the Kubernetes integration using Helm, you will need your New Relic account license key and your Kubernetes cluster's name: Find and copy your New Relic license key. Find the name of your cluster with this command: kubectl config current-context Copy Note these values somewhere safe, as you will need them later during the installation process. Install Kubernetes integration with Helm Install with Helm 3 and nri-bundle (recommended) New Relic has several charts for the different components which offer different features for the platform: newrelic-infrastructure: Contains the main Kubernetes integration and the infrastructure agent. This is the core component for the New Relic Kubernetes experience, responsible of reporting most of the data that is surfaced in dashboard and the Kubernetes Cluster Explorer. newrelic-logging: Provides a DaemonSet with New Relic's Fluent Bit output plugin to easily forward your logs to New Relic. nri-kube-events: Collects and reports cluster events (such as kubectl get events) to New Relic. nri-prometheus: New Relic's Prometheus OpenMetrics Integration, automatically scrapes Prometheus endpoints present in the cluser and reports metrics to New Relic. nri-metadata-injection: Sets up a minimal MutatingAdmissionWebhook that injects a couple of environment to containers. These contain metadata about the cluster and New Relic installation and will be later picked up by applications instrumented using APM, allowing to correlate APM and infrastructure data. nri-statsd: New Relic StatsD integration. Additionally, New Relic provides nri-bundle, a chart which pulls a selectable set of the charts mentioned above. nri-bundle can also install Kube State Metrics and Pixie for you if needed. While it is possible to install those charts separately, we strongly recommend using the nri-bundle chart for Kubernetes deployments, as it ensures that values across all the charts are consistent and provides full control over which components are installed, as well as the possibility to configure all of them as Helm dependencies. This is the same chart that is used and referenced by our automated installer. Installing and configuring nri-bundle with Helm Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Please notice and adjust the following flags: global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY: Must be set to a valid License Key for your account. global.cluster=K8S_CLUSTER_NAME: Is used to identify the cluster in the New Relic UI, so should be a descriptive value not used by any other Kubernetes cluster configured in your New Relic account. ksm.enabled=true: Setting this to true will automatically install Kube State Metrics (KSM) for you, which is required for our integration to run. You can set this to false if KSM is already present in your cluster, even if it is on a different namespace. newrelic-infrastructure.privileged=true: Can be set to false to install a trimmed down version of our integration that does not require extra privileges, such as hostPath mounts or running containers as root. Please note that this will disable detailed process collection from the host. For performance reasons, our logging solution still requires hostPath mounts, regardless of the value of this flag. If this is not allowedin your cluster, you will need to disable the logging solution by specifying logging.enabled=false. prometheus.enabled=true: Will deploy our Prometheus OpenMetrics integration, which automatically collects data from prometheus endpoints present in the cluster. webhook.enabled=true: Will install our minimal webhook, which adds environment variables that, in turn, allows linking applications instrumented with APM with infrastructure data. Our chart has a comprehensive set of flags and tunables that can be edited to configure our solution to your particular needs. For a full list of all the flags that can be configured, please check the chart's README and the default values.yaml file. Install the Kubernetes integration by running the customized command without --debug and --dry-run: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl -n newrelic get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node for newrelic-infrastructure, as well as one Deployment and one pod for any other component that you have enabled. Install with Helm 2 and nri-bundle (legacy) Installation instructions for Helm 2 Important Helm 2 has been deprecated and New Relic does not recommend using it for deployments. Instructions in this section are provided for legacy systems only. To install using Helm 2: Ensure that Helm 2, including their cluster-side components (such as Tiller) are properly installed and configured. Please check the official documentation for more details. Set the cluster where you want to install the agent: kubectl config set-cluster DESIRED_CLUSTER Copy To see the available clusters, run kubectl config get-clusters Make sure that kube-state-metrics is installed on your machine: kubectl get deployment --all-namespaces | grep kube-state-metrics Copy If it's not installed, follow the instructions in the kube-state-metrics GitHub repo to install it. Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. This step uses the --dry-run and --debug switches and therefore the agent is not installed. helm install newrelic/newrelic-infrastructure \\ --dry-run \\ --debug \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Install the New Relic Kubernetes integration: helm install newrelic/newrelic-infrastructure \\ --set licenseKey=your_new_relic_license_key \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Tip Note that the --dry-run and --debug switches have been removed. Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node. Helm configuration options When you install or upgrade the Kubernetes integration with Helm using the command line, you can pass your configuration variables with the --set flag. helm install newrelic/newrelic-infrastructure \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=YOUR_CLUSTER_NAME Copy A full list of the configuration parameters can be found in the newrelic-infrastructure chart README Upgrade using Helm To update your Kubernetes integration installed via Helm: Update the local chart repository: helm repo update Copy Update the release by running again the appropriate helm upgrade --install ... command in the section above. Monitor services running on Kubernetes After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages. Reduce data ingest Our charts support setting an option to reduce the amount of data ingest at the cost of dropping detailed information. To enable it, set global.lowDataMode=true in the nri-bundle chart. lowDataMode affects three specific components of the nri-bundle chart outlined below. Prometheus OpenMetrics Integration If lowDataMode is enabled, the following metrics are excluded by default as they are already collected and used by the New Relic Kubernetes Integration. - kube_ - container_ - machine_ - cadvisor_ Copy New Relic Logging If lowDataMode is enabled, Labels and Annotations are set to Off in the Filter section of the fluent-bit.conf file. This means that this detail will be dropped from the container log files which reduces the overall data ingest into New Relic. The following fields are retained: Allowlist_key container_name Allowlist_key namespace_name Allowlist_key pod_name Allowlist_key stream Allowlist_key log Copy Low Data Mode Log Example Complete Log Record [ { \"cluster_name\": \"api-test\", \"kubernetes\": { \"annotations\": { \"kubernetes.io/psp\": \"eks.privileged\" }, \"container_hash\": \"fryckbos/test@sha256:5b098eaf3c7d5b3585eb10cebee63665b6208bea31ef31a3f0856c5ffdda644b\", \"container_image\": \"fryckbos/test:latest\", \"container_name\": \"newrelic-logging\", \"docker_id\": \"134e1daf63761baa15e035b08b7aea04518a0f0e50af4215131a50c6a379a072\", \"host\": \"ip-192-168-17-123.ec2.internal\", \"labels\": { \"app\": \"newrelic-logging\", \"app.kubernetes.io/name\": \"newrelic-logging\", \"controller-revision-hash\": \"84db95db86\", \"pod-template-generation\": \"1\", \"release\": \"nri-bundle\" }, \"namespace_name\": \"nrlogs\", \"pod_id\": \"54556e3e-719c-46b5-af69-020b75d69bf1\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\" }, \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"plugin\": { \"source\": \"kubernetes\", \"type\": \"fluent-bit\", \"version\": \"1.8.1\" }, \"stream\": \"stderr\", \"time\": \"2021-09-14T12:30:49.138824971Z\", \"timestamp\": 1631622649138 } ] Copy Log Record after enabling lowDataMode. [ { \"cluster_name\": \"api-test\", \"container_name\": \"newrelic-logging\", \"namespace_name\": \"nrlogs\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\", \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"stream\": \"stderr\", \"timestamp\": 1631622649138 } ] Copy New Relic Pixie Integration If lowDataMode is enabled, the newrelic-pixie integration performs heavier sampling on Pixie spans and reduces the collection interval from 10 seconds to 15 seconds. lowDataMode settings: HTTP_SPAN_LIMIT: 750 DB_SPAN_LIMIT: 250 COLLECT_INTERVAL_SEC: 15 Copy The default settings for these parameters and others can be found in the newrelic-pixie-integration Github repo. Uninstall Kubernetes integration To uninstall the Kubernetes integration using Helm, run the following command. Replace some-manifest-file.yml with your specific filename. helm uninstall newrelic -n newrelic if kubectl then : kubectl delete -f some-manifest-file.yml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 218.82089,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "sections": "<em>Install</em> the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "Helm is a package manager on top of <em>Kubernetes</em>. It facilitates <em>installation</em>, upgrades, or revision tracking, and it manages dependencies for the services that you install in <em>Kubernetes</em>. To install the <em>integration</em> using Helm, we recommend our <em>Kubernetes</em> automated installer, which will prompt"
      },
      "id": "617d5841196a67bb40f7c1de"
    },
    {
      "sections": [
        "Configure Kubernetes with a proxy",
        "Install the infrastructure agent with a proxy",
        "Set logging with a proxy",
        "Install Kubernetes events with a proxy",
        "Install the Prometheus OpenMetrics integration with a proxy",
        "Set the synthetics minion to use a proxy"
      ],
      "title": "Configure Kubernetes with a proxy",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "9d7794244b451d878d00db0f84c4709e43ce138c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/configure-kubernetes-proxy/",
      "published_at": "2022-01-08T09:49:01Z",
      "updated_at": "2021-12-25T07:57:45Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are running our Kubernetes integration with a proxy, you need to configure each component (the infrastructure agent, logging, Kubernetes events, etc.) to correctly work with your proxy. Install the infrastructure agent with a proxy There are three options to install the infrastructure agent with a proxy: Installing with chart newrelic-infrastructure: The proxy can be configured setting the configuration option config.proxy as described in the values.yaml of the chart. The whole config object is used to generate a configMap that is mounted as the file /etc/newrelic-infra.yml used to configure the agent. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option newrelic-infrastructure.config.proxy. The configuration option is passed down to the dependency newrelic-infrastructure modifying the values.yaml of nri-bundle: newrelic-infrastructure.config.proxy: https://user:password@hostname:port Copy Installing with manifest: When installing through the manifest, the proxy can be configured adding the NRIA_PROXY environment variable to the environment variable section of the DaemonSet configuration. The environment variable NRIA_VERBOSE in the DaemonSet YAML can be used as an example of the environment variable configuration. Set logging with a proxy If you’re using log forwarding for the Kubernetes integration, the plugin automatically detects the HTTP_PROXY and HTTPS_PROXY environment variables, and automatically uses them to set up the proxy configuration. For more information, see how to customize the proxy, or bypass it. Install Kubernetes events with a proxy There are three options to install the Kubernetes events integration with a proxy: Installing with chart nri-kube-events: The proxy can be configured setting the configuration option proxy in the values.yaml file. If set, the variable NRIA_PROXY will be added into the deployment.yaml template. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option nri-kube-events.proxy. The configuration option is passed down to the dependency nri-kube-events modifying the values.yaml of nri-bundle: nri-kube-events.proxy: https://user:password@hostname:port Copy Installing with manifest: When installing through the manifest, to configure the proxy set the NRIA_PROXY in the environment variable section of the infra container. Check the deployment template of the chart as an example of environment variable configuration. Install the Prometheus OpenMetrics integration with a proxy There are three options to install the Prometheus OpenMetric integration with a proxy: Installing with charts nri-prometheus: The proxy can be configured setting the configuration option config.emitter_proxy in the values.yaml. The config object is used to generate a configMap that mounted into the deployment under /etc/nri-prometheus/. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option nri-prometheus.config.emitter_proxy. The configuration option is passed down to the dependency nri-prometheus modifying the values.yaml of nri-bundle: nri-prometheus.config.emitter_proxy: \"http://localhost:8888 Copy Installing with the manifest: Configure the proxy uncommenting the emitter_proxy option directly in the manifest: # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" Copy Set the synthetics minion to use a proxy Check this README on how to set the proxy for the Synthetics minions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 144.9012,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure <em>Kubernetes</em> with a proxy",
        "sections": "<em>Install</em> <em>Kubernetes</em> events with a proxy",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "If you are running our <em>Kubernetes</em> <em>integration</em> with a proxy, you need to configure each component (the infrastructure agent, logging, <em>Kubernetes</em> events, etc.) to correctly work with your proxy. Install the infrastructure agent with a proxy There are three options to install the infrastructure agent"
      },
      "id": "617d584264441f2d01fbc6d7"
    },
    {
      "sections": [
        "Configure control plane monitoring",
        "Features",
        "Compatibility and requirements",
        "Discovery of master nodes and control plane components",
        "Configuration",
        "Important",
        "ETCD",
        "API server",
        "OpenShift configuration",
        "Set up mTLS from the ETCD client CA",
        "Tip",
        "To ease future installations",
        "Set up mTLS for ETCD in OpenShift",
        "See your data"
      ],
      "title": "Configure control plane monitoring",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "d648be240fd2f1b09bc1f29ef805bc0dd5c59e7a",
      "image": "https://docs.newrelic.com/static/209f301630c770f87ea8cbb1cace8e6e/8c557/new-relic-one-k8s-cluster-explorer-control-plane-parameters.png",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/configure-control-plane-monitoring/",
      "published_at": "2022-01-08T09:49:01Z",
      "updated_at": "2021-12-09T15:16:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic provides Control Plane support for your Kubernetes integration, allowing you to monitor and collect metrics from your cluster's Control Plane components. That data can then be found in New Relic and used to create queries and charts. Features We monitor and collect metrics from the following control plane components: ETCD: leader information, resident memory size, number of OS threads, consensus proposals data, etc. For a list of supported metrics, see ETCD data. API server: rate of apiserver requests, breakdown of apiserver requests by HTTP method and response code, etc. For the complete list of supported metrics, see API server data. Scheduler: requested CPU/memory vs available on the node, tolerations to taints, any set affinity or anti-affinity, etc. For the complete list of supported metrics, see Scheduler data. Controller manager: resident memory size, number of OS threads created, goroutines currently existing, etc. For the complete list of supported metrics, see Controller manager data. Compatibility and requirements Control plane monitoring requires Kubernetes integration version 1.16.0 or higher. Control plane monitoring support is not enabled for managed clusters. This is because providers (EKS, GKE, AKS, etc.) abstract away the concept of master nodes and control plane components, so that access to them is limited or non-existent. External control planes are not supported. The unprivileged version of the Kubernetes integration does not support control plane monitoring. OpenShift 4.x uses control plane component metric endpoints that are different than the default. Discovery of master nodes and control plane components The Kubernetes integration relies on the kubeadm labeling conventions to discover the master nodes and the control plane components. This means that master nodes should be labeled with node-role.kubernetes.io/master=\"\" or kubernetes.io/role=\"master\". The control plane components should have either the k8s-app or the tier and component labels. Refer to the following table for accepted label combinations and values: Component Label Endpoint API server Kubeadm / Kops / ClusterAPI k8s-app=kube-apiserver tier=control-plane component=kube-apiserver OpenShift app=openshift-kube-apiserver apiserver=true localhost:443/metrics by default (can be configured) if the request fails falls back to localhost:8080/metrics ETCD Kubeadm / Kops / ClusterAPI k8s-app=etcd-manager-main tier=control-plane component=etcd OpenShift k8s-app=etcd localhost:4001/metrics Scheduler Kubeadm / Kops / ClusterAPI k8s-app=kube-scheduler tier=control-plane component=kube-scheduler OpenShift app=openshift-kube-scheduler scheduler=true localhost:10251/metrics Controller manager Kubeadm / Kops / ClusterAPI k8s-app=kube-controller-manager tier=control-plane component=kube-controller-manager​ OpenShift app=kube-controller-manager kube-controller-manager=true localhost:10252/metrics When the integration detects that it is running inside a master node, it tries to find which components are running on the node by looking for pods that match the labels listed in the table above. For every running component, the integration makes a request to its metrics endpoint. Configuration Control plane monitoring is automatic for agents running inside master nodes. The only component that requires an extra step to run is ETCD, because it uses mutual TLS authentication (mTLS) for client requests. The API Server can also be configured to be queried using the Secure Port. Important Control plane monitoring for OpenShift 4.x requires additional configuration. For more information, see the OpenShift 4.x Configuration section. ETCD In order to set mTLS for querying ETCD, there are two configuration options that need to be set: Option Value ETCD_TLS_SECRET_NAME Name of a Kubernetes secret that contains the mTLS configuration. The secret should contain the following keys: cert: the certificate that identifies the client making the request. It should be signed by an ETCD trusted CA. key: the private key used to generate the client certificate. cacert: the root CA used to identify the ETCD server certificate. If the ETCD_TLS_SECRET_NAME option is not set, ETCD metrics won't be fetched. For step by step instructions on how to create a certificate and sign it with the ETCD client CA, see Set up mTLS from the ETCD client CA. ETCD_TLS_SECRET_NAMESPACE The namespace where the secret specified in the ETCD_TLS_SECRET_NAME was created. If not set, the default namespace is used. API server By default, the API server metrics are queried using the localhost:8080 unsecured endpoint. If this port is disabled, you can also query these metrics over the secure port. To enable this, set the following configuration option in the Kubernetes integration manifest file: Option Value API_SERVER_ENDPOINT_URL The (secure) URL to query the metrics. The API server uses localhost:443 by default Ensure that the ClusterRole has been updated to the newest version found in the manifest Added in version 1.15.0 Important Note that the port can be different according to the secure port used by the API server. For example, in Minikube the API server secure port is 8443 and therefore API_SERVER_ENDPOINT_URL should be set to https://localhost:8443 OpenShift configuration Control plane components on OpenShift 4.x use endpoint URLs that require SSL and service account based authentication. Therefore, the default endpoint URLs can not be used. To configure control plane monitoring on OpenShift, uncomment the following environment variables in the customized manifest. URL values are pre-configured to the default base URLs for the control plane monitoring metrics endpoints in OpenShift 4.x. - name: \"SCHEDULER_ENDPOINT_URL\" value: \"https://localhost:10259 - name: \"ETCD_ENDPOINT_URL\" value: \"https://localhost:9979\" - name: \"CONTROLLER_MANAGER_ENDPOINT_URL\" value: \"https://localhost:10257\" - name: \"API_SERVER_ENDPOINT_URL\" value: \"https://localhost:6443\" Copy Important Even though the custom ETCD_ENDPOINT_URL is defined, ETCD requires HTTPS and mTLS authentication to be configured. For more on configuring mTLS for ETCD in OpenShift, see Set up mTLS for ETCD in OpenShift. Important When installing through Helm openshift, specify the config to automatically include these endpoints. Setting openshift.enabled=true and openshift.version=\"4.x\" will include the secure endpoints and enable the /var/run/crio.sock runtime. Set up mTLS from the ETCD client CA The instructions below are based on the Kubernetes documentation. For more information, see Managing TLS certificates in a cluster. For OpenShift, see Set up mTLS for ETCD in OpenShift. To set up mTLS from the ETCD client CA: Download and install the tool cfssl, selecting the correct binaries for your OS from the list. Once installed, execute the following command: cat <<EOF | cfssl genkey - | cfssljson -bare server { \"hosts\": [ \"localhost\" ], \"CN\": \"newrelic-infra.pod.cluster.local\", \"key\": { \"algo\": \"ecdsa\", \"size\": 256 } } EOF Copy This command generates two files; server.csr containing the PEM encoded pkcs#10 certification request and server-key.pem containing the PEM encoded key to the certificate to be created. Use the generated certificate authority (CA) of ETCD to sign your CSR. Depending on your cluster configuration, you may already have this information. For default install configuration, download the CA certificate and the private key directly from ETCD with the following commands: kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.crt ./cacert -n kube-system kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.key ./cacert.key -n kube-system Copy Tip This requires that the etcd-manager-main pod has the label k8s-app=etcd-manager-main, which is a requirement for control plane monitoring . If your etc-manager-main pod is located in a different namespace, change the -n kube-system flags accordingly. With those files downloaded, use the following command to sign your CSRF: cfssl sign -ca cacert -ca-key cacert.key server.csr | cfssljson -bare cert Copy Create the secret that is used to retrieve the TLS config for making requests to ETC. We recommend renaming the certificate and the private key: cp cert.pem cert && cp server-key.pem key Copy kubectl -n default create secret generic newrelic-infra-etcd-tls-secret --from-file=./cert --from-file=./key --from-file=./cacert Copy To ease future installations Use the following commands to simultaneously create the CSR, retrieve the CA, generate the certificate by signing the CSR, and create the secret with all the required fields: cat <<EOF | cfssl genkey - | cfssljson -bare server && \\ kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.crt ./cacert -n kube-system && \\ kubectl cp $(kubectl get pods -l k8s-app=etcd-manager-main -n kube-system -o jsonpath=\"{.items[0].metadata.name}\"):/etc/kubernetes/pki/etcd-manager/etcd-clients-ca.key ./cacert.key -n kube-system && \\ cp server-key.pem key && \\ cfssl sign -ca cacert -ca-key cacert.key server.csr | cfssljson -bare cert && \\ cp cert.pem cert && \\ kubectl -n default create secret generic newrelic-infra-etcd-tls-secret --from-file=./cert --from-file=./key --from-file=./cacert { \"hosts\": [ \"localhost\" ], \"CN\": \"newrelic-infra.pod.cluster.local\", \"key\": { \"algo\": \"ecdsa\", \"size\": 256 } } EOF Copy The last step is to update the configuration in the manifest and apply it. In the configuration section, there are two options related to ETCD mTLS: ETCD_TLS_SECRET_NAME with the name of the secret that we just created. ETCD_TLS_SECRET_NAMESPACE with the namespace that we used to create the secret. To complete the installation, add these variables to the container spec of the integration DaemonSet and apply the changes: - name: \"ETCD_TLS_SECRET_NAME” value: \"newrelic-infra-etcd-tls-secret\" - name: \"ETCD_TLS_SECRET_NAMESPACE\" value: \"default\" Copy Set up mTLS for ETCD in OpenShift Follow these instructions to set up mutual TLS authentication for ETCD in OpenShift 4.x: Export the ETCD client certificates from the cluster to an opaque secret. In a default managed OpenShift cluster, the secret is named kube-etcd-client-certs and it is stored in the openshift-monitoring namespace. kubectl get secret/kube-etcd-client-certs -n openshift-monitoring -o yaml > etcd-secret.yaml Copy Open the secret file and change the keys: Rename the certificate authority to cacert. Rename the client certificate to cert. Rename the client key to key. Optional: change the secret name and namespace to something meaningful. Remove these unnecessary keys in the metadata section: creationTimestamp resourceVersion selfLink uid Install the manifest with its new name and namespace: kubectl apply -f etcd-secret.yaml Copy Go to Update manifest configuration (the last step under Set up MTL from ETCD client) to configure the required environment variables. See your data If the integration has been been set up correctly, the Kubernetes cluster explorer contains all the Control Plane components and their status in a dedicated section, as shown below. one.newrelic.com > Kubernetes Cluster Explorer: Use the Kubernetes cluster explorer to monitor and collect metrics from your cluster's Control Plane components You can also check for Control Plane data with this NRQL query: SELECT latest(timestamp) FROM K8sApiServerSample, K8sEtcdSample, K8sSchedulerSample, K8sControllerManagerSample FACET entityName where clusterName = 'MY_CLUSTER_NAME' Copy Tip If you still can't see Control Plane data, try the solution described in Kubernetes integration troubleshooting: Not seeing data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 125.89959,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "To ease future <em>installations</em>",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "New Relic provides Control Plane support for your <em>Kubernetes</em> <em>integration</em>, allowing you to monitor and collect metrics from your cluster&#x27;s Control Plane components. That data can then be found in New Relic and used to create queries and charts. Features We monitor and collect metrics from"
      },
      "id": "617d735f64441f9d39fbe7ee"
    }
  ],
  "/docs/kubernetes-pixie/kubernetes-integration/link-apps-services/monitor-services-running-kubernetes": [
    {
      "sections": [
        "New Relic Metrics Adapter",
        "BETA FEATURE",
        "Requirements",
        "Installation",
        "Tip",
        "Configuration",
        "How it works",
        "Caution",
        "Troubleshooting",
        "Get verbose logs",
        "Get raw metrics",
        "Metrics not working"
      ],
      "title": "New Relic Metrics Adapter",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Link apps and services"
      ],
      "external_id": "e2a825763b10ccf4bd1bd8423e2209f66dfb61bb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/newrelic-hpa-metrics-adapter/newrelic-metrics-adapter/",
      "published_at": "2022-01-08T09:01:12Z",
      "updated_at": "2021-11-13T08:47:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is still in development, but we encourage you to try it out! You can use metrics from your New Relic account to autoscale applications and services in your Kubernetes cluster by deploying the New Relic Metrics Adapter. This adapter fetches the metric values from New Relic and makes them available for the Horizontal Pod Autoscalers. The newrelic-k8s-metrics-adapter implements the external.metrics.k8s.io API to support the use of external metrics based New Relic NRQL queries results. Once deployed, the value for each configured metric is fetched using the NerdGraph API based on the configured NRQL query. The metrics adapter exposes the metrics over a secured endpoint with TLS. New Relic metrics adapter in a cluster. Requirements Kubernetes 1.16 or higher. The New Relic Kubernetes integration. New Relic's user API key. No other External Metrics Adapter installed in the cluster. Installation To install the New Relic Metrics Adapter, we provide the newrelic-k8s-metrics-adapter Helm chart, which is also included in the nri-bundle chart used to deploy all New Relic Kubernetes components. If not already installed, install our Kubernetes integration. Upgrade the installation to include the New Relic Metrics Adapter with the following command: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace --reuse-values \\ --set metrics-adapter.enabled=true \\ --set newrelic-k8s-metrics-adapter.personalAPIKey=YOUR_NEW_RELIC_PERSONAL_API_KEY \\ --set newrelic-k8s-metrics-adapter.config.accountID=YOUR_NEW_RELIC_ACCOUNT_ID \\ --set newrelic-k8s-metrics-adapter.config.externalMetrics.external_metric_name.query=NRQL query Copy Please notice and adjust the following flags: metrics-adapter.enabled: Must be set to true so the metrics adapter chart is installed. newrelic-k8s-metrics-adapter.personalAPIKey: Must be set to valid New Relic Personal API key. newrelic-k8s-metrics-adapter.accountID: Must be set to valid New Relic account where metrics are going to be fetched from. newrelic-k8s-metrics-adapter.config.externalMetrics.<var>external_metric_name</var>.<var>query</var>: Adds a new external metric where: <var>external_metric_name</var>: The metric name. <var>query</var>: The base NRQL query that is used to get the value for the metric. Tip Alternatively, you can use a values.yaml file that can be passed to the helm command with the --values flag. Values files can contain all parameters needed to configure the metrics explained in the configuration section. Configuration You can configure multiple metrics in the metrics adapter and change some parameters to modify the behaviour of the metrics cache and filtering. To see the full list and descriptions of all parameters that can be modified, refer to the chart README.md and values.yaml files. How it works The following example is a Helm values file that enable the metrics adapter on the nri-bundle chart installation, and configures the nginx_average_requests metric: metrics-adapter: enabled: true newrelic-k8s-metrics-adapter: personalAPIKey: <Personal API Key> config: accountID: <Account ID> externalMetrics: nginx_average_requests: query: \"FROM Metric SELECT average(nginx.server.net.requestsPerSecond) SINCE 2 MINUTES AGO\" Copy Caution The default time span for metrics is 1h. Therefore, you should define queries with the SINCE clause to adjust the time span according to your environment and needs. There is an HPA consuming the external metric as follows: kind: HorizontalPodAutoscaler apiVersion: autoscaling/v2beta2 metadata: name: nginx-scaler spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: nginx minReplicas: 1 maxReplicas: 10 metrics: - type: External external: metric: name: nginx_average_requests selector: matchLabels: k8s.namespaceName: nginx target: type: Value value: 10000 Copy Based on the HPA definition, the controller manager fetches the metrics from the external metrics API which are served by the New Relic metrics adapter. The New Relic metrics adapter receives the query including the nginx_average_requests metric name and all the selectors, and searches for a matching metric name in the internal memory based on the configured metrics. Then, it adds the selectors to the query to form a final query that is executed using NerdGraph to fetch the value from New Relic. The above example will generate a query like the following: FROM Metric SELECT average(nginx.server.net.requestsPerSecond) WHERE clusterName=<clusterName> AND `k8s.namespaceName`='nginx' SINCE 2 MINUTES AGO Copy Notice that a clusterName filter has been automatically added to the query to exclude metrics from other clusters in the same account. You can remove it by using the removeClusterFilter configuration parameter. Also the value is cached for a period of time defined by the cacheTTLSeconds configuration parameter, whose default is 30 seconds. Troubleshooting Get verbose logs Most common errors are displayed in the standard (non-verbose) logs. If you're doing a more in-depth investigation on your own or with New Relic Support, you can enable verbose mode. To get verbose logging details for an integration using Helm: Enable verbose logging: bash Copy $ helm upgrade -n <namespace> --reuse-values newrelic-bundle --set newrelic-k8s-metrics-adapter.verboseLog=true newrelic/nri-bundle Leave on verbose mode for a few minutes, or until enough activity has occurred. When you have the information you need, disable verbose logging: bash Copy $ helm upgrade --reuse-values newrelic-bundle --set newrelic-k8s-metrics-adapter.verboseLog=false newrelic/nri-bundle Caution Verbose mode increases significantly the amount of information sent to log files. Enable this mode temporarily, only for troubleshooting purposes, and reset the log level when finished. Get raw metrics Sometimes it's useful to get the list of available metrics and also to get the current value of an specific metric. To get the list of metrics available, run: bash Copy $ kubectl get --raw \"/apis/external.metrics.k8s.io/v1beta1/\" To get the value for a specific metric with a selector, run: bash Copy $ kubectl get --raw \"/apis/external.metrics.k8s.io/v1beta1/namespaces/*/<metric_name>?labelSelector=<selector_key>=<selector_value>\" Tip You must replace <metric_name>, <selector_key> and <selector_value> with your values. Metrics not working There are some usual errors that could cause a metric fail to retrieve the value. These errors are showed in the status of the metrics when you describe the HPA or are printed when you get the raw metrics directly. executing query: NRQL Syntax Error: Error at line...: The query that is being run has syntax errors. The same error message gives you the executed query and position of the error. You can try this query inside the New Relic query builder and correct the configuration from the adapter. extracting return value: expected first value to be of type \"float64\", got %!q(<nil>): The query doesn't return any value. The same error message gives you the executed query so you can try this query inside the New Relic query builder and correct the configuration from the adapter or the match selectors in the HPA.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 216.98819,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Link</em> <em>apps</em> <em>and</em> <em>services</em>",
        "body": "BETA FEATURE This feature is still in development, but we encourage you to try it out! You can use metrics from your New Relic account to autoscale applications and <em>services</em> in your <em>Kubernetes</em> cluster by deploying the New Relic Metrics Adapter. This adapter fetches the metric values from New Relic"
      },
      "id": "6175209d28ccbcf310c6bb2f"
    },
    {
      "sections": [
        "Tutorial: Monitor Redis running on Kubernetes",
        "What you need",
        "Step 1: Set up an example Redis application",
        "Step 2: Enable monitoring of Redis instances"
      ],
      "title": "Tutorial: Monitor Redis running on Kubernetes",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Link apps and services"
      ],
      "external_id": "52427a948d6fa2c673688066cc87cbd6c97863b4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/link-apps-services/tutorial-monitor-redis-running-kubernetes/",
      "published_at": "2022-01-08T12:35:55Z",
      "updated_at": "2021-10-24T03:07:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have a service running on Kubernetes, and it's a service we support, you can enable monitoring of that service by adding a configuration section for that integration to the Kubernetes integration's config. This tutorial shows how to enable monitoring for a Redis service running on the Kubernetes PHP Guestbook. For the general procedure, see Monitor a Kubernetes-running service. What you need See the general requirements for this feature, including supported services. The kubectl command-line tool must be configured to communicate with your cluster. If you don't have a cluster, you can create one using Minikube. Step 1: Set up an example Redis application This tutorial builds on the Kubernetes tutorial Deploying a PHP Guestbook application with Redis. Skip the Kubernetes tutorial and run the following command to set up the application needed for our tutorial: kubectl create -f https://raw.githubusercontent.com/kubernetes/examples/master/guestbook/all-in-one/guestbook-all-in-one.yaml Copy If you'd like to first complete the Kubernetes tutorial, follow their tutorial instructions but do not follow the instructions in the Cleaning up section. Step 2: Enable monitoring of Redis instances The PHP Guestbook application has three Redis instances: one master and two slave instances. Each instance is tagged with a label where app=redis. For this example, we're using our Redis monitoring integration. It can monitor both master and slave instances of Redis, so we don’t have to distinguish between them. In the Kubernetes integration's YAML config file (newrelic-infrastructure-k8s-latest.yaml), you need to update the nri-integration-cfg section. From the list of integration configs, get the Redis integration YAML and add it to the Kubernetes config. The Redis YAML is highlighted below. --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: redis-config.yml: | --- # Run auto discovery to find pods with label \"app=redis\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Run discovery for Kubernetes. Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: redis integrations: - name: nri-redis env: # using the discovered IP as the hostname address HOSTNAME: ${discovery.ip} PORT: 6379 KEYS: '{\"0\":[\"<KEY_1>\"],\"1\":[\"<KEY_2>\"]}' REMOTE_MONITORING: true labels: env: production Copy Deploy the updated service: kubectl create -f newrelic-infrastructure-k8s-latest.yaml Copy You should be able to see the following in the logs for the pod newrelic-infra: time=\"2019-12-23T17:37:07Z\" level=info msg=\"Integration health check starting\" instance=redis-metrics integration=com.newrelic.redis prefix=integration/com.newrelic.redis working-dir=/var/db/newrelic-infra/newrelic-integrations time=\"2019-12-23T17:37:07Z\" level=info msg=\"Integration health check finished with success\" instance=redis-metrics integration=com.newrelic.redis prefix=integration/com.newrelic.redis working-dir=/var/db/newrelic-infra/newrelic-integrations Copy If there are no errors, you should see Redis data in the Infrastructure UI. To find the Redis dashboards, go to one.newrelic.com > Infrastructure > Third party services, and select the Redis dashboard. For the general procedure of how to monitor services running on Kubernetes, including more detail about how configuration works, see Monitor a Kubernetes-running service.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 211.79741,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Tutorial: Monitor Redis running on <em>Kubernetes</em>",
        "sections": "Tutorial: Monitor Redis running on <em>Kubernetes</em>",
        "tags": "<em>Link</em> <em>apps</em> <em>and</em> <em>services</em>",
        "body": " is highlighted below. --- apiVersion: v1 kind: ConfigMap metadata: name: nri-<em>integration</em>-cfg namespace: default data: redis-config.yml: | --- # Run auto discovery to find pods with label &quot;<em>app</em>=redis&quot; # https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>integrations</em>&#x2F;host-<em>integrations</em>&#x2F;installation&#x2F;container-auto-discovery discovery"
      },
      "id": "617daead64441fd15bfbdfcb"
    },
    {
      "sections": [
        "Link your applications to Kubernetes",
        "Tip",
        "Compatibility and requirements",
        "Kubernetes requirements",
        "Network requirements",
        "APM agent compatibility",
        "Openshift requirements",
        "Important",
        "Configure the injection of metadata",
        "Default configuration",
        "Custom configuration",
        "Manage custom certificates",
        "Validate the injection of metadata",
        "Disable the injection of metadata",
        "Troubleshooting"
      ],
      "title": "Link your applications to Kubernetes",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Link apps and services"
      ],
      "external_id": "0fe0951312aaf683f6614d5956f8c402b9693780",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/link-your-applications/link-your-applications-kubernetes/",
      "published_at": "2022-01-08T09:12:18Z",
      "updated_at": "2021-10-30T20:44:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can surface Kubernetes metadata and link it to your APM agents as distributed traces to explore performance issues and troubleshoot transaction errors. For more information, see this New Relic blog post. You can quickly start monitoring Kubernetes clusters using Auto-telemetry with Pixie, which is currently a beta release. This Pixie integration into New Relic does not require a language agent. Learn more about Auto-telemetry with Pixie here. Tip Our Kubernetes metadata injection project is open source. Here's the code to link APM and infrastructure data and the code to automatically manage certificates. Compatibility and requirements Before linking Kubernetes metadata to your APM agents, make sure you meet the following requirements: Kubernetes requirements Network requirements APM agent compatibility OpenShift requirements Kubernetes requirements To link your applications and Kubernetes, your cluster must have the MutatingAdmissionWebhook controller enabled, which requires Kubernetes 1.9 or higher. To verify that your cluster is compatible, run the following command: kubectl api-versions | grep admissionregistration.k8s.io/v1beta1 admissionregistration.k8s.io/v1beta1 Copy If you see a different result, follow the Kubernetes documentation to enable admission control in your cluster. Network requirements For Kubernetes to speak to our MutatingAdmissionWebhook, the master node (or the API server container, depending on how the cluster is set up) should be allowed egress for HTTPS traffic on port 443 to pods in all of the other nodes in the cluster. This might require specific configuration depending on how the infrastructure is set up (on-premises, AWS, Google Cloud, etc). Tip Until Kubernetes v1.14, users were only allowed to register admission webhooks on port 443. Since v1.15 it's possible to register them on different ports. To ensure backward compatibility, the webhook is registered by default on port 443 in the YAML config file we distribute. APM agent compatibility The following New Relic agents collect Kubernetes metadata: Go 2.3.0 or higher Java 4.10.0 or higher Node.js 5.3.0 or higher Python 4.14.0 or higher Ruby 6.1.0 or higher .NET 8.17.438 or higher Openshift requirements To link Openshift and Kubernetes you must enable mutating admission webhooks, which requires Openshift 3.9 or higher. During the process, install a resource that requires admin permissions to the cluster. Run this to log in as admin: oc login -u system:admin Copy Check that webhooks are correctly configured. If they are not, update the master-config.yaml file. admissionConfig: pluginConfig: MutatingAdmissionWebhook: configuration: apiVersion: apiserver.config.k8s.io/v1alpha1 kubeConfigFile: /dev/null kind: WebhookAdmission ValidatingAdmissionWebhook: configuration: apiVersion: apiserver.config.k8s.io/v1alpha1 kubeConfigFile: /dev/null kind: WebhookAdmission location: \"\" Copy Important Add kubeConfigFile: /dev/null to address some issues in Openshift. Enable certificate signing by editing the YAML file and updating your configuration: kubernetesMasterConfig: controllerArguments: cluster-signing-cert-file: - \"/etc/origin/master/ca.crt\" cluster-signing-key-file: - \"/etc/origin/master/ca.key\" Copy Restart the Openshift services in the master node. Configure the injection of metadata By default, all the pods you create that include APM agents have the correct environment variables set and the metadata injection applies to the entire cluster. To check that the environment variables have been set, any container that is running must be stopped, and a new instance started (see Validate the injection of metadata). This default configuration also uses the Kubernetes certificates API to automatically manage the certificates required for the injection. If needed, you can limit the injection of metadata to specific namespaces in your cluster or self-manage your certificates. Default configuration We offer instructions for deploying our integration using Helm. Just be sure that, when you are configuring the chart, the webhook that inject the metadata is enabled. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled=true \\ --set newrelic-infrastructure.privileged=true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled= true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Custom configuration You can limit the injection of metadata only to specific namespaces by using labels. To enable this feature, edit nri-bundle Helm values.yaml file: nri-metadata-injector: injectOnlyLabeledNamespaces: true Copy Or add a --set when installing or upgrading your Helm release: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled=true \\ --set newrelic-infrastructure.privileged=true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled= true \\ --set nri-metadata-injector.injectOnlyLabeledNamespaces=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy With this option, injection is only applied to those namespaces that have the newrelic-metadata-injection label set to enabled: kubectl label namespace YOUR_NAMESPACE newrelic-metadata-injection=enabled Copy Manage custom certificates To use custom certificates you need to disable the automatic installation of certificates when you are installing using Helm. To disable the installation for certificates just modify nri-bundle Helm values.yaml like this: nri-metadata-injector: customTLSCertificate: true Copy Or add a --set when installing or upgrading your Helm release: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled=true \\ --set newrelic-infrastructure.privileged=true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled= true \\ --set nri-metadata-injector.customTLSCertificate=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Now you can proceed with the custom certificate management option. You need your certificate, server key, and Certification Authority (CA) bundle encoded in PEM format. If you have them in the standard certificate format (X.509), install openssl, and run the following: openssl x509 -in CERTIFICATE_FILENAME -outform PEM -out CERTIFICATE_FILENAME.pem openssl x509 -in SERVER_KEY_FILENAME -outform PEM -out SERVER_KEY_FILENAME.pem openssl x509 -in CA_BUNDLE_FILENAME -outform PEM -out BUNDLE_FILENAME.pem Copy If your certificate/key pair are in another format, see the Digicert knowledgebase for more help. Create the TLS secret with the signed certificate/key pair, and patch the mutating webhook configuration with the CA using the following commands: kubectl create secret tls newrelic-metadata-injection-admission \\ --key=PEM_ENCODED_SERVER_KEY \\ --cert=PEM_ENCODED_CERTIFICATE \\ --dry-run -o yaml | kubectl -n newrelic apply -f - caBundle=$(cat PEM_ENCODED_CA_BUNDLE | base64 | td -d $'\\n') kubectl patch mutatingwebhookconfiguration newrelic-metadata-injection-cfg --type='json' -p \"[{'op': 'replace', 'path': '/webhooks/0/clientConfig/caBundle', 'value':'${caBundle}'}]\" Copy Important Certificates signed by Kubernetes have an expiration of one year. For more information, see the Kubernetes source code in GitHub. Validate the injection of metadata In order to validate that the webhook (responsible for injecting the metadata) was installed correctly, deploy a new pod and check for the New Relic environment variables. Create a dummy pod containing Busybox by running: kubectl create -f https://git.io/vPieo Copy Check if New Relic environment variables were injected: kubectl exec busybox0 -- env | grep NEW_RELIC_METADATA_KUBERNETES NEW_RELIC_METADATA_KUBERNETES_CLUSTER_NAME=fsi NEW_RELIC_METADATA_KUBERNETES_NODE_NAME=nodea NEW_RELIC_METADATA_KUBERNETES_NAMESPACE_NAME=default NEW_RELIC_METADATA_KUBERNETES_POD_NAME=busybox0 NEW_RELIC_METADATA_KUBERNETES_CONTAINER_NAME=busybox Copy Disable the injection of metadata To disable/uninstall the injection of metadata, use the following commands: Delete the Kubernetes objects using the yaml file: kubectl delete -f k8s-metadata-injection-latest.yaml Copy Delete the TLS secret containing the certificate/key pair: kubectl delete secret/newrelic-metadata-injection-secret Copy Troubleshooting Follow these troubleshooting tips as needed. No Kubernetes metadata in APM or distributed tracing transactions Problem The creation of the secret by the k8s-webhook-cert-manager job used to fail due to the kubectl version used by the image when running in Kubernetes version 1.19.x, The new version 1.3.2 fixes this issue, therefore it is enough to run again the job using an update version of the image to fix the issue. Solution Update the image k8s-webhook-cert-manager (to a version >= 1.3.2) and re-run the job. The secret will be correctly created and the k8s-metadata-injection pod will be able to start. Note that the new version of the manifest and of the nri-bundle are already updated with the correct version of the image. Problem In OpenShift version 4.x, the CA that is used in order to patch the mutatingwebhookconfiguration resource is not the one used when signing the certificates. This is a known issue currently tracked here. In the logs of the Pod nri-metadata-injection, you'll see the following error message: TLS handshake error from 10.131.0.29:37428: remote error: tls: unknown certificate authority TLS handshake error from 10.129.0.1:49314: remote error: tls: bad certificate Copy Workaround Manually update the certificate stored in the mutatingwebhookconfiguration object. The correct CA locations might change according to the cluster configuration. However, you can usually find the CA in the secret csr-signer in the namespace openshift-kube-controller-manager. Problem There is no Kubernetes metadata included in the transactions' attributes of your APM agent or in distributed tracing. Solution Verify that the environment variables are being correctly injected by following the instructions described in the Validate your installation step. If they are not present, get the name of the metadata injection pod by running: kubectl get pods | grep newrelic-metadata-injection-deployment kubectl logs -f pod/podname Copy In another terminal, create a new pod (for example, see Validate your installation), and inspect the logs of the metadata injection deployment for errors. For every created pod there should be a set of 4 new entries in the logs like: {\"level\":\"info\",\"ts\":\"2020-04-09T12:55:32.107Z\",\"caller\":\"server/main.go:139\",\"msg\":\"POST https://newrelic-metadata-injection-svc.default.svc:443/mutate?timeout=30s HTTP/2.0\\\" from 10.11.49.2:32836\"} {\"level\":\"info\",\"ts\":\"2020-04-09T12:55:32.110Z\",\"caller\":\"server/webhook.go:168\",\"msg\":\"received admission review\",\"kind\":\"/v1, Kind=Pod\",\"namespace\":\"default\",\"name\":\"\",\"pod\":\"busybox1\",\"UID\":\"6577519b-7a61-11ea-965e-0e46d1c9335c\",\"operation\":\"CREATE\",\"userinfo\":{\"username\":\"admin\",\"uid\":\"admin\",\"groups\":[\"system:masters\",\"system:authenticated\"]}} {\"level\":\"info\",\"ts\":\"2020-04-09T12:55:32.111Z\",\"caller\":\"server/webhook.go:182\",\"msg\":\"admission response created\",\"response\":\"[{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env\\\",\\\"value\\\":[{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_CLUSTER_NAME\\\",\\\"value\\\":\\\"adn_kops\\\"}]},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_NODE_NAME\\\",\\\"valueFrom\\\":{\\\"fieldRef\\\":{\\\"fieldPath\\\":\\\"spec.nodeName\\\"}}}},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_NAMESPACE_NAME\\\",\\\"valueFrom\\\":{\\\"fieldRef\\\":{\\\"fieldPath\\\":\\\"metadata.namespace\\\"}}}},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_POD_NAME\\\",\\\"valueFrom\\\":{\\\"fieldRef\\\":{\\\"fieldPath\\\":\\\"metadata.name\\\"}}}},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_CONTAINER_NAME\\\",\\\"value\\\":\\\"busybox\\\"}},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_CONTAINER_IMAGE_NAME\\\",\\\"value\\\":\\\"busybox\\\"}}]\"} {\"level\":\"info\",\"ts\":\"2020-04-09T12:55:32.111Z\",\"caller\":\"server/webhook.go:257\",\"msg\":\"writing response\"} Copy If there are no new entries on the logs, it means that the apiserver is not being able to communicate with the webhook service, this could be due to networking rules or security groups rejecting the communication. To check if the apiserver is not being able to communicate with the webhook you should inspect the apiserver logs for errors like: failed calling webhook \"metadata-injection.newrelic.com\": ERROR_REASON Copy To get the apiserver logs: Start a proxy to the Kubernetes API server by the executing the following command in a terminal window and keep it running. kubectl proxy --port=8001 Copy Create a new pod in your cluster, this will make the apiserver try to communicate with the webhook. The following command will create a busybox. kubectl create -f https://git.io/vPieo Copy Retrieve the apiserver logs. curl localhost:8001/logs/kube-apiserver.log > apiserver.log Copy Delete the busybox container. kubectl delete -f https://git.io/vPieo Copy Inspect the logs for errors. grep -E 'failed calling webhook' apiserver.log Copy Remember that one of the requirements for the metadata injection is that the apiserver must be allowed egress to the pods running on the cluster. If you encounter errors regarding connection timeouts or failed connections, make sure to check the security groups and firewall rules of the cluster. If there are no log entries in either the apiserver logs or the metadata injection deployment, it means that the webhook was not properly registered. Ensure the metadata injection setup job ran successfully by inspecting the output of: kubectl get job newrelic-metadata-setup Copy If the job is not completed, investigate the logs of the setup job: kubectl logs job/newrelic-metadata-setup Copy Ensure the CertificateSigningRequest is approved and issued by running: kubectl get csr newrelic-metadata-injection-svc.default Copy Ensure the TLS secret is present by running: kubectl get secret newrelic-metadata-injection-secret Copy Ensure the CA bundle is present in the mutating webhook configuration: kubectl get mutatingwebhookconfiguration newrelic-metadata-injection-cfg -o json Copy Ensure the TargetPort of the Service resource matches the Port of the Deployment's container: kubectl describe service/newrelic-metadata-injection-svc kubectl describe deployment/newrelic-metadata-injection-deployment Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 211.58652,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Link</em> your <em>applications</em> to <em>Kubernetes</em>",
        "sections": "<em>Link</em> your <em>applications</em> to <em>Kubernetes</em>",
        "tags": "<em>Link</em> <em>apps</em> <em>and</em> <em>services</em>",
        "body": " is currently a beta release. This Pixie <em>integration</em> into New Relic does not require a language agent. Learn more about Auto-telemetry with Pixie here. Tip Our <em>Kubernetes</em> metadata injection project is open source. Here&#x27;s the code to <em>link</em> APM and infrastructure data and the code to automatically"
      },
      "id": "617daead28ccbc662b7ffe23"
    }
  ],
  "/docs/kubernetes-pixie/kubernetes-integration/link-apps-services/tutorial-monitor-redis-running-kubernetes": [
    {
      "sections": [
        "New Relic Metrics Adapter",
        "BETA FEATURE",
        "Requirements",
        "Installation",
        "Tip",
        "Configuration",
        "How it works",
        "Caution",
        "Troubleshooting",
        "Get verbose logs",
        "Get raw metrics",
        "Metrics not working"
      ],
      "title": "New Relic Metrics Adapter",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Link apps and services"
      ],
      "external_id": "e2a825763b10ccf4bd1bd8423e2209f66dfb61bb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/newrelic-hpa-metrics-adapter/newrelic-metrics-adapter/",
      "published_at": "2022-01-08T09:01:12Z",
      "updated_at": "2021-11-13T08:47:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is still in development, but we encourage you to try it out! You can use metrics from your New Relic account to autoscale applications and services in your Kubernetes cluster by deploying the New Relic Metrics Adapter. This adapter fetches the metric values from New Relic and makes them available for the Horizontal Pod Autoscalers. The newrelic-k8s-metrics-adapter implements the external.metrics.k8s.io API to support the use of external metrics based New Relic NRQL queries results. Once deployed, the value for each configured metric is fetched using the NerdGraph API based on the configured NRQL query. The metrics adapter exposes the metrics over a secured endpoint with TLS. New Relic metrics adapter in a cluster. Requirements Kubernetes 1.16 or higher. The New Relic Kubernetes integration. New Relic's user API key. No other External Metrics Adapter installed in the cluster. Installation To install the New Relic Metrics Adapter, we provide the newrelic-k8s-metrics-adapter Helm chart, which is also included in the nri-bundle chart used to deploy all New Relic Kubernetes components. If not already installed, install our Kubernetes integration. Upgrade the installation to include the New Relic Metrics Adapter with the following command: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace --reuse-values \\ --set metrics-adapter.enabled=true \\ --set newrelic-k8s-metrics-adapter.personalAPIKey=YOUR_NEW_RELIC_PERSONAL_API_KEY \\ --set newrelic-k8s-metrics-adapter.config.accountID=YOUR_NEW_RELIC_ACCOUNT_ID \\ --set newrelic-k8s-metrics-adapter.config.externalMetrics.external_metric_name.query=NRQL query Copy Please notice and adjust the following flags: metrics-adapter.enabled: Must be set to true so the metrics adapter chart is installed. newrelic-k8s-metrics-adapter.personalAPIKey: Must be set to valid New Relic Personal API key. newrelic-k8s-metrics-adapter.accountID: Must be set to valid New Relic account where metrics are going to be fetched from. newrelic-k8s-metrics-adapter.config.externalMetrics.<var>external_metric_name</var>.<var>query</var>: Adds a new external metric where: <var>external_metric_name</var>: The metric name. <var>query</var>: The base NRQL query that is used to get the value for the metric. Tip Alternatively, you can use a values.yaml file that can be passed to the helm command with the --values flag. Values files can contain all parameters needed to configure the metrics explained in the configuration section. Configuration You can configure multiple metrics in the metrics adapter and change some parameters to modify the behaviour of the metrics cache and filtering. To see the full list and descriptions of all parameters that can be modified, refer to the chart README.md and values.yaml files. How it works The following example is a Helm values file that enable the metrics adapter on the nri-bundle chart installation, and configures the nginx_average_requests metric: metrics-adapter: enabled: true newrelic-k8s-metrics-adapter: personalAPIKey: <Personal API Key> config: accountID: <Account ID> externalMetrics: nginx_average_requests: query: \"FROM Metric SELECT average(nginx.server.net.requestsPerSecond) SINCE 2 MINUTES AGO\" Copy Caution The default time span for metrics is 1h. Therefore, you should define queries with the SINCE clause to adjust the time span according to your environment and needs. There is an HPA consuming the external metric as follows: kind: HorizontalPodAutoscaler apiVersion: autoscaling/v2beta2 metadata: name: nginx-scaler spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: nginx minReplicas: 1 maxReplicas: 10 metrics: - type: External external: metric: name: nginx_average_requests selector: matchLabels: k8s.namespaceName: nginx target: type: Value value: 10000 Copy Based on the HPA definition, the controller manager fetches the metrics from the external metrics API which are served by the New Relic metrics adapter. The New Relic metrics adapter receives the query including the nginx_average_requests metric name and all the selectors, and searches for a matching metric name in the internal memory based on the configured metrics. Then, it adds the selectors to the query to form a final query that is executed using NerdGraph to fetch the value from New Relic. The above example will generate a query like the following: FROM Metric SELECT average(nginx.server.net.requestsPerSecond) WHERE clusterName=<clusterName> AND `k8s.namespaceName`='nginx' SINCE 2 MINUTES AGO Copy Notice that a clusterName filter has been automatically added to the query to exclude metrics from other clusters in the same account. You can remove it by using the removeClusterFilter configuration parameter. Also the value is cached for a period of time defined by the cacheTTLSeconds configuration parameter, whose default is 30 seconds. Troubleshooting Get verbose logs Most common errors are displayed in the standard (non-verbose) logs. If you're doing a more in-depth investigation on your own or with New Relic Support, you can enable verbose mode. To get verbose logging details for an integration using Helm: Enable verbose logging: bash Copy $ helm upgrade -n <namespace> --reuse-values newrelic-bundle --set newrelic-k8s-metrics-adapter.verboseLog=true newrelic/nri-bundle Leave on verbose mode for a few minutes, or until enough activity has occurred. When you have the information you need, disable verbose logging: bash Copy $ helm upgrade --reuse-values newrelic-bundle --set newrelic-k8s-metrics-adapter.verboseLog=false newrelic/nri-bundle Caution Verbose mode increases significantly the amount of information sent to log files. Enable this mode temporarily, only for troubleshooting purposes, and reset the log level when finished. Get raw metrics Sometimes it's useful to get the list of available metrics and also to get the current value of an specific metric. To get the list of metrics available, run: bash Copy $ kubectl get --raw \"/apis/external.metrics.k8s.io/v1beta1/\" To get the value for a specific metric with a selector, run: bash Copy $ kubectl get --raw \"/apis/external.metrics.k8s.io/v1beta1/namespaces/*/<metric_name>?labelSelector=<selector_key>=<selector_value>\" Tip You must replace <metric_name>, <selector_key> and <selector_value> with your values. Metrics not working There are some usual errors that could cause a metric fail to retrieve the value. These errors are showed in the status of the metrics when you describe the HPA or are printed when you get the raw metrics directly. executing query: NRQL Syntax Error: Error at line...: The query that is being run has syntax errors. The same error message gives you the executed query and position of the error. You can try this query inside the New Relic query builder and correct the configuration from the adapter. extracting return value: expected first value to be of type \"float64\", got %!q(<nil>): The query doesn't return any value. The same error message gives you the executed query so you can try this query inside the New Relic query builder and correct the configuration from the adapter or the match selectors in the HPA.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 216.98819,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Link</em> <em>apps</em> <em>and</em> <em>services</em>",
        "body": "BETA FEATURE This feature is still in development, but we encourage you to try it out! You can use metrics from your New Relic account to autoscale applications and <em>services</em> in your <em>Kubernetes</em> cluster by deploying the New Relic Metrics Adapter. This adapter fetches the metric values from New Relic"
      },
      "id": "6175209d28ccbcf310c6bb2f"
    },
    {
      "sections": [
        "Monitor services running on Kubernetes",
        "Get started",
        "What you need",
        "Enable monitoring of services",
        "Get the config YAML for the integration",
        "Example configuration",
        "Configuration options for each integration",
        "Monitor services in our Kubernetes integration installed with Helm",
        "Learn more",
        "Manually configure service monitoring",
        "How the service-specific YAML config works",
        "Add a service YAML to the Kubernetes integration config",
        "Add multiple services to the same config"
      ],
      "title": "Monitor services running on Kubernetes",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Link apps and services"
      ],
      "external_id": "d2f12474df5b90e0226b8aa203450417e783b996",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/link-apps-services/monitor-services-running-kubernetes/",
      "published_at": "2022-01-08T13:08:12Z",
      "updated_at": "2021-10-24T00:51:45Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic's Kubernetes integration you can monitor both Kubernetes and the services running on it, such as Cassandra, Redis, MySQL, and other supported services. Get started Our Kubernetes integration comes bundled with some of our on-host integrations (like Cassandra, MySQL, and Apache). This lets you get data for those supported services by adding a section to the Kubernetes integration's configuration, which lives as a ConfigMap inside a manifest. What you need Enable this feature for a service Details about how configuration works For an example of how to monitor Redis running on a Kubernetes PHP Guestbook, see this tutorial. What you need To monitor services running on Kubernetes, you only need a Kubernetes cluster running the Kubernetes integration, version 1.16.0 or higher (install | check version | update). We support the following services running on Kubernetes: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP Enable monitoring of services To enable our Kubernetes integration to monitor one or more services: Expand this dropdown and get the YAML snippets for the service(s) you want to monitor: Get the config YAML for the integration For the services you want to monitor, follow the links to GitHub to get the YAML snippets you'll need for the next step: Apache Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ Redis SNMP Add the snippet to the Kubernetes integration's ConfigMap, after the data: section: Example configuration This example shows the YAML config for the Apache integration ( highlighted ) added to the Kubernetes integration's config. Respect the indentation levels. --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: apache-config.yaml: | --- # Run auto discovery to find pods with label \"app=apache\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Use the optional arguments: # --namespaces: Comma separated namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: apache integrations: - name: nri-apache env: # Use the discovered IP as the host address STATUS_URL: http://${discovery.ip}/server-status?auto METRICS: 1 Copy You can add snippets for multiple services to the same config file. See an example. Depending on your environment, you may need or want to set additional config options. Expand the dropdown below for links to configuration options. Configuration options for each integration Select a service to see available config options: Apache Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ Redis SNMP Verify monitoring is enabled: Go to one.newrelic.com > Infrastructure, select Third party services, and then select the service's dashboard. You should see data being reported. Additional notes about enabling services: Enabling multiple services may use more resources than what is set in the resource limits of the Kubernetes integration config file. If this becomes an issue, raise the limit in the resources section. The Kubernetes integration does not automatically update. For best results, regularly update. Monitor services in our Kubernetes integration installed with Helm If you installed our Kubernetes integration using Helm, to monitor services you need to update the existing installation with the new configuration, which contains the services to monitor: helm upgrade --reuse-values -f values.yaml [RELEASE] [CHART] Copy If you use nri-bundle charts, you need to update the children's chart values. Find some examples here. Learn more More resources for learning about configuration: Learn technical details about how configuration works. Learn how to configure monitoring of multiple services with the same config file. See a step-by-step tutorial showing how to monitor a Redis service on Kubernetes. Manually configure service monitoring The enable procedure should be all you need to get monitoring working, but if you run into problems, understanding some technical details about configuration can be helpful. This section goes into more detail about how configuration works. For each service you wish to monitor, you must add a configuration file for that integration to our Kubernetes integration's configuration. This document will cover these subjects: How the service-specific configuration YAML snippet works Adding the service-specific YAML in the Kubernetes integration's config file Adding multiple services to the Kubernetes integration's config file How the service-specific YAML config works Our Kubernetes integration's configuration follows the ConfigMap format. Using a ConfigMap allows us to decouple the configuration for the integrations from the Kubernetes image. The other benefit is that a ConfigMap can be updated automatically without reloading the running container. Because the infrastructure agent uses YAML to configure its associated integrations, ConfigMaps are a good choice for storing YAML. (For more information on config file format, see the Integration config file format.) The Kubernetes integration image comes with an auto-discovery feature that simplifies the configuration of multiple instances of services using a single configuration file. For example, if you have several NGINX instances running, creating an NGINX integration configuration file for every instance would be hard to implement and hard to update. With our auto-discovery option, you can discover and monitor all your NGINX instances with a single configuration file. Each integration has its own specific configuration YAML. Our NGINX integration default config file looks like this: nginx-config.yml: | --- discovery: command: # Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --port: Port used to connect to the kubelet. Default is 10255 # --tls: Use secure (TLS) connection # Custom Example: # exec: /var/db/newrelic-infra/nri-discovery-kubernetes --namespaces namespace1,namespace2 --port 10250 --tls # Default exec: /var/db/newrelic-infra/nri-discovery-kubernetes match: label.app: nginx integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}/status STATUS_MODULE: discover METRICS: 1 Copy The above config enables the following: Runs nri-discovery-kubernetes to query the data for the node we are currently on. Parses the data that comes back and looks for any Kubernetes pod that has a Kubernetes container with an app= label with value nginx. For any matches, it attempts to run the NGINX integration. The status URL is built from: The pod's IP address The status page is pulled from the label on K8s pod called status_url This automatic discovery works the same as the container auto-discovery used by the infrastructure agent. For more advanced options, see Container auto-discovery. Add a service YAML to the Kubernetes integration config It's best practice to configure enabled integrations alongside the Kubernetes integration configuration. This is easier than maintaining configuration files for every single service/integration instance. Below is an example of a Kubernetes integration's ConfigMap. The highlighted section shows where an integration configuration YAML (in this case, NGINX) is placed. For more information on discovery:, see Container auto-discovery for on-host integrations. --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: nginx-config.yml: | --- # Run auto discovery to find pods with label \"app=nginx\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: nginx integrations: - name: nri-nginx env: # If you're using ngx_http_api_module be certain to use the full path up to and including the version number # Use the discovered IP as the host address STATUS_URL: http://${discovery.ip}/status # Comma separated list of ngx_http_api_module, NON PARAMETERIZED, Endpoints # endpoints: /nginx,/processes,/connections,/ssl,/slabs,/http,/http/requests,/http/server_zones,/http/caches,/http/upstreams,/http/keyvals,/stream,/stream/server_zones,/stream/upstreams,/stream/keyvals,/stream/zone_sync # Name of Nginx status module OHI is to query against. discover | ngx_http_stub_status_module | ngx_http_status_module | ngx_http_api_module STATUS_MODULE: discover METRICS: 1 Copy This configuration map can then be referenced in the DaemonSet, the same as the one that was generated via the command line. Make sure the namespace used is the same one used by the Kubernetes integration manifest. If you haven't changed it in the downloaded manifest file, the value is default. Add multiple services to the same config You can monitor several services using the same Kubernetes integration config file. To do this, add another integration configuration YAML to the same Kubernetes integration config file. Below is the Kubernetes config created in the last section, with a new section for the Cassandra integration's config (highlighted). --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: nginx-config.yml: | --- # Run auto discovery to find pods with label \"app=nginx\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Run discovery for Kubernetes. Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: nginx integrations: - name: nri-nginx env: # If you're using ngx_http_api_module be certain to use the full path up to and including the version number # Use the discovered IP as the host address STATUS_URL: http://${discovery.ip}/status # Comma separated list of ngx_http_api_module, NON PARAMETERIZED, Endpoints # endpoints: /nginx,/processes,/connections,/ssl,/slabs,/http,/http/requests,/http/server_zones,/http/caches,/http/upstreams,/http/keyvals,/stream,/stream/server_zones,/stream/upstreams,/stream/keyvals,/stream/zone_sync # Name of Nginx status module OHI is to query against. discover | ngx_http_stub_status_module | ngx_http_status_module | ngx_http_api_module STATUS_MODULE: discover METRICS: 1 cassandra-configuration.yml: | --- # Run auto discovery to find pods with label \"app=cassandra\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Run discovery for Kubernetes. Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: cassandra integrations: - name: nri-cassandra env: # Use the discovered IP as the host address HOSTNAME: ${discovery.ip} PORT: 7199 USERNAME: cassandra PASSWORD: cassandra METRICS: 1/mark Copy The Kubernetes integration config is now set up to monitor these two services. Additionally, depending on your environment, there may be some additional service-specific configuration you must do. When you've completed configuration, our infrastructure agent looks for any pod with a label cassandra and runs the integration against it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 213.66853,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor <em>services</em> running on <em>Kubernetes</em>",
        "sections": "Monitor <em>services</em> in our <em>Kubernetes</em> <em>integration</em> installed with Helm",
        "tags": "<em>Link</em> <em>apps</em> <em>and</em> <em>services</em>",
        "body": "With New Relic&#x27;s <em>Kubernetes</em> <em>integration</em> you can monitor both <em>Kubernetes</em> and the <em>services</em> running on it, such as Cassandra, Redis, MySQL, and other supported <em>services</em>. Get started Our <em>Kubernetes</em> <em>integration</em> comes bundled with some of our on-host <em>integrations</em> (like Cassandra, MySQL, and Apache"
      },
      "id": "617d587828ccbc645c8000ae"
    },
    {
      "sections": [
        "Link your applications to Kubernetes",
        "Tip",
        "Compatibility and requirements",
        "Kubernetes requirements",
        "Network requirements",
        "APM agent compatibility",
        "Openshift requirements",
        "Important",
        "Configure the injection of metadata",
        "Default configuration",
        "Custom configuration",
        "Manage custom certificates",
        "Validate the injection of metadata",
        "Disable the injection of metadata",
        "Troubleshooting"
      ],
      "title": "Link your applications to Kubernetes",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Link apps and services"
      ],
      "external_id": "0fe0951312aaf683f6614d5956f8c402b9693780",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/link-your-applications/link-your-applications-kubernetes/",
      "published_at": "2022-01-08T09:12:18Z",
      "updated_at": "2021-10-30T20:44:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can surface Kubernetes metadata and link it to your APM agents as distributed traces to explore performance issues and troubleshoot transaction errors. For more information, see this New Relic blog post. You can quickly start monitoring Kubernetes clusters using Auto-telemetry with Pixie, which is currently a beta release. This Pixie integration into New Relic does not require a language agent. Learn more about Auto-telemetry with Pixie here. Tip Our Kubernetes metadata injection project is open source. Here's the code to link APM and infrastructure data and the code to automatically manage certificates. Compatibility and requirements Before linking Kubernetes metadata to your APM agents, make sure you meet the following requirements: Kubernetes requirements Network requirements APM agent compatibility OpenShift requirements Kubernetes requirements To link your applications and Kubernetes, your cluster must have the MutatingAdmissionWebhook controller enabled, which requires Kubernetes 1.9 or higher. To verify that your cluster is compatible, run the following command: kubectl api-versions | grep admissionregistration.k8s.io/v1beta1 admissionregistration.k8s.io/v1beta1 Copy If you see a different result, follow the Kubernetes documentation to enable admission control in your cluster. Network requirements For Kubernetes to speak to our MutatingAdmissionWebhook, the master node (or the API server container, depending on how the cluster is set up) should be allowed egress for HTTPS traffic on port 443 to pods in all of the other nodes in the cluster. This might require specific configuration depending on how the infrastructure is set up (on-premises, AWS, Google Cloud, etc). Tip Until Kubernetes v1.14, users were only allowed to register admission webhooks on port 443. Since v1.15 it's possible to register them on different ports. To ensure backward compatibility, the webhook is registered by default on port 443 in the YAML config file we distribute. APM agent compatibility The following New Relic agents collect Kubernetes metadata: Go 2.3.0 or higher Java 4.10.0 or higher Node.js 5.3.0 or higher Python 4.14.0 or higher Ruby 6.1.0 or higher .NET 8.17.438 or higher Openshift requirements To link Openshift and Kubernetes you must enable mutating admission webhooks, which requires Openshift 3.9 or higher. During the process, install a resource that requires admin permissions to the cluster. Run this to log in as admin: oc login -u system:admin Copy Check that webhooks are correctly configured. If they are not, update the master-config.yaml file. admissionConfig: pluginConfig: MutatingAdmissionWebhook: configuration: apiVersion: apiserver.config.k8s.io/v1alpha1 kubeConfigFile: /dev/null kind: WebhookAdmission ValidatingAdmissionWebhook: configuration: apiVersion: apiserver.config.k8s.io/v1alpha1 kubeConfigFile: /dev/null kind: WebhookAdmission location: \"\" Copy Important Add kubeConfigFile: /dev/null to address some issues in Openshift. Enable certificate signing by editing the YAML file and updating your configuration: kubernetesMasterConfig: controllerArguments: cluster-signing-cert-file: - \"/etc/origin/master/ca.crt\" cluster-signing-key-file: - \"/etc/origin/master/ca.key\" Copy Restart the Openshift services in the master node. Configure the injection of metadata By default, all the pods you create that include APM agents have the correct environment variables set and the metadata injection applies to the entire cluster. To check that the environment variables have been set, any container that is running must be stopped, and a new instance started (see Validate the injection of metadata). This default configuration also uses the Kubernetes certificates API to automatically manage the certificates required for the injection. If needed, you can limit the injection of metadata to specific namespaces in your cluster or self-manage your certificates. Default configuration We offer instructions for deploying our integration using Helm. Just be sure that, when you are configuring the chart, the webhook that inject the metadata is enabled. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled=true \\ --set newrelic-infrastructure.privileged=true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled= true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Custom configuration You can limit the injection of metadata only to specific namespaces by using labels. To enable this feature, edit nri-bundle Helm values.yaml file: nri-metadata-injector: injectOnlyLabeledNamespaces: true Copy Or add a --set when installing or upgrading your Helm release: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled=true \\ --set newrelic-infrastructure.privileged=true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled= true \\ --set nri-metadata-injector.injectOnlyLabeledNamespaces=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy With this option, injection is only applied to those namespaces that have the newrelic-metadata-injection label set to enabled: kubectl label namespace YOUR_NAMESPACE newrelic-metadata-injection=enabled Copy Manage custom certificates To use custom certificates you need to disable the automatic installation of certificates when you are installing using Helm. To disable the installation for certificates just modify nri-bundle Helm values.yaml like this: nri-metadata-injector: customTLSCertificate: true Copy Or add a --set when installing or upgrading your Helm release: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled=true \\ --set newrelic-infrastructure.privileged=true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled= true \\ --set nri-metadata-injector.customTLSCertificate=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Now you can proceed with the custom certificate management option. You need your certificate, server key, and Certification Authority (CA) bundle encoded in PEM format. If you have them in the standard certificate format (X.509), install openssl, and run the following: openssl x509 -in CERTIFICATE_FILENAME -outform PEM -out CERTIFICATE_FILENAME.pem openssl x509 -in SERVER_KEY_FILENAME -outform PEM -out SERVER_KEY_FILENAME.pem openssl x509 -in CA_BUNDLE_FILENAME -outform PEM -out BUNDLE_FILENAME.pem Copy If your certificate/key pair are in another format, see the Digicert knowledgebase for more help. Create the TLS secret with the signed certificate/key pair, and patch the mutating webhook configuration with the CA using the following commands: kubectl create secret tls newrelic-metadata-injection-admission \\ --key=PEM_ENCODED_SERVER_KEY \\ --cert=PEM_ENCODED_CERTIFICATE \\ --dry-run -o yaml | kubectl -n newrelic apply -f - caBundle=$(cat PEM_ENCODED_CA_BUNDLE | base64 | td -d $'\\n') kubectl patch mutatingwebhookconfiguration newrelic-metadata-injection-cfg --type='json' -p \"[{'op': 'replace', 'path': '/webhooks/0/clientConfig/caBundle', 'value':'${caBundle}'}]\" Copy Important Certificates signed by Kubernetes have an expiration of one year. For more information, see the Kubernetes source code in GitHub. Validate the injection of metadata In order to validate that the webhook (responsible for injecting the metadata) was installed correctly, deploy a new pod and check for the New Relic environment variables. Create a dummy pod containing Busybox by running: kubectl create -f https://git.io/vPieo Copy Check if New Relic environment variables were injected: kubectl exec busybox0 -- env | grep NEW_RELIC_METADATA_KUBERNETES NEW_RELIC_METADATA_KUBERNETES_CLUSTER_NAME=fsi NEW_RELIC_METADATA_KUBERNETES_NODE_NAME=nodea NEW_RELIC_METADATA_KUBERNETES_NAMESPACE_NAME=default NEW_RELIC_METADATA_KUBERNETES_POD_NAME=busybox0 NEW_RELIC_METADATA_KUBERNETES_CONTAINER_NAME=busybox Copy Disable the injection of metadata To disable/uninstall the injection of metadata, use the following commands: Delete the Kubernetes objects using the yaml file: kubectl delete -f k8s-metadata-injection-latest.yaml Copy Delete the TLS secret containing the certificate/key pair: kubectl delete secret/newrelic-metadata-injection-secret Copy Troubleshooting Follow these troubleshooting tips as needed. No Kubernetes metadata in APM or distributed tracing transactions Problem The creation of the secret by the k8s-webhook-cert-manager job used to fail due to the kubectl version used by the image when running in Kubernetes version 1.19.x, The new version 1.3.2 fixes this issue, therefore it is enough to run again the job using an update version of the image to fix the issue. Solution Update the image k8s-webhook-cert-manager (to a version >= 1.3.2) and re-run the job. The secret will be correctly created and the k8s-metadata-injection pod will be able to start. Note that the new version of the manifest and of the nri-bundle are already updated with the correct version of the image. Problem In OpenShift version 4.x, the CA that is used in order to patch the mutatingwebhookconfiguration resource is not the one used when signing the certificates. This is a known issue currently tracked here. In the logs of the Pod nri-metadata-injection, you'll see the following error message: TLS handshake error from 10.131.0.29:37428: remote error: tls: unknown certificate authority TLS handshake error from 10.129.0.1:49314: remote error: tls: bad certificate Copy Workaround Manually update the certificate stored in the mutatingwebhookconfiguration object. The correct CA locations might change according to the cluster configuration. However, you can usually find the CA in the secret csr-signer in the namespace openshift-kube-controller-manager. Problem There is no Kubernetes metadata included in the transactions' attributes of your APM agent or in distributed tracing. Solution Verify that the environment variables are being correctly injected by following the instructions described in the Validate your installation step. If they are not present, get the name of the metadata injection pod by running: kubectl get pods | grep newrelic-metadata-injection-deployment kubectl logs -f pod/podname Copy In another terminal, create a new pod (for example, see Validate your installation), and inspect the logs of the metadata injection deployment for errors. For every created pod there should be a set of 4 new entries in the logs like: {\"level\":\"info\",\"ts\":\"2020-04-09T12:55:32.107Z\",\"caller\":\"server/main.go:139\",\"msg\":\"POST https://newrelic-metadata-injection-svc.default.svc:443/mutate?timeout=30s HTTP/2.0\\\" from 10.11.49.2:32836\"} {\"level\":\"info\",\"ts\":\"2020-04-09T12:55:32.110Z\",\"caller\":\"server/webhook.go:168\",\"msg\":\"received admission review\",\"kind\":\"/v1, Kind=Pod\",\"namespace\":\"default\",\"name\":\"\",\"pod\":\"busybox1\",\"UID\":\"6577519b-7a61-11ea-965e-0e46d1c9335c\",\"operation\":\"CREATE\",\"userinfo\":{\"username\":\"admin\",\"uid\":\"admin\",\"groups\":[\"system:masters\",\"system:authenticated\"]}} {\"level\":\"info\",\"ts\":\"2020-04-09T12:55:32.111Z\",\"caller\":\"server/webhook.go:182\",\"msg\":\"admission response created\",\"response\":\"[{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env\\\",\\\"value\\\":[{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_CLUSTER_NAME\\\",\\\"value\\\":\\\"adn_kops\\\"}]},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_NODE_NAME\\\",\\\"valueFrom\\\":{\\\"fieldRef\\\":{\\\"fieldPath\\\":\\\"spec.nodeName\\\"}}}},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_NAMESPACE_NAME\\\",\\\"valueFrom\\\":{\\\"fieldRef\\\":{\\\"fieldPath\\\":\\\"metadata.namespace\\\"}}}},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_POD_NAME\\\",\\\"valueFrom\\\":{\\\"fieldRef\\\":{\\\"fieldPath\\\":\\\"metadata.name\\\"}}}},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_CONTAINER_NAME\\\",\\\"value\\\":\\\"busybox\\\"}},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_CONTAINER_IMAGE_NAME\\\",\\\"value\\\":\\\"busybox\\\"}}]\"} {\"level\":\"info\",\"ts\":\"2020-04-09T12:55:32.111Z\",\"caller\":\"server/webhook.go:257\",\"msg\":\"writing response\"} Copy If there are no new entries on the logs, it means that the apiserver is not being able to communicate with the webhook service, this could be due to networking rules or security groups rejecting the communication. To check if the apiserver is not being able to communicate with the webhook you should inspect the apiserver logs for errors like: failed calling webhook \"metadata-injection.newrelic.com\": ERROR_REASON Copy To get the apiserver logs: Start a proxy to the Kubernetes API server by the executing the following command in a terminal window and keep it running. kubectl proxy --port=8001 Copy Create a new pod in your cluster, this will make the apiserver try to communicate with the webhook. The following command will create a busybox. kubectl create -f https://git.io/vPieo Copy Retrieve the apiserver logs. curl localhost:8001/logs/kube-apiserver.log > apiserver.log Copy Delete the busybox container. kubectl delete -f https://git.io/vPieo Copy Inspect the logs for errors. grep -E 'failed calling webhook' apiserver.log Copy Remember that one of the requirements for the metadata injection is that the apiserver must be allowed egress to the pods running on the cluster. If you encounter errors regarding connection timeouts or failed connections, make sure to check the security groups and firewall rules of the cluster. If there are no log entries in either the apiserver logs or the metadata injection deployment, it means that the webhook was not properly registered. Ensure the metadata injection setup job ran successfully by inspecting the output of: kubectl get job newrelic-metadata-setup Copy If the job is not completed, investigate the logs of the setup job: kubectl logs job/newrelic-metadata-setup Copy Ensure the CertificateSigningRequest is approved and issued by running: kubectl get csr newrelic-metadata-injection-svc.default Copy Ensure the TLS secret is present by running: kubectl get secret newrelic-metadata-injection-secret Copy Ensure the CA bundle is present in the mutating webhook configuration: kubectl get mutatingwebhookconfiguration newrelic-metadata-injection-cfg -o json Copy Ensure the TargetPort of the Service resource matches the Port of the Deployment's container: kubectl describe service/newrelic-metadata-injection-svc kubectl describe deployment/newrelic-metadata-injection-deployment Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 211.58652,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Link</em> your <em>applications</em> to <em>Kubernetes</em>",
        "sections": "<em>Link</em> your <em>applications</em> to <em>Kubernetes</em>",
        "tags": "<em>Link</em> <em>apps</em> <em>and</em> <em>services</em>",
        "body": " is currently a beta release. This Pixie <em>integration</em> into New Relic does not require a language agent. Learn more about Auto-telemetry with Pixie here. Tip Our <em>Kubernetes</em> metadata injection project is open source. Here&#x27;s the code to <em>link</em> APM and infrastructure data and the code to automatically"
      },
      "id": "617daead28ccbc662b7ffe23"
    }
  ],
  "/docs/kubernetes-pixie/kubernetes-integration/link-your-applications/link-your-applications-kubernetes": [
    {
      "sections": [
        "New Relic Metrics Adapter",
        "BETA FEATURE",
        "Requirements",
        "Installation",
        "Tip",
        "Configuration",
        "How it works",
        "Caution",
        "Troubleshooting",
        "Get verbose logs",
        "Get raw metrics",
        "Metrics not working"
      ],
      "title": "New Relic Metrics Adapter",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Link apps and services"
      ],
      "external_id": "e2a825763b10ccf4bd1bd8423e2209f66dfb61bb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/newrelic-hpa-metrics-adapter/newrelic-metrics-adapter/",
      "published_at": "2022-01-08T09:01:12Z",
      "updated_at": "2021-11-13T08:47:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "BETA FEATURE This feature is still in development, but we encourage you to try it out! You can use metrics from your New Relic account to autoscale applications and services in your Kubernetes cluster by deploying the New Relic Metrics Adapter. This adapter fetches the metric values from New Relic and makes them available for the Horizontal Pod Autoscalers. The newrelic-k8s-metrics-adapter implements the external.metrics.k8s.io API to support the use of external metrics based New Relic NRQL queries results. Once deployed, the value for each configured metric is fetched using the NerdGraph API based on the configured NRQL query. The metrics adapter exposes the metrics over a secured endpoint with TLS. New Relic metrics adapter in a cluster. Requirements Kubernetes 1.16 or higher. The New Relic Kubernetes integration. New Relic's user API key. No other External Metrics Adapter installed in the cluster. Installation To install the New Relic Metrics Adapter, we provide the newrelic-k8s-metrics-adapter Helm chart, which is also included in the nri-bundle chart used to deploy all New Relic Kubernetes components. If not already installed, install our Kubernetes integration. Upgrade the installation to include the New Relic Metrics Adapter with the following command: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace --reuse-values \\ --set metrics-adapter.enabled=true \\ --set newrelic-k8s-metrics-adapter.personalAPIKey=YOUR_NEW_RELIC_PERSONAL_API_KEY \\ --set newrelic-k8s-metrics-adapter.config.accountID=YOUR_NEW_RELIC_ACCOUNT_ID \\ --set newrelic-k8s-metrics-adapter.config.externalMetrics.external_metric_name.query=NRQL query Copy Please notice and adjust the following flags: metrics-adapter.enabled: Must be set to true so the metrics adapter chart is installed. newrelic-k8s-metrics-adapter.personalAPIKey: Must be set to valid New Relic Personal API key. newrelic-k8s-metrics-adapter.accountID: Must be set to valid New Relic account where metrics are going to be fetched from. newrelic-k8s-metrics-adapter.config.externalMetrics.<var>external_metric_name</var>.<var>query</var>: Adds a new external metric where: <var>external_metric_name</var>: The metric name. <var>query</var>: The base NRQL query that is used to get the value for the metric. Tip Alternatively, you can use a values.yaml file that can be passed to the helm command with the --values flag. Values files can contain all parameters needed to configure the metrics explained in the configuration section. Configuration You can configure multiple metrics in the metrics adapter and change some parameters to modify the behaviour of the metrics cache and filtering. To see the full list and descriptions of all parameters that can be modified, refer to the chart README.md and values.yaml files. How it works The following example is a Helm values file that enable the metrics adapter on the nri-bundle chart installation, and configures the nginx_average_requests metric: metrics-adapter: enabled: true newrelic-k8s-metrics-adapter: personalAPIKey: <Personal API Key> config: accountID: <Account ID> externalMetrics: nginx_average_requests: query: \"FROM Metric SELECT average(nginx.server.net.requestsPerSecond) SINCE 2 MINUTES AGO\" Copy Caution The default time span for metrics is 1h. Therefore, you should define queries with the SINCE clause to adjust the time span according to your environment and needs. There is an HPA consuming the external metric as follows: kind: HorizontalPodAutoscaler apiVersion: autoscaling/v2beta2 metadata: name: nginx-scaler spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: nginx minReplicas: 1 maxReplicas: 10 metrics: - type: External external: metric: name: nginx_average_requests selector: matchLabels: k8s.namespaceName: nginx target: type: Value value: 10000 Copy Based on the HPA definition, the controller manager fetches the metrics from the external metrics API which are served by the New Relic metrics adapter. The New Relic metrics adapter receives the query including the nginx_average_requests metric name and all the selectors, and searches for a matching metric name in the internal memory based on the configured metrics. Then, it adds the selectors to the query to form a final query that is executed using NerdGraph to fetch the value from New Relic. The above example will generate a query like the following: FROM Metric SELECT average(nginx.server.net.requestsPerSecond) WHERE clusterName=<clusterName> AND `k8s.namespaceName`='nginx' SINCE 2 MINUTES AGO Copy Notice that a clusterName filter has been automatically added to the query to exclude metrics from other clusters in the same account. You can remove it by using the removeClusterFilter configuration parameter. Also the value is cached for a period of time defined by the cacheTTLSeconds configuration parameter, whose default is 30 seconds. Troubleshooting Get verbose logs Most common errors are displayed in the standard (non-verbose) logs. If you're doing a more in-depth investigation on your own or with New Relic Support, you can enable verbose mode. To get verbose logging details for an integration using Helm: Enable verbose logging: bash Copy $ helm upgrade -n <namespace> --reuse-values newrelic-bundle --set newrelic-k8s-metrics-adapter.verboseLog=true newrelic/nri-bundle Leave on verbose mode for a few minutes, or until enough activity has occurred. When you have the information you need, disable verbose logging: bash Copy $ helm upgrade --reuse-values newrelic-bundle --set newrelic-k8s-metrics-adapter.verboseLog=false newrelic/nri-bundle Caution Verbose mode increases significantly the amount of information sent to log files. Enable this mode temporarily, only for troubleshooting purposes, and reset the log level when finished. Get raw metrics Sometimes it's useful to get the list of available metrics and also to get the current value of an specific metric. To get the list of metrics available, run: bash Copy $ kubectl get --raw \"/apis/external.metrics.k8s.io/v1beta1/\" To get the value for a specific metric with a selector, run: bash Copy $ kubectl get --raw \"/apis/external.metrics.k8s.io/v1beta1/namespaces/*/<metric_name>?labelSelector=<selector_key>=<selector_value>\" Tip You must replace <metric_name>, <selector_key> and <selector_value> with your values. Metrics not working There are some usual errors that could cause a metric fail to retrieve the value. These errors are showed in the status of the metrics when you describe the HPA or are printed when you get the raw metrics directly. executing query: NRQL Syntax Error: Error at line...: The query that is being run has syntax errors. The same error message gives you the executed query and position of the error. You can try this query inside the New Relic query builder and correct the configuration from the adapter. extracting return value: expected first value to be of type \"float64\", got %!q(<nil>): The query doesn't return any value. The same error message gives you the executed query so you can try this query inside the New Relic query builder and correct the configuration from the adapter or the match selectors in the HPA.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 216.98819,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Link</em> <em>apps</em> <em>and</em> <em>services</em>",
        "body": "BETA FEATURE This feature is still in development, but we encourage you to try it out! You can use metrics from your New Relic account to autoscale applications and <em>services</em> in your <em>Kubernetes</em> cluster by deploying the New Relic Metrics Adapter. This adapter fetches the metric values from New Relic"
      },
      "id": "6175209d28ccbcf310c6bb2f"
    },
    {
      "sections": [
        "Monitor services running on Kubernetes",
        "Get started",
        "What you need",
        "Enable monitoring of services",
        "Get the config YAML for the integration",
        "Example configuration",
        "Configuration options for each integration",
        "Monitor services in our Kubernetes integration installed with Helm",
        "Learn more",
        "Manually configure service monitoring",
        "How the service-specific YAML config works",
        "Add a service YAML to the Kubernetes integration config",
        "Add multiple services to the same config"
      ],
      "title": "Monitor services running on Kubernetes",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Link apps and services"
      ],
      "external_id": "d2f12474df5b90e0226b8aa203450417e783b996",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/link-apps-services/monitor-services-running-kubernetes/",
      "published_at": "2022-01-08T13:08:12Z",
      "updated_at": "2021-10-24T00:51:45Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic's Kubernetes integration you can monitor both Kubernetes and the services running on it, such as Cassandra, Redis, MySQL, and other supported services. Get started Our Kubernetes integration comes bundled with some of our on-host integrations (like Cassandra, MySQL, and Apache). This lets you get data for those supported services by adding a section to the Kubernetes integration's configuration, which lives as a ConfigMap inside a manifest. What you need Enable this feature for a service Details about how configuration works For an example of how to monitor Redis running on a Kubernetes PHP Guestbook, see this tutorial. What you need To monitor services running on Kubernetes, you only need a Kubernetes cluster running the Kubernetes integration, version 1.16.0 or higher (install | check version | update). We support the following services running on Kubernetes: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP Enable monitoring of services To enable our Kubernetes integration to monitor one or more services: Expand this dropdown and get the YAML snippets for the service(s) you want to monitor: Get the config YAML for the integration For the services you want to monitor, follow the links to GitHub to get the YAML snippets you'll need for the next step: Apache Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ Redis SNMP Add the snippet to the Kubernetes integration's ConfigMap, after the data: section: Example configuration This example shows the YAML config for the Apache integration ( highlighted ) added to the Kubernetes integration's config. Respect the indentation levels. --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: apache-config.yaml: | --- # Run auto discovery to find pods with label \"app=apache\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Use the optional arguments: # --namespaces: Comma separated namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: apache integrations: - name: nri-apache env: # Use the discovered IP as the host address STATUS_URL: http://${discovery.ip}/server-status?auto METRICS: 1 Copy You can add snippets for multiple services to the same config file. See an example. Depending on your environment, you may need or want to set additional config options. Expand the dropdown below for links to configuration options. Configuration options for each integration Select a service to see available config options: Apache Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ Redis SNMP Verify monitoring is enabled: Go to one.newrelic.com > Infrastructure, select Third party services, and then select the service's dashboard. You should see data being reported. Additional notes about enabling services: Enabling multiple services may use more resources than what is set in the resource limits of the Kubernetes integration config file. If this becomes an issue, raise the limit in the resources section. The Kubernetes integration does not automatically update. For best results, regularly update. Monitor services in our Kubernetes integration installed with Helm If you installed our Kubernetes integration using Helm, to monitor services you need to update the existing installation with the new configuration, which contains the services to monitor: helm upgrade --reuse-values -f values.yaml [RELEASE] [CHART] Copy If you use nri-bundle charts, you need to update the children's chart values. Find some examples here. Learn more More resources for learning about configuration: Learn technical details about how configuration works. Learn how to configure monitoring of multiple services with the same config file. See a step-by-step tutorial showing how to monitor a Redis service on Kubernetes. Manually configure service monitoring The enable procedure should be all you need to get monitoring working, but if you run into problems, understanding some technical details about configuration can be helpful. This section goes into more detail about how configuration works. For each service you wish to monitor, you must add a configuration file for that integration to our Kubernetes integration's configuration. This document will cover these subjects: How the service-specific configuration YAML snippet works Adding the service-specific YAML in the Kubernetes integration's config file Adding multiple services to the Kubernetes integration's config file How the service-specific YAML config works Our Kubernetes integration's configuration follows the ConfigMap format. Using a ConfigMap allows us to decouple the configuration for the integrations from the Kubernetes image. The other benefit is that a ConfigMap can be updated automatically without reloading the running container. Because the infrastructure agent uses YAML to configure its associated integrations, ConfigMaps are a good choice for storing YAML. (For more information on config file format, see the Integration config file format.) The Kubernetes integration image comes with an auto-discovery feature that simplifies the configuration of multiple instances of services using a single configuration file. For example, if you have several NGINX instances running, creating an NGINX integration configuration file for every instance would be hard to implement and hard to update. With our auto-discovery option, you can discover and monitor all your NGINX instances with a single configuration file. Each integration has its own specific configuration YAML. Our NGINX integration default config file looks like this: nginx-config.yml: | --- discovery: command: # Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --port: Port used to connect to the kubelet. Default is 10255 # --tls: Use secure (TLS) connection # Custom Example: # exec: /var/db/newrelic-infra/nri-discovery-kubernetes --namespaces namespace1,namespace2 --port 10250 --tls # Default exec: /var/db/newrelic-infra/nri-discovery-kubernetes match: label.app: nginx integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}/status STATUS_MODULE: discover METRICS: 1 Copy The above config enables the following: Runs nri-discovery-kubernetes to query the data for the node we are currently on. Parses the data that comes back and looks for any Kubernetes pod that has a Kubernetes container with an app= label with value nginx. For any matches, it attempts to run the NGINX integration. The status URL is built from: The pod's IP address The status page is pulled from the label on K8s pod called status_url This automatic discovery works the same as the container auto-discovery used by the infrastructure agent. For more advanced options, see Container auto-discovery. Add a service YAML to the Kubernetes integration config It's best practice to configure enabled integrations alongside the Kubernetes integration configuration. This is easier than maintaining configuration files for every single service/integration instance. Below is an example of a Kubernetes integration's ConfigMap. The highlighted section shows where an integration configuration YAML (in this case, NGINX) is placed. For more information on discovery:, see Container auto-discovery for on-host integrations. --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: nginx-config.yml: | --- # Run auto discovery to find pods with label \"app=nginx\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: nginx integrations: - name: nri-nginx env: # If you're using ngx_http_api_module be certain to use the full path up to and including the version number # Use the discovered IP as the host address STATUS_URL: http://${discovery.ip}/status # Comma separated list of ngx_http_api_module, NON PARAMETERIZED, Endpoints # endpoints: /nginx,/processes,/connections,/ssl,/slabs,/http,/http/requests,/http/server_zones,/http/caches,/http/upstreams,/http/keyvals,/stream,/stream/server_zones,/stream/upstreams,/stream/keyvals,/stream/zone_sync # Name of Nginx status module OHI is to query against. discover | ngx_http_stub_status_module | ngx_http_status_module | ngx_http_api_module STATUS_MODULE: discover METRICS: 1 Copy This configuration map can then be referenced in the DaemonSet, the same as the one that was generated via the command line. Make sure the namespace used is the same one used by the Kubernetes integration manifest. If you haven't changed it in the downloaded manifest file, the value is default. Add multiple services to the same config You can monitor several services using the same Kubernetes integration config file. To do this, add another integration configuration YAML to the same Kubernetes integration config file. Below is the Kubernetes config created in the last section, with a new section for the Cassandra integration's config (highlighted). --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: nginx-config.yml: | --- # Run auto discovery to find pods with label \"app=nginx\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Run discovery for Kubernetes. Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: nginx integrations: - name: nri-nginx env: # If you're using ngx_http_api_module be certain to use the full path up to and including the version number # Use the discovered IP as the host address STATUS_URL: http://${discovery.ip}/status # Comma separated list of ngx_http_api_module, NON PARAMETERIZED, Endpoints # endpoints: /nginx,/processes,/connections,/ssl,/slabs,/http,/http/requests,/http/server_zones,/http/caches,/http/upstreams,/http/keyvals,/stream,/stream/server_zones,/stream/upstreams,/stream/keyvals,/stream/zone_sync # Name of Nginx status module OHI is to query against. discover | ngx_http_stub_status_module | ngx_http_status_module | ngx_http_api_module STATUS_MODULE: discover METRICS: 1 cassandra-configuration.yml: | --- # Run auto discovery to find pods with label \"app=cassandra\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Run discovery for Kubernetes. Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: cassandra integrations: - name: nri-cassandra env: # Use the discovered IP as the host address HOSTNAME: ${discovery.ip} PORT: 7199 USERNAME: cassandra PASSWORD: cassandra METRICS: 1/mark Copy The Kubernetes integration config is now set up to monitor these two services. Additionally, depending on your environment, there may be some additional service-specific configuration you must do. When you've completed configuration, our infrastructure agent looks for any pod with a label cassandra and runs the integration against it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 213.66853,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor <em>services</em> running on <em>Kubernetes</em>",
        "sections": "Monitor <em>services</em> in our <em>Kubernetes</em> <em>integration</em> installed with Helm",
        "tags": "<em>Link</em> <em>apps</em> <em>and</em> <em>services</em>",
        "body": "With New Relic&#x27;s <em>Kubernetes</em> <em>integration</em> you can monitor both <em>Kubernetes</em> and the <em>services</em> running on it, such as Cassandra, Redis, MySQL, and other supported <em>services</em>. Get started Our <em>Kubernetes</em> <em>integration</em> comes bundled with some of our on-host <em>integrations</em> (like Cassandra, MySQL, and Apache"
      },
      "id": "617d587828ccbc645c8000ae"
    },
    {
      "sections": [
        "Tutorial: Monitor Redis running on Kubernetes",
        "What you need",
        "Step 1: Set up an example Redis application",
        "Step 2: Enable monitoring of Redis instances"
      ],
      "title": "Tutorial: Monitor Redis running on Kubernetes",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Link apps and services"
      ],
      "external_id": "52427a948d6fa2c673688066cc87cbd6c97863b4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/link-apps-services/tutorial-monitor-redis-running-kubernetes/",
      "published_at": "2022-01-08T12:35:55Z",
      "updated_at": "2021-10-24T03:07:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have a service running on Kubernetes, and it's a service we support, you can enable monitoring of that service by adding a configuration section for that integration to the Kubernetes integration's config. This tutorial shows how to enable monitoring for a Redis service running on the Kubernetes PHP Guestbook. For the general procedure, see Monitor a Kubernetes-running service. What you need See the general requirements for this feature, including supported services. The kubectl command-line tool must be configured to communicate with your cluster. If you don't have a cluster, you can create one using Minikube. Step 1: Set up an example Redis application This tutorial builds on the Kubernetes tutorial Deploying a PHP Guestbook application with Redis. Skip the Kubernetes tutorial and run the following command to set up the application needed for our tutorial: kubectl create -f https://raw.githubusercontent.com/kubernetes/examples/master/guestbook/all-in-one/guestbook-all-in-one.yaml Copy If you'd like to first complete the Kubernetes tutorial, follow their tutorial instructions but do not follow the instructions in the Cleaning up section. Step 2: Enable monitoring of Redis instances The PHP Guestbook application has three Redis instances: one master and two slave instances. Each instance is tagged with a label where app=redis. For this example, we're using our Redis monitoring integration. It can monitor both master and slave instances of Redis, so we don’t have to distinguish between them. In the Kubernetes integration's YAML config file (newrelic-infrastructure-k8s-latest.yaml), you need to update the nri-integration-cfg section. From the list of integration configs, get the Redis integration YAML and add it to the Kubernetes config. The Redis YAML is highlighted below. --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: redis-config.yml: | --- # Run auto discovery to find pods with label \"app=redis\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Run discovery for Kubernetes. Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: redis integrations: - name: nri-redis env: # using the discovered IP as the hostname address HOSTNAME: ${discovery.ip} PORT: 6379 KEYS: '{\"0\":[\"<KEY_1>\"],\"1\":[\"<KEY_2>\"]}' REMOTE_MONITORING: true labels: env: production Copy Deploy the updated service: kubectl create -f newrelic-infrastructure-k8s-latest.yaml Copy You should be able to see the following in the logs for the pod newrelic-infra: time=\"2019-12-23T17:37:07Z\" level=info msg=\"Integration health check starting\" instance=redis-metrics integration=com.newrelic.redis prefix=integration/com.newrelic.redis working-dir=/var/db/newrelic-infra/newrelic-integrations time=\"2019-12-23T17:37:07Z\" level=info msg=\"Integration health check finished with success\" instance=redis-metrics integration=com.newrelic.redis prefix=integration/com.newrelic.redis working-dir=/var/db/newrelic-infra/newrelic-integrations Copy If there are no errors, you should see Redis data in the Infrastructure UI. To find the Redis dashboards, go to one.newrelic.com > Infrastructure > Third party services, and select the Redis dashboard. For the general procedure of how to monitor services running on Kubernetes, including more detail about how configuration works, see Monitor a Kubernetes-running service.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 211.79741,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Tutorial: Monitor Redis running on <em>Kubernetes</em>",
        "sections": "Tutorial: Monitor Redis running on <em>Kubernetes</em>",
        "tags": "<em>Link</em> <em>apps</em> <em>and</em> <em>services</em>",
        "body": " is highlighted below. --- apiVersion: v1 kind: ConfigMap metadata: name: nri-<em>integration</em>-cfg namespace: default data: redis-config.yml: | --- # Run auto discovery to find pods with label &quot;<em>app</em>=redis&quot; # https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>integrations</em>&#x2F;host-<em>integrations</em>&#x2F;installation&#x2F;container-auto-discovery discovery"
      },
      "id": "617daead64441fd15bfbdfcb"
    }
  ],
  "/docs/kubernetes-pixie/kubernetes-integration/newrelic-hpa-metrics-adapter/newrelic-metrics-adapter": [
    {
      "sections": [
        "Monitor services running on Kubernetes",
        "Get started",
        "What you need",
        "Enable monitoring of services",
        "Get the config YAML for the integration",
        "Example configuration",
        "Configuration options for each integration",
        "Monitor services in our Kubernetes integration installed with Helm",
        "Learn more",
        "Manually configure service monitoring",
        "How the service-specific YAML config works",
        "Add a service YAML to the Kubernetes integration config",
        "Add multiple services to the same config"
      ],
      "title": "Monitor services running on Kubernetes",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Link apps and services"
      ],
      "external_id": "d2f12474df5b90e0226b8aa203450417e783b996",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/link-apps-services/monitor-services-running-kubernetes/",
      "published_at": "2022-01-08T13:08:12Z",
      "updated_at": "2021-10-24T00:51:45Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic's Kubernetes integration you can monitor both Kubernetes and the services running on it, such as Cassandra, Redis, MySQL, and other supported services. Get started Our Kubernetes integration comes bundled with some of our on-host integrations (like Cassandra, MySQL, and Apache). This lets you get data for those supported services by adding a section to the Kubernetes integration's configuration, which lives as a ConfigMap inside a manifest. What you need Enable this feature for a service Details about how configuration works For an example of how to monitor Redis running on a Kubernetes PHP Guestbook, see this tutorial. What you need To monitor services running on Kubernetes, you only need a Kubernetes cluster running the Kubernetes integration, version 1.16.0 or higher (install | check version | update). We support the following services running on Kubernetes: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP Enable monitoring of services To enable our Kubernetes integration to monitor one or more services: Expand this dropdown and get the YAML snippets for the service(s) you want to monitor: Get the config YAML for the integration For the services you want to monitor, follow the links to GitHub to get the YAML snippets you'll need for the next step: Apache Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ Redis SNMP Add the snippet to the Kubernetes integration's ConfigMap, after the data: section: Example configuration This example shows the YAML config for the Apache integration ( highlighted ) added to the Kubernetes integration's config. Respect the indentation levels. --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: apache-config.yaml: | --- # Run auto discovery to find pods with label \"app=apache\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Use the optional arguments: # --namespaces: Comma separated namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: apache integrations: - name: nri-apache env: # Use the discovered IP as the host address STATUS_URL: http://${discovery.ip}/server-status?auto METRICS: 1 Copy You can add snippets for multiple services to the same config file. See an example. Depending on your environment, you may need or want to set additional config options. Expand the dropdown below for links to configuration options. Configuration options for each integration Select a service to see available config options: Apache Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ Redis SNMP Verify monitoring is enabled: Go to one.newrelic.com > Infrastructure, select Third party services, and then select the service's dashboard. You should see data being reported. Additional notes about enabling services: Enabling multiple services may use more resources than what is set in the resource limits of the Kubernetes integration config file. If this becomes an issue, raise the limit in the resources section. The Kubernetes integration does not automatically update. For best results, regularly update. Monitor services in our Kubernetes integration installed with Helm If you installed our Kubernetes integration using Helm, to monitor services you need to update the existing installation with the new configuration, which contains the services to monitor: helm upgrade --reuse-values -f values.yaml [RELEASE] [CHART] Copy If you use nri-bundle charts, you need to update the children's chart values. Find some examples here. Learn more More resources for learning about configuration: Learn technical details about how configuration works. Learn how to configure monitoring of multiple services with the same config file. See a step-by-step tutorial showing how to monitor a Redis service on Kubernetes. Manually configure service monitoring The enable procedure should be all you need to get monitoring working, but if you run into problems, understanding some technical details about configuration can be helpful. This section goes into more detail about how configuration works. For each service you wish to monitor, you must add a configuration file for that integration to our Kubernetes integration's configuration. This document will cover these subjects: How the service-specific configuration YAML snippet works Adding the service-specific YAML in the Kubernetes integration's config file Adding multiple services to the Kubernetes integration's config file How the service-specific YAML config works Our Kubernetes integration's configuration follows the ConfigMap format. Using a ConfigMap allows us to decouple the configuration for the integrations from the Kubernetes image. The other benefit is that a ConfigMap can be updated automatically without reloading the running container. Because the infrastructure agent uses YAML to configure its associated integrations, ConfigMaps are a good choice for storing YAML. (For more information on config file format, see the Integration config file format.) The Kubernetes integration image comes with an auto-discovery feature that simplifies the configuration of multiple instances of services using a single configuration file. For example, if you have several NGINX instances running, creating an NGINX integration configuration file for every instance would be hard to implement and hard to update. With our auto-discovery option, you can discover and monitor all your NGINX instances with a single configuration file. Each integration has its own specific configuration YAML. Our NGINX integration default config file looks like this: nginx-config.yml: | --- discovery: command: # Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --port: Port used to connect to the kubelet. Default is 10255 # --tls: Use secure (TLS) connection # Custom Example: # exec: /var/db/newrelic-infra/nri-discovery-kubernetes --namespaces namespace1,namespace2 --port 10250 --tls # Default exec: /var/db/newrelic-infra/nri-discovery-kubernetes match: label.app: nginx integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}/status STATUS_MODULE: discover METRICS: 1 Copy The above config enables the following: Runs nri-discovery-kubernetes to query the data for the node we are currently on. Parses the data that comes back and looks for any Kubernetes pod that has a Kubernetes container with an app= label with value nginx. For any matches, it attempts to run the NGINX integration. The status URL is built from: The pod's IP address The status page is pulled from the label on K8s pod called status_url This automatic discovery works the same as the container auto-discovery used by the infrastructure agent. For more advanced options, see Container auto-discovery. Add a service YAML to the Kubernetes integration config It's best practice to configure enabled integrations alongside the Kubernetes integration configuration. This is easier than maintaining configuration files for every single service/integration instance. Below is an example of a Kubernetes integration's ConfigMap. The highlighted section shows where an integration configuration YAML (in this case, NGINX) is placed. For more information on discovery:, see Container auto-discovery for on-host integrations. --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: nginx-config.yml: | --- # Run auto discovery to find pods with label \"app=nginx\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: nginx integrations: - name: nri-nginx env: # If you're using ngx_http_api_module be certain to use the full path up to and including the version number # Use the discovered IP as the host address STATUS_URL: http://${discovery.ip}/status # Comma separated list of ngx_http_api_module, NON PARAMETERIZED, Endpoints # endpoints: /nginx,/processes,/connections,/ssl,/slabs,/http,/http/requests,/http/server_zones,/http/caches,/http/upstreams,/http/keyvals,/stream,/stream/server_zones,/stream/upstreams,/stream/keyvals,/stream/zone_sync # Name of Nginx status module OHI is to query against. discover | ngx_http_stub_status_module | ngx_http_status_module | ngx_http_api_module STATUS_MODULE: discover METRICS: 1 Copy This configuration map can then be referenced in the DaemonSet, the same as the one that was generated via the command line. Make sure the namespace used is the same one used by the Kubernetes integration manifest. If you haven't changed it in the downloaded manifest file, the value is default. Add multiple services to the same config You can monitor several services using the same Kubernetes integration config file. To do this, add another integration configuration YAML to the same Kubernetes integration config file. Below is the Kubernetes config created in the last section, with a new section for the Cassandra integration's config (highlighted). --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: nginx-config.yml: | --- # Run auto discovery to find pods with label \"app=nginx\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Run discovery for Kubernetes. Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: nginx integrations: - name: nri-nginx env: # If you're using ngx_http_api_module be certain to use the full path up to and including the version number # Use the discovered IP as the host address STATUS_URL: http://${discovery.ip}/status # Comma separated list of ngx_http_api_module, NON PARAMETERIZED, Endpoints # endpoints: /nginx,/processes,/connections,/ssl,/slabs,/http,/http/requests,/http/server_zones,/http/caches,/http/upstreams,/http/keyvals,/stream,/stream/server_zones,/stream/upstreams,/stream/keyvals,/stream/zone_sync # Name of Nginx status module OHI is to query against. discover | ngx_http_stub_status_module | ngx_http_status_module | ngx_http_api_module STATUS_MODULE: discover METRICS: 1 cassandra-configuration.yml: | --- # Run auto discovery to find pods with label \"app=cassandra\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Run discovery for Kubernetes. Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: cassandra integrations: - name: nri-cassandra env: # Use the discovered IP as the host address HOSTNAME: ${discovery.ip} PORT: 7199 USERNAME: cassandra PASSWORD: cassandra METRICS: 1/mark Copy The Kubernetes integration config is now set up to monitor these two services. Additionally, depending on your environment, there may be some additional service-specific configuration you must do. When you've completed configuration, our infrastructure agent looks for any pod with a label cassandra and runs the integration against it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 213.66853,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor <em>services</em> running on <em>Kubernetes</em>",
        "sections": "Monitor <em>services</em> in our <em>Kubernetes</em> <em>integration</em> installed with Helm",
        "tags": "<em>Link</em> <em>apps</em> <em>and</em> <em>services</em>",
        "body": "With New Relic&#x27;s <em>Kubernetes</em> <em>integration</em> you can monitor both <em>Kubernetes</em> and the <em>services</em> running on it, such as Cassandra, Redis, MySQL, and other supported <em>services</em>. Get started Our <em>Kubernetes</em> <em>integration</em> comes bundled with some of our on-host <em>integrations</em> (like Cassandra, MySQL, and Apache"
      },
      "id": "617d587828ccbc645c8000ae"
    },
    {
      "sections": [
        "Tutorial: Monitor Redis running on Kubernetes",
        "What you need",
        "Step 1: Set up an example Redis application",
        "Step 2: Enable monitoring of Redis instances"
      ],
      "title": "Tutorial: Monitor Redis running on Kubernetes",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Link apps and services"
      ],
      "external_id": "52427a948d6fa2c673688066cc87cbd6c97863b4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/link-apps-services/tutorial-monitor-redis-running-kubernetes/",
      "published_at": "2022-01-08T12:35:55Z",
      "updated_at": "2021-10-24T03:07:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have a service running on Kubernetes, and it's a service we support, you can enable monitoring of that service by adding a configuration section for that integration to the Kubernetes integration's config. This tutorial shows how to enable monitoring for a Redis service running on the Kubernetes PHP Guestbook. For the general procedure, see Monitor a Kubernetes-running service. What you need See the general requirements for this feature, including supported services. The kubectl command-line tool must be configured to communicate with your cluster. If you don't have a cluster, you can create one using Minikube. Step 1: Set up an example Redis application This tutorial builds on the Kubernetes tutorial Deploying a PHP Guestbook application with Redis. Skip the Kubernetes tutorial and run the following command to set up the application needed for our tutorial: kubectl create -f https://raw.githubusercontent.com/kubernetes/examples/master/guestbook/all-in-one/guestbook-all-in-one.yaml Copy If you'd like to first complete the Kubernetes tutorial, follow their tutorial instructions but do not follow the instructions in the Cleaning up section. Step 2: Enable monitoring of Redis instances The PHP Guestbook application has three Redis instances: one master and two slave instances. Each instance is tagged with a label where app=redis. For this example, we're using our Redis monitoring integration. It can monitor both master and slave instances of Redis, so we don’t have to distinguish between them. In the Kubernetes integration's YAML config file (newrelic-infrastructure-k8s-latest.yaml), you need to update the nri-integration-cfg section. From the list of integration configs, get the Redis integration YAML and add it to the Kubernetes config. The Redis YAML is highlighted below. --- apiVersion: v1 kind: ConfigMap metadata: name: nri-integration-cfg namespace: default data: redis-config.yml: | --- # Run auto discovery to find pods with label \"app=redis\" # https://docs.newrelic.com/docs/integrations/host-integrations/installation/container-auto-discovery discovery: command: # Run discovery for Kubernetes. Use the following optional arguments : # --namespaces: Comma separated list of namespaces to discover pods on # --tls: Use secure (TLS) connection # --port: Port used to connect to the kubelet. Default is 10255 exec: /var/db/newrelic-infra/nri-discovery-kubernetes --port PORT --tls match: label.app: redis integrations: - name: nri-redis env: # using the discovered IP as the hostname address HOSTNAME: ${discovery.ip} PORT: 6379 KEYS: '{\"0\":[\"<KEY_1>\"],\"1\":[\"<KEY_2>\"]}' REMOTE_MONITORING: true labels: env: production Copy Deploy the updated service: kubectl create -f newrelic-infrastructure-k8s-latest.yaml Copy You should be able to see the following in the logs for the pod newrelic-infra: time=\"2019-12-23T17:37:07Z\" level=info msg=\"Integration health check starting\" instance=redis-metrics integration=com.newrelic.redis prefix=integration/com.newrelic.redis working-dir=/var/db/newrelic-infra/newrelic-integrations time=\"2019-12-23T17:37:07Z\" level=info msg=\"Integration health check finished with success\" instance=redis-metrics integration=com.newrelic.redis prefix=integration/com.newrelic.redis working-dir=/var/db/newrelic-infra/newrelic-integrations Copy If there are no errors, you should see Redis data in the Infrastructure UI. To find the Redis dashboards, go to one.newrelic.com > Infrastructure > Third party services, and select the Redis dashboard. For the general procedure of how to monitor services running on Kubernetes, including more detail about how configuration works, see Monitor a Kubernetes-running service.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 211.79741,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Tutorial: Monitor Redis running on <em>Kubernetes</em>",
        "sections": "Tutorial: Monitor Redis running on <em>Kubernetes</em>",
        "tags": "<em>Link</em> <em>apps</em> <em>and</em> <em>services</em>",
        "body": " is highlighted below. --- apiVersion: v1 kind: ConfigMap metadata: name: nri-<em>integration</em>-cfg namespace: default data: redis-config.yml: | --- # Run auto discovery to find pods with label &quot;<em>app</em>=redis&quot; # https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>integrations</em>&#x2F;host-<em>integrations</em>&#x2F;installation&#x2F;container-auto-discovery discovery"
      },
      "id": "617daead64441fd15bfbdfcb"
    },
    {
      "sections": [
        "Link your applications to Kubernetes",
        "Tip",
        "Compatibility and requirements",
        "Kubernetes requirements",
        "Network requirements",
        "APM agent compatibility",
        "Openshift requirements",
        "Important",
        "Configure the injection of metadata",
        "Default configuration",
        "Custom configuration",
        "Manage custom certificates",
        "Validate the injection of metadata",
        "Disable the injection of metadata",
        "Troubleshooting"
      ],
      "title": "Link your applications to Kubernetes",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Link apps and services"
      ],
      "external_id": "0fe0951312aaf683f6614d5956f8c402b9693780",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/link-your-applications/link-your-applications-kubernetes/",
      "published_at": "2022-01-08T09:12:18Z",
      "updated_at": "2021-10-30T20:44:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can surface Kubernetes metadata and link it to your APM agents as distributed traces to explore performance issues and troubleshoot transaction errors. For more information, see this New Relic blog post. You can quickly start monitoring Kubernetes clusters using Auto-telemetry with Pixie, which is currently a beta release. This Pixie integration into New Relic does not require a language agent. Learn more about Auto-telemetry with Pixie here. Tip Our Kubernetes metadata injection project is open source. Here's the code to link APM and infrastructure data and the code to automatically manage certificates. Compatibility and requirements Before linking Kubernetes metadata to your APM agents, make sure you meet the following requirements: Kubernetes requirements Network requirements APM agent compatibility OpenShift requirements Kubernetes requirements To link your applications and Kubernetes, your cluster must have the MutatingAdmissionWebhook controller enabled, which requires Kubernetes 1.9 or higher. To verify that your cluster is compatible, run the following command: kubectl api-versions | grep admissionregistration.k8s.io/v1beta1 admissionregistration.k8s.io/v1beta1 Copy If you see a different result, follow the Kubernetes documentation to enable admission control in your cluster. Network requirements For Kubernetes to speak to our MutatingAdmissionWebhook, the master node (or the API server container, depending on how the cluster is set up) should be allowed egress for HTTPS traffic on port 443 to pods in all of the other nodes in the cluster. This might require specific configuration depending on how the infrastructure is set up (on-premises, AWS, Google Cloud, etc). Tip Until Kubernetes v1.14, users were only allowed to register admission webhooks on port 443. Since v1.15 it's possible to register them on different ports. To ensure backward compatibility, the webhook is registered by default on port 443 in the YAML config file we distribute. APM agent compatibility The following New Relic agents collect Kubernetes metadata: Go 2.3.0 or higher Java 4.10.0 or higher Node.js 5.3.0 or higher Python 4.14.0 or higher Ruby 6.1.0 or higher .NET 8.17.438 or higher Openshift requirements To link Openshift and Kubernetes you must enable mutating admission webhooks, which requires Openshift 3.9 or higher. During the process, install a resource that requires admin permissions to the cluster. Run this to log in as admin: oc login -u system:admin Copy Check that webhooks are correctly configured. If they are not, update the master-config.yaml file. admissionConfig: pluginConfig: MutatingAdmissionWebhook: configuration: apiVersion: apiserver.config.k8s.io/v1alpha1 kubeConfigFile: /dev/null kind: WebhookAdmission ValidatingAdmissionWebhook: configuration: apiVersion: apiserver.config.k8s.io/v1alpha1 kubeConfigFile: /dev/null kind: WebhookAdmission location: \"\" Copy Important Add kubeConfigFile: /dev/null to address some issues in Openshift. Enable certificate signing by editing the YAML file and updating your configuration: kubernetesMasterConfig: controllerArguments: cluster-signing-cert-file: - \"/etc/origin/master/ca.crt\" cluster-signing-key-file: - \"/etc/origin/master/ca.key\" Copy Restart the Openshift services in the master node. Configure the injection of metadata By default, all the pods you create that include APM agents have the correct environment variables set and the metadata injection applies to the entire cluster. To check that the environment variables have been set, any container that is running must be stopped, and a new instance started (see Validate the injection of metadata). This default configuration also uses the Kubernetes certificates API to automatically manage the certificates required for the injection. If needed, you can limit the injection of metadata to specific namespaces in your cluster or self-manage your certificates. Default configuration We offer instructions for deploying our integration using Helm. Just be sure that, when you are configuring the chart, the webhook that inject the metadata is enabled. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled=true \\ --set newrelic-infrastructure.privileged=true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled= true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Custom configuration You can limit the injection of metadata only to specific namespaces by using labels. To enable this feature, edit nri-bundle Helm values.yaml file: nri-metadata-injector: injectOnlyLabeledNamespaces: true Copy Or add a --set when installing or upgrading your Helm release: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled=true \\ --set newrelic-infrastructure.privileged=true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled= true \\ --set nri-metadata-injector.injectOnlyLabeledNamespaces=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy With this option, injection is only applied to those namespaces that have the newrelic-metadata-injection label set to enabled: kubectl label namespace YOUR_NAMESPACE newrelic-metadata-injection=enabled Copy Manage custom certificates To use custom certificates you need to disable the automatic installation of certificates when you are installing using Helm. To disable the installation for certificates just modify nri-bundle Helm values.yaml like this: nri-metadata-injector: customTLSCertificate: true Copy Or add a --set when installing or upgrading your Helm release: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled=true \\ --set newrelic-infrastructure.privileged=true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled= true \\ --set nri-metadata-injector.customTLSCertificate=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Now you can proceed with the custom certificate management option. You need your certificate, server key, and Certification Authority (CA) bundle encoded in PEM format. If you have them in the standard certificate format (X.509), install openssl, and run the following: openssl x509 -in CERTIFICATE_FILENAME -outform PEM -out CERTIFICATE_FILENAME.pem openssl x509 -in SERVER_KEY_FILENAME -outform PEM -out SERVER_KEY_FILENAME.pem openssl x509 -in CA_BUNDLE_FILENAME -outform PEM -out BUNDLE_FILENAME.pem Copy If your certificate/key pair are in another format, see the Digicert knowledgebase for more help. Create the TLS secret with the signed certificate/key pair, and patch the mutating webhook configuration with the CA using the following commands: kubectl create secret tls newrelic-metadata-injection-admission \\ --key=PEM_ENCODED_SERVER_KEY \\ --cert=PEM_ENCODED_CERTIFICATE \\ --dry-run -o yaml | kubectl -n newrelic apply -f - caBundle=$(cat PEM_ENCODED_CA_BUNDLE | base64 | td -d $'\\n') kubectl patch mutatingwebhookconfiguration newrelic-metadata-injection-cfg --type='json' -p \"[{'op': 'replace', 'path': '/webhooks/0/clientConfig/caBundle', 'value':'${caBundle}'}]\" Copy Important Certificates signed by Kubernetes have an expiration of one year. For more information, see the Kubernetes source code in GitHub. Validate the injection of metadata In order to validate that the webhook (responsible for injecting the metadata) was installed correctly, deploy a new pod and check for the New Relic environment variables. Create a dummy pod containing Busybox by running: kubectl create -f https://git.io/vPieo Copy Check if New Relic environment variables were injected: kubectl exec busybox0 -- env | grep NEW_RELIC_METADATA_KUBERNETES NEW_RELIC_METADATA_KUBERNETES_CLUSTER_NAME=fsi NEW_RELIC_METADATA_KUBERNETES_NODE_NAME=nodea NEW_RELIC_METADATA_KUBERNETES_NAMESPACE_NAME=default NEW_RELIC_METADATA_KUBERNETES_POD_NAME=busybox0 NEW_RELIC_METADATA_KUBERNETES_CONTAINER_NAME=busybox Copy Disable the injection of metadata To disable/uninstall the injection of metadata, use the following commands: Delete the Kubernetes objects using the yaml file: kubectl delete -f k8s-metadata-injection-latest.yaml Copy Delete the TLS secret containing the certificate/key pair: kubectl delete secret/newrelic-metadata-injection-secret Copy Troubleshooting Follow these troubleshooting tips as needed. No Kubernetes metadata in APM or distributed tracing transactions Problem The creation of the secret by the k8s-webhook-cert-manager job used to fail due to the kubectl version used by the image when running in Kubernetes version 1.19.x, The new version 1.3.2 fixes this issue, therefore it is enough to run again the job using an update version of the image to fix the issue. Solution Update the image k8s-webhook-cert-manager (to a version >= 1.3.2) and re-run the job. The secret will be correctly created and the k8s-metadata-injection pod will be able to start. Note that the new version of the manifest and of the nri-bundle are already updated with the correct version of the image. Problem In OpenShift version 4.x, the CA that is used in order to patch the mutatingwebhookconfiguration resource is not the one used when signing the certificates. This is a known issue currently tracked here. In the logs of the Pod nri-metadata-injection, you'll see the following error message: TLS handshake error from 10.131.0.29:37428: remote error: tls: unknown certificate authority TLS handshake error from 10.129.0.1:49314: remote error: tls: bad certificate Copy Workaround Manually update the certificate stored in the mutatingwebhookconfiguration object. The correct CA locations might change according to the cluster configuration. However, you can usually find the CA in the secret csr-signer in the namespace openshift-kube-controller-manager. Problem There is no Kubernetes metadata included in the transactions' attributes of your APM agent or in distributed tracing. Solution Verify that the environment variables are being correctly injected by following the instructions described in the Validate your installation step. If they are not present, get the name of the metadata injection pod by running: kubectl get pods | grep newrelic-metadata-injection-deployment kubectl logs -f pod/podname Copy In another terminal, create a new pod (for example, see Validate your installation), and inspect the logs of the metadata injection deployment for errors. For every created pod there should be a set of 4 new entries in the logs like: {\"level\":\"info\",\"ts\":\"2020-04-09T12:55:32.107Z\",\"caller\":\"server/main.go:139\",\"msg\":\"POST https://newrelic-metadata-injection-svc.default.svc:443/mutate?timeout=30s HTTP/2.0\\\" from 10.11.49.2:32836\"} {\"level\":\"info\",\"ts\":\"2020-04-09T12:55:32.110Z\",\"caller\":\"server/webhook.go:168\",\"msg\":\"received admission review\",\"kind\":\"/v1, Kind=Pod\",\"namespace\":\"default\",\"name\":\"\",\"pod\":\"busybox1\",\"UID\":\"6577519b-7a61-11ea-965e-0e46d1c9335c\",\"operation\":\"CREATE\",\"userinfo\":{\"username\":\"admin\",\"uid\":\"admin\",\"groups\":[\"system:masters\",\"system:authenticated\"]}} {\"level\":\"info\",\"ts\":\"2020-04-09T12:55:32.111Z\",\"caller\":\"server/webhook.go:182\",\"msg\":\"admission response created\",\"response\":\"[{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env\\\",\\\"value\\\":[{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_CLUSTER_NAME\\\",\\\"value\\\":\\\"adn_kops\\\"}]},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_NODE_NAME\\\",\\\"valueFrom\\\":{\\\"fieldRef\\\":{\\\"fieldPath\\\":\\\"spec.nodeName\\\"}}}},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_NAMESPACE_NAME\\\",\\\"valueFrom\\\":{\\\"fieldRef\\\":{\\\"fieldPath\\\":\\\"metadata.namespace\\\"}}}},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_POD_NAME\\\",\\\"valueFrom\\\":{\\\"fieldRef\\\":{\\\"fieldPath\\\":\\\"metadata.name\\\"}}}},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_CONTAINER_NAME\\\",\\\"value\\\":\\\"busybox\\\"}},{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"/spec/containers/0/env/-\\\",\\\"value\\\":{\\\"name\\\":\\\"NEW_RELIC_METADATA_KUBERNETES_CONTAINER_IMAGE_NAME\\\",\\\"value\\\":\\\"busybox\\\"}}]\"} {\"level\":\"info\",\"ts\":\"2020-04-09T12:55:32.111Z\",\"caller\":\"server/webhook.go:257\",\"msg\":\"writing response\"} Copy If there are no new entries on the logs, it means that the apiserver is not being able to communicate with the webhook service, this could be due to networking rules or security groups rejecting the communication. To check if the apiserver is not being able to communicate with the webhook you should inspect the apiserver logs for errors like: failed calling webhook \"metadata-injection.newrelic.com\": ERROR_REASON Copy To get the apiserver logs: Start a proxy to the Kubernetes API server by the executing the following command in a terminal window and keep it running. kubectl proxy --port=8001 Copy Create a new pod in your cluster, this will make the apiserver try to communicate with the webhook. The following command will create a busybox. kubectl create -f https://git.io/vPieo Copy Retrieve the apiserver logs. curl localhost:8001/logs/kube-apiserver.log > apiserver.log Copy Delete the busybox container. kubectl delete -f https://git.io/vPieo Copy Inspect the logs for errors. grep -E 'failed calling webhook' apiserver.log Copy Remember that one of the requirements for the metadata injection is that the apiserver must be allowed egress to the pods running on the cluster. If you encounter errors regarding connection timeouts or failed connections, make sure to check the security groups and firewall rules of the cluster. If there are no log entries in either the apiserver logs or the metadata injection deployment, it means that the webhook was not properly registered. Ensure the metadata injection setup job ran successfully by inspecting the output of: kubectl get job newrelic-metadata-setup Copy If the job is not completed, investigate the logs of the setup job: kubectl logs job/newrelic-metadata-setup Copy Ensure the CertificateSigningRequest is approved and issued by running: kubectl get csr newrelic-metadata-injection-svc.default Copy Ensure the TLS secret is present by running: kubectl get secret newrelic-metadata-injection-secret Copy Ensure the CA bundle is present in the mutating webhook configuration: kubectl get mutatingwebhookconfiguration newrelic-metadata-injection-cfg -o json Copy Ensure the TargetPort of the Service resource matches the Port of the Deployment's container: kubectl describe service/newrelic-metadata-injection-svc kubectl describe deployment/newrelic-metadata-injection-deployment Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 211.58652,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Link</em> your <em>applications</em> to <em>Kubernetes</em>",
        "sections": "<em>Link</em> your <em>applications</em> to <em>Kubernetes</em>",
        "tags": "<em>Link</em> <em>apps</em> <em>and</em> <em>services</em>",
        "body": " is currently a beta release. This Pixie <em>integration</em> into New Relic does not require a language agent. Learn more about Auto-telemetry with Pixie here. Tip Our <em>Kubernetes</em> metadata injection project is open source. Here&#x27;s the code to <em>link</em> APM and infrastructure data and the code to automatically"
      },
      "id": "617daead28ccbc662b7ffe23"
    }
  ],
  "/docs/kubernetes-pixie/kubernetes-integration/troubleshooting/certificate-signed-unknown-authority": [
    {
      "sections": [
        "Install the Kubernetes integration manually using Helm",
        "Compatibility and requirements",
        "Install Kubernetes integration with Helm",
        "Install with Helm 3 and nri-bundle (recommended)",
        "Installing and configuring nri-bundle with Helm",
        "Install with Helm 2 and nri-bundle (legacy)",
        "Installation instructions for Helm 2",
        "Important",
        "Tip",
        "Helm configuration options",
        "Upgrade using Helm",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data",
        "Reduce data ingest",
        "Prometheus OpenMetrics Integration",
        "New Relic Logging",
        "New Relic Pixie Integration",
        "Uninstall Kubernetes integration"
      ],
      "title": "Install the Kubernetes integration manually using Helm",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "29d42af98d41e7e4e7be86f0150254e132cb6b6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/install-kubernetes-integration-using-helm/",
      "published_at": "2022-01-08T09:49:44Z",
      "updated_at": "2022-01-08T09:49:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Helm is a package manager on top of Kubernetes. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in Kubernetes. To install the integration using Helm, we recommend our Kubernetes automated installer, which will prompt for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our integration as plain manifests rather than a Helm release. See Kubernetes integration: install and configure for more details about how to use our automated installer. Start the installer This page describes in more depth how to install and configure the New Relic integration without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the Kubernetes integration. Our charts are also compatible with Helm 2, but support for it might be removed in the future. To install the Kubernetes integration using Helm, you will need your New Relic account license key and your Kubernetes cluster's name: Find and copy your New Relic license key. Find the name of your cluster with this command: kubectl config current-context Copy Note these values somewhere safe, as you will need them later during the installation process. Install Kubernetes integration with Helm Install with Helm 3 and nri-bundle (recommended) New Relic has several charts for the different components which offer different features for the platform: newrelic-infrastructure: Contains the main Kubernetes integration and the infrastructure agent. This is the core component for the New Relic Kubernetes experience, responsible of reporting most of the data that is surfaced in dashboard and the Kubernetes Cluster Explorer. newrelic-logging: Provides a DaemonSet with New Relic's Fluent Bit output plugin to easily forward your logs to New Relic. nri-kube-events: Collects and reports cluster events (such as kubectl get events) to New Relic. nri-prometheus: New Relic's Prometheus OpenMetrics Integration, automatically scrapes Prometheus endpoints present in the cluser and reports metrics to New Relic. nri-metadata-injection: Sets up a minimal MutatingAdmissionWebhook that injects a couple of environment to containers. These contain metadata about the cluster and New Relic installation and will be later picked up by applications instrumented using APM, allowing to correlate APM and infrastructure data. nri-statsd: New Relic StatsD integration. Additionally, New Relic provides nri-bundle, a chart which pulls a selectable set of the charts mentioned above. nri-bundle can also install Kube State Metrics and Pixie for you if needed. While it is possible to install those charts separately, we strongly recommend using the nri-bundle chart for Kubernetes deployments, as it ensures that values across all the charts are consistent and provides full control over which components are installed, as well as the possibility to configure all of them as Helm dependencies. This is the same chart that is used and referenced by our automated installer. Installing and configuring nri-bundle with Helm Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Please notice and adjust the following flags: global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY: Must be set to a valid License Key for your account. global.cluster=K8S_CLUSTER_NAME: Is used to identify the cluster in the New Relic UI, so should be a descriptive value not used by any other Kubernetes cluster configured in your New Relic account. ksm.enabled=true: Setting this to true will automatically install Kube State Metrics (KSM) for you, which is required for our integration to run. You can set this to false if KSM is already present in your cluster, even if it is on a different namespace. newrelic-infrastructure.privileged=true: Can be set to false to install a trimmed down version of our integration that does not require extra privileges, such as hostPath mounts or running containers as root. Please note that this will disable detailed process collection from the host. For performance reasons, our logging solution still requires hostPath mounts, regardless of the value of this flag. If this is not allowedin your cluster, you will need to disable the logging solution by specifying logging.enabled=false. prometheus.enabled=true: Will deploy our Prometheus OpenMetrics integration, which automatically collects data from prometheus endpoints present in the cluster. webhook.enabled=true: Will install our minimal webhook, which adds environment variables that, in turn, allows linking applications instrumented with APM with infrastructure data. Our chart has a comprehensive set of flags and tunables that can be edited to configure our solution to your particular needs. For a full list of all the flags that can be configured, please check the chart's README and the default values.yaml file. Install the Kubernetes integration by running the customized command without --debug and --dry-run: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl -n newrelic get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node for newrelic-infrastructure, as well as one Deployment and one pod for any other component that you have enabled. Install with Helm 2 and nri-bundle (legacy) Installation instructions for Helm 2 Important Helm 2 has been deprecated and New Relic does not recommend using it for deployments. Instructions in this section are provided for legacy systems only. To install using Helm 2: Ensure that Helm 2, including their cluster-side components (such as Tiller) are properly installed and configured. Please check the official documentation for more details. Set the cluster where you want to install the agent: kubectl config set-cluster DESIRED_CLUSTER Copy To see the available clusters, run kubectl config get-clusters Make sure that kube-state-metrics is installed on your machine: kubectl get deployment --all-namespaces | grep kube-state-metrics Copy If it's not installed, follow the instructions in the kube-state-metrics GitHub repo to install it. Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. This step uses the --dry-run and --debug switches and therefore the agent is not installed. helm install newrelic/newrelic-infrastructure \\ --dry-run \\ --debug \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Install the New Relic Kubernetes integration: helm install newrelic/newrelic-infrastructure \\ --set licenseKey=your_new_relic_license_key \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Tip Note that the --dry-run and --debug switches have been removed. Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node. Helm configuration options When you install or upgrade the Kubernetes integration with Helm using the command line, you can pass your configuration variables with the --set flag. helm install newrelic/newrelic-infrastructure \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=YOUR_CLUSTER_NAME Copy A full list of the configuration parameters can be found in the newrelic-infrastructure chart README Upgrade using Helm To update your Kubernetes integration installed via Helm: Update the local chart repository: helm repo update Copy Update the release by running again the appropriate helm upgrade --install ... command in the section above. Monitor services running on Kubernetes After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages. Reduce data ingest Our charts support setting an option to reduce the amount of data ingest at the cost of dropping detailed information. To enable it, set global.lowDataMode=true in the nri-bundle chart. lowDataMode affects three specific components of the nri-bundle chart outlined below. Prometheus OpenMetrics Integration If lowDataMode is enabled, the following metrics are excluded by default as they are already collected and used by the New Relic Kubernetes Integration. - kube_ - container_ - machine_ - cadvisor_ Copy New Relic Logging If lowDataMode is enabled, Labels and Annotations are set to Off in the Filter section of the fluent-bit.conf file. This means that this detail will be dropped from the container log files which reduces the overall data ingest into New Relic. The following fields are retained: Allowlist_key container_name Allowlist_key namespace_name Allowlist_key pod_name Allowlist_key stream Allowlist_key log Copy Low Data Mode Log Example Complete Log Record [ { \"cluster_name\": \"api-test\", \"kubernetes\": { \"annotations\": { \"kubernetes.io/psp\": \"eks.privileged\" }, \"container_hash\": \"fryckbos/test@sha256:5b098eaf3c7d5b3585eb10cebee63665b6208bea31ef31a3f0856c5ffdda644b\", \"container_image\": \"fryckbos/test:latest\", \"container_name\": \"newrelic-logging\", \"docker_id\": \"134e1daf63761baa15e035b08b7aea04518a0f0e50af4215131a50c6a379a072\", \"host\": \"ip-192-168-17-123.ec2.internal\", \"labels\": { \"app\": \"newrelic-logging\", \"app.kubernetes.io/name\": \"newrelic-logging\", \"controller-revision-hash\": \"84db95db86\", \"pod-template-generation\": \"1\", \"release\": \"nri-bundle\" }, \"namespace_name\": \"nrlogs\", \"pod_id\": \"54556e3e-719c-46b5-af69-020b75d69bf1\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\" }, \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"plugin\": { \"source\": \"kubernetes\", \"type\": \"fluent-bit\", \"version\": \"1.8.1\" }, \"stream\": \"stderr\", \"time\": \"2021-09-14T12:30:49.138824971Z\", \"timestamp\": 1631622649138 } ] Copy Log Record after enabling lowDataMode. [ { \"cluster_name\": \"api-test\", \"container_name\": \"newrelic-logging\", \"namespace_name\": \"nrlogs\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\", \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"stream\": \"stderr\", \"timestamp\": 1631622649138 } ] Copy New Relic Pixie Integration If lowDataMode is enabled, the newrelic-pixie integration performs heavier sampling on Pixie spans and reduces the collection interval from 10 seconds to 15 seconds. lowDataMode settings: HTTP_SPAN_LIMIT: 750 DB_SPAN_LIMIT: 250 COLLECT_INTERVAL_SEC: 15 Copy The default settings for these parameters and others can be found in the newrelic-pixie-integration Github repo. Uninstall Kubernetes integration To uninstall the Kubernetes integration using Helm, run the following command. Replace some-manifest-file.yml with your specific filename. helm uninstall newrelic -n newrelic if kubectl then : kubectl delete -f some-manifest-file.yml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 160.49757,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "sections": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "Helm is a package manager on top of <em>Kubernetes</em>. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in <em>Kubernetes</em>. To install the <em>integration</em> using Helm, we recommend our <em>Kubernetes</em> automated installer, which will prompt"
      },
      "id": "617d5841196a67bb40f7c1de"
    },
    {
      "sections": [
        "Configure Kubernetes with a proxy",
        "Install the infrastructure agent with a proxy",
        "Set logging with a proxy",
        "Install Kubernetes events with a proxy",
        "Install the Prometheus OpenMetrics integration with a proxy",
        "Set the synthetics minion to use a proxy"
      ],
      "title": "Configure Kubernetes with a proxy",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "9d7794244b451d878d00db0f84c4709e43ce138c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/configure-kubernetes-proxy/",
      "published_at": "2022-01-08T09:49:01Z",
      "updated_at": "2021-12-25T07:57:45Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are running our Kubernetes integration with a proxy, you need to configure each component (the infrastructure agent, logging, Kubernetes events, etc.) to correctly work with your proxy. Install the infrastructure agent with a proxy There are three options to install the infrastructure agent with a proxy: Installing with chart newrelic-infrastructure: The proxy can be configured setting the configuration option config.proxy as described in the values.yaml of the chart. The whole config object is used to generate a configMap that is mounted as the file /etc/newrelic-infra.yml used to configure the agent. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option newrelic-infrastructure.config.proxy. The configuration option is passed down to the dependency newrelic-infrastructure modifying the values.yaml of nri-bundle: newrelic-infrastructure.config.proxy: https://user:password@hostname:port Copy Installing with manifest: When installing through the manifest, the proxy can be configured adding the NRIA_PROXY environment variable to the environment variable section of the DaemonSet configuration. The environment variable NRIA_VERBOSE in the DaemonSet YAML can be used as an example of the environment variable configuration. Set logging with a proxy If you’re using log forwarding for the Kubernetes integration, the plugin automatically detects the HTTP_PROXY and HTTPS_PROXY environment variables, and automatically uses them to set up the proxy configuration. For more information, see how to customize the proxy, or bypass it. Install Kubernetes events with a proxy There are three options to install the Kubernetes events integration with a proxy: Installing with chart nri-kube-events: The proxy can be configured setting the configuration option proxy in the values.yaml file. If set, the variable NRIA_PROXY will be added into the deployment.yaml template. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option nri-kube-events.proxy. The configuration option is passed down to the dependency nri-kube-events modifying the values.yaml of nri-bundle: nri-kube-events.proxy: https://user:password@hostname:port Copy Installing with manifest: When installing through the manifest, to configure the proxy set the NRIA_PROXY in the environment variable section of the infra container. Check the deployment template of the chart as an example of environment variable configuration. Install the Prometheus OpenMetrics integration with a proxy There are three options to install the Prometheus OpenMetric integration with a proxy: Installing with charts nri-prometheus: The proxy can be configured setting the configuration option config.emitter_proxy in the values.yaml. The config object is used to generate a configMap that mounted into the deployment under /etc/nri-prometheus/. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option nri-prometheus.config.emitter_proxy. The configuration option is passed down to the dependency nri-prometheus modifying the values.yaml of nri-bundle: nri-prometheus.config.emitter_proxy: \"http://localhost:8888 Copy Installing with the manifest: Configure the proxy uncommenting the emitter_proxy option directly in the manifest: # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" Copy Set the synthetics minion to use a proxy Check this README on how to set the proxy for the Synthetics minions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 106.280136,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure <em>Kubernetes</em> with a proxy",
        "sections": "Configure <em>Kubernetes</em> with a proxy",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "If you are running our <em>Kubernetes</em> <em>integration</em> with a proxy, you need to configure each component (the infrastructure agent, logging, <em>Kubernetes</em> events, etc.) to correctly work with your proxy. Install the infrastructure agent with a proxy There are three options to install the infrastructure agent"
      },
      "id": "617d584264441f2d01fbc6d7"
    },
    {
      "sections": [
        "Get logs and version",
        "Get verbose logs for installations using a manifest file",
        "Caution",
        "Get verbose logs for installations using Helm",
        "Get the infrastructure agent version",
        "Get kube-state-metrics version",
        "Get logs from pods connecting to kube-state-metrics",
        "Retrieve kube-state-metrics service configuration",
        "Get logs from pod running on a master node"
      ],
      "title": "Get logs and version",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Troubleshooting"
      ],
      "external_id": "35fd6fb08a49327a4d2ce5797511e55e39793c67",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/troubleshooting/get-logs-version/",
      "published_at": "2022-01-08T09:51:19Z",
      "updated_at": "2021-10-30T20:45:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To generate verbose logs and get version and configuration information, follow these procedures. Get verbose logs for installations using a manifest file For the Kubernetes integration, the infrastructure agent adds a log entry only in the event of an error. Most common errors are displayed in the standard (non-verbose) logs. If you are doing a more in-depth investigation on your own or with New Relic Support, you can enable verbose mode. Caution Verbose mode significantly increases the amount of info sent to log files. Temporarily enable this mode only for troubleshooting purposes, and reset the log level when finished. To get verbose logging details for an integration using a manifest file: Enable verbose logging: In the deployment file, set the value of NRIA_VERBOSE to 1. Apply the modified configuration by running: kubectl apply -f your_newrelic_k8s.yaml Copy Leave on verbose mode for a few minutes, or until you feel enough activity has occurred. Disable verbose mode: Set the NRIA_VERBOSE value back to 0. Apply the restored configuration by running: kubectl apply -f your_newrelic_k8s.yaml Copy Get a list of nodes in the environment: kubectl get nodes --all-namespaces Copy Get a list of infrastructure and kube-state-metrics pods: kubectl get pods --all-namespaces -o wide | egrep 'newrelic|kube-state-metrics' Copy Get logs from the pods connecting to kube-state-metrics. Retrieve kube-state-metrics service configuration. Get verbose logs for installations using Helm For the Kubernetes integration, the infrastructure agent adds a log entry only in the event of an error. Most common errors are displayed in the standard (non-verbose) logs. If you're doing a more in-depth investigation on your own or with New Relic Support, you can enable verbose mode. Caution Verbose mode significantly increases the amount of info sent to log files. Temporarily enable this mode only for troubleshooting purposes, and reset the log level when finished. To get verbose logging details for an integration using Helm: Enable verbose logging: helm upgrade -n <namespace> --reuse-values newrelic-bundle --set newrelic-infrastructure.verboseLog=true newrelic/nri-bundle Copy Leave on verbose mode for a few minutes, or until enough activity has occurred. When you have the information you need, disable verbose logging: helm upgrade --reuse-values newrelic-bundle --set newrelic-infrastructure.verboseLog=false newrelic/nri-bundle Copy Follow the steps to restore your configuration from step 5 in the section, Get verbose logs for installations using a manifest file. Get the infrastructure agent version For the Kubernetes integration, the infrastructure agent is distributed as a Docker image that contains the infrastructure agent and the Kubernetes integration. The Docker image is tagged with a version, and the infrastructure agent also has its own version. When the agent is successfully sending information to New Relic, you can retrieve the versions of the infrastructure agent for Kubernetes (the Docker image) you are running in your clusters by using the following NRQL query: FROM K8sContainerSample SELECT uniqueCount(entityId) WHERE containerName = 'newrelic-infra' facet clusterName, containerImage Copy If the agent is not reporting any data: Get the version(s) of the New Relic integration for Kubernetes that you are running in a cluster using kubectl: kubectl get pods --all-namespaces -l name=newrelic-infra -o jsonpath=\"{.items..spec..containers..image}\" Copy Look for output similar to this: newrelic/infrastructure-k8s:1.0.0 Copy Get kube-state-metrics version To retrieve the version of kube-state-metrics running on your clusters, run the following NRQL query: FROM K8sContainerSample SELECT uniqueCount(entityId) WHERE containerName LIKE '%kube-state-metrics%' facet clusterName, containerImage Copy Get logs from pods connecting to kube-state-metrics To get the logs from pods connecting to kube-state-metrics: Get the nodes that kube-state-metrics is running on: kubectl get pods --all-namespaces -o wide | grep kube-state-metrics Copy Look for output similar to this: kube-system kube-state-metrics-5c6f5cb9b5-pclhh 2/2 Running 4 4d 172.17.0.3 minikube Copy Get the New Relic pods that are running on the same node as kube-state-metrics: kubectl describe node minikube | grep newrelic-infra Copy Look for output similar to this: default newrelic-infra-5wcv6 100m (5%) 0 (0%) 100Mi (5%) 100Mi (5%) Copy Retrieve the logs for the nodes by running: kubectl logs newrelic-infra-5wcv6 Copy Retrieve kube-state-metrics service configuration To retrieve the configuration: Run: kubectl get pods --all-namespaces | grep \"kube-state-metrics\" Copy Look for a response similar to this: kube-system kube-state-metrics-5c6f5cb9b5-5wf9m 2/2 Running 8 6d Copy Review the namespace in the first column. kubectl describe service kube-state-metrics -n <namespace> Copy Get logs from pod running on a master node To get the logs from a pod running on a master node: Get the nodes that are labelled as master: kubectl get nodes -l node-role.kubernetes.io/master=\"\" Copy Or, kubectl get nodes -l kubernetes.io/role=\"master\" Copy Look for output similar to this: NAME STATUS ROLES AGE VERSION ip-10-42-24-4.ec2.internal Ready master 42d v1.14.8 Copy Get the New Relic pods that are running on one of the nodes returned in the previous step: kubectl get pods --field-selector spec.nodeName=ip-10-42-24-4.ec2.internal -l name=newrelic-infra --all-namespaces Copy Look for output similar to this: newrelic-infra-whvzt Copy Retrieve the logs for the nodes by running: kubectl logs newrelic-infra-whvzt Copy For troubleshooting help, see Not seeing data or Error messages.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 105.96205,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "To generate verbose logs and get version and configuration information, follow these procedures. Get verbose logs for installations using a manifest file For the <em>Kubernetes</em> <em>integration</em>, the infrastructure agent adds a log entry only in the event of an error. Most common errors are displayed"
      },
      "id": "617daee5e7b9d2337dc03982"
    }
  ],
  "/docs/kubernetes-pixie/kubernetes-integration/troubleshooting/get-logs-version": [
    {
      "sections": [
        "Install the Kubernetes integration manually using Helm",
        "Compatibility and requirements",
        "Install Kubernetes integration with Helm",
        "Install with Helm 3 and nri-bundle (recommended)",
        "Installing and configuring nri-bundle with Helm",
        "Install with Helm 2 and nri-bundle (legacy)",
        "Installation instructions for Helm 2",
        "Important",
        "Tip",
        "Helm configuration options",
        "Upgrade using Helm",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data",
        "Reduce data ingest",
        "Prometheus OpenMetrics Integration",
        "New Relic Logging",
        "New Relic Pixie Integration",
        "Uninstall Kubernetes integration"
      ],
      "title": "Install the Kubernetes integration manually using Helm",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "29d42af98d41e7e4e7be86f0150254e132cb6b6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/install-kubernetes-integration-using-helm/",
      "published_at": "2022-01-08T09:49:44Z",
      "updated_at": "2022-01-08T09:49:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Helm is a package manager on top of Kubernetes. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in Kubernetes. To install the integration using Helm, we recommend our Kubernetes automated installer, which will prompt for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our integration as plain manifests rather than a Helm release. See Kubernetes integration: install and configure for more details about how to use our automated installer. Start the installer This page describes in more depth how to install and configure the New Relic integration without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the Kubernetes integration. Our charts are also compatible with Helm 2, but support for it might be removed in the future. To install the Kubernetes integration using Helm, you will need your New Relic account license key and your Kubernetes cluster's name: Find and copy your New Relic license key. Find the name of your cluster with this command: kubectl config current-context Copy Note these values somewhere safe, as you will need them later during the installation process. Install Kubernetes integration with Helm Install with Helm 3 and nri-bundle (recommended) New Relic has several charts for the different components which offer different features for the platform: newrelic-infrastructure: Contains the main Kubernetes integration and the infrastructure agent. This is the core component for the New Relic Kubernetes experience, responsible of reporting most of the data that is surfaced in dashboard and the Kubernetes Cluster Explorer. newrelic-logging: Provides a DaemonSet with New Relic's Fluent Bit output plugin to easily forward your logs to New Relic. nri-kube-events: Collects and reports cluster events (such as kubectl get events) to New Relic. nri-prometheus: New Relic's Prometheus OpenMetrics Integration, automatically scrapes Prometheus endpoints present in the cluser and reports metrics to New Relic. nri-metadata-injection: Sets up a minimal MutatingAdmissionWebhook that injects a couple of environment to containers. These contain metadata about the cluster and New Relic installation and will be later picked up by applications instrumented using APM, allowing to correlate APM and infrastructure data. nri-statsd: New Relic StatsD integration. Additionally, New Relic provides nri-bundle, a chart which pulls a selectable set of the charts mentioned above. nri-bundle can also install Kube State Metrics and Pixie for you if needed. While it is possible to install those charts separately, we strongly recommend using the nri-bundle chart for Kubernetes deployments, as it ensures that values across all the charts are consistent and provides full control over which components are installed, as well as the possibility to configure all of them as Helm dependencies. This is the same chart that is used and referenced by our automated installer. Installing and configuring nri-bundle with Helm Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Please notice and adjust the following flags: global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY: Must be set to a valid License Key for your account. global.cluster=K8S_CLUSTER_NAME: Is used to identify the cluster in the New Relic UI, so should be a descriptive value not used by any other Kubernetes cluster configured in your New Relic account. ksm.enabled=true: Setting this to true will automatically install Kube State Metrics (KSM) for you, which is required for our integration to run. You can set this to false if KSM is already present in your cluster, even if it is on a different namespace. newrelic-infrastructure.privileged=true: Can be set to false to install a trimmed down version of our integration that does not require extra privileges, such as hostPath mounts or running containers as root. Please note that this will disable detailed process collection from the host. For performance reasons, our logging solution still requires hostPath mounts, regardless of the value of this flag. If this is not allowedin your cluster, you will need to disable the logging solution by specifying logging.enabled=false. prometheus.enabled=true: Will deploy our Prometheus OpenMetrics integration, which automatically collects data from prometheus endpoints present in the cluster. webhook.enabled=true: Will install our minimal webhook, which adds environment variables that, in turn, allows linking applications instrumented with APM with infrastructure data. Our chart has a comprehensive set of flags and tunables that can be edited to configure our solution to your particular needs. For a full list of all the flags that can be configured, please check the chart's README and the default values.yaml file. Install the Kubernetes integration by running the customized command without --debug and --dry-run: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl -n newrelic get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node for newrelic-infrastructure, as well as one Deployment and one pod for any other component that you have enabled. Install with Helm 2 and nri-bundle (legacy) Installation instructions for Helm 2 Important Helm 2 has been deprecated and New Relic does not recommend using it for deployments. Instructions in this section are provided for legacy systems only. To install using Helm 2: Ensure that Helm 2, including their cluster-side components (such as Tiller) are properly installed and configured. Please check the official documentation for more details. Set the cluster where you want to install the agent: kubectl config set-cluster DESIRED_CLUSTER Copy To see the available clusters, run kubectl config get-clusters Make sure that kube-state-metrics is installed on your machine: kubectl get deployment --all-namespaces | grep kube-state-metrics Copy If it's not installed, follow the instructions in the kube-state-metrics GitHub repo to install it. Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. This step uses the --dry-run and --debug switches and therefore the agent is not installed. helm install newrelic/newrelic-infrastructure \\ --dry-run \\ --debug \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Install the New Relic Kubernetes integration: helm install newrelic/newrelic-infrastructure \\ --set licenseKey=your_new_relic_license_key \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Tip Note that the --dry-run and --debug switches have been removed. Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node. Helm configuration options When you install or upgrade the Kubernetes integration with Helm using the command line, you can pass your configuration variables with the --set flag. helm install newrelic/newrelic-infrastructure \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=YOUR_CLUSTER_NAME Copy A full list of the configuration parameters can be found in the newrelic-infrastructure chart README Upgrade using Helm To update your Kubernetes integration installed via Helm: Update the local chart repository: helm repo update Copy Update the release by running again the appropriate helm upgrade --install ... command in the section above. Monitor services running on Kubernetes After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages. Reduce data ingest Our charts support setting an option to reduce the amount of data ingest at the cost of dropping detailed information. To enable it, set global.lowDataMode=true in the nri-bundle chart. lowDataMode affects three specific components of the nri-bundle chart outlined below. Prometheus OpenMetrics Integration If lowDataMode is enabled, the following metrics are excluded by default as they are already collected and used by the New Relic Kubernetes Integration. - kube_ - container_ - machine_ - cadvisor_ Copy New Relic Logging If lowDataMode is enabled, Labels and Annotations are set to Off in the Filter section of the fluent-bit.conf file. This means that this detail will be dropped from the container log files which reduces the overall data ingest into New Relic. The following fields are retained: Allowlist_key container_name Allowlist_key namespace_name Allowlist_key pod_name Allowlist_key stream Allowlist_key log Copy Low Data Mode Log Example Complete Log Record [ { \"cluster_name\": \"api-test\", \"kubernetes\": { \"annotations\": { \"kubernetes.io/psp\": \"eks.privileged\" }, \"container_hash\": \"fryckbos/test@sha256:5b098eaf3c7d5b3585eb10cebee63665b6208bea31ef31a3f0856c5ffdda644b\", \"container_image\": \"fryckbos/test:latest\", \"container_name\": \"newrelic-logging\", \"docker_id\": \"134e1daf63761baa15e035b08b7aea04518a0f0e50af4215131a50c6a379a072\", \"host\": \"ip-192-168-17-123.ec2.internal\", \"labels\": { \"app\": \"newrelic-logging\", \"app.kubernetes.io/name\": \"newrelic-logging\", \"controller-revision-hash\": \"84db95db86\", \"pod-template-generation\": \"1\", \"release\": \"nri-bundle\" }, \"namespace_name\": \"nrlogs\", \"pod_id\": \"54556e3e-719c-46b5-af69-020b75d69bf1\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\" }, \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"plugin\": { \"source\": \"kubernetes\", \"type\": \"fluent-bit\", \"version\": \"1.8.1\" }, \"stream\": \"stderr\", \"time\": \"2021-09-14T12:30:49.138824971Z\", \"timestamp\": 1631622649138 } ] Copy Log Record after enabling lowDataMode. [ { \"cluster_name\": \"api-test\", \"container_name\": \"newrelic-logging\", \"namespace_name\": \"nrlogs\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\", \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"stream\": \"stderr\", \"timestamp\": 1631622649138 } ] Copy New Relic Pixie Integration If lowDataMode is enabled, the newrelic-pixie integration performs heavier sampling on Pixie spans and reduces the collection interval from 10 seconds to 15 seconds. lowDataMode settings: HTTP_SPAN_LIMIT: 750 DB_SPAN_LIMIT: 250 COLLECT_INTERVAL_SEC: 15 Copy The default settings for these parameters and others can be found in the newrelic-pixie-integration Github repo. Uninstall Kubernetes integration To uninstall the Kubernetes integration using Helm, run the following command. Replace some-manifest-file.yml with your specific filename. helm uninstall newrelic -n newrelic if kubectl then : kubectl delete -f some-manifest-file.yml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 160.49757,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "sections": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "Helm is a package manager on top of <em>Kubernetes</em>. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in <em>Kubernetes</em>. To install the <em>integration</em> using Helm, we recommend our <em>Kubernetes</em> automated installer, which will prompt"
      },
      "id": "617d5841196a67bb40f7c1de"
    },
    {
      "sections": [
        "Configure Kubernetes with a proxy",
        "Install the infrastructure agent with a proxy",
        "Set logging with a proxy",
        "Install Kubernetes events with a proxy",
        "Install the Prometheus OpenMetrics integration with a proxy",
        "Set the synthetics minion to use a proxy"
      ],
      "title": "Configure Kubernetes with a proxy",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "9d7794244b451d878d00db0f84c4709e43ce138c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/configure-kubernetes-proxy/",
      "published_at": "2022-01-08T09:49:01Z",
      "updated_at": "2021-12-25T07:57:45Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are running our Kubernetes integration with a proxy, you need to configure each component (the infrastructure agent, logging, Kubernetes events, etc.) to correctly work with your proxy. Install the infrastructure agent with a proxy There are three options to install the infrastructure agent with a proxy: Installing with chart newrelic-infrastructure: The proxy can be configured setting the configuration option config.proxy as described in the values.yaml of the chart. The whole config object is used to generate a configMap that is mounted as the file /etc/newrelic-infra.yml used to configure the agent. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option newrelic-infrastructure.config.proxy. The configuration option is passed down to the dependency newrelic-infrastructure modifying the values.yaml of nri-bundle: newrelic-infrastructure.config.proxy: https://user:password@hostname:port Copy Installing with manifest: When installing through the manifest, the proxy can be configured adding the NRIA_PROXY environment variable to the environment variable section of the DaemonSet configuration. The environment variable NRIA_VERBOSE in the DaemonSet YAML can be used as an example of the environment variable configuration. Set logging with a proxy If you’re using log forwarding for the Kubernetes integration, the plugin automatically detects the HTTP_PROXY and HTTPS_PROXY environment variables, and automatically uses them to set up the proxy configuration. For more information, see how to customize the proxy, or bypass it. Install Kubernetes events with a proxy There are three options to install the Kubernetes events integration with a proxy: Installing with chart nri-kube-events: The proxy can be configured setting the configuration option proxy in the values.yaml file. If set, the variable NRIA_PROXY will be added into the deployment.yaml template. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option nri-kube-events.proxy. The configuration option is passed down to the dependency nri-kube-events modifying the values.yaml of nri-bundle: nri-kube-events.proxy: https://user:password@hostname:port Copy Installing with manifest: When installing through the manifest, to configure the proxy set the NRIA_PROXY in the environment variable section of the infra container. Check the deployment template of the chart as an example of environment variable configuration. Install the Prometheus OpenMetrics integration with a proxy There are three options to install the Prometheus OpenMetric integration with a proxy: Installing with charts nri-prometheus: The proxy can be configured setting the configuration option config.emitter_proxy in the values.yaml. The config object is used to generate a configMap that mounted into the deployment under /etc/nri-prometheus/. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option nri-prometheus.config.emitter_proxy. The configuration option is passed down to the dependency nri-prometheus modifying the values.yaml of nri-bundle: nri-prometheus.config.emitter_proxy: \"http://localhost:8888 Copy Installing with the manifest: Configure the proxy uncommenting the emitter_proxy option directly in the manifest: # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" Copy Set the synthetics minion to use a proxy Check this README on how to set the proxy for the Synthetics minions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 106.280136,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure <em>Kubernetes</em> with a proxy",
        "sections": "Configure <em>Kubernetes</em> with a proxy",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "If you are running our <em>Kubernetes</em> <em>integration</em> with a proxy, you need to configure each component (the infrastructure agent, logging, <em>Kubernetes</em> events, etc.) to correctly work with your proxy. Install the infrastructure agent with a proxy There are three options to install the infrastructure agent"
      },
      "id": "617d584264441f2d01fbc6d7"
    },
    {
      "sections": [
        "Certificate signed by unknown authority",
        "Problem",
        "Solution"
      ],
      "title": "Certificate signed by unknown authority",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Troubleshooting"
      ],
      "external_id": "cfdbc8508c0380d97870bcd0d88663219654964e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/troubleshooting/certificate-signed-unknown-authority/",
      "published_at": "2022-01-08T09:01:11Z",
      "updated_at": "2021-10-30T20:45:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Problem You are getting the message x509: certificate signed by unknown authority. Solution You need to ensure your signed certificates are properly configured. To do so, use the following variables in the DaemonSet portion of your manifest to set the .pem file: - name: NRIA_CA_BUNDLE_DIR value: YOUR_CA_BUNDLE_DIR - name: NRIA_CA_BUNDLE_FILE value: YOUR_CA_BUNDLE_NAME Copy YAML key path: spec.template.spec.containers.name.env",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 105.96205,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Kubernetes</em> <em>integration</em>"
      },
      "id": "617daee5e7b9d22f47c04f7f"
    }
  ],
  "/docs/kubernetes-pixie/kubernetes-integration/troubleshooting/kubernetes-integration-troubleshooting-error-messages": [
    {
      "sections": [
        "Install the Kubernetes integration manually using Helm",
        "Compatibility and requirements",
        "Install Kubernetes integration with Helm",
        "Install with Helm 3 and nri-bundle (recommended)",
        "Installing and configuring nri-bundle with Helm",
        "Install with Helm 2 and nri-bundle (legacy)",
        "Installation instructions for Helm 2",
        "Important",
        "Tip",
        "Helm configuration options",
        "Upgrade using Helm",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data",
        "Reduce data ingest",
        "Prometheus OpenMetrics Integration",
        "New Relic Logging",
        "New Relic Pixie Integration",
        "Uninstall Kubernetes integration"
      ],
      "title": "Install the Kubernetes integration manually using Helm",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "29d42af98d41e7e4e7be86f0150254e132cb6b6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/install-kubernetes-integration-using-helm/",
      "published_at": "2022-01-08T09:49:44Z",
      "updated_at": "2022-01-08T09:49:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Helm is a package manager on top of Kubernetes. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in Kubernetes. To install the integration using Helm, we recommend our Kubernetes automated installer, which will prompt for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our integration as plain manifests rather than a Helm release. See Kubernetes integration: install and configure for more details about how to use our automated installer. Start the installer This page describes in more depth how to install and configure the New Relic integration without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the Kubernetes integration. Our charts are also compatible with Helm 2, but support for it might be removed in the future. To install the Kubernetes integration using Helm, you will need your New Relic account license key and your Kubernetes cluster's name: Find and copy your New Relic license key. Find the name of your cluster with this command: kubectl config current-context Copy Note these values somewhere safe, as you will need them later during the installation process. Install Kubernetes integration with Helm Install with Helm 3 and nri-bundle (recommended) New Relic has several charts for the different components which offer different features for the platform: newrelic-infrastructure: Contains the main Kubernetes integration and the infrastructure agent. This is the core component for the New Relic Kubernetes experience, responsible of reporting most of the data that is surfaced in dashboard and the Kubernetes Cluster Explorer. newrelic-logging: Provides a DaemonSet with New Relic's Fluent Bit output plugin to easily forward your logs to New Relic. nri-kube-events: Collects and reports cluster events (such as kubectl get events) to New Relic. nri-prometheus: New Relic's Prometheus OpenMetrics Integration, automatically scrapes Prometheus endpoints present in the cluser and reports metrics to New Relic. nri-metadata-injection: Sets up a minimal MutatingAdmissionWebhook that injects a couple of environment to containers. These contain metadata about the cluster and New Relic installation and will be later picked up by applications instrumented using APM, allowing to correlate APM and infrastructure data. nri-statsd: New Relic StatsD integration. Additionally, New Relic provides nri-bundle, a chart which pulls a selectable set of the charts mentioned above. nri-bundle can also install Kube State Metrics and Pixie for you if needed. While it is possible to install those charts separately, we strongly recommend using the nri-bundle chart for Kubernetes deployments, as it ensures that values across all the charts are consistent and provides full control over which components are installed, as well as the possibility to configure all of them as Helm dependencies. This is the same chart that is used and referenced by our automated installer. Installing and configuring nri-bundle with Helm Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Please notice and adjust the following flags: global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY: Must be set to a valid License Key for your account. global.cluster=K8S_CLUSTER_NAME: Is used to identify the cluster in the New Relic UI, so should be a descriptive value not used by any other Kubernetes cluster configured in your New Relic account. ksm.enabled=true: Setting this to true will automatically install Kube State Metrics (KSM) for you, which is required for our integration to run. You can set this to false if KSM is already present in your cluster, even if it is on a different namespace. newrelic-infrastructure.privileged=true: Can be set to false to install a trimmed down version of our integration that does not require extra privileges, such as hostPath mounts or running containers as root. Please note that this will disable detailed process collection from the host. For performance reasons, our logging solution still requires hostPath mounts, regardless of the value of this flag. If this is not allowedin your cluster, you will need to disable the logging solution by specifying logging.enabled=false. prometheus.enabled=true: Will deploy our Prometheus OpenMetrics integration, which automatically collects data from prometheus endpoints present in the cluster. webhook.enabled=true: Will install our minimal webhook, which adds environment variables that, in turn, allows linking applications instrumented with APM with infrastructure data. Our chart has a comprehensive set of flags and tunables that can be edited to configure our solution to your particular needs. For a full list of all the flags that can be configured, please check the chart's README and the default values.yaml file. Install the Kubernetes integration by running the customized command without --debug and --dry-run: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl -n newrelic get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node for newrelic-infrastructure, as well as one Deployment and one pod for any other component that you have enabled. Install with Helm 2 and nri-bundle (legacy) Installation instructions for Helm 2 Important Helm 2 has been deprecated and New Relic does not recommend using it for deployments. Instructions in this section are provided for legacy systems only. To install using Helm 2: Ensure that Helm 2, including their cluster-side components (such as Tiller) are properly installed and configured. Please check the official documentation for more details. Set the cluster where you want to install the agent: kubectl config set-cluster DESIRED_CLUSTER Copy To see the available clusters, run kubectl config get-clusters Make sure that kube-state-metrics is installed on your machine: kubectl get deployment --all-namespaces | grep kube-state-metrics Copy If it's not installed, follow the instructions in the kube-state-metrics GitHub repo to install it. Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. This step uses the --dry-run and --debug switches and therefore the agent is not installed. helm install newrelic/newrelic-infrastructure \\ --dry-run \\ --debug \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Install the New Relic Kubernetes integration: helm install newrelic/newrelic-infrastructure \\ --set licenseKey=your_new_relic_license_key \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Tip Note that the --dry-run and --debug switches have been removed. Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node. Helm configuration options When you install or upgrade the Kubernetes integration with Helm using the command line, you can pass your configuration variables with the --set flag. helm install newrelic/newrelic-infrastructure \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=YOUR_CLUSTER_NAME Copy A full list of the configuration parameters can be found in the newrelic-infrastructure chart README Upgrade using Helm To update your Kubernetes integration installed via Helm: Update the local chart repository: helm repo update Copy Update the release by running again the appropriate helm upgrade --install ... command in the section above. Monitor services running on Kubernetes After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages. Reduce data ingest Our charts support setting an option to reduce the amount of data ingest at the cost of dropping detailed information. To enable it, set global.lowDataMode=true in the nri-bundle chart. lowDataMode affects three specific components of the nri-bundle chart outlined below. Prometheus OpenMetrics Integration If lowDataMode is enabled, the following metrics are excluded by default as they are already collected and used by the New Relic Kubernetes Integration. - kube_ - container_ - machine_ - cadvisor_ Copy New Relic Logging If lowDataMode is enabled, Labels and Annotations are set to Off in the Filter section of the fluent-bit.conf file. This means that this detail will be dropped from the container log files which reduces the overall data ingest into New Relic. The following fields are retained: Allowlist_key container_name Allowlist_key namespace_name Allowlist_key pod_name Allowlist_key stream Allowlist_key log Copy Low Data Mode Log Example Complete Log Record [ { \"cluster_name\": \"api-test\", \"kubernetes\": { \"annotations\": { \"kubernetes.io/psp\": \"eks.privileged\" }, \"container_hash\": \"fryckbos/test@sha256:5b098eaf3c7d5b3585eb10cebee63665b6208bea31ef31a3f0856c5ffdda644b\", \"container_image\": \"fryckbos/test:latest\", \"container_name\": \"newrelic-logging\", \"docker_id\": \"134e1daf63761baa15e035b08b7aea04518a0f0e50af4215131a50c6a379a072\", \"host\": \"ip-192-168-17-123.ec2.internal\", \"labels\": { \"app\": \"newrelic-logging\", \"app.kubernetes.io/name\": \"newrelic-logging\", \"controller-revision-hash\": \"84db95db86\", \"pod-template-generation\": \"1\", \"release\": \"nri-bundle\" }, \"namespace_name\": \"nrlogs\", \"pod_id\": \"54556e3e-719c-46b5-af69-020b75d69bf1\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\" }, \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"plugin\": { \"source\": \"kubernetes\", \"type\": \"fluent-bit\", \"version\": \"1.8.1\" }, \"stream\": \"stderr\", \"time\": \"2021-09-14T12:30:49.138824971Z\", \"timestamp\": 1631622649138 } ] Copy Log Record after enabling lowDataMode. [ { \"cluster_name\": \"api-test\", \"container_name\": \"newrelic-logging\", \"namespace_name\": \"nrlogs\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\", \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"stream\": \"stderr\", \"timestamp\": 1631622649138 } ] Copy New Relic Pixie Integration If lowDataMode is enabled, the newrelic-pixie integration performs heavier sampling on Pixie spans and reduces the collection interval from 10 seconds to 15 seconds. lowDataMode settings: HTTP_SPAN_LIMIT: 750 DB_SPAN_LIMIT: 250 COLLECT_INTERVAL_SEC: 15 Copy The default settings for these parameters and others can be found in the newrelic-pixie-integration Github repo. Uninstall Kubernetes integration To uninstall the Kubernetes integration using Helm, run the following command. Replace some-manifest-file.yml with your specific filename. helm uninstall newrelic -n newrelic if kubectl then : kubectl delete -f some-manifest-file.yml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 160.49757,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "sections": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "Helm is a package manager on top of <em>Kubernetes</em>. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in <em>Kubernetes</em>. To install the <em>integration</em> using Helm, we recommend our <em>Kubernetes</em> automated installer, which will prompt"
      },
      "id": "617d5841196a67bb40f7c1de"
    },
    {
      "sections": [
        "Configure Kubernetes with a proxy",
        "Install the infrastructure agent with a proxy",
        "Set logging with a proxy",
        "Install Kubernetes events with a proxy",
        "Install the Prometheus OpenMetrics integration with a proxy",
        "Set the synthetics minion to use a proxy"
      ],
      "title": "Configure Kubernetes with a proxy",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "9d7794244b451d878d00db0f84c4709e43ce138c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/configure-kubernetes-proxy/",
      "published_at": "2022-01-08T09:49:01Z",
      "updated_at": "2021-12-25T07:57:45Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are running our Kubernetes integration with a proxy, you need to configure each component (the infrastructure agent, logging, Kubernetes events, etc.) to correctly work with your proxy. Install the infrastructure agent with a proxy There are three options to install the infrastructure agent with a proxy: Installing with chart newrelic-infrastructure: The proxy can be configured setting the configuration option config.proxy as described in the values.yaml of the chart. The whole config object is used to generate a configMap that is mounted as the file /etc/newrelic-infra.yml used to configure the agent. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option newrelic-infrastructure.config.proxy. The configuration option is passed down to the dependency newrelic-infrastructure modifying the values.yaml of nri-bundle: newrelic-infrastructure.config.proxy: https://user:password@hostname:port Copy Installing with manifest: When installing through the manifest, the proxy can be configured adding the NRIA_PROXY environment variable to the environment variable section of the DaemonSet configuration. The environment variable NRIA_VERBOSE in the DaemonSet YAML can be used as an example of the environment variable configuration. Set logging with a proxy If you’re using log forwarding for the Kubernetes integration, the plugin automatically detects the HTTP_PROXY and HTTPS_PROXY environment variables, and automatically uses them to set up the proxy configuration. For more information, see how to customize the proxy, or bypass it. Install Kubernetes events with a proxy There are three options to install the Kubernetes events integration with a proxy: Installing with chart nri-kube-events: The proxy can be configured setting the configuration option proxy in the values.yaml file. If set, the variable NRIA_PROXY will be added into the deployment.yaml template. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option nri-kube-events.proxy. The configuration option is passed down to the dependency nri-kube-events modifying the values.yaml of nri-bundle: nri-kube-events.proxy: https://user:password@hostname:port Copy Installing with manifest: When installing through the manifest, to configure the proxy set the NRIA_PROXY in the environment variable section of the infra container. Check the deployment template of the chart as an example of environment variable configuration. Install the Prometheus OpenMetrics integration with a proxy There are three options to install the Prometheus OpenMetric integration with a proxy: Installing with charts nri-prometheus: The proxy can be configured setting the configuration option config.emitter_proxy in the values.yaml. The config object is used to generate a configMap that mounted into the deployment under /etc/nri-prometheus/. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option nri-prometheus.config.emitter_proxy. The configuration option is passed down to the dependency nri-prometheus modifying the values.yaml of nri-bundle: nri-prometheus.config.emitter_proxy: \"http://localhost:8888 Copy Installing with the manifest: Configure the proxy uncommenting the emitter_proxy option directly in the manifest: # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" Copy Set the synthetics minion to use a proxy Check this README on how to set the proxy for the Synthetics minions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 106.280136,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure <em>Kubernetes</em> with a proxy",
        "sections": "Configure <em>Kubernetes</em> with a proxy",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "If you are running our <em>Kubernetes</em> <em>integration</em> with a proxy, you need to configure each component (the infrastructure agent, logging, <em>Kubernetes</em> events, etc.) to correctly work with your proxy. Install the infrastructure agent with a proxy There are three options to install the infrastructure agent"
      },
      "id": "617d584264441f2d01fbc6d7"
    },
    {
      "sections": [
        "Get logs and version",
        "Get verbose logs for installations using a manifest file",
        "Caution",
        "Get verbose logs for installations using Helm",
        "Get the infrastructure agent version",
        "Get kube-state-metrics version",
        "Get logs from pods connecting to kube-state-metrics",
        "Retrieve kube-state-metrics service configuration",
        "Get logs from pod running on a master node"
      ],
      "title": "Get logs and version",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Troubleshooting"
      ],
      "external_id": "35fd6fb08a49327a4d2ce5797511e55e39793c67",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/troubleshooting/get-logs-version/",
      "published_at": "2022-01-08T09:51:19Z",
      "updated_at": "2021-10-30T20:45:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To generate verbose logs and get version and configuration information, follow these procedures. Get verbose logs for installations using a manifest file For the Kubernetes integration, the infrastructure agent adds a log entry only in the event of an error. Most common errors are displayed in the standard (non-verbose) logs. If you are doing a more in-depth investigation on your own or with New Relic Support, you can enable verbose mode. Caution Verbose mode significantly increases the amount of info sent to log files. Temporarily enable this mode only for troubleshooting purposes, and reset the log level when finished. To get verbose logging details for an integration using a manifest file: Enable verbose logging: In the deployment file, set the value of NRIA_VERBOSE to 1. Apply the modified configuration by running: kubectl apply -f your_newrelic_k8s.yaml Copy Leave on verbose mode for a few minutes, or until you feel enough activity has occurred. Disable verbose mode: Set the NRIA_VERBOSE value back to 0. Apply the restored configuration by running: kubectl apply -f your_newrelic_k8s.yaml Copy Get a list of nodes in the environment: kubectl get nodes --all-namespaces Copy Get a list of infrastructure and kube-state-metrics pods: kubectl get pods --all-namespaces -o wide | egrep 'newrelic|kube-state-metrics' Copy Get logs from the pods connecting to kube-state-metrics. Retrieve kube-state-metrics service configuration. Get verbose logs for installations using Helm For the Kubernetes integration, the infrastructure agent adds a log entry only in the event of an error. Most common errors are displayed in the standard (non-verbose) logs. If you're doing a more in-depth investigation on your own or with New Relic Support, you can enable verbose mode. Caution Verbose mode significantly increases the amount of info sent to log files. Temporarily enable this mode only for troubleshooting purposes, and reset the log level when finished. To get verbose logging details for an integration using Helm: Enable verbose logging: helm upgrade -n <namespace> --reuse-values newrelic-bundle --set newrelic-infrastructure.verboseLog=true newrelic/nri-bundle Copy Leave on verbose mode for a few minutes, or until enough activity has occurred. When you have the information you need, disable verbose logging: helm upgrade --reuse-values newrelic-bundle --set newrelic-infrastructure.verboseLog=false newrelic/nri-bundle Copy Follow the steps to restore your configuration from step 5 in the section, Get verbose logs for installations using a manifest file. Get the infrastructure agent version For the Kubernetes integration, the infrastructure agent is distributed as a Docker image that contains the infrastructure agent and the Kubernetes integration. The Docker image is tagged with a version, and the infrastructure agent also has its own version. When the agent is successfully sending information to New Relic, you can retrieve the versions of the infrastructure agent for Kubernetes (the Docker image) you are running in your clusters by using the following NRQL query: FROM K8sContainerSample SELECT uniqueCount(entityId) WHERE containerName = 'newrelic-infra' facet clusterName, containerImage Copy If the agent is not reporting any data: Get the version(s) of the New Relic integration for Kubernetes that you are running in a cluster using kubectl: kubectl get pods --all-namespaces -l name=newrelic-infra -o jsonpath=\"{.items..spec..containers..image}\" Copy Look for output similar to this: newrelic/infrastructure-k8s:1.0.0 Copy Get kube-state-metrics version To retrieve the version of kube-state-metrics running on your clusters, run the following NRQL query: FROM K8sContainerSample SELECT uniqueCount(entityId) WHERE containerName LIKE '%kube-state-metrics%' facet clusterName, containerImage Copy Get logs from pods connecting to kube-state-metrics To get the logs from pods connecting to kube-state-metrics: Get the nodes that kube-state-metrics is running on: kubectl get pods --all-namespaces -o wide | grep kube-state-metrics Copy Look for output similar to this: kube-system kube-state-metrics-5c6f5cb9b5-pclhh 2/2 Running 4 4d 172.17.0.3 minikube Copy Get the New Relic pods that are running on the same node as kube-state-metrics: kubectl describe node minikube | grep newrelic-infra Copy Look for output similar to this: default newrelic-infra-5wcv6 100m (5%) 0 (0%) 100Mi (5%) 100Mi (5%) Copy Retrieve the logs for the nodes by running: kubectl logs newrelic-infra-5wcv6 Copy Retrieve kube-state-metrics service configuration To retrieve the configuration: Run: kubectl get pods --all-namespaces | grep \"kube-state-metrics\" Copy Look for a response similar to this: kube-system kube-state-metrics-5c6f5cb9b5-5wf9m 2/2 Running 8 6d Copy Review the namespace in the first column. kubectl describe service kube-state-metrics -n <namespace> Copy Get logs from pod running on a master node To get the logs from a pod running on a master node: Get the nodes that are labelled as master: kubectl get nodes -l node-role.kubernetes.io/master=\"\" Copy Or, kubectl get nodes -l kubernetes.io/role=\"master\" Copy Look for output similar to this: NAME STATUS ROLES AGE VERSION ip-10-42-24-4.ec2.internal Ready master 42d v1.14.8 Copy Get the New Relic pods that are running on one of the nodes returned in the previous step: kubectl get pods --field-selector spec.nodeName=ip-10-42-24-4.ec2.internal -l name=newrelic-infra --all-namespaces Copy Look for output similar to this: newrelic-infra-whvzt Copy Retrieve the logs for the nodes by running: kubectl logs newrelic-infra-whvzt Copy For troubleshooting help, see Not seeing data or Error messages.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 105.96205,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "To generate verbose logs and get version and configuration information, follow these procedures. Get verbose logs for installations using a manifest file For the <em>Kubernetes</em> <em>integration</em>, the infrastructure agent adds a log entry only in the event of an error. Most common errors are displayed"
      },
      "id": "617daee5e7b9d2337dc03982"
    }
  ],
  "/docs/kubernetes-pixie/kubernetes-integration/troubleshooting/kubernetes-integration-troubleshooting-missing-nodes": [
    {
      "sections": [
        "Install the Kubernetes integration manually using Helm",
        "Compatibility and requirements",
        "Install Kubernetes integration with Helm",
        "Install with Helm 3 and nri-bundle (recommended)",
        "Installing and configuring nri-bundle with Helm",
        "Install with Helm 2 and nri-bundle (legacy)",
        "Installation instructions for Helm 2",
        "Important",
        "Tip",
        "Helm configuration options",
        "Upgrade using Helm",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data",
        "Reduce data ingest",
        "Prometheus OpenMetrics Integration",
        "New Relic Logging",
        "New Relic Pixie Integration",
        "Uninstall Kubernetes integration"
      ],
      "title": "Install the Kubernetes integration manually using Helm",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "29d42af98d41e7e4e7be86f0150254e132cb6b6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/install-kubernetes-integration-using-helm/",
      "published_at": "2022-01-08T09:49:44Z",
      "updated_at": "2022-01-08T09:49:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Helm is a package manager on top of Kubernetes. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in Kubernetes. To install the integration using Helm, we recommend our Kubernetes automated installer, which will prompt for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our integration as plain manifests rather than a Helm release. See Kubernetes integration: install and configure for more details about how to use our automated installer. Start the installer This page describes in more depth how to install and configure the New Relic integration without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the Kubernetes integration. Our charts are also compatible with Helm 2, but support for it might be removed in the future. To install the Kubernetes integration using Helm, you will need your New Relic account license key and your Kubernetes cluster's name: Find and copy your New Relic license key. Find the name of your cluster with this command: kubectl config current-context Copy Note these values somewhere safe, as you will need them later during the installation process. Install Kubernetes integration with Helm Install with Helm 3 and nri-bundle (recommended) New Relic has several charts for the different components which offer different features for the platform: newrelic-infrastructure: Contains the main Kubernetes integration and the infrastructure agent. This is the core component for the New Relic Kubernetes experience, responsible of reporting most of the data that is surfaced in dashboard and the Kubernetes Cluster Explorer. newrelic-logging: Provides a DaemonSet with New Relic's Fluent Bit output plugin to easily forward your logs to New Relic. nri-kube-events: Collects and reports cluster events (such as kubectl get events) to New Relic. nri-prometheus: New Relic's Prometheus OpenMetrics Integration, automatically scrapes Prometheus endpoints present in the cluser and reports metrics to New Relic. nri-metadata-injection: Sets up a minimal MutatingAdmissionWebhook that injects a couple of environment to containers. These contain metadata about the cluster and New Relic installation and will be later picked up by applications instrumented using APM, allowing to correlate APM and infrastructure data. nri-statsd: New Relic StatsD integration. Additionally, New Relic provides nri-bundle, a chart which pulls a selectable set of the charts mentioned above. nri-bundle can also install Kube State Metrics and Pixie for you if needed. While it is possible to install those charts separately, we strongly recommend using the nri-bundle chart for Kubernetes deployments, as it ensures that values across all the charts are consistent and provides full control over which components are installed, as well as the possibility to configure all of them as Helm dependencies. This is the same chart that is used and referenced by our automated installer. Installing and configuring nri-bundle with Helm Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Please notice and adjust the following flags: global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY: Must be set to a valid License Key for your account. global.cluster=K8S_CLUSTER_NAME: Is used to identify the cluster in the New Relic UI, so should be a descriptive value not used by any other Kubernetes cluster configured in your New Relic account. ksm.enabled=true: Setting this to true will automatically install Kube State Metrics (KSM) for you, which is required for our integration to run. You can set this to false if KSM is already present in your cluster, even if it is on a different namespace. newrelic-infrastructure.privileged=true: Can be set to false to install a trimmed down version of our integration that does not require extra privileges, such as hostPath mounts or running containers as root. Please note that this will disable detailed process collection from the host. For performance reasons, our logging solution still requires hostPath mounts, regardless of the value of this flag. If this is not allowedin your cluster, you will need to disable the logging solution by specifying logging.enabled=false. prometheus.enabled=true: Will deploy our Prometheus OpenMetrics integration, which automatically collects data from prometheus endpoints present in the cluster. webhook.enabled=true: Will install our minimal webhook, which adds environment variables that, in turn, allows linking applications instrumented with APM with infrastructure data. Our chart has a comprehensive set of flags and tunables that can be edited to configure our solution to your particular needs. For a full list of all the flags that can be configured, please check the chart's README and the default values.yaml file. Install the Kubernetes integration by running the customized command without --debug and --dry-run: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl -n newrelic get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node for newrelic-infrastructure, as well as one Deployment and one pod for any other component that you have enabled. Install with Helm 2 and nri-bundle (legacy) Installation instructions for Helm 2 Important Helm 2 has been deprecated and New Relic does not recommend using it for deployments. Instructions in this section are provided for legacy systems only. To install using Helm 2: Ensure that Helm 2, including their cluster-side components (such as Tiller) are properly installed and configured. Please check the official documentation for more details. Set the cluster where you want to install the agent: kubectl config set-cluster DESIRED_CLUSTER Copy To see the available clusters, run kubectl config get-clusters Make sure that kube-state-metrics is installed on your machine: kubectl get deployment --all-namespaces | grep kube-state-metrics Copy If it's not installed, follow the instructions in the kube-state-metrics GitHub repo to install it. Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. This step uses the --dry-run and --debug switches and therefore the agent is not installed. helm install newrelic/newrelic-infrastructure \\ --dry-run \\ --debug \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Install the New Relic Kubernetes integration: helm install newrelic/newrelic-infrastructure \\ --set licenseKey=your_new_relic_license_key \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Tip Note that the --dry-run and --debug switches have been removed. Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node. Helm configuration options When you install or upgrade the Kubernetes integration with Helm using the command line, you can pass your configuration variables with the --set flag. helm install newrelic/newrelic-infrastructure \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=YOUR_CLUSTER_NAME Copy A full list of the configuration parameters can be found in the newrelic-infrastructure chart README Upgrade using Helm To update your Kubernetes integration installed via Helm: Update the local chart repository: helm repo update Copy Update the release by running again the appropriate helm upgrade --install ... command in the section above. Monitor services running on Kubernetes After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages. Reduce data ingest Our charts support setting an option to reduce the amount of data ingest at the cost of dropping detailed information. To enable it, set global.lowDataMode=true in the nri-bundle chart. lowDataMode affects three specific components of the nri-bundle chart outlined below. Prometheus OpenMetrics Integration If lowDataMode is enabled, the following metrics are excluded by default as they are already collected and used by the New Relic Kubernetes Integration. - kube_ - container_ - machine_ - cadvisor_ Copy New Relic Logging If lowDataMode is enabled, Labels and Annotations are set to Off in the Filter section of the fluent-bit.conf file. This means that this detail will be dropped from the container log files which reduces the overall data ingest into New Relic. The following fields are retained: Allowlist_key container_name Allowlist_key namespace_name Allowlist_key pod_name Allowlist_key stream Allowlist_key log Copy Low Data Mode Log Example Complete Log Record [ { \"cluster_name\": \"api-test\", \"kubernetes\": { \"annotations\": { \"kubernetes.io/psp\": \"eks.privileged\" }, \"container_hash\": \"fryckbos/test@sha256:5b098eaf3c7d5b3585eb10cebee63665b6208bea31ef31a3f0856c5ffdda644b\", \"container_image\": \"fryckbos/test:latest\", \"container_name\": \"newrelic-logging\", \"docker_id\": \"134e1daf63761baa15e035b08b7aea04518a0f0e50af4215131a50c6a379a072\", \"host\": \"ip-192-168-17-123.ec2.internal\", \"labels\": { \"app\": \"newrelic-logging\", \"app.kubernetes.io/name\": \"newrelic-logging\", \"controller-revision-hash\": \"84db95db86\", \"pod-template-generation\": \"1\", \"release\": \"nri-bundle\" }, \"namespace_name\": \"nrlogs\", \"pod_id\": \"54556e3e-719c-46b5-af69-020b75d69bf1\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\" }, \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"plugin\": { \"source\": \"kubernetes\", \"type\": \"fluent-bit\", \"version\": \"1.8.1\" }, \"stream\": \"stderr\", \"time\": \"2021-09-14T12:30:49.138824971Z\", \"timestamp\": 1631622649138 } ] Copy Log Record after enabling lowDataMode. [ { \"cluster_name\": \"api-test\", \"container_name\": \"newrelic-logging\", \"namespace_name\": \"nrlogs\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\", \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"stream\": \"stderr\", \"timestamp\": 1631622649138 } ] Copy New Relic Pixie Integration If lowDataMode is enabled, the newrelic-pixie integration performs heavier sampling on Pixie spans and reduces the collection interval from 10 seconds to 15 seconds. lowDataMode settings: HTTP_SPAN_LIMIT: 750 DB_SPAN_LIMIT: 250 COLLECT_INTERVAL_SEC: 15 Copy The default settings for these parameters and others can be found in the newrelic-pixie-integration Github repo. Uninstall Kubernetes integration To uninstall the Kubernetes integration using Helm, run the following command. Replace some-manifest-file.yml with your specific filename. helm uninstall newrelic -n newrelic if kubectl then : kubectl delete -f some-manifest-file.yml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 160.49747,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "sections": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "Helm is a package manager on top of <em>Kubernetes</em>. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in <em>Kubernetes</em>. To install the <em>integration</em> using Helm, we recommend our <em>Kubernetes</em> automated installer, which will prompt"
      },
      "id": "617d5841196a67bb40f7c1de"
    },
    {
      "sections": [
        "Configure Kubernetes with a proxy",
        "Install the infrastructure agent with a proxy",
        "Set logging with a proxy",
        "Install Kubernetes events with a proxy",
        "Install the Prometheus OpenMetrics integration with a proxy",
        "Set the synthetics minion to use a proxy"
      ],
      "title": "Configure Kubernetes with a proxy",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "9d7794244b451d878d00db0f84c4709e43ce138c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/configure-kubernetes-proxy/",
      "published_at": "2022-01-08T09:49:01Z",
      "updated_at": "2021-12-25T07:57:45Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are running our Kubernetes integration with a proxy, you need to configure each component (the infrastructure agent, logging, Kubernetes events, etc.) to correctly work with your proxy. Install the infrastructure agent with a proxy There are three options to install the infrastructure agent with a proxy: Installing with chart newrelic-infrastructure: The proxy can be configured setting the configuration option config.proxy as described in the values.yaml of the chart. The whole config object is used to generate a configMap that is mounted as the file /etc/newrelic-infra.yml used to configure the agent. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option newrelic-infrastructure.config.proxy. The configuration option is passed down to the dependency newrelic-infrastructure modifying the values.yaml of nri-bundle: newrelic-infrastructure.config.proxy: https://user:password@hostname:port Copy Installing with manifest: When installing through the manifest, the proxy can be configured adding the NRIA_PROXY environment variable to the environment variable section of the DaemonSet configuration. The environment variable NRIA_VERBOSE in the DaemonSet YAML can be used as an example of the environment variable configuration. Set logging with a proxy If you’re using log forwarding for the Kubernetes integration, the plugin automatically detects the HTTP_PROXY and HTTPS_PROXY environment variables, and automatically uses them to set up the proxy configuration. For more information, see how to customize the proxy, or bypass it. Install Kubernetes events with a proxy There are three options to install the Kubernetes events integration with a proxy: Installing with chart nri-kube-events: The proxy can be configured setting the configuration option proxy in the values.yaml file. If set, the variable NRIA_PROXY will be added into the deployment.yaml template. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option nri-kube-events.proxy. The configuration option is passed down to the dependency nri-kube-events modifying the values.yaml of nri-bundle: nri-kube-events.proxy: https://user:password@hostname:port Copy Installing with manifest: When installing through the manifest, to configure the proxy set the NRIA_PROXY in the environment variable section of the infra container. Check the deployment template of the chart as an example of environment variable configuration. Install the Prometheus OpenMetrics integration with a proxy There are three options to install the Prometheus OpenMetric integration with a proxy: Installing with charts nri-prometheus: The proxy can be configured setting the configuration option config.emitter_proxy in the values.yaml. The config object is used to generate a configMap that mounted into the deployment under /etc/nri-prometheus/. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option nri-prometheus.config.emitter_proxy. The configuration option is passed down to the dependency nri-prometheus modifying the values.yaml of nri-bundle: nri-prometheus.config.emitter_proxy: \"http://localhost:8888 Copy Installing with the manifest: Configure the proxy uncommenting the emitter_proxy option directly in the manifest: # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" Copy Set the synthetics minion to use a proxy Check this README on how to set the proxy for the Synthetics minions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 106.28012,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure <em>Kubernetes</em> with a proxy",
        "sections": "Configure <em>Kubernetes</em> with a proxy",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "If you are running our <em>Kubernetes</em> <em>integration</em> with a proxy, you need to configure each component (the infrastructure agent, logging, <em>Kubernetes</em> events, etc.) to correctly work with your proxy. Install the infrastructure agent with a proxy There are three options to install the infrastructure agent"
      },
      "id": "617d584264441f2d01fbc6d7"
    },
    {
      "sections": [
        "Get logs and version",
        "Get verbose logs for installations using a manifest file",
        "Caution",
        "Get verbose logs for installations using Helm",
        "Get the infrastructure agent version",
        "Get kube-state-metrics version",
        "Get logs from pods connecting to kube-state-metrics",
        "Retrieve kube-state-metrics service configuration",
        "Get logs from pod running on a master node"
      ],
      "title": "Get logs and version",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Troubleshooting"
      ],
      "external_id": "35fd6fb08a49327a4d2ce5797511e55e39793c67",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/troubleshooting/get-logs-version/",
      "published_at": "2022-01-08T09:51:19Z",
      "updated_at": "2021-10-30T20:45:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To generate verbose logs and get version and configuration information, follow these procedures. Get verbose logs for installations using a manifest file For the Kubernetes integration, the infrastructure agent adds a log entry only in the event of an error. Most common errors are displayed in the standard (non-verbose) logs. If you are doing a more in-depth investigation on your own or with New Relic Support, you can enable verbose mode. Caution Verbose mode significantly increases the amount of info sent to log files. Temporarily enable this mode only for troubleshooting purposes, and reset the log level when finished. To get verbose logging details for an integration using a manifest file: Enable verbose logging: In the deployment file, set the value of NRIA_VERBOSE to 1. Apply the modified configuration by running: kubectl apply -f your_newrelic_k8s.yaml Copy Leave on verbose mode for a few minutes, or until you feel enough activity has occurred. Disable verbose mode: Set the NRIA_VERBOSE value back to 0. Apply the restored configuration by running: kubectl apply -f your_newrelic_k8s.yaml Copy Get a list of nodes in the environment: kubectl get nodes --all-namespaces Copy Get a list of infrastructure and kube-state-metrics pods: kubectl get pods --all-namespaces -o wide | egrep 'newrelic|kube-state-metrics' Copy Get logs from the pods connecting to kube-state-metrics. Retrieve kube-state-metrics service configuration. Get verbose logs for installations using Helm For the Kubernetes integration, the infrastructure agent adds a log entry only in the event of an error. Most common errors are displayed in the standard (non-verbose) logs. If you're doing a more in-depth investigation on your own or with New Relic Support, you can enable verbose mode. Caution Verbose mode significantly increases the amount of info sent to log files. Temporarily enable this mode only for troubleshooting purposes, and reset the log level when finished. To get verbose logging details for an integration using Helm: Enable verbose logging: helm upgrade -n <namespace> --reuse-values newrelic-bundle --set newrelic-infrastructure.verboseLog=true newrelic/nri-bundle Copy Leave on verbose mode for a few minutes, or until enough activity has occurred. When you have the information you need, disable verbose logging: helm upgrade --reuse-values newrelic-bundle --set newrelic-infrastructure.verboseLog=false newrelic/nri-bundle Copy Follow the steps to restore your configuration from step 5 in the section, Get verbose logs for installations using a manifest file. Get the infrastructure agent version For the Kubernetes integration, the infrastructure agent is distributed as a Docker image that contains the infrastructure agent and the Kubernetes integration. The Docker image is tagged with a version, and the infrastructure agent also has its own version. When the agent is successfully sending information to New Relic, you can retrieve the versions of the infrastructure agent for Kubernetes (the Docker image) you are running in your clusters by using the following NRQL query: FROM K8sContainerSample SELECT uniqueCount(entityId) WHERE containerName = 'newrelic-infra' facet clusterName, containerImage Copy If the agent is not reporting any data: Get the version(s) of the New Relic integration for Kubernetes that you are running in a cluster using kubectl: kubectl get pods --all-namespaces -l name=newrelic-infra -o jsonpath=\"{.items..spec..containers..image}\" Copy Look for output similar to this: newrelic/infrastructure-k8s:1.0.0 Copy Get kube-state-metrics version To retrieve the version of kube-state-metrics running on your clusters, run the following NRQL query: FROM K8sContainerSample SELECT uniqueCount(entityId) WHERE containerName LIKE '%kube-state-metrics%' facet clusterName, containerImage Copy Get logs from pods connecting to kube-state-metrics To get the logs from pods connecting to kube-state-metrics: Get the nodes that kube-state-metrics is running on: kubectl get pods --all-namespaces -o wide | grep kube-state-metrics Copy Look for output similar to this: kube-system kube-state-metrics-5c6f5cb9b5-pclhh 2/2 Running 4 4d 172.17.0.3 minikube Copy Get the New Relic pods that are running on the same node as kube-state-metrics: kubectl describe node minikube | grep newrelic-infra Copy Look for output similar to this: default newrelic-infra-5wcv6 100m (5%) 0 (0%) 100Mi (5%) 100Mi (5%) Copy Retrieve the logs for the nodes by running: kubectl logs newrelic-infra-5wcv6 Copy Retrieve kube-state-metrics service configuration To retrieve the configuration: Run: kubectl get pods --all-namespaces | grep \"kube-state-metrics\" Copy Look for a response similar to this: kube-system kube-state-metrics-5c6f5cb9b5-5wf9m 2/2 Running 8 6d Copy Review the namespace in the first column. kubectl describe service kube-state-metrics -n <namespace> Copy Get logs from pod running on a master node To get the logs from a pod running on a master node: Get the nodes that are labelled as master: kubectl get nodes -l node-role.kubernetes.io/master=\"\" Copy Or, kubectl get nodes -l kubernetes.io/role=\"master\" Copy Look for output similar to this: NAME STATUS ROLES AGE VERSION ip-10-42-24-4.ec2.internal Ready master 42d v1.14.8 Copy Get the New Relic pods that are running on one of the nodes returned in the previous step: kubectl get pods --field-selector spec.nodeName=ip-10-42-24-4.ec2.internal -l name=newrelic-infra --all-namespaces Copy Look for output similar to this: newrelic-infra-whvzt Copy Retrieve the logs for the nodes by running: kubectl logs newrelic-infra-whvzt Copy For troubleshooting help, see Not seeing data or Error messages.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 105.96204,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "To generate verbose logs and get version and configuration information, follow these procedures. Get verbose logs for installations using a manifest file For the <em>Kubernetes</em> <em>integration</em>, the infrastructure agent adds a log entry only in the event of an error. Most common errors are displayed"
      },
      "id": "617daee5e7b9d2337dc03982"
    }
  ],
  "/docs/kubernetes-pixie/kubernetes-integration/troubleshooting/kubernetes-integration-troubleshooting-not-seeing-data": [
    {
      "sections": [
        "Install the Kubernetes integration manually using Helm",
        "Compatibility and requirements",
        "Install Kubernetes integration with Helm",
        "Install with Helm 3 and nri-bundle (recommended)",
        "Installing and configuring nri-bundle with Helm",
        "Install with Helm 2 and nri-bundle (legacy)",
        "Installation instructions for Helm 2",
        "Important",
        "Tip",
        "Helm configuration options",
        "Upgrade using Helm",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data",
        "Reduce data ingest",
        "Prometheus OpenMetrics Integration",
        "New Relic Logging",
        "New Relic Pixie Integration",
        "Uninstall Kubernetes integration"
      ],
      "title": "Install the Kubernetes integration manually using Helm",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "29d42af98d41e7e4e7be86f0150254e132cb6b6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/install-kubernetes-integration-using-helm/",
      "published_at": "2022-01-08T09:49:44Z",
      "updated_at": "2022-01-08T09:49:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Helm is a package manager on top of Kubernetes. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in Kubernetes. To install the integration using Helm, we recommend our Kubernetes automated installer, which will prompt for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our integration as plain manifests rather than a Helm release. See Kubernetes integration: install and configure for more details about how to use our automated installer. Start the installer This page describes in more depth how to install and configure the New Relic integration without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the Kubernetes integration. Our charts are also compatible with Helm 2, but support for it might be removed in the future. To install the Kubernetes integration using Helm, you will need your New Relic account license key and your Kubernetes cluster's name: Find and copy your New Relic license key. Find the name of your cluster with this command: kubectl config current-context Copy Note these values somewhere safe, as you will need them later during the installation process. Install Kubernetes integration with Helm Install with Helm 3 and nri-bundle (recommended) New Relic has several charts for the different components which offer different features for the platform: newrelic-infrastructure: Contains the main Kubernetes integration and the infrastructure agent. This is the core component for the New Relic Kubernetes experience, responsible of reporting most of the data that is surfaced in dashboard and the Kubernetes Cluster Explorer. newrelic-logging: Provides a DaemonSet with New Relic's Fluent Bit output plugin to easily forward your logs to New Relic. nri-kube-events: Collects and reports cluster events (such as kubectl get events) to New Relic. nri-prometheus: New Relic's Prometheus OpenMetrics Integration, automatically scrapes Prometheus endpoints present in the cluser and reports metrics to New Relic. nri-metadata-injection: Sets up a minimal MutatingAdmissionWebhook that injects a couple of environment to containers. These contain metadata about the cluster and New Relic installation and will be later picked up by applications instrumented using APM, allowing to correlate APM and infrastructure data. nri-statsd: New Relic StatsD integration. Additionally, New Relic provides nri-bundle, a chart which pulls a selectable set of the charts mentioned above. nri-bundle can also install Kube State Metrics and Pixie for you if needed. While it is possible to install those charts separately, we strongly recommend using the nri-bundle chart for Kubernetes deployments, as it ensures that values across all the charts are consistent and provides full control over which components are installed, as well as the possibility to configure all of them as Helm dependencies. This is the same chart that is used and referenced by our automated installer. Installing and configuring nri-bundle with Helm Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Please notice and adjust the following flags: global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY: Must be set to a valid License Key for your account. global.cluster=K8S_CLUSTER_NAME: Is used to identify the cluster in the New Relic UI, so should be a descriptive value not used by any other Kubernetes cluster configured in your New Relic account. ksm.enabled=true: Setting this to true will automatically install Kube State Metrics (KSM) for you, which is required for our integration to run. You can set this to false if KSM is already present in your cluster, even if it is on a different namespace. newrelic-infrastructure.privileged=true: Can be set to false to install a trimmed down version of our integration that does not require extra privileges, such as hostPath mounts or running containers as root. Please note that this will disable detailed process collection from the host. For performance reasons, our logging solution still requires hostPath mounts, regardless of the value of this flag. If this is not allowedin your cluster, you will need to disable the logging solution by specifying logging.enabled=false. prometheus.enabled=true: Will deploy our Prometheus OpenMetrics integration, which automatically collects data from prometheus endpoints present in the cluster. webhook.enabled=true: Will install our minimal webhook, which adds environment variables that, in turn, allows linking applications instrumented with APM with infrastructure data. Our chart has a comprehensive set of flags and tunables that can be edited to configure our solution to your particular needs. For a full list of all the flags that can be configured, please check the chart's README and the default values.yaml file. Install the Kubernetes integration by running the customized command without --debug and --dry-run: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl -n newrelic get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node for newrelic-infrastructure, as well as one Deployment and one pod for any other component that you have enabled. Install with Helm 2 and nri-bundle (legacy) Installation instructions for Helm 2 Important Helm 2 has been deprecated and New Relic does not recommend using it for deployments. Instructions in this section are provided for legacy systems only. To install using Helm 2: Ensure that Helm 2, including their cluster-side components (such as Tiller) are properly installed and configured. Please check the official documentation for more details. Set the cluster where you want to install the agent: kubectl config set-cluster DESIRED_CLUSTER Copy To see the available clusters, run kubectl config get-clusters Make sure that kube-state-metrics is installed on your machine: kubectl get deployment --all-namespaces | grep kube-state-metrics Copy If it's not installed, follow the instructions in the kube-state-metrics GitHub repo to install it. Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. This step uses the --dry-run and --debug switches and therefore the agent is not installed. helm install newrelic/newrelic-infrastructure \\ --dry-run \\ --debug \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Install the New Relic Kubernetes integration: helm install newrelic/newrelic-infrastructure \\ --set licenseKey=your_new_relic_license_key \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Tip Note that the --dry-run and --debug switches have been removed. Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node. Helm configuration options When you install or upgrade the Kubernetes integration with Helm using the command line, you can pass your configuration variables with the --set flag. helm install newrelic/newrelic-infrastructure \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=YOUR_CLUSTER_NAME Copy A full list of the configuration parameters can be found in the newrelic-infrastructure chart README Upgrade using Helm To update your Kubernetes integration installed via Helm: Update the local chart repository: helm repo update Copy Update the release by running again the appropriate helm upgrade --install ... command in the section above. Monitor services running on Kubernetes After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages. Reduce data ingest Our charts support setting an option to reduce the amount of data ingest at the cost of dropping detailed information. To enable it, set global.lowDataMode=true in the nri-bundle chart. lowDataMode affects three specific components of the nri-bundle chart outlined below. Prometheus OpenMetrics Integration If lowDataMode is enabled, the following metrics are excluded by default as they are already collected and used by the New Relic Kubernetes Integration. - kube_ - container_ - machine_ - cadvisor_ Copy New Relic Logging If lowDataMode is enabled, Labels and Annotations are set to Off in the Filter section of the fluent-bit.conf file. This means that this detail will be dropped from the container log files which reduces the overall data ingest into New Relic. The following fields are retained: Allowlist_key container_name Allowlist_key namespace_name Allowlist_key pod_name Allowlist_key stream Allowlist_key log Copy Low Data Mode Log Example Complete Log Record [ { \"cluster_name\": \"api-test\", \"kubernetes\": { \"annotations\": { \"kubernetes.io/psp\": \"eks.privileged\" }, \"container_hash\": \"fryckbos/test@sha256:5b098eaf3c7d5b3585eb10cebee63665b6208bea31ef31a3f0856c5ffdda644b\", \"container_image\": \"fryckbos/test:latest\", \"container_name\": \"newrelic-logging\", \"docker_id\": \"134e1daf63761baa15e035b08b7aea04518a0f0e50af4215131a50c6a379a072\", \"host\": \"ip-192-168-17-123.ec2.internal\", \"labels\": { \"app\": \"newrelic-logging\", \"app.kubernetes.io/name\": \"newrelic-logging\", \"controller-revision-hash\": \"84db95db86\", \"pod-template-generation\": \"1\", \"release\": \"nri-bundle\" }, \"namespace_name\": \"nrlogs\", \"pod_id\": \"54556e3e-719c-46b5-af69-020b75d69bf1\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\" }, \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"plugin\": { \"source\": \"kubernetes\", \"type\": \"fluent-bit\", \"version\": \"1.8.1\" }, \"stream\": \"stderr\", \"time\": \"2021-09-14T12:30:49.138824971Z\", \"timestamp\": 1631622649138 } ] Copy Log Record after enabling lowDataMode. [ { \"cluster_name\": \"api-test\", \"container_name\": \"newrelic-logging\", \"namespace_name\": \"nrlogs\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\", \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"stream\": \"stderr\", \"timestamp\": 1631622649138 } ] Copy New Relic Pixie Integration If lowDataMode is enabled, the newrelic-pixie integration performs heavier sampling on Pixie spans and reduces the collection interval from 10 seconds to 15 seconds. lowDataMode settings: HTTP_SPAN_LIMIT: 750 DB_SPAN_LIMIT: 250 COLLECT_INTERVAL_SEC: 15 Copy The default settings for these parameters and others can be found in the newrelic-pixie-integration Github repo. Uninstall Kubernetes integration To uninstall the Kubernetes integration using Helm, run the following command. Replace some-manifest-file.yml with your specific filename. helm uninstall newrelic -n newrelic if kubectl then : kubectl delete -f some-manifest-file.yml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 160.49747,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "sections": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "Helm is a package manager on top of <em>Kubernetes</em>. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in <em>Kubernetes</em>. To install the <em>integration</em> using Helm, we recommend our <em>Kubernetes</em> automated installer, which will prompt"
      },
      "id": "617d5841196a67bb40f7c1de"
    },
    {
      "sections": [
        "Configure Kubernetes with a proxy",
        "Install the infrastructure agent with a proxy",
        "Set logging with a proxy",
        "Install Kubernetes events with a proxy",
        "Install the Prometheus OpenMetrics integration with a proxy",
        "Set the synthetics minion to use a proxy"
      ],
      "title": "Configure Kubernetes with a proxy",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "9d7794244b451d878d00db0f84c4709e43ce138c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/configure-kubernetes-proxy/",
      "published_at": "2022-01-08T09:49:01Z",
      "updated_at": "2021-12-25T07:57:45Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are running our Kubernetes integration with a proxy, you need to configure each component (the infrastructure agent, logging, Kubernetes events, etc.) to correctly work with your proxy. Install the infrastructure agent with a proxy There are three options to install the infrastructure agent with a proxy: Installing with chart newrelic-infrastructure: The proxy can be configured setting the configuration option config.proxy as described in the values.yaml of the chart. The whole config object is used to generate a configMap that is mounted as the file /etc/newrelic-infra.yml used to configure the agent. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option newrelic-infrastructure.config.proxy. The configuration option is passed down to the dependency newrelic-infrastructure modifying the values.yaml of nri-bundle: newrelic-infrastructure.config.proxy: https://user:password@hostname:port Copy Installing with manifest: When installing through the manifest, the proxy can be configured adding the NRIA_PROXY environment variable to the environment variable section of the DaemonSet configuration. The environment variable NRIA_VERBOSE in the DaemonSet YAML can be used as an example of the environment variable configuration. Set logging with a proxy If you’re using log forwarding for the Kubernetes integration, the plugin automatically detects the HTTP_PROXY and HTTPS_PROXY environment variables, and automatically uses them to set up the proxy configuration. For more information, see how to customize the proxy, or bypass it. Install Kubernetes events with a proxy There are three options to install the Kubernetes events integration with a proxy: Installing with chart nri-kube-events: The proxy can be configured setting the configuration option proxy in the values.yaml file. If set, the variable NRIA_PROXY will be added into the deployment.yaml template. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option nri-kube-events.proxy. The configuration option is passed down to the dependency nri-kube-events modifying the values.yaml of nri-bundle: nri-kube-events.proxy: https://user:password@hostname:port Copy Installing with manifest: When installing through the manifest, to configure the proxy set the NRIA_PROXY in the environment variable section of the infra container. Check the deployment template of the chart as an example of environment variable configuration. Install the Prometheus OpenMetrics integration with a proxy There are three options to install the Prometheus OpenMetric integration with a proxy: Installing with charts nri-prometheus: The proxy can be configured setting the configuration option config.emitter_proxy in the values.yaml. The config object is used to generate a configMap that mounted into the deployment under /etc/nri-prometheus/. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option nri-prometheus.config.emitter_proxy. The configuration option is passed down to the dependency nri-prometheus modifying the values.yaml of nri-bundle: nri-prometheus.config.emitter_proxy: \"http://localhost:8888 Copy Installing with the manifest: Configure the proxy uncommenting the emitter_proxy option directly in the manifest: # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" Copy Set the synthetics minion to use a proxy Check this README on how to set the proxy for the Synthetics minions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 106.28012,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure <em>Kubernetes</em> with a proxy",
        "sections": "Configure <em>Kubernetes</em> with a proxy",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "If you are running our <em>Kubernetes</em> <em>integration</em> with a proxy, you need to configure each component (the infrastructure agent, logging, <em>Kubernetes</em> events, etc.) to correctly work with your proxy. Install the infrastructure agent with a proxy There are three options to install the infrastructure agent"
      },
      "id": "617d584264441f2d01fbc6d7"
    },
    {
      "sections": [
        "Get logs and version",
        "Get verbose logs for installations using a manifest file",
        "Caution",
        "Get verbose logs for installations using Helm",
        "Get the infrastructure agent version",
        "Get kube-state-metrics version",
        "Get logs from pods connecting to kube-state-metrics",
        "Retrieve kube-state-metrics service configuration",
        "Get logs from pod running on a master node"
      ],
      "title": "Get logs and version",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Troubleshooting"
      ],
      "external_id": "35fd6fb08a49327a4d2ce5797511e55e39793c67",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/troubleshooting/get-logs-version/",
      "published_at": "2022-01-08T09:51:19Z",
      "updated_at": "2021-10-30T20:45:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To generate verbose logs and get version and configuration information, follow these procedures. Get verbose logs for installations using a manifest file For the Kubernetes integration, the infrastructure agent adds a log entry only in the event of an error. Most common errors are displayed in the standard (non-verbose) logs. If you are doing a more in-depth investigation on your own or with New Relic Support, you can enable verbose mode. Caution Verbose mode significantly increases the amount of info sent to log files. Temporarily enable this mode only for troubleshooting purposes, and reset the log level when finished. To get verbose logging details for an integration using a manifest file: Enable verbose logging: In the deployment file, set the value of NRIA_VERBOSE to 1. Apply the modified configuration by running: kubectl apply -f your_newrelic_k8s.yaml Copy Leave on verbose mode for a few minutes, or until you feel enough activity has occurred. Disable verbose mode: Set the NRIA_VERBOSE value back to 0. Apply the restored configuration by running: kubectl apply -f your_newrelic_k8s.yaml Copy Get a list of nodes in the environment: kubectl get nodes --all-namespaces Copy Get a list of infrastructure and kube-state-metrics pods: kubectl get pods --all-namespaces -o wide | egrep 'newrelic|kube-state-metrics' Copy Get logs from the pods connecting to kube-state-metrics. Retrieve kube-state-metrics service configuration. Get verbose logs for installations using Helm For the Kubernetes integration, the infrastructure agent adds a log entry only in the event of an error. Most common errors are displayed in the standard (non-verbose) logs. If you're doing a more in-depth investigation on your own or with New Relic Support, you can enable verbose mode. Caution Verbose mode significantly increases the amount of info sent to log files. Temporarily enable this mode only for troubleshooting purposes, and reset the log level when finished. To get verbose logging details for an integration using Helm: Enable verbose logging: helm upgrade -n <namespace> --reuse-values newrelic-bundle --set newrelic-infrastructure.verboseLog=true newrelic/nri-bundle Copy Leave on verbose mode for a few minutes, or until enough activity has occurred. When you have the information you need, disable verbose logging: helm upgrade --reuse-values newrelic-bundle --set newrelic-infrastructure.verboseLog=false newrelic/nri-bundle Copy Follow the steps to restore your configuration from step 5 in the section, Get verbose logs for installations using a manifest file. Get the infrastructure agent version For the Kubernetes integration, the infrastructure agent is distributed as a Docker image that contains the infrastructure agent and the Kubernetes integration. The Docker image is tagged with a version, and the infrastructure agent also has its own version. When the agent is successfully sending information to New Relic, you can retrieve the versions of the infrastructure agent for Kubernetes (the Docker image) you are running in your clusters by using the following NRQL query: FROM K8sContainerSample SELECT uniqueCount(entityId) WHERE containerName = 'newrelic-infra' facet clusterName, containerImage Copy If the agent is not reporting any data: Get the version(s) of the New Relic integration for Kubernetes that you are running in a cluster using kubectl: kubectl get pods --all-namespaces -l name=newrelic-infra -o jsonpath=\"{.items..spec..containers..image}\" Copy Look for output similar to this: newrelic/infrastructure-k8s:1.0.0 Copy Get kube-state-metrics version To retrieve the version of kube-state-metrics running on your clusters, run the following NRQL query: FROM K8sContainerSample SELECT uniqueCount(entityId) WHERE containerName LIKE '%kube-state-metrics%' facet clusterName, containerImage Copy Get logs from pods connecting to kube-state-metrics To get the logs from pods connecting to kube-state-metrics: Get the nodes that kube-state-metrics is running on: kubectl get pods --all-namespaces -o wide | grep kube-state-metrics Copy Look for output similar to this: kube-system kube-state-metrics-5c6f5cb9b5-pclhh 2/2 Running 4 4d 172.17.0.3 minikube Copy Get the New Relic pods that are running on the same node as kube-state-metrics: kubectl describe node minikube | grep newrelic-infra Copy Look for output similar to this: default newrelic-infra-5wcv6 100m (5%) 0 (0%) 100Mi (5%) 100Mi (5%) Copy Retrieve the logs for the nodes by running: kubectl logs newrelic-infra-5wcv6 Copy Retrieve kube-state-metrics service configuration To retrieve the configuration: Run: kubectl get pods --all-namespaces | grep \"kube-state-metrics\" Copy Look for a response similar to this: kube-system kube-state-metrics-5c6f5cb9b5-5wf9m 2/2 Running 8 6d Copy Review the namespace in the first column. kubectl describe service kube-state-metrics -n <namespace> Copy Get logs from pod running on a master node To get the logs from a pod running on a master node: Get the nodes that are labelled as master: kubectl get nodes -l node-role.kubernetes.io/master=\"\" Copy Or, kubectl get nodes -l kubernetes.io/role=\"master\" Copy Look for output similar to this: NAME STATUS ROLES AGE VERSION ip-10-42-24-4.ec2.internal Ready master 42d v1.14.8 Copy Get the New Relic pods that are running on one of the nodes returned in the previous step: kubectl get pods --field-selector spec.nodeName=ip-10-42-24-4.ec2.internal -l name=newrelic-infra --all-namespaces Copy Look for output similar to this: newrelic-infra-whvzt Copy Retrieve the logs for the nodes by running: kubectl logs newrelic-infra-whvzt Copy For troubleshooting help, see Not seeing data or Error messages.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 105.96204,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "To generate verbose logs and get version and configuration information, follow these procedures. Get verbose logs for installations using a manifest file For the <em>Kubernetes</em> <em>integration</em>, the infrastructure agent adds a log entry only in the event of an error. Most common errors are displayed"
      },
      "id": "617daee5e7b9d2337dc03982"
    }
  ],
  "/docs/kubernetes-pixie/kubernetes-integration/troubleshooting/not-seeing-control-plane-data": [
    {
      "sections": [
        "Install the Kubernetes integration manually using Helm",
        "Compatibility and requirements",
        "Install Kubernetes integration with Helm",
        "Install with Helm 3 and nri-bundle (recommended)",
        "Installing and configuring nri-bundle with Helm",
        "Install with Helm 2 and nri-bundle (legacy)",
        "Installation instructions for Helm 2",
        "Important",
        "Tip",
        "Helm configuration options",
        "Upgrade using Helm",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data",
        "Reduce data ingest",
        "Prometheus OpenMetrics Integration",
        "New Relic Logging",
        "New Relic Pixie Integration",
        "Uninstall Kubernetes integration"
      ],
      "title": "Install the Kubernetes integration manually using Helm",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "29d42af98d41e7e4e7be86f0150254e132cb6b6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/install-kubernetes-integration-using-helm/",
      "published_at": "2022-01-08T09:49:44Z",
      "updated_at": "2022-01-08T09:49:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Helm is a package manager on top of Kubernetes. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in Kubernetes. To install the integration using Helm, we recommend our Kubernetes automated installer, which will prompt for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our integration as plain manifests rather than a Helm release. See Kubernetes integration: install and configure for more details about how to use our automated installer. Start the installer This page describes in more depth how to install and configure the New Relic integration without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the Kubernetes integration. Our charts are also compatible with Helm 2, but support for it might be removed in the future. To install the Kubernetes integration using Helm, you will need your New Relic account license key and your Kubernetes cluster's name: Find and copy your New Relic license key. Find the name of your cluster with this command: kubectl config current-context Copy Note these values somewhere safe, as you will need them later during the installation process. Install Kubernetes integration with Helm Install with Helm 3 and nri-bundle (recommended) New Relic has several charts for the different components which offer different features for the platform: newrelic-infrastructure: Contains the main Kubernetes integration and the infrastructure agent. This is the core component for the New Relic Kubernetes experience, responsible of reporting most of the data that is surfaced in dashboard and the Kubernetes Cluster Explorer. newrelic-logging: Provides a DaemonSet with New Relic's Fluent Bit output plugin to easily forward your logs to New Relic. nri-kube-events: Collects and reports cluster events (such as kubectl get events) to New Relic. nri-prometheus: New Relic's Prometheus OpenMetrics Integration, automatically scrapes Prometheus endpoints present in the cluser and reports metrics to New Relic. nri-metadata-injection: Sets up a minimal MutatingAdmissionWebhook that injects a couple of environment to containers. These contain metadata about the cluster and New Relic installation and will be later picked up by applications instrumented using APM, allowing to correlate APM and infrastructure data. nri-statsd: New Relic StatsD integration. Additionally, New Relic provides nri-bundle, a chart which pulls a selectable set of the charts mentioned above. nri-bundle can also install Kube State Metrics and Pixie for you if needed. While it is possible to install those charts separately, we strongly recommend using the nri-bundle chart for Kubernetes deployments, as it ensures that values across all the charts are consistent and provides full control over which components are installed, as well as the possibility to configure all of them as Helm dependencies. This is the same chart that is used and referenced by our automated installer. Installing and configuring nri-bundle with Helm Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Please notice and adjust the following flags: global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY: Must be set to a valid License Key for your account. global.cluster=K8S_CLUSTER_NAME: Is used to identify the cluster in the New Relic UI, so should be a descriptive value not used by any other Kubernetes cluster configured in your New Relic account. ksm.enabled=true: Setting this to true will automatically install Kube State Metrics (KSM) for you, which is required for our integration to run. You can set this to false if KSM is already present in your cluster, even if it is on a different namespace. newrelic-infrastructure.privileged=true: Can be set to false to install a trimmed down version of our integration that does not require extra privileges, such as hostPath mounts or running containers as root. Please note that this will disable detailed process collection from the host. For performance reasons, our logging solution still requires hostPath mounts, regardless of the value of this flag. If this is not allowedin your cluster, you will need to disable the logging solution by specifying logging.enabled=false. prometheus.enabled=true: Will deploy our Prometheus OpenMetrics integration, which automatically collects data from prometheus endpoints present in the cluster. webhook.enabled=true: Will install our minimal webhook, which adds environment variables that, in turn, allows linking applications instrumented with APM with infrastructure data. Our chart has a comprehensive set of flags and tunables that can be edited to configure our solution to your particular needs. For a full list of all the flags that can be configured, please check the chart's README and the default values.yaml file. Install the Kubernetes integration by running the customized command without --debug and --dry-run: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl -n newrelic get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node for newrelic-infrastructure, as well as one Deployment and one pod for any other component that you have enabled. Install with Helm 2 and nri-bundle (legacy) Installation instructions for Helm 2 Important Helm 2 has been deprecated and New Relic does not recommend using it for deployments. Instructions in this section are provided for legacy systems only. To install using Helm 2: Ensure that Helm 2, including their cluster-side components (such as Tiller) are properly installed and configured. Please check the official documentation for more details. Set the cluster where you want to install the agent: kubectl config set-cluster DESIRED_CLUSTER Copy To see the available clusters, run kubectl config get-clusters Make sure that kube-state-metrics is installed on your machine: kubectl get deployment --all-namespaces | grep kube-state-metrics Copy If it's not installed, follow the instructions in the kube-state-metrics GitHub repo to install it. Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. This step uses the --dry-run and --debug switches and therefore the agent is not installed. helm install newrelic/newrelic-infrastructure \\ --dry-run \\ --debug \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Install the New Relic Kubernetes integration: helm install newrelic/newrelic-infrastructure \\ --set licenseKey=your_new_relic_license_key \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Tip Note that the --dry-run and --debug switches have been removed. Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node. Helm configuration options When you install or upgrade the Kubernetes integration with Helm using the command line, you can pass your configuration variables with the --set flag. helm install newrelic/newrelic-infrastructure \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=YOUR_CLUSTER_NAME Copy A full list of the configuration parameters can be found in the newrelic-infrastructure chart README Upgrade using Helm To update your Kubernetes integration installed via Helm: Update the local chart repository: helm repo update Copy Update the release by running again the appropriate helm upgrade --install ... command in the section above. Monitor services running on Kubernetes After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages. Reduce data ingest Our charts support setting an option to reduce the amount of data ingest at the cost of dropping detailed information. To enable it, set global.lowDataMode=true in the nri-bundle chart. lowDataMode affects three specific components of the nri-bundle chart outlined below. Prometheus OpenMetrics Integration If lowDataMode is enabled, the following metrics are excluded by default as they are already collected and used by the New Relic Kubernetes Integration. - kube_ - container_ - machine_ - cadvisor_ Copy New Relic Logging If lowDataMode is enabled, Labels and Annotations are set to Off in the Filter section of the fluent-bit.conf file. This means that this detail will be dropped from the container log files which reduces the overall data ingest into New Relic. The following fields are retained: Allowlist_key container_name Allowlist_key namespace_name Allowlist_key pod_name Allowlist_key stream Allowlist_key log Copy Low Data Mode Log Example Complete Log Record [ { \"cluster_name\": \"api-test\", \"kubernetes\": { \"annotations\": { \"kubernetes.io/psp\": \"eks.privileged\" }, \"container_hash\": \"fryckbos/test@sha256:5b098eaf3c7d5b3585eb10cebee63665b6208bea31ef31a3f0856c5ffdda644b\", \"container_image\": \"fryckbos/test:latest\", \"container_name\": \"newrelic-logging\", \"docker_id\": \"134e1daf63761baa15e035b08b7aea04518a0f0e50af4215131a50c6a379a072\", \"host\": \"ip-192-168-17-123.ec2.internal\", \"labels\": { \"app\": \"newrelic-logging\", \"app.kubernetes.io/name\": \"newrelic-logging\", \"controller-revision-hash\": \"84db95db86\", \"pod-template-generation\": \"1\", \"release\": \"nri-bundle\" }, \"namespace_name\": \"nrlogs\", \"pod_id\": \"54556e3e-719c-46b5-af69-020b75d69bf1\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\" }, \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"plugin\": { \"source\": \"kubernetes\", \"type\": \"fluent-bit\", \"version\": \"1.8.1\" }, \"stream\": \"stderr\", \"time\": \"2021-09-14T12:30:49.138824971Z\", \"timestamp\": 1631622649138 } ] Copy Log Record after enabling lowDataMode. [ { \"cluster_name\": \"api-test\", \"container_name\": \"newrelic-logging\", \"namespace_name\": \"nrlogs\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\", \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"stream\": \"stderr\", \"timestamp\": 1631622649138 } ] Copy New Relic Pixie Integration If lowDataMode is enabled, the newrelic-pixie integration performs heavier sampling on Pixie spans and reduces the collection interval from 10 seconds to 15 seconds. lowDataMode settings: HTTP_SPAN_LIMIT: 750 DB_SPAN_LIMIT: 250 COLLECT_INTERVAL_SEC: 15 Copy The default settings for these parameters and others can be found in the newrelic-pixie-integration Github repo. Uninstall Kubernetes integration To uninstall the Kubernetes integration using Helm, run the following command. Replace some-manifest-file.yml with your specific filename. helm uninstall newrelic -n newrelic if kubectl then : kubectl delete -f some-manifest-file.yml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 160.49747,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "sections": "Install the <em>Kubernetes</em> <em>integration</em> manually using Helm",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "Helm is a package manager on top of <em>Kubernetes</em>. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in <em>Kubernetes</em>. To install the <em>integration</em> using Helm, we recommend our <em>Kubernetes</em> automated installer, which will prompt"
      },
      "id": "617d5841196a67bb40f7c1de"
    },
    {
      "sections": [
        "Configure Kubernetes with a proxy",
        "Install the infrastructure agent with a proxy",
        "Set logging with a proxy",
        "Install Kubernetes events with a proxy",
        "Install the Prometheus OpenMetrics integration with a proxy",
        "Set the synthetics minion to use a proxy"
      ],
      "title": "Configure Kubernetes with a proxy",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "9d7794244b451d878d00db0f84c4709e43ce138c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/configure-kubernetes-proxy/",
      "published_at": "2022-01-08T09:49:01Z",
      "updated_at": "2021-12-25T07:57:45Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are running our Kubernetes integration with a proxy, you need to configure each component (the infrastructure agent, logging, Kubernetes events, etc.) to correctly work with your proxy. Install the infrastructure agent with a proxy There are three options to install the infrastructure agent with a proxy: Installing with chart newrelic-infrastructure: The proxy can be configured setting the configuration option config.proxy as described in the values.yaml of the chart. The whole config object is used to generate a configMap that is mounted as the file /etc/newrelic-infra.yml used to configure the agent. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option newrelic-infrastructure.config.proxy. The configuration option is passed down to the dependency newrelic-infrastructure modifying the values.yaml of nri-bundle: newrelic-infrastructure.config.proxy: https://user:password@hostname:port Copy Installing with manifest: When installing through the manifest, the proxy can be configured adding the NRIA_PROXY environment variable to the environment variable section of the DaemonSet configuration. The environment variable NRIA_VERBOSE in the DaemonSet YAML can be used as an example of the environment variable configuration. Set logging with a proxy If you’re using log forwarding for the Kubernetes integration, the plugin automatically detects the HTTP_PROXY and HTTPS_PROXY environment variables, and automatically uses them to set up the proxy configuration. For more information, see how to customize the proxy, or bypass it. Install Kubernetes events with a proxy There are three options to install the Kubernetes events integration with a proxy: Installing with chart nri-kube-events: The proxy can be configured setting the configuration option proxy in the values.yaml file. If set, the variable NRIA_PROXY will be added into the deployment.yaml template. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option nri-kube-events.proxy. The configuration option is passed down to the dependency nri-kube-events modifying the values.yaml of nri-bundle: nri-kube-events.proxy: https://user:password@hostname:port Copy Installing with manifest: When installing through the manifest, to configure the proxy set the NRIA_PROXY in the environment variable section of the infra container. Check the deployment template of the chart as an example of environment variable configuration. Install the Prometheus OpenMetrics integration with a proxy There are three options to install the Prometheus OpenMetric integration with a proxy: Installing with charts nri-prometheus: The proxy can be configured setting the configuration option config.emitter_proxy in the values.yaml. The config object is used to generate a configMap that mounted into the deployment under /etc/nri-prometheus/. Installing with chart nri-bundle as a dependency: The proxy can be configured setting the configuration option nri-prometheus.config.emitter_proxy. The configuration option is passed down to the dependency nri-prometheus modifying the values.yaml of nri-bundle: nri-prometheus.config.emitter_proxy: \"http://localhost:8888 Copy Installing with the manifest: Configure the proxy uncommenting the emitter_proxy option directly in the manifest: # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" Copy Set the synthetics minion to use a proxy Check this README on how to set the proxy for the Synthetics minions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 106.28012,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure <em>Kubernetes</em> with a proxy",
        "sections": "Configure <em>Kubernetes</em> with a proxy",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "If you are running our <em>Kubernetes</em> <em>integration</em> with a proxy, you need to configure each component (the infrastructure agent, logging, <em>Kubernetes</em> events, etc.) to correctly work with your proxy. Install the infrastructure agent with a proxy There are three options to install the infrastructure agent"
      },
      "id": "617d584264441f2d01fbc6d7"
    },
    {
      "sections": [
        "Get logs and version",
        "Get verbose logs for installations using a manifest file",
        "Caution",
        "Get verbose logs for installations using Helm",
        "Get the infrastructure agent version",
        "Get kube-state-metrics version",
        "Get logs from pods connecting to kube-state-metrics",
        "Retrieve kube-state-metrics service configuration",
        "Get logs from pod running on a master node"
      ],
      "title": "Get logs and version",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Troubleshooting"
      ],
      "external_id": "35fd6fb08a49327a4d2ce5797511e55e39793c67",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/troubleshooting/get-logs-version/",
      "published_at": "2022-01-08T09:51:19Z",
      "updated_at": "2021-10-30T20:45:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To generate verbose logs and get version and configuration information, follow these procedures. Get verbose logs for installations using a manifest file For the Kubernetes integration, the infrastructure agent adds a log entry only in the event of an error. Most common errors are displayed in the standard (non-verbose) logs. If you are doing a more in-depth investigation on your own or with New Relic Support, you can enable verbose mode. Caution Verbose mode significantly increases the amount of info sent to log files. Temporarily enable this mode only for troubleshooting purposes, and reset the log level when finished. To get verbose logging details for an integration using a manifest file: Enable verbose logging: In the deployment file, set the value of NRIA_VERBOSE to 1. Apply the modified configuration by running: kubectl apply -f your_newrelic_k8s.yaml Copy Leave on verbose mode for a few minutes, or until you feel enough activity has occurred. Disable verbose mode: Set the NRIA_VERBOSE value back to 0. Apply the restored configuration by running: kubectl apply -f your_newrelic_k8s.yaml Copy Get a list of nodes in the environment: kubectl get nodes --all-namespaces Copy Get a list of infrastructure and kube-state-metrics pods: kubectl get pods --all-namespaces -o wide | egrep 'newrelic|kube-state-metrics' Copy Get logs from the pods connecting to kube-state-metrics. Retrieve kube-state-metrics service configuration. Get verbose logs for installations using Helm For the Kubernetes integration, the infrastructure agent adds a log entry only in the event of an error. Most common errors are displayed in the standard (non-verbose) logs. If you're doing a more in-depth investigation on your own or with New Relic Support, you can enable verbose mode. Caution Verbose mode significantly increases the amount of info sent to log files. Temporarily enable this mode only for troubleshooting purposes, and reset the log level when finished. To get verbose logging details for an integration using Helm: Enable verbose logging: helm upgrade -n <namespace> --reuse-values newrelic-bundle --set newrelic-infrastructure.verboseLog=true newrelic/nri-bundle Copy Leave on verbose mode for a few minutes, or until enough activity has occurred. When you have the information you need, disable verbose logging: helm upgrade --reuse-values newrelic-bundle --set newrelic-infrastructure.verboseLog=false newrelic/nri-bundle Copy Follow the steps to restore your configuration from step 5 in the section, Get verbose logs for installations using a manifest file. Get the infrastructure agent version For the Kubernetes integration, the infrastructure agent is distributed as a Docker image that contains the infrastructure agent and the Kubernetes integration. The Docker image is tagged with a version, and the infrastructure agent also has its own version. When the agent is successfully sending information to New Relic, you can retrieve the versions of the infrastructure agent for Kubernetes (the Docker image) you are running in your clusters by using the following NRQL query: FROM K8sContainerSample SELECT uniqueCount(entityId) WHERE containerName = 'newrelic-infra' facet clusterName, containerImage Copy If the agent is not reporting any data: Get the version(s) of the New Relic integration for Kubernetes that you are running in a cluster using kubectl: kubectl get pods --all-namespaces -l name=newrelic-infra -o jsonpath=\"{.items..spec..containers..image}\" Copy Look for output similar to this: newrelic/infrastructure-k8s:1.0.0 Copy Get kube-state-metrics version To retrieve the version of kube-state-metrics running on your clusters, run the following NRQL query: FROM K8sContainerSample SELECT uniqueCount(entityId) WHERE containerName LIKE '%kube-state-metrics%' facet clusterName, containerImage Copy Get logs from pods connecting to kube-state-metrics To get the logs from pods connecting to kube-state-metrics: Get the nodes that kube-state-metrics is running on: kubectl get pods --all-namespaces -o wide | grep kube-state-metrics Copy Look for output similar to this: kube-system kube-state-metrics-5c6f5cb9b5-pclhh 2/2 Running 4 4d 172.17.0.3 minikube Copy Get the New Relic pods that are running on the same node as kube-state-metrics: kubectl describe node minikube | grep newrelic-infra Copy Look for output similar to this: default newrelic-infra-5wcv6 100m (5%) 0 (0%) 100Mi (5%) 100Mi (5%) Copy Retrieve the logs for the nodes by running: kubectl logs newrelic-infra-5wcv6 Copy Retrieve kube-state-metrics service configuration To retrieve the configuration: Run: kubectl get pods --all-namespaces | grep \"kube-state-metrics\" Copy Look for a response similar to this: kube-system kube-state-metrics-5c6f5cb9b5-5wf9m 2/2 Running 8 6d Copy Review the namespace in the first column. kubectl describe service kube-state-metrics -n <namespace> Copy Get logs from pod running on a master node To get the logs from a pod running on a master node: Get the nodes that are labelled as master: kubectl get nodes -l node-role.kubernetes.io/master=\"\" Copy Or, kubectl get nodes -l kubernetes.io/role=\"master\" Copy Look for output similar to this: NAME STATUS ROLES AGE VERSION ip-10-42-24-4.ec2.internal Ready master 42d v1.14.8 Copy Get the New Relic pods that are running on one of the nodes returned in the previous step: kubectl get pods --field-selector spec.nodeName=ip-10-42-24-4.ec2.internal -l name=newrelic-infra --all-namespaces Copy Look for output similar to this: newrelic-infra-whvzt Copy Retrieve the logs for the nodes by running: kubectl logs newrelic-infra-whvzt Copy For troubleshooting help, see Not seeing data or Error messages.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 105.96204,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "To generate verbose logs and get version and configuration information, follow these procedures. Get verbose logs for installations using a manifest file For the <em>Kubernetes</em> <em>integration</em>, the infrastructure agent adds a log entry only in the event of an error. Most common errors are displayed"
      },
      "id": "617daee5e7b9d2337dc03982"
    }
  ],
  "/docs/kubernetes-pixie/kubernetes-integration/understand-use-data/find-use-your-kubernetes-data": [
    {
      "sections": [
        "Navigate the Kubernetes cluster explorer",
        "Meet the cluster explorer",
        "Cluster dashboard",
        "Cluster explorer node table",
        "Search and filter your cluster data",
        "Browse your Kubernetes events"
      ],
      "title": "Navigate the Kubernetes cluster explorer",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Understand and use data"
      ],
      "external_id": "f4e8acd0df7b51805faba3774684718822b93431",
      "image": "https://docs.newrelic.com/static/34f90215b59ab8d7b4ec986bbb110805/9b7bd/nr1-cluster-explorer-node-tooltip.png",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/understand-use-data/kubernetes-cluster-explorer/",
      "published_at": "2022-01-08T09:13:14Z",
      "updated_at": "2021-10-24T03:11:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes cluster explorer uses the data collected by the Kubernetes integration to show the status of your cluster, from the control plane to nodes and pods. You can find out about the health of each entity, explore logs, and see how your apps are performing. With the Events integration, everything that happens in your cluster becomes visible, and logs brought in using the logs plugin are also available. Meet the cluster explorer The cluster explorer represents your most relevant cluster data on a chart with the shape of a ship's wheel — which is also Kubernetes' logo. Outer ring: Contains up to 24 nodes of your cluster, the most relevant based on the amount of alerts. Hover over each node to check resource consumption and the percentage of allocable pods used. Inner rings: Contain the pods ( ) of each node. Pods with active alerts are shown in the third innermost ring, and pods that are pending or unable to run are in the center. Hover the mouse over each node or pod to get a quick overview of its resource usage. You can click each node and pod to view its resource usage over time or to get more information about its health and active alerts. Colors are based on predefined alert conditions: Yellow pods have active warning alerts, while red pods have active critical alerts. one.newrelic.com > Kubernetes cluster explorer: Click any pod to get more information about its status and health, and to dig deeper into application data and traces, logs, and events. Click a node to see the following data: Pod statistics CPU, memory, and storage consumption against allocatable amounts Amount of pods used by the node against the allocatable amount of pods For each pod, depending on the integrations and features you've enabled, you can see: Pod status and metadata, including namespace and deployment Container status and statistics Active alerts (both warning and critical) Kubernetes events that happened in that pod APM data and traces (if you've linked your APM data) A link to the pods' and containers' logs, collected using the Kubernetes plugin for New Relic Logs Cluster and control plane statistics are always visible on the left side. Cluster dashboard The cluster dashboard can be accessed at any time from the cluster explorer by clicking Kubernetes dashboard. It provides a curated dashboard experience for your Kubernetes cluster. one.newrelic.com > Kubernetes cluster explorer > Kubernetes dashboard: The Kubernetes dashboard can be accessed from the Kubernetes cluster explorer. It shows useful Kubernetes metric data. Cluster explorer node table Below the cluster explorer is the node table, which shows all the nodes of the cluster, namespace, or deployment. Like all other usage indicators, the table shows consumption against allocatable resources. Search and filter your cluster data The main way to modify the data view in the cluster explorer is by using the top bar to search for specific attributes or values. All the attributes and values collected by the Kubernetes integration can be combined to narrow down the cluster view. one.newrelic.com > Kubernetes cluster explorer: All your Kubernetes cluster's attributes and data points can be used to filter the cluster explorer view. You can also change the time frame using the time picker in the upper right corner. The Auto-refresh box turns the cluster explorer into a real-time dashboard that refreshes every 60 seconds. one.newrelic.com > Kubernetes cluster explorer: The time picker lets you select several predefined time spans. To reload the data every minute, check the auto-refresh box. Browse your Kubernetes events If you’ve enabled the Kubernetes events integration, you can click the Events tab to browse everything that happened in your cluster, from warnings to normal events. To set it up, select the Kubernetes events box in step 3 of our install wizard, or follow the instructions. one.newrelic.com > Kubernetes cluster explorer > Events: Browse and filter all your Kubernetes events, and dig into logs and infrastructure data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.65405,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Navigate the <em>Kubernetes</em> cluster explorer",
        "sections": "Navigate the <em>Kubernetes</em> cluster explorer",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": "New Relic&#x27;s <em>Kubernetes</em> cluster explorer uses the <em>data</em> collected by the <em>Kubernetes</em> <em>integration</em> to show the status of your cluster, from the control plane to nodes and pods. You can find out about the health of each entity, explore logs, and see how your apps are performing. With the Events"
      },
      "id": "617daf22196a670f66f7bddd"
    },
    {
      "sections": [
        "Install the Kubernetes integration manually using Helm",
        "Compatibility and requirements",
        "Install Kubernetes integration with Helm",
        "Install with Helm 3 and nri-bundle (recommended)",
        "Installing and configuring nri-bundle with Helm",
        "Install with Helm 2 and nri-bundle (legacy)",
        "Installation instructions for Helm 2",
        "Important",
        "Tip",
        "Helm configuration options",
        "Upgrade using Helm",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data",
        "Reduce data ingest",
        "Prometheus OpenMetrics Integration",
        "New Relic Logging",
        "New Relic Pixie Integration",
        "Uninstall Kubernetes integration"
      ],
      "title": "Install the Kubernetes integration manually using Helm",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "29d42af98d41e7e4e7be86f0150254e132cb6b6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/install-kubernetes-integration-using-helm/",
      "published_at": "2022-01-08T09:49:44Z",
      "updated_at": "2022-01-08T09:49:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Helm is a package manager on top of Kubernetes. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in Kubernetes. To install the integration using Helm, we recommend our Kubernetes automated installer, which will prompt for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our integration as plain manifests rather than a Helm release. See Kubernetes integration: install and configure for more details about how to use our automated installer. Start the installer This page describes in more depth how to install and configure the New Relic integration without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the Kubernetes integration. Our charts are also compatible with Helm 2, but support for it might be removed in the future. To install the Kubernetes integration using Helm, you will need your New Relic account license key and your Kubernetes cluster's name: Find and copy your New Relic license key. Find the name of your cluster with this command: kubectl config current-context Copy Note these values somewhere safe, as you will need them later during the installation process. Install Kubernetes integration with Helm Install with Helm 3 and nri-bundle (recommended) New Relic has several charts for the different components which offer different features for the platform: newrelic-infrastructure: Contains the main Kubernetes integration and the infrastructure agent. This is the core component for the New Relic Kubernetes experience, responsible of reporting most of the data that is surfaced in dashboard and the Kubernetes Cluster Explorer. newrelic-logging: Provides a DaemonSet with New Relic's Fluent Bit output plugin to easily forward your logs to New Relic. nri-kube-events: Collects and reports cluster events (such as kubectl get events) to New Relic. nri-prometheus: New Relic's Prometheus OpenMetrics Integration, automatically scrapes Prometheus endpoints present in the cluser and reports metrics to New Relic. nri-metadata-injection: Sets up a minimal MutatingAdmissionWebhook that injects a couple of environment to containers. These contain metadata about the cluster and New Relic installation and will be later picked up by applications instrumented using APM, allowing to correlate APM and infrastructure data. nri-statsd: New Relic StatsD integration. Additionally, New Relic provides nri-bundle, a chart which pulls a selectable set of the charts mentioned above. nri-bundle can also install Kube State Metrics and Pixie for you if needed. While it is possible to install those charts separately, we strongly recommend using the nri-bundle chart for Kubernetes deployments, as it ensures that values across all the charts are consistent and provides full control over which components are installed, as well as the possibility to configure all of them as Helm dependencies. This is the same chart that is used and referenced by our automated installer. Installing and configuring nri-bundle with Helm Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Please notice and adjust the following flags: global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY: Must be set to a valid License Key for your account. global.cluster=K8S_CLUSTER_NAME: Is used to identify the cluster in the New Relic UI, so should be a descriptive value not used by any other Kubernetes cluster configured in your New Relic account. ksm.enabled=true: Setting this to true will automatically install Kube State Metrics (KSM) for you, which is required for our integration to run. You can set this to false if KSM is already present in your cluster, even if it is on a different namespace. newrelic-infrastructure.privileged=true: Can be set to false to install a trimmed down version of our integration that does not require extra privileges, such as hostPath mounts or running containers as root. Please note that this will disable detailed process collection from the host. For performance reasons, our logging solution still requires hostPath mounts, regardless of the value of this flag. If this is not allowedin your cluster, you will need to disable the logging solution by specifying logging.enabled=false. prometheus.enabled=true: Will deploy our Prometheus OpenMetrics integration, which automatically collects data from prometheus endpoints present in the cluster. webhook.enabled=true: Will install our minimal webhook, which adds environment variables that, in turn, allows linking applications instrumented with APM with infrastructure data. Our chart has a comprehensive set of flags and tunables that can be edited to configure our solution to your particular needs. For a full list of all the flags that can be configured, please check the chart's README and the default values.yaml file. Install the Kubernetes integration by running the customized command without --debug and --dry-run: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl -n newrelic get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node for newrelic-infrastructure, as well as one Deployment and one pod for any other component that you have enabled. Install with Helm 2 and nri-bundle (legacy) Installation instructions for Helm 2 Important Helm 2 has been deprecated and New Relic does not recommend using it for deployments. Instructions in this section are provided for legacy systems only. To install using Helm 2: Ensure that Helm 2, including their cluster-side components (such as Tiller) are properly installed and configured. Please check the official documentation for more details. Set the cluster where you want to install the agent: kubectl config set-cluster DESIRED_CLUSTER Copy To see the available clusters, run kubectl config get-clusters Make sure that kube-state-metrics is installed on your machine: kubectl get deployment --all-namespaces | grep kube-state-metrics Copy If it's not installed, follow the instructions in the kube-state-metrics GitHub repo to install it. Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. This step uses the --dry-run and --debug switches and therefore the agent is not installed. helm install newrelic/newrelic-infrastructure \\ --dry-run \\ --debug \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Install the New Relic Kubernetes integration: helm install newrelic/newrelic-infrastructure \\ --set licenseKey=your_new_relic_license_key \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Tip Note that the --dry-run and --debug switches have been removed. Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node. Helm configuration options When you install or upgrade the Kubernetes integration with Helm using the command line, you can pass your configuration variables with the --set flag. helm install newrelic/newrelic-infrastructure \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=YOUR_CLUSTER_NAME Copy A full list of the configuration parameters can be found in the newrelic-infrastructure chart README Upgrade using Helm To update your Kubernetes integration installed via Helm: Update the local chart repository: helm repo update Copy Update the release by running again the appropriate helm upgrade --install ... command in the section above. Monitor services running on Kubernetes After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages. Reduce data ingest Our charts support setting an option to reduce the amount of data ingest at the cost of dropping detailed information. To enable it, set global.lowDataMode=true in the nri-bundle chart. lowDataMode affects three specific components of the nri-bundle chart outlined below. Prometheus OpenMetrics Integration If lowDataMode is enabled, the following metrics are excluded by default as they are already collected and used by the New Relic Kubernetes Integration. - kube_ - container_ - machine_ - cadvisor_ Copy New Relic Logging If lowDataMode is enabled, Labels and Annotations are set to Off in the Filter section of the fluent-bit.conf file. This means that this detail will be dropped from the container log files which reduces the overall data ingest into New Relic. The following fields are retained: Allowlist_key container_name Allowlist_key namespace_name Allowlist_key pod_name Allowlist_key stream Allowlist_key log Copy Low Data Mode Log Example Complete Log Record [ { \"cluster_name\": \"api-test\", \"kubernetes\": { \"annotations\": { \"kubernetes.io/psp\": \"eks.privileged\" }, \"container_hash\": \"fryckbos/test@sha256:5b098eaf3c7d5b3585eb10cebee63665b6208bea31ef31a3f0856c5ffdda644b\", \"container_image\": \"fryckbos/test:latest\", \"container_name\": \"newrelic-logging\", \"docker_id\": \"134e1daf63761baa15e035b08b7aea04518a0f0e50af4215131a50c6a379a072\", \"host\": \"ip-192-168-17-123.ec2.internal\", \"labels\": { \"app\": \"newrelic-logging\", \"app.kubernetes.io/name\": \"newrelic-logging\", \"controller-revision-hash\": \"84db95db86\", \"pod-template-generation\": \"1\", \"release\": \"nri-bundle\" }, \"namespace_name\": \"nrlogs\", \"pod_id\": \"54556e3e-719c-46b5-af69-020b75d69bf1\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\" }, \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"plugin\": { \"source\": \"kubernetes\", \"type\": \"fluent-bit\", \"version\": \"1.8.1\" }, \"stream\": \"stderr\", \"time\": \"2021-09-14T12:30:49.138824971Z\", \"timestamp\": 1631622649138 } ] Copy Log Record after enabling lowDataMode. [ { \"cluster_name\": \"api-test\", \"container_name\": \"newrelic-logging\", \"namespace_name\": \"nrlogs\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\", \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"stream\": \"stderr\", \"timestamp\": 1631622649138 } ] Copy New Relic Pixie Integration If lowDataMode is enabled, the newrelic-pixie integration performs heavier sampling on Pixie spans and reduces the collection interval from 10 seconds to 15 seconds. lowDataMode settings: HTTP_SPAN_LIMIT: 750 DB_SPAN_LIMIT: 250 COLLECT_INTERVAL_SEC: 15 Copy The default settings for these parameters and others can be found in the newrelic-pixie-integration Github repo. Uninstall Kubernetes integration To uninstall the Kubernetes integration using Helm, run the following command. Replace some-manifest-file.yml with your specific filename. helm uninstall newrelic -n newrelic if kubectl then : kubectl delete -f some-manifest-file.yml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 160.49747,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>Kubernetes</em> <em>integration</em> manually <em>using</em> Helm",
        "sections": "Install the <em>Kubernetes</em> <em>integration</em> manually <em>using</em> Helm",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": " for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our <em>integration</em> as plain manifests rather than a Helm release. See <em>Kubernetes</em> <em>integration</em>: install and configure for more details about how to <em>use</em> our automated"
      },
      "id": "617d5841196a67bb40f7c1de"
    },
    {
      "sections": [
        "On-host integration data collection and reporting",
        "Data collection and reporting process",
        "File structure and specifications"
      ],
      "title": "On-host integration data collection and reporting",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "Understand and use data"
      ],
      "external_id": "d8d2680dcd67f4094b6a243a5a643080bca9248f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/understand-use-data/host-integration-data-collection-reporting/",
      "published_at": "2022-01-08T12:30:18Z",
      "updated_at": "2021-10-24T01:22:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Read on to learn how New Relic on-host integrations collect and report data to New Relic. Data collection and reporting process This is how an infrastructure on-host integration sends data to New Relic: On startup, the infrastructure agent scans the directory that contains the integration's definition files. The infrastructure agent registers every integration executable defined in the definition file. The agent scans a dedicated directory for integration configuration files. If those config files specify integrations that have been registered with the infrastructure agent, the agent sets up and schedules the integrations. At the scheduled interval (the default is 15 seconds), the agent harvests the data from the integration and prepares it for transmission. Every 60 seconds, it sends that data to New Relic, along with any other infrastructure data. After a successful collection pass, the integration executable exits. File structure and specifications Understanding the file structure of New Relic on-host integrations can help you customize your integration, understand and use your data, and troubleshoot problems. On-host integrations adhere to a set of open source specifications, allowing anyone to build their own infrastructure on-host integration. For an explanation of these file specifications, see File specs.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.40985,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integration</em> <em>data</em> collection <em>and</em> reporting",
        "sections": "On-host <em>integration</em> <em>data</em> collection <em>and</em> reporting",
        "tags": "<em>Understand</em> <em>and</em> <em>use</em> <em>data</em>",
        "body": ", the <em>integration</em> executable exits. File structure and specifications Understanding the file structure of New Relic on-host <em>integrations</em> can help you customize your <em>integration</em>, <em>understand</em> and <em>use</em> your <em>data</em>, and troubleshoot problems. On-host <em>integrations</em> adhere to a set of open source specifications, allowing anyone to build their own infrastructure on-host <em>integration</em>. For an explanation of these file specifications, see File specs."
      },
      "id": "617d6f8d28ccbca08b80083a"
    }
  ],
  "/docs/kubernetes-pixie/kubernetes-integration/understand-use-data/kubernetes-cluster-explorer": [
    {
      "sections": [
        "Find and use your Kubernetes data",
        "Query Kubernetes data",
        "Event types",
        "Manage alerts",
        "Create an alert condition",
        "Use the predefined alert types and thresholds",
        "Select alert notifications",
        "Pod alert notification example",
        "Container resource notification example",
        "Create alert conditions using NRQL",
        "Kubernetes attributes and metrics",
        "Node data",
        "Namespace data",
        "Deployment data",
        "ReplicaSet data",
        "DaemonSet data",
        "StatefulSet data",
        "Pod data",
        "Cluster data",
        "Container data",
        "Volume data",
        "API server data",
        "Controller manager data",
        "Scheduler data",
        "ETCD data",
        "Endpoint data",
        "Service data",
        "Horizontal Pod Autoscaler data",
        "Kubernetes metadata in APM-monitored applications",
        "For more help"
      ],
      "title": "Find and use your Kubernetes data",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Understand and use data"
      ],
      "external_id": "636617521998343c5bb96b0500843229b9263712",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/understand-use-data/find-use-your-kubernetes-data/",
      "published_at": "2022-01-08T09:14:46Z",
      "updated_at": "2021-10-24T03:11:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can build your own charts and query all your Kubernetes integration data using the query builder and the NerdGraph API. Our integration collects Kubernetes data by instrumenting the container orchestration layer. For a simpler and more visual experience, use the cluster explorer. one.newrelic.com > Dashboards: Using the query builder you can query your Kubernetes data and create clear visualizations. Query Kubernetes data The simplest way to query your Kubernetes data is using the query builder, which accepts NRQL queries. Alternatively, you can use the NerdGraph API to retrieve Kubernetes data. Event types Kubernetes data is attached to the following event types: Event name Type of Kubernetes data Available since K8sNodeSample Node data v1.0.0 K8sNamespaceSample Namespace data v1.0.0 K8sDeploymentSample Deployment data v1.0.0 K8sReplicasetSample ReplicaSet data v1.0.0 K8sDaemonsetSample DaemonSet data v1.13.0 K8sStatefulsetSample StatefulSet data v1.13.0 K8sPodSample Pod data v1.0.0 K8sClusterSample Cluster data v1.0.0 K8sContainerSample Container data v1.0.0 K8sVolumeSample Volume data v1.0.0 K8sApiServerSample API server data v1.11.0 K8sControllerManagerSample Controller manager data v1.11.0 K8sSchedulerSample Scheduler data v1.11.0 K8sEtcdSample ETCD data v1.11.0 K8sEndpointSample Endpoint data v1.13.0 K8sServiceSample Service data v1.13.0 K8sHpaSample Horizontal Pod Autoscaler data v2.3.0 Manage alerts You can be notified about alert violations for your Kubernetes data: Create an alert condition To create an alert condition for the Kubernetes integration: Go to one.newrelic.com > Infrastructure > Settings > Alerts > Kubernetes, then select Create alert condition. To filter the alert to Kubernetes entities that only have the chosen attributes, select Filter. Select the threshold settings. For more on the Trigger an alert when... options, see Alert types. Select an existing alert policy, or create a new one. Select Create. When an alert condition's threshold is triggered, New Relic sends a notification to the policy's notification channels. Use the predefined alert types and thresholds The Kubernetes integration comes with its own alert policy and alert conditions. To see what the predefined alert conditions are, see Kubernetes integration: Predefined alert policy. In addition, you can create an alert condition for any metric collected by any New Relic integration you use, including the Kubernetes integration: Select the alert type Integrations. From the Select a data source dropdown, select a Kubernetes (K8s) data source. Select alert notifications When an alert condition's threshold is triggered, New Relic sends a message to the notification channel(s) chosen in the alert policy. Depending on the type of notification, you may have the following options: View the incident. Acknowledge the incident. Go to a chart of the incident data by selecting the identifier name. The entity identifier that triggered the alert appears near the top of the notification message. The format of the identifier depends on the alert type: Available pods are less than desired pods alerts: K8s:CLUSTER_NAME:PARENT_NAMESPACE:replicaset:REPLICASET_NAME Copy CPU or memory usage alerts: K8s:CLUSTER_NAME:PARENT_NAMESPACE:POD_NAME:container:CONTAINER_NAME Copy Here are some examples. Pod alert notification example For Available pods are less than desired pods alerts, the ID of the ReplicaSet triggering the issue might look like this: k8s:beam-production:default:replicaset:nginx-deployment-1623441481 Copy This identifier contains the following information: Cluster name: beam-production Parent namespace: default ReplicaSet name: nginx-deployment-1623441481 Container resource notification example For container CPU or memory usage alerts, the entity might look like this: k8s:beam-production:kube-system:kube-state-metrics-797bb87c75-zncwn:container:kube-state-metrics Copy This identifier contains the following information: Cluster name: beam-production Parent namespace: kube-system Pod namespace: kube-state-metrics-797bb87c75-zncwn Container name: kube-state-metrics Create alert conditions using NRQL Follow standard procedures to create alert conditions for NRQL queries. Kubernetes attributes and metrics The Kubernetes integration collects the following metrics and other attributes. Node data Query the K8sNodeSample event for node data: Node attribute Description allocatableCpuCores Node allocatable CPU cores allocatableMemoryBytes Node allocatable memory bytes allocatablePods Node allocatable pods allocatableEphemeralStorageBytes Node allocatable ephemeral-storage bytes capacityCpuCores Node CPU capacity capacityMemoryBytes Node memory capacity (in bytes) capacityPods Pod capacity of the node capacityEphemeralStorageBytes Node ephemeral-storage capacity clusterName Name that you assigned to the cluster when you installed the Kubernetes integration condition.{conditionName}={conditionValue} Status of the current observed node condition. The reported conditions can vary depending on your Kubernetes flavor and installed operators. Examples of common conditions are: Ready, DiskPressure, MemoryPressure, PIDPressure and NetworkUnavailable. Condition values can be 1 (true), 0 (false), or -1 (unknown). cpuUsedCoreMilliseconds Node CPU usage measured in core milliseconds cpuUsedCores Node CPU usage measured in cores cpuRequestedCores Total amount of CPU cores requested allocatableCpuCoresUtilization Percentage of CPU cores actually used with respect to the CPU cores allocatable fsAvailableBytes Bytes available in the node filesystem fsCapacityBytes Total capacity of the node filesystem in bytes fsInodes Total number of inodes in the node filesystem fsInodesFree Free inodes in the node filesystem fsInodesUsed Used inodes in the node filesystem fsUsedBytes Used bytes in the node filesystem fsCapacityUtilization Percentage of used bytes in the node filesystem with respect to the capacity memoryAvailableBytes Bytes of memory available in the node memoryMajorPageFaultsPerSecond Number of major page faults per second in the node memoryPageFaults Number of page faults in the node memoryRssBytes Bytes of rss memory memoryUsedBytes Bytes of memory used memoryWorkingSetBytes Bytes of memory in the working set memoryRequestedBytes Total amount of requested memory allocatableMemoryUtilization Percentage of bytes of memory in the working set with respect to the node allocatable memory net.errorCountPerSecond Number of errors per second while receiving/transmitting over the network nodeName Host name that the pod is running on runtimeAvailableBytes Bytes available to the container runtime filesystem runtimeCapacityBytes Total capacity assigned to the container runtime filesystem in bytes runtimeInodes Total number of inodes in the container runtime filesystem runtimeInodesFree Free inodes in the container runtime filesystem runtimeInodesUsed Used inodes in the container runtime filesystem runtimeUsedBytes Used bytes in the container runtime filesystem unschedulable Status of node schedulability of new pods. Its value can be 0 (false) or 1 (true) label.LABEL_NAME Labels associated with your node, so you can filter and query for specific nodes Namespace data Query the K8sNamespaceSample event for namespace data: Namespace attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of the namespace when it was created namespace Name of the namespace to be used as an identifier label.LABEL_NAME Labels associated with your namespace, so you can filter and query for specific namespaces status Current status of the namespace. The value can be Active or Terminated Deployment data Query the K8sDeploymentSample event for deployment data: Deployment attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the deployment was created deploymentName Name of the deployment to be used as an identifier namespace Name of the namespace that the deployment belongs to label.LABEL_NAME Labels associated with your deployment, so you can filter and query for specific deployments podsAvailable Number of replicas that are currently available podsDesired Number of replicas that you defined in the deployment podsTotal Total number of replicas that are currently running podsUnavailable Number of replicas that are currently unavailable podsUpdated Number of replicas that have been updated to achieve the desired state of the deployment podsMissing Total number of replicas that are missing (number of desired replicas, podsDesired, minus the total number of replicas, podsTotal) ReplicaSet data Query the K8sReplicasetSample event for ReplicaSet data: Replica attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the ReplicaSet was created deploymentName Name of the deployment to be used as an identifier namespace Name of the namespace that the ReplicaSet belongs to observedGeneration Integer representing generation observed by the ReplicaSet podsDesired Number of replicas that you defined in the deployment podsFullyLabeled Number of pods that have labels that match the ReplicaSet pod template labels podsReady Number of replicas that are ready for this ReplicaSet podsTotal Total number of replicas that are currently running podsMissing Total number of replicas that are currently missing (number of desired replicas, podsDesired, minus the number of ready replicas, podsReady) replicasetName Name of the ReplicaSet to be used as an identifier DaemonSet data Query the K8sDaemonsetSample event for DaemonSet data: DaemonSet attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the DaemonSet was created namespaceName Name of the namespace that the DaemonSet belongs to label.LABEL_NAME Labels associated with your DaemonSet, so you can filter and query for specific DaemonSet daemonsetName Name associated with the DaemonSet podsDesired The number of nodes that should be running the daemon pod podsScheduled The number of nodes running at least one daemon pod and are supposed to podsAvailable The number of nodes that should be running the daemon pod and have one or more of the daemon pod running and available podsReady The number of nodes that should be running the daemon pod and have one or more of the daemon pod running and ready podsUnavailable The number of nodes that should be running the daemon pod and have none of the daemon pod running and available podsMisscheduled The number of nodes running a daemon pod but are not supposed to podsUpdatedScheduled The total number of nodes that are running updated daemon pod podsMissing Total number of replicas that are currently missing (number of desired replicas, podsDesired, minus the number of ready replicas, podsReady) metadataGeneration Sequence number representing a specific generation of the desired state StatefulSet data Query the K8sStatefulsetSample event for StatefulSet data: StatefulSet attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the StatefulSet was created namespaceName Name of the namespace that the StatefulSet belongs to label.LABEL_NAME Labels associated with your StatefulSet, so you can filter and query for specific StatefulSet statefulsetName Name associated with the StatefulSet podsDesired Number of desired pods for a StatefulSet podsReady The number of ready replicas per StatefulSet podsCurrent The number of current replicas per StatefulSet podsTotal The number of replicas per StatefulSet podsUpdated The number of updated replicas per StatefulSet podsMissing Total number of replicas that are currently missing (number of desired replicas, podsDesired, minus the number of ready replicas, podsReady) observedGeneration The generation observed by the StatefulSet controller metadataGeneration Sequence number representing a specific generation of the desired state for the StatefulSet currentRevision Indicates the version of the StatefulSet used to generate pods in the sequence. Value range: between 0 and podsCurrent updateRevision Indicates the version of the StatefulSet used to generate pods in the sequence. Value range: between podsDesired-podsUpdated and podsDesired Pod data Query the K8sPodSample event for pod data: Pod attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the pod was created in epoch seconds createdBy Name of the Kubernetes object that created the pod. For example, newrelic-infra createdKind Kind of Kubernetes object that created the pod. For example, DaemonSet. deploymentName Name of the deployment to be used as an identifier isReady Boolean representing whether or not the pod is ready to serve requests isScheduled Boolean representing whether or not the pod has been scheduled to run on a node label.LABEL_NAME Labels associated with your pod, so you can filter and query for specific pods message Details related to the last pod status change namespace Name of the namespace that the pod belongs to net.errorCountPerSecond Number of errors per second while receiving/transmitting over the network net.errorsPerSecond Number of errors per second net.rxBytesPerSecond Number of bytes per second received over the network net.txBytesPerSecond Number of bytes per second transmitted over the network nodeIP Host IP address that the pod is running on nodeName Host name that the pod is running on podIP IP address of the pod. If it doesn't have an IP, it'll be empty podName Name of the pod to be used as an identifier reason Reason why the pod is in the current status startTime Timestamp of when the pod started running in epoch seconds status Current status of the pod. Value can be Pending, Running, Succeeded, Failed, Unknown Cluster data Query the K8sClusterSample event to see cluster data: Cluster attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration clusterK8sVersion Kubernetes version that the cluster is running Container data Query the K8sContainerSample event for container data: Container attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration containerID Unique ID associated with the container. If you are running Docker, this is the Docker container id containerImage Name of the image that the container is running containerImageID Unique ID associated with the image that the container is running containerName Name associated with the container cpuLimitCores Integer representing limit CPU cores defined for the container in the pod specification cpuRequestedCores Requested CPU cores defined for the container in the pod specification cpuUsedCores CPU cores actually used by the container cpuCoresUtilization Percentage of CPU cores actually used by the container with respect to the CPU limit specified. This percentage is based on this calculation: (cpuUsedCores / cpuLimitCores) * 100 requestedCpuCoresUtilization Percentage of CPU cores actually used by the container with respect to the CPU request specified deploymentName Name of the deployment to be used as an identifier isReady Boolean. Whether or not the container's readiness check succeeded label.LABEL_NAME Labels associated with your container, so you can filter and query for specific containers memoryLimitBytes Integer representing limit bytes of memory defined for the container in the pod specification memoryRequestedBytes Integer. Requested bytes of memory defined for the container in the pod specification memoryUsedBytes Integer. Bytes of memory actually used by the container memoryUtilization Percentage of memory actually used by the container with respect to the memory limit specified requestedMemoryUtilization Percentage of memory actually used by the container with respect to the memory request specified memoryWorkingSetBytes Integer. Bytes of memory in the working set memoryWorkingSetUtilization Percentage of working set memory actually used by the container with respect to the memory limit specified requestedMemoryWorkingSetUtilization Percentage of working set memory actually used by the container with respect to the memory request specified namespace Name of the namespace that the container belongs to nodeIP Host IP address the container is running on nodeName Host name that the container is running on podName Name of the pod that the container is in, to be used as an identifier reason Provides a reason why the container is in the current status restartCount Number of times the container has been restarted status Current status of the container. Value can be Running, Terminated, or Unknown containerCpuCfsPeriodsDelta Delta change of elapsed enforcement period intervals containerCpuCfsThrottledPeriodsDelta Delta change of throttled period intervals containerCpuCfsThrottledSecondsDelta Delta change of duration the container has been throttled, in seconds containerCpuCfsPeriodsTotal Total number of elapsed enforcement period intervals containerCpuCfsThrottledPeriodsTotal Total number of throttled period intervals containerCpuCfsThrottledSecondsTotal Total time duration the container has been throttled, in seconds containerMemoryMappedFileBytes Total size of memory mapped files used by this container, in bytes Volume data Query the K8sVolumeSample event for volume data: Volume attribute Description volumeName Name that you assigned to the volume at creation clusterName Cluster where the volume is configured namespace Namespace where the volume is configured podName The pod that the volume is attached to. The Kubernetes monitoring integration lists Volumes that are attached to a pod persistent If this is a persistent volume, this value is set to true pvcNamespace Namespace where the Persistent Volume Claim is configured pvcName Name that you assigned to the Persistent Volume Claim at creation fsCapacityBytes Capacity of the volume, in bytes fsUsedBytes Usage of the volume, in bytes fsAvailableBytes Capacity available of the volume, in bytes fsUsedPercent Usage of the volume in percentage fsInodes Total inodes of the volume fsInodesUsed inodes used in the volume fsInodesFree inodes available in the volume Volume data is available for volume plugins that implement the MetricsProvider interface: AWSElasticBlockStore AzureDisk AzureFile Cinder Flexvolume Flocker GCEPersistentDisk GlusterFS iSCSI StorageOS VsphereVolume API server data Query the K8sApiServerSample event to see API Server data. For more information, see Configure control plane monitoring: API server attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent, in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist apiserverRequestDelta_verb_VERB_code_CODE Difference of the number of apiserver requests, broken out for each verb and HTTP response code apiserverRequestRate_verb_VERB_code_CODE Rate of apiserver requests, broken out for each verb and HTTP response code restClientRequestsDelta_code_CODE_method_METHOD Difference of the number of HTTP requests, partitioned by method and code restClientRequestsRate_code_CODE_method_METHOD Rate of the number of HTTP requests, partitioned by method and code etcdObjectCounts_resource_RESOURCE-KIND Number of stored objects at the time of last check, split by kind Controller manager data Query the K8sControllerManagerSample event to see Controller manager data. For more information, see Configure control plane monitoring: Controller manager attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist workqueueAddsDelta_name_WORK-QUEUE-NAME Difference of the total number of adds handled by workqueue workqueueDepth_name_WORK-QUEUE-NAME Current depth of workqueue workqueueRetriesDelta_name_WORK-QUEUE-NAME Difference of the total number of retries handled by workqueue leaderElectionMasterStatus Gauge of if the reporting system is master of the relevant lease, 0 indicates backup, 1 indicates master Scheduler data Query the K8sSchedulerSample event in New Relic Insights to see Scheduler data. For more information, see Configure control plane monitoring: Scheduler attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist leaderElectionMasterStatus Gauge of if the reporting system is master of the relevant lease, 0 indicates backup, 1 indicates master httpRequestDurationMicroseconds_handler_HANDLER_quantile_QUANTILE The HTTP request latencies in microseconds, per quantile httpRequestDurationMicroseconds_handler_HANDLER_sum The sum of the HTTP request latencies, in microseconds httpRequestDurationMicroseconds_handler_HANDLER_count The number of observed HTTP requests events restClientRequestsDelta_code_CODE_host_HOST_method_METHOD Difference of the number of HTTP requests, partitioned by status code, method, and host restClientRequestsRate_code_CODE_host_HOST_method_METHOD Rate of the number of HTTP requests, partitioned by status code, method, and host schedulerScheduleAttemptsDelta_result_RESULT Difference of the number of attempts to schedule pods, by the result. unschedulable means a pod could not be scheduled, while error means an internal scheduler problem schedulerScheduleAttemptsRate_result_RESULT Rate of the number of attempts to schedule pods, by the result. unschedulable means a pod could not be scheduled, while error means an internal scheduler problem schedulerSchedulingDurationSeconds_operation_OPERATION_quantile_QUANTILE Scheduling latency in seconds split by sub-parts of the scheduling operation schedulerSchedulingDurationSeconds_operation_OPERATION_sum The sum of scheduling latency in seconds split by sub-parts of the scheduling operation schedulerSchedulingDurationSeconds_operation_OPERATION_count The number of observed events of schedulings split by sub-parts of the scheduling operation. schedulerPreemptionAttemptsDelta Difference of the total preemption attempts in the cluster till now schedulerPodPreemptionVictims Number of selected preemption victims ETCD data Query the K8sEtcdSample event to see ETCD data. For more information, see Configure control plane monitoring: ETCD attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist etcdServerHasLeader Whether or not a leader exists. 1 is existence, 0 is not etcdServerLeaderChangesSeenDelta Difference of the number of leader changes seen etcdMvccDbTotalSizeInBytes Total size of the underlying database physically allocated, in bytes etcdServerProposalsCommittedDelta Difference of the total number of consensus proposals committed etcdServerProposalsCommittedRate Rate of the total number of consensus proposals committed etcdServerProposalsAppliedDelta Difference of the total number of consensus proposals applied etcdServerProposalsAppliedRate Rate of the total number of consensus proposals applied etcdServerProposalsPending The current number of pending proposals to commit etcdServerProposalsFailedDelta Difference of the total number of failed proposals seen etcdServerProposalsFailedRate Rate of the total number of failed proposals seen processOpenFds Number of open file descriptors processMaxFds Maximum number of open file descriptors processFdsUtilization Percentage open file descriptors with respect to the maximum number that can be opened etcdNetworkClientGrpcReceivedBytesRate Rate of the total number of bytes received from gRPC clients etcdNetworkClientGrpcSentBytesRate Rate of the total number of bytes sent to gRPC clients Endpoint data Query the K8sEndpointSample event in New Relic Insights for endpoint data: Endpoint attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the endpoint was created namespaceName Name of the namespace that the endpoint belongs to endpointName Name associated with the endpoint label.LABEL_NAME Labels associated with your endpoint, so you can filter and query for specific endpoints addressAvailable Number of addresses available in endpoint addressNotReady Number of addresses not ready in endpoint Service data Query the K8sServiceSample event for service data: Service attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the service was created namespaceName Name of the namespace that the service belongs to label.LABEL_NAME Labels associated with your service, so you can filter and query for specific service serviceName Name associated with the service loadBalancerIP The IP of the external load balancer, if Spectype is LoadBalancer. externalName The external name value, if Spectype is ExternalName clusterIP The internal cluster IP, if Spectype is ClusterIP specType Type of the service selector.LABEL_NAME The label selector that this service targets Horizontal Pod Autoscaler data Query the K8sHpaSample event in New Relic Insights for Horizontal Pod Autoscaler data: HPA attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration label.LABEL_NAME Labels associated with your HPA, so you can filter and query for specific autoscaler currentReplicas Current number of replicas of pods managed by this autoscaler desiredReplicas Desired number of replicas of pods managed by this autoscaler minReplicas Lower limit for the number of pods that can be set by the autoscaler, 1 by default maxReplicas Upper limit for the number of pods that can be set by the autoscaler; cannot be smaller than minReplicas targetMetric The metric specifications used by this autoscaler when calculating the desired replica count isAble Boolean representing whether or not the autoscaler is able to fetch and update scales, as well as whether or not any backoff-related conditions would prevent scaling isActive Boolean representing whether or not the autoscaler is enabled (if it's able to calculate the desired scales) isLimited Boolean representing whether or not the autoscaler is capped, either up or down, by the maximum or minimum replicas configured labels Number of Kubernetes labels converted to Prometheus labels metadataGeneration The generation observed by the HorizontalPodAutoscaler controller Kubernetes metadata in APM-monitored applications By linking your applications with Kubernetes, the following attributes are added to application trace and distributed trace: nodeName containerName podName clusterName deploymentName namespaceName For more help",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.65405,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Find <em>and</em> <em>use</em> your <em>Kubernetes</em> <em>data</em>",
        "sections": "Find <em>and</em> <em>use</em> your <em>Kubernetes</em> <em>data</em>",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": " Relic <em>integration</em> you <em>use</em>, including the <em>Kubernetes</em> <em>integration</em>: Select the alert type <em>Integrations</em>. From the Select a <em>data</em> source dropdown, select a <em>Kubernetes</em> (K8s) <em>data</em> source. Select alert notifications When an alert condition&#x27;s threshold is triggered, New Relic sends a message to the notification"
      },
      "id": "617d58a9196a6775cbf7c43d"
    },
    {
      "sections": [
        "Install the Kubernetes integration manually using Helm",
        "Compatibility and requirements",
        "Install Kubernetes integration with Helm",
        "Install with Helm 3 and nri-bundle (recommended)",
        "Installing and configuring nri-bundle with Helm",
        "Install with Helm 2 and nri-bundle (legacy)",
        "Installation instructions for Helm 2",
        "Important",
        "Tip",
        "Helm configuration options",
        "Upgrade using Helm",
        "Monitor services running on Kubernetes",
        "Use your Kubernetes data",
        "Reduce data ingest",
        "Prometheus OpenMetrics Integration",
        "New Relic Logging",
        "New Relic Pixie Integration",
        "Uninstall Kubernetes integration"
      ],
      "title": "Install the Kubernetes integration manually using Helm",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Installation"
      ],
      "external_id": "29d42af98d41e7e4e7be86f0150254e132cb6b6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/installation/install-kubernetes-integration-using-helm/",
      "published_at": "2022-01-08T09:49:44Z",
      "updated_at": "2022-01-08T09:49:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Helm is a package manager on top of Kubernetes. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in Kubernetes. To install the integration using Helm, we recommend our Kubernetes automated installer, which will prompt for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our integration as plain manifests rather than a Helm release. See Kubernetes integration: install and configure for more details about how to use our automated installer. Start the installer This page describes in more depth how to install and configure the New Relic integration without using the automated installer. Compatibility and requirements Make sure Helm is installed on your machine. We strongly recommend using Helm 3 to manage the Kubernetes integration. Our charts are also compatible with Helm 2, but support for it might be removed in the future. To install the Kubernetes integration using Helm, you will need your New Relic account license key and your Kubernetes cluster's name: Find and copy your New Relic license key. Find the name of your cluster with this command: kubectl config current-context Copy Note these values somewhere safe, as you will need them later during the installation process. Install Kubernetes integration with Helm Install with Helm 3 and nri-bundle (recommended) New Relic has several charts for the different components which offer different features for the platform: newrelic-infrastructure: Contains the main Kubernetes integration and the infrastructure agent. This is the core component for the New Relic Kubernetes experience, responsible of reporting most of the data that is surfaced in dashboard and the Kubernetes Cluster Explorer. newrelic-logging: Provides a DaemonSet with New Relic's Fluent Bit output plugin to easily forward your logs to New Relic. nri-kube-events: Collects and reports cluster events (such as kubectl get events) to New Relic. nri-prometheus: New Relic's Prometheus OpenMetrics Integration, automatically scrapes Prometheus endpoints present in the cluser and reports metrics to New Relic. nri-metadata-injection: Sets up a minimal MutatingAdmissionWebhook that injects a couple of environment to containers. These contain metadata about the cluster and New Relic installation and will be later picked up by applications instrumented using APM, allowing to correlate APM and infrastructure data. nri-statsd: New Relic StatsD integration. Additionally, New Relic provides nri-bundle, a chart which pulls a selectable set of the charts mentioned above. nri-bundle can also install Kube State Metrics and Pixie for you if needed. While it is possible to install those charts separately, we strongly recommend using the nri-bundle chart for Kubernetes deployments, as it ensures that values across all the charts are consistent and provides full control over which components are installed, as well as the possibility to configure all of them as Helm dependencies. This is the same chart that is used and referenced by our automated installer. Installing and configuring nri-bundle with Helm Ensure you are using the appropriate context in the machine where you will run Helm and kubectl: You can check the available contexts with: kubectl config get-contexts Copy And switch to the desired context using: kubectl config use-context CONTEXT_NAME Copy Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. Notice that we are specifying --dry-run and --debug, so nothing will be installed in this step: helm upgrade --install newrelic newrelic/nri-bundle \\ --dry-run \\ --debug \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Please notice and adjust the following flags: global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY: Must be set to a valid License Key for your account. global.cluster=K8S_CLUSTER_NAME: Is used to identify the cluster in the New Relic UI, so should be a descriptive value not used by any other Kubernetes cluster configured in your New Relic account. ksm.enabled=true: Setting this to true will automatically install Kube State Metrics (KSM) for you, which is required for our integration to run. You can set this to false if KSM is already present in your cluster, even if it is on a different namespace. newrelic-infrastructure.privileged=true: Can be set to false to install a trimmed down version of our integration that does not require extra privileges, such as hostPath mounts or running containers as root. Please note that this will disable detailed process collection from the host. For performance reasons, our logging solution still requires hostPath mounts, regardless of the value of this flag. If this is not allowedin your cluster, you will need to disable the logging solution by specifying logging.enabled=false. prometheus.enabled=true: Will deploy our Prometheus OpenMetrics integration, which automatically collects data from prometheus endpoints present in the cluster. webhook.enabled=true: Will install our minimal webhook, which adds environment variables that, in turn, allows linking applications instrumented with APM with infrastructure data. Our chart has a comprehensive set of flags and tunables that can be edited to configure our solution to your particular needs. For a full list of all the flags that can be configured, please check the chart's README and the default values.yaml file. Install the Kubernetes integration by running the customized command without --debug and --dry-run: helm upgrade --install newrelic newrelic/nri-bundle \\ --namespace newrelic --create-namespace \\ --set global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set global.cluster=K8S_CLUSTER_NAME \\ --set ksm.enabled= true \\ --set newrelic-infrastructure.privileged= true \\ --set infrastructure.enabled=true \\ --set prometheus.enabled=true \\ --set webhook.enabled=true \\ --set kubeEvents.enabled=true \\ --set logging.enabled=true Copy Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl -n newrelic get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node for newrelic-infrastructure, as well as one Deployment and one pod for any other component that you have enabled. Install with Helm 2 and nri-bundle (legacy) Installation instructions for Helm 2 Important Helm 2 has been deprecated and New Relic does not recommend using it for deployments. Instructions in this section are provided for legacy systems only. To install using Helm 2: Ensure that Helm 2, including their cluster-side components (such as Tiller) are properly installed and configured. Please check the official documentation for more details. Set the cluster where you want to install the agent: kubectl config set-cluster DESIRED_CLUSTER Copy To see the available clusters, run kubectl config get-clusters Make sure that kube-state-metrics is installed on your machine: kubectl get deployment --all-namespaces | grep kube-state-metrics Copy If it's not installed, follow the instructions in the kube-state-metrics GitHub repo to install it. Add the New Relic Helm charts repo: helm repo add newrelic https://helm-charts.newrelic.com Copy Make sure everything is configured properly in the chart by running the following command. This step uses the --dry-run and --debug switches and therefore the agent is not installed. helm install newrelic/newrelic-infrastructure \\ --dry-run \\ --debug \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Install the New Relic Kubernetes integration: helm install newrelic/newrelic-infrastructure \\ --set licenseKey=your_new_relic_license_key \\ --set cluster=K8S_CLUSTER_NAME \\ --set config.custom_attributes.cluster=K8S_CLUSTER_NAME Copy Tip Note that the --dry-run and --debug switches have been removed. Wait a few seconds, then check that the DaemonSet and pods have been created: kubectl get daemonsets,pods Copy Make sure you see a DaemonSet, and one pod per node. Helm configuration options When you install or upgrade the Kubernetes integration with Helm using the command line, you can pass your configuration variables with the --set flag. helm install newrelic/newrelic-infrastructure \\ --set licenseKey=YOUR_NEW_RELIC_LICENSE_KEY \\ --set cluster=YOUR_CLUSTER_NAME Copy A full list of the configuration parameters can be found in the newrelic-infrastructure chart README Upgrade using Helm To update your Kubernetes integration installed via Helm: Update the local chart repository: helm repo update Copy Update the release by running again the appropriate helm upgrade --install ... command in the section above. Monitor services running on Kubernetes After having installed our Kubernetes integration, you can start instrumenting the services than run in your cluster. To learn more about how to do this, please check our Monitor services running on Kubernetes page. Use your Kubernetes data To learn more about how to use your Kubernetes data, please head to our detailed Find and use your Kubernetes data pages. Reduce data ingest Our charts support setting an option to reduce the amount of data ingest at the cost of dropping detailed information. To enable it, set global.lowDataMode=true in the nri-bundle chart. lowDataMode affects three specific components of the nri-bundle chart outlined below. Prometheus OpenMetrics Integration If lowDataMode is enabled, the following metrics are excluded by default as they are already collected and used by the New Relic Kubernetes Integration. - kube_ - container_ - machine_ - cadvisor_ Copy New Relic Logging If lowDataMode is enabled, Labels and Annotations are set to Off in the Filter section of the fluent-bit.conf file. This means that this detail will be dropped from the container log files which reduces the overall data ingest into New Relic. The following fields are retained: Allowlist_key container_name Allowlist_key namespace_name Allowlist_key pod_name Allowlist_key stream Allowlist_key log Copy Low Data Mode Log Example Complete Log Record [ { \"cluster_name\": \"api-test\", \"kubernetes\": { \"annotations\": { \"kubernetes.io/psp\": \"eks.privileged\" }, \"container_hash\": \"fryckbos/test@sha256:5b098eaf3c7d5b3585eb10cebee63665b6208bea31ef31a3f0856c5ffdda644b\", \"container_image\": \"fryckbos/test:latest\", \"container_name\": \"newrelic-logging\", \"docker_id\": \"134e1daf63761baa15e035b08b7aea04518a0f0e50af4215131a50c6a379a072\", \"host\": \"ip-192-168-17-123.ec2.internal\", \"labels\": { \"app\": \"newrelic-logging\", \"app.kubernetes.io/name\": \"newrelic-logging\", \"controller-revision-hash\": \"84db95db86\", \"pod-template-generation\": \"1\", \"release\": \"nri-bundle\" }, \"namespace_name\": \"nrlogs\", \"pod_id\": \"54556e3e-719c-46b5-af69-020b75d69bf1\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\" }, \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"plugin\": { \"source\": \"kubernetes\", \"type\": \"fluent-bit\", \"version\": \"1.8.1\" }, \"stream\": \"stderr\", \"time\": \"2021-09-14T12:30:49.138824971Z\", \"timestamp\": 1631622649138 } ] Copy Log Record after enabling lowDataMode. [ { \"cluster_name\": \"api-test\", \"container_name\": \"newrelic-logging\", \"namespace_name\": \"nrlogs\", \"pod_name\": \"nri-bundle-newrelic-logging-jxnbj\", \"message\": \"[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\\n\", \"stream\": \"stderr\", \"timestamp\": 1631622649138 } ] Copy New Relic Pixie Integration If lowDataMode is enabled, the newrelic-pixie integration performs heavier sampling on Pixie spans and reduces the collection interval from 10 seconds to 15 seconds. lowDataMode settings: HTTP_SPAN_LIMIT: 750 DB_SPAN_LIMIT: 250 COLLECT_INTERVAL_SEC: 15 Copy The default settings for these parameters and others can be found in the newrelic-pixie-integration Github repo. Uninstall Kubernetes integration To uninstall the Kubernetes integration using Helm, run the following command. Replace some-manifest-file.yml with your specific filename. helm uninstall newrelic -n newrelic if kubectl then : kubectl delete -f some-manifest-file.yml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 160.49734,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>Kubernetes</em> <em>integration</em> manually <em>using</em> Helm",
        "sections": "Install the <em>Kubernetes</em> <em>integration</em> manually <em>using</em> Helm",
        "tags": "<em>Kubernetes</em> <em>integration</em>",
        "body": " for almost all configuration options and autopopulate secrets and values for you. Additionally, our automated installer also allows to install our <em>integration</em> as plain manifests rather than a Helm release. See <em>Kubernetes</em> <em>integration</em>: install and configure for more details about how to <em>use</em> our automated"
      },
      "id": "617d5841196a67bb40f7c1de"
    },
    {
      "sections": [
        "On-host integration data collection and reporting",
        "Data collection and reporting process",
        "File structure and specifications"
      ],
      "title": "On-host integration data collection and reporting",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "Understand and use data"
      ],
      "external_id": "d8d2680dcd67f4094b6a243a5a643080bca9248f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/host-integrations/understand-use-data/host-integration-data-collection-reporting/",
      "published_at": "2022-01-08T12:30:18Z",
      "updated_at": "2021-10-24T01:22:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Read on to learn how New Relic on-host integrations collect and report data to New Relic. Data collection and reporting process This is how an infrastructure on-host integration sends data to New Relic: On startup, the infrastructure agent scans the directory that contains the integration's definition files. The infrastructure agent registers every integration executable defined in the definition file. The agent scans a dedicated directory for integration configuration files. If those config files specify integrations that have been registered with the infrastructure agent, the agent sets up and schedules the integrations. At the scheduled interval (the default is 15 seconds), the agent harvests the data from the integration and prepares it for transmission. Every 60 seconds, it sends that data to New Relic, along with any other infrastructure data. After a successful collection pass, the integration executable exits. File structure and specifications Understanding the file structure of New Relic on-host integrations can help you customize your integration, understand and use your data, and troubleshoot problems. On-host integrations adhere to a set of open source specifications, allowing anyone to build their own infrastructure on-host integration. For an explanation of these file specifications, see File specs.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.40984,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integration</em> <em>data</em> collection <em>and</em> reporting",
        "sections": "On-host <em>integration</em> <em>data</em> collection <em>and</em> reporting",
        "tags": "<em>Understand</em> <em>and</em> <em>use</em> <em>data</em>",
        "body": ", the <em>integration</em> executable exits. File structure and specifications Understanding the file structure of New Relic on-host <em>integrations</em> can help you customize your <em>integration</em>, <em>understand</em> and <em>use</em> your <em>data</em>, and troubleshoot problems. On-host <em>integrations</em> adhere to a set of open source specifications, allowing anyone to build their own infrastructure on-host <em>integration</em>. For an explanation of these file specifications, see File specs."
      },
      "id": "617d6f8d28ccbca08b80083a"
    }
  ],
  "/docs/licenses/end-of-life/notification-changes-new-relic-saas-features-distributed-software": [
    {
      "sections": [
        "Networks",
        "Tip",
        "TLS encryption",
        "User-facing domains",
        "APM agents",
        "Port 443 recommended",
        "Agent downloads",
        "Infrastructure agents",
        "Browser domains",
        "Mobile domains",
        "Synthetic monitor public locations",
        "Synthetic monitor private locations",
        "Alerts webhooks, api.newrelic.com, cloud integrations, and ticketing integrations",
        "Pixie integration",
        "OpenTelemetry"
      ],
      "title": "Networks",
      "type": "docs",
      "tags": [
        "Using New Relic",
        "Cross-product functions",
        "Install and configure"
      ],
      "external_id": "9f7555daaafae1753bf1e741a5d607e7f0f87b7c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/networks/",
      "published_at": "2022-01-08T17:44:07Z",
      "updated_at": "2022-01-08T17:44:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This list is current. Networks, IPs, domains, ports, and endpoints last updated January 6, 2022. This is a list of the networks, IP addresses, domains, ports, and endpoints used by API clients or agents to communicate with New Relic. TLS is required for all domains. For information on our FedRAMP endpoints, see our FedRAMP endpoints documentation. Tip This doc describes how to ensure our agents and integrations can access New Relic's domains. To monitor the performance of your network, see Get started with Network Performance Monitoring. TLS encryption To ensure data security for our customers and to be in compliance with FedRAMP and other standards for data encryption, Transport Layer Security (TLS) is required for all domains. Our preferred protocol for all domains is TLS 1.2. For more information, see New Relic's Explorers Hub post about TLS 1.2. In addition, TLS 1.2 is required for most domains, except: APM agent connections Browser agent connections Event API For future updates to required and supported protocol versions, follow the Security Notifications tag in New Relic's Explorers Hub. User-facing domains Your browser must be able to communicate with a number of domains for New Relic One to work properly. Update your allow list to ensure New Relic can communicate with a number of integral domains that are listed in this section. Blocking domains can cause issues with individual product features or prevent pages from loading altogether. This list doesn't cover domains that New Relic connects to that can be blocked without affecting your usage of the product. It also doesn't cover Nerdpacks or other features that communicate with external services that have additional domain requirements. If your organization uses a firewall that restricts outbound traffic, follow the specific procedures for the operating system and the firewall you use to add the following domains to the allow list. Domain Description *.newrelic.com New Relic One and supporting services *.nr-assets.net Static New Relic assets *.nr-ext.net New Relic One Nerdpacks and assets *.amazonaws.com New Relic One catalog assets behind AWS S3 *.cloudfront.net Static New Relic assets behind AWS CloudFront CDN secure.gravatar.com Support for Gravatar avatars fonts.googleapis.com Support for Google Fonts fonts.gstatic.com Support for Google Fonts www.google.com Support for reCAPTCHA www.gstatic.com Support for reCAPTCHA *.nr-data.net OpenTelemetry and Pixie onenr.io New Relic One sharing permalinks APM agents To enhance network performance and data security, New Relic uses a CDN and DDoS prevention service with a large IP range. New Relic agents require your firewall to allow outgoing connections to the following networks and ports. To add the following IP connections to the allow list, follow the specific procedures for the operating system and the firewall you use. TLS is required for all domains. Use the IP connections for account data in the US or European Union region as appropriate: IP connections APM data Networks US region accounts: 162.247.240.0/22 EU region accounts: 185.221.84.0/22 Ports US region accounts: Default: TCP 443 (recommended) TCP 80 EU region accounts: Default: TCP 443 (recommended) TCP 80 Endpoints US region accounts: collector*.newrelic.com EU region accounts: collector*.eu01.nr-data.net:443 (recommended) Port 443 recommended Recommendation: Use port 443, a secured channel for encrypted HTTPS traffic. Some New Relic agents also offer port 80, an unsecured channel open to all HTTP traffic. While some agents can be configured to use both port 80 and port 443, we recommend that you choose the port 443 (default). If you have an existing configuration that uses port 80, you can update it to use port 443, the default New Relic connection. Agent downloads TLS is required for all domains. Service for download.newrelic.com is provided through Fastly and is subject to change without warning. For the most current list of public IP addresses for New Relic agent downloads, see api.fastly.com/public-ip-list. Infrastructure agents In order to report data to New Relic, our infrastructure monitoring needs outbound access to these domains, networks, and ports. TLS is required for all domains. Use the IP connections for account data in the US or European Union region as appropriate: IP connections Infrastructure data Domains infra-api.newrelic.com: Required to submit events, metrics, and inventory data. identity-api.newrelic.com: Required for entity registration (for example, a host entity). infrastructure-command-api.newrelic.com: Required to determine feature flags. Also used for gradual rollout of new capabilities. log-api.newrelic.com: Required to submit logs to a US datacenter. log-api.eu.newrelic.com: Required to submit logs to an EU datacenter. metric-api.newrelic.com: Required to submit dimensional metrics. Networks For US region accounts: 162.247.240.0/22 For EU region accounts: 185.221.84.0/22 Port 443 Domains + Port For US region accounts: infra-api.newrelic.com:443 identity-api.newrelic.com:443 infrastructure-command-api.newrelic.com:443 log-api.newrelic.com:443 metric-api.newrelic.com:443 For EU region accounts: infra-api.eu.newrelic.com:443 identity-api.eu.newrelic.com:443 infrastructure-command-api.eu.newrelic.com:443 log-api.eu.newrelic.com:443 metric-api.eu.newrelic.com:443 Proxy If your system needs a proxy to connect to this domain, use the Infrastructure proxy setting. Browser domains In addition to the IP addresses for APM agents, applications monitored by our browser agents use outgoing connections to the following domains. TLS is required for all domains. Use the IP connections for account data in the US or European Union region as appropriate: For US region accounts: bam.nr-data.net js-agent.newrelic.com For EU region accounts: eu01.nr-data.net bam.eu01.nr-data.net For more information about CDN access for the js-agent.newrelic.com file to the domain bam.nr-data.net or to one of the New Relic beacons, see Security for browser monitoring. Mobile domains In addition to the IP addresses for APM agents, applications monitored by our mobile agents use outgoing connections to the following domains. TLS is required for all domains. Use the IP connections for account data in the US or European Union region as appropriate: For US region accounts: mobile-collector.newrelic.com mobile-crash.newrelic.com mobile-symbol-upload.newrelic.com For EU region accounts: mobile-collector.eu01.nr-data.net mobile-crash.eu01.nr-data.net mobile-symbol-upload.eu01.nr-data.net Synthetic monitor public locations To configure your firewall to allow synthetic monitors to access your monitored URL, use Synthetic public minion IPs. TLS is required for all domains. Synthetic monitor private locations Synthetic private minions report to a specific endpoint based on region. To allow the private minion to access the endpoint or the static IP addresses associated with the endpoint, follow the specific procedures for the operating system and the firewall you use. These IP addresses may change in the future. TLS is required for all domains. Use the IP connections for account data in the US or European Union region as appropriate: IP connections Synthetics private location data Endpoint For US region accounts: https://synthetics-horde.nr-data.net/ For EU region accounts: https://synthetics-horde.eu01.nr-data.net/ IP addresses For US region accounts: 13.248.153.51 76.223.21.185 For EU region accounts: 185.221.86.57 185.221.86.25 Alerts webhooks, api.newrelic.com, cloud integrations, and ticketing integrations Endpoints that use api.newrelic.com (such as our GraphQL API for NerdGraph) and our New Relic-generated webhooks for alert policies use an IP address from designated network blocks for the US or European Union region. TLS is required for all addresses in these blocks. Network blocks for US region accounts: 162.247.240.0/22 Network blocks for EU region accounts: 158.177.65.64/29 159.122.103.184/29 161.156.125.32/28 These network blocks also apply to third-party ticketing integrations and New Relic cloud integrations. Pixie integration The Pixie integration runs in your Kubernetes cluster and pulls a set of curated observability data from Pixie to send it to New Relic using the OpenTelemetry line protocol. The Pixie integration requires outbound network access to the following: work.withpixie.ai:443 otlp.nr-data.net:4317 (US region accounts) otlp.eu01.nr-data.net:4317 (EU region accounts) OpenTelemetry New Relic supports the native OpenTelemetry Protocol (OTLP) for exporting telemetry data. This allows you to use the vendor neutral components developed by the OpenTelemetry community to export your data to New Relic. To export OTLP data to New Relic, configure the OTLP exporter to add a header ( api-key ) whose value is your account license key. And, based on your region, configure the endpoint where the exporter sends data to New Relic. See the OpenTelemetry quick start for more information. otlp.nr-data.net:4317 (US region accounts) otlp.eu01.nr-data.net:4317 (EU region accounts) Network blocks for US region accounts: 162.247.240.0/22 Network blocks for EU region accounts: 185.221.84.0/22",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 359.02347,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>User</em>-facing domains",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em>",
        "body": " for <em>New</em> <em>Relic</em> One to work properly. Update your allow list to ensure <em>New</em> <em>Relic</em> can communicate with a number of integral domains that are listed in this section. Blocking domains can cause issues with individual <em>product</em> features or prevent pages from loading altogether. This list doesn&#x27;t cover domains"
      },
      "id": "603eb81364441f64a24e88b6"
    },
    {
      "sections": [
        "Install New Relic",
        "Install APM",
        "Install browser monitoring",
        "Install infrastructure monitoring",
        "Install mobile monitoring",
        "Install synthetic monitors",
        "Troubleshooting"
      ],
      "title": "Install New Relic ",
      "type": "docs",
      "tags": [
        "Using New Relic",
        "Cross-product functions",
        "Install and configure"
      ],
      "external_id": "819ccfd8df22ff322271245ca0831bf53609b91f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/new-relic-one/use-new-relic-one/cross-product-functions/install-configure/install-new-relic/",
      "published_at": "2022-01-08T06:43:39Z",
      "updated_at": "2021-12-30T20:03:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you sign up for a New Relic account (it's free, forever!) and install any of our monitoring services, you can start working with your data. Get started quickly with our New Relic Instant Observability quickstarts. Alternatively, use our guided install. Here are links to instructions on how to install New Relic monitoring services: APM Browser Infrastructure Mobile Synthetic monitors Integrations for third-party telemetry services Data ingest APIs (metrics, events, logs, traces) Install APM C Go Java .NET For Windows .NET applications on IIS, use our guided install in New Relic One to get started with APM. If you're on an EU server, use our launcher for EU accounts instead. Node.js PHP Python Ruby Install browser monitoring See browser monitoring install. Install infrastructure monitoring Linux guided install for infrastructure monitoring Linux procedures for infrastructure monitoring Windows Kubernetes Prometheus On-host integrations (for services like NGINX, StatsD, MySQL, etc.) AWS cloud integrations Azure cloud integrations Google Cloud Platform Install mobile monitoring Android iOS Install synthetic monitors Synthetic monitoring doesn't require installation, except for its private minions feature. Troubleshooting You should start seeing your data in the New Relic UI after installing the agent, generating some traffic, and waiting a few minutes. If no data appears, follow our troubleshooting procedures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 256.4958,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> <em>New</em> <em>Relic</em> ",
        "sections": "<em>Install</em> <em>New</em> <em>Relic</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em>",
        "body": "After you sign up for a <em>New</em> <em>Relic</em> account (it&#x27;s free, forever!) and <em>install</em> any of our monitoring services, you can start working with your data. Get started quickly with our <em>New</em> <em>Relic</em> Instant Observability quickstarts. Alternatively, <em>use</em> our guided <em>install</em>. Here are links to instructions on how"
      },
      "id": "61b8148ce7b9d22373ef3a8b"
    },
    {
      "sections": [
        "Uninstall New Relic agents",
        "Before you uninstall",
        "Important",
        "Uninstall APM",
        "Remove apps from UI",
        "Uninstall browser agent",
        "Uninstall infrastructure agent",
        "Delete account",
        "Uninstall other tools"
      ],
      "title": "Uninstall New Relic agents",
      "type": "docs",
      "tags": [
        "Using New Relic",
        "Cross-product functions",
        "Install and configure"
      ],
      "external_id": "15ae05c4370739bb72086bc5bd3ac2e00c438f09",
      "image": "",
      "url": "https://docs.newrelic.com/docs/new-relic-one/use-new-relic-one/cross-product-functions/install-configure/uninstall-agent/",
      "published_at": "2022-01-08T06:44:24Z",
      "updated_at": "2021-12-14T03:51:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some New Relic solutions require installation of an agent. You can uninstall the agent if you want it to stop reporting data. If you need to uninstall New Relic completely, follow the procedures for your agent. If you encounter problems, see support.newrelic.com. Before you uninstall Important For paying New Relic accounts, note that an alternative option to uninstalling is to downgrade your account to a less expensive or free tier. Uninstall APM After uninstalling APM agents or making a change to your startup script, you must restart your app (or the app server or web server where it is located). You must do this because New Relic agents typically run in the memory of the process running the app. Simply removing those files will not stop the applications that are currently running from sending data to New Relic. C SDK Go Java .NET Node.js PHP Python Ruby Remove apps from UI See Remove apps from UI. Uninstall browser agent Choose a procedure: Disable browser agent on specific pages Uninstall browser agent Uninstall infrastructure agent Choose a procedure: Uninstall infrastructure agent Uninstall a cloud or on-host integration Delete account See Downgrade/cancel your account. Uninstall other tools For uninstall instructions for solutions not listed here, see the docs for a specific New Relic solution.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 208.2097,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Uninstall <em>New</em> <em>Relic</em> agents",
        "sections": "Uninstall <em>New</em> <em>Relic</em> agents",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em>",
        "body": "Some <em>New</em> <em>Relic</em> solutions require installation of an agent. You can uninstall the agent if you want it to stop reporting data. If you need to uninstall <em>New</em> <em>Relic</em> completely, follow the procedures for your agent. If you encounter problems, see support.newrelic.com. Before you uninstall Important"
      },
      "id": "61c087d064441fd18799eba6"
    }
  ],
  "/docs/licenses/license-information/distributed-licenses/add-end-user-license-agreement": [
    {
      "sections": [
        "Services licenses",
        "Contents",
        "Java internal services",
        "Java crash data API",
        "CoffeeScript",
        "JavaScript",
        "Crash reporting",
        "Label services",
        "Infrastructure Monitoring services",
        ".NET support uploader service",
        "Ruby gems",
        "Storage services",
        "Go packages"
      ],
      "title": "Services licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "49a2ad450d31dacfc2aae690ca947d2326d18761",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/other-licenses/services-licenses/",
      "published_at": "2022-01-08T07:39:37Z",
      "updated_at": "2021-04-06T00:26:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Contents Java internal services Java crash data API Framework License Apache TomCat Apache 2.0 Bean Validation Apache 2.0 Cassandra Driver Core Apache 2.0 ClassMate Apache 2.0 Hamcrest BSD Mockito MIT Netty Apache 2.0 Objenesis MIT SnakeYAML Apache 2.0 Spring Boot Apache 2.0 Java internal services Library License json_simple Apache 2.0 newrelic-api New Relic newrelic-api New Relic antlr BSD aopalliance Public Domain asm-analysis BSD asm-commons BSD asm-tree BSD asm-util BSD asm BSD c3p0 EPLv1.0 cglib-nodep Apache 2.0 cglib Apache 2.0 cglib Apache 2.0 cal10n-api MIT logback-classic EPLv1.0 logback-core EPLv1.0 guava-jetty-service Apache 2.0 zkclient Apache 2.0 reporter-config-base Apache 2.0 reporter-config3 Apache 2.0 reporter-config Apache 2.0 aws-java-sdk-acm Apache 2.0 aws-java-sdk-api-gateway Apache 2.0 aws-java-sdk-applicationautoscaling Apache 2.0 aws-java-sdk-autoscaling Apache 2.0 aws-java-sdk-cloudformation Apache 2.0 aws-java-sdk-cloudfront Apache 2.0 aws-java-sdk-cloudhsm Apache 2.0 aws-java-sdk-cloudsearch Apache 2.0 aws-java-sdk-cloudtrail Apache 2.0 aws-java-sdk-cloudwatch Apache 2.0 aws-java-sdk-cloudwatchmetrics Apache 2.0 aws-java-sdk-codecommit Apache 2.0 aws-java-sdk-codedeploy Apache 2.0 aws-java-sdk-codepipeline Apache 2.0 aws-java-sdk-cognitoidentity Apache 2.0 aws-java-sdk-cognitoidp Apache 2.0 aws-java-sdk-cognitosync Apache 2.0 aws-java-sdk-config Apache 2.0 aws-java-sdk-core Apache 2.0 aws-java-sdk-datapipeline Apache 2.0 aws-java-sdk-devicefarm Apache 2.0 aws-java-sdk-directconnect Apache 2.0 aws-java-sdk-directory Apache 2.0 aws-java-sdk-discovery Apache 2.0 aws-java-sdk-dms Apache 2.0 aws-java-sdk-dynamodb Apache 2.0 aws-java-sdk-ec2 Apache 2.0 aws-java-sdk-ecr Apache 2.0 aws-java-sdk-ecs Apache 2.0 aws-java-sdk-efs Apache 2.0 aws-java-sdk-elasticache Apache 2.0 aws-java-sdk-elasticbeanstalk Apache 2.0 aws-java-sdk-elasticloadbalancing Apache 2.0 aws-java-sdk-elasticloadbalancingv2 Apache 2.0 aws-java-sdk-elasticsearch Apache 2.0 aws-java-sdk-elastictranscoder Apache 2.0 aws-java-sdk-emr Apache 2.0 aws-java-sdk-events Apache 2.0 aws-java-sdk-gamelift Apache 2.0 aws-java-sdk-glacier Apache 2.0 aws-java-sdk-iam Apache 2.0 aws-java-sdk-importexport Apache 2.0 aws-java-sdk-inspector Apache 2.0 aws-java-sdk-iot Apache 2.0 aws-java-sdk-kinesis Apache 2.0 aws-java-sdk-kms Apache 2.0 aws-java-sdk-lambda Apache 2.0 aws-java-sdk-logs Apache 2.0 aws-java-sdk-machinelearning Apache 2.0 aws-java-sdk-marketplacecommerceanalytics Apache 2.0 aws-java-sdk-marketplacemeteringservice Apache 2.0 aws-java-sdk-models Apache 2.0 aws-java-sdk-opsworks Apache 2.0 aws-java-sdk-rds Apache 2.0 aws-java-sdk-redshift Apache 2.0 aws-java-sdk-route53 Apache 2.0 aws-java-sdk-s3 Apache 2.0 aws-java-sdk-servicecatalog Apache 2.0 aws-java-sdk-ses Apache 2.0 aws-java-sdk-simpledb Apache 2.0 aws-java-sdk-simpleworkflow Apache 2.0 aws-java-sdk-snowball Apache 2.0 aws-java-sdk-sns Apache 2.0 aws-java-sdk-sqs Apache 2.0 aws-java-sdk-ssm Apache 2.0 aws-java-sdk-storagegateway Apache 2.0 aws-java-sdk-sts Apache 2.0 aws-java-sdk-support Apache 2.0 aws-java-sdk-swf-libraries Apache 2.0 aws-java-sdk-waf Apache 2.0 aws-java-sdk-workspaces Apache 2.0 aws-java-sdk Apache 2.0 jmespath-java Apache 2.0 AppleJavaExtensions BSD jcommander Apache 2.0 high-scale-lib MIT hppc Apache 2.0 clover Paid stream Apache 2.0 metrics-core Apache 2.0 cassandra-driver-core Apache 2.0 cassandra-driver-core Apache 2.0 uuid MIT grabbag MIT speed4j Apache 2.0 yamlbeans BSD jackson-annotations Apache 2.0 jackson-core Apache 2.0 jackson-databind Apache 2.0 jackson-dataformat-cbor Apache 2.0 jackson-dataformat-csv Apache 2.0 jackson-dataformat-xml Apache 2.0 jackson-dataformat-yaml Apache 2.0 jackson-datatype-guava Apache 2.0 jackson-datatype-jdk7 Apache 2.0 jackson-datatype-jdk8 Apache 2.0 jackson-datatype-joda Apache 2.0 jackson-datatype-jsr310 Apache 2.0 jackson-jaxrs-base Apache 2.0 jackson-jaxrs-json-provider Apache 2.0 jackson-module-afterburner Apache 2.0 jackson-module-jaxb-annotations Apache 2.0 jackson-module-kotlin Apache 2.0 woodstox-core Apache 2.0 classmate Apache 2.0 zjsonpatch Apache 2.0 caffeine Apache 2.0 waffle-jna EPLv1.0 docker-java Apache 2.0 docker-java Apache 2.0 btf Apache 2.0 jackson-coreutils Apache 2.0 json-schema-core Apache 2.0 json-schema-validator Apache 2.0 msg-simple Apache 2.0 uri-template Apache 2.0 jamm Apache 2.0 jffi Apache 2.0 jnr-constants Apache 2.0 jnr-ffi Apache 2.0 jnr-ffi Apache 2.0 jnr-posix EPLv1.0 jnr-x86asm MIT embedded-redis Apache 2.0 memoryfilesystem MIT guava-retrying Apache 2.0 guava-retrying Apache 2.0 snowball-stemmer BSD system-rules CPL uuid Apache 2.0 wiremock Apache 2.0 auto-common Apache 2.0 annotations LGPLv2.1 annotations LGPLv2.1 bcel-findbugs LGPLv2.1 findbugs LGPLv2.1 jFormatString BSD jFormatString BSD jsr305 BSD jsr305 BSD jsr305 BSD gson Apache 2.0 spymemcached MIT guava-testlib Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guice-multibindings Apache 2.0 guice Apache 2.0 truth Apache 2.0 concurrent-trees Apache 2.0 concurrentlinkedhashmap-lru Apache 2.0 json-simple Apache 2.0 libphonenumber Apache 2.0 locality-uuid BSD h2 EPLv1.0 annotations Apache 2.0 json-path-assert Apache 2.0 json-path Apache 2.0 json-path Apache 2.0 jzlib BSD bonecp-provider Apache 2.0 bonecp Apache 2.0 junixsocket-common Apache 2.0 junixsocket-native-common Apache 2.0 disruptor Apache 2.0 geoip-api LGPLv2.1 c3p0 LGPLv2.1 mchange-commons-java LGPLv2.1 archaius-core Apache 2.0 hystrix-core Apache 2.0 hystrix-servo-metrics-publisher Apache 2.0 rxjava-core Apache 2.0 servo-core Apache 2.0 kafka-clients Apache 2.0 kafka_2.10 Apache 2.0 kafka_2.11 Apache 2.0 common-cassandra Apache 2.0 timeslice_utils New Relic mockito-kotlin MIT compress-lzf Apache 2.0 checkstyle Apache 2.0 dagger-compiler Apache 2.0 dagger Apache 2.0 okhttp Apache 2.0 okio Apache 2.0 javapoet Apache 2.0 javawriter Apache 2.0 jaxb-impl CDDLv1 thrift-server Apache 2.0 finagle-core_2.10 Apache 2.0 finagle-core_2.11 Apache 2.0 finagle-http_2.10 Apache 2.0 finagle-http_2.11 Apache 2.0 jsr166e Creative Commons util-app_2.10 Apache 2.0 util-app_2.11 Apache 2.0 util-cache_2.10 Apache 2.0 util-cache_2.11 Apache 2.0 util-codec_2.10 Apache 2.0 util-codec_2.11 Apache 2.0 util-collection_2.10 Apache 2.0 util-collection_2.11 Apache 2.0 util-core_2.10 Apache 2.0 util-core_2.11 Apache 2.0 util-function_2.10 Apache 2.0 util-function_2.11 Apache 2.0 util-hashing_2.10 Apache 2.0 util-hashing_2.11 Apache 2.0 util-jvm_2.10 Apache 2.0 util-jvm_2.11 Apache 2.0 util-lint_2.10 Apache 2.0 util-lint_2.11 Apache 2.0 util-logging_2.10 Apache 2.0 util-logging_2.11 Apache 2.0 util-registry_2.10 Apache 2.0 util-registry_2.11 Apache 2.0 util-stats_2.10 Apache 2.0 util-stats_2.11 Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 HikariCP Apache 2.0 commons-beanutils Apache 2.0 commons-cli Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-io Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 commons-pool Apache 2.0 unix-socket-factory Apache 2.0 unix-socket-factory Apache 2.0 jflex BSD dom4j BSD airline Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 metrics-healthchecks Apache 2.0 metrics-httpclient Apache 2.0 metrics-jdbi Apache 2.0 metrics-jersey2 Apache 2.0 metrics-jetty9 Apache 2.0 metrics-json Apache 2.0 metrics-jvm Apache 2.0 metrics-logback Apache 2.0 metrics-servlets Apache 2.0 dropwizard-java8-jdbi Apache 2.0 dropwizard-client Apache 2.0 dropwizard-configuration Apache 2.0 dropwizard-core Apache 2.0 dropwizard-db Apache 2.0 dropwizard-jackson Apache 2.0 dropwizard-jdbi Apache 2.0 dropwizard-jersey Apache 2.0 dropwizard-jetty Apache 2.0 dropwizard-lifecycle Apache 2.0 dropwizard-logging Apache 2.0 dropwizard-metrics Apache 2.0 dropwizard-servlets Apache 2.0 dropwizard-testing Apache 2.0 dropwizard-util Apache 2.0 dropwizard-validation Apache 2.0 netty-all Apache 2.0 netty-buffer Apache 2.0 netty-codec-http Apache 2.0 netty-codec-socks Apache 2.0 netty-codec Apache 2.0 netty-common Apache 2.0 netty-handler-proxy Apache 2.0 netty-handler Apache 2.0 netty-resolver Apache 2.0 netty-transport-native-epoll Apache 2.0 netty-transport Apache 2.0 netty Apache 2.0 ratpack-core Apache 2.0 ratpack-groovy-test Apache 2.0 ratpack-groovy Apache 2.0 ratpack-guice Apache 2.0 ratpack-test Apache 2.0 rxjava Apache 2.0 fastutil Apache 2.0 janino BSD activation Apache 2.0 javax.annotation-api CDDLv1 jsr250-api CDDLv1 javax.inject Apache 2.0 mail CDDLv1 mailapi CDDLv1 javax.servlet-api Apache 2.0 servlet-api Apache 2.0 javax.transaction-api CDDLv1 validation-api Apache 2.0 javax.websocket-api CDDLv1 javax.ws.rs-api CDDLv1 jaxb-api CDDLv1 stax-api CDDLv1 jaxen Apache 2.0 jline BSD jline BSD joda-time Apache 2.0 junit-dep CPL junit CPL junit EPLv1.0 kafka_2.10 Apache 2.0 kafka_2.9.2 Apache 2.0 apache-log4j-extras Apache 2.0 log4j Apache 2.0 mysql-connector-java GPLv2 with Classpath Exception byte-buddy Apache 2.0 byte-buddy Apache 2.0 jna-platform LGPLv2.1 jna Apache 2.0 jcip-annotations Apache 2.0 lz4 Apache 2.0 accessors-smart Apache 2.0 asm Apache 2.0 json-smart Apache 2.0 json-smart Apache 2.0 primitive GPLv2 with Classpath Exception compiler Apache 2.0 lang Apache 2.0 beaver-cc BSD ehcache-core Apache 2.0 ehcache Apache 2.0 jopt-simple MIT jopt-simple MIT opencsv Apache 2.0 quality-check Apache 2.0 scannotation Apache 2.0 super-csv Apache 2.0 uadetector-core Apache 2.0 uadetector-resources Apache 2.0 argparse4j MIT spymemcached MIT kalium Apache 2.0 ST4 BSD antlr-runtime BSD antlr-runtime BSD antlr BSD stringtemplate BSD ant-launcher Apache 2.0 ant Apache 2.0 cassandra-all Apache 2.0 cassandra-all Apache 2.0 cassandra-thrift Apache 2.0 cassandra-thrift Apache 2.0 commons-collections4 Apache 2.0 commons-compress Apache 2.0 commons-csv Apache 2.0 commons-dbcp2 Apache 2.0 commons-io Apache 2.0 commons-lang3 Apache 2.0 commons-math3 Apache 2.0 commons-math Apache 2.0 commons-pool2 Apache 2.0 curator-client Apache 2.0 curator-framework Apache 2.0 curator-recipes Apache 2.0 curator-test Apache 2.0 derby Apache 2.0 geronimo-servlet_3.0_spec Apache 2.0 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 ivy Apache 2.0 kafka-clients Apache 2.0 kafka_2.11 Apache 2.0 libthrift Apache 2.0 tomcat-embed-core Apache 2.0 tomcat-embed-el Apache 2.0 tomcat-embed-logging-juli Apache 2.0 tomcat-embed-websocket Apache 2.0 tomcat-jdbc Apache 2.0 tomcat-juli Apache 2.0 zookeeper Apache 2.0 aspectjweaver EPLv1.0 assertj-core Apache 2.0 assertj-core Apache 2.0 evo-inflector Apache 2.0 bcmail-jdk15on MIT bcpkix-jdk15on MIT bcprov-jdk15on MIT ohc-core Apache 2.0 cassandra-unit-spring LGPLv3 cassandra-unit-spring LGPLv3 cassandra-unit LGPLv3 cassandra-unit LGPLv3 groovy-all Apache 2.0 groovy-all Apache 2.0 groovy Apache 2.0 groovy Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 easymock Apache 2.0 ecj EPLv1.0 javax.activation CDDLv1 javax.annotation CDDLv1 javax.mail.glassfish CDDLv1 javax.servlet CDDLv1 javax.transaction CDDLv1 org.objectweb.asm BSD jetty-setuid-java EPLv1.0 jetty-schemas Apache 2.0 javax-websocket-client-impl Apache 2.0 javax-websocket-server-impl Apache 2.0 websocket-api Apache 2.0 websocket-client Apache 2.0 websocket-common Apache 2.0 websocket-server Apache 2.0 websocket-servlet Apache 2.0 apache-jsp Apache 2.0 jetty-annotations CDDLv1 jetty-client Apache 2.0 jetty-continuation Apache 2.0 jetty-http Apache 2.0 jetty-io Apache 2.0 jetty-jmx Apache 2.0 jetty-jndi Apache 2.0 jetty-plus Apache 2.0 jetty-proxy Apache 2.0 jetty-security Apache 2.0 jetty-server Apache 2.0 jetty-servlet Apache 2.0 jetty-servlets Apache 2.0 jetty-util Apache 2.0 jetty-webapp Apache 2.0 jetty-xml Apache 2.0 sigar Apache 2.0 aopalliance-repackaged CDDLv1 javax.inject CDDLv1 hk2-api CDDLv1 hk2-locator CDDLv1 hk2-utils CDDLv1 osgi-resource-locator CDDLv1 jersey-guava CDDLv1 jersey-client CDDLv1 jersey-apache-connector CDDLv1 jersey-container-servlet-core CDDLv1 jersey-container-servlet CDDLv1 jersey-client CDDLv1 jersey-common CDDLv1 jersey-server CDDLv1 jersey-bean-validation CDDLv1 jersey-metainf-services CDDLv1 jersey-media-jaxb CDDLv1 jersey-client CDDLv1 jersey-test-framework-provider-inmemory CDDLv1 jersey-test-framework-core CDDLv1 javax.el CDDLv1 hamcrest-all BSD hamcrest-core BSD hamcrest-integration BSD hamcrest-library BSD HdrHistogram Public Domain hector-core MIT hibernate-commons-annotations LGPLv2.1 hibernate-jpa-2.0-api LGPLv2.1 hibernate-jpa-2.1-api LGPLv2.1 hibernate-c3p0 Apache 2.0 hibernate-core Apache 2.0 hibernate-ehcache Apache 2.0 hibernate-entitymanager Apache 2.0 hibernate-jmx Apache 2.0 hibernate-validator Apache 2.0 freebuilder Apache 2.0 org.jacoco.agent EPLv1.0 org.jacoco.ant EPLv1.0 org.jacoco.core EPLv1.0 org.jacoco.report EPLv1.0 javassist Apache 2.0 jboss-logging-annotations LGPLv2.1 jboss-logging Apache 2.0 jboss-transaction-api_1.1_spec LGPLv2.1 jboss-transaction-api_1.2_spec LGPLv2.1 jandex Apache 2.0 jdbi Apache 2.0 job-dsl-core Apache 2.0 version-number MIT kotlin-annotation-processing Apache 2.0 kotlin-reflect Apache 2.0 kotlin-runtime Apache 2.0 kotlin-stdlib Apache 2.0 kotlin-test-junit Apache 2.0 kotlin-test Apache 2.0 annotations Apache 2.0 jolokia-core Apache 2.0 jruby-complete LGPLv2.1 LGPLv2.1 json Public Domain json Apache 2.0 xstream BSD annotations Apache 2.0 mapdb Apache 2.0 mapdb Apache 2.0 jbcrypt ISC mockserver-client-java Apache 2.0 mockserver-core Apache 2.0 mockserver-logging Apache 2.0 mockserver-netty Apache 2.0 mockito-all MIT mockito-core MIT mockito-core MIT apache-el Apache 2.0 apache-jsp Apache 2.0 alpn-boot Apache 2.0 etcd4j Apache 2.0 msgpack Apache 2.0 objenesis Apache 2.0 objenesis Apache 2.0 jmh-core GPLv2 with Classpath Exception jmh-generator-annprocess GPLv2 with Classpath Exception jol-core GPLv2 with Classpath Exception asm-analysis BSD asm-analysis BSD asm-commons BSD asm-debug-all BSD asm-tree BSD asm-util BSD asm-util BSD asm BSD asm Public Domain postgresql BSD powermock-api-mockito Apache 2.0 powermock-api-support Apache 2.0 powermock-core Apache 2.0 powermock-module-junit4-common Apache 2.0 powermock-module-junit4 Apache 2.0 powermock-reflect Apache 2.0 lombok MIT reactive-streams Public Domain reflections WTFPL scala-java8-compat_2.11 Scala scala-parser-combinators_2.11 Scala scala-xml_2.11 Scala scala-library Scala scala-reflect Scala scalatest_2.10 Apache 2.0 scalatest_2.11 Apache 2.0 native-lib-loader BSD jsonassert Apache 2.0 jcl-over-slf4j MIT jul-to-slf4j MIT log4j-log4j12 MIT log4j-over-slf4j MIT slf4j-api MIT slf4j-ext MIT slf4j-log4j12 MIT slf4j-simple MIT spock-core Apache 2.0 spock-core Apache 2.0 spring-boot-actuator Apache 2.0 spring-boot-autoconfigure Apache 2.0 spring-boot-configuration-processor Apache 2.0 spring-boot-starter-actuator Apache 2.0 spring-boot-starter-aop Apache 2.0 spring-boot-starter-data-jpa Apache 2.0 spring-boot-starter-data-rest Apache 2.0 spring-boot-starter-jdbc Apache 2.0 spring-boot-starter-jetty Apache 2.0 spring-boot-starter-logging Apache 2.0 spring-boot-starter-test Apache 2.0 spring-boot-starter-tomcat Apache 2.0 spring-boot-starter-web Apache 2.0 spring-boot-starter Apache 2.0 spring-boot-test-autoconfigure Apache 2.0 spring-boot-test Apache 2.0 spring-boot Apache 2.0 spring-data-commons Apache 2.0 spring-data-jpa Apache 2.0 spring-data-rest-core Apache 2.0 spring-data-rest-webmvc Apache 2.0 spring-hateoas Apache 2.0 spring-plugin-core Apache 2.0 spring-aop Apache 2.0 spring-aspects Apache 2.0 spring-beans Apache 2.0 spring-context-support Apache 2.0 spring-context Apache 2.0 spring-core Apache 2.0 spring-core Apache 2.0 spring-expression Apache 2.0 spring-jdbc Apache 2.0 spring-orm Apache 2.0 spring-test Apache 2.0 spring-test Apache 2.0 spring-tx Apache 2.0 spring-web Apache 2.0 spring-webmvc Apache 2.0 xz Public Domain wasabi Apache 2.0 snappy-java Apache 2.0 xmlunit-core Apache 2.0 xmlunit-legacy Apache 2.0 snakeyaml Apache 2.0 JUnitParams Apache 2.0 postgresql BSD postgresql BSD jedis MIT scala-library Scala ion-java Apache 2.0 timeslice_service-thrift New Relic timeslice_service New Relic xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3_min Public Domain zkclient Apache 2.0 zookeeper Apache 2.0 CoffeeScript Library License CoffeeScript MIT coffin MIT JavaScript JavaScript License ie_html5/ie_html5.js MIT Angular.js MIT Angular UI directives for Bootstrap MIT Chart.js MIT Chosen JQuery plugin MIT Fullcalendar MIT Moment MIT Crash reporting Software License Apple OS X OS X Yosemite Xcode & Apple SDK Xcode license Label services Service License BoneCP Apache 2.0 Gradle Apache 2.0 Hibernate Apache 2.0 Spring Apache 2.0 Infrastructure Monitoring services Service License antlr BSD aopalliance Public Domain asm MIT cglib Apache 2.0 ch.qos.logback EPLv1.0 com.101tec Apache 2.0 com.amazonaws Apache 2.0 com.cenqua.clover Paid com.fasterxml.jackson.core Apache 2.0 com.fasterxml Apache 2.0 com.google.code.findbugs BSD com.google.guava Apache 2.0 com.googlecode.json-simple Apache 2.0 com.intellij Apache 2.0 New Relic com.squareup.dagger Apache 2.0 com.squareup Apache 2.0 com.yammer.metrics Apache 2.0 commons-codec Apache 2.0 commons-configuration Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 dom4j BSD io.netty Apache 2.0 javax.annotation CDDLv1 javax.inject Apache 2.0 javax.servlet.jsp CDDLv1 javax.servlet Apache 2.0 javax.validation Apache 2.0 javax.websocket CDDLv1 jline BSD joda-time Apache 2.0 junit CPL log4j Apache 2.0 mysql GPLv2 with Classpath Exception net.sf.jopt-simple MIT org.apache.commons Apache 2.0 org.apache.httpcomponents Apache 2.0 org.apache.ivy Apache 2.0 org.apache.thrift Apache 2.0 org.apache.tomcat.embed Apache 2.0 org.apache.zookeeper Apache 2.0 org.codehaus.groovy Apache 2.0 org.codehaus.jackson Apache 2.0 org.easymock Apache 2.0 org.eclipse.jetty.orbit CDDLv1 org.eclipse.jetty.toolchain Apache 2.0 org.eclipse.jetty.websocket Apache 2.0 org.eclipse.jetty CDDLv1 org.glassfish.jersey.core CDDLv1 org.glassfish.web CDDLv1 org.glassfish CDDLv1 org.hamcrest BSD org.hibernate Apache 2.0 org.javassist Apache 2.0 org.jboss.logging LGPLv2.1 org.jenkins-ci.plugins Apache 2.0 org.jenkins-ci Apache 2.0 org.jetbrains Apache 2.0 org.jolokia Apache 2.0 org.jvnet.hudson BSD org.mockito MIT org.objenesis Apache 2.0 org.ow2.asm MIT org.reflections WTFPL org.scala-lang Scala org.slf4j MIT org.springframework.boot Apache 2.0 org.springframework Apache 2.0 org.xerial.snappy Apache 2.0 org.yaml Apache 2.0 xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3 Public Domain .NET support uploader service Library License Amazon AWS SDK for Java Apache 2.0 Apache Commons Codec Apache 2.0 Gson Apache 2.0 Jetty Apache 2.0 Ruby gems Ruby gems Gem License aasm MIT actionmailer MIT actionpack MIT activeadmin MIT actionview MIT activejob MIT active_model_serializers MIT activemodel MIT activerecord MIT activerecord-deprecated_finders MIT activerecord-mysql2-adapter MIT activeresource MIT activesupport MIT addressable Apache 2.0 angularjs-rails MIT arbre MIT arel MIT arel MIT atomic Apache 2.0 atomic Apache-2.0 attr_required MIT awesome_print MIT backports MIT better_errors MIT binding_of_caller MIT bourbon MIT brwsr MIT browser-timezone-rails MIT builder MIT bundler MIT byebug MIT callsite MIT capistrano MIT capistrano-bundler MIT capistrano-ext MIT capistrano-rbenv MIT capistrano-stats MIT capybara MIT celluloid MIT CFPropertyList MIT chunky_png MIT ci_reporter MIT ci_reporter_rspec MIT clockwork MIT codeclimate-test-reporter MIT coderay MIT coderay MIT coffee-rails MIT coffee-script MIT coffee-script-source MIT columnize Ruby compass MIT compass-core MIT compass-import-once MIT compass-rails MIT concurrent-ruby MIT crack MIT crypt Unlicensed daemons MIT dalli MIT database_cleaner MIT debug_inspector MIT debugger-linecache MIT diff-lcs MIT docile MIT dotenv MIT dotenv-deployment MIT erubis MIT etcd MIT ethon MIT eventmachine Ruby excon MIT execjs MIT factory_girl MIT factory_girl_rails MIT faraday MIT faraday-middleware MIT faye-websocket MIT ffi MIT ffi BSD fission MIT fog MIT fog-atmos MIT fog-aws MIT fog-brightbox MIT fog-core MIT fog-ecloud MIT fog-google MIT fog-json MIT fog-local MIT fog-powerdns MIT fog-profitbricks MIT fog-radosgw MIT fog-riakcs MIT fog-sakuracloud MIT fog-serverlove MIT fog-softlayer MIT fog-storm_on_demand MIT fog-terremark MIT fog-vmfusion MIT fog-voxel MIT fog-xml MIT font-awesome-rails MIT foreman MIT formatador MIT formtastic MIT formtastic i18n MIT fssm MIT globalid MIT guard MIT guard-rspec MIT haml MIT haml-rails MIT has scope MIT hashie MIT highline Ruby hike MIT hipchat-api MIT histogram MIT hitimes ISC hiredis BSD http_parser.rb MIT httpclient Ruby httpclient Ruby httparty MIT i18n MIT inflecto MIT inherited resources MIT intercom-rails MIT ipaddress MIT jasmine MIT jasmine-core MIT jasmine-rails MIT jbuilder MIT jira MIT jira-ruby OSL-3.0 journey MIT jquery-rails MIT jquery-turbolinks MIT jquery-ui-rails MIT json Ruby jwt MIT kaminari MIT kgio LGPLv3 kgio LGPLv3 kgio LGPL-v2.1+ libv8 MIT listen MIT logger Ruby loofah MIT lumberjack MIT macaddr MIT mail MIT mailcatcher MIT memoist MIT metaclass MIT meta_request MIT method_source MIT mime-types MIT mini_portile MIT minitest MIT minitest-rails MIT mixlib-log Apache 2.0 mocha MIT mono_logger MIT multi_json MIT multi_xml MIT multipart-post MIT mysql2 MIT net-http-persistent MIT net-scp MIT net-sftp MIT net-ssh MIT net-ssh-gateway MIT netrc MIT nokogiri MIT oauth2 MIT oj MIT omniauth MIT omniauth-oauth2 MIT pagerduty MIT papers MIT pg BSD pg Ruby phantomjs BSD phantomjs-binaries Unlicensed poltergeist MIT polyamorous MIT polyglot MIT polyglot MIT protected_attributes MIT pry MIT pry-nav MIT pry-rails MIT pry-stack_explorer MIT puma BSD quiet assets MIT rack MIT rack-cache MIT rack-contrib MIT rack-oauth2 MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-ssl MIT rack-ssl-enforcer MIT rack-test MIT rails MIT rails-deprecated_sanitizer MIT rails-dom-testing MIT rails-html-sanitizer MIT rails-observers MIT rails_12factor MIT rails_serve_static_assets MIT rails_stdout_logging MIT railties MIT raindrops LGPLv3 raindrops LGPLv3 raindrops LGPLv2.1+ rake MIT rake MIT rake MIT rake MIT ransack MIT rb-fsevent MIT rb-inotify MIT rdoc Ruby redcarpet MIT redis MIT redis-namespace MIT redis-queue MIT ref MIT remote_syslog_logger MIT request_store MIT responders MIT resque MIT resque-cleaner MIT resque-pool MIT resque-scheduler MIT resque-status MIT rest-client MIT restforce MIT rspec MIT rspec-core MIT rspec-expectations MIT rspec-mocks MIT rspec-rails MIT rspec-support MIT rspec_junit_formatter MIT rubyntlm MIT ruby-saml MIT rufus-scheduler MIT safe_yaml MIT salesforce_bulk_query BSD sass MIT sass-rails MIT sass-rails MIT sdoc MIT secure_headers Apache 2.0 sequel MIT serveza MIT settingslogic MIT shoulda MIT shoulda-context MIT shoulda-matchers MIT simplecov MIT simplecov-html MIT simplecov-rcov MIT sinatra MIT sinatra-activerecord MIT sinatra-contrib MIT skinny MIT slop MIT spring MIT sprockets MIT sprockets-rails MIT sqlite3 BSD sshkit GPL-No Distro sys-uname Ruby syslog_protocol MIT systemu BSDL term-ansicolor GPL-No Distro terminal-table MIT therubyracer MIT thin Ruby thor MIT thread_safe Apache-2.0 thrift Apache 2.0 thrift-rack MIT tilt MIT timecop MIT timers MIT tins MIT treetop MIT turbolinks MIT twitter-bootstrap-rails MIT typhoeus MIT tzinfo MIT uglifier MIT unicorn Ruby unicorn-rails MIT uuid MIT uuidtools Apache-2.0 vegas MIT webmock MIT web-console MIT xml-simple Ruby xpath MIT yard MIT Storage services Library License Amazon AWS SDK for Java Apache 2.0 Amazon AWS Command Line Interface Apache 2.0 Go packages Library License Go-Mysql-Driver Mozilla Public License 2.0 gocql BSD 3-Clause groupcache Apache 2.0 Migrate MIT go-sqlite3 MIT Revel MIT Pathtree MIT Ansicolor MIT fsnotify BSD 3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.20683,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Services <em>licenses</em>",
        "sections": "Services <em>licenses</em>",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Contents Java internal"
      },
      "id": "603ea24364441f91fb4e8864"
    },
    {
      "sections": [
        "New Relic Agent Software Notice"
      ],
      "title": "New Relic Agent Software Notice",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "2bf9501c2767105130d3808f1bf3a91a032d903e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/new-relic-agent-software-notice/",
      "published_at": "2022-01-08T09:14:04Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This software is © 2008-2021 New Relic, Inc. and its licensors. This software is solely for use with New Relic’s proprietary SaaS service (“New Relic Service”), so to use the software you must have a valid account for the New Relic Service under a separate agreement with New Relic (“Subscription Agreement”). You may only use the software to support your use of the New Relic Service as permitted in the Subscription Agreement. Without a Subscription Agreement, you may not use the software. All other use is prohibited. New Relic and its suppliers retain all right, title and interest (including intellectual property rights) in the software. The Subscription Agreement will control in event of a conflict with this notice. Unless otherwise agreed by New Relic in your Subscription Agreement: You may not use, copy, distribute or sublicense the software, use the software on behalf of third parties, reverse engineer or decompile the software, modify or create derivative works of the software, use the software for competitive analysis or benchmarking, or remove or obscure any proprietary notices in the software. The software is provided “AS IS” and New Relic disclaims all warranties, whether express, implied, statutory or otherwise, including warranties of merchantability, fitness for a particular purpose, title or noninfringement. To the full extent permitted by law, New Relic will have no liability arising from or related to the software or under this notice for any direct, indirect, special, incidental, or consequential damages of any kind, even if advised of their possibility in advance, and regardless of legal theory (whether contract, tort, negligence, strict liability or otherwise). The software may contain third-party open source software (“OSS”) as described here and at https://github.com/newrelic. To the extent required by the OSS license, that license will apply to the OSS when used on a stand-alone basis. For avoidance of doubt, you may copy and distribute New Relic agents pursuant to your Subscription Agreement for New Relic Browser and New Relic Mobile and as set forth: https://docs.newrelic.com/docs/mobile-monitoring/new-relic-mobile/get-started and https://docs.newrelic.com/docs/browser/new-relic-browser/installation/install-new-relic-browser-agent Software versions New Relic makes available under an OSS license (such as Apache 2.0) are governed by the terms of the applicable OSS license. For a current list of New Relic software versions released as OSS please visit https://opensource.newrelic.com/.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.76753,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": " and at https:&#x2F;&#x2F;github.com&#x2F;newrelic. To the extent required by the OSS <em>license</em>, that <em>license</em> will apply to the OSS when used on a stand-alone basis. For avoidance of doubt, you may copy and distribute New Relic agents pursuant to your Subscription Agreement for New Relic Browser and New Relic Mobile"
      },
      "id": "603eb73828ccbc1f99eba74a"
    },
    {
      "sections": [
        "FIT instrumentation end user license agreement"
      ],
      "title": "FIT instrumentation end user license agreement",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "e8e10baf120678407d08c9f78ab708d271cf0223",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/fit-instrumentation-end-user-license-agreement/",
      "published_at": "2022-01-08T09:14:46Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "NEW RELIC, INC. FIT INSTRUMENTATION END USER LICENSE AGREEMENT In connection with the work provided by New Relic’s Field Instrumentation Team, you may be provided with certain custom-created software to enable, optimize, or enhance your use of New Relic’s Services. By downloading, installing, authorizing installation, or using the FIT Instrumentation with your Underlying Software (defined below), you (“Customer”) agree to the terms and conditions herein (“Agreement”) with New Relic, Inc., a Delaware corporation with offices located at 188 Spear Street, Suite 1200 San Francisco, CA 94105 (“New Relic”), (collectively the “Parties”). Capitalized terms not defined herein shall have the meanings set forth in the New Relic Terms of Service (the “Terms of Service”) available here: http://newrelic.com/terms IF YOU DO NOT AGREE TO THIS AGREEMENT, PLEASE DO NOT USE THE FIT INSTRUMENTATION. 1. DEFINITIONS “FIT Instrumentation” means the New Relic custom-made software, including but not limited to connectors, extensions, and plugins, provided by New Relic to Customer to enable the Services to operate with certain third party or Customer software or systems (“Underlying Software”). For the purposes of the Terms of Service, the FIT Instrumentation shall be treated like an “Agent”, subject to the separate terms herein. 2. USE OF THE FIT INSTRUMENTATION 2.1 Software Evaluation License. Subject to the terms herein, New Relic grants to Customer a limited, non-exclusive, non-transferrable, non-sublicensable right to install, use, and configure the FIT Instrumentation solely as needed to enable the Services for internal evaluation purposes. New Relic reserves all rights and licenses not expressly granted herein. 2.2 Ownership. As between the Parties, New Relic owns all right, title, and interest to the FIT Instrumentation and Feedback, including but not limited to any intellectual property and proprietary rights therein. Customer retains all right, title, and interest in any Customer Data processed by the FIT Instrumentation in connection with the Services. 2.3 Feedback. Customer agrees to provide feedback, suggestions, ideas, requests or recommendations (“Feedback”) regarding the FIT Instrumentation, and hereby irrevocably assigns all intellectual property and proprietary rights it holds in the Feedback to New Relic. 2.4 Notice. Customer acknowledges and agrees that the FIT Instrumentation is made to work with the Underlying Software as configured at the time of creation and there is no guarantee that FIT Instrumentation will continue to work in the event Customer changes, replaces, upgrades versions of, updates, or otherwise changes the Underlying Software (an “Upgrade Event”). Customer shall hold New Relic harmless from any claims or damages arising from Customer’s Upgrade Event. 2.5 Restrictions. Customer will not: (i) use the FIT Instrumentation except as permitted hereunder; (ii) distribute, sell, sublicense, or otherwise transfer the FIT Instrumentation; (iii) decompile, disassemble or reverse engineer any software underlying the FIT Instrumentation; (iv) use the FIT Instrumentation to damage, disable, overburden or impair any New Relic server or network(s) connected to any New Relic server or interfere with any other party’s use and enjoyment of the Services; (v) use the FIT Instrumentation to defraud, defame, abuse, harass, stalk, threaten or infringe the rights of privacy or other intellectual property rights of others or otherwise violate any applicable law; (vi) circumvent or modify any security technologies included as part of the FIT Instrumentation; or (vii) attempt or permit any third party to do any of the foregoing. New Relic may suspend usage of the Services by any user, without notice, pending any investigation of misuse.  These restrictions may be supplemented or superceded (to the extent they conflict) by the New Relic Acceptable Use Policy as may be published and updated from time to time at https://docs.newrelic.com/docs/licenses/license-information/acceptable-use-policy/acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential Information” means: (a) the FIT Instrumentation, and any features, results or output produced by, and other information relating to the FIT Instrumentation (including, without limitation, all Feedback); and (b) any business or technical information of New Relic including but not limited to any technical information, research, development, know-how that a reasonable person would understand to be confidential. 3.2 Restrictions. Customer will not use or disclose any Confidential Information, except as necessary for the performance of this Agreement. Customer will use all reasonable efforts to protect Confidential Information from unauthorized use or disclosure, but in no event less than the efforts that it ordinarily uses with respect to its own proprietary information. Customer may disclose Confidential Information to those of its employees who have a bona fide need to know such Confidential Information for the performance of this Agreement; provided that each such employee first executes a written agreement that contains use and nondisclosure restrictions at least as protective as those set forth herein. Confidential Information shall not include any information that: (a) is or becomes generally known to the public through no fault or breach of this Agreement by Customer; (b) is rightfully known by Customer at the time of disclosure without an obligation of confidentiality; (c) is independently developed by Customer without access or use of any Confidential Information; or (d) is rightfully obtained from a third party without restriction on use or disclosure. 4. DISCLAIMER 4.1 DISCLAIMER. FIT INSTRUMENTATION IS PROVIDED AS-IS AND AS-AVAILABLE AND NEW RELIC DISCLAIMS AND MAKES NO WARRANTIES, EITHER EXPRESS OR IMPLIED, INCLUDING ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR NON-INFRINGEMENT AND WHETHER OR NOT ARISING THROUGH A COURSE OF DEALING. FIT INSTRUMENTATION IS NOT GUARANTEED TO BE ERROR-FREE, COMPATIBLE WITH THE UNDERLYING SOFTWARE. , OR THAT CUSTOMER WILL ACHIEVE ANY RESULTS FROM USE OF THE FIT INSTRUMENTATION THEREFROM. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OF IMPLIED WARRANTIES OR LIMITATIONS ON APPLICABLE STATUTORY RIGHTS OF A CONSUMER, SO THE ABOVE EXCLUSION AND LIMITATIONS MAY NOT APPLY TO THE CUSTOMER. 4.2 LIMITATION OF LIABILITY. TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE TO CUSTOMER OR ANY THIRD PARTY FOR DAMAGES OF ANY KIND, INCLUDING, WITHOUT LIMITATION, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE INSTALLATION, USE OR INABILITY TO USE THE FIT INSTRUMENTATION OR FOR ANY ERROR OR DEFECT IN THE FIT INSTRUMENTATION OR THE SERVICES, WHETHER SUCH LIABILITY ARISES FROM ANY CLAIM BASED UPON CONTRACT, WARRANTY, TORT, STRICT LIABILITY, OR OTHERWISE, WHETHER OR NOT NEW RELIC HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSS OR DAMAGE. NEW RELIC'S AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS AGREEMENT WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE PARTIES HAVE AGREED THAT THESE LIMITATIONS WILL SURVIVE AND APPLY EVEN IF ANY LIMITED REMEDY SPECIFIED IN THIS AGREEMENT IS FOUND TO HAVE FAILED OF ITS ESSENTIAL PURPOSE. 4.3 EVALUATION VERSION. Customer agrees and acknowledges that: (a) the FIT Instrumentation is not an official product and has not been commercially released for sale by New Relic; (b) the FIT Instrumentation may not operate properly, being in final form, or fully functional; (c) the FIT Instrumentation may contain errors, security vulnerabilities, design flaws, or other problems; (d) it may not be possible to make the FIT Instrumentation fully functional; (e) the information obtained using the FIT Instrumentation may not be accurate; (f) use of the FIT Instrumentation may result in unexpected results, loss of data, delays or other unpredictable damages or loss; (g) New Relic is under no obligation to release a commercial version of the FIT Instrumentation; and (h) New Relic has the right unilaterally to abandon development of the FIT Instrumentation, at any time and without any obligation or liability to Customer. 5. GENERAL PROVISIONS 5.1 Terms of Service; Documentation. This Agreement shall be considered a part of the New Relic documentation, located at: https://docs.newrelic.com (the \"Documentation\").This Agreement and the Terms of Service constitute the entire and exclusive agreement between New Relic and Customer with respect to the FIT Instrumentation. In the absence of a separate agreement between New Relic and Customer with respect to the FIT Instrumentation, the terms and conditions of this Agreement shall govern the relationship between New Relic and Customer with respect to such FIT Instrumentation.  To the extent of a conflict between the Agreement and the Terms of Service, this Agreement shall govern with respect to the FIT Instrumentation only.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.76753,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "FIT instrumentation end user <em>license</em> agreement",
        "sections": "FIT instrumentation end user <em>license</em> agreement",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": " Acceptable Use Policy as may be published and updated from time to time at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;acceptable-use-policy&#x2F;acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential <em>Information</em>” means: (a) the FIT Instrumentation, and any features, results"
      },
      "id": "603e9f3ee7b9d206e32a0800"
    }
  ],
  "/docs/licenses/license-information/distributed-licenses/fit-instrumentation-end-user-license-agreement": [
    {
      "sections": [
        "Services licenses",
        "Contents",
        "Java internal services",
        "Java crash data API",
        "CoffeeScript",
        "JavaScript",
        "Crash reporting",
        "Label services",
        "Infrastructure Monitoring services",
        ".NET support uploader service",
        "Ruby gems",
        "Storage services",
        "Go packages"
      ],
      "title": "Services licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "49a2ad450d31dacfc2aae690ca947d2326d18761",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/other-licenses/services-licenses/",
      "published_at": "2022-01-08T07:39:37Z",
      "updated_at": "2021-04-06T00:26:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Contents Java internal services Java crash data API Framework License Apache TomCat Apache 2.0 Bean Validation Apache 2.0 Cassandra Driver Core Apache 2.0 ClassMate Apache 2.0 Hamcrest BSD Mockito MIT Netty Apache 2.0 Objenesis MIT SnakeYAML Apache 2.0 Spring Boot Apache 2.0 Java internal services Library License json_simple Apache 2.0 newrelic-api New Relic newrelic-api New Relic antlr BSD aopalliance Public Domain asm-analysis BSD asm-commons BSD asm-tree BSD asm-util BSD asm BSD c3p0 EPLv1.0 cglib-nodep Apache 2.0 cglib Apache 2.0 cglib Apache 2.0 cal10n-api MIT logback-classic EPLv1.0 logback-core EPLv1.0 guava-jetty-service Apache 2.0 zkclient Apache 2.0 reporter-config-base Apache 2.0 reporter-config3 Apache 2.0 reporter-config Apache 2.0 aws-java-sdk-acm Apache 2.0 aws-java-sdk-api-gateway Apache 2.0 aws-java-sdk-applicationautoscaling Apache 2.0 aws-java-sdk-autoscaling Apache 2.0 aws-java-sdk-cloudformation Apache 2.0 aws-java-sdk-cloudfront Apache 2.0 aws-java-sdk-cloudhsm Apache 2.0 aws-java-sdk-cloudsearch Apache 2.0 aws-java-sdk-cloudtrail Apache 2.0 aws-java-sdk-cloudwatch Apache 2.0 aws-java-sdk-cloudwatchmetrics Apache 2.0 aws-java-sdk-codecommit Apache 2.0 aws-java-sdk-codedeploy Apache 2.0 aws-java-sdk-codepipeline Apache 2.0 aws-java-sdk-cognitoidentity Apache 2.0 aws-java-sdk-cognitoidp Apache 2.0 aws-java-sdk-cognitosync Apache 2.0 aws-java-sdk-config Apache 2.0 aws-java-sdk-core Apache 2.0 aws-java-sdk-datapipeline Apache 2.0 aws-java-sdk-devicefarm Apache 2.0 aws-java-sdk-directconnect Apache 2.0 aws-java-sdk-directory Apache 2.0 aws-java-sdk-discovery Apache 2.0 aws-java-sdk-dms Apache 2.0 aws-java-sdk-dynamodb Apache 2.0 aws-java-sdk-ec2 Apache 2.0 aws-java-sdk-ecr Apache 2.0 aws-java-sdk-ecs Apache 2.0 aws-java-sdk-efs Apache 2.0 aws-java-sdk-elasticache Apache 2.0 aws-java-sdk-elasticbeanstalk Apache 2.0 aws-java-sdk-elasticloadbalancing Apache 2.0 aws-java-sdk-elasticloadbalancingv2 Apache 2.0 aws-java-sdk-elasticsearch Apache 2.0 aws-java-sdk-elastictranscoder Apache 2.0 aws-java-sdk-emr Apache 2.0 aws-java-sdk-events Apache 2.0 aws-java-sdk-gamelift Apache 2.0 aws-java-sdk-glacier Apache 2.0 aws-java-sdk-iam Apache 2.0 aws-java-sdk-importexport Apache 2.0 aws-java-sdk-inspector Apache 2.0 aws-java-sdk-iot Apache 2.0 aws-java-sdk-kinesis Apache 2.0 aws-java-sdk-kms Apache 2.0 aws-java-sdk-lambda Apache 2.0 aws-java-sdk-logs Apache 2.0 aws-java-sdk-machinelearning Apache 2.0 aws-java-sdk-marketplacecommerceanalytics Apache 2.0 aws-java-sdk-marketplacemeteringservice Apache 2.0 aws-java-sdk-models Apache 2.0 aws-java-sdk-opsworks Apache 2.0 aws-java-sdk-rds Apache 2.0 aws-java-sdk-redshift Apache 2.0 aws-java-sdk-route53 Apache 2.0 aws-java-sdk-s3 Apache 2.0 aws-java-sdk-servicecatalog Apache 2.0 aws-java-sdk-ses Apache 2.0 aws-java-sdk-simpledb Apache 2.0 aws-java-sdk-simpleworkflow Apache 2.0 aws-java-sdk-snowball Apache 2.0 aws-java-sdk-sns Apache 2.0 aws-java-sdk-sqs Apache 2.0 aws-java-sdk-ssm Apache 2.0 aws-java-sdk-storagegateway Apache 2.0 aws-java-sdk-sts Apache 2.0 aws-java-sdk-support Apache 2.0 aws-java-sdk-swf-libraries Apache 2.0 aws-java-sdk-waf Apache 2.0 aws-java-sdk-workspaces Apache 2.0 aws-java-sdk Apache 2.0 jmespath-java Apache 2.0 AppleJavaExtensions BSD jcommander Apache 2.0 high-scale-lib MIT hppc Apache 2.0 clover Paid stream Apache 2.0 metrics-core Apache 2.0 cassandra-driver-core Apache 2.0 cassandra-driver-core Apache 2.0 uuid MIT grabbag MIT speed4j Apache 2.0 yamlbeans BSD jackson-annotations Apache 2.0 jackson-core Apache 2.0 jackson-databind Apache 2.0 jackson-dataformat-cbor Apache 2.0 jackson-dataformat-csv Apache 2.0 jackson-dataformat-xml Apache 2.0 jackson-dataformat-yaml Apache 2.0 jackson-datatype-guava Apache 2.0 jackson-datatype-jdk7 Apache 2.0 jackson-datatype-jdk8 Apache 2.0 jackson-datatype-joda Apache 2.0 jackson-datatype-jsr310 Apache 2.0 jackson-jaxrs-base Apache 2.0 jackson-jaxrs-json-provider Apache 2.0 jackson-module-afterburner Apache 2.0 jackson-module-jaxb-annotations Apache 2.0 jackson-module-kotlin Apache 2.0 woodstox-core Apache 2.0 classmate Apache 2.0 zjsonpatch Apache 2.0 caffeine Apache 2.0 waffle-jna EPLv1.0 docker-java Apache 2.0 docker-java Apache 2.0 btf Apache 2.0 jackson-coreutils Apache 2.0 json-schema-core Apache 2.0 json-schema-validator Apache 2.0 msg-simple Apache 2.0 uri-template Apache 2.0 jamm Apache 2.0 jffi Apache 2.0 jnr-constants Apache 2.0 jnr-ffi Apache 2.0 jnr-ffi Apache 2.0 jnr-posix EPLv1.0 jnr-x86asm MIT embedded-redis Apache 2.0 memoryfilesystem MIT guava-retrying Apache 2.0 guava-retrying Apache 2.0 snowball-stemmer BSD system-rules CPL uuid Apache 2.0 wiremock Apache 2.0 auto-common Apache 2.0 annotations LGPLv2.1 annotations LGPLv2.1 bcel-findbugs LGPLv2.1 findbugs LGPLv2.1 jFormatString BSD jFormatString BSD jsr305 BSD jsr305 BSD jsr305 BSD gson Apache 2.0 spymemcached MIT guava-testlib Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guice-multibindings Apache 2.0 guice Apache 2.0 truth Apache 2.0 concurrent-trees Apache 2.0 concurrentlinkedhashmap-lru Apache 2.0 json-simple Apache 2.0 libphonenumber Apache 2.0 locality-uuid BSD h2 EPLv1.0 annotations Apache 2.0 json-path-assert Apache 2.0 json-path Apache 2.0 json-path Apache 2.0 jzlib BSD bonecp-provider Apache 2.0 bonecp Apache 2.0 junixsocket-common Apache 2.0 junixsocket-native-common Apache 2.0 disruptor Apache 2.0 geoip-api LGPLv2.1 c3p0 LGPLv2.1 mchange-commons-java LGPLv2.1 archaius-core Apache 2.0 hystrix-core Apache 2.0 hystrix-servo-metrics-publisher Apache 2.0 rxjava-core Apache 2.0 servo-core Apache 2.0 kafka-clients Apache 2.0 kafka_2.10 Apache 2.0 kafka_2.11 Apache 2.0 common-cassandra Apache 2.0 timeslice_utils New Relic mockito-kotlin MIT compress-lzf Apache 2.0 checkstyle Apache 2.0 dagger-compiler Apache 2.0 dagger Apache 2.0 okhttp Apache 2.0 okio Apache 2.0 javapoet Apache 2.0 javawriter Apache 2.0 jaxb-impl CDDLv1 thrift-server Apache 2.0 finagle-core_2.10 Apache 2.0 finagle-core_2.11 Apache 2.0 finagle-http_2.10 Apache 2.0 finagle-http_2.11 Apache 2.0 jsr166e Creative Commons util-app_2.10 Apache 2.0 util-app_2.11 Apache 2.0 util-cache_2.10 Apache 2.0 util-cache_2.11 Apache 2.0 util-codec_2.10 Apache 2.0 util-codec_2.11 Apache 2.0 util-collection_2.10 Apache 2.0 util-collection_2.11 Apache 2.0 util-core_2.10 Apache 2.0 util-core_2.11 Apache 2.0 util-function_2.10 Apache 2.0 util-function_2.11 Apache 2.0 util-hashing_2.10 Apache 2.0 util-hashing_2.11 Apache 2.0 util-jvm_2.10 Apache 2.0 util-jvm_2.11 Apache 2.0 util-lint_2.10 Apache 2.0 util-lint_2.11 Apache 2.0 util-logging_2.10 Apache 2.0 util-logging_2.11 Apache 2.0 util-registry_2.10 Apache 2.0 util-registry_2.11 Apache 2.0 util-stats_2.10 Apache 2.0 util-stats_2.11 Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 HikariCP Apache 2.0 commons-beanutils Apache 2.0 commons-cli Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-io Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 commons-pool Apache 2.0 unix-socket-factory Apache 2.0 unix-socket-factory Apache 2.0 jflex BSD dom4j BSD airline Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 metrics-healthchecks Apache 2.0 metrics-httpclient Apache 2.0 metrics-jdbi Apache 2.0 metrics-jersey2 Apache 2.0 metrics-jetty9 Apache 2.0 metrics-json Apache 2.0 metrics-jvm Apache 2.0 metrics-logback Apache 2.0 metrics-servlets Apache 2.0 dropwizard-java8-jdbi Apache 2.0 dropwizard-client Apache 2.0 dropwizard-configuration Apache 2.0 dropwizard-core Apache 2.0 dropwizard-db Apache 2.0 dropwizard-jackson Apache 2.0 dropwizard-jdbi Apache 2.0 dropwizard-jersey Apache 2.0 dropwizard-jetty Apache 2.0 dropwizard-lifecycle Apache 2.0 dropwizard-logging Apache 2.0 dropwizard-metrics Apache 2.0 dropwizard-servlets Apache 2.0 dropwizard-testing Apache 2.0 dropwizard-util Apache 2.0 dropwizard-validation Apache 2.0 netty-all Apache 2.0 netty-buffer Apache 2.0 netty-codec-http Apache 2.0 netty-codec-socks Apache 2.0 netty-codec Apache 2.0 netty-common Apache 2.0 netty-handler-proxy Apache 2.0 netty-handler Apache 2.0 netty-resolver Apache 2.0 netty-transport-native-epoll Apache 2.0 netty-transport Apache 2.0 netty Apache 2.0 ratpack-core Apache 2.0 ratpack-groovy-test Apache 2.0 ratpack-groovy Apache 2.0 ratpack-guice Apache 2.0 ratpack-test Apache 2.0 rxjava Apache 2.0 fastutil Apache 2.0 janino BSD activation Apache 2.0 javax.annotation-api CDDLv1 jsr250-api CDDLv1 javax.inject Apache 2.0 mail CDDLv1 mailapi CDDLv1 javax.servlet-api Apache 2.0 servlet-api Apache 2.0 javax.transaction-api CDDLv1 validation-api Apache 2.0 javax.websocket-api CDDLv1 javax.ws.rs-api CDDLv1 jaxb-api CDDLv1 stax-api CDDLv1 jaxen Apache 2.0 jline BSD jline BSD joda-time Apache 2.0 junit-dep CPL junit CPL junit EPLv1.0 kafka_2.10 Apache 2.0 kafka_2.9.2 Apache 2.0 apache-log4j-extras Apache 2.0 log4j Apache 2.0 mysql-connector-java GPLv2 with Classpath Exception byte-buddy Apache 2.0 byte-buddy Apache 2.0 jna-platform LGPLv2.1 jna Apache 2.0 jcip-annotations Apache 2.0 lz4 Apache 2.0 accessors-smart Apache 2.0 asm Apache 2.0 json-smart Apache 2.0 json-smart Apache 2.0 primitive GPLv2 with Classpath Exception compiler Apache 2.0 lang Apache 2.0 beaver-cc BSD ehcache-core Apache 2.0 ehcache Apache 2.0 jopt-simple MIT jopt-simple MIT opencsv Apache 2.0 quality-check Apache 2.0 scannotation Apache 2.0 super-csv Apache 2.0 uadetector-core Apache 2.0 uadetector-resources Apache 2.0 argparse4j MIT spymemcached MIT kalium Apache 2.0 ST4 BSD antlr-runtime BSD antlr-runtime BSD antlr BSD stringtemplate BSD ant-launcher Apache 2.0 ant Apache 2.0 cassandra-all Apache 2.0 cassandra-all Apache 2.0 cassandra-thrift Apache 2.0 cassandra-thrift Apache 2.0 commons-collections4 Apache 2.0 commons-compress Apache 2.0 commons-csv Apache 2.0 commons-dbcp2 Apache 2.0 commons-io Apache 2.0 commons-lang3 Apache 2.0 commons-math3 Apache 2.0 commons-math Apache 2.0 commons-pool2 Apache 2.0 curator-client Apache 2.0 curator-framework Apache 2.0 curator-recipes Apache 2.0 curator-test Apache 2.0 derby Apache 2.0 geronimo-servlet_3.0_spec Apache 2.0 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 ivy Apache 2.0 kafka-clients Apache 2.0 kafka_2.11 Apache 2.0 libthrift Apache 2.0 tomcat-embed-core Apache 2.0 tomcat-embed-el Apache 2.0 tomcat-embed-logging-juli Apache 2.0 tomcat-embed-websocket Apache 2.0 tomcat-jdbc Apache 2.0 tomcat-juli Apache 2.0 zookeeper Apache 2.0 aspectjweaver EPLv1.0 assertj-core Apache 2.0 assertj-core Apache 2.0 evo-inflector Apache 2.0 bcmail-jdk15on MIT bcpkix-jdk15on MIT bcprov-jdk15on MIT ohc-core Apache 2.0 cassandra-unit-spring LGPLv3 cassandra-unit-spring LGPLv3 cassandra-unit LGPLv3 cassandra-unit LGPLv3 groovy-all Apache 2.0 groovy-all Apache 2.0 groovy Apache 2.0 groovy Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 easymock Apache 2.0 ecj EPLv1.0 javax.activation CDDLv1 javax.annotation CDDLv1 javax.mail.glassfish CDDLv1 javax.servlet CDDLv1 javax.transaction CDDLv1 org.objectweb.asm BSD jetty-setuid-java EPLv1.0 jetty-schemas Apache 2.0 javax-websocket-client-impl Apache 2.0 javax-websocket-server-impl Apache 2.0 websocket-api Apache 2.0 websocket-client Apache 2.0 websocket-common Apache 2.0 websocket-server Apache 2.0 websocket-servlet Apache 2.0 apache-jsp Apache 2.0 jetty-annotations CDDLv1 jetty-client Apache 2.0 jetty-continuation Apache 2.0 jetty-http Apache 2.0 jetty-io Apache 2.0 jetty-jmx Apache 2.0 jetty-jndi Apache 2.0 jetty-plus Apache 2.0 jetty-proxy Apache 2.0 jetty-security Apache 2.0 jetty-server Apache 2.0 jetty-servlet Apache 2.0 jetty-servlets Apache 2.0 jetty-util Apache 2.0 jetty-webapp Apache 2.0 jetty-xml Apache 2.0 sigar Apache 2.0 aopalliance-repackaged CDDLv1 javax.inject CDDLv1 hk2-api CDDLv1 hk2-locator CDDLv1 hk2-utils CDDLv1 osgi-resource-locator CDDLv1 jersey-guava CDDLv1 jersey-client CDDLv1 jersey-apache-connector CDDLv1 jersey-container-servlet-core CDDLv1 jersey-container-servlet CDDLv1 jersey-client CDDLv1 jersey-common CDDLv1 jersey-server CDDLv1 jersey-bean-validation CDDLv1 jersey-metainf-services CDDLv1 jersey-media-jaxb CDDLv1 jersey-client CDDLv1 jersey-test-framework-provider-inmemory CDDLv1 jersey-test-framework-core CDDLv1 javax.el CDDLv1 hamcrest-all BSD hamcrest-core BSD hamcrest-integration BSD hamcrest-library BSD HdrHistogram Public Domain hector-core MIT hibernate-commons-annotations LGPLv2.1 hibernate-jpa-2.0-api LGPLv2.1 hibernate-jpa-2.1-api LGPLv2.1 hibernate-c3p0 Apache 2.0 hibernate-core Apache 2.0 hibernate-ehcache Apache 2.0 hibernate-entitymanager Apache 2.0 hibernate-jmx Apache 2.0 hibernate-validator Apache 2.0 freebuilder Apache 2.0 org.jacoco.agent EPLv1.0 org.jacoco.ant EPLv1.0 org.jacoco.core EPLv1.0 org.jacoco.report EPLv1.0 javassist Apache 2.0 jboss-logging-annotations LGPLv2.1 jboss-logging Apache 2.0 jboss-transaction-api_1.1_spec LGPLv2.1 jboss-transaction-api_1.2_spec LGPLv2.1 jandex Apache 2.0 jdbi Apache 2.0 job-dsl-core Apache 2.0 version-number MIT kotlin-annotation-processing Apache 2.0 kotlin-reflect Apache 2.0 kotlin-runtime Apache 2.0 kotlin-stdlib Apache 2.0 kotlin-test-junit Apache 2.0 kotlin-test Apache 2.0 annotations Apache 2.0 jolokia-core Apache 2.0 jruby-complete LGPLv2.1 LGPLv2.1 json Public Domain json Apache 2.0 xstream BSD annotations Apache 2.0 mapdb Apache 2.0 mapdb Apache 2.0 jbcrypt ISC mockserver-client-java Apache 2.0 mockserver-core Apache 2.0 mockserver-logging Apache 2.0 mockserver-netty Apache 2.0 mockito-all MIT mockito-core MIT mockito-core MIT apache-el Apache 2.0 apache-jsp Apache 2.0 alpn-boot Apache 2.0 etcd4j Apache 2.0 msgpack Apache 2.0 objenesis Apache 2.0 objenesis Apache 2.0 jmh-core GPLv2 with Classpath Exception jmh-generator-annprocess GPLv2 with Classpath Exception jol-core GPLv2 with Classpath Exception asm-analysis BSD asm-analysis BSD asm-commons BSD asm-debug-all BSD asm-tree BSD asm-util BSD asm-util BSD asm BSD asm Public Domain postgresql BSD powermock-api-mockito Apache 2.0 powermock-api-support Apache 2.0 powermock-core Apache 2.0 powermock-module-junit4-common Apache 2.0 powermock-module-junit4 Apache 2.0 powermock-reflect Apache 2.0 lombok MIT reactive-streams Public Domain reflections WTFPL scala-java8-compat_2.11 Scala scala-parser-combinators_2.11 Scala scala-xml_2.11 Scala scala-library Scala scala-reflect Scala scalatest_2.10 Apache 2.0 scalatest_2.11 Apache 2.0 native-lib-loader BSD jsonassert Apache 2.0 jcl-over-slf4j MIT jul-to-slf4j MIT log4j-log4j12 MIT log4j-over-slf4j MIT slf4j-api MIT slf4j-ext MIT slf4j-log4j12 MIT slf4j-simple MIT spock-core Apache 2.0 spock-core Apache 2.0 spring-boot-actuator Apache 2.0 spring-boot-autoconfigure Apache 2.0 spring-boot-configuration-processor Apache 2.0 spring-boot-starter-actuator Apache 2.0 spring-boot-starter-aop Apache 2.0 spring-boot-starter-data-jpa Apache 2.0 spring-boot-starter-data-rest Apache 2.0 spring-boot-starter-jdbc Apache 2.0 spring-boot-starter-jetty Apache 2.0 spring-boot-starter-logging Apache 2.0 spring-boot-starter-test Apache 2.0 spring-boot-starter-tomcat Apache 2.0 spring-boot-starter-web Apache 2.0 spring-boot-starter Apache 2.0 spring-boot-test-autoconfigure Apache 2.0 spring-boot-test Apache 2.0 spring-boot Apache 2.0 spring-data-commons Apache 2.0 spring-data-jpa Apache 2.0 spring-data-rest-core Apache 2.0 spring-data-rest-webmvc Apache 2.0 spring-hateoas Apache 2.0 spring-plugin-core Apache 2.0 spring-aop Apache 2.0 spring-aspects Apache 2.0 spring-beans Apache 2.0 spring-context-support Apache 2.0 spring-context Apache 2.0 spring-core Apache 2.0 spring-core Apache 2.0 spring-expression Apache 2.0 spring-jdbc Apache 2.0 spring-orm Apache 2.0 spring-test Apache 2.0 spring-test Apache 2.0 spring-tx Apache 2.0 spring-web Apache 2.0 spring-webmvc Apache 2.0 xz Public Domain wasabi Apache 2.0 snappy-java Apache 2.0 xmlunit-core Apache 2.0 xmlunit-legacy Apache 2.0 snakeyaml Apache 2.0 JUnitParams Apache 2.0 postgresql BSD postgresql BSD jedis MIT scala-library Scala ion-java Apache 2.0 timeslice_service-thrift New Relic timeslice_service New Relic xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3_min Public Domain zkclient Apache 2.0 zookeeper Apache 2.0 CoffeeScript Library License CoffeeScript MIT coffin MIT JavaScript JavaScript License ie_html5/ie_html5.js MIT Angular.js MIT Angular UI directives for Bootstrap MIT Chart.js MIT Chosen JQuery plugin MIT Fullcalendar MIT Moment MIT Crash reporting Software License Apple OS X OS X Yosemite Xcode & Apple SDK Xcode license Label services Service License BoneCP Apache 2.0 Gradle Apache 2.0 Hibernate Apache 2.0 Spring Apache 2.0 Infrastructure Monitoring services Service License antlr BSD aopalliance Public Domain asm MIT cglib Apache 2.0 ch.qos.logback EPLv1.0 com.101tec Apache 2.0 com.amazonaws Apache 2.0 com.cenqua.clover Paid com.fasterxml.jackson.core Apache 2.0 com.fasterxml Apache 2.0 com.google.code.findbugs BSD com.google.guava Apache 2.0 com.googlecode.json-simple Apache 2.0 com.intellij Apache 2.0 New Relic com.squareup.dagger Apache 2.0 com.squareup Apache 2.0 com.yammer.metrics Apache 2.0 commons-codec Apache 2.0 commons-configuration Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 dom4j BSD io.netty Apache 2.0 javax.annotation CDDLv1 javax.inject Apache 2.0 javax.servlet.jsp CDDLv1 javax.servlet Apache 2.0 javax.validation Apache 2.0 javax.websocket CDDLv1 jline BSD joda-time Apache 2.0 junit CPL log4j Apache 2.0 mysql GPLv2 with Classpath Exception net.sf.jopt-simple MIT org.apache.commons Apache 2.0 org.apache.httpcomponents Apache 2.0 org.apache.ivy Apache 2.0 org.apache.thrift Apache 2.0 org.apache.tomcat.embed Apache 2.0 org.apache.zookeeper Apache 2.0 org.codehaus.groovy Apache 2.0 org.codehaus.jackson Apache 2.0 org.easymock Apache 2.0 org.eclipse.jetty.orbit CDDLv1 org.eclipse.jetty.toolchain Apache 2.0 org.eclipse.jetty.websocket Apache 2.0 org.eclipse.jetty CDDLv1 org.glassfish.jersey.core CDDLv1 org.glassfish.web CDDLv1 org.glassfish CDDLv1 org.hamcrest BSD org.hibernate Apache 2.0 org.javassist Apache 2.0 org.jboss.logging LGPLv2.1 org.jenkins-ci.plugins Apache 2.0 org.jenkins-ci Apache 2.0 org.jetbrains Apache 2.0 org.jolokia Apache 2.0 org.jvnet.hudson BSD org.mockito MIT org.objenesis Apache 2.0 org.ow2.asm MIT org.reflections WTFPL org.scala-lang Scala org.slf4j MIT org.springframework.boot Apache 2.0 org.springframework Apache 2.0 org.xerial.snappy Apache 2.0 org.yaml Apache 2.0 xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3 Public Domain .NET support uploader service Library License Amazon AWS SDK for Java Apache 2.0 Apache Commons Codec Apache 2.0 Gson Apache 2.0 Jetty Apache 2.0 Ruby gems Ruby gems Gem License aasm MIT actionmailer MIT actionpack MIT activeadmin MIT actionview MIT activejob MIT active_model_serializers MIT activemodel MIT activerecord MIT activerecord-deprecated_finders MIT activerecord-mysql2-adapter MIT activeresource MIT activesupport MIT addressable Apache 2.0 angularjs-rails MIT arbre MIT arel MIT arel MIT atomic Apache 2.0 atomic Apache-2.0 attr_required MIT awesome_print MIT backports MIT better_errors MIT binding_of_caller MIT bourbon MIT brwsr MIT browser-timezone-rails MIT builder MIT bundler MIT byebug MIT callsite MIT capistrano MIT capistrano-bundler MIT capistrano-ext MIT capistrano-rbenv MIT capistrano-stats MIT capybara MIT celluloid MIT CFPropertyList MIT chunky_png MIT ci_reporter MIT ci_reporter_rspec MIT clockwork MIT codeclimate-test-reporter MIT coderay MIT coderay MIT coffee-rails MIT coffee-script MIT coffee-script-source MIT columnize Ruby compass MIT compass-core MIT compass-import-once MIT compass-rails MIT concurrent-ruby MIT crack MIT crypt Unlicensed daemons MIT dalli MIT database_cleaner MIT debug_inspector MIT debugger-linecache MIT diff-lcs MIT docile MIT dotenv MIT dotenv-deployment MIT erubis MIT etcd MIT ethon MIT eventmachine Ruby excon MIT execjs MIT factory_girl MIT factory_girl_rails MIT faraday MIT faraday-middleware MIT faye-websocket MIT ffi MIT ffi BSD fission MIT fog MIT fog-atmos MIT fog-aws MIT fog-brightbox MIT fog-core MIT fog-ecloud MIT fog-google MIT fog-json MIT fog-local MIT fog-powerdns MIT fog-profitbricks MIT fog-radosgw MIT fog-riakcs MIT fog-sakuracloud MIT fog-serverlove MIT fog-softlayer MIT fog-storm_on_demand MIT fog-terremark MIT fog-vmfusion MIT fog-voxel MIT fog-xml MIT font-awesome-rails MIT foreman MIT formatador MIT formtastic MIT formtastic i18n MIT fssm MIT globalid MIT guard MIT guard-rspec MIT haml MIT haml-rails MIT has scope MIT hashie MIT highline Ruby hike MIT hipchat-api MIT histogram MIT hitimes ISC hiredis BSD http_parser.rb MIT httpclient Ruby httpclient Ruby httparty MIT i18n MIT inflecto MIT inherited resources MIT intercom-rails MIT ipaddress MIT jasmine MIT jasmine-core MIT jasmine-rails MIT jbuilder MIT jira MIT jira-ruby OSL-3.0 journey MIT jquery-rails MIT jquery-turbolinks MIT jquery-ui-rails MIT json Ruby jwt MIT kaminari MIT kgio LGPLv3 kgio LGPLv3 kgio LGPL-v2.1+ libv8 MIT listen MIT logger Ruby loofah MIT lumberjack MIT macaddr MIT mail MIT mailcatcher MIT memoist MIT metaclass MIT meta_request MIT method_source MIT mime-types MIT mini_portile MIT minitest MIT minitest-rails MIT mixlib-log Apache 2.0 mocha MIT mono_logger MIT multi_json MIT multi_xml MIT multipart-post MIT mysql2 MIT net-http-persistent MIT net-scp MIT net-sftp MIT net-ssh MIT net-ssh-gateway MIT netrc MIT nokogiri MIT oauth2 MIT oj MIT omniauth MIT omniauth-oauth2 MIT pagerduty MIT papers MIT pg BSD pg Ruby phantomjs BSD phantomjs-binaries Unlicensed poltergeist MIT polyamorous MIT polyglot MIT polyglot MIT protected_attributes MIT pry MIT pry-nav MIT pry-rails MIT pry-stack_explorer MIT puma BSD quiet assets MIT rack MIT rack-cache MIT rack-contrib MIT rack-oauth2 MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-ssl MIT rack-ssl-enforcer MIT rack-test MIT rails MIT rails-deprecated_sanitizer MIT rails-dom-testing MIT rails-html-sanitizer MIT rails-observers MIT rails_12factor MIT rails_serve_static_assets MIT rails_stdout_logging MIT railties MIT raindrops LGPLv3 raindrops LGPLv3 raindrops LGPLv2.1+ rake MIT rake MIT rake MIT rake MIT ransack MIT rb-fsevent MIT rb-inotify MIT rdoc Ruby redcarpet MIT redis MIT redis-namespace MIT redis-queue MIT ref MIT remote_syslog_logger MIT request_store MIT responders MIT resque MIT resque-cleaner MIT resque-pool MIT resque-scheduler MIT resque-status MIT rest-client MIT restforce MIT rspec MIT rspec-core MIT rspec-expectations MIT rspec-mocks MIT rspec-rails MIT rspec-support MIT rspec_junit_formatter MIT rubyntlm MIT ruby-saml MIT rufus-scheduler MIT safe_yaml MIT salesforce_bulk_query BSD sass MIT sass-rails MIT sass-rails MIT sdoc MIT secure_headers Apache 2.0 sequel MIT serveza MIT settingslogic MIT shoulda MIT shoulda-context MIT shoulda-matchers MIT simplecov MIT simplecov-html MIT simplecov-rcov MIT sinatra MIT sinatra-activerecord MIT sinatra-contrib MIT skinny MIT slop MIT spring MIT sprockets MIT sprockets-rails MIT sqlite3 BSD sshkit GPL-No Distro sys-uname Ruby syslog_protocol MIT systemu BSDL term-ansicolor GPL-No Distro terminal-table MIT therubyracer MIT thin Ruby thor MIT thread_safe Apache-2.0 thrift Apache 2.0 thrift-rack MIT tilt MIT timecop MIT timers MIT tins MIT treetop MIT turbolinks MIT twitter-bootstrap-rails MIT typhoeus MIT tzinfo MIT uglifier MIT unicorn Ruby unicorn-rails MIT uuid MIT uuidtools Apache-2.0 vegas MIT webmock MIT web-console MIT xml-simple Ruby xpath MIT yard MIT Storage services Library License Amazon AWS SDK for Java Apache 2.0 Amazon AWS Command Line Interface Apache 2.0 Go packages Library License Go-Mysql-Driver Mozilla Public License 2.0 gocql BSD 3-Clause groupcache Apache 2.0 Migrate MIT go-sqlite3 MIT Revel MIT Pathtree MIT Ansicolor MIT fsnotify BSD 3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.20683,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Services <em>licenses</em>",
        "sections": "Services <em>licenses</em>",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Contents Java internal"
      },
      "id": "603ea24364441f91fb4e8864"
    },
    {
      "sections": [
        "Add-on end user license agreement",
        "New Relic, Inc. Add-on End User License Agreement"
      ],
      "title": "Add-on end user license agreement",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "a01c225ca30f95dab7db856cd946c76de557c31f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/add-end-user-license-agreement/",
      "published_at": "2022-01-08T09:14:04Z",
      "updated_at": "2021-03-16T06:19:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic, Inc. Add-on End User License Agreement In connection with the work provided by New Relic's Expert Services, Field Instrumentation Team, or Sales team, you may be provided with certain custom-created software to visualize, enable, optimize, or enhance your use of New Relic's Services. By downloading, installing, authorizing installation, or using the Add-on (defined below) with your Underlying Software (defined below), you (“Customer”) agree to the terms and conditions herein (“Agreement”) with New Relic, Inc., a Delaware corporation with offices located at 188 Spear Street, Suite 1200 San Francisco, CA 94105 (“New Relic\"), (collectively the “Parties”). Capitalized terms not defined herein shall have the meanings set forth in the New Relic Terms of Service (the “Terms of Service”) available here: newrelic.com/termsandconditions/terms. IF YOU DO NOT AGREE TO THIS AGREEMENT, YOUR SOLE REMEDY IS TO NOT USE THE ADD-ON. 1. DEFINITIONS “Add-on” means the New Relic applications and/or software, including but not limited to connectors, extensions, UI extension, and plugins, provided by New Relic to Customer to enable the Services to operate with certain third party or Customer software or systems (“Underlying Software”). For the purposes of the Terms of Service, the Add-on shall be treated like an Agent, subject to the separate terms herein. 2. USE OF THE PROGRAMMABILITY ADD-ON 2.1 Software Evaluation License. Subject to the terms herein, New Relic grants to Customer a limited, non-exclusive, non-transferrable, non-sublicensable right to install, use, and configure the Add-on solely as needed to enable the Services for internal evaluation purposes. New Relic reserves all rights and licenses not expressly granted herein. 2.2 Ownership. As between the Parties, New Relic owns all right, title, and interest to the Add-on and Feedback, including but not limited to any intellectual property and proprietary rights therein. Customer retains all right, title, and interest in any Customer Data processed by the Add-on in connection with the Services. 2.3 Feedback. Customer agrees to provide feedback, suggestions, ideas, requests or recommendations (“Feedback”) regarding the Add-on, and hereby irrevocably assigns all intellectual property and proprietary rights it holds in the Feedback to New Relic. 2.4 Notice. Customer acknowledges and agrees that the Add-on is made to work with the Underlying Software as configured at the time of creation and there is no guarantee that Add-on will continue to work in the event Customer changes, replaces, upgrades versions of, updates, or otherwise changes the Underlying Software (an “Upgrade Event”). Customer shall hold New Relic harmless from any claims or damages arising from Customer’s Upgrade Event. 2.5 Restrictions. Customer will not: (i) use the Add-on except as permitted hereunder; (ii) distribute, sell, sublicense, or otherwise transfer the Add-on; (iii) decompile, disassemble or reverse engineer any software underlying the Add-on; (iv) use the Add-on to damage, disable, overburden or impair any New Relic server or network(s) connected to any New Relic server or interfere with any other party’s use and enjoyment of the Services; (v) use the Add-on to defraud, defame, abuse, harass, stalk, threaten or infringe the rights of privacy or other intellectual property rights of others or otherwise violate any applicable law; (vi) circumvent or modify any security technologies included as part of the Add-on; or (vii) attempt or permit any third party to do any of the foregoing. New Relic may suspend Customer usage of the Services, without notice, pending any investigation of misuse. These restrictions may be supplemented or superceded (to the extent they conflict) by the New Relic Acceptable Use Policy as may be published and updated from time to time on the New Relic web site at docs.newrelic.com/docs/licenses/license-information/acceptable-use-policy/acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential Information” means: (a) the Programmability Add-on, and any features, results or output produced by, and other information relating to the Add-on (including, without limitation, all Feedback); and (b) any business or technical information of New Relic including but not limited to any technical information, research, development, know-how that a reasonable person would understand to be confidential. 3.2 Restrictions. Customer will not use or disclose any Confidential Information, except as necessary for the performance of this Agreement. Customer will use all reasonable efforts to protect Confidential Information from unauthorized use or disclosure, but in no event less than the efforts that it ordinarily uses with respect to its own proprietary information. Customer may disclose Confidential Information to those of its employees who have a bona fide need to know such Confidential Information for the performance of this Agreement; provided that each such employee first executes a written agreement that contains use and nondisclosure restrictions at least as protective as those set forth herein. Confidential Information shall not include any information that: (a) is or becomes generally known to the public through no fault or breach of this Agreement by Customer; (b) is rightfully known by Customer at the time of disclosure without an obligation of confidentiality; (c) is independently developed by Customer without access or use of any Confidential Information; or (d) is rightfully obtained from a third party without restriction on use or disclosure. 4. DISCLAIMER 4.1 DISCLAIMER. THE ADD-ON IS PROVIDED AS-IS AND AS-AVAILABLE AND NEW RELIC DISCLAIMS AND MAKES NO WARRANTIES, EITHER EXPRESS OR IMPLIED, INCLUDING ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR NON-INFRINGEMENT AND WHETHER OR NOT ARISING THROUGH A COURSE OF DEALING. THE ADD-ON IS NOT GUARANTEED TO BE ERROR-FREE, COMPATIBLE WITH THE UNDERLYING SOFTWARE, OR THAT CUSTOMER WILL ACHIEVE ANY RESULTS FROM USE OF THE ADD-ON THEREFROM. 4.2 LIMITATION OF LIABILITY. TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC, ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE TO CUSTOMER OR ANY THIRD PARTY FOR DAMAGES OF ANY KIND, INCLUDING, WITHOUT LIMITATION, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE INSTALLATION, USE OR INABILITY TO USE THE ADD-ON OR FOR ANY ERROR OR DEFECT IN THE ADD-ON OR THE SERVICES, REGARDLESS OF LEGAL THEORY, WHETHER OR NOT NEW RELIC HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSS OR DAMAGE. NEW RELIC’S AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS AGREEMENT WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE PARTIES HAVE AGREED THAT THESE LIMITATIONS WILL SURVIVE AND APPLY EVEN IF ANY LIMITED REMEDY SPECIFIED IN THIS AGREEMENT IS FOUND TO HAVE FAILED OF ITS ESSENTIAL PURPOSE. THE LIMITATIONS OF DAMAGES SET FORTH ABOVE ARE FUNDAMENTAL ELEMENTS OF THE BASIS OF THE BARGAIN BETWEEN NEW RELIC AND CUSTOMER. 4.3 EVALUATION VERSION. Customer agrees and acknowledges that: (a) the Add-on is not an official product and has not been commercially released for sale by New Relic; (b) the Add-on may not operate properly, being in final form, or fully functional; (c) the Add-on may contain errors, security vulnerabilities, design flaws, or other problems; (d) it may not be possible to make the Add-on fully functional; (e) the information obtained using the Add-on may not be accurate; (f) use of the Add-on may result in unexpected results, loss of data, delays or other unpredictable damages or loss; (g) New Relic is under no obligation to release a commercial version of the Add-on; and (h) New Relic has the right unilaterally to abandon development of the Add-on, at any time and without any obligation or liability to Customer. 5. GENERAL PROVISIONS 5.1 Terms of Service; Documentation. This Agreement shall be considered a part of the New Relic documentation, located at: https://docs.newrelic.com (the “Documentation). This Agreement and the Terms of Service constitute the entire and exclusive agreement between New Relic and Customer with respect to the Add-on. In the absence of a separate agreement, upon agreement between New Relic and Customer with respect to the Add-On, the terms and conditions of this Agreement shall govern the relationship between New Relic and Customer with respect to such Add-On. To the extent of a conflict between the Agreement and the Terms of Service, this Agreement shall govern with respect to the Add-on only.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.76915,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Add-on end user <em>license</em> agreement",
        "sections": "Add-on end user <em>license</em> agreement",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": "&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;acceptable-use-policy&#x2F;acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential <em>Information</em>” means: (a) the Programmability Add-on, and any features, results or output produced by, and other <em>information</em> relating to the Add-on (including, without limitation"
      },
      "id": "603ec23328ccbccf1beba79a"
    },
    {
      "sections": [
        "New Relic Agent Software Notice"
      ],
      "title": "New Relic Agent Software Notice",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "2bf9501c2767105130d3808f1bf3a91a032d903e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/new-relic-agent-software-notice/",
      "published_at": "2022-01-08T09:14:04Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This software is © 2008-2021 New Relic, Inc. and its licensors. This software is solely for use with New Relic’s proprietary SaaS service (“New Relic Service”), so to use the software you must have a valid account for the New Relic Service under a separate agreement with New Relic (“Subscription Agreement”). You may only use the software to support your use of the New Relic Service as permitted in the Subscription Agreement. Without a Subscription Agreement, you may not use the software. All other use is prohibited. New Relic and its suppliers retain all right, title and interest (including intellectual property rights) in the software. The Subscription Agreement will control in event of a conflict with this notice. Unless otherwise agreed by New Relic in your Subscription Agreement: You may not use, copy, distribute or sublicense the software, use the software on behalf of third parties, reverse engineer or decompile the software, modify or create derivative works of the software, use the software for competitive analysis or benchmarking, or remove or obscure any proprietary notices in the software. The software is provided “AS IS” and New Relic disclaims all warranties, whether express, implied, statutory or otherwise, including warranties of merchantability, fitness for a particular purpose, title or noninfringement. To the full extent permitted by law, New Relic will have no liability arising from or related to the software or under this notice for any direct, indirect, special, incidental, or consequential damages of any kind, even if advised of their possibility in advance, and regardless of legal theory (whether contract, tort, negligence, strict liability or otherwise). The software may contain third-party open source software (“OSS”) as described here and at https://github.com/newrelic. To the extent required by the OSS license, that license will apply to the OSS when used on a stand-alone basis. For avoidance of doubt, you may copy and distribute New Relic agents pursuant to your Subscription Agreement for New Relic Browser and New Relic Mobile and as set forth: https://docs.newrelic.com/docs/mobile-monitoring/new-relic-mobile/get-started and https://docs.newrelic.com/docs/browser/new-relic-browser/installation/install-new-relic-browser-agent Software versions New Relic makes available under an OSS license (such as Apache 2.0) are governed by the terms of the applicable OSS license. For a current list of New Relic software versions released as OSS please visit https://opensource.newrelic.com/.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.76753,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": " and at https:&#x2F;&#x2F;github.com&#x2F;newrelic. To the extent required by the OSS <em>license</em>, that <em>license</em> will apply to the OSS when used on a stand-alone basis. For avoidance of doubt, you may copy and distribute New Relic agents pursuant to your Subscription Agreement for New Relic Browser and New Relic Mobile"
      },
      "id": "603eb73828ccbc1f99eba74a"
    }
  ],
  "/docs/licenses/license-information/distributed-licenses/new-relic-agent-software-notice": [
    {
      "sections": [
        "Services licenses",
        "Contents",
        "Java internal services",
        "Java crash data API",
        "CoffeeScript",
        "JavaScript",
        "Crash reporting",
        "Label services",
        "Infrastructure Monitoring services",
        ".NET support uploader service",
        "Ruby gems",
        "Storage services",
        "Go packages"
      ],
      "title": "Services licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "49a2ad450d31dacfc2aae690ca947d2326d18761",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/other-licenses/services-licenses/",
      "published_at": "2022-01-08T07:39:37Z",
      "updated_at": "2021-04-06T00:26:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Contents Java internal services Java crash data API Framework License Apache TomCat Apache 2.0 Bean Validation Apache 2.0 Cassandra Driver Core Apache 2.0 ClassMate Apache 2.0 Hamcrest BSD Mockito MIT Netty Apache 2.0 Objenesis MIT SnakeYAML Apache 2.0 Spring Boot Apache 2.0 Java internal services Library License json_simple Apache 2.0 newrelic-api New Relic newrelic-api New Relic antlr BSD aopalliance Public Domain asm-analysis BSD asm-commons BSD asm-tree BSD asm-util BSD asm BSD c3p0 EPLv1.0 cglib-nodep Apache 2.0 cglib Apache 2.0 cglib Apache 2.0 cal10n-api MIT logback-classic EPLv1.0 logback-core EPLv1.0 guava-jetty-service Apache 2.0 zkclient Apache 2.0 reporter-config-base Apache 2.0 reporter-config3 Apache 2.0 reporter-config Apache 2.0 aws-java-sdk-acm Apache 2.0 aws-java-sdk-api-gateway Apache 2.0 aws-java-sdk-applicationautoscaling Apache 2.0 aws-java-sdk-autoscaling Apache 2.0 aws-java-sdk-cloudformation Apache 2.0 aws-java-sdk-cloudfront Apache 2.0 aws-java-sdk-cloudhsm Apache 2.0 aws-java-sdk-cloudsearch Apache 2.0 aws-java-sdk-cloudtrail Apache 2.0 aws-java-sdk-cloudwatch Apache 2.0 aws-java-sdk-cloudwatchmetrics Apache 2.0 aws-java-sdk-codecommit Apache 2.0 aws-java-sdk-codedeploy Apache 2.0 aws-java-sdk-codepipeline Apache 2.0 aws-java-sdk-cognitoidentity Apache 2.0 aws-java-sdk-cognitoidp Apache 2.0 aws-java-sdk-cognitosync Apache 2.0 aws-java-sdk-config Apache 2.0 aws-java-sdk-core Apache 2.0 aws-java-sdk-datapipeline Apache 2.0 aws-java-sdk-devicefarm Apache 2.0 aws-java-sdk-directconnect Apache 2.0 aws-java-sdk-directory Apache 2.0 aws-java-sdk-discovery Apache 2.0 aws-java-sdk-dms Apache 2.0 aws-java-sdk-dynamodb Apache 2.0 aws-java-sdk-ec2 Apache 2.0 aws-java-sdk-ecr Apache 2.0 aws-java-sdk-ecs Apache 2.0 aws-java-sdk-efs Apache 2.0 aws-java-sdk-elasticache Apache 2.0 aws-java-sdk-elasticbeanstalk Apache 2.0 aws-java-sdk-elasticloadbalancing Apache 2.0 aws-java-sdk-elasticloadbalancingv2 Apache 2.0 aws-java-sdk-elasticsearch Apache 2.0 aws-java-sdk-elastictranscoder Apache 2.0 aws-java-sdk-emr Apache 2.0 aws-java-sdk-events Apache 2.0 aws-java-sdk-gamelift Apache 2.0 aws-java-sdk-glacier Apache 2.0 aws-java-sdk-iam Apache 2.0 aws-java-sdk-importexport Apache 2.0 aws-java-sdk-inspector Apache 2.0 aws-java-sdk-iot Apache 2.0 aws-java-sdk-kinesis Apache 2.0 aws-java-sdk-kms Apache 2.0 aws-java-sdk-lambda Apache 2.0 aws-java-sdk-logs Apache 2.0 aws-java-sdk-machinelearning Apache 2.0 aws-java-sdk-marketplacecommerceanalytics Apache 2.0 aws-java-sdk-marketplacemeteringservice Apache 2.0 aws-java-sdk-models Apache 2.0 aws-java-sdk-opsworks Apache 2.0 aws-java-sdk-rds Apache 2.0 aws-java-sdk-redshift Apache 2.0 aws-java-sdk-route53 Apache 2.0 aws-java-sdk-s3 Apache 2.0 aws-java-sdk-servicecatalog Apache 2.0 aws-java-sdk-ses Apache 2.0 aws-java-sdk-simpledb Apache 2.0 aws-java-sdk-simpleworkflow Apache 2.0 aws-java-sdk-snowball Apache 2.0 aws-java-sdk-sns Apache 2.0 aws-java-sdk-sqs Apache 2.0 aws-java-sdk-ssm Apache 2.0 aws-java-sdk-storagegateway Apache 2.0 aws-java-sdk-sts Apache 2.0 aws-java-sdk-support Apache 2.0 aws-java-sdk-swf-libraries Apache 2.0 aws-java-sdk-waf Apache 2.0 aws-java-sdk-workspaces Apache 2.0 aws-java-sdk Apache 2.0 jmespath-java Apache 2.0 AppleJavaExtensions BSD jcommander Apache 2.0 high-scale-lib MIT hppc Apache 2.0 clover Paid stream Apache 2.0 metrics-core Apache 2.0 cassandra-driver-core Apache 2.0 cassandra-driver-core Apache 2.0 uuid MIT grabbag MIT speed4j Apache 2.0 yamlbeans BSD jackson-annotations Apache 2.0 jackson-core Apache 2.0 jackson-databind Apache 2.0 jackson-dataformat-cbor Apache 2.0 jackson-dataformat-csv Apache 2.0 jackson-dataformat-xml Apache 2.0 jackson-dataformat-yaml Apache 2.0 jackson-datatype-guava Apache 2.0 jackson-datatype-jdk7 Apache 2.0 jackson-datatype-jdk8 Apache 2.0 jackson-datatype-joda Apache 2.0 jackson-datatype-jsr310 Apache 2.0 jackson-jaxrs-base Apache 2.0 jackson-jaxrs-json-provider Apache 2.0 jackson-module-afterburner Apache 2.0 jackson-module-jaxb-annotations Apache 2.0 jackson-module-kotlin Apache 2.0 woodstox-core Apache 2.0 classmate Apache 2.0 zjsonpatch Apache 2.0 caffeine Apache 2.0 waffle-jna EPLv1.0 docker-java Apache 2.0 docker-java Apache 2.0 btf Apache 2.0 jackson-coreutils Apache 2.0 json-schema-core Apache 2.0 json-schema-validator Apache 2.0 msg-simple Apache 2.0 uri-template Apache 2.0 jamm Apache 2.0 jffi Apache 2.0 jnr-constants Apache 2.0 jnr-ffi Apache 2.0 jnr-ffi Apache 2.0 jnr-posix EPLv1.0 jnr-x86asm MIT embedded-redis Apache 2.0 memoryfilesystem MIT guava-retrying Apache 2.0 guava-retrying Apache 2.0 snowball-stemmer BSD system-rules CPL uuid Apache 2.0 wiremock Apache 2.0 auto-common Apache 2.0 annotations LGPLv2.1 annotations LGPLv2.1 bcel-findbugs LGPLv2.1 findbugs LGPLv2.1 jFormatString BSD jFormatString BSD jsr305 BSD jsr305 BSD jsr305 BSD gson Apache 2.0 spymemcached MIT guava-testlib Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guice-multibindings Apache 2.0 guice Apache 2.0 truth Apache 2.0 concurrent-trees Apache 2.0 concurrentlinkedhashmap-lru Apache 2.0 json-simple Apache 2.0 libphonenumber Apache 2.0 locality-uuid BSD h2 EPLv1.0 annotations Apache 2.0 json-path-assert Apache 2.0 json-path Apache 2.0 json-path Apache 2.0 jzlib BSD bonecp-provider Apache 2.0 bonecp Apache 2.0 junixsocket-common Apache 2.0 junixsocket-native-common Apache 2.0 disruptor Apache 2.0 geoip-api LGPLv2.1 c3p0 LGPLv2.1 mchange-commons-java LGPLv2.1 archaius-core Apache 2.0 hystrix-core Apache 2.0 hystrix-servo-metrics-publisher Apache 2.0 rxjava-core Apache 2.0 servo-core Apache 2.0 kafka-clients Apache 2.0 kafka_2.10 Apache 2.0 kafka_2.11 Apache 2.0 common-cassandra Apache 2.0 timeslice_utils New Relic mockito-kotlin MIT compress-lzf Apache 2.0 checkstyle Apache 2.0 dagger-compiler Apache 2.0 dagger Apache 2.0 okhttp Apache 2.0 okio Apache 2.0 javapoet Apache 2.0 javawriter Apache 2.0 jaxb-impl CDDLv1 thrift-server Apache 2.0 finagle-core_2.10 Apache 2.0 finagle-core_2.11 Apache 2.0 finagle-http_2.10 Apache 2.0 finagle-http_2.11 Apache 2.0 jsr166e Creative Commons util-app_2.10 Apache 2.0 util-app_2.11 Apache 2.0 util-cache_2.10 Apache 2.0 util-cache_2.11 Apache 2.0 util-codec_2.10 Apache 2.0 util-codec_2.11 Apache 2.0 util-collection_2.10 Apache 2.0 util-collection_2.11 Apache 2.0 util-core_2.10 Apache 2.0 util-core_2.11 Apache 2.0 util-function_2.10 Apache 2.0 util-function_2.11 Apache 2.0 util-hashing_2.10 Apache 2.0 util-hashing_2.11 Apache 2.0 util-jvm_2.10 Apache 2.0 util-jvm_2.11 Apache 2.0 util-lint_2.10 Apache 2.0 util-lint_2.11 Apache 2.0 util-logging_2.10 Apache 2.0 util-logging_2.11 Apache 2.0 util-registry_2.10 Apache 2.0 util-registry_2.11 Apache 2.0 util-stats_2.10 Apache 2.0 util-stats_2.11 Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 HikariCP Apache 2.0 commons-beanutils Apache 2.0 commons-cli Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-io Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 commons-pool Apache 2.0 unix-socket-factory Apache 2.0 unix-socket-factory Apache 2.0 jflex BSD dom4j BSD airline Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 metrics-healthchecks Apache 2.0 metrics-httpclient Apache 2.0 metrics-jdbi Apache 2.0 metrics-jersey2 Apache 2.0 metrics-jetty9 Apache 2.0 metrics-json Apache 2.0 metrics-jvm Apache 2.0 metrics-logback Apache 2.0 metrics-servlets Apache 2.0 dropwizard-java8-jdbi Apache 2.0 dropwizard-client Apache 2.0 dropwizard-configuration Apache 2.0 dropwizard-core Apache 2.0 dropwizard-db Apache 2.0 dropwizard-jackson Apache 2.0 dropwizard-jdbi Apache 2.0 dropwizard-jersey Apache 2.0 dropwizard-jetty Apache 2.0 dropwizard-lifecycle Apache 2.0 dropwizard-logging Apache 2.0 dropwizard-metrics Apache 2.0 dropwizard-servlets Apache 2.0 dropwizard-testing Apache 2.0 dropwizard-util Apache 2.0 dropwizard-validation Apache 2.0 netty-all Apache 2.0 netty-buffer Apache 2.0 netty-codec-http Apache 2.0 netty-codec-socks Apache 2.0 netty-codec Apache 2.0 netty-common Apache 2.0 netty-handler-proxy Apache 2.0 netty-handler Apache 2.0 netty-resolver Apache 2.0 netty-transport-native-epoll Apache 2.0 netty-transport Apache 2.0 netty Apache 2.0 ratpack-core Apache 2.0 ratpack-groovy-test Apache 2.0 ratpack-groovy Apache 2.0 ratpack-guice Apache 2.0 ratpack-test Apache 2.0 rxjava Apache 2.0 fastutil Apache 2.0 janino BSD activation Apache 2.0 javax.annotation-api CDDLv1 jsr250-api CDDLv1 javax.inject Apache 2.0 mail CDDLv1 mailapi CDDLv1 javax.servlet-api Apache 2.0 servlet-api Apache 2.0 javax.transaction-api CDDLv1 validation-api Apache 2.0 javax.websocket-api CDDLv1 javax.ws.rs-api CDDLv1 jaxb-api CDDLv1 stax-api CDDLv1 jaxen Apache 2.0 jline BSD jline BSD joda-time Apache 2.0 junit-dep CPL junit CPL junit EPLv1.0 kafka_2.10 Apache 2.0 kafka_2.9.2 Apache 2.0 apache-log4j-extras Apache 2.0 log4j Apache 2.0 mysql-connector-java GPLv2 with Classpath Exception byte-buddy Apache 2.0 byte-buddy Apache 2.0 jna-platform LGPLv2.1 jna Apache 2.0 jcip-annotations Apache 2.0 lz4 Apache 2.0 accessors-smart Apache 2.0 asm Apache 2.0 json-smart Apache 2.0 json-smart Apache 2.0 primitive GPLv2 with Classpath Exception compiler Apache 2.0 lang Apache 2.0 beaver-cc BSD ehcache-core Apache 2.0 ehcache Apache 2.0 jopt-simple MIT jopt-simple MIT opencsv Apache 2.0 quality-check Apache 2.0 scannotation Apache 2.0 super-csv Apache 2.0 uadetector-core Apache 2.0 uadetector-resources Apache 2.0 argparse4j MIT spymemcached MIT kalium Apache 2.0 ST4 BSD antlr-runtime BSD antlr-runtime BSD antlr BSD stringtemplate BSD ant-launcher Apache 2.0 ant Apache 2.0 cassandra-all Apache 2.0 cassandra-all Apache 2.0 cassandra-thrift Apache 2.0 cassandra-thrift Apache 2.0 commons-collections4 Apache 2.0 commons-compress Apache 2.0 commons-csv Apache 2.0 commons-dbcp2 Apache 2.0 commons-io Apache 2.0 commons-lang3 Apache 2.0 commons-math3 Apache 2.0 commons-math Apache 2.0 commons-pool2 Apache 2.0 curator-client Apache 2.0 curator-framework Apache 2.0 curator-recipes Apache 2.0 curator-test Apache 2.0 derby Apache 2.0 geronimo-servlet_3.0_spec Apache 2.0 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 ivy Apache 2.0 kafka-clients Apache 2.0 kafka_2.11 Apache 2.0 libthrift Apache 2.0 tomcat-embed-core Apache 2.0 tomcat-embed-el Apache 2.0 tomcat-embed-logging-juli Apache 2.0 tomcat-embed-websocket Apache 2.0 tomcat-jdbc Apache 2.0 tomcat-juli Apache 2.0 zookeeper Apache 2.0 aspectjweaver EPLv1.0 assertj-core Apache 2.0 assertj-core Apache 2.0 evo-inflector Apache 2.0 bcmail-jdk15on MIT bcpkix-jdk15on MIT bcprov-jdk15on MIT ohc-core Apache 2.0 cassandra-unit-spring LGPLv3 cassandra-unit-spring LGPLv3 cassandra-unit LGPLv3 cassandra-unit LGPLv3 groovy-all Apache 2.0 groovy-all Apache 2.0 groovy Apache 2.0 groovy Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 easymock Apache 2.0 ecj EPLv1.0 javax.activation CDDLv1 javax.annotation CDDLv1 javax.mail.glassfish CDDLv1 javax.servlet CDDLv1 javax.transaction CDDLv1 org.objectweb.asm BSD jetty-setuid-java EPLv1.0 jetty-schemas Apache 2.0 javax-websocket-client-impl Apache 2.0 javax-websocket-server-impl Apache 2.0 websocket-api Apache 2.0 websocket-client Apache 2.0 websocket-common Apache 2.0 websocket-server Apache 2.0 websocket-servlet Apache 2.0 apache-jsp Apache 2.0 jetty-annotations CDDLv1 jetty-client Apache 2.0 jetty-continuation Apache 2.0 jetty-http Apache 2.0 jetty-io Apache 2.0 jetty-jmx Apache 2.0 jetty-jndi Apache 2.0 jetty-plus Apache 2.0 jetty-proxy Apache 2.0 jetty-security Apache 2.0 jetty-server Apache 2.0 jetty-servlet Apache 2.0 jetty-servlets Apache 2.0 jetty-util Apache 2.0 jetty-webapp Apache 2.0 jetty-xml Apache 2.0 sigar Apache 2.0 aopalliance-repackaged CDDLv1 javax.inject CDDLv1 hk2-api CDDLv1 hk2-locator CDDLv1 hk2-utils CDDLv1 osgi-resource-locator CDDLv1 jersey-guava CDDLv1 jersey-client CDDLv1 jersey-apache-connector CDDLv1 jersey-container-servlet-core CDDLv1 jersey-container-servlet CDDLv1 jersey-client CDDLv1 jersey-common CDDLv1 jersey-server CDDLv1 jersey-bean-validation CDDLv1 jersey-metainf-services CDDLv1 jersey-media-jaxb CDDLv1 jersey-client CDDLv1 jersey-test-framework-provider-inmemory CDDLv1 jersey-test-framework-core CDDLv1 javax.el CDDLv1 hamcrest-all BSD hamcrest-core BSD hamcrest-integration BSD hamcrest-library BSD HdrHistogram Public Domain hector-core MIT hibernate-commons-annotations LGPLv2.1 hibernate-jpa-2.0-api LGPLv2.1 hibernate-jpa-2.1-api LGPLv2.1 hibernate-c3p0 Apache 2.0 hibernate-core Apache 2.0 hibernate-ehcache Apache 2.0 hibernate-entitymanager Apache 2.0 hibernate-jmx Apache 2.0 hibernate-validator Apache 2.0 freebuilder Apache 2.0 org.jacoco.agent EPLv1.0 org.jacoco.ant EPLv1.0 org.jacoco.core EPLv1.0 org.jacoco.report EPLv1.0 javassist Apache 2.0 jboss-logging-annotations LGPLv2.1 jboss-logging Apache 2.0 jboss-transaction-api_1.1_spec LGPLv2.1 jboss-transaction-api_1.2_spec LGPLv2.1 jandex Apache 2.0 jdbi Apache 2.0 job-dsl-core Apache 2.0 version-number MIT kotlin-annotation-processing Apache 2.0 kotlin-reflect Apache 2.0 kotlin-runtime Apache 2.0 kotlin-stdlib Apache 2.0 kotlin-test-junit Apache 2.0 kotlin-test Apache 2.0 annotations Apache 2.0 jolokia-core Apache 2.0 jruby-complete LGPLv2.1 LGPLv2.1 json Public Domain json Apache 2.0 xstream BSD annotations Apache 2.0 mapdb Apache 2.0 mapdb Apache 2.0 jbcrypt ISC mockserver-client-java Apache 2.0 mockserver-core Apache 2.0 mockserver-logging Apache 2.0 mockserver-netty Apache 2.0 mockito-all MIT mockito-core MIT mockito-core MIT apache-el Apache 2.0 apache-jsp Apache 2.0 alpn-boot Apache 2.0 etcd4j Apache 2.0 msgpack Apache 2.0 objenesis Apache 2.0 objenesis Apache 2.0 jmh-core GPLv2 with Classpath Exception jmh-generator-annprocess GPLv2 with Classpath Exception jol-core GPLv2 with Classpath Exception asm-analysis BSD asm-analysis BSD asm-commons BSD asm-debug-all BSD asm-tree BSD asm-util BSD asm-util BSD asm BSD asm Public Domain postgresql BSD powermock-api-mockito Apache 2.0 powermock-api-support Apache 2.0 powermock-core Apache 2.0 powermock-module-junit4-common Apache 2.0 powermock-module-junit4 Apache 2.0 powermock-reflect Apache 2.0 lombok MIT reactive-streams Public Domain reflections WTFPL scala-java8-compat_2.11 Scala scala-parser-combinators_2.11 Scala scala-xml_2.11 Scala scala-library Scala scala-reflect Scala scalatest_2.10 Apache 2.0 scalatest_2.11 Apache 2.0 native-lib-loader BSD jsonassert Apache 2.0 jcl-over-slf4j MIT jul-to-slf4j MIT log4j-log4j12 MIT log4j-over-slf4j MIT slf4j-api MIT slf4j-ext MIT slf4j-log4j12 MIT slf4j-simple MIT spock-core Apache 2.0 spock-core Apache 2.0 spring-boot-actuator Apache 2.0 spring-boot-autoconfigure Apache 2.0 spring-boot-configuration-processor Apache 2.0 spring-boot-starter-actuator Apache 2.0 spring-boot-starter-aop Apache 2.0 spring-boot-starter-data-jpa Apache 2.0 spring-boot-starter-data-rest Apache 2.0 spring-boot-starter-jdbc Apache 2.0 spring-boot-starter-jetty Apache 2.0 spring-boot-starter-logging Apache 2.0 spring-boot-starter-test Apache 2.0 spring-boot-starter-tomcat Apache 2.0 spring-boot-starter-web Apache 2.0 spring-boot-starter Apache 2.0 spring-boot-test-autoconfigure Apache 2.0 spring-boot-test Apache 2.0 spring-boot Apache 2.0 spring-data-commons Apache 2.0 spring-data-jpa Apache 2.0 spring-data-rest-core Apache 2.0 spring-data-rest-webmvc Apache 2.0 spring-hateoas Apache 2.0 spring-plugin-core Apache 2.0 spring-aop Apache 2.0 spring-aspects Apache 2.0 spring-beans Apache 2.0 spring-context-support Apache 2.0 spring-context Apache 2.0 spring-core Apache 2.0 spring-core Apache 2.0 spring-expression Apache 2.0 spring-jdbc Apache 2.0 spring-orm Apache 2.0 spring-test Apache 2.0 spring-test Apache 2.0 spring-tx Apache 2.0 spring-web Apache 2.0 spring-webmvc Apache 2.0 xz Public Domain wasabi Apache 2.0 snappy-java Apache 2.0 xmlunit-core Apache 2.0 xmlunit-legacy Apache 2.0 snakeyaml Apache 2.0 JUnitParams Apache 2.0 postgresql BSD postgresql BSD jedis MIT scala-library Scala ion-java Apache 2.0 timeslice_service-thrift New Relic timeslice_service New Relic xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3_min Public Domain zkclient Apache 2.0 zookeeper Apache 2.0 CoffeeScript Library License CoffeeScript MIT coffin MIT JavaScript JavaScript License ie_html5/ie_html5.js MIT Angular.js MIT Angular UI directives for Bootstrap MIT Chart.js MIT Chosen JQuery plugin MIT Fullcalendar MIT Moment MIT Crash reporting Software License Apple OS X OS X Yosemite Xcode & Apple SDK Xcode license Label services Service License BoneCP Apache 2.0 Gradle Apache 2.0 Hibernate Apache 2.0 Spring Apache 2.0 Infrastructure Monitoring services Service License antlr BSD aopalliance Public Domain asm MIT cglib Apache 2.0 ch.qos.logback EPLv1.0 com.101tec Apache 2.0 com.amazonaws Apache 2.0 com.cenqua.clover Paid com.fasterxml.jackson.core Apache 2.0 com.fasterxml Apache 2.0 com.google.code.findbugs BSD com.google.guava Apache 2.0 com.googlecode.json-simple Apache 2.0 com.intellij Apache 2.0 New Relic com.squareup.dagger Apache 2.0 com.squareup Apache 2.0 com.yammer.metrics Apache 2.0 commons-codec Apache 2.0 commons-configuration Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 dom4j BSD io.netty Apache 2.0 javax.annotation CDDLv1 javax.inject Apache 2.0 javax.servlet.jsp CDDLv1 javax.servlet Apache 2.0 javax.validation Apache 2.0 javax.websocket CDDLv1 jline BSD joda-time Apache 2.0 junit CPL log4j Apache 2.0 mysql GPLv2 with Classpath Exception net.sf.jopt-simple MIT org.apache.commons Apache 2.0 org.apache.httpcomponents Apache 2.0 org.apache.ivy Apache 2.0 org.apache.thrift Apache 2.0 org.apache.tomcat.embed Apache 2.0 org.apache.zookeeper Apache 2.0 org.codehaus.groovy Apache 2.0 org.codehaus.jackson Apache 2.0 org.easymock Apache 2.0 org.eclipse.jetty.orbit CDDLv1 org.eclipse.jetty.toolchain Apache 2.0 org.eclipse.jetty.websocket Apache 2.0 org.eclipse.jetty CDDLv1 org.glassfish.jersey.core CDDLv1 org.glassfish.web CDDLv1 org.glassfish CDDLv1 org.hamcrest BSD org.hibernate Apache 2.0 org.javassist Apache 2.0 org.jboss.logging LGPLv2.1 org.jenkins-ci.plugins Apache 2.0 org.jenkins-ci Apache 2.0 org.jetbrains Apache 2.0 org.jolokia Apache 2.0 org.jvnet.hudson BSD org.mockito MIT org.objenesis Apache 2.0 org.ow2.asm MIT org.reflections WTFPL org.scala-lang Scala org.slf4j MIT org.springframework.boot Apache 2.0 org.springframework Apache 2.0 org.xerial.snappy Apache 2.0 org.yaml Apache 2.0 xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3 Public Domain .NET support uploader service Library License Amazon AWS SDK for Java Apache 2.0 Apache Commons Codec Apache 2.0 Gson Apache 2.0 Jetty Apache 2.0 Ruby gems Ruby gems Gem License aasm MIT actionmailer MIT actionpack MIT activeadmin MIT actionview MIT activejob MIT active_model_serializers MIT activemodel MIT activerecord MIT activerecord-deprecated_finders MIT activerecord-mysql2-adapter MIT activeresource MIT activesupport MIT addressable Apache 2.0 angularjs-rails MIT arbre MIT arel MIT arel MIT atomic Apache 2.0 atomic Apache-2.0 attr_required MIT awesome_print MIT backports MIT better_errors MIT binding_of_caller MIT bourbon MIT brwsr MIT browser-timezone-rails MIT builder MIT bundler MIT byebug MIT callsite MIT capistrano MIT capistrano-bundler MIT capistrano-ext MIT capistrano-rbenv MIT capistrano-stats MIT capybara MIT celluloid MIT CFPropertyList MIT chunky_png MIT ci_reporter MIT ci_reporter_rspec MIT clockwork MIT codeclimate-test-reporter MIT coderay MIT coderay MIT coffee-rails MIT coffee-script MIT coffee-script-source MIT columnize Ruby compass MIT compass-core MIT compass-import-once MIT compass-rails MIT concurrent-ruby MIT crack MIT crypt Unlicensed daemons MIT dalli MIT database_cleaner MIT debug_inspector MIT debugger-linecache MIT diff-lcs MIT docile MIT dotenv MIT dotenv-deployment MIT erubis MIT etcd MIT ethon MIT eventmachine Ruby excon MIT execjs MIT factory_girl MIT factory_girl_rails MIT faraday MIT faraday-middleware MIT faye-websocket MIT ffi MIT ffi BSD fission MIT fog MIT fog-atmos MIT fog-aws MIT fog-brightbox MIT fog-core MIT fog-ecloud MIT fog-google MIT fog-json MIT fog-local MIT fog-powerdns MIT fog-profitbricks MIT fog-radosgw MIT fog-riakcs MIT fog-sakuracloud MIT fog-serverlove MIT fog-softlayer MIT fog-storm_on_demand MIT fog-terremark MIT fog-vmfusion MIT fog-voxel MIT fog-xml MIT font-awesome-rails MIT foreman MIT formatador MIT formtastic MIT formtastic i18n MIT fssm MIT globalid MIT guard MIT guard-rspec MIT haml MIT haml-rails MIT has scope MIT hashie MIT highline Ruby hike MIT hipchat-api MIT histogram MIT hitimes ISC hiredis BSD http_parser.rb MIT httpclient Ruby httpclient Ruby httparty MIT i18n MIT inflecto MIT inherited resources MIT intercom-rails MIT ipaddress MIT jasmine MIT jasmine-core MIT jasmine-rails MIT jbuilder MIT jira MIT jira-ruby OSL-3.0 journey MIT jquery-rails MIT jquery-turbolinks MIT jquery-ui-rails MIT json Ruby jwt MIT kaminari MIT kgio LGPLv3 kgio LGPLv3 kgio LGPL-v2.1+ libv8 MIT listen MIT logger Ruby loofah MIT lumberjack MIT macaddr MIT mail MIT mailcatcher MIT memoist MIT metaclass MIT meta_request MIT method_source MIT mime-types MIT mini_portile MIT minitest MIT minitest-rails MIT mixlib-log Apache 2.0 mocha MIT mono_logger MIT multi_json MIT multi_xml MIT multipart-post MIT mysql2 MIT net-http-persistent MIT net-scp MIT net-sftp MIT net-ssh MIT net-ssh-gateway MIT netrc MIT nokogiri MIT oauth2 MIT oj MIT omniauth MIT omniauth-oauth2 MIT pagerduty MIT papers MIT pg BSD pg Ruby phantomjs BSD phantomjs-binaries Unlicensed poltergeist MIT polyamorous MIT polyglot MIT polyglot MIT protected_attributes MIT pry MIT pry-nav MIT pry-rails MIT pry-stack_explorer MIT puma BSD quiet assets MIT rack MIT rack-cache MIT rack-contrib MIT rack-oauth2 MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-ssl MIT rack-ssl-enforcer MIT rack-test MIT rails MIT rails-deprecated_sanitizer MIT rails-dom-testing MIT rails-html-sanitizer MIT rails-observers MIT rails_12factor MIT rails_serve_static_assets MIT rails_stdout_logging MIT railties MIT raindrops LGPLv3 raindrops LGPLv3 raindrops LGPLv2.1+ rake MIT rake MIT rake MIT rake MIT ransack MIT rb-fsevent MIT rb-inotify MIT rdoc Ruby redcarpet MIT redis MIT redis-namespace MIT redis-queue MIT ref MIT remote_syslog_logger MIT request_store MIT responders MIT resque MIT resque-cleaner MIT resque-pool MIT resque-scheduler MIT resque-status MIT rest-client MIT restforce MIT rspec MIT rspec-core MIT rspec-expectations MIT rspec-mocks MIT rspec-rails MIT rspec-support MIT rspec_junit_formatter MIT rubyntlm MIT ruby-saml MIT rufus-scheduler MIT safe_yaml MIT salesforce_bulk_query BSD sass MIT sass-rails MIT sass-rails MIT sdoc MIT secure_headers Apache 2.0 sequel MIT serveza MIT settingslogic MIT shoulda MIT shoulda-context MIT shoulda-matchers MIT simplecov MIT simplecov-html MIT simplecov-rcov MIT sinatra MIT sinatra-activerecord MIT sinatra-contrib MIT skinny MIT slop MIT spring MIT sprockets MIT sprockets-rails MIT sqlite3 BSD sshkit GPL-No Distro sys-uname Ruby syslog_protocol MIT systemu BSDL term-ansicolor GPL-No Distro terminal-table MIT therubyracer MIT thin Ruby thor MIT thread_safe Apache-2.0 thrift Apache 2.0 thrift-rack MIT tilt MIT timecop MIT timers MIT tins MIT treetop MIT turbolinks MIT twitter-bootstrap-rails MIT typhoeus MIT tzinfo MIT uglifier MIT unicorn Ruby unicorn-rails MIT uuid MIT uuidtools Apache-2.0 vegas MIT webmock MIT web-console MIT xml-simple Ruby xpath MIT yard MIT Storage services Library License Amazon AWS SDK for Java Apache 2.0 Amazon AWS Command Line Interface Apache 2.0 Go packages Library License Go-Mysql-Driver Mozilla Public License 2.0 gocql BSD 3-Clause groupcache Apache 2.0 Migrate MIT go-sqlite3 MIT Revel MIT Pathtree MIT Ansicolor MIT fsnotify BSD 3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.20683,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Services <em>licenses</em>",
        "sections": "Services <em>licenses</em>",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Contents Java internal"
      },
      "id": "603ea24364441f91fb4e8864"
    },
    {
      "sections": [
        "Add-on end user license agreement",
        "New Relic, Inc. Add-on End User License Agreement"
      ],
      "title": "Add-on end user license agreement",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "a01c225ca30f95dab7db856cd946c76de557c31f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/add-end-user-license-agreement/",
      "published_at": "2022-01-08T09:14:04Z",
      "updated_at": "2021-03-16T06:19:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic, Inc. Add-on End User License Agreement In connection with the work provided by New Relic's Expert Services, Field Instrumentation Team, or Sales team, you may be provided with certain custom-created software to visualize, enable, optimize, or enhance your use of New Relic's Services. By downloading, installing, authorizing installation, or using the Add-on (defined below) with your Underlying Software (defined below), you (“Customer”) agree to the terms and conditions herein (“Agreement”) with New Relic, Inc., a Delaware corporation with offices located at 188 Spear Street, Suite 1200 San Francisco, CA 94105 (“New Relic\"), (collectively the “Parties”). Capitalized terms not defined herein shall have the meanings set forth in the New Relic Terms of Service (the “Terms of Service”) available here: newrelic.com/termsandconditions/terms. IF YOU DO NOT AGREE TO THIS AGREEMENT, YOUR SOLE REMEDY IS TO NOT USE THE ADD-ON. 1. DEFINITIONS “Add-on” means the New Relic applications and/or software, including but not limited to connectors, extensions, UI extension, and plugins, provided by New Relic to Customer to enable the Services to operate with certain third party or Customer software or systems (“Underlying Software”). For the purposes of the Terms of Service, the Add-on shall be treated like an Agent, subject to the separate terms herein. 2. USE OF THE PROGRAMMABILITY ADD-ON 2.1 Software Evaluation License. Subject to the terms herein, New Relic grants to Customer a limited, non-exclusive, non-transferrable, non-sublicensable right to install, use, and configure the Add-on solely as needed to enable the Services for internal evaluation purposes. New Relic reserves all rights and licenses not expressly granted herein. 2.2 Ownership. As between the Parties, New Relic owns all right, title, and interest to the Add-on and Feedback, including but not limited to any intellectual property and proprietary rights therein. Customer retains all right, title, and interest in any Customer Data processed by the Add-on in connection with the Services. 2.3 Feedback. Customer agrees to provide feedback, suggestions, ideas, requests or recommendations (“Feedback”) regarding the Add-on, and hereby irrevocably assigns all intellectual property and proprietary rights it holds in the Feedback to New Relic. 2.4 Notice. Customer acknowledges and agrees that the Add-on is made to work with the Underlying Software as configured at the time of creation and there is no guarantee that Add-on will continue to work in the event Customer changes, replaces, upgrades versions of, updates, or otherwise changes the Underlying Software (an “Upgrade Event”). Customer shall hold New Relic harmless from any claims or damages arising from Customer’s Upgrade Event. 2.5 Restrictions. Customer will not: (i) use the Add-on except as permitted hereunder; (ii) distribute, sell, sublicense, or otherwise transfer the Add-on; (iii) decompile, disassemble or reverse engineer any software underlying the Add-on; (iv) use the Add-on to damage, disable, overburden or impair any New Relic server or network(s) connected to any New Relic server or interfere with any other party’s use and enjoyment of the Services; (v) use the Add-on to defraud, defame, abuse, harass, stalk, threaten or infringe the rights of privacy or other intellectual property rights of others or otherwise violate any applicable law; (vi) circumvent or modify any security technologies included as part of the Add-on; or (vii) attempt or permit any third party to do any of the foregoing. New Relic may suspend Customer usage of the Services, without notice, pending any investigation of misuse. These restrictions may be supplemented or superceded (to the extent they conflict) by the New Relic Acceptable Use Policy as may be published and updated from time to time on the New Relic web site at docs.newrelic.com/docs/licenses/license-information/acceptable-use-policy/acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential Information” means: (a) the Programmability Add-on, and any features, results or output produced by, and other information relating to the Add-on (including, without limitation, all Feedback); and (b) any business or technical information of New Relic including but not limited to any technical information, research, development, know-how that a reasonable person would understand to be confidential. 3.2 Restrictions. Customer will not use or disclose any Confidential Information, except as necessary for the performance of this Agreement. Customer will use all reasonable efforts to protect Confidential Information from unauthorized use or disclosure, but in no event less than the efforts that it ordinarily uses with respect to its own proprietary information. Customer may disclose Confidential Information to those of its employees who have a bona fide need to know such Confidential Information for the performance of this Agreement; provided that each such employee first executes a written agreement that contains use and nondisclosure restrictions at least as protective as those set forth herein. Confidential Information shall not include any information that: (a) is or becomes generally known to the public through no fault or breach of this Agreement by Customer; (b) is rightfully known by Customer at the time of disclosure without an obligation of confidentiality; (c) is independently developed by Customer without access or use of any Confidential Information; or (d) is rightfully obtained from a third party without restriction on use or disclosure. 4. DISCLAIMER 4.1 DISCLAIMER. THE ADD-ON IS PROVIDED AS-IS AND AS-AVAILABLE AND NEW RELIC DISCLAIMS AND MAKES NO WARRANTIES, EITHER EXPRESS OR IMPLIED, INCLUDING ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR NON-INFRINGEMENT AND WHETHER OR NOT ARISING THROUGH A COURSE OF DEALING. THE ADD-ON IS NOT GUARANTEED TO BE ERROR-FREE, COMPATIBLE WITH THE UNDERLYING SOFTWARE, OR THAT CUSTOMER WILL ACHIEVE ANY RESULTS FROM USE OF THE ADD-ON THEREFROM. 4.2 LIMITATION OF LIABILITY. TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC, ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE TO CUSTOMER OR ANY THIRD PARTY FOR DAMAGES OF ANY KIND, INCLUDING, WITHOUT LIMITATION, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE INSTALLATION, USE OR INABILITY TO USE THE ADD-ON OR FOR ANY ERROR OR DEFECT IN THE ADD-ON OR THE SERVICES, REGARDLESS OF LEGAL THEORY, WHETHER OR NOT NEW RELIC HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSS OR DAMAGE. NEW RELIC’S AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS AGREEMENT WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE PARTIES HAVE AGREED THAT THESE LIMITATIONS WILL SURVIVE AND APPLY EVEN IF ANY LIMITED REMEDY SPECIFIED IN THIS AGREEMENT IS FOUND TO HAVE FAILED OF ITS ESSENTIAL PURPOSE. THE LIMITATIONS OF DAMAGES SET FORTH ABOVE ARE FUNDAMENTAL ELEMENTS OF THE BASIS OF THE BARGAIN BETWEEN NEW RELIC AND CUSTOMER. 4.3 EVALUATION VERSION. Customer agrees and acknowledges that: (a) the Add-on is not an official product and has not been commercially released for sale by New Relic; (b) the Add-on may not operate properly, being in final form, or fully functional; (c) the Add-on may contain errors, security vulnerabilities, design flaws, or other problems; (d) it may not be possible to make the Add-on fully functional; (e) the information obtained using the Add-on may not be accurate; (f) use of the Add-on may result in unexpected results, loss of data, delays or other unpredictable damages or loss; (g) New Relic is under no obligation to release a commercial version of the Add-on; and (h) New Relic has the right unilaterally to abandon development of the Add-on, at any time and without any obligation or liability to Customer. 5. GENERAL PROVISIONS 5.1 Terms of Service; Documentation. This Agreement shall be considered a part of the New Relic documentation, located at: https://docs.newrelic.com (the “Documentation). This Agreement and the Terms of Service constitute the entire and exclusive agreement between New Relic and Customer with respect to the Add-on. In the absence of a separate agreement, upon agreement between New Relic and Customer with respect to the Add-On, the terms and conditions of this Agreement shall govern the relationship between New Relic and Customer with respect to such Add-On. To the extent of a conflict between the Agreement and the Terms of Service, this Agreement shall govern with respect to the Add-on only.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.76915,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Add-on end user <em>license</em> agreement",
        "sections": "Add-on end user <em>license</em> agreement",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": "&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;acceptable-use-policy&#x2F;acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential <em>Information</em>” means: (a) the Programmability Add-on, and any features, results or output produced by, and other <em>information</em> relating to the Add-on (including, without limitation"
      },
      "id": "603ec23328ccbccf1beba79a"
    },
    {
      "sections": [
        "FIT instrumentation end user license agreement"
      ],
      "title": "FIT instrumentation end user license agreement",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "e8e10baf120678407d08c9f78ab708d271cf0223",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/fit-instrumentation-end-user-license-agreement/",
      "published_at": "2022-01-08T09:14:46Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "NEW RELIC, INC. FIT INSTRUMENTATION END USER LICENSE AGREEMENT In connection with the work provided by New Relic’s Field Instrumentation Team, you may be provided with certain custom-created software to enable, optimize, or enhance your use of New Relic’s Services. By downloading, installing, authorizing installation, or using the FIT Instrumentation with your Underlying Software (defined below), you (“Customer”) agree to the terms and conditions herein (“Agreement”) with New Relic, Inc., a Delaware corporation with offices located at 188 Spear Street, Suite 1200 San Francisco, CA 94105 (“New Relic”), (collectively the “Parties”). Capitalized terms not defined herein shall have the meanings set forth in the New Relic Terms of Service (the “Terms of Service”) available here: http://newrelic.com/terms IF YOU DO NOT AGREE TO THIS AGREEMENT, PLEASE DO NOT USE THE FIT INSTRUMENTATION. 1. DEFINITIONS “FIT Instrumentation” means the New Relic custom-made software, including but not limited to connectors, extensions, and plugins, provided by New Relic to Customer to enable the Services to operate with certain third party or Customer software or systems (“Underlying Software”). For the purposes of the Terms of Service, the FIT Instrumentation shall be treated like an “Agent”, subject to the separate terms herein. 2. USE OF THE FIT INSTRUMENTATION 2.1 Software Evaluation License. Subject to the terms herein, New Relic grants to Customer a limited, non-exclusive, non-transferrable, non-sublicensable right to install, use, and configure the FIT Instrumentation solely as needed to enable the Services for internal evaluation purposes. New Relic reserves all rights and licenses not expressly granted herein. 2.2 Ownership. As between the Parties, New Relic owns all right, title, and interest to the FIT Instrumentation and Feedback, including but not limited to any intellectual property and proprietary rights therein. Customer retains all right, title, and interest in any Customer Data processed by the FIT Instrumentation in connection with the Services. 2.3 Feedback. Customer agrees to provide feedback, suggestions, ideas, requests or recommendations (“Feedback”) regarding the FIT Instrumentation, and hereby irrevocably assigns all intellectual property and proprietary rights it holds in the Feedback to New Relic. 2.4 Notice. Customer acknowledges and agrees that the FIT Instrumentation is made to work with the Underlying Software as configured at the time of creation and there is no guarantee that FIT Instrumentation will continue to work in the event Customer changes, replaces, upgrades versions of, updates, or otherwise changes the Underlying Software (an “Upgrade Event”). Customer shall hold New Relic harmless from any claims or damages arising from Customer’s Upgrade Event. 2.5 Restrictions. Customer will not: (i) use the FIT Instrumentation except as permitted hereunder; (ii) distribute, sell, sublicense, or otherwise transfer the FIT Instrumentation; (iii) decompile, disassemble or reverse engineer any software underlying the FIT Instrumentation; (iv) use the FIT Instrumentation to damage, disable, overburden or impair any New Relic server or network(s) connected to any New Relic server or interfere with any other party’s use and enjoyment of the Services; (v) use the FIT Instrumentation to defraud, defame, abuse, harass, stalk, threaten or infringe the rights of privacy or other intellectual property rights of others or otherwise violate any applicable law; (vi) circumvent or modify any security technologies included as part of the FIT Instrumentation; or (vii) attempt or permit any third party to do any of the foregoing. New Relic may suspend usage of the Services by any user, without notice, pending any investigation of misuse.  These restrictions may be supplemented or superceded (to the extent they conflict) by the New Relic Acceptable Use Policy as may be published and updated from time to time at https://docs.newrelic.com/docs/licenses/license-information/acceptable-use-policy/acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential Information” means: (a) the FIT Instrumentation, and any features, results or output produced by, and other information relating to the FIT Instrumentation (including, without limitation, all Feedback); and (b) any business or technical information of New Relic including but not limited to any technical information, research, development, know-how that a reasonable person would understand to be confidential. 3.2 Restrictions. Customer will not use or disclose any Confidential Information, except as necessary for the performance of this Agreement. Customer will use all reasonable efforts to protect Confidential Information from unauthorized use or disclosure, but in no event less than the efforts that it ordinarily uses with respect to its own proprietary information. Customer may disclose Confidential Information to those of its employees who have a bona fide need to know such Confidential Information for the performance of this Agreement; provided that each such employee first executes a written agreement that contains use and nondisclosure restrictions at least as protective as those set forth herein. Confidential Information shall not include any information that: (a) is or becomes generally known to the public through no fault or breach of this Agreement by Customer; (b) is rightfully known by Customer at the time of disclosure without an obligation of confidentiality; (c) is independently developed by Customer without access or use of any Confidential Information; or (d) is rightfully obtained from a third party without restriction on use or disclosure. 4. DISCLAIMER 4.1 DISCLAIMER. FIT INSTRUMENTATION IS PROVIDED AS-IS AND AS-AVAILABLE AND NEW RELIC DISCLAIMS AND MAKES NO WARRANTIES, EITHER EXPRESS OR IMPLIED, INCLUDING ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR NON-INFRINGEMENT AND WHETHER OR NOT ARISING THROUGH A COURSE OF DEALING. FIT INSTRUMENTATION IS NOT GUARANTEED TO BE ERROR-FREE, COMPATIBLE WITH THE UNDERLYING SOFTWARE. , OR THAT CUSTOMER WILL ACHIEVE ANY RESULTS FROM USE OF THE FIT INSTRUMENTATION THEREFROM. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OF IMPLIED WARRANTIES OR LIMITATIONS ON APPLICABLE STATUTORY RIGHTS OF A CONSUMER, SO THE ABOVE EXCLUSION AND LIMITATIONS MAY NOT APPLY TO THE CUSTOMER. 4.2 LIMITATION OF LIABILITY. TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE TO CUSTOMER OR ANY THIRD PARTY FOR DAMAGES OF ANY KIND, INCLUDING, WITHOUT LIMITATION, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE INSTALLATION, USE OR INABILITY TO USE THE FIT INSTRUMENTATION OR FOR ANY ERROR OR DEFECT IN THE FIT INSTRUMENTATION OR THE SERVICES, WHETHER SUCH LIABILITY ARISES FROM ANY CLAIM BASED UPON CONTRACT, WARRANTY, TORT, STRICT LIABILITY, OR OTHERWISE, WHETHER OR NOT NEW RELIC HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSS OR DAMAGE. NEW RELIC'S AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS AGREEMENT WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE PARTIES HAVE AGREED THAT THESE LIMITATIONS WILL SURVIVE AND APPLY EVEN IF ANY LIMITED REMEDY SPECIFIED IN THIS AGREEMENT IS FOUND TO HAVE FAILED OF ITS ESSENTIAL PURPOSE. 4.3 EVALUATION VERSION. Customer agrees and acknowledges that: (a) the FIT Instrumentation is not an official product and has not been commercially released for sale by New Relic; (b) the FIT Instrumentation may not operate properly, being in final form, or fully functional; (c) the FIT Instrumentation may contain errors, security vulnerabilities, design flaws, or other problems; (d) it may not be possible to make the FIT Instrumentation fully functional; (e) the information obtained using the FIT Instrumentation may not be accurate; (f) use of the FIT Instrumentation may result in unexpected results, loss of data, delays or other unpredictable damages or loss; (g) New Relic is under no obligation to release a commercial version of the FIT Instrumentation; and (h) New Relic has the right unilaterally to abandon development of the FIT Instrumentation, at any time and without any obligation or liability to Customer. 5. GENERAL PROVISIONS 5.1 Terms of Service; Documentation. This Agreement shall be considered a part of the New Relic documentation, located at: https://docs.newrelic.com (the \"Documentation\").This Agreement and the Terms of Service constitute the entire and exclusive agreement between New Relic and Customer with respect to the FIT Instrumentation. In the absence of a separate agreement between New Relic and Customer with respect to the FIT Instrumentation, the terms and conditions of this Agreement shall govern the relationship between New Relic and Customer with respect to such FIT Instrumentation.  To the extent of a conflict between the Agreement and the Terms of Service, this Agreement shall govern with respect to the FIT Instrumentation only.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.76753,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "FIT instrumentation end user <em>license</em> agreement",
        "sections": "FIT instrumentation end user <em>license</em> agreement",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": " Acceptable Use Policy as may be published and updated from time to time at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;acceptable-use-policy&#x2F;acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential <em>Information</em>” means: (a) the FIT Instrumentation, and any features, results"
      },
      "id": "603e9f3ee7b9d206e32a0800"
    }
  ],
  "/docs/licenses/license-information/faq/new-relic-one-pricing-plan-frequently-asked-questions": [
    {
      "sections": [
        "Translation disclaimer"
      ],
      "title": "Translation disclaimer",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Translation"
      ],
      "external_id": "cb23af79521feaaa6193002aef89648a9973cf1d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/translated-documentation/translation-disclaimer/",
      "published_at": "2022-01-08T07:40:57Z",
      "updated_at": "2021-12-09T01:58:23Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The official version of the Documentation is in English. Portions of the New Relic Documentation may be translated for your convenience as shown on this site. Translated content may contain errors, discrepancies, or differences created in the translation, are not binding, and have no legal effect for enforcement or compliance purposes. New Relic does not make any warranty, express or implied, as to the accuracy, reliability, or correctness of any translated Documentation from the English original to any other language. Any guarantees or commitments in your Agreement with New Relic that the Services, products, or other offerings conform or are consistent with the Documentation do not apply to the extent to which the Documentation has been translated.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 142.2494,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>"
      },
      "id": "61b162bf196a672b190d46f7"
    },
    {
      "sections": [
        "Security guide",
        "Tip",
        "Security Program",
        "Security Domains",
        "Security Certifications",
        "Data Control, Facilities, and Encryption",
        "Law Enforcement Request Report"
      ],
      "title": "Security guide",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "356f0d11ffcb62208a743a0a7c127f5f6da9c940",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-guide/",
      "published_at": "2022-01-08T12:36:19Z",
      "updated_at": "2021-12-09T15:28:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Last updated September 17, 2021. This is supplement to our security policy and serves as a guide to New Relic’s description of its Services, functionalities, and features. Tip We may update the URLs in this document without notice. Security Program New Relic follows \"privacy by design\" principles as described here: https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Security Domains New Relic’s policies and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless Security; Configuration Management; Vulnerability Management; Network Protection; Transmission Protection; Password Management; Access Control, Audit Logging & Monitoring; Education, Training, and Awareness; Third Party Assurance; Incident Management; Business Continuity and Disaster Recover; Risk Management; Data Protection & Privacy; and Service Management Systems. Security Certifications New Relic audits its Services against industry standards as described at https://docs.newrelic.com/docs/security/security-privacy/compliance/regulatory-audits-new-relic-services/. Data Control, Facilities, and Encryption New Relic's customers can send data to New Relic's APIs by (1) using New Relic's software, (2) using vendor-neutral software that is managed and maintained by a third-party such as via OpenTelemetry instrumentation provided by opentelemetry.io, or (3) from third-party systems that customer's manage and/or control. New Relic's customers can use New Relic's Services such as NerdGraph to filter out and drop data. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/drop-data-using-nerdgraph/. New Relic's customers can adjust their data retention periods as appropriate for their needs. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/manage-data-retention/#adjust-retention. New Relic Logs obfuscates numbers that match known patterns, such as bank card and social security numbers as described here: https://docs.newrelic.com/docs/logs/log-management/get-started/new-relics-log-management-security-privacy/. New Relic honors requests to delete personal data in accordance with applicable privacy laws. Please see https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Customers may use New Relic's APIs to query data, such as NerdGraph described here, and New Relic Services to export the data to other cloud providers. Customers can configure its log forwarder [https://docs.newrelic.com/docs/logs/enable-log-management-new-relic/enable-log-monitoring-new-relic/forward-your-logs-using-infrastructure-agent/] before sending infrastructure logs to New Relic. For New Relic Customers in New Relic US, FedRAMP and HIPAA-enabled environments, Customer Data is replicated to the off-site backup system via Amazon Simple Storage Service (S3). Category of Customer Description FedRAMP HIPAA-enabled US Gen Pop EU Gen Pop Data is stored in Amazon Web Services (“AWS”). Limited Data is stored in IBM Data for New Relic Incident Intelligence is stored in Google Cloud New Relic regularly tests, assess, and evaluates its measures to ensure the security of processing using industry-recognized standards and uses independent third-party auditors as provided below: Annual SOC 2 Type 2 Annual FedRAMP assessment by an independent third-party pursuant to NIST 800-53 rev 4 Moderate authorization. Annual HITRUST-validated assessment by an independent third-party *Pursuing CY2021 Q4 ISO 27001 TISAX The Services that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed information about AWS security is available at https://aws.amazon.com/security/ and http://aws.amazon.com/security/sharing-the-security-responsibility/. Data encryption at rest utilizes FIPS 140-2 compliant encryption methodology. For AWS SOC Reports, please see https://aws.amazon.com/compliance/soc-faqs/. The Services that operate on Google Cloud Platform (\"GCP\") are protected by the security and environmental controls of GCP. Detailed information about GCP security is available at https://cloud.google.com/docs/tutorials#security. For GCP reports, please see https://cloud.google.com/security/compliance/. IBM Deft Zayo QTS Law Enforcement Request Report New Relic has not to date received any request for customer data from a law enforcement or other government agency (including under any national security process), and has not made any corresponding disclosures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 131.12225,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed <em>information</em> about AWS security is available at https:&#x2F;&#x2F;aws.amazon.com&#x2F;security&#x2F; and http:&#x2F;&#x2F;aws.amazon.com&#x2F;security&#x2F;sharing-the-security-responsibility&#x2F;. Data encryption at rest utilizes"
      },
      "id": "6147558128ccbc973a56a863"
    },
    {
      "sections": [
        "New Relic One pricing: Definitions",
        "Account",
        "Commitment Term",
        "Customer Data",
        "Customer Properties",
        "Documentation",
        "GB Ingested",
        "Incident event",
        "Login Credentials",
        "Monthly Provisioned User",
        "Order",
        "Paid Terms of Service",
        "Product(s)",
        "Software",
        "Terms",
        "Third-Party Services",
        "Unpaid Terms of Service",
        "Usage Plan"
      ],
      "title": "New Relic One pricing: Definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "609575acd671fecf7899378157eabc57bc8d68e2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/new-relic-one-pricing-definitions/",
      "published_at": "2022-01-08T09:16:18Z",
      "updated_at": "2021-11-25T06:51:33Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our New Relic One pricing model (for a glossary for our original pricing, see Original pricing definitions. Account Account refers to the online account or subaccounts that New Relic provides for customers to manage their use of the Products. Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, information, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer Properties Customer Properties means Customer’s websites, infrastructure, networks, mobile applications, or other systems, as well as Customer accounts on Third-Party Services. Documentation Documentation means the New Relic technical guides and documentation made available from the dedicated ‘Documentation’ page of the New Relic website. GB Ingested A GB Ingested is a measurement of the volume of metrics, events, logs, traces, or other telemetry data sent to or generated by the Products for the benefit of the Customer, including from the Software, the Customer Properties, or Third-Party Services. In this context, a GB is defined as 1 billion bytes. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Login Credentials Login Credentials means the username, email address, password, or other personal information that is provided by a Customer user in order to manage an Account. Monthly Provisioned User A Monthly Provisioned User is any user who can log into Customer’s Account(s) and access the New Relic One Product functionality as specified in an Order and the Documentation. In our public docs, this is referred to as a full platform user or a full user. Order Order means the purchasing order for access to the Service or related services that: (1) is either executed by the Parties or entered into by you via self-service, and references this Agreement, or (2) is entered into by you and a Channel Partner. Paid Terms of Service Paid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/paid. Product(s) Product(s) mean the purchase of the New Relic subscription products described in the applicable Order and any updates, corrections, bug fixes, modifications, improvements, related services, new features, and functionality (made generally available to New Relic’s customer base) thereto. Software Software means the distributed software, APIs, scripts, or other code proprietary to New Relic provided with the Products. Terms Terms means the underlying Customer-New Relic agreement and the Order. Third-Party Services Third-Party Services means any third party platform, add-on, service, or product not provided by New Relic and that a user integrates or enables for use with the Products, including third-party applications and plug-ins. Unpaid Terms of Service Unpaid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/unpaid. Usage Plan Usage Plan refers to the Service or Product pricing, invoicing related information, and product-specific terms (e.g. concurrent user account sessions) contained within the Documentation. To learn more about this pricing model, see New Relic One pricing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 123.27113,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": ". Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, <em>information</em>, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer"
      },
      "id": "6044e6e528ccbc26f22c6084"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/acceptable-use-policy": [
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Important",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos",
        "RSS - Atom"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2022-01-08T09:15:34Z",
      "updated_at": "2021-12-04T17:54:46Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Important If you are a New Relic HIPAA customer, please follow the requirements specified in the Global Technical Support Section of HIPAA enablement - what you need to know and do when you request support and engage with the New Relic Global Technical Support team. Support plan for New Relic One pricing and packaging model The New Relic One Support Plan below applies only to a customer’s paid subscription on the New Relic One pricing model. Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing model, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customer’s paid service subscription to non-New Relic One Products (our original product-based pricing model). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relic’s distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relic’s Global Technical Support provides assistance for: the New Relic One platform and its features and capabilities, New Relic's monitoring and observability solutions, and our alerting and Applied Intelligence features. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and we’re here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relic’s Open Source Community Plus Projects. New Relic’s Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relic’s Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relic’s Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered “generally available.” If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we can’t help with things we didn’t build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and we’ll help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. We’ll do our best to determine whether an issue is with New Relic’s Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the “account owner,” such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relic’s Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Important As of March 10, 2021, you're no longer able to create a New Relic support ticket by emailing support@newrelic.com. If you have ticketed support as part of your subscription, when logged into your New Relic account, go to one.newrelic.com to create a support ticket. Once you're there, follow these steps: Click the ? icon in the upper right hand corner. Click I need more help. Select Create a Support Ticket. By creating a support ticket directly within your account, it will speed up the support process and improve our team’s ability to troubleshoot your issue more effectively. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relic’s Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources. RSS - Atom Subscribe to the RSS feed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 239.20935,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " Relic <em>General</em> <em>usage</em> and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments &amp; frameworks Our Products are fully tested with the compatible"
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "New Relic data usage limits and policies",
        "View limits and manage data"
      ],
      "title": "New Relic data usage limits and policies",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "fc32c25b40a030ffa0fad6bfc95be7fca1360ee1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies/",
      "published_at": "2022-01-08T09:15:35Z",
      "updated_at": "2021-10-19T04:02:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers' accounts, we have various data volume and rate limits in place. We reserve the right to enforce these limits to protect our system and to avoid issues for you and other customers. If your New Relic account, whether by configuration or by error, exceeds one of these limits, it or its child accounts might experience one or both of the following: Sampling of data Temporary pause or cessation of data collection To learn more about how hitting a limit can affect your data, see View limits. If you have further questions about these limits, your contract, or a limit you've reached, contact your New Relic account representative. We can work with you to adjust any rate limits to meet your needs. View limits and manage data For information about system and account limits, and for links to data ingest API limits, go to View limits. To manage your data ingest, storage, and limits for organization or billing purposes, go to Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 214.32797,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic data <em>usage</em> limits and policies",
        "sections": "New Relic data <em>usage</em> limits and policies",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data <em>usage</em> spikes in one New Relic account from impacting other customers&#x27; accounts, we have various data volume and rate limits in place. We reserve the right to enforce"
      },
      "id": "603eb1c528ccbc0311eba7c7"
    },
    {
      "sections": [
        "The People's Republic of China"
      ],
      "title": "The People's Republic of China",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "c3c078b8139d695b928d2001cd6c6c9318c43599",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/peoples-republic-china/",
      "published_at": "2022-01-08T08:25:09Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important Information Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory requirements in the PRC. New Relic does not provide support for, including but not limited to, the deployment, access, or use of the Service(s) or Software in the PRC, or otherwise in respect of assets in the PRC (“PRC Use”). Without overriding any express prohibitions that you may have agreed to as part of your agreement with New Relic, you (and you as an agent of the respective New Relic Customer (\"Customer\")) acknowledge that any PRC use is subject to the PRC national firewall system and may be subject to outages and other interference outside of the control of New Relic. Accordingly, you acknowledge and agree (on behalf of yourself and including any Customer on whose behalf you use the Service(s) or Software) that any PRC Use is at your sole risk and is fully excepted from all representations and warranties, including, but not limited to, any terms that assure the confidentiality, integrity, availability or privacy of your data. New Relic makes no other representations and hereby expressly disclaims any and all warranties in respect of PRC Use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 197.88037,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "Important <em>Information</em> Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory"
      },
      "id": "603eb3a328ccbcb488eba76b"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings": [
    {
      "sections": [
        "Acceptable use policy",
        "In connection with your use of the New Relic Properties, you will not, and you will not allow third parties to::",
        "Harm New Relic’s Properties and interests, such as:",
        "Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as:",
        "Violate any applicable law or regulation or for high-risk purposes, such as:",
        "New Relic Properties do not include Third-Party Services",
        "Updates, Contact Information and Violations"
      ],
      "title": "Acceptable use policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b006ab295dae6522e8c76fcd47b3a0d4a45938e4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/acceptable-use-policy/",
      "published_at": "2022-01-08T09:14:45Z",
      "updated_at": "2021-10-30T21:48:45Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic’s mission is to instrument, measure and improve the internet to help our customers create more perfect software, experiences and businesses. We strive to keep our resources operating efficiently, so our services are available to all subscribers. Because you have access to shared resources, we have put these rules in place to ensure everyone has a great experience. For example, you as a tenant would not want other tenants to engage in the types of activities described below. To help us do this, we have put some rules in place regarding your use of the New Relic Properties and created this Acceptable Use Policy (“AUP”). This AUP applies if you use any New Relic product, service, software, website, forum, page or system (collectively, the “New Relic Properties”) and is part of the technical guides documentation that New Relic makes available at dedicated 'Documentation' pages on New Relic websites, including, but not limited to, https://docs.newrelic.com, https://docs.pixielabs.ai/, and https://docs.newrelic.com/docs/codestream (the “Documentation”). In connection with your use of the New Relic Properties, you will not, and you will not allow third parties to:: Harm New Relic’s Properties and interests, such as: Uploading, transmitting or otherwise provide content that infringes New Relic’s or a third party’s intellectual property, privacy or other rights, violates applicable laws or regulations or contains viruses, worms, harmful code, malware or other harmful materials; Hosting, selling, reselling, renting, exploiting, sublicensing, leasing, or otherwise providing the New Relic Properties or any portion thereof or use such for time sharing purposes or on a service bureau basis without our express written permission; Modifying, disabling, or compromising the integrity or performance of the New Relic Properties or related systems, networks, or data; including by: Attempting to compromise the integrity of the New Relic Properties, including probing, scanning or testing the vulnerability of any part of the New Relic Properties without proper authorization; Overwhelming our infrastructure (such as by using “botnets”, “robots,” “spiders” and “offline readers”); Going beyond the use parameters for any given service as described in the corresponding Documentation; Using metatags or other “hidden text”; Drastically exceeding your contracted rate of use as set forth in your order or the Documentation; or Consuming an unreasonable amount of storage. Accessing any unauthorized part of the New Relic Properties, or accessing or searching any part of the New Relic Properties by means other than those provided or authorized by New Relic (including “scraping” or using any data mining methods); Sharing your New Relic Properties account or login credentials, including API keys, with any other individual; Deciphering or decrypting transmissions, circumventing any access, authentication or copy restrictions, or otherwise attempting to compromise the security of the New Relic Properties (including any other user’s account); Accessing the New Relic Properties in order to build a similar or competitive website, application or service; or Attempting to do anything else that may result in some form of adverse impact to the New Relic Properties or use of the New Relic Properties by any of our other customers. Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as: Posting or transmitting abusive messages, defamatory, libelous, false or misleading statements, hate speech or messages that incite or threaten violence, or stalk or harass others; Promoting, encouraging, or facilitating hate speech, violence, discrimination based on race, color, sexual orientation, marital status, gender or identity expression, parental status, religion or creed, national origin or ancestry, sex, age, physical or mental disability, veteran status, genetic information, citizenship and/or any other characteristic protected by law. You are not permitted to use New Relic Properties if you are an entity identified by nationally-recognized non-profits as engaging in such activities. Using the New Relic Properties in violation of your company's policy(ies) and procedures, or attempting to modify or gain unauthorized use of or access to, another user's account, website, application, system, equipment or data; Misrepresenting yourself, impersonating another person, falsely implying any sponsorship or association with New Relic or affiliation with any third party, engaging in fraud, hiding or attempt to hide your identity or disguising the origin of any content (including by “spoofing” or “phishing”); Collecting or harvesting any personally identifiable information, including account names, from any other user’s account or the New Relic Properties, or using the New Relic Properties to violate the privacy of others; Including, publishing or posting other people’s private and confidential information without their express permission; Using anyone’s name or trademarks without their express written permission; Using the New Relic Properties to generate or send unsolicited communications, advertising, chain letters, or spam; Soliciting our users for commercial purposes, unless expressly permitted in writing by New Relic; Disparaging anyone; or Disclosing any confidential information obtained through any method contrary to this AUP. Violate any applicable law or regulation or for high-risk purposes, such as: Using the New Relic Properties in violation of any applicable law or regulation, including data, privacy, and export control laws in applicable jurisdictions; Using the New Relic Properties in any situation for which they are not designed, manufactured or intended, such as for use in life support, emergency or mission critical circumstances, or in any activities where use or failure of the New Relic Properties could lead to death, personal injury or property or environmental damage. For example, you may not use, or permit any other person to use, the New Relic Properties in connection with aircraft or other modes of human mass transportation or nuclear or chemical facilities, life support systems, implantable medical equipment, motor vehicles, or weaponry systems; or Querying or otherwise configuring, processing or submitting any personal data that could be legally considered sensitive in any applicable jurisdiction, including, but not limited to: (i) patient, medical, or other protected health information regulated by the Health Insurance Portability and Accountability Act (as amended and supplemented) (“HIPAA”); (ii) personal data about individuals under the age of 16, which for the avoidance of doubt includes any “personal information” as such term is defined under the Children’s Online Privacy Protection Act; (iii) government issued identification numbers, including Social Security numbers, driver’s license numbers and other state-issued identification numbers; (iv) financial account information, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting access to an online account, including the combination of a username or email address along with a password or security question and answer that would permit access to an online account; (vi) special categories of sensitive personal data, (such as defined under Regulation (EU) 2016/679 of the European Parliament), including personal data revealing racial or ethnic origin, political opinions, religious beliefs, trade union membership, physical or mental health or condition, sexual life, sexual orientation, genetic data, biometric data, or the commission or alleged commission any crime or offense; (vii) precise geo-location data; or (viii) any data similar to the above protected under foreign or domestic laws, including without limitation any data subject to regulation under the International Traffic in Arms Regulations (ITAR), 22 C.F.R. §§ 120-130. You represent and warrant to New Relic that you have all necessary rights, consents, and permissions to use and submit data that you send to the New Relic Properties, all without violating or infringing any applicable laws, third-party rights (including intellectual property, publicity, or privacy rights), or any terms or policies governing such data. New Relic Properties do not include Third-Party Services If you choose to use any Third Party Services, your use of Third-Party Services is wholly subject to your separate agreement with the relevant provider. New Relic bears no responsibility or liability for Third-Party Services. If you enable a Third-Party Service with the New Relic Properties, New Relic may access and exchange Customer Data with the Third-Party Service on your behalf and instruction. “Third-Party Services” means any third party platform, add-on, service, or product not provided by New Relic and that a User integrates or enables for use with the Service, including third-party applications and plug-ins. Open source software that New Relic makes separately available for download (e.g. community tools) is, as required, governed by the terms of the applicable open source license. The license for any open source software identified as included in New Relic Properties will, as required, wholly apply to your use of that open source software. Updates, Contact Information and Violations We will occasionally need to modify this AUP to help us continue to provide you with a great experience while using the New Relic Properties. In the event we modify this AUP, we will do so by posting a revised version, and any changes will be effective immediately if you’re a new user of the New Relic Properties and thirty (30) days after posting for all other users. If you continue using the New Relic Properties after we update this AUP, you agree to the latest version of this AUP. You can report a violation of this AUP to: AUP@newrelic.com. Or by mail at: Attn: Legal New Relic, Inc. 188 Spear Street, Suite 1200 San Francisco, CA 94105 This AUP, and our customers’ compliance with it, is essential for enabling us to provide you and our other customers with the New Relic Properties, which we take very seriously. You are wholly and solely responsible for appropriate configuration of systems and software that you own or can control to ensure your compliance with this AUP. So, if we determine in our sole discretion that you have violated this AUP, we may, without limiting any other remedies available to us, permanently or temporarily suspend, limit, or terminate your access to the New Relic Properties without notice or liability. This right applies even if the breach is unintentional or unauthorized if we believe that any such suspension, limitation, or termination is necessary to ensure compliance with laws, or to protect the rights, safety, privacy, security, or property of us or others. In this AUP, the term “content” means: (1) any information, data, text, software, code, scripts, music, sound, photos, graphics, videos, messages, tags, interactive features, or other materials that you post, upload, share, submit, or otherwise provide in any manner to the services and (2) any other materials, content, or data you provide to New Relic or use with the New Relic Properties. As used in this AUP, “you” may refer to an individual user or the legal entity an individual user is employed by that has contracted with New Relic, and “we” means New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 217.81358,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Updates, Contact <em>Information</em> and Violations",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " issued identification numbers, including Social Security numbers, driver’s <em>license</em> numbers and other state-issued identification numbers; (iv) financial account <em>information</em>, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting"
      },
      "id": "603e93bf28ccbc99f3eba7bc"
    },
    {
      "sections": [
        "New Relic data usage limits and policies",
        "View limits and manage data"
      ],
      "title": "New Relic data usage limits and policies",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "fc32c25b40a030ffa0fad6bfc95be7fca1360ee1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies/",
      "published_at": "2022-01-08T09:15:35Z",
      "updated_at": "2021-10-19T04:02:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers' accounts, we have various data volume and rate limits in place. We reserve the right to enforce these limits to protect our system and to avoid issues for you and other customers. If your New Relic account, whether by configuration or by error, exceeds one of these limits, it or its child accounts might experience one or both of the following: Sampling of data Temporary pause or cessation of data collection To learn more about how hitting a limit can affect your data, see View limits. If you have further questions about these limits, your contract, or a limit you've reached, contact your New Relic account representative. We can work with you to adjust any rate limits to meet your needs. View limits and manage data For information about system and account limits, and for links to data ingest API limits, go to View limits. To manage your data ingest, storage, and limits for organization or billing purposes, go to Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 214.32797,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic data <em>usage</em> limits and policies",
        "sections": "New Relic data <em>usage</em> limits and policies",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data <em>usage</em> spikes in one New Relic account from impacting other customers&#x27; accounts, we have various data volume and rate limits in place. We reserve the right to enforce"
      },
      "id": "603eb1c528ccbc0311eba7c7"
    },
    {
      "sections": [
        "The People's Republic of China"
      ],
      "title": "The People's Republic of China",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "c3c078b8139d695b928d2001cd6c6c9318c43599",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/peoples-republic-china/",
      "published_at": "2022-01-08T08:25:09Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important Information Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory requirements in the PRC. New Relic does not provide support for, including but not limited to, the deployment, access, or use of the Service(s) or Software in the PRC, or otherwise in respect of assets in the PRC (“PRC Use”). Without overriding any express prohibitions that you may have agreed to as part of your agreement with New Relic, you (and you as an agent of the respective New Relic Customer (\"Customer\")) acknowledge that any PRC use is subject to the PRC national firewall system and may be subject to outages and other interference outside of the control of New Relic. Accordingly, you acknowledge and agree (on behalf of yourself and including any Customer on whose behalf you use the Service(s) or Software) that any PRC Use is at your sole risk and is fully excepted from all representations and warranties, including, but not limited to, any terms that assure the confidentiality, integrity, availability or privacy of your data. New Relic makes no other representations and hereby expressly disclaims any and all warranties in respect of PRC Use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 197.88037,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "Important <em>Information</em> Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory"
      },
      "id": "603eb3a328ccbcb488eba76b"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/government-addendum": [
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Important",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos",
        "RSS - Atom"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2022-01-08T09:15:34Z",
      "updated_at": "2021-12-04T17:54:46Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Important If you are a New Relic HIPAA customer, please follow the requirements specified in the Global Technical Support Section of HIPAA enablement - what you need to know and do when you request support and engage with the New Relic Global Technical Support team. Support plan for New Relic One pricing and packaging model The New Relic One Support Plan below applies only to a customer’s paid subscription on the New Relic One pricing model. Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing model, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customer’s paid service subscription to non-New Relic One Products (our original product-based pricing model). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relic’s distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relic’s Global Technical Support provides assistance for: the New Relic One platform and its features and capabilities, New Relic's monitoring and observability solutions, and our alerting and Applied Intelligence features. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and we’re here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relic’s Open Source Community Plus Projects. New Relic’s Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relic’s Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relic’s Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered “generally available.” If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we can’t help with things we didn’t build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and we’ll help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. We’ll do our best to determine whether an issue is with New Relic’s Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the “account owner,” such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relic’s Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Important As of March 10, 2021, you're no longer able to create a New Relic support ticket by emailing support@newrelic.com. If you have ticketed support as part of your subscription, when logged into your New Relic account, go to one.newrelic.com to create a support ticket. Once you're there, follow these steps: Click the ? icon in the upper right hand corner. Click I need more help. Select Create a Support Ticket. By creating a support ticket directly within your account, it will speed up the support process and improve our team’s ability to troubleshoot your issue more effectively. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relic’s Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources. RSS - Atom Subscribe to the RSS feed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 239.20935,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " Relic <em>General</em> <em>usage</em> and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments &amp; frameworks Our Products are fully tested with the compatible"
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "Acceptable use policy",
        "In connection with your use of the New Relic Properties, you will not, and you will not allow third parties to::",
        "Harm New Relic’s Properties and interests, such as:",
        "Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as:",
        "Violate any applicable law or regulation or for high-risk purposes, such as:",
        "New Relic Properties do not include Third-Party Services",
        "Updates, Contact Information and Violations"
      ],
      "title": "Acceptable use policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b006ab295dae6522e8c76fcd47b3a0d4a45938e4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/acceptable-use-policy/",
      "published_at": "2022-01-08T09:14:45Z",
      "updated_at": "2021-10-30T21:48:45Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic’s mission is to instrument, measure and improve the internet to help our customers create more perfect software, experiences and businesses. We strive to keep our resources operating efficiently, so our services are available to all subscribers. Because you have access to shared resources, we have put these rules in place to ensure everyone has a great experience. For example, you as a tenant would not want other tenants to engage in the types of activities described below. To help us do this, we have put some rules in place regarding your use of the New Relic Properties and created this Acceptable Use Policy (“AUP”). This AUP applies if you use any New Relic product, service, software, website, forum, page or system (collectively, the “New Relic Properties”) and is part of the technical guides documentation that New Relic makes available at dedicated 'Documentation' pages on New Relic websites, including, but not limited to, https://docs.newrelic.com, https://docs.pixielabs.ai/, and https://docs.newrelic.com/docs/codestream (the “Documentation”). In connection with your use of the New Relic Properties, you will not, and you will not allow third parties to:: Harm New Relic’s Properties and interests, such as: Uploading, transmitting or otherwise provide content that infringes New Relic’s or a third party’s intellectual property, privacy or other rights, violates applicable laws or regulations or contains viruses, worms, harmful code, malware or other harmful materials; Hosting, selling, reselling, renting, exploiting, sublicensing, leasing, or otherwise providing the New Relic Properties or any portion thereof or use such for time sharing purposes or on a service bureau basis without our express written permission; Modifying, disabling, or compromising the integrity or performance of the New Relic Properties or related systems, networks, or data; including by: Attempting to compromise the integrity of the New Relic Properties, including probing, scanning or testing the vulnerability of any part of the New Relic Properties without proper authorization; Overwhelming our infrastructure (such as by using “botnets”, “robots,” “spiders” and “offline readers”); Going beyond the use parameters for any given service as described in the corresponding Documentation; Using metatags or other “hidden text”; Drastically exceeding your contracted rate of use as set forth in your order or the Documentation; or Consuming an unreasonable amount of storage. Accessing any unauthorized part of the New Relic Properties, or accessing or searching any part of the New Relic Properties by means other than those provided or authorized by New Relic (including “scraping” or using any data mining methods); Sharing your New Relic Properties account or login credentials, including API keys, with any other individual; Deciphering or decrypting transmissions, circumventing any access, authentication or copy restrictions, or otherwise attempting to compromise the security of the New Relic Properties (including any other user’s account); Accessing the New Relic Properties in order to build a similar or competitive website, application or service; or Attempting to do anything else that may result in some form of adverse impact to the New Relic Properties or use of the New Relic Properties by any of our other customers. Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as: Posting or transmitting abusive messages, defamatory, libelous, false or misleading statements, hate speech or messages that incite or threaten violence, or stalk or harass others; Promoting, encouraging, or facilitating hate speech, violence, discrimination based on race, color, sexual orientation, marital status, gender or identity expression, parental status, religion or creed, national origin or ancestry, sex, age, physical or mental disability, veteran status, genetic information, citizenship and/or any other characteristic protected by law. You are not permitted to use New Relic Properties if you are an entity identified by nationally-recognized non-profits as engaging in such activities. Using the New Relic Properties in violation of your company's policy(ies) and procedures, or attempting to modify or gain unauthorized use of or access to, another user's account, website, application, system, equipment or data; Misrepresenting yourself, impersonating another person, falsely implying any sponsorship or association with New Relic or affiliation with any third party, engaging in fraud, hiding or attempt to hide your identity or disguising the origin of any content (including by “spoofing” or “phishing”); Collecting or harvesting any personally identifiable information, including account names, from any other user’s account or the New Relic Properties, or using the New Relic Properties to violate the privacy of others; Including, publishing or posting other people’s private and confidential information without their express permission; Using anyone’s name or trademarks without their express written permission; Using the New Relic Properties to generate or send unsolicited communications, advertising, chain letters, or spam; Soliciting our users for commercial purposes, unless expressly permitted in writing by New Relic; Disparaging anyone; or Disclosing any confidential information obtained through any method contrary to this AUP. Violate any applicable law or regulation or for high-risk purposes, such as: Using the New Relic Properties in violation of any applicable law or regulation, including data, privacy, and export control laws in applicable jurisdictions; Using the New Relic Properties in any situation for which they are not designed, manufactured or intended, such as for use in life support, emergency or mission critical circumstances, or in any activities where use or failure of the New Relic Properties could lead to death, personal injury or property or environmental damage. For example, you may not use, or permit any other person to use, the New Relic Properties in connection with aircraft or other modes of human mass transportation or nuclear or chemical facilities, life support systems, implantable medical equipment, motor vehicles, or weaponry systems; or Querying or otherwise configuring, processing or submitting any personal data that could be legally considered sensitive in any applicable jurisdiction, including, but not limited to: (i) patient, medical, or other protected health information regulated by the Health Insurance Portability and Accountability Act (as amended and supplemented) (“HIPAA”); (ii) personal data about individuals under the age of 16, which for the avoidance of doubt includes any “personal information” as such term is defined under the Children’s Online Privacy Protection Act; (iii) government issued identification numbers, including Social Security numbers, driver’s license numbers and other state-issued identification numbers; (iv) financial account information, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting access to an online account, including the combination of a username or email address along with a password or security question and answer that would permit access to an online account; (vi) special categories of sensitive personal data, (such as defined under Regulation (EU) 2016/679 of the European Parliament), including personal data revealing racial or ethnic origin, political opinions, religious beliefs, trade union membership, physical or mental health or condition, sexual life, sexual orientation, genetic data, biometric data, or the commission or alleged commission any crime or offense; (vii) precise geo-location data; or (viii) any data similar to the above protected under foreign or domestic laws, including without limitation any data subject to regulation under the International Traffic in Arms Regulations (ITAR), 22 C.F.R. §§ 120-130. You represent and warrant to New Relic that you have all necessary rights, consents, and permissions to use and submit data that you send to the New Relic Properties, all without violating or infringing any applicable laws, third-party rights (including intellectual property, publicity, or privacy rights), or any terms or policies governing such data. New Relic Properties do not include Third-Party Services If you choose to use any Third Party Services, your use of Third-Party Services is wholly subject to your separate agreement with the relevant provider. New Relic bears no responsibility or liability for Third-Party Services. If you enable a Third-Party Service with the New Relic Properties, New Relic may access and exchange Customer Data with the Third-Party Service on your behalf and instruction. “Third-Party Services” means any third party platform, add-on, service, or product not provided by New Relic and that a User integrates or enables for use with the Service, including third-party applications and plug-ins. Open source software that New Relic makes separately available for download (e.g. community tools) is, as required, governed by the terms of the applicable open source license. The license for any open source software identified as included in New Relic Properties will, as required, wholly apply to your use of that open source software. Updates, Contact Information and Violations We will occasionally need to modify this AUP to help us continue to provide you with a great experience while using the New Relic Properties. In the event we modify this AUP, we will do so by posting a revised version, and any changes will be effective immediately if you’re a new user of the New Relic Properties and thirty (30) days after posting for all other users. If you continue using the New Relic Properties after we update this AUP, you agree to the latest version of this AUP. You can report a violation of this AUP to: AUP@newrelic.com. Or by mail at: Attn: Legal New Relic, Inc. 188 Spear Street, Suite 1200 San Francisco, CA 94105 This AUP, and our customers’ compliance with it, is essential for enabling us to provide you and our other customers with the New Relic Properties, which we take very seriously. You are wholly and solely responsible for appropriate configuration of systems and software that you own or can control to ensure your compliance with this AUP. So, if we determine in our sole discretion that you have violated this AUP, we may, without limiting any other remedies available to us, permanently or temporarily suspend, limit, or terminate your access to the New Relic Properties without notice or liability. This right applies even if the breach is unintentional or unauthorized if we believe that any such suspension, limitation, or termination is necessary to ensure compliance with laws, or to protect the rights, safety, privacy, security, or property of us or others. In this AUP, the term “content” means: (1) any information, data, text, software, code, scripts, music, sound, photos, graphics, videos, messages, tags, interactive features, or other materials that you post, upload, share, submit, or otherwise provide in any manner to the services and (2) any other materials, content, or data you provide to New Relic or use with the New Relic Properties. As used in this AUP, “you” may refer to an individual user or the legal entity an individual user is employed by that has contracted with New Relic, and “we” means New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 217.81358,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Updates, Contact <em>Information</em> and Violations",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " issued identification numbers, including Social Security numbers, driver’s <em>license</em> numbers and other state-issued identification numbers; (iv) financial account <em>information</em>, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting"
      },
      "id": "603e93bf28ccbc99f3eba7bc"
    },
    {
      "sections": [
        "New Relic data usage limits and policies",
        "View limits and manage data"
      ],
      "title": "New Relic data usage limits and policies",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "fc32c25b40a030ffa0fad6bfc95be7fca1360ee1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies/",
      "published_at": "2022-01-08T09:15:35Z",
      "updated_at": "2021-10-19T04:02:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers' accounts, we have various data volume and rate limits in place. We reserve the right to enforce these limits to protect our system and to avoid issues for you and other customers. If your New Relic account, whether by configuration or by error, exceeds one of these limits, it or its child accounts might experience one or both of the following: Sampling of data Temporary pause or cessation of data collection To learn more about how hitting a limit can affect your data, see View limits. If you have further questions about these limits, your contract, or a limit you've reached, contact your New Relic account representative. We can work with you to adjust any rate limits to meet your needs. View limits and manage data For information about system and account limits, and for links to data ingest API limits, go to View limits. To manage your data ingest, storage, and limits for organization or billing purposes, go to Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 214.32797,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic data <em>usage</em> limits and policies",
        "sections": "New Relic data <em>usage</em> limits and policies",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data <em>usage</em> spikes in one New Relic account from impacting other customers&#x27; accounts, we have various data volume and rate limits in place. We reserve the right to enforce"
      },
      "id": "603eb1c528ccbc0311eba7c7"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies": [
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Important",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos",
        "RSS - Atom"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2022-01-08T09:15:34Z",
      "updated_at": "2021-12-04T17:54:46Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Important If you are a New Relic HIPAA customer, please follow the requirements specified in the Global Technical Support Section of HIPAA enablement - what you need to know and do when you request support and engage with the New Relic Global Technical Support team. Support plan for New Relic One pricing and packaging model The New Relic One Support Plan below applies only to a customer’s paid subscription on the New Relic One pricing model. Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing model, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customer’s paid service subscription to non-New Relic One Products (our original product-based pricing model). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relic’s distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relic’s Global Technical Support provides assistance for: the New Relic One platform and its features and capabilities, New Relic's monitoring and observability solutions, and our alerting and Applied Intelligence features. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and we’re here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relic’s Open Source Community Plus Projects. New Relic’s Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relic’s Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relic’s Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered “generally available.” If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we can’t help with things we didn’t build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and we’ll help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. We’ll do our best to determine whether an issue is with New Relic’s Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the “account owner,” such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relic’s Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Important As of March 10, 2021, you're no longer able to create a New Relic support ticket by emailing support@newrelic.com. If you have ticketed support as part of your subscription, when logged into your New Relic account, go to one.newrelic.com to create a support ticket. Once you're there, follow these steps: Click the ? icon in the upper right hand corner. Click I need more help. Select Create a Support Ticket. By creating a support ticket directly within your account, it will speed up the support process and improve our team’s ability to troubleshoot your issue more effectively. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relic’s Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources. RSS - Atom Subscribe to the RSS feed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 239.20934,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " Relic <em>General</em> <em>usage</em> and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments &amp; frameworks Our Products are fully tested with the compatible"
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "Acceptable use policy",
        "In connection with your use of the New Relic Properties, you will not, and you will not allow third parties to::",
        "Harm New Relic’s Properties and interests, such as:",
        "Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as:",
        "Violate any applicable law or regulation or for high-risk purposes, such as:",
        "New Relic Properties do not include Third-Party Services",
        "Updates, Contact Information and Violations"
      ],
      "title": "Acceptable use policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b006ab295dae6522e8c76fcd47b3a0d4a45938e4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/acceptable-use-policy/",
      "published_at": "2022-01-08T09:14:45Z",
      "updated_at": "2021-10-30T21:48:45Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic’s mission is to instrument, measure and improve the internet to help our customers create more perfect software, experiences and businesses. We strive to keep our resources operating efficiently, so our services are available to all subscribers. Because you have access to shared resources, we have put these rules in place to ensure everyone has a great experience. For example, you as a tenant would not want other tenants to engage in the types of activities described below. To help us do this, we have put some rules in place regarding your use of the New Relic Properties and created this Acceptable Use Policy (“AUP”). This AUP applies if you use any New Relic product, service, software, website, forum, page or system (collectively, the “New Relic Properties”) and is part of the technical guides documentation that New Relic makes available at dedicated 'Documentation' pages on New Relic websites, including, but not limited to, https://docs.newrelic.com, https://docs.pixielabs.ai/, and https://docs.newrelic.com/docs/codestream (the “Documentation”). In connection with your use of the New Relic Properties, you will not, and you will not allow third parties to:: Harm New Relic’s Properties and interests, such as: Uploading, transmitting or otherwise provide content that infringes New Relic’s or a third party’s intellectual property, privacy or other rights, violates applicable laws or regulations or contains viruses, worms, harmful code, malware or other harmful materials; Hosting, selling, reselling, renting, exploiting, sublicensing, leasing, or otherwise providing the New Relic Properties or any portion thereof or use such for time sharing purposes or on a service bureau basis without our express written permission; Modifying, disabling, or compromising the integrity or performance of the New Relic Properties or related systems, networks, or data; including by: Attempting to compromise the integrity of the New Relic Properties, including probing, scanning or testing the vulnerability of any part of the New Relic Properties without proper authorization; Overwhelming our infrastructure (such as by using “botnets”, “robots,” “spiders” and “offline readers”); Going beyond the use parameters for any given service as described in the corresponding Documentation; Using metatags or other “hidden text”; Drastically exceeding your contracted rate of use as set forth in your order or the Documentation; or Consuming an unreasonable amount of storage. Accessing any unauthorized part of the New Relic Properties, or accessing or searching any part of the New Relic Properties by means other than those provided or authorized by New Relic (including “scraping” or using any data mining methods); Sharing your New Relic Properties account or login credentials, including API keys, with any other individual; Deciphering or decrypting transmissions, circumventing any access, authentication or copy restrictions, or otherwise attempting to compromise the security of the New Relic Properties (including any other user’s account); Accessing the New Relic Properties in order to build a similar or competitive website, application or service; or Attempting to do anything else that may result in some form of adverse impact to the New Relic Properties or use of the New Relic Properties by any of our other customers. Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as: Posting or transmitting abusive messages, defamatory, libelous, false or misleading statements, hate speech or messages that incite or threaten violence, or stalk or harass others; Promoting, encouraging, or facilitating hate speech, violence, discrimination based on race, color, sexual orientation, marital status, gender or identity expression, parental status, religion or creed, national origin or ancestry, sex, age, physical or mental disability, veteran status, genetic information, citizenship and/or any other characteristic protected by law. You are not permitted to use New Relic Properties if you are an entity identified by nationally-recognized non-profits as engaging in such activities. Using the New Relic Properties in violation of your company's policy(ies) and procedures, or attempting to modify or gain unauthorized use of or access to, another user's account, website, application, system, equipment or data; Misrepresenting yourself, impersonating another person, falsely implying any sponsorship or association with New Relic or affiliation with any third party, engaging in fraud, hiding or attempt to hide your identity or disguising the origin of any content (including by “spoofing” or “phishing”); Collecting or harvesting any personally identifiable information, including account names, from any other user’s account or the New Relic Properties, or using the New Relic Properties to violate the privacy of others; Including, publishing or posting other people’s private and confidential information without their express permission; Using anyone’s name or trademarks without their express written permission; Using the New Relic Properties to generate or send unsolicited communications, advertising, chain letters, or spam; Soliciting our users for commercial purposes, unless expressly permitted in writing by New Relic; Disparaging anyone; or Disclosing any confidential information obtained through any method contrary to this AUP. Violate any applicable law or regulation or for high-risk purposes, such as: Using the New Relic Properties in violation of any applicable law or regulation, including data, privacy, and export control laws in applicable jurisdictions; Using the New Relic Properties in any situation for which they are not designed, manufactured or intended, such as for use in life support, emergency or mission critical circumstances, or in any activities where use or failure of the New Relic Properties could lead to death, personal injury or property or environmental damage. For example, you may not use, or permit any other person to use, the New Relic Properties in connection with aircraft or other modes of human mass transportation or nuclear or chemical facilities, life support systems, implantable medical equipment, motor vehicles, or weaponry systems; or Querying or otherwise configuring, processing or submitting any personal data that could be legally considered sensitive in any applicable jurisdiction, including, but not limited to: (i) patient, medical, or other protected health information regulated by the Health Insurance Portability and Accountability Act (as amended and supplemented) (“HIPAA”); (ii) personal data about individuals under the age of 16, which for the avoidance of doubt includes any “personal information” as such term is defined under the Children’s Online Privacy Protection Act; (iii) government issued identification numbers, including Social Security numbers, driver’s license numbers and other state-issued identification numbers; (iv) financial account information, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting access to an online account, including the combination of a username or email address along with a password or security question and answer that would permit access to an online account; (vi) special categories of sensitive personal data, (such as defined under Regulation (EU) 2016/679 of the European Parliament), including personal data revealing racial or ethnic origin, political opinions, religious beliefs, trade union membership, physical or mental health or condition, sexual life, sexual orientation, genetic data, biometric data, or the commission or alleged commission any crime or offense; (vii) precise geo-location data; or (viii) any data similar to the above protected under foreign or domestic laws, including without limitation any data subject to regulation under the International Traffic in Arms Regulations (ITAR), 22 C.F.R. §§ 120-130. You represent and warrant to New Relic that you have all necessary rights, consents, and permissions to use and submit data that you send to the New Relic Properties, all without violating or infringing any applicable laws, third-party rights (including intellectual property, publicity, or privacy rights), or any terms or policies governing such data. New Relic Properties do not include Third-Party Services If you choose to use any Third Party Services, your use of Third-Party Services is wholly subject to your separate agreement with the relevant provider. New Relic bears no responsibility or liability for Third-Party Services. If you enable a Third-Party Service with the New Relic Properties, New Relic may access and exchange Customer Data with the Third-Party Service on your behalf and instruction. “Third-Party Services” means any third party platform, add-on, service, or product not provided by New Relic and that a User integrates or enables for use with the Service, including third-party applications and plug-ins. Open source software that New Relic makes separately available for download (e.g. community tools) is, as required, governed by the terms of the applicable open source license. The license for any open source software identified as included in New Relic Properties will, as required, wholly apply to your use of that open source software. Updates, Contact Information and Violations We will occasionally need to modify this AUP to help us continue to provide you with a great experience while using the New Relic Properties. In the event we modify this AUP, we will do so by posting a revised version, and any changes will be effective immediately if you’re a new user of the New Relic Properties and thirty (30) days after posting for all other users. If you continue using the New Relic Properties after we update this AUP, you agree to the latest version of this AUP. You can report a violation of this AUP to: AUP@newrelic.com. Or by mail at: Attn: Legal New Relic, Inc. 188 Spear Street, Suite 1200 San Francisco, CA 94105 This AUP, and our customers’ compliance with it, is essential for enabling us to provide you and our other customers with the New Relic Properties, which we take very seriously. You are wholly and solely responsible for appropriate configuration of systems and software that you own or can control to ensure your compliance with this AUP. So, if we determine in our sole discretion that you have violated this AUP, we may, without limiting any other remedies available to us, permanently or temporarily suspend, limit, or terminate your access to the New Relic Properties without notice or liability. This right applies even if the breach is unintentional or unauthorized if we believe that any such suspension, limitation, or termination is necessary to ensure compliance with laws, or to protect the rights, safety, privacy, security, or property of us or others. In this AUP, the term “content” means: (1) any information, data, text, software, code, scripts, music, sound, photos, graphics, videos, messages, tags, interactive features, or other materials that you post, upload, share, submit, or otherwise provide in any manner to the services and (2) any other materials, content, or data you provide to New Relic or use with the New Relic Properties. As used in this AUP, “you” may refer to an individual user or the legal entity an individual user is employed by that has contracted with New Relic, and “we” means New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 217.81358,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Updates, Contact <em>Information</em> and Violations",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " issued identification numbers, including Social Security numbers, driver’s <em>license</em> numbers and other state-issued identification numbers; (iv) financial account <em>information</em>, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting"
      },
      "id": "603e93bf28ccbc99f3eba7bc"
    },
    {
      "sections": [
        "The People's Republic of China"
      ],
      "title": "The People's Republic of China",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "c3c078b8139d695b928d2001cd6c6c9318c43599",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/peoples-republic-china/",
      "published_at": "2022-01-08T08:25:09Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important Information Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory requirements in the PRC. New Relic does not provide support for, including but not limited to, the deployment, access, or use of the Service(s) or Software in the PRC, or otherwise in respect of assets in the PRC (“PRC Use”). Without overriding any express prohibitions that you may have agreed to as part of your agreement with New Relic, you (and you as an agent of the respective New Relic Customer (\"Customer\")) acknowledge that any PRC use is subject to the PRC national firewall system and may be subject to outages and other interference outside of the control of New Relic. Accordingly, you acknowledge and agree (on behalf of yourself and including any Customer on whose behalf you use the Service(s) or Software) that any PRC Use is at your sole risk and is fully excepted from all representations and warranties, including, but not limited to, any terms that assure the confidentiality, integrity, availability or privacy of your data. New Relic makes no other representations and hereby expressly disclaims any and all warranties in respect of PRC Use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 197.88037,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "Important <em>Information</em> Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory"
      },
      "id": "603eb3a328ccbcb488eba76b"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/new-relics-provision-services": [
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Important",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos",
        "RSS - Atom"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2022-01-08T09:15:34Z",
      "updated_at": "2021-12-04T17:54:46Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Important If you are a New Relic HIPAA customer, please follow the requirements specified in the Global Technical Support Section of HIPAA enablement - what you need to know and do when you request support and engage with the New Relic Global Technical Support team. Support plan for New Relic One pricing and packaging model The New Relic One Support Plan below applies only to a customer’s paid subscription on the New Relic One pricing model. Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing model, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customer’s paid service subscription to non-New Relic One Products (our original product-based pricing model). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relic’s distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relic’s Global Technical Support provides assistance for: the New Relic One platform and its features and capabilities, New Relic's monitoring and observability solutions, and our alerting and Applied Intelligence features. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and we’re here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relic’s Open Source Community Plus Projects. New Relic’s Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relic’s Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relic’s Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered “generally available.” If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we can’t help with things we didn’t build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and we’ll help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. We’ll do our best to determine whether an issue is with New Relic’s Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the “account owner,” such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relic’s Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Important As of March 10, 2021, you're no longer able to create a New Relic support ticket by emailing support@newrelic.com. If you have ticketed support as part of your subscription, when logged into your New Relic account, go to one.newrelic.com to create a support ticket. Once you're there, follow these steps: Click the ? icon in the upper right hand corner. Click I need more help. Select Create a Support Ticket. By creating a support ticket directly within your account, it will speed up the support process and improve our team’s ability to troubleshoot your issue more effectively. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relic’s Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources. RSS - Atom Subscribe to the RSS feed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 239.20934,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " Relic <em>General</em> <em>usage</em> and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments &amp; frameworks Our Products are fully tested with the compatible"
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "Acceptable use policy",
        "In connection with your use of the New Relic Properties, you will not, and you will not allow third parties to::",
        "Harm New Relic’s Properties and interests, such as:",
        "Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as:",
        "Violate any applicable law or regulation or for high-risk purposes, such as:",
        "New Relic Properties do not include Third-Party Services",
        "Updates, Contact Information and Violations"
      ],
      "title": "Acceptable use policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b006ab295dae6522e8c76fcd47b3a0d4a45938e4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/acceptable-use-policy/",
      "published_at": "2022-01-08T09:14:45Z",
      "updated_at": "2021-10-30T21:48:45Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic’s mission is to instrument, measure and improve the internet to help our customers create more perfect software, experiences and businesses. We strive to keep our resources operating efficiently, so our services are available to all subscribers. Because you have access to shared resources, we have put these rules in place to ensure everyone has a great experience. For example, you as a tenant would not want other tenants to engage in the types of activities described below. To help us do this, we have put some rules in place regarding your use of the New Relic Properties and created this Acceptable Use Policy (“AUP”). This AUP applies if you use any New Relic product, service, software, website, forum, page or system (collectively, the “New Relic Properties”) and is part of the technical guides documentation that New Relic makes available at dedicated 'Documentation' pages on New Relic websites, including, but not limited to, https://docs.newrelic.com, https://docs.pixielabs.ai/, and https://docs.newrelic.com/docs/codestream (the “Documentation”). In connection with your use of the New Relic Properties, you will not, and you will not allow third parties to:: Harm New Relic’s Properties and interests, such as: Uploading, transmitting or otherwise provide content that infringes New Relic’s or a third party’s intellectual property, privacy or other rights, violates applicable laws or regulations or contains viruses, worms, harmful code, malware or other harmful materials; Hosting, selling, reselling, renting, exploiting, sublicensing, leasing, or otherwise providing the New Relic Properties or any portion thereof or use such for time sharing purposes or on a service bureau basis without our express written permission; Modifying, disabling, or compromising the integrity or performance of the New Relic Properties or related systems, networks, or data; including by: Attempting to compromise the integrity of the New Relic Properties, including probing, scanning or testing the vulnerability of any part of the New Relic Properties without proper authorization; Overwhelming our infrastructure (such as by using “botnets”, “robots,” “spiders” and “offline readers”); Going beyond the use parameters for any given service as described in the corresponding Documentation; Using metatags or other “hidden text”; Drastically exceeding your contracted rate of use as set forth in your order or the Documentation; or Consuming an unreasonable amount of storage. Accessing any unauthorized part of the New Relic Properties, or accessing or searching any part of the New Relic Properties by means other than those provided or authorized by New Relic (including “scraping” or using any data mining methods); Sharing your New Relic Properties account or login credentials, including API keys, with any other individual; Deciphering or decrypting transmissions, circumventing any access, authentication or copy restrictions, or otherwise attempting to compromise the security of the New Relic Properties (including any other user’s account); Accessing the New Relic Properties in order to build a similar or competitive website, application or service; or Attempting to do anything else that may result in some form of adverse impact to the New Relic Properties or use of the New Relic Properties by any of our other customers. Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as: Posting or transmitting abusive messages, defamatory, libelous, false or misleading statements, hate speech or messages that incite or threaten violence, or stalk or harass others; Promoting, encouraging, or facilitating hate speech, violence, discrimination based on race, color, sexual orientation, marital status, gender or identity expression, parental status, religion or creed, national origin or ancestry, sex, age, physical or mental disability, veteran status, genetic information, citizenship and/or any other characteristic protected by law. You are not permitted to use New Relic Properties if you are an entity identified by nationally-recognized non-profits as engaging in such activities. Using the New Relic Properties in violation of your company's policy(ies) and procedures, or attempting to modify or gain unauthorized use of or access to, another user's account, website, application, system, equipment or data; Misrepresenting yourself, impersonating another person, falsely implying any sponsorship or association with New Relic or affiliation with any third party, engaging in fraud, hiding or attempt to hide your identity or disguising the origin of any content (including by “spoofing” or “phishing”); Collecting or harvesting any personally identifiable information, including account names, from any other user’s account or the New Relic Properties, or using the New Relic Properties to violate the privacy of others; Including, publishing or posting other people’s private and confidential information without their express permission; Using anyone’s name or trademarks without their express written permission; Using the New Relic Properties to generate or send unsolicited communications, advertising, chain letters, or spam; Soliciting our users for commercial purposes, unless expressly permitted in writing by New Relic; Disparaging anyone; or Disclosing any confidential information obtained through any method contrary to this AUP. Violate any applicable law or regulation or for high-risk purposes, such as: Using the New Relic Properties in violation of any applicable law or regulation, including data, privacy, and export control laws in applicable jurisdictions; Using the New Relic Properties in any situation for which they are not designed, manufactured or intended, such as for use in life support, emergency or mission critical circumstances, or in any activities where use or failure of the New Relic Properties could lead to death, personal injury or property or environmental damage. For example, you may not use, or permit any other person to use, the New Relic Properties in connection with aircraft or other modes of human mass transportation or nuclear or chemical facilities, life support systems, implantable medical equipment, motor vehicles, or weaponry systems; or Querying or otherwise configuring, processing or submitting any personal data that could be legally considered sensitive in any applicable jurisdiction, including, but not limited to: (i) patient, medical, or other protected health information regulated by the Health Insurance Portability and Accountability Act (as amended and supplemented) (“HIPAA”); (ii) personal data about individuals under the age of 16, which for the avoidance of doubt includes any “personal information” as such term is defined under the Children’s Online Privacy Protection Act; (iii) government issued identification numbers, including Social Security numbers, driver’s license numbers and other state-issued identification numbers; (iv) financial account information, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting access to an online account, including the combination of a username or email address along with a password or security question and answer that would permit access to an online account; (vi) special categories of sensitive personal data, (such as defined under Regulation (EU) 2016/679 of the European Parliament), including personal data revealing racial or ethnic origin, political opinions, religious beliefs, trade union membership, physical or mental health or condition, sexual life, sexual orientation, genetic data, biometric data, or the commission or alleged commission any crime or offense; (vii) precise geo-location data; or (viii) any data similar to the above protected under foreign or domestic laws, including without limitation any data subject to regulation under the International Traffic in Arms Regulations (ITAR), 22 C.F.R. §§ 120-130. You represent and warrant to New Relic that you have all necessary rights, consents, and permissions to use and submit data that you send to the New Relic Properties, all without violating or infringing any applicable laws, third-party rights (including intellectual property, publicity, or privacy rights), or any terms or policies governing such data. New Relic Properties do not include Third-Party Services If you choose to use any Third Party Services, your use of Third-Party Services is wholly subject to your separate agreement with the relevant provider. New Relic bears no responsibility or liability for Third-Party Services. If you enable a Third-Party Service with the New Relic Properties, New Relic may access and exchange Customer Data with the Third-Party Service on your behalf and instruction. “Third-Party Services” means any third party platform, add-on, service, or product not provided by New Relic and that a User integrates or enables for use with the Service, including third-party applications and plug-ins. Open source software that New Relic makes separately available for download (e.g. community tools) is, as required, governed by the terms of the applicable open source license. The license for any open source software identified as included in New Relic Properties will, as required, wholly apply to your use of that open source software. Updates, Contact Information and Violations We will occasionally need to modify this AUP to help us continue to provide you with a great experience while using the New Relic Properties. In the event we modify this AUP, we will do so by posting a revised version, and any changes will be effective immediately if you’re a new user of the New Relic Properties and thirty (30) days after posting for all other users. If you continue using the New Relic Properties after we update this AUP, you agree to the latest version of this AUP. You can report a violation of this AUP to: AUP@newrelic.com. Or by mail at: Attn: Legal New Relic, Inc. 188 Spear Street, Suite 1200 San Francisco, CA 94105 This AUP, and our customers’ compliance with it, is essential for enabling us to provide you and our other customers with the New Relic Properties, which we take very seriously. You are wholly and solely responsible for appropriate configuration of systems and software that you own or can control to ensure your compliance with this AUP. So, if we determine in our sole discretion that you have violated this AUP, we may, without limiting any other remedies available to us, permanently or temporarily suspend, limit, or terminate your access to the New Relic Properties without notice or liability. This right applies even if the breach is unintentional or unauthorized if we believe that any such suspension, limitation, or termination is necessary to ensure compliance with laws, or to protect the rights, safety, privacy, security, or property of us or others. In this AUP, the term “content” means: (1) any information, data, text, software, code, scripts, music, sound, photos, graphics, videos, messages, tags, interactive features, or other materials that you post, upload, share, submit, or otherwise provide in any manner to the services and (2) any other materials, content, or data you provide to New Relic or use with the New Relic Properties. As used in this AUP, “you” may refer to an individual user or the legal entity an individual user is employed by that has contracted with New Relic, and “we” means New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 217.81358,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Updates, Contact <em>Information</em> and Violations",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " issued identification numbers, including Social Security numbers, driver’s <em>license</em> numbers and other state-issued identification numbers; (iv) financial account <em>information</em>, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting"
      },
      "id": "603e93bf28ccbc99f3eba7bc"
    },
    {
      "sections": [
        "New Relic data usage limits and policies",
        "View limits and manage data"
      ],
      "title": "New Relic data usage limits and policies",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "fc32c25b40a030ffa0fad6bfc95be7fca1360ee1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies/",
      "published_at": "2022-01-08T09:15:35Z",
      "updated_at": "2021-10-19T04:02:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers' accounts, we have various data volume and rate limits in place. We reserve the right to enforce these limits to protect our system and to avoid issues for you and other customers. If your New Relic account, whether by configuration or by error, exceeds one of these limits, it or its child accounts might experience one or both of the following: Sampling of data Temporary pause or cessation of data collection To learn more about how hitting a limit can affect your data, see View limits. If you have further questions about these limits, your contract, or a limit you've reached, contact your New Relic account representative. We can work with you to adjust any rate limits to meet your needs. View limits and manage data For information about system and account limits, and for links to data ingest API limits, go to View limits. To manage your data ingest, storage, and limits for organization or billing purposes, go to Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 214.32797,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic data <em>usage</em> limits and policies",
        "sections": "New Relic data <em>usage</em> limits and policies",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data <em>usage</em> spikes in one New Relic account from impacting other customers&#x27; accounts, we have various data volume and rate limits in place. We reserve the right to enforce"
      },
      "id": "603eb1c528ccbc0311eba7c7"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/peoples-republic-china": [
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Important",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos",
        "RSS - Atom"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2022-01-08T09:15:34Z",
      "updated_at": "2021-12-04T17:54:46Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Important If you are a New Relic HIPAA customer, please follow the requirements specified in the Global Technical Support Section of HIPAA enablement - what you need to know and do when you request support and engage with the New Relic Global Technical Support team. Support plan for New Relic One pricing and packaging model The New Relic One Support Plan below applies only to a customer’s paid subscription on the New Relic One pricing model. Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing model, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customer’s paid service subscription to non-New Relic One Products (our original product-based pricing model). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relic’s distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relic’s Global Technical Support provides assistance for: the New Relic One platform and its features and capabilities, New Relic's monitoring and observability solutions, and our alerting and Applied Intelligence features. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and we’re here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relic’s Open Source Community Plus Projects. New Relic’s Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relic’s Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relic’s Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered “generally available.” If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we can’t help with things we didn’t build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and we’ll help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. We’ll do our best to determine whether an issue is with New Relic’s Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the “account owner,” such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relic’s Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Important As of March 10, 2021, you're no longer able to create a New Relic support ticket by emailing support@newrelic.com. If you have ticketed support as part of your subscription, when logged into your New Relic account, go to one.newrelic.com to create a support ticket. Once you're there, follow these steps: Click the ? icon in the upper right hand corner. Click I need more help. Select Create a Support Ticket. By creating a support ticket directly within your account, it will speed up the support process and improve our team’s ability to troubleshoot your issue more effectively. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relic’s Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources. RSS - Atom Subscribe to the RSS feed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 239.20934,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " Relic <em>General</em> <em>usage</em> and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments &amp; frameworks Our Products are fully tested with the compatible"
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "Acceptable use policy",
        "In connection with your use of the New Relic Properties, you will not, and you will not allow third parties to::",
        "Harm New Relic’s Properties and interests, such as:",
        "Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as:",
        "Violate any applicable law or regulation or for high-risk purposes, such as:",
        "New Relic Properties do not include Third-Party Services",
        "Updates, Contact Information and Violations"
      ],
      "title": "Acceptable use policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b006ab295dae6522e8c76fcd47b3a0d4a45938e4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/acceptable-use-policy/",
      "published_at": "2022-01-08T09:14:45Z",
      "updated_at": "2021-10-30T21:48:45Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic’s mission is to instrument, measure and improve the internet to help our customers create more perfect software, experiences and businesses. We strive to keep our resources operating efficiently, so our services are available to all subscribers. Because you have access to shared resources, we have put these rules in place to ensure everyone has a great experience. For example, you as a tenant would not want other tenants to engage in the types of activities described below. To help us do this, we have put some rules in place regarding your use of the New Relic Properties and created this Acceptable Use Policy (“AUP”). This AUP applies if you use any New Relic product, service, software, website, forum, page or system (collectively, the “New Relic Properties”) and is part of the technical guides documentation that New Relic makes available at dedicated 'Documentation' pages on New Relic websites, including, but not limited to, https://docs.newrelic.com, https://docs.pixielabs.ai/, and https://docs.newrelic.com/docs/codestream (the “Documentation”). In connection with your use of the New Relic Properties, you will not, and you will not allow third parties to:: Harm New Relic’s Properties and interests, such as: Uploading, transmitting or otherwise provide content that infringes New Relic’s or a third party’s intellectual property, privacy or other rights, violates applicable laws or regulations or contains viruses, worms, harmful code, malware or other harmful materials; Hosting, selling, reselling, renting, exploiting, sublicensing, leasing, or otherwise providing the New Relic Properties or any portion thereof or use such for time sharing purposes or on a service bureau basis without our express written permission; Modifying, disabling, or compromising the integrity or performance of the New Relic Properties or related systems, networks, or data; including by: Attempting to compromise the integrity of the New Relic Properties, including probing, scanning or testing the vulnerability of any part of the New Relic Properties without proper authorization; Overwhelming our infrastructure (such as by using “botnets”, “robots,” “spiders” and “offline readers”); Going beyond the use parameters for any given service as described in the corresponding Documentation; Using metatags or other “hidden text”; Drastically exceeding your contracted rate of use as set forth in your order or the Documentation; or Consuming an unreasonable amount of storage. Accessing any unauthorized part of the New Relic Properties, or accessing or searching any part of the New Relic Properties by means other than those provided or authorized by New Relic (including “scraping” or using any data mining methods); Sharing your New Relic Properties account or login credentials, including API keys, with any other individual; Deciphering or decrypting transmissions, circumventing any access, authentication or copy restrictions, or otherwise attempting to compromise the security of the New Relic Properties (including any other user’s account); Accessing the New Relic Properties in order to build a similar or competitive website, application or service; or Attempting to do anything else that may result in some form of adverse impact to the New Relic Properties or use of the New Relic Properties by any of our other customers. Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as: Posting or transmitting abusive messages, defamatory, libelous, false or misleading statements, hate speech or messages that incite or threaten violence, or stalk or harass others; Promoting, encouraging, or facilitating hate speech, violence, discrimination based on race, color, sexual orientation, marital status, gender or identity expression, parental status, religion or creed, national origin or ancestry, sex, age, physical or mental disability, veteran status, genetic information, citizenship and/or any other characteristic protected by law. You are not permitted to use New Relic Properties if you are an entity identified by nationally-recognized non-profits as engaging in such activities. Using the New Relic Properties in violation of your company's policy(ies) and procedures, or attempting to modify or gain unauthorized use of or access to, another user's account, website, application, system, equipment or data; Misrepresenting yourself, impersonating another person, falsely implying any sponsorship or association with New Relic or affiliation with any third party, engaging in fraud, hiding or attempt to hide your identity or disguising the origin of any content (including by “spoofing” or “phishing”); Collecting or harvesting any personally identifiable information, including account names, from any other user’s account or the New Relic Properties, or using the New Relic Properties to violate the privacy of others; Including, publishing or posting other people’s private and confidential information without their express permission; Using anyone’s name or trademarks without their express written permission; Using the New Relic Properties to generate or send unsolicited communications, advertising, chain letters, or spam; Soliciting our users for commercial purposes, unless expressly permitted in writing by New Relic; Disparaging anyone; or Disclosing any confidential information obtained through any method contrary to this AUP. Violate any applicable law or regulation or for high-risk purposes, such as: Using the New Relic Properties in violation of any applicable law or regulation, including data, privacy, and export control laws in applicable jurisdictions; Using the New Relic Properties in any situation for which they are not designed, manufactured or intended, such as for use in life support, emergency or mission critical circumstances, or in any activities where use or failure of the New Relic Properties could lead to death, personal injury or property or environmental damage. For example, you may not use, or permit any other person to use, the New Relic Properties in connection with aircraft or other modes of human mass transportation or nuclear or chemical facilities, life support systems, implantable medical equipment, motor vehicles, or weaponry systems; or Querying or otherwise configuring, processing or submitting any personal data that could be legally considered sensitive in any applicable jurisdiction, including, but not limited to: (i) patient, medical, or other protected health information regulated by the Health Insurance Portability and Accountability Act (as amended and supplemented) (“HIPAA”); (ii) personal data about individuals under the age of 16, which for the avoidance of doubt includes any “personal information” as such term is defined under the Children’s Online Privacy Protection Act; (iii) government issued identification numbers, including Social Security numbers, driver’s license numbers and other state-issued identification numbers; (iv) financial account information, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting access to an online account, including the combination of a username or email address along with a password or security question and answer that would permit access to an online account; (vi) special categories of sensitive personal data, (such as defined under Regulation (EU) 2016/679 of the European Parliament), including personal data revealing racial or ethnic origin, political opinions, religious beliefs, trade union membership, physical or mental health or condition, sexual life, sexual orientation, genetic data, biometric data, or the commission or alleged commission any crime or offense; (vii) precise geo-location data; or (viii) any data similar to the above protected under foreign or domestic laws, including without limitation any data subject to regulation under the International Traffic in Arms Regulations (ITAR), 22 C.F.R. §§ 120-130. You represent and warrant to New Relic that you have all necessary rights, consents, and permissions to use and submit data that you send to the New Relic Properties, all without violating or infringing any applicable laws, third-party rights (including intellectual property, publicity, or privacy rights), or any terms or policies governing such data. New Relic Properties do not include Third-Party Services If you choose to use any Third Party Services, your use of Third-Party Services is wholly subject to your separate agreement with the relevant provider. New Relic bears no responsibility or liability for Third-Party Services. If you enable a Third-Party Service with the New Relic Properties, New Relic may access and exchange Customer Data with the Third-Party Service on your behalf and instruction. “Third-Party Services” means any third party platform, add-on, service, or product not provided by New Relic and that a User integrates or enables for use with the Service, including third-party applications and plug-ins. Open source software that New Relic makes separately available for download (e.g. community tools) is, as required, governed by the terms of the applicable open source license. The license for any open source software identified as included in New Relic Properties will, as required, wholly apply to your use of that open source software. Updates, Contact Information and Violations We will occasionally need to modify this AUP to help us continue to provide you with a great experience while using the New Relic Properties. In the event we modify this AUP, we will do so by posting a revised version, and any changes will be effective immediately if you’re a new user of the New Relic Properties and thirty (30) days after posting for all other users. If you continue using the New Relic Properties after we update this AUP, you agree to the latest version of this AUP. You can report a violation of this AUP to: AUP@newrelic.com. Or by mail at: Attn: Legal New Relic, Inc. 188 Spear Street, Suite 1200 San Francisco, CA 94105 This AUP, and our customers’ compliance with it, is essential for enabling us to provide you and our other customers with the New Relic Properties, which we take very seriously. You are wholly and solely responsible for appropriate configuration of systems and software that you own or can control to ensure your compliance with this AUP. So, if we determine in our sole discretion that you have violated this AUP, we may, without limiting any other remedies available to us, permanently or temporarily suspend, limit, or terminate your access to the New Relic Properties without notice or liability. This right applies even if the breach is unintentional or unauthorized if we believe that any such suspension, limitation, or termination is necessary to ensure compliance with laws, or to protect the rights, safety, privacy, security, or property of us or others. In this AUP, the term “content” means: (1) any information, data, text, software, code, scripts, music, sound, photos, graphics, videos, messages, tags, interactive features, or other materials that you post, upload, share, submit, or otherwise provide in any manner to the services and (2) any other materials, content, or data you provide to New Relic or use with the New Relic Properties. As used in this AUP, “you” may refer to an individual user or the legal entity an individual user is employed by that has contracted with New Relic, and “we” means New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 217.81358,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Updates, Contact <em>Information</em> and Violations",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " issued identification numbers, including Social Security numbers, driver’s <em>license</em> numbers and other state-issued identification numbers; (iv) financial account <em>information</em>, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting"
      },
      "id": "603e93bf28ccbc99f3eba7bc"
    },
    {
      "sections": [
        "New Relic data usage limits and policies",
        "View limits and manage data"
      ],
      "title": "New Relic data usage limits and policies",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "fc32c25b40a030ffa0fad6bfc95be7fca1360ee1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies/",
      "published_at": "2022-01-08T09:15:35Z",
      "updated_at": "2021-10-19T04:02:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers' accounts, we have various data volume and rate limits in place. We reserve the right to enforce these limits to protect our system and to avoid issues for you and other customers. If your New Relic account, whether by configuration or by error, exceeds one of these limits, it or its child accounts might experience one or both of the following: Sampling of data Temporary pause or cessation of data collection To learn more about how hitting a limit can affect your data, see View limits. If you have further questions about these limits, your contract, or a limit you've reached, contact your New Relic account representative. We can work with you to adjust any rate limits to meet your needs. View limits and manage data For information about system and account limits, and for links to data ingest API limits, go to View limits. To manage your data ingest, storage, and limits for organization or billing purposes, go to Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 214.32797,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic data <em>usage</em> limits and policies",
        "sections": "New Relic data <em>usage</em> limits and policies",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data <em>usage</em> spikes in one New Relic account from impacting other customers&#x27; accounts, we have various data volume and rate limits in place. We reserve the right to enforce"
      },
      "id": "603eb1c528ccbc0311eba7c7"
    }
  ],
  "/docs/licenses/license-information/other-licenses/services-licenses": [
    {
      "sections": [
        "Add-on end user license agreement",
        "New Relic, Inc. Add-on End User License Agreement"
      ],
      "title": "Add-on end user license agreement",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "a01c225ca30f95dab7db856cd946c76de557c31f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/add-end-user-license-agreement/",
      "published_at": "2022-01-08T09:14:04Z",
      "updated_at": "2021-03-16T06:19:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic, Inc. Add-on End User License Agreement In connection with the work provided by New Relic's Expert Services, Field Instrumentation Team, or Sales team, you may be provided with certain custom-created software to visualize, enable, optimize, or enhance your use of New Relic's Services. By downloading, installing, authorizing installation, or using the Add-on (defined below) with your Underlying Software (defined below), you (“Customer”) agree to the terms and conditions herein (“Agreement”) with New Relic, Inc., a Delaware corporation with offices located at 188 Spear Street, Suite 1200 San Francisco, CA 94105 (“New Relic\"), (collectively the “Parties”). Capitalized terms not defined herein shall have the meanings set forth in the New Relic Terms of Service (the “Terms of Service”) available here: newrelic.com/termsandconditions/terms. IF YOU DO NOT AGREE TO THIS AGREEMENT, YOUR SOLE REMEDY IS TO NOT USE THE ADD-ON. 1. DEFINITIONS “Add-on” means the New Relic applications and/or software, including but not limited to connectors, extensions, UI extension, and plugins, provided by New Relic to Customer to enable the Services to operate with certain third party or Customer software or systems (“Underlying Software”). For the purposes of the Terms of Service, the Add-on shall be treated like an Agent, subject to the separate terms herein. 2. USE OF THE PROGRAMMABILITY ADD-ON 2.1 Software Evaluation License. Subject to the terms herein, New Relic grants to Customer a limited, non-exclusive, non-transferrable, non-sublicensable right to install, use, and configure the Add-on solely as needed to enable the Services for internal evaluation purposes. New Relic reserves all rights and licenses not expressly granted herein. 2.2 Ownership. As between the Parties, New Relic owns all right, title, and interest to the Add-on and Feedback, including but not limited to any intellectual property and proprietary rights therein. Customer retains all right, title, and interest in any Customer Data processed by the Add-on in connection with the Services. 2.3 Feedback. Customer agrees to provide feedback, suggestions, ideas, requests or recommendations (“Feedback”) regarding the Add-on, and hereby irrevocably assigns all intellectual property and proprietary rights it holds in the Feedback to New Relic. 2.4 Notice. Customer acknowledges and agrees that the Add-on is made to work with the Underlying Software as configured at the time of creation and there is no guarantee that Add-on will continue to work in the event Customer changes, replaces, upgrades versions of, updates, or otherwise changes the Underlying Software (an “Upgrade Event”). Customer shall hold New Relic harmless from any claims or damages arising from Customer’s Upgrade Event. 2.5 Restrictions. Customer will not: (i) use the Add-on except as permitted hereunder; (ii) distribute, sell, sublicense, or otherwise transfer the Add-on; (iii) decompile, disassemble or reverse engineer any software underlying the Add-on; (iv) use the Add-on to damage, disable, overburden or impair any New Relic server or network(s) connected to any New Relic server or interfere with any other party’s use and enjoyment of the Services; (v) use the Add-on to defraud, defame, abuse, harass, stalk, threaten or infringe the rights of privacy or other intellectual property rights of others or otherwise violate any applicable law; (vi) circumvent or modify any security technologies included as part of the Add-on; or (vii) attempt or permit any third party to do any of the foregoing. New Relic may suspend Customer usage of the Services, without notice, pending any investigation of misuse. These restrictions may be supplemented or superceded (to the extent they conflict) by the New Relic Acceptable Use Policy as may be published and updated from time to time on the New Relic web site at docs.newrelic.com/docs/licenses/license-information/acceptable-use-policy/acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential Information” means: (a) the Programmability Add-on, and any features, results or output produced by, and other information relating to the Add-on (including, without limitation, all Feedback); and (b) any business or technical information of New Relic including but not limited to any technical information, research, development, know-how that a reasonable person would understand to be confidential. 3.2 Restrictions. Customer will not use or disclose any Confidential Information, except as necessary for the performance of this Agreement. Customer will use all reasonable efforts to protect Confidential Information from unauthorized use or disclosure, but in no event less than the efforts that it ordinarily uses with respect to its own proprietary information. Customer may disclose Confidential Information to those of its employees who have a bona fide need to know such Confidential Information for the performance of this Agreement; provided that each such employee first executes a written agreement that contains use and nondisclosure restrictions at least as protective as those set forth herein. Confidential Information shall not include any information that: (a) is or becomes generally known to the public through no fault or breach of this Agreement by Customer; (b) is rightfully known by Customer at the time of disclosure without an obligation of confidentiality; (c) is independently developed by Customer without access or use of any Confidential Information; or (d) is rightfully obtained from a third party without restriction on use or disclosure. 4. DISCLAIMER 4.1 DISCLAIMER. THE ADD-ON IS PROVIDED AS-IS AND AS-AVAILABLE AND NEW RELIC DISCLAIMS AND MAKES NO WARRANTIES, EITHER EXPRESS OR IMPLIED, INCLUDING ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR NON-INFRINGEMENT AND WHETHER OR NOT ARISING THROUGH A COURSE OF DEALING. THE ADD-ON IS NOT GUARANTEED TO BE ERROR-FREE, COMPATIBLE WITH THE UNDERLYING SOFTWARE, OR THAT CUSTOMER WILL ACHIEVE ANY RESULTS FROM USE OF THE ADD-ON THEREFROM. 4.2 LIMITATION OF LIABILITY. TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC, ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE TO CUSTOMER OR ANY THIRD PARTY FOR DAMAGES OF ANY KIND, INCLUDING, WITHOUT LIMITATION, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE INSTALLATION, USE OR INABILITY TO USE THE ADD-ON OR FOR ANY ERROR OR DEFECT IN THE ADD-ON OR THE SERVICES, REGARDLESS OF LEGAL THEORY, WHETHER OR NOT NEW RELIC HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSS OR DAMAGE. NEW RELIC’S AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS AGREEMENT WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE PARTIES HAVE AGREED THAT THESE LIMITATIONS WILL SURVIVE AND APPLY EVEN IF ANY LIMITED REMEDY SPECIFIED IN THIS AGREEMENT IS FOUND TO HAVE FAILED OF ITS ESSENTIAL PURPOSE. THE LIMITATIONS OF DAMAGES SET FORTH ABOVE ARE FUNDAMENTAL ELEMENTS OF THE BASIS OF THE BARGAIN BETWEEN NEW RELIC AND CUSTOMER. 4.3 EVALUATION VERSION. Customer agrees and acknowledges that: (a) the Add-on is not an official product and has not been commercially released for sale by New Relic; (b) the Add-on may not operate properly, being in final form, or fully functional; (c) the Add-on may contain errors, security vulnerabilities, design flaws, or other problems; (d) it may not be possible to make the Add-on fully functional; (e) the information obtained using the Add-on may not be accurate; (f) use of the Add-on may result in unexpected results, loss of data, delays or other unpredictable damages or loss; (g) New Relic is under no obligation to release a commercial version of the Add-on; and (h) New Relic has the right unilaterally to abandon development of the Add-on, at any time and without any obligation or liability to Customer. 5. GENERAL PROVISIONS 5.1 Terms of Service; Documentation. This Agreement shall be considered a part of the New Relic documentation, located at: https://docs.newrelic.com (the “Documentation). This Agreement and the Terms of Service constitute the entire and exclusive agreement between New Relic and Customer with respect to the Add-on. In the absence of a separate agreement, upon agreement between New Relic and Customer with respect to the Add-On, the terms and conditions of this Agreement shall govern the relationship between New Relic and Customer with respect to such Add-On. To the extent of a conflict between the Agreement and the Terms of Service, this Agreement shall govern with respect to the Add-on only.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.76915,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Add-on end user <em>license</em> agreement",
        "sections": "Add-on end user <em>license</em> agreement",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": "&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;acceptable-use-policy&#x2F;acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential <em>Information</em>” means: (a) the Programmability Add-on, and any features, results or output produced by, and other <em>information</em> relating to the Add-on (including, without limitation"
      },
      "id": "603ec23328ccbccf1beba79a"
    },
    {
      "sections": [
        "New Relic Agent Software Notice"
      ],
      "title": "New Relic Agent Software Notice",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "2bf9501c2767105130d3808f1bf3a91a032d903e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/new-relic-agent-software-notice/",
      "published_at": "2022-01-08T09:14:04Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This software is © 2008-2021 New Relic, Inc. and its licensors. This software is solely for use with New Relic’s proprietary SaaS service (“New Relic Service”), so to use the software you must have a valid account for the New Relic Service under a separate agreement with New Relic (“Subscription Agreement”). You may only use the software to support your use of the New Relic Service as permitted in the Subscription Agreement. Without a Subscription Agreement, you may not use the software. All other use is prohibited. New Relic and its suppliers retain all right, title and interest (including intellectual property rights) in the software. The Subscription Agreement will control in event of a conflict with this notice. Unless otherwise agreed by New Relic in your Subscription Agreement: You may not use, copy, distribute or sublicense the software, use the software on behalf of third parties, reverse engineer or decompile the software, modify or create derivative works of the software, use the software for competitive analysis or benchmarking, or remove or obscure any proprietary notices in the software. The software is provided “AS IS” and New Relic disclaims all warranties, whether express, implied, statutory or otherwise, including warranties of merchantability, fitness for a particular purpose, title or noninfringement. To the full extent permitted by law, New Relic will have no liability arising from or related to the software or under this notice for any direct, indirect, special, incidental, or consequential damages of any kind, even if advised of their possibility in advance, and regardless of legal theory (whether contract, tort, negligence, strict liability or otherwise). The software may contain third-party open source software (“OSS”) as described here and at https://github.com/newrelic. To the extent required by the OSS license, that license will apply to the OSS when used on a stand-alone basis. For avoidance of doubt, you may copy and distribute New Relic agents pursuant to your Subscription Agreement for New Relic Browser and New Relic Mobile and as set forth: https://docs.newrelic.com/docs/mobile-monitoring/new-relic-mobile/get-started and https://docs.newrelic.com/docs/browser/new-relic-browser/installation/install-new-relic-browser-agent Software versions New Relic makes available under an OSS license (such as Apache 2.0) are governed by the terms of the applicable OSS license. For a current list of New Relic software versions released as OSS please visit https://opensource.newrelic.com/.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.76753,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": " and at https:&#x2F;&#x2F;github.com&#x2F;newrelic. To the extent required by the OSS <em>license</em>, that <em>license</em> will apply to the OSS when used on a stand-alone basis. For avoidance of doubt, you may copy and distribute New Relic agents pursuant to your Subscription Agreement for New Relic Browser and New Relic Mobile"
      },
      "id": "603eb73828ccbc1f99eba74a"
    },
    {
      "sections": [
        "FIT instrumentation end user license agreement"
      ],
      "title": "FIT instrumentation end user license agreement",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "e8e10baf120678407d08c9f78ab708d271cf0223",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/fit-instrumentation-end-user-license-agreement/",
      "published_at": "2022-01-08T09:14:46Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "NEW RELIC, INC. FIT INSTRUMENTATION END USER LICENSE AGREEMENT In connection with the work provided by New Relic’s Field Instrumentation Team, you may be provided with certain custom-created software to enable, optimize, or enhance your use of New Relic’s Services. By downloading, installing, authorizing installation, or using the FIT Instrumentation with your Underlying Software (defined below), you (“Customer”) agree to the terms and conditions herein (“Agreement”) with New Relic, Inc., a Delaware corporation with offices located at 188 Spear Street, Suite 1200 San Francisco, CA 94105 (“New Relic”), (collectively the “Parties”). Capitalized terms not defined herein shall have the meanings set forth in the New Relic Terms of Service (the “Terms of Service”) available here: http://newrelic.com/terms IF YOU DO NOT AGREE TO THIS AGREEMENT, PLEASE DO NOT USE THE FIT INSTRUMENTATION. 1. DEFINITIONS “FIT Instrumentation” means the New Relic custom-made software, including but not limited to connectors, extensions, and plugins, provided by New Relic to Customer to enable the Services to operate with certain third party or Customer software or systems (“Underlying Software”). For the purposes of the Terms of Service, the FIT Instrumentation shall be treated like an “Agent”, subject to the separate terms herein. 2. USE OF THE FIT INSTRUMENTATION 2.1 Software Evaluation License. Subject to the terms herein, New Relic grants to Customer a limited, non-exclusive, non-transferrable, non-sublicensable right to install, use, and configure the FIT Instrumentation solely as needed to enable the Services for internal evaluation purposes. New Relic reserves all rights and licenses not expressly granted herein. 2.2 Ownership. As between the Parties, New Relic owns all right, title, and interest to the FIT Instrumentation and Feedback, including but not limited to any intellectual property and proprietary rights therein. Customer retains all right, title, and interest in any Customer Data processed by the FIT Instrumentation in connection with the Services. 2.3 Feedback. Customer agrees to provide feedback, suggestions, ideas, requests or recommendations (“Feedback”) regarding the FIT Instrumentation, and hereby irrevocably assigns all intellectual property and proprietary rights it holds in the Feedback to New Relic. 2.4 Notice. Customer acknowledges and agrees that the FIT Instrumentation is made to work with the Underlying Software as configured at the time of creation and there is no guarantee that FIT Instrumentation will continue to work in the event Customer changes, replaces, upgrades versions of, updates, or otherwise changes the Underlying Software (an “Upgrade Event”). Customer shall hold New Relic harmless from any claims or damages arising from Customer’s Upgrade Event. 2.5 Restrictions. Customer will not: (i) use the FIT Instrumentation except as permitted hereunder; (ii) distribute, sell, sublicense, or otherwise transfer the FIT Instrumentation; (iii) decompile, disassemble or reverse engineer any software underlying the FIT Instrumentation; (iv) use the FIT Instrumentation to damage, disable, overburden or impair any New Relic server or network(s) connected to any New Relic server or interfere with any other party’s use and enjoyment of the Services; (v) use the FIT Instrumentation to defraud, defame, abuse, harass, stalk, threaten or infringe the rights of privacy or other intellectual property rights of others or otherwise violate any applicable law; (vi) circumvent or modify any security technologies included as part of the FIT Instrumentation; or (vii) attempt or permit any third party to do any of the foregoing. New Relic may suspend usage of the Services by any user, without notice, pending any investigation of misuse.  These restrictions may be supplemented or superceded (to the extent they conflict) by the New Relic Acceptable Use Policy as may be published and updated from time to time at https://docs.newrelic.com/docs/licenses/license-information/acceptable-use-policy/acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential Information” means: (a) the FIT Instrumentation, and any features, results or output produced by, and other information relating to the FIT Instrumentation (including, without limitation, all Feedback); and (b) any business or technical information of New Relic including but not limited to any technical information, research, development, know-how that a reasonable person would understand to be confidential. 3.2 Restrictions. Customer will not use or disclose any Confidential Information, except as necessary for the performance of this Agreement. Customer will use all reasonable efforts to protect Confidential Information from unauthorized use or disclosure, but in no event less than the efforts that it ordinarily uses with respect to its own proprietary information. Customer may disclose Confidential Information to those of its employees who have a bona fide need to know such Confidential Information for the performance of this Agreement; provided that each such employee first executes a written agreement that contains use and nondisclosure restrictions at least as protective as those set forth herein. Confidential Information shall not include any information that: (a) is or becomes generally known to the public through no fault or breach of this Agreement by Customer; (b) is rightfully known by Customer at the time of disclosure without an obligation of confidentiality; (c) is independently developed by Customer without access or use of any Confidential Information; or (d) is rightfully obtained from a third party without restriction on use or disclosure. 4. DISCLAIMER 4.1 DISCLAIMER. FIT INSTRUMENTATION IS PROVIDED AS-IS AND AS-AVAILABLE AND NEW RELIC DISCLAIMS AND MAKES NO WARRANTIES, EITHER EXPRESS OR IMPLIED, INCLUDING ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR NON-INFRINGEMENT AND WHETHER OR NOT ARISING THROUGH A COURSE OF DEALING. FIT INSTRUMENTATION IS NOT GUARANTEED TO BE ERROR-FREE, COMPATIBLE WITH THE UNDERLYING SOFTWARE. , OR THAT CUSTOMER WILL ACHIEVE ANY RESULTS FROM USE OF THE FIT INSTRUMENTATION THEREFROM. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OF IMPLIED WARRANTIES OR LIMITATIONS ON APPLICABLE STATUTORY RIGHTS OF A CONSUMER, SO THE ABOVE EXCLUSION AND LIMITATIONS MAY NOT APPLY TO THE CUSTOMER. 4.2 LIMITATION OF LIABILITY. TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE TO CUSTOMER OR ANY THIRD PARTY FOR DAMAGES OF ANY KIND, INCLUDING, WITHOUT LIMITATION, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE INSTALLATION, USE OR INABILITY TO USE THE FIT INSTRUMENTATION OR FOR ANY ERROR OR DEFECT IN THE FIT INSTRUMENTATION OR THE SERVICES, WHETHER SUCH LIABILITY ARISES FROM ANY CLAIM BASED UPON CONTRACT, WARRANTY, TORT, STRICT LIABILITY, OR OTHERWISE, WHETHER OR NOT NEW RELIC HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSS OR DAMAGE. NEW RELIC'S AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS AGREEMENT WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE PARTIES HAVE AGREED THAT THESE LIMITATIONS WILL SURVIVE AND APPLY EVEN IF ANY LIMITED REMEDY SPECIFIED IN THIS AGREEMENT IS FOUND TO HAVE FAILED OF ITS ESSENTIAL PURPOSE. 4.3 EVALUATION VERSION. Customer agrees and acknowledges that: (a) the FIT Instrumentation is not an official product and has not been commercially released for sale by New Relic; (b) the FIT Instrumentation may not operate properly, being in final form, or fully functional; (c) the FIT Instrumentation may contain errors, security vulnerabilities, design flaws, or other problems; (d) it may not be possible to make the FIT Instrumentation fully functional; (e) the information obtained using the FIT Instrumentation may not be accurate; (f) use of the FIT Instrumentation may result in unexpected results, loss of data, delays or other unpredictable damages or loss; (g) New Relic is under no obligation to release a commercial version of the FIT Instrumentation; and (h) New Relic has the right unilaterally to abandon development of the FIT Instrumentation, at any time and without any obligation or liability to Customer. 5. GENERAL PROVISIONS 5.1 Terms of Service; Documentation. This Agreement shall be considered a part of the New Relic documentation, located at: https://docs.newrelic.com (the \"Documentation\").This Agreement and the Terms of Service constitute the entire and exclusive agreement between New Relic and Customer with respect to the FIT Instrumentation. In the absence of a separate agreement between New Relic and Customer with respect to the FIT Instrumentation, the terms and conditions of this Agreement shall govern the relationship between New Relic and Customer with respect to such FIT Instrumentation.  To the extent of a conflict between the Agreement and the Terms of Service, this Agreement shall govern with respect to the FIT Instrumentation only.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.76753,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "FIT instrumentation end user <em>license</em> agreement",
        "sections": "FIT instrumentation end user <em>license</em> agreement",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": " Acceptable Use Policy as may be published and updated from time to time at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;acceptable-use-policy&#x2F;acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential <em>Information</em>” means: (a) the FIT Instrumentation, and any features, results"
      },
      "id": "603e9f3ee7b9d206e32a0800"
    }
  ],
  "/docs/licenses/license-information/product-definitions/legacy-product-definitions": [
    {
      "sections": [
        "New Relic One pricing: Definitions",
        "Account",
        "Commitment Term",
        "Customer Data",
        "Customer Properties",
        "Documentation",
        "GB Ingested",
        "Incident event",
        "Login Credentials",
        "Monthly Provisioned User",
        "Order",
        "Paid Terms of Service",
        "Product(s)",
        "Software",
        "Terms",
        "Third-Party Services",
        "Unpaid Terms of Service",
        "Usage Plan"
      ],
      "title": "New Relic One pricing: Definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "609575acd671fecf7899378157eabc57bc8d68e2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/new-relic-one-pricing-definitions/",
      "published_at": "2022-01-08T09:16:18Z",
      "updated_at": "2021-11-25T06:51:33Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our New Relic One pricing model (for a glossary for our original pricing, see Original pricing definitions. Account Account refers to the online account or subaccounts that New Relic provides for customers to manage their use of the Products. Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, information, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer Properties Customer Properties means Customer’s websites, infrastructure, networks, mobile applications, or other systems, as well as Customer accounts on Third-Party Services. Documentation Documentation means the New Relic technical guides and documentation made available from the dedicated ‘Documentation’ page of the New Relic website. GB Ingested A GB Ingested is a measurement of the volume of metrics, events, logs, traces, or other telemetry data sent to or generated by the Products for the benefit of the Customer, including from the Software, the Customer Properties, or Third-Party Services. In this context, a GB is defined as 1 billion bytes. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Login Credentials Login Credentials means the username, email address, password, or other personal information that is provided by a Customer user in order to manage an Account. Monthly Provisioned User A Monthly Provisioned User is any user who can log into Customer’s Account(s) and access the New Relic One Product functionality as specified in an Order and the Documentation. In our public docs, this is referred to as a full platform user or a full user. Order Order means the purchasing order for access to the Service or related services that: (1) is either executed by the Parties or entered into by you via self-service, and references this Agreement, or (2) is entered into by you and a Channel Partner. Paid Terms of Service Paid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/paid. Product(s) Product(s) mean the purchase of the New Relic subscription products described in the applicable Order and any updates, corrections, bug fixes, modifications, improvements, related services, new features, and functionality (made generally available to New Relic’s customer base) thereto. Software Software means the distributed software, APIs, scripts, or other code proprietary to New Relic provided with the Products. Terms Terms means the underlying Customer-New Relic agreement and the Order. Third-Party Services Third-Party Services means any third party platform, add-on, service, or product not provided by New Relic and that a user integrates or enables for use with the Products, including third-party applications and plug-ins. Unpaid Terms of Service Unpaid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/unpaid. Usage Plan Usage Plan refers to the Service or Product pricing, invoicing related information, and product-specific terms (e.g. concurrent user account sessions) contained within the Documentation. To learn more about this pricing model, see New Relic One pricing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 220.20161,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic One pricing: <em>Definitions</em>",
        "sections": "New Relic One pricing: <em>Definitions</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " to the Service or <em>Product</em> pricing, invoicing related <em>information</em>, and <em>product</em>-specific terms (e.g. concurrent user account sessions) contained within the Documentation. To learn more about this pricing model, see New Relic One pricing."
      },
      "id": "6044e6e528ccbc26f22c6084"
    },
    {
      "sections": [
        "Product-based pricing usage and New Relic Platform Pricing Usage Plan",
        "Important",
        "Usage Plan: New Relic Platform Pricing",
        "Eligible Services",
        "Eligible Services (Partners)",
        "Subscriptions with indeterminate pricing or usage quantities",
        "Product Usage Ratio"
      ],
      "title": "Product-based pricing usage and New Relic Platform Pricing Usage Plan",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Usage plans"
      ],
      "external_id": "e2686dc773c4e844544ce633a9a41a16f15edf5a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/usage-plans/product-based-pricing-usage-new-relic-platform-pricing-usage-plan/",
      "published_at": "2022-01-08T09:17:02Z",
      "updated_at": "2021-11-13T14:45:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This doc explains our original product-based pricing model. For more about pricing changes, see Overview of pricing changes. The following provisions are applicable to existing customers (i) that have existing Terms, and (ii) that to the extent its subscription to the New Relic products reference the usage plans set forth below or where a subscription has indeterminate product pricing or usage quantities. New customers are eligible for New Relic One pricing as described here. Capitalized terms not defined below shall take on the meaning set forth in such New Relic order form. Usage Plan: New Relic Platform Pricing The following Eligible Services with New Relic are products that are referenced in specific order forms for New Relic Platform Pricing. For more information, contact your New Relic account representative. Eligible Services Eligible Services Per Unit Unit of Measure Monthly Standard Fee Rate New Relic APM Pro CU Annual* 10,000 Compute Units $166.70 New Relic APM Pro Host Annual* 1 Hosts $149 New Relic Insights Pro Annual 50,000,000 Events $165 New Relic Infrastructure Pro Annual 10,000 Compute Units $12 New Relic Browser Pro Annual 500,000 Page Views $149 New Relic Logs Annual - 8 Days* * 1 Per GB Daily $55 New Relic Logs Annual - 15 Days* * 1 Per GB Daily $65 New Relic Logs Annual - 30 Days* * 1 Per GB Daily $75 New Relic Metrics Annual 1,000 Data Points per Minute $25 New Relic Mobile Enterprise Annual 50,000 Total Users $499 New Relic Serverless for AWS Lambda Annual 1,000,000 AWS Lambda Events $15 New Relic Synthetics Pro Annual 10,000 Checks $69 New Relic Synthetics private locations (for New Relic Synthetics Pro Annual product) Fixed fee N/A $1,000 New Relic Traces Annual 1,000,000 Spans $1 New Relic AI Incident Intelligence Annual 1,000 Incident Events $500 New Relic AI Proactive Detection Annual 1,000,000,000 App Transactions $250 * Customer may utilize either New Relic APM Pro Annual or New Relic APM Pro CUs Annual, but not both. Customer will be provisioned New Relic APM Pro CUs Annual by default, unless otherwise stated in the relevant Order Form. * * Customer may utilize only one of New Relic Logs Annual - 8 Days, New Relic Logs Annual - 15 Days, or New Relic Logs Annual - 30 days. For the avoidance of doubt, Customer may not utilize New Relic Logs Annual with varying numbers of days retention concurrently. Customer will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. For more information about units of measures, see Product definitions. Eligible Services (Partners) Eligible services for New Relic Partners Measurement New Relic APM Pro CU Annual* Sold in Units of 8,900 Compute Units New Relic APM Pro Host Annual* Sold in Units of 1 Host New Relic Browser Pro Annual Sold in Units of 500,000 Page Views New Relic Infrastructure Pro Annual Sold in Units of 125,000 Compute Units New Relic Insights Pro Annual Sold in Units of 45,000,000 Events New Relic Logs Annual - 8 Days* * Sold in Units of 3 GB Daily New Relic Logs Annual - 15 Days* * Sold in Units of 2.5 GB Daily New Relic Logs Annual - 30 Days* * Sold in Units of 2 GB Daily New Relic Metrics Annual Sold in Units of 6,000 Data Points per Minute New Relic Mobile Enterprise Annual Sold in Units of 15,000 Total Users New Relic Serverless for AWS Lambda Annual Sold in Units of 10,000,000 AWS Lambda Events New Relic Synthetics Pro with Private Locations Annual Sold in Units of 22,000 Checks New Relic Traces Annual Sold in Units of 149,000,000 Spans * Customer and/or Partner may utilize either New Relic APM Pro Annual or New Relic APM Pro CUs Annual, but not both. Customer and/or Partner will be provisioned New Relic APM Pro CUs Annual by default, unless otherwise stated in the relevant Order Form. * * Customer and/or Partner may utilize only one of New Relic Logs Annual - 8 Days, New Relic Logs Annual - 15 Days, or New Relic Logs Annual - 30 days. For the avoidance of doubt, Customer may not utilize New Relic Logs Annual with varying numbers of days retention concurrently. Customer and/or Partner will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. Subscriptions with indeterminate pricing or usage quantities Product Usage Ratio Where a Customer subscription to the Products contain no specific pricing or quantities, Product-specific usage for a specific calendar month shall be determined through the calculation of the following ratio (the “Product Usage Ratio”) where: (1) the numerator shall be the Product specific usage for such calendar month multiplied by such Product’s list price, and (2) the denominator shall be the the aggregate Customer usage of all Products during such calendar month multiplied by all such Product(s) list price. The Product Usage Ratio shall be the percentage of usage for a specific Product for a calendar month period, or if usage cannot be measured for a period, the last Product Usage Ratio that can be calculated shall be assumed constant.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 150.99573,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Product</em>-based pricing usage and New Relic Platform Pricing Usage Plan",
        "sections": "<em>Product</em>-based pricing usage and New Relic Platform Pricing Usage Plan",
        "tags": "<em>License</em> <em>information</em>",
        "body": " will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. For more <em>information</em> about units of measures, see <em>Product</em> <em>definitions</em>. Eligible Services (Partners) Eligible services for New Relic Partners Measurement New Relic APM Pro CU Annual* Sold"
      },
      "id": "603ea32a28ccbc7e22eba768"
    },
    {
      "sections": [
        "Translation disclaimer"
      ],
      "title": "Translation disclaimer",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Translation"
      ],
      "external_id": "cb23af79521feaaa6193002aef89648a9973cf1d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/translated-documentation/translation-disclaimer/",
      "published_at": "2022-01-08T07:40:57Z",
      "updated_at": "2021-12-09T01:58:23Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The official version of the Documentation is in English. Portions of the New Relic Documentation may be translated for your convenience as shown on this site. Translated content may contain errors, discrepancies, or differences created in the translation, are not binding, and have no legal effect for enforcement or compliance purposes. New Relic does not make any warranty, express or implied, as to the accuracy, reliability, or correctness of any translated Documentation from the English original to any other language. Any guarantees or commitments in your Agreement with New Relic that the Services, products, or other offerings conform or are consistent with the Documentation do not apply to the extent to which the Documentation has been translated.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 142.24937,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>"
      },
      "id": "61b162bf196a672b190d46f7"
    }
  ],
  "/docs/licenses/license-information/product-definitions/new-relic-one-pricing-definitions": [
    {
      "sections": [
        "Original product-based pricing definitions",
        "App",
        "App transaction",
        "AWS Lambda event",
        "Check",
        "Compute unit",
        "Datapoints per minute",
        "Event",
        "Host",
        "Incident event",
        "Page view",
        "Per GB daily",
        "Span",
        "User"
      ],
      "title": "Original product-based pricing definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "42087e53167736831855bf9a4c2967c465677b45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/legacy-product-definitions/",
      "published_at": "2022-01-08T09:51:19Z",
      "updated_at": "2021-11-14T03:06:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our original product-based pricing. For New Relic One pricing model terms, see New Relic One pricing definitions. App An app means the application software designed to run on smartphones, tablet computers, and other mobile devices. Each app includes a predefined number of users. App transaction An app transaction is an APM application's attempt to process a web or non-web request. In New Relic APM these manifest as throughput TIMESERIES or individually as events in the Transaction event type. AWS Lambda event An AWS Lambda event means the row of data collected from the customer's AWS Lambda function by the New Relic agent or sent from an external service into the New Relic platform. It consists of the AwsLambdaInvocation, AwsLambdaInvocationError, or custom event types. Check A check means the single instance of a Synthetics monitor running in New Relic's monitoring network and reporting back response time, and whether the check was a success or failure. Compute unit A compute unit means the measure of resources associated with a unit of computation on a physical or virtual host. Datapoints per minute Datapoints per minute (DPM) refers to the per-minute rate at which individual metric values are sent to the New Relic Metric Ingest API. For billing purposes, datapoints per minute are calculated as a monthly average value by summing the datapoints ingested during a 30 day period and dividing by the number of minutes in that period (43,200). Event An event means the row of data collected from the customer's application by the New Relic agent or sent from an external service into our database. Host A host means the physical computer or virtual machine instance running a single copy of an operating system. Host usage is tracked monthly by summing the hours that every host in the account is connected to New Relic and dividing by 750. A host is counted if it is connected any time during an hour. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Page view A page view means the full page load (triggering an onLoad event) or a recorded URL change (state change). Per GB daily Per GB daily represents a daily average of Log data sent to New Relic over a 30 day period. Span A span represents an operation summary collected from the customer's application via the New Relic APM agent or New Relic Serverless for AWS Lambda agent, or sent from other tracing tools to the New Relic Trace API. User A user means the individual that connects to your app from a single device. Each unique device is considered as a unique user.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 213.63542,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Original <em>product</em>-based pricing <em>definitions</em>",
        "sections": "Original <em>product</em>-based pricing <em>definitions</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": "This is a glossary of terms that appear in contracts for our original <em>product</em>-based pricing. For New Relic One pricing model terms, see New Relic One pricing <em>definitions</em>. App An app means the application software designed to run on smartphones, tablet computers, and other mobile devices. Each app"
      },
      "id": "603ebacc64441f77774e8872"
    },
    {
      "sections": [
        "Product-based pricing usage and New Relic Platform Pricing Usage Plan",
        "Important",
        "Usage Plan: New Relic Platform Pricing",
        "Eligible Services",
        "Eligible Services (Partners)",
        "Subscriptions with indeterminate pricing or usage quantities",
        "Product Usage Ratio"
      ],
      "title": "Product-based pricing usage and New Relic Platform Pricing Usage Plan",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Usage plans"
      ],
      "external_id": "e2686dc773c4e844544ce633a9a41a16f15edf5a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/usage-plans/product-based-pricing-usage-new-relic-platform-pricing-usage-plan/",
      "published_at": "2022-01-08T09:17:02Z",
      "updated_at": "2021-11-13T14:45:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This doc explains our original product-based pricing model. For more about pricing changes, see Overview of pricing changes. The following provisions are applicable to existing customers (i) that have existing Terms, and (ii) that to the extent its subscription to the New Relic products reference the usage plans set forth below or where a subscription has indeterminate product pricing or usage quantities. New customers are eligible for New Relic One pricing as described here. Capitalized terms not defined below shall take on the meaning set forth in such New Relic order form. Usage Plan: New Relic Platform Pricing The following Eligible Services with New Relic are products that are referenced in specific order forms for New Relic Platform Pricing. For more information, contact your New Relic account representative. Eligible Services Eligible Services Per Unit Unit of Measure Monthly Standard Fee Rate New Relic APM Pro CU Annual* 10,000 Compute Units $166.70 New Relic APM Pro Host Annual* 1 Hosts $149 New Relic Insights Pro Annual 50,000,000 Events $165 New Relic Infrastructure Pro Annual 10,000 Compute Units $12 New Relic Browser Pro Annual 500,000 Page Views $149 New Relic Logs Annual - 8 Days* * 1 Per GB Daily $55 New Relic Logs Annual - 15 Days* * 1 Per GB Daily $65 New Relic Logs Annual - 30 Days* * 1 Per GB Daily $75 New Relic Metrics Annual 1,000 Data Points per Minute $25 New Relic Mobile Enterprise Annual 50,000 Total Users $499 New Relic Serverless for AWS Lambda Annual 1,000,000 AWS Lambda Events $15 New Relic Synthetics Pro Annual 10,000 Checks $69 New Relic Synthetics private locations (for New Relic Synthetics Pro Annual product) Fixed fee N/A $1,000 New Relic Traces Annual 1,000,000 Spans $1 New Relic AI Incident Intelligence Annual 1,000 Incident Events $500 New Relic AI Proactive Detection Annual 1,000,000,000 App Transactions $250 * Customer may utilize either New Relic APM Pro Annual or New Relic APM Pro CUs Annual, but not both. Customer will be provisioned New Relic APM Pro CUs Annual by default, unless otherwise stated in the relevant Order Form. * * Customer may utilize only one of New Relic Logs Annual - 8 Days, New Relic Logs Annual - 15 Days, or New Relic Logs Annual - 30 days. For the avoidance of doubt, Customer may not utilize New Relic Logs Annual with varying numbers of days retention concurrently. Customer will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. For more information about units of measures, see Product definitions. Eligible Services (Partners) Eligible services for New Relic Partners Measurement New Relic APM Pro CU Annual* Sold in Units of 8,900 Compute Units New Relic APM Pro Host Annual* Sold in Units of 1 Host New Relic Browser Pro Annual Sold in Units of 500,000 Page Views New Relic Infrastructure Pro Annual Sold in Units of 125,000 Compute Units New Relic Insights Pro Annual Sold in Units of 45,000,000 Events New Relic Logs Annual - 8 Days* * Sold in Units of 3 GB Daily New Relic Logs Annual - 15 Days* * Sold in Units of 2.5 GB Daily New Relic Logs Annual - 30 Days* * Sold in Units of 2 GB Daily New Relic Metrics Annual Sold in Units of 6,000 Data Points per Minute New Relic Mobile Enterprise Annual Sold in Units of 15,000 Total Users New Relic Serverless for AWS Lambda Annual Sold in Units of 10,000,000 AWS Lambda Events New Relic Synthetics Pro with Private Locations Annual Sold in Units of 22,000 Checks New Relic Traces Annual Sold in Units of 149,000,000 Spans * Customer and/or Partner may utilize either New Relic APM Pro Annual or New Relic APM Pro CUs Annual, but not both. Customer and/or Partner will be provisioned New Relic APM Pro CUs Annual by default, unless otherwise stated in the relevant Order Form. * * Customer and/or Partner may utilize only one of New Relic Logs Annual - 8 Days, New Relic Logs Annual - 15 Days, or New Relic Logs Annual - 30 days. For the avoidance of doubt, Customer may not utilize New Relic Logs Annual with varying numbers of days retention concurrently. Customer and/or Partner will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. Subscriptions with indeterminate pricing or usage quantities Product Usage Ratio Where a Customer subscription to the Products contain no specific pricing or quantities, Product-specific usage for a specific calendar month shall be determined through the calculation of the following ratio (the “Product Usage Ratio”) where: (1) the numerator shall be the Product specific usage for such calendar month multiplied by such Product’s list price, and (2) the denominator shall be the the aggregate Customer usage of all Products during such calendar month multiplied by all such Product(s) list price. The Product Usage Ratio shall be the percentage of usage for a specific Product for a calendar month period, or if usage cannot be measured for a period, the last Product Usage Ratio that can be calculated shall be assumed constant.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 150.99573,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Product</em>-based pricing usage and New Relic Platform Pricing Usage Plan",
        "sections": "<em>Product</em>-based pricing usage and New Relic Platform Pricing Usage Plan",
        "tags": "<em>License</em> <em>information</em>",
        "body": " will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. For more <em>information</em> about units of measures, see <em>Product</em> <em>definitions</em>. Eligible Services (Partners) Eligible services for New Relic Partners Measurement New Relic APM Pro CU Annual* Sold"
      },
      "id": "603ea32a28ccbc7e22eba768"
    },
    {
      "sections": [
        "Translation disclaimer"
      ],
      "title": "Translation disclaimer",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Translation"
      ],
      "external_id": "cb23af79521feaaa6193002aef89648a9973cf1d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/translated-documentation/translation-disclaimer/",
      "published_at": "2022-01-08T07:40:57Z",
      "updated_at": "2021-12-09T01:58:23Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The official version of the Documentation is in English. Portions of the New Relic Documentation may be translated for your convenience as shown on this site. Translated content may contain errors, discrepancies, or differences created in the translation, are not binding, and have no legal effect for enforcement or compliance purposes. New Relic does not make any warranty, express or implied, as to the accuracy, reliability, or correctness of any translated Documentation from the English original to any other language. Any guarantees or commitments in your Agreement with New Relic that the Services, products, or other offerings conform or are consistent with the Documentation do not apply to the extent to which the Documentation has been translated.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 142.24937,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>"
      },
      "id": "61b162bf196a672b190d46f7"
    }
  ],
  "/docs/licenses/license-information/referenced-policies/new-relic-pre-release-policy": [
    {
      "sections": [
        "Security guide",
        "Tip",
        "Security Program",
        "Security Domains",
        "Security Certifications",
        "Data Control, Facilities, and Encryption",
        "Law Enforcement Request Report"
      ],
      "title": "Security guide",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "356f0d11ffcb62208a743a0a7c127f5f6da9c940",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-guide/",
      "published_at": "2022-01-08T12:36:19Z",
      "updated_at": "2021-12-09T15:28:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Last updated September 17, 2021. This is supplement to our security policy and serves as a guide to New Relic’s description of its Services, functionalities, and features. Tip We may update the URLs in this document without notice. Security Program New Relic follows \"privacy by design\" principles as described here: https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Security Domains New Relic’s policies and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless Security; Configuration Management; Vulnerability Management; Network Protection; Transmission Protection; Password Management; Access Control, Audit Logging & Monitoring; Education, Training, and Awareness; Third Party Assurance; Incident Management; Business Continuity and Disaster Recover; Risk Management; Data Protection & Privacy; and Service Management Systems. Security Certifications New Relic audits its Services against industry standards as described at https://docs.newrelic.com/docs/security/security-privacy/compliance/regulatory-audits-new-relic-services/. Data Control, Facilities, and Encryption New Relic's customers can send data to New Relic's APIs by (1) using New Relic's software, (2) using vendor-neutral software that is managed and maintained by a third-party such as via OpenTelemetry instrumentation provided by opentelemetry.io, or (3) from third-party systems that customer's manage and/or control. New Relic's customers can use New Relic's Services such as NerdGraph to filter out and drop data. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/drop-data-using-nerdgraph/. New Relic's customers can adjust their data retention periods as appropriate for their needs. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/manage-data-retention/#adjust-retention. New Relic Logs obfuscates numbers that match known patterns, such as bank card and social security numbers as described here: https://docs.newrelic.com/docs/logs/log-management/get-started/new-relics-log-management-security-privacy/. New Relic honors requests to delete personal data in accordance with applicable privacy laws. Please see https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Customers may use New Relic's APIs to query data, such as NerdGraph described here, and New Relic Services to export the data to other cloud providers. Customers can configure its log forwarder [https://docs.newrelic.com/docs/logs/enable-log-management-new-relic/enable-log-monitoring-new-relic/forward-your-logs-using-infrastructure-agent/] before sending infrastructure logs to New Relic. For New Relic Customers in New Relic US, FedRAMP and HIPAA-enabled environments, Customer Data is replicated to the off-site backup system via Amazon Simple Storage Service (S3). Category of Customer Description FedRAMP HIPAA-enabled US Gen Pop EU Gen Pop Data is stored in Amazon Web Services (“AWS”). Limited Data is stored in IBM Data for New Relic Incident Intelligence is stored in Google Cloud New Relic regularly tests, assess, and evaluates its measures to ensure the security of processing using industry-recognized standards and uses independent third-party auditors as provided below: Annual SOC 2 Type 2 Annual FedRAMP assessment by an independent third-party pursuant to NIST 800-53 rev 4 Moderate authorization. Annual HITRUST-validated assessment by an independent third-party *Pursuing CY2021 Q4 ISO 27001 TISAX The Services that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed information about AWS security is available at https://aws.amazon.com/security/ and http://aws.amazon.com/security/sharing-the-security-responsibility/. Data encryption at rest utilizes FIPS 140-2 compliant encryption methodology. For AWS SOC Reports, please see https://aws.amazon.com/compliance/soc-faqs/. The Services that operate on Google Cloud Platform (\"GCP\") are protected by the security and environmental controls of GCP. Detailed information about GCP security is available at https://cloud.google.com/docs/tutorials#security. For GCP reports, please see https://cloud.google.com/security/compliance/. IBM Deft Zayo QTS Law Enforcement Request Report New Relic has not to date received any request for customer data from a law enforcement or other government agency (including under any national security process), and has not made any corresponding disclosures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 246.06125,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " as described here: https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;security&#x2F;security-privacy&#x2F;data-privacy&#x2F;data-privacy-new-relic&#x2F;. Security Domains New Relic’s <em>policies</em> and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless"
      },
      "id": "6147558128ccbc973a56a863"
    },
    {
      "sections": [
        "Service level availability commitment",
        "New Relic Service Level Availability"
      ],
      "title": "Service level availability commitment",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "5489f268010887c72446f12059a1a0e4279ad960",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/service-level-availability-commitment/",
      "published_at": "2022-01-08T13:09:37Z",
      "updated_at": "2021-10-24T17:41:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you subscribe to \"New Relic One - Pro Users\" or \"New Relic One - Enterprise Users\", the service level availability commitments available to you are those set forth (i) in the Terms, or (ii) as set forth on this page. If you subscribe to \"New Relic One - Standard Users\", any service level availability commitment or related remedies contained in your Terms are vacated and nullified, and New Relic will use commercially reasonable efforts to make \"New Relic One - Standard Users\" available in line with industry standards. If you subscribe to any other New Relic Products on a product-based pricing basis, any service level availability commitment are contained in (i) your Terms, or (ii) if your Terms explicitly references this Service Level Availability commitment applying to the Service purchased in an Order, this page for the Services you use. Capitalized terms not defined below shall take on the meaning set forth in your Terms. New Relic Service Level Availability The Service will be considered available so long as Customer is able to log in to its interface and view Customer Data (\"Service Availability\"). The applicable Service Availability will be calculated as a percentage of: (1) the total number of minutes in a month after (2) subtracting any periods of unavailability during such month from the total number of minutes in a month. New Relic will use commercially reasonable efforts to maintain Service Availability of at least 99.8% during any calendar month. In the event the Service Availability drops below: (i) 98.5% for two consecutive calendar months during the Subscription Term, or (ii) 96.5% in any single calendar month, Customer may request to terminate the relevant Service with no penalty. Such termination will be effective as of the end of the then-current billing period and no additional fees will be charged. The service level within New Relic's control is the Service Availability, not, for example, the transmission of data over the public Internet. Service Availability calculations will exclude unavailability arising from any: (a) planned maintenance periods; (b) emergency maintenance that is necessary to prevent imminent harm to the Service; (c) force majeure events; (d) Third-Party Services, Customer application, equipment, software or other technology, or Customer or its User's use of the Service, in violation of the Agreement or not in accordance with the Documentation; or (e) suspension, limitation, and/or termination of Customer’s access or use of the Service in accordance with this Agreement. This describes Customer’s sole and exclusive remedy for failures of Service Availability. Customer may request the Service Availability attainment for the previous month by filing a support ticket on the New Relic support site.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 216.52391,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>"
      },
      "id": "603ea44828ccbcd9c8eba7b7"
    },
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Purpose",
        "2. Security Overview",
        "3.Security Certifications",
        "4. Data Control and Encryption",
        "5. Facilities",
        "6. Employee Access, Screening and Controls.",
        "7. Security Incident and Data Breach Response",
        "8. Network and Systems Security",
        "9. Authentication and Access Management",
        "10. Vulnerability Management",
        "11. System Access and Logging",
        "12. Disaster Recovery and Data Backup",
        "13. Copies and Removal",
        "14. Third Party Vendor Management",
        "15. Disclosure by Law",
        "16. Updates"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2022-01-08T13:08:54Z",
      "updated_at": "2021-09-27T15:05:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Updated 23 September 2021. The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Purpose This Policy describes New Relic's security program and the technical and organizational security controls to protect New Relic’s systems as shown in New Relic’s third-party audits and certifications. New Relic may update this Policy from time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/. 2. Security Overview 2.1. New Relic maintains a comprehensive Information Protection Program to manage information security within New Relic that includes administrative, technical, and physical safeguards designed to protect the confidentiality, integrity, and availability of Customer Data and that is appropriate to the nature, size, and complexity of New Relic's business operations. New Relic’s Information Protection Program includes: 2.1.1. executive review, support and accountability for all security related policies and practices; 2.1.2. formal written policies and procedures that are designed to protect against the loss, theft, or other unauthorized access or alteration of Customer Data, meet or exceed applicable industry standards, outlines a definition of information security and its overall objectives, include defined information security roles and responsibilities, a framework for setting control objectives and controls, a formal and effective risk mitigation program and a service provider security management program; 2.1.3. periodic risk assessments, including internal audits and/or independent 3rd party audits, to measure the effectiveness and appropriateness of controls for all systems processing Customer Data; 2.1.4. in-depth review of security incidents, including effective determination of root cause and corrective action; formal, industry-recognized controls frameworks based on an external audit standard; 2.1.5. employee screening and access management; and 2.1.6. comprehensive security testing, vulnerability identification, and mitigation methodology that consists of a variety of independent approaches that, when combined, are designed to maximize coverage for a diverse set of attack vectors. 2.2. New Relic's Chief Information Security Officer (CISO) leads New Relic's Information Security Program and develops, reviews, and approves, with appropriate stakeholders, New Relic's security policies and procedures. New Relic has appointed data protection officer(s) as described in New Relic’s Privacy Notices who are consulted as necessary under applicable laws. 2.3. Information security objectives, approach, scope, importance, goals, and principles for the organization’s security program are formally identified, communicated throughout the organization to users in a form that is relevant, accessible, and understandable to the intended reader; and supported by a controls framework that considers legislative, regulatory, contractual requirements, and other policy-related requirements. 2.4. New Relic will maintain written policies and procedures to review, test, and approve (as appropriate) changes affecting New Relic infrastructure and systems that process Customer Data including, but not limited to, peer reviews prior to introducing new code into production. New Relic will establish acceptance criteria for new information systems, upgrades, and new versions and will carry out suitable tests of the system(s) during development and prior to acceptance. Any changes or updates will not materially decrease the security of the systems. 3.Security Certifications 3.1. New Relic regularly tests, assesses, and evaluates its security measures for protecting Customer Data using industry-recognized standards and uses independent third-party auditors to verify such controls. 3.2. New Relic agrees to provide Customer, upon request, with applicable certifications or reports about New Relic systems. All information exchanged in connection with the audit activities described in this section is deemed to be the Confidential Information of New Relic. 3.3. Additional information about New Relic’s security certifications are available on New Relic’s Security Guide. 4. Data Control and Encryption 4.1. The Service and related features are designed to provide Customer with control over Customer’s data sources and Customer’s environments that are monitored and sending data to New Relic. 4.2. New Relic will have established methods to: (i) enable Customer to use encryption on Customer Data in transit, and (ii) securely hash passwords in storage following industry standard practices (e.g. scrypt) as described in the Documentation. Server certificate-based authentication is used as part of the TLS encryption with a trusted certificate authority. 4.3. New Relic receives and processes data in accordance with the Agreement and the Services as described in the Documentation. New Relic permits customers to delete personal data in accordance with applicable privacy laws as further described in the Documentation. In the event of error in personal data sent in Customer Data Customers may request personal data deletion and re-send data that is accurate. 4.4. Additional information for Customer data control and encryption, including encryption of data at rest, is available on New Relic’s Security Guide. 5. Facilities 5.1. All physical locations that process Customer Data are co-locations and third party data centers. All physical locations have security measures in place that are designed to prevent unauthorized physical access to data processing facilities and unlawful access, modification, or destruction of Customer Data. 5.2. New Relic will seek assurances (e.g., in the form of an independent 3rd party audit report such as the SOC 2 Type 2, ISO 27001, and vendor security evaluations) from its data processing facilities vendors that store or process Customer Data: 5.2.1. secure its data process facilities in an access-controlled location and have protections in place to prevent unauthorized access, damage, and interference; 5.2.2 employ physical security appropriate to the classification of the assets and information being managed which may include card key access, security cameras, and solid wall construction for all exterior walls; 5.2.3 limit and screen all entrants, which may include measures such as on-site security guard, badge reader, electronic lock, or a monitored closed caption television (CCTV); and 5.2.4 maintain relevant access logs. 5.3. Additional information about New Relic’s third party data centers are available on New Relic’s Security Guide. 6. Employee Access, Screening and Controls. New Relic will have and maintain policies and practices that include, at a minimum, the following controls and safeguards applied to New Relic employees and contractors who may access Customer Data: 6.1. For U.S. New Relic employees and subject to applicable law, a criminal background screening of each of its new employees to whom it gives access to Customer Data at the federal, state, and county levels. For non-U.S. New Relic employees, New Relic will use commercially reasonable efforts to meet the same criteria as established for U.S.-based New Relic employees, subject to general business practices in the respective country and in compliance with applicable local law requirements; 6.2. All New Relic employees with access to Customer Data will undergo adequate training, such as annual security awareness training, in the care, protection and handling of Customer Data, and will align with the privacy and security measures set out in this Addendum by following the guidance provided in their welcome package, which includes New Relic’s security policies and a security acknowledgement; 6.3. A disciplinary policy and process, to be used when New Relic employees violate New Relic security or privacy policies or access Customer Data without prior authorization; 6.4. Administrative or remote access to New Relic systems that process Customer Data will align with industry standard practices, including multi-factor authentication; 6.5. Restricted access to New Relic proprietary source code to prevent unauthorized access; 6.6. Controls designed to ensure that only those New Relic employees with an actual need-to-know will have access to New Relic systems including, but not limited to, the use of a formal access management process for the request, review, approval, and provisioning; 6.7. Controls designed to ensure that New Relic employees are granted access to New Relic systems based on least-privilege principles; and 6.8. Revoke access to New Relic employees no longer requiring access. 7. Security Incident and Data Breach Response 7.1. New Relic will take appropriate physical, technical, and administrative security measures that are commercially reasonable and consistent with industry standards to prevent a Data Breach, and as required by any applicable law or regulation. “Data Breach” means the theft, loss, or unauthorized access of Customer Data. Without limiting the foregoing, New Relic will implement security measures at least as stringent as those set out in this Addendum. New Relic will designate a senior representative to provide incident briefings, as needed in case of a Data Breach, and to respond to reasonable requests by Customer pertaining to privacy and data security issues within a commercially reasonable time frame. 7.2. New Relic will: 7.2.1. Notify Customer without undue delay if New Relic becomes aware of a Data Breach; 7.2.2. Maintain a security incident response plan, including procedures and means to respond in a manner consistent with industry standards; 7.2.3. Reasonably cooperate with Customer to investigate and remediate a Data Breach and mitigate any further risk to the Customer Data, or risk to data subjects and/or Customer’s reputation or brand; 7.2.4. Provide reasonable assistance to Customer at New Relic’s sole cost and expense; and 7.2.5. Make commercially reasonable efforts to preserve evidence and reasonably cooperate with Customer and legal authorities (as applicable and legally permissible) during an investigation of a Data Breach. 8. Network and Systems Security 8.1. All extranet connectivity by New Relic personnel to systems processing Customer Data will be through secure remote connections. 8.2. Network segments connected to the Internet will be protected by secure access control mechanisms, such as a firewall configured to secure all devices behind it and properly addresses security concerns according to industry standard practices. 8.3. New Relic will have industry standard measures in place to actively monitor its systems and help detect a potential hostile attack, such as Network Intrusion Detection (NID) or Host Intrusion Detection (HID)/Prevention. 8.4. Applications, ports, services, and similar access points installed on a computer or network facility, which are not specifically required for business functionality, will be disabled or removed. 8.5. New Relic maintains configuration standards for authorized operating systems and software for systems that support processing of Customer Data. New Relic will establish and follow server configuration guidelines and processes for preventing unauthorized access to Customer Data. New Relic maintains secure images or templates for systems based on the organization’s approved configuration standards. 8.6. Development, test, and operational environments will be logically separated to reduce the risks of unauthorized access or changes to the operational system. 8.7. New Relic network architecture will be designed to limit site access and restrict the availability of information systems that are considered to be vulnerable to attack. 8.8. New Relic provides appropriate TLS certificates when users access and view Customer Data in the Service. New Relic’s software for sending Customer Data to New Relic will encrypt Customer Data in transit by default. 9. Authentication and Access Management If you subscribe to the requisite New Relic One Service, New Relic will provide industry standard authentication and access controls to protect Customer Data, including industry standard authentication methods utilized to help prevent unauthorized access to the Service. New Relic’s access control methods will clearly state the rules and rights for each user or group of users including applications and information sharing and will include a process for granting and removing access to all information systems processing Customer Data. A record of all privileges allocated will be maintained pursuant to the requirements herein. 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. 10. Vulnerability Management New Relic will have and maintain the following vulnerability management processes for all devices used to connect to the New Relic network and Services. 10.1. New Relic will align to industry standard practices for build out, minimization of services and secure configuration, in accordance with, industry-recognized minimum security baselines for the New Relic platform and subcontractor systems connected to the New Relic platform in relation to the provision of the Services. 10.2. New Relic will employ industry-recognized standards and tools to conduct frequent infrastructure vulnerability scanning to test New Relic’s network and infrastructure and application penetration testing to test the New Relic Services. New Relic applies “Medium”, “High” and “Critical” security patches for all components in production and development environments as soon as commercially possible in accordance with its vulnerability management protocol, and consistent with industry standard practices and standards; 10.3. New Relic will have processes in place designed to ensure adherence to industry standard security development practices for development and testing for all code, APIs, and applications deployed and implemented in support of the Service; 10.4. New Relic will have and maintain solutions to identify and prevent malicious attackers or code from accessing or compromising Customer Data or systems that process Customer Data. These include, but are not limited to, software that identifies and removes malware and detects attempted intrusions. New Relic will have a security event and incident monitoring system and supporting processes to notify appropriate personnel in response to threats. 11. System Access and Logging 11.1. Access to New Relic’s systems will not be granted to employees of New Relic unless they have been uniquely identified and have sufficient credentials. 11.2. Access to New Relic’s systems will be logged and retained for no less than 6 months to assist in investigations and access control monitoring, including, but not limited to, end user access and activities, and information security events. 11.3. New Relic agrees to provide Customer the capability to access log records relating to Customer Accounts and the New Relic systems that process Customer Data in the event of a Data Breach or if required in connection with a law enforcement request. 12. Disaster Recovery and Data Backup 12.1. New Relic will have plans designed to respond to loss of services, which are tested and reviewed at least annually. This plan will include documented policies and procedures to restore service in the event of a service failure. 12.2. New Relic will establish and follow backup and restore procedures for Customer Data. 12.3. New Relic business continuity plan identifies critical systems. Annual disaster recovery tests are performed to check and restore customer data in the event of an incident. 12.4. New Relic will provide Customer with redacted copies of its plan(s) and evidence of tests/reviews upon request, but not more frequently than once annually, and subject to confidentiality requirements. 12.5. Additional information for Customer Data control and encryption are available on New Relic’s Security Guide. 13. Copies and Removal 13.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of the Agreement for any reason: (a) New Relic will, and will cause its personnel, to cease all access and use of any Customer Data, and (b) New Relic will delete all copies of Customer Data within ninety (90) days. 13.2. New Relic will maintain a process of ensuring secure destruction and deletion of all Customer Data, when reasonably requested by Customer or as otherwise provided in the Agreement. The process will include industry standard processes so that: (i) Customer Data cannot be practicably read or reconstructed, and (ii) the systems that store Customer Data are securely erased and/or decommissioned disks are destroyed. 14. Third Party Vendor Management New Relic may use third party vendors to provide the Services. New Relic performs a security risk-based assessment of prospective vendors before working with them to validate they meet New Relic’s security and business continuity standards, including the type of access and classification of data being accessed (if any), controls necessary to protect data, and legal/regulatory requirements. New Relic enters into written agreements with its vendors that process Customer Data which include confidentiality, privacy, and security obligations that provide an appropriate level of protection for Customer Data that these vendors may process for New Relic to maintain the security posture in this Policy, including following industry security standards. 15. Disclosure by Law In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic will (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. New Relic publishes its law enforcement requests report on New Relic’s Security Guide. 16. Updates As New Relic releases new products, services, functionality, and features, New Relic may update this Policy to account for such products, services, functionality, and features. For additional information, see our Security Guide.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 210.48322,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Security <em>policy</em>",
        "sections": "Security <em>policy</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;<em>referenced</em>-<em>policies</em>&#x2F;security-policy&#x2F;. 2. Security Overview 2.1. New Relic"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    }
  ],
  "/docs/licenses/license-information/referenced-policies/security-guide": [
    {
      "sections": [
        "Service level availability commitment",
        "New Relic Service Level Availability"
      ],
      "title": "Service level availability commitment",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "5489f268010887c72446f12059a1a0e4279ad960",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/service-level-availability-commitment/",
      "published_at": "2022-01-08T13:09:37Z",
      "updated_at": "2021-10-24T17:41:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you subscribe to \"New Relic One - Pro Users\" or \"New Relic One - Enterprise Users\", the service level availability commitments available to you are those set forth (i) in the Terms, or (ii) as set forth on this page. If you subscribe to \"New Relic One - Standard Users\", any service level availability commitment or related remedies contained in your Terms are vacated and nullified, and New Relic will use commercially reasonable efforts to make \"New Relic One - Standard Users\" available in line with industry standards. If you subscribe to any other New Relic Products on a product-based pricing basis, any service level availability commitment are contained in (i) your Terms, or (ii) if your Terms explicitly references this Service Level Availability commitment applying to the Service purchased in an Order, this page for the Services you use. Capitalized terms not defined below shall take on the meaning set forth in your Terms. New Relic Service Level Availability The Service will be considered available so long as Customer is able to log in to its interface and view Customer Data (\"Service Availability\"). The applicable Service Availability will be calculated as a percentage of: (1) the total number of minutes in a month after (2) subtracting any periods of unavailability during such month from the total number of minutes in a month. New Relic will use commercially reasonable efforts to maintain Service Availability of at least 99.8% during any calendar month. In the event the Service Availability drops below: (i) 98.5% for two consecutive calendar months during the Subscription Term, or (ii) 96.5% in any single calendar month, Customer may request to terminate the relevant Service with no penalty. Such termination will be effective as of the end of the then-current billing period and no additional fees will be charged. The service level within New Relic's control is the Service Availability, not, for example, the transmission of data over the public Internet. Service Availability calculations will exclude unavailability arising from any: (a) planned maintenance periods; (b) emergency maintenance that is necessary to prevent imminent harm to the Service; (c) force majeure events; (d) Third-Party Services, Customer application, equipment, software or other technology, or Customer or its User's use of the Service, in violation of the Agreement or not in accordance with the Documentation; or (e) suspension, limitation, and/or termination of Customer’s access or use of the Service in accordance with this Agreement. This describes Customer’s sole and exclusive remedy for failures of Service Availability. Customer may request the Service Availability attainment for the previous month by filing a support ticket on the New Relic support site.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 216.52391,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>"
      },
      "id": "603ea44828ccbcd9c8eba7b7"
    },
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Purpose",
        "2. Security Overview",
        "3.Security Certifications",
        "4. Data Control and Encryption",
        "5. Facilities",
        "6. Employee Access, Screening and Controls.",
        "7. Security Incident and Data Breach Response",
        "8. Network and Systems Security",
        "9. Authentication and Access Management",
        "10. Vulnerability Management",
        "11. System Access and Logging",
        "12. Disaster Recovery and Data Backup",
        "13. Copies and Removal",
        "14. Third Party Vendor Management",
        "15. Disclosure by Law",
        "16. Updates"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2022-01-08T13:08:54Z",
      "updated_at": "2021-09-27T15:05:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Updated 23 September 2021. The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Purpose This Policy describes New Relic's security program and the technical and organizational security controls to protect New Relic’s systems as shown in New Relic’s third-party audits and certifications. New Relic may update this Policy from time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/. 2. Security Overview 2.1. New Relic maintains a comprehensive Information Protection Program to manage information security within New Relic that includes administrative, technical, and physical safeguards designed to protect the confidentiality, integrity, and availability of Customer Data and that is appropriate to the nature, size, and complexity of New Relic's business operations. New Relic’s Information Protection Program includes: 2.1.1. executive review, support and accountability for all security related policies and practices; 2.1.2. formal written policies and procedures that are designed to protect against the loss, theft, or other unauthorized access or alteration of Customer Data, meet or exceed applicable industry standards, outlines a definition of information security and its overall objectives, include defined information security roles and responsibilities, a framework for setting control objectives and controls, a formal and effective risk mitigation program and a service provider security management program; 2.1.3. periodic risk assessments, including internal audits and/or independent 3rd party audits, to measure the effectiveness and appropriateness of controls for all systems processing Customer Data; 2.1.4. in-depth review of security incidents, including effective determination of root cause and corrective action; formal, industry-recognized controls frameworks based on an external audit standard; 2.1.5. employee screening and access management; and 2.1.6. comprehensive security testing, vulnerability identification, and mitigation methodology that consists of a variety of independent approaches that, when combined, are designed to maximize coverage for a diverse set of attack vectors. 2.2. New Relic's Chief Information Security Officer (CISO) leads New Relic's Information Security Program and develops, reviews, and approves, with appropriate stakeholders, New Relic's security policies and procedures. New Relic has appointed data protection officer(s) as described in New Relic’s Privacy Notices who are consulted as necessary under applicable laws. 2.3. Information security objectives, approach, scope, importance, goals, and principles for the organization’s security program are formally identified, communicated throughout the organization to users in a form that is relevant, accessible, and understandable to the intended reader; and supported by a controls framework that considers legislative, regulatory, contractual requirements, and other policy-related requirements. 2.4. New Relic will maintain written policies and procedures to review, test, and approve (as appropriate) changes affecting New Relic infrastructure and systems that process Customer Data including, but not limited to, peer reviews prior to introducing new code into production. New Relic will establish acceptance criteria for new information systems, upgrades, and new versions and will carry out suitable tests of the system(s) during development and prior to acceptance. Any changes or updates will not materially decrease the security of the systems. 3.Security Certifications 3.1. New Relic regularly tests, assesses, and evaluates its security measures for protecting Customer Data using industry-recognized standards and uses independent third-party auditors to verify such controls. 3.2. New Relic agrees to provide Customer, upon request, with applicable certifications or reports about New Relic systems. All information exchanged in connection with the audit activities described in this section is deemed to be the Confidential Information of New Relic. 3.3. Additional information about New Relic’s security certifications are available on New Relic’s Security Guide. 4. Data Control and Encryption 4.1. The Service and related features are designed to provide Customer with control over Customer’s data sources and Customer’s environments that are monitored and sending data to New Relic. 4.2. New Relic will have established methods to: (i) enable Customer to use encryption on Customer Data in transit, and (ii) securely hash passwords in storage following industry standard practices (e.g. scrypt) as described in the Documentation. Server certificate-based authentication is used as part of the TLS encryption with a trusted certificate authority. 4.3. New Relic receives and processes data in accordance with the Agreement and the Services as described in the Documentation. New Relic permits customers to delete personal data in accordance with applicable privacy laws as further described in the Documentation. In the event of error in personal data sent in Customer Data Customers may request personal data deletion and re-send data that is accurate. 4.4. Additional information for Customer data control and encryption, including encryption of data at rest, is available on New Relic’s Security Guide. 5. Facilities 5.1. All physical locations that process Customer Data are co-locations and third party data centers. All physical locations have security measures in place that are designed to prevent unauthorized physical access to data processing facilities and unlawful access, modification, or destruction of Customer Data. 5.2. New Relic will seek assurances (e.g., in the form of an independent 3rd party audit report such as the SOC 2 Type 2, ISO 27001, and vendor security evaluations) from its data processing facilities vendors that store or process Customer Data: 5.2.1. secure its data process facilities in an access-controlled location and have protections in place to prevent unauthorized access, damage, and interference; 5.2.2 employ physical security appropriate to the classification of the assets and information being managed which may include card key access, security cameras, and solid wall construction for all exterior walls; 5.2.3 limit and screen all entrants, which may include measures such as on-site security guard, badge reader, electronic lock, or a monitored closed caption television (CCTV); and 5.2.4 maintain relevant access logs. 5.3. Additional information about New Relic’s third party data centers are available on New Relic’s Security Guide. 6. Employee Access, Screening and Controls. New Relic will have and maintain policies and practices that include, at a minimum, the following controls and safeguards applied to New Relic employees and contractors who may access Customer Data: 6.1. For U.S. New Relic employees and subject to applicable law, a criminal background screening of each of its new employees to whom it gives access to Customer Data at the federal, state, and county levels. For non-U.S. New Relic employees, New Relic will use commercially reasonable efforts to meet the same criteria as established for U.S.-based New Relic employees, subject to general business practices in the respective country and in compliance with applicable local law requirements; 6.2. All New Relic employees with access to Customer Data will undergo adequate training, such as annual security awareness training, in the care, protection and handling of Customer Data, and will align with the privacy and security measures set out in this Addendum by following the guidance provided in their welcome package, which includes New Relic’s security policies and a security acknowledgement; 6.3. A disciplinary policy and process, to be used when New Relic employees violate New Relic security or privacy policies or access Customer Data without prior authorization; 6.4. Administrative or remote access to New Relic systems that process Customer Data will align with industry standard practices, including multi-factor authentication; 6.5. Restricted access to New Relic proprietary source code to prevent unauthorized access; 6.6. Controls designed to ensure that only those New Relic employees with an actual need-to-know will have access to New Relic systems including, but not limited to, the use of a formal access management process for the request, review, approval, and provisioning; 6.7. Controls designed to ensure that New Relic employees are granted access to New Relic systems based on least-privilege principles; and 6.8. Revoke access to New Relic employees no longer requiring access. 7. Security Incident and Data Breach Response 7.1. New Relic will take appropriate physical, technical, and administrative security measures that are commercially reasonable and consistent with industry standards to prevent a Data Breach, and as required by any applicable law or regulation. “Data Breach” means the theft, loss, or unauthorized access of Customer Data. Without limiting the foregoing, New Relic will implement security measures at least as stringent as those set out in this Addendum. New Relic will designate a senior representative to provide incident briefings, as needed in case of a Data Breach, and to respond to reasonable requests by Customer pertaining to privacy and data security issues within a commercially reasonable time frame. 7.2. New Relic will: 7.2.1. Notify Customer without undue delay if New Relic becomes aware of a Data Breach; 7.2.2. Maintain a security incident response plan, including procedures and means to respond in a manner consistent with industry standards; 7.2.3. Reasonably cooperate with Customer to investigate and remediate a Data Breach and mitigate any further risk to the Customer Data, or risk to data subjects and/or Customer’s reputation or brand; 7.2.4. Provide reasonable assistance to Customer at New Relic’s sole cost and expense; and 7.2.5. Make commercially reasonable efforts to preserve evidence and reasonably cooperate with Customer and legal authorities (as applicable and legally permissible) during an investigation of a Data Breach. 8. Network and Systems Security 8.1. All extranet connectivity by New Relic personnel to systems processing Customer Data will be through secure remote connections. 8.2. Network segments connected to the Internet will be protected by secure access control mechanisms, such as a firewall configured to secure all devices behind it and properly addresses security concerns according to industry standard practices. 8.3. New Relic will have industry standard measures in place to actively monitor its systems and help detect a potential hostile attack, such as Network Intrusion Detection (NID) or Host Intrusion Detection (HID)/Prevention. 8.4. Applications, ports, services, and similar access points installed on a computer or network facility, which are not specifically required for business functionality, will be disabled or removed. 8.5. New Relic maintains configuration standards for authorized operating systems and software for systems that support processing of Customer Data. New Relic will establish and follow server configuration guidelines and processes for preventing unauthorized access to Customer Data. New Relic maintains secure images or templates for systems based on the organization’s approved configuration standards. 8.6. Development, test, and operational environments will be logically separated to reduce the risks of unauthorized access or changes to the operational system. 8.7. New Relic network architecture will be designed to limit site access and restrict the availability of information systems that are considered to be vulnerable to attack. 8.8. New Relic provides appropriate TLS certificates when users access and view Customer Data in the Service. New Relic’s software for sending Customer Data to New Relic will encrypt Customer Data in transit by default. 9. Authentication and Access Management If you subscribe to the requisite New Relic One Service, New Relic will provide industry standard authentication and access controls to protect Customer Data, including industry standard authentication methods utilized to help prevent unauthorized access to the Service. New Relic’s access control methods will clearly state the rules and rights for each user or group of users including applications and information sharing and will include a process for granting and removing access to all information systems processing Customer Data. A record of all privileges allocated will be maintained pursuant to the requirements herein. 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. 10. Vulnerability Management New Relic will have and maintain the following vulnerability management processes for all devices used to connect to the New Relic network and Services. 10.1. New Relic will align to industry standard practices for build out, minimization of services and secure configuration, in accordance with, industry-recognized minimum security baselines for the New Relic platform and subcontractor systems connected to the New Relic platform in relation to the provision of the Services. 10.2. New Relic will employ industry-recognized standards and tools to conduct frequent infrastructure vulnerability scanning to test New Relic’s network and infrastructure and application penetration testing to test the New Relic Services. New Relic applies “Medium”, “High” and “Critical” security patches for all components in production and development environments as soon as commercially possible in accordance with its vulnerability management protocol, and consistent with industry standard practices and standards; 10.3. New Relic will have processes in place designed to ensure adherence to industry standard security development practices for development and testing for all code, APIs, and applications deployed and implemented in support of the Service; 10.4. New Relic will have and maintain solutions to identify and prevent malicious attackers or code from accessing or compromising Customer Data or systems that process Customer Data. These include, but are not limited to, software that identifies and removes malware and detects attempted intrusions. New Relic will have a security event and incident monitoring system and supporting processes to notify appropriate personnel in response to threats. 11. System Access and Logging 11.1. Access to New Relic’s systems will not be granted to employees of New Relic unless they have been uniquely identified and have sufficient credentials. 11.2. Access to New Relic’s systems will be logged and retained for no less than 6 months to assist in investigations and access control monitoring, including, but not limited to, end user access and activities, and information security events. 11.3. New Relic agrees to provide Customer the capability to access log records relating to Customer Accounts and the New Relic systems that process Customer Data in the event of a Data Breach or if required in connection with a law enforcement request. 12. Disaster Recovery and Data Backup 12.1. New Relic will have plans designed to respond to loss of services, which are tested and reviewed at least annually. This plan will include documented policies and procedures to restore service in the event of a service failure. 12.2. New Relic will establish and follow backup and restore procedures for Customer Data. 12.3. New Relic business continuity plan identifies critical systems. Annual disaster recovery tests are performed to check and restore customer data in the event of an incident. 12.4. New Relic will provide Customer with redacted copies of its plan(s) and evidence of tests/reviews upon request, but not more frequently than once annually, and subject to confidentiality requirements. 12.5. Additional information for Customer Data control and encryption are available on New Relic’s Security Guide. 13. Copies and Removal 13.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of the Agreement for any reason: (a) New Relic will, and will cause its personnel, to cease all access and use of any Customer Data, and (b) New Relic will delete all copies of Customer Data within ninety (90) days. 13.2. New Relic will maintain a process of ensuring secure destruction and deletion of all Customer Data, when reasonably requested by Customer or as otherwise provided in the Agreement. The process will include industry standard processes so that: (i) Customer Data cannot be practicably read or reconstructed, and (ii) the systems that store Customer Data are securely erased and/or decommissioned disks are destroyed. 14. Third Party Vendor Management New Relic may use third party vendors to provide the Services. New Relic performs a security risk-based assessment of prospective vendors before working with them to validate they meet New Relic’s security and business continuity standards, including the type of access and classification of data being accessed (if any), controls necessary to protect data, and legal/regulatory requirements. New Relic enters into written agreements with its vendors that process Customer Data which include confidentiality, privacy, and security obligations that provide an appropriate level of protection for Customer Data that these vendors may process for New Relic to maintain the security posture in this Policy, including following industry security standards. 15. Disclosure by Law In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic will (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. New Relic publishes its law enforcement requests report on New Relic’s Security Guide. 16. Updates As New Relic releases new products, services, functionality, and features, New Relic may update this Policy to account for such products, services, functionality, and features. For additional information, see our Security Guide.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 210.48322,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Security <em>policy</em>",
        "sections": "Security <em>policy</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;<em>referenced</em>-<em>policies</em>&#x2F;security-policy&#x2F;. 2. Security Overview 2.1. New Relic"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    },
    {
      "sections": [
        "New Relic Pre-release policy",
        "New Relic Pre-Release Policy",
        "1. Introduction",
        "2. License and Restrictions",
        "3. Confidential Information",
        "4. Disclaimers and Acknowledgement",
        "5. Limitation of Liability",
        "6. Indemnity",
        "7. Export Restrictions",
        "8. Termination"
      ],
      "title": "New Relic Pre-release policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "5ca324e942906913fc346245cfd80d405e90ad1f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/new-relic-pre-release-policy/",
      "published_at": "2022-01-08T13:08:54Z",
      "updated_at": "2021-03-13T01:17:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Without overriding any express provisions that you may have agreed to in a separate written beta or pre-release agreement executed by you and New Relic, the following terms apply to your use of any New Relic Pre-Release Service(s). New Relic Pre-Release Policy Through your use of any New Relic, Inc. (“New Relic”) Pre-Release Services (as further defined), you signify that you have read, understood, and accept the terms of this Pre-Release Policy (the “Policy”) governing your use and participation in New Relic’s program for making beta, alpha and pre-release versions of New Relic software (i.e. agents, SDKs, APIs, integrations, private locations, code, etc.), services, features, user-interfaces, and platforms made available for evaluation purposes (the “Pre-Release Service(s)”). This Policy is effective on behalf of yourself or the entity(ies) associated with your New Relic account (collectively referred to herein as “you” or “your”) as of your date of first use of the Pre-Release Service(s) (the “Effective Date”), which means that your choice to use the Pre-Release Service(s) means that you have the legal authority to use the Pre-Release Service(s) personally or on behalf of the company or organization associated with your New Relic account in accordance with this Policy. Please read this Policy carefully as your use of Pre-Release Service(s) may have unintended consequences and materially impact your data or use of the New Relic services and products. If you do not agree to this Policy, your sole remedy is to not make use of the Pre-Release Services. 1. Introduction This Policy and any New Relic technical guides and documentation made available with the Pre-Release Service(s) or from the dedicated ‘Documentation’ page of the New Relic website (the “Documentation”) enables you to test experimental features and products before they are made generally available. New Relic reserves the right to withhold or discontinue any Pre-Release Service(s), which may be designated as an alpha, beta, pilot, limited release, developer preview, technology preview, non-production, evaluation, or by a similar description, in its sole discretion and provides no guarantee that any Pre-Release Service(s) will eventually be made commercially or generally available. Pre-Release Service(s) are versions provided before they are generally available and may have bugs, stability issues, or other problems. Use of the Pre-Release Service(s) may result in unexpected results, loss of data, outages, or other damage to your systems or networks. New Relic advises against using the Pre-Release Service(s) in production. No support is offered with or for Pre-Release Service(s). You hereby release New Relic from any liability arising from or related to your use of the Pre-Release Service(s). In connection with your use of the Pre-Release Services, you may provide feedback, comments, and/or suggestions based on or relating to the Pre-Release Services (“Feedback”). The Pre-Release Services and any related information, including without limitation features, functionality, designs, user interface, capabilities, specifications, architectural diagrams, usage data, APIs, deployment schedules, email lists, bug databases, know how, source code, potential features, Feedback, and information related to any business or technical information of New Relic, including without limitation product plans, costs, prices, finances, marketing plans, business opportunities, research, development, and negotiations are considered confidential information of New Relic (“Confidential Information”). 2. License and Restrictions 2.1 Evaluation License. Subject to your compliance with the Policy, New Relic hereby grants you a limited, non-exclusive, non-transferable, revocable license, during the term of this Policy, to use the Pre-Release Service(s) in accordance with the Documentation solely for the purpose of internal evaluation and supplying Feedback to New Relic. 2.2 License Restrictions. You may not (and may not authorize or enable a third party to): (i) reverse engineer, decompile, disassemble, or otherwise attempt to discover the source code or other trade secrets in the Pre-Release Service(s); (ii) access the Pre-Release Service(s) in order to build a similar or competitive application, service, feature or other competitive purpose; (iii) remove or destroy any copyright notices or other proprietary marks contained on or in the Pre-Release Service(s); (iv) use the Pre-Release Service(s) for any benchmarking purposes or in connection with a service bureau, timeshare, service provider, or like activity where you operate the Pre-Release Service(s) on behalf of a third party; (v) mirror, frame, copy, modify, host, rent, lease, sell, commercialize, sublicense, assign or otherwise transfer the Pre-Release Service(s), or the access or use of the Pre-Release Service(s); (vi) use the Pre-Release Service(s) in a manner that may violate or infringe the intellectual property, data protection, or other proprietary right of a third party; (vii) use or access the Pre-Release Service(s) in an unauthorized manner and/or in a manner that violates any applicable law, rule, contract, or guideline (including but not limited to our Documentation or Community Guidelines); (viii) use the Pre-Release Service(s) to transmit worms, viruses, malicious code, security vulnerabilities, or otherwise negatively impact network operations, third parties, or New Relic; (ix) use any data mining or similar data gathering and extraction methods in connection with the Pre-Release Service(s); or (x) use the Pre-Release Service(s) to process sensitive personal information, e.g., “personal information” of children as defined by the Children’s Online Privacy Protection Act, “protected health information” as defined by the Health Insurance Portability and Accountability Act of 1996, government issued identification numbers, financial account information, payment card data, “special categories of data” as described in the EU General Data Protection Regulation (GDPR), or other information subject to regulatory, statutory, or contractual restrictions. New Relic reserves the right, but not the obligation, to monitor or review your use of the Pre-Release Service(s) at any time and may investigate any suspected violations of this Policy. 2.3 Intellectual Property Rights. You acknowledge and agree that, as between you and New Relic, New Relic owns all right, title, and interest in and to the Pre-Release Service(s), Confidential Information, and Feedback, including but not limited to all intellectual property and proprietary rights therein. New Relic and its licensors reserve all rights and licenses not expressly granted herein. 2.4 Feedback. You acknowledge and agree that all Feedback will be the sole and exclusive property of New Relic. You hereby irrevocably transfer and assign to New Relic all right, title, and interest in all Feedback, including but not limited to all intellectual property and proprietary rights therein. 3. Confidential Information You will not use or disclose any Confidential Information, except as necessary for the performance of this Policy, and you will use reasonable efforts to protect Confidential Information from unauthorized use or disclosure. You may disclose Confidential Information only to those employees with a bona fide need to know, provided that each employee has signed a written agreement with nondisclosure restrictions at least as protective of the Confidential Information as those set forth herein. For the purposes of this section, information will not be deemed Confidential Information if it: (a) is or becomes generally known to the public through no fault or breach of this Policy; (b) is rightfully known by you at the time of disclosure without an obligation of confidentiality; (c) is independently developed by you without access to or use of the Confidential Information; or (d) is rightfully obtained by you from a third party without restriction on use or disclosure. Upon New Relic’s request, you agree to destroy all Confidential Information in your possession within 30 days of termination of this Policy. 4. Disclaimers and Acknowledgement 4.1 Acknowledgement of Pre-Release Service(s). You acknowledge and agree that: (a) the Pre-Release Service(s) is not an official product and has not been commercially released for sale by New Relic; (b) the Pre-Release Service(s) may not operate properly, be in final form, or fully function; (c) the Pre-Release Service(s) may contain errors, design flaws, security vulnerabilities, or other problems; (d) it may not be possible to make the Pre-Release Service(s) fully functional or secure; (e) the information obtained using the Pre-Release Service(s) may not be accurate; (f) use of the Pre-Release Service(s) may result in unexpected results, vulnerabilities, loss of data, project delays, or other unpredictable damage or loss, including without limitation to your use of New Relic services and products not governed by this Policy (collectively “Unintended Effects”); (g) New Relic is under no obligation to release a commercial or generally available version of the Pre-Release Service(s); and (h) New Relic has the right unilaterally to abandon development of the Pre-Release Service(s) at any time and without any obligation or liability to you. 4.2 Your Data. You acknowledge and agree that you should not rely on the Pre-Release Service(s) for any reason. You further acknowledge and agree that you are solely responsible for maintaining and protecting all data and information that is stored, retrieved, or otherwise processed by the Pre-Release Service(s) in accordance with your contractual obligations and requirements under applicable law. Without limiting the foregoing, you are responsible for all costs and expenses required to backup and restore any data and information that is lost or corrupted as a result of your use of the Pre-Release Service(s) or any Unintended Effects. You understand and agree that your Data may be transferred to the United States for storage, processing, and use by New Relic to provide the Pre-Release Service(s). For any data that you process using the Pre-Release Service(s), you represent and warrant that you have all necessary rights and consent to do so. New Relic has no obligation to store any data you process using the Pre-Release Service(s) and shall have no liability for the deletion or accuracy of such data. 4.3 Warranty Disclaimers. YOU ACKNOWLEDGE THAT PRE-RELEASE SERVICE(S) ARE PROVIDED “AS IS.” TO THE EXTENT PERMITTED BY APPLICABLE LAW, NEW RELIC DISCLAIMS ALL WARRANTIES, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO ANY WARRANTIES REGARDING MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, SECURITY, AND ANY WARRANTIES ARISING OUT OF THE COURSE OF DEALING OR USAGE OF TRADE. NEW RELIC MAKES NO WARRANTY THAT PRE-RELEASE SERVICE(S) WILL MEET YOUR REQUIREMENTS OR WILL BE ACCURATE, RELIABLE, ERROR-FREE, UNINTERRUPTED, TIMELY, OR SECURE. YOUR USE OF PRE-RELEASE SERVICE(S) IS AT YOUR OWN RISK. 4.4 Modifications. New Relic may change the Policy from time to time as our business changes and technology evolves, and future versions of our generally available services may not be compatible with Pre-Release Service(s) built using previous versions. The most current version of this Policy will be posted in the Documentation. Any changes to this Policy will be effective immediately for all users in instances to comply with applicable law, for new users of a Pre-Release Service(s) and, for all other users, any changes to this Policy will be effective as of fifteen (15) days after posting notice of such changes. If we determine in our sole discretion that an update is material, we will provide notice of such material change to you through your New Relic account, the Pre-Release Service(s), the Documentation, our blogs or community forums, and/or by email to the email address of your account administrator. We may require you to provide consent to the updated Policy in a specified manner before further use of the Pre-Release Service(s) is permitted. If you do not agree to any change(s), your sole remedy is to stop using the Pre-Release Service(s) and any such termination by you shall be without penalty except as specified in this Policy. Otherwise, your use of any Pre-Release Service(s) after any such update shall constitute acceptance of the then-current Policy. 5. Limitation of Liability TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC, ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE FOR ANY INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, EXEMPLARY OR CONSEQUENTIAL (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) DAMAGES, REGARDLESS OF LEGAL THEORY, WHETHER OR NOT NEW RELIC HAS BEEN WARNED OF THE POSSIBILITY OF SUCH DAMAGES, AND EVEN IF A REMEDY FAILS OF ITS ESSENTIAL PURPOSE. NEW RELIC’S AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS POLICY, THE PRE-RELEASE SERVICES, OR ANY UNINTENDED EFFECTS WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE LIMITATIONS OF DAMAGES SET FORTH ABOVE ARE FUNDAMENTAL ELEMENTS OF THE BASIS OF THE BARGAIN BETWEEN NEW RELIC AND YOU. 6. Indemnity You agree to indemnify and hold New Relic, its parents, subsidiaries, affiliates, officers, agents, employees, and licensors harmless from any claims, fees, fines, demands, losses, liabilities, damages, and costs, including reasonable attorney’s fees, arising from or related to your use of the Pre-Release Service(s), including but not limited to allegations arising from your breach of any terms herein and/or allegations that data processed by the Pre-Release Service(s) violates or infringes the privacy, data protection, or intellectual property rights of a third party. 7. Export Restrictions You acknowledge that the software in the Pre-Release Service(s) licensed hereunder may be subject to the export control laws and regulations of the U.S. and other countries. You agree that you will not export or re-export the Pre-Release Service(s), any part thereof, or any process or service that is the direct product of the Pre-Release Service(s) to any country, person or entity subject to U.S. export restrictions. 8. Termination Your access to the Pre-Release Service(s) will terminate upon the earliest of: (i) New Relic making the Pre-Release Service(s) or a successor version of the Pre-Release Service(s) generally available or available for commercial release; (ii) New Relic ceasing to make the Pre-Release Service(s) available to you; or (iii) New Relic terminating this Policy. New Relic may terminate this Policy for any reason upon notice to you. Sections 2.2, 2.3, 2.4, 3, 4, 5, 6, and 7 will survive your use of the Pre-Release Service(s).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 198.41896,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic Pre-release <em>policy</em>",
        "sections": "2. <em>License</em> and Restrictions",
        "tags": "<em>License</em> <em>information</em>",
        "body": " Services and any related <em>information</em>, including without limitation features, functionality, designs, user interface, capabilities, specifications, architectural diagrams, usage data, APIs, deployment schedules, email lists, bug databases, know how, source code, potential features, Feedback"
      },
      "id": "6044e71b28ccbc984d2c60c8"
    }
  ],
  "/docs/licenses/license-information/referenced-policies/security-policy": [
    {
      "sections": [
        "Security guide",
        "Tip",
        "Security Program",
        "Security Domains",
        "Security Certifications",
        "Data Control, Facilities, and Encryption",
        "Law Enforcement Request Report"
      ],
      "title": "Security guide",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "356f0d11ffcb62208a743a0a7c127f5f6da9c940",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-guide/",
      "published_at": "2022-01-08T12:36:19Z",
      "updated_at": "2021-12-09T15:28:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Last updated September 17, 2021. This is supplement to our security policy and serves as a guide to New Relic’s description of its Services, functionalities, and features. Tip We may update the URLs in this document without notice. Security Program New Relic follows \"privacy by design\" principles as described here: https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Security Domains New Relic’s policies and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless Security; Configuration Management; Vulnerability Management; Network Protection; Transmission Protection; Password Management; Access Control, Audit Logging & Monitoring; Education, Training, and Awareness; Third Party Assurance; Incident Management; Business Continuity and Disaster Recover; Risk Management; Data Protection & Privacy; and Service Management Systems. Security Certifications New Relic audits its Services against industry standards as described at https://docs.newrelic.com/docs/security/security-privacy/compliance/regulatory-audits-new-relic-services/. Data Control, Facilities, and Encryption New Relic's customers can send data to New Relic's APIs by (1) using New Relic's software, (2) using vendor-neutral software that is managed and maintained by a third-party such as via OpenTelemetry instrumentation provided by opentelemetry.io, or (3) from third-party systems that customer's manage and/or control. New Relic's customers can use New Relic's Services such as NerdGraph to filter out and drop data. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/drop-data-using-nerdgraph/. New Relic's customers can adjust their data retention periods as appropriate for their needs. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/manage-data-retention/#adjust-retention. New Relic Logs obfuscates numbers that match known patterns, such as bank card and social security numbers as described here: https://docs.newrelic.com/docs/logs/log-management/get-started/new-relics-log-management-security-privacy/. New Relic honors requests to delete personal data in accordance with applicable privacy laws. Please see https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Customers may use New Relic's APIs to query data, such as NerdGraph described here, and New Relic Services to export the data to other cloud providers. Customers can configure its log forwarder [https://docs.newrelic.com/docs/logs/enable-log-management-new-relic/enable-log-monitoring-new-relic/forward-your-logs-using-infrastructure-agent/] before sending infrastructure logs to New Relic. For New Relic Customers in New Relic US, FedRAMP and HIPAA-enabled environments, Customer Data is replicated to the off-site backup system via Amazon Simple Storage Service (S3). Category of Customer Description FedRAMP HIPAA-enabled US Gen Pop EU Gen Pop Data is stored in Amazon Web Services (“AWS”). Limited Data is stored in IBM Data for New Relic Incident Intelligence is stored in Google Cloud New Relic regularly tests, assess, and evaluates its measures to ensure the security of processing using industry-recognized standards and uses independent third-party auditors as provided below: Annual SOC 2 Type 2 Annual FedRAMP assessment by an independent third-party pursuant to NIST 800-53 rev 4 Moderate authorization. Annual HITRUST-validated assessment by an independent third-party *Pursuing CY2021 Q4 ISO 27001 TISAX The Services that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed information about AWS security is available at https://aws.amazon.com/security/ and http://aws.amazon.com/security/sharing-the-security-responsibility/. Data encryption at rest utilizes FIPS 140-2 compliant encryption methodology. For AWS SOC Reports, please see https://aws.amazon.com/compliance/soc-faqs/. The Services that operate on Google Cloud Platform (\"GCP\") are protected by the security and environmental controls of GCP. Detailed information about GCP security is available at https://cloud.google.com/docs/tutorials#security. For GCP reports, please see https://cloud.google.com/security/compliance/. IBM Deft Zayo QTS Law Enforcement Request Report New Relic has not to date received any request for customer data from a law enforcement or other government agency (including under any national security process), and has not made any corresponding disclosures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 246.06125,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " as described here: https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;security&#x2F;security-privacy&#x2F;data-privacy&#x2F;data-privacy-new-relic&#x2F;. Security Domains New Relic’s <em>policies</em> and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless"
      },
      "id": "6147558128ccbc973a56a863"
    },
    {
      "sections": [
        "Service level availability commitment",
        "New Relic Service Level Availability"
      ],
      "title": "Service level availability commitment",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "5489f268010887c72446f12059a1a0e4279ad960",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/service-level-availability-commitment/",
      "published_at": "2022-01-08T13:09:37Z",
      "updated_at": "2021-10-24T17:41:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you subscribe to \"New Relic One - Pro Users\" or \"New Relic One - Enterprise Users\", the service level availability commitments available to you are those set forth (i) in the Terms, or (ii) as set forth on this page. If you subscribe to \"New Relic One - Standard Users\", any service level availability commitment or related remedies contained in your Terms are vacated and nullified, and New Relic will use commercially reasonable efforts to make \"New Relic One - Standard Users\" available in line with industry standards. If you subscribe to any other New Relic Products on a product-based pricing basis, any service level availability commitment are contained in (i) your Terms, or (ii) if your Terms explicitly references this Service Level Availability commitment applying to the Service purchased in an Order, this page for the Services you use. Capitalized terms not defined below shall take on the meaning set forth in your Terms. New Relic Service Level Availability The Service will be considered available so long as Customer is able to log in to its interface and view Customer Data (\"Service Availability\"). The applicable Service Availability will be calculated as a percentage of: (1) the total number of minutes in a month after (2) subtracting any periods of unavailability during such month from the total number of minutes in a month. New Relic will use commercially reasonable efforts to maintain Service Availability of at least 99.8% during any calendar month. In the event the Service Availability drops below: (i) 98.5% for two consecutive calendar months during the Subscription Term, or (ii) 96.5% in any single calendar month, Customer may request to terminate the relevant Service with no penalty. Such termination will be effective as of the end of the then-current billing period and no additional fees will be charged. The service level within New Relic's control is the Service Availability, not, for example, the transmission of data over the public Internet. Service Availability calculations will exclude unavailability arising from any: (a) planned maintenance periods; (b) emergency maintenance that is necessary to prevent imminent harm to the Service; (c) force majeure events; (d) Third-Party Services, Customer application, equipment, software or other technology, or Customer or its User's use of the Service, in violation of the Agreement or not in accordance with the Documentation; or (e) suspension, limitation, and/or termination of Customer’s access or use of the Service in accordance with this Agreement. This describes Customer’s sole and exclusive remedy for failures of Service Availability. Customer may request the Service Availability attainment for the previous month by filing a support ticket on the New Relic support site.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 216.52391,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>"
      },
      "id": "603ea44828ccbcd9c8eba7b7"
    },
    {
      "sections": [
        "New Relic Pre-release policy",
        "New Relic Pre-Release Policy",
        "1. Introduction",
        "2. License and Restrictions",
        "3. Confidential Information",
        "4. Disclaimers and Acknowledgement",
        "5. Limitation of Liability",
        "6. Indemnity",
        "7. Export Restrictions",
        "8. Termination"
      ],
      "title": "New Relic Pre-release policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "5ca324e942906913fc346245cfd80d405e90ad1f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/new-relic-pre-release-policy/",
      "published_at": "2022-01-08T13:08:54Z",
      "updated_at": "2021-03-13T01:17:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Without overriding any express provisions that you may have agreed to in a separate written beta or pre-release agreement executed by you and New Relic, the following terms apply to your use of any New Relic Pre-Release Service(s). New Relic Pre-Release Policy Through your use of any New Relic, Inc. (“New Relic”) Pre-Release Services (as further defined), you signify that you have read, understood, and accept the terms of this Pre-Release Policy (the “Policy”) governing your use and participation in New Relic’s program for making beta, alpha and pre-release versions of New Relic software (i.e. agents, SDKs, APIs, integrations, private locations, code, etc.), services, features, user-interfaces, and platforms made available for evaluation purposes (the “Pre-Release Service(s)”). This Policy is effective on behalf of yourself or the entity(ies) associated with your New Relic account (collectively referred to herein as “you” or “your”) as of your date of first use of the Pre-Release Service(s) (the “Effective Date”), which means that your choice to use the Pre-Release Service(s) means that you have the legal authority to use the Pre-Release Service(s) personally or on behalf of the company or organization associated with your New Relic account in accordance with this Policy. Please read this Policy carefully as your use of Pre-Release Service(s) may have unintended consequences and materially impact your data or use of the New Relic services and products. If you do not agree to this Policy, your sole remedy is to not make use of the Pre-Release Services. 1. Introduction This Policy and any New Relic technical guides and documentation made available with the Pre-Release Service(s) or from the dedicated ‘Documentation’ page of the New Relic website (the “Documentation”) enables you to test experimental features and products before they are made generally available. New Relic reserves the right to withhold or discontinue any Pre-Release Service(s), which may be designated as an alpha, beta, pilot, limited release, developer preview, technology preview, non-production, evaluation, or by a similar description, in its sole discretion and provides no guarantee that any Pre-Release Service(s) will eventually be made commercially or generally available. Pre-Release Service(s) are versions provided before they are generally available and may have bugs, stability issues, or other problems. Use of the Pre-Release Service(s) may result in unexpected results, loss of data, outages, or other damage to your systems or networks. New Relic advises against using the Pre-Release Service(s) in production. No support is offered with or for Pre-Release Service(s). You hereby release New Relic from any liability arising from or related to your use of the Pre-Release Service(s). In connection with your use of the Pre-Release Services, you may provide feedback, comments, and/or suggestions based on or relating to the Pre-Release Services (“Feedback”). The Pre-Release Services and any related information, including without limitation features, functionality, designs, user interface, capabilities, specifications, architectural diagrams, usage data, APIs, deployment schedules, email lists, bug databases, know how, source code, potential features, Feedback, and information related to any business or technical information of New Relic, including without limitation product plans, costs, prices, finances, marketing plans, business opportunities, research, development, and negotiations are considered confidential information of New Relic (“Confidential Information”). 2. License and Restrictions 2.1 Evaluation License. Subject to your compliance with the Policy, New Relic hereby grants you a limited, non-exclusive, non-transferable, revocable license, during the term of this Policy, to use the Pre-Release Service(s) in accordance with the Documentation solely for the purpose of internal evaluation and supplying Feedback to New Relic. 2.2 License Restrictions. You may not (and may not authorize or enable a third party to): (i) reverse engineer, decompile, disassemble, or otherwise attempt to discover the source code or other trade secrets in the Pre-Release Service(s); (ii) access the Pre-Release Service(s) in order to build a similar or competitive application, service, feature or other competitive purpose; (iii) remove or destroy any copyright notices or other proprietary marks contained on or in the Pre-Release Service(s); (iv) use the Pre-Release Service(s) for any benchmarking purposes or in connection with a service bureau, timeshare, service provider, or like activity where you operate the Pre-Release Service(s) on behalf of a third party; (v) mirror, frame, copy, modify, host, rent, lease, sell, commercialize, sublicense, assign or otherwise transfer the Pre-Release Service(s), or the access or use of the Pre-Release Service(s); (vi) use the Pre-Release Service(s) in a manner that may violate or infringe the intellectual property, data protection, or other proprietary right of a third party; (vii) use or access the Pre-Release Service(s) in an unauthorized manner and/or in a manner that violates any applicable law, rule, contract, or guideline (including but not limited to our Documentation or Community Guidelines); (viii) use the Pre-Release Service(s) to transmit worms, viruses, malicious code, security vulnerabilities, or otherwise negatively impact network operations, third parties, or New Relic; (ix) use any data mining or similar data gathering and extraction methods in connection with the Pre-Release Service(s); or (x) use the Pre-Release Service(s) to process sensitive personal information, e.g., “personal information” of children as defined by the Children’s Online Privacy Protection Act, “protected health information” as defined by the Health Insurance Portability and Accountability Act of 1996, government issued identification numbers, financial account information, payment card data, “special categories of data” as described in the EU General Data Protection Regulation (GDPR), or other information subject to regulatory, statutory, or contractual restrictions. New Relic reserves the right, but not the obligation, to monitor or review your use of the Pre-Release Service(s) at any time and may investigate any suspected violations of this Policy. 2.3 Intellectual Property Rights. You acknowledge and agree that, as between you and New Relic, New Relic owns all right, title, and interest in and to the Pre-Release Service(s), Confidential Information, and Feedback, including but not limited to all intellectual property and proprietary rights therein. New Relic and its licensors reserve all rights and licenses not expressly granted herein. 2.4 Feedback. You acknowledge and agree that all Feedback will be the sole and exclusive property of New Relic. You hereby irrevocably transfer and assign to New Relic all right, title, and interest in all Feedback, including but not limited to all intellectual property and proprietary rights therein. 3. Confidential Information You will not use or disclose any Confidential Information, except as necessary for the performance of this Policy, and you will use reasonable efforts to protect Confidential Information from unauthorized use or disclosure. You may disclose Confidential Information only to those employees with a bona fide need to know, provided that each employee has signed a written agreement with nondisclosure restrictions at least as protective of the Confidential Information as those set forth herein. For the purposes of this section, information will not be deemed Confidential Information if it: (a) is or becomes generally known to the public through no fault or breach of this Policy; (b) is rightfully known by you at the time of disclosure without an obligation of confidentiality; (c) is independently developed by you without access to or use of the Confidential Information; or (d) is rightfully obtained by you from a third party without restriction on use or disclosure. Upon New Relic’s request, you agree to destroy all Confidential Information in your possession within 30 days of termination of this Policy. 4. Disclaimers and Acknowledgement 4.1 Acknowledgement of Pre-Release Service(s). You acknowledge and agree that: (a) the Pre-Release Service(s) is not an official product and has not been commercially released for sale by New Relic; (b) the Pre-Release Service(s) may not operate properly, be in final form, or fully function; (c) the Pre-Release Service(s) may contain errors, design flaws, security vulnerabilities, or other problems; (d) it may not be possible to make the Pre-Release Service(s) fully functional or secure; (e) the information obtained using the Pre-Release Service(s) may not be accurate; (f) use of the Pre-Release Service(s) may result in unexpected results, vulnerabilities, loss of data, project delays, or other unpredictable damage or loss, including without limitation to your use of New Relic services and products not governed by this Policy (collectively “Unintended Effects”); (g) New Relic is under no obligation to release a commercial or generally available version of the Pre-Release Service(s); and (h) New Relic has the right unilaterally to abandon development of the Pre-Release Service(s) at any time and without any obligation or liability to you. 4.2 Your Data. You acknowledge and agree that you should not rely on the Pre-Release Service(s) for any reason. You further acknowledge and agree that you are solely responsible for maintaining and protecting all data and information that is stored, retrieved, or otherwise processed by the Pre-Release Service(s) in accordance with your contractual obligations and requirements under applicable law. Without limiting the foregoing, you are responsible for all costs and expenses required to backup and restore any data and information that is lost or corrupted as a result of your use of the Pre-Release Service(s) or any Unintended Effects. You understand and agree that your Data may be transferred to the United States for storage, processing, and use by New Relic to provide the Pre-Release Service(s). For any data that you process using the Pre-Release Service(s), you represent and warrant that you have all necessary rights and consent to do so. New Relic has no obligation to store any data you process using the Pre-Release Service(s) and shall have no liability for the deletion or accuracy of such data. 4.3 Warranty Disclaimers. YOU ACKNOWLEDGE THAT PRE-RELEASE SERVICE(S) ARE PROVIDED “AS IS.” TO THE EXTENT PERMITTED BY APPLICABLE LAW, NEW RELIC DISCLAIMS ALL WARRANTIES, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO ANY WARRANTIES REGARDING MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, SECURITY, AND ANY WARRANTIES ARISING OUT OF THE COURSE OF DEALING OR USAGE OF TRADE. NEW RELIC MAKES NO WARRANTY THAT PRE-RELEASE SERVICE(S) WILL MEET YOUR REQUIREMENTS OR WILL BE ACCURATE, RELIABLE, ERROR-FREE, UNINTERRUPTED, TIMELY, OR SECURE. YOUR USE OF PRE-RELEASE SERVICE(S) IS AT YOUR OWN RISK. 4.4 Modifications. New Relic may change the Policy from time to time as our business changes and technology evolves, and future versions of our generally available services may not be compatible with Pre-Release Service(s) built using previous versions. The most current version of this Policy will be posted in the Documentation. Any changes to this Policy will be effective immediately for all users in instances to comply with applicable law, for new users of a Pre-Release Service(s) and, for all other users, any changes to this Policy will be effective as of fifteen (15) days after posting notice of such changes. If we determine in our sole discretion that an update is material, we will provide notice of such material change to you through your New Relic account, the Pre-Release Service(s), the Documentation, our blogs or community forums, and/or by email to the email address of your account administrator. We may require you to provide consent to the updated Policy in a specified manner before further use of the Pre-Release Service(s) is permitted. If you do not agree to any change(s), your sole remedy is to stop using the Pre-Release Service(s) and any such termination by you shall be without penalty except as specified in this Policy. Otherwise, your use of any Pre-Release Service(s) after any such update shall constitute acceptance of the then-current Policy. 5. Limitation of Liability TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC, ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE FOR ANY INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, EXEMPLARY OR CONSEQUENTIAL (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) DAMAGES, REGARDLESS OF LEGAL THEORY, WHETHER OR NOT NEW RELIC HAS BEEN WARNED OF THE POSSIBILITY OF SUCH DAMAGES, AND EVEN IF A REMEDY FAILS OF ITS ESSENTIAL PURPOSE. NEW RELIC’S AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS POLICY, THE PRE-RELEASE SERVICES, OR ANY UNINTENDED EFFECTS WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE LIMITATIONS OF DAMAGES SET FORTH ABOVE ARE FUNDAMENTAL ELEMENTS OF THE BASIS OF THE BARGAIN BETWEEN NEW RELIC AND YOU. 6. Indemnity You agree to indemnify and hold New Relic, its parents, subsidiaries, affiliates, officers, agents, employees, and licensors harmless from any claims, fees, fines, demands, losses, liabilities, damages, and costs, including reasonable attorney’s fees, arising from or related to your use of the Pre-Release Service(s), including but not limited to allegations arising from your breach of any terms herein and/or allegations that data processed by the Pre-Release Service(s) violates or infringes the privacy, data protection, or intellectual property rights of a third party. 7. Export Restrictions You acknowledge that the software in the Pre-Release Service(s) licensed hereunder may be subject to the export control laws and regulations of the U.S. and other countries. You agree that you will not export or re-export the Pre-Release Service(s), any part thereof, or any process or service that is the direct product of the Pre-Release Service(s) to any country, person or entity subject to U.S. export restrictions. 8. Termination Your access to the Pre-Release Service(s) will terminate upon the earliest of: (i) New Relic making the Pre-Release Service(s) or a successor version of the Pre-Release Service(s) generally available or available for commercial release; (ii) New Relic ceasing to make the Pre-Release Service(s) available to you; or (iii) New Relic terminating this Policy. New Relic may terminate this Policy for any reason upon notice to you. Sections 2.2, 2.3, 2.4, 3, 4, 5, 6, and 7 will survive your use of the Pre-Release Service(s).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 198.41896,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic Pre-release <em>policy</em>",
        "sections": "2. <em>License</em> and Restrictions",
        "tags": "<em>License</em> <em>information</em>",
        "body": " Services and any related <em>information</em>, including without limitation features, functionality, designs, user interface, capabilities, specifications, architectural diagrams, usage data, APIs, deployment schedules, email lists, bug databases, know how, source code, potential features, Feedback"
      },
      "id": "6044e71b28ccbc984d2c60c8"
    }
  ],
  "/docs/licenses/license-information/referenced-policies/service-level-availability-commitment": [
    {
      "sections": [
        "Security guide",
        "Tip",
        "Security Program",
        "Security Domains",
        "Security Certifications",
        "Data Control, Facilities, and Encryption",
        "Law Enforcement Request Report"
      ],
      "title": "Security guide",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "356f0d11ffcb62208a743a0a7c127f5f6da9c940",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-guide/",
      "published_at": "2022-01-08T12:36:19Z",
      "updated_at": "2021-12-09T15:28:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Last updated September 17, 2021. This is supplement to our security policy and serves as a guide to New Relic’s description of its Services, functionalities, and features. Tip We may update the URLs in this document without notice. Security Program New Relic follows \"privacy by design\" principles as described here: https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Security Domains New Relic’s policies and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless Security; Configuration Management; Vulnerability Management; Network Protection; Transmission Protection; Password Management; Access Control, Audit Logging & Monitoring; Education, Training, and Awareness; Third Party Assurance; Incident Management; Business Continuity and Disaster Recover; Risk Management; Data Protection & Privacy; and Service Management Systems. Security Certifications New Relic audits its Services against industry standards as described at https://docs.newrelic.com/docs/security/security-privacy/compliance/regulatory-audits-new-relic-services/. Data Control, Facilities, and Encryption New Relic's customers can send data to New Relic's APIs by (1) using New Relic's software, (2) using vendor-neutral software that is managed and maintained by a third-party such as via OpenTelemetry instrumentation provided by opentelemetry.io, or (3) from third-party systems that customer's manage and/or control. New Relic's customers can use New Relic's Services such as NerdGraph to filter out and drop data. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/drop-data-using-nerdgraph/. New Relic's customers can adjust their data retention periods as appropriate for their needs. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/manage-data-retention/#adjust-retention. New Relic Logs obfuscates numbers that match known patterns, such as bank card and social security numbers as described here: https://docs.newrelic.com/docs/logs/log-management/get-started/new-relics-log-management-security-privacy/. New Relic honors requests to delete personal data in accordance with applicable privacy laws. Please see https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Customers may use New Relic's APIs to query data, such as NerdGraph described here, and New Relic Services to export the data to other cloud providers. Customers can configure its log forwarder [https://docs.newrelic.com/docs/logs/enable-log-management-new-relic/enable-log-monitoring-new-relic/forward-your-logs-using-infrastructure-agent/] before sending infrastructure logs to New Relic. For New Relic Customers in New Relic US, FedRAMP and HIPAA-enabled environments, Customer Data is replicated to the off-site backup system via Amazon Simple Storage Service (S3). Category of Customer Description FedRAMP HIPAA-enabled US Gen Pop EU Gen Pop Data is stored in Amazon Web Services (“AWS”). Limited Data is stored in IBM Data for New Relic Incident Intelligence is stored in Google Cloud New Relic regularly tests, assess, and evaluates its measures to ensure the security of processing using industry-recognized standards and uses independent third-party auditors as provided below: Annual SOC 2 Type 2 Annual FedRAMP assessment by an independent third-party pursuant to NIST 800-53 rev 4 Moderate authorization. Annual HITRUST-validated assessment by an independent third-party *Pursuing CY2021 Q4 ISO 27001 TISAX The Services that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed information about AWS security is available at https://aws.amazon.com/security/ and http://aws.amazon.com/security/sharing-the-security-responsibility/. Data encryption at rest utilizes FIPS 140-2 compliant encryption methodology. For AWS SOC Reports, please see https://aws.amazon.com/compliance/soc-faqs/. The Services that operate on Google Cloud Platform (\"GCP\") are protected by the security and environmental controls of GCP. Detailed information about GCP security is available at https://cloud.google.com/docs/tutorials#security. For GCP reports, please see https://cloud.google.com/security/compliance/. IBM Deft Zayo QTS Law Enforcement Request Report New Relic has not to date received any request for customer data from a law enforcement or other government agency (including under any national security process), and has not made any corresponding disclosures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 246.06125,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " as described here: https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;security&#x2F;security-privacy&#x2F;data-privacy&#x2F;data-privacy-new-relic&#x2F;. Security Domains New Relic’s <em>policies</em> and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless"
      },
      "id": "6147558128ccbc973a56a863"
    },
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Purpose",
        "2. Security Overview",
        "3.Security Certifications",
        "4. Data Control and Encryption",
        "5. Facilities",
        "6. Employee Access, Screening and Controls.",
        "7. Security Incident and Data Breach Response",
        "8. Network and Systems Security",
        "9. Authentication and Access Management",
        "10. Vulnerability Management",
        "11. System Access and Logging",
        "12. Disaster Recovery and Data Backup",
        "13. Copies and Removal",
        "14. Third Party Vendor Management",
        "15. Disclosure by Law",
        "16. Updates"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2022-01-08T13:08:54Z",
      "updated_at": "2021-09-27T15:05:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Updated 23 September 2021. The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Purpose This Policy describes New Relic's security program and the technical and organizational security controls to protect New Relic’s systems as shown in New Relic’s third-party audits and certifications. New Relic may update this Policy from time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/. 2. Security Overview 2.1. New Relic maintains a comprehensive Information Protection Program to manage information security within New Relic that includes administrative, technical, and physical safeguards designed to protect the confidentiality, integrity, and availability of Customer Data and that is appropriate to the nature, size, and complexity of New Relic's business operations. New Relic’s Information Protection Program includes: 2.1.1. executive review, support and accountability for all security related policies and practices; 2.1.2. formal written policies and procedures that are designed to protect against the loss, theft, or other unauthorized access or alteration of Customer Data, meet or exceed applicable industry standards, outlines a definition of information security and its overall objectives, include defined information security roles and responsibilities, a framework for setting control objectives and controls, a formal and effective risk mitigation program and a service provider security management program; 2.1.3. periodic risk assessments, including internal audits and/or independent 3rd party audits, to measure the effectiveness and appropriateness of controls for all systems processing Customer Data; 2.1.4. in-depth review of security incidents, including effective determination of root cause and corrective action; formal, industry-recognized controls frameworks based on an external audit standard; 2.1.5. employee screening and access management; and 2.1.6. comprehensive security testing, vulnerability identification, and mitigation methodology that consists of a variety of independent approaches that, when combined, are designed to maximize coverage for a diverse set of attack vectors. 2.2. New Relic's Chief Information Security Officer (CISO) leads New Relic's Information Security Program and develops, reviews, and approves, with appropriate stakeholders, New Relic's security policies and procedures. New Relic has appointed data protection officer(s) as described in New Relic’s Privacy Notices who are consulted as necessary under applicable laws. 2.3. Information security objectives, approach, scope, importance, goals, and principles for the organization’s security program are formally identified, communicated throughout the organization to users in a form that is relevant, accessible, and understandable to the intended reader; and supported by a controls framework that considers legislative, regulatory, contractual requirements, and other policy-related requirements. 2.4. New Relic will maintain written policies and procedures to review, test, and approve (as appropriate) changes affecting New Relic infrastructure and systems that process Customer Data including, but not limited to, peer reviews prior to introducing new code into production. New Relic will establish acceptance criteria for new information systems, upgrades, and new versions and will carry out suitable tests of the system(s) during development and prior to acceptance. Any changes or updates will not materially decrease the security of the systems. 3.Security Certifications 3.1. New Relic regularly tests, assesses, and evaluates its security measures for protecting Customer Data using industry-recognized standards and uses independent third-party auditors to verify such controls. 3.2. New Relic agrees to provide Customer, upon request, with applicable certifications or reports about New Relic systems. All information exchanged in connection with the audit activities described in this section is deemed to be the Confidential Information of New Relic. 3.3. Additional information about New Relic’s security certifications are available on New Relic’s Security Guide. 4. Data Control and Encryption 4.1. The Service and related features are designed to provide Customer with control over Customer’s data sources and Customer’s environments that are monitored and sending data to New Relic. 4.2. New Relic will have established methods to: (i) enable Customer to use encryption on Customer Data in transit, and (ii) securely hash passwords in storage following industry standard practices (e.g. scrypt) as described in the Documentation. Server certificate-based authentication is used as part of the TLS encryption with a trusted certificate authority. 4.3. New Relic receives and processes data in accordance with the Agreement and the Services as described in the Documentation. New Relic permits customers to delete personal data in accordance with applicable privacy laws as further described in the Documentation. In the event of error in personal data sent in Customer Data Customers may request personal data deletion and re-send data that is accurate. 4.4. Additional information for Customer data control and encryption, including encryption of data at rest, is available on New Relic’s Security Guide. 5. Facilities 5.1. All physical locations that process Customer Data are co-locations and third party data centers. All physical locations have security measures in place that are designed to prevent unauthorized physical access to data processing facilities and unlawful access, modification, or destruction of Customer Data. 5.2. New Relic will seek assurances (e.g., in the form of an independent 3rd party audit report such as the SOC 2 Type 2, ISO 27001, and vendor security evaluations) from its data processing facilities vendors that store or process Customer Data: 5.2.1. secure its data process facilities in an access-controlled location and have protections in place to prevent unauthorized access, damage, and interference; 5.2.2 employ physical security appropriate to the classification of the assets and information being managed which may include card key access, security cameras, and solid wall construction for all exterior walls; 5.2.3 limit and screen all entrants, which may include measures such as on-site security guard, badge reader, electronic lock, or a monitored closed caption television (CCTV); and 5.2.4 maintain relevant access logs. 5.3. Additional information about New Relic’s third party data centers are available on New Relic’s Security Guide. 6. Employee Access, Screening and Controls. New Relic will have and maintain policies and practices that include, at a minimum, the following controls and safeguards applied to New Relic employees and contractors who may access Customer Data: 6.1. For U.S. New Relic employees and subject to applicable law, a criminal background screening of each of its new employees to whom it gives access to Customer Data at the federal, state, and county levels. For non-U.S. New Relic employees, New Relic will use commercially reasonable efforts to meet the same criteria as established for U.S.-based New Relic employees, subject to general business practices in the respective country and in compliance with applicable local law requirements; 6.2. All New Relic employees with access to Customer Data will undergo adequate training, such as annual security awareness training, in the care, protection and handling of Customer Data, and will align with the privacy and security measures set out in this Addendum by following the guidance provided in their welcome package, which includes New Relic’s security policies and a security acknowledgement; 6.3. A disciplinary policy and process, to be used when New Relic employees violate New Relic security or privacy policies or access Customer Data without prior authorization; 6.4. Administrative or remote access to New Relic systems that process Customer Data will align with industry standard practices, including multi-factor authentication; 6.5. Restricted access to New Relic proprietary source code to prevent unauthorized access; 6.6. Controls designed to ensure that only those New Relic employees with an actual need-to-know will have access to New Relic systems including, but not limited to, the use of a formal access management process for the request, review, approval, and provisioning; 6.7. Controls designed to ensure that New Relic employees are granted access to New Relic systems based on least-privilege principles; and 6.8. Revoke access to New Relic employees no longer requiring access. 7. Security Incident and Data Breach Response 7.1. New Relic will take appropriate physical, technical, and administrative security measures that are commercially reasonable and consistent with industry standards to prevent a Data Breach, and as required by any applicable law or regulation. “Data Breach” means the theft, loss, or unauthorized access of Customer Data. Without limiting the foregoing, New Relic will implement security measures at least as stringent as those set out in this Addendum. New Relic will designate a senior representative to provide incident briefings, as needed in case of a Data Breach, and to respond to reasonable requests by Customer pertaining to privacy and data security issues within a commercially reasonable time frame. 7.2. New Relic will: 7.2.1. Notify Customer without undue delay if New Relic becomes aware of a Data Breach; 7.2.2. Maintain a security incident response plan, including procedures and means to respond in a manner consistent with industry standards; 7.2.3. Reasonably cooperate with Customer to investigate and remediate a Data Breach and mitigate any further risk to the Customer Data, or risk to data subjects and/or Customer’s reputation or brand; 7.2.4. Provide reasonable assistance to Customer at New Relic’s sole cost and expense; and 7.2.5. Make commercially reasonable efforts to preserve evidence and reasonably cooperate with Customer and legal authorities (as applicable and legally permissible) during an investigation of a Data Breach. 8. Network and Systems Security 8.1. All extranet connectivity by New Relic personnel to systems processing Customer Data will be through secure remote connections. 8.2. Network segments connected to the Internet will be protected by secure access control mechanisms, such as a firewall configured to secure all devices behind it and properly addresses security concerns according to industry standard practices. 8.3. New Relic will have industry standard measures in place to actively monitor its systems and help detect a potential hostile attack, such as Network Intrusion Detection (NID) or Host Intrusion Detection (HID)/Prevention. 8.4. Applications, ports, services, and similar access points installed on a computer or network facility, which are not specifically required for business functionality, will be disabled or removed. 8.5. New Relic maintains configuration standards for authorized operating systems and software for systems that support processing of Customer Data. New Relic will establish and follow server configuration guidelines and processes for preventing unauthorized access to Customer Data. New Relic maintains secure images or templates for systems based on the organization’s approved configuration standards. 8.6. Development, test, and operational environments will be logically separated to reduce the risks of unauthorized access or changes to the operational system. 8.7. New Relic network architecture will be designed to limit site access and restrict the availability of information systems that are considered to be vulnerable to attack. 8.8. New Relic provides appropriate TLS certificates when users access and view Customer Data in the Service. New Relic’s software for sending Customer Data to New Relic will encrypt Customer Data in transit by default. 9. Authentication and Access Management If you subscribe to the requisite New Relic One Service, New Relic will provide industry standard authentication and access controls to protect Customer Data, including industry standard authentication methods utilized to help prevent unauthorized access to the Service. New Relic’s access control methods will clearly state the rules and rights for each user or group of users including applications and information sharing and will include a process for granting and removing access to all information systems processing Customer Data. A record of all privileges allocated will be maintained pursuant to the requirements herein. 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. 10. Vulnerability Management New Relic will have and maintain the following vulnerability management processes for all devices used to connect to the New Relic network and Services. 10.1. New Relic will align to industry standard practices for build out, minimization of services and secure configuration, in accordance with, industry-recognized minimum security baselines for the New Relic platform and subcontractor systems connected to the New Relic platform in relation to the provision of the Services. 10.2. New Relic will employ industry-recognized standards and tools to conduct frequent infrastructure vulnerability scanning to test New Relic’s network and infrastructure and application penetration testing to test the New Relic Services. New Relic applies “Medium”, “High” and “Critical” security patches for all components in production and development environments as soon as commercially possible in accordance with its vulnerability management protocol, and consistent with industry standard practices and standards; 10.3. New Relic will have processes in place designed to ensure adherence to industry standard security development practices for development and testing for all code, APIs, and applications deployed and implemented in support of the Service; 10.4. New Relic will have and maintain solutions to identify and prevent malicious attackers or code from accessing or compromising Customer Data or systems that process Customer Data. These include, but are not limited to, software that identifies and removes malware and detects attempted intrusions. New Relic will have a security event and incident monitoring system and supporting processes to notify appropriate personnel in response to threats. 11. System Access and Logging 11.1. Access to New Relic’s systems will not be granted to employees of New Relic unless they have been uniquely identified and have sufficient credentials. 11.2. Access to New Relic’s systems will be logged and retained for no less than 6 months to assist in investigations and access control monitoring, including, but not limited to, end user access and activities, and information security events. 11.3. New Relic agrees to provide Customer the capability to access log records relating to Customer Accounts and the New Relic systems that process Customer Data in the event of a Data Breach or if required in connection with a law enforcement request. 12. Disaster Recovery and Data Backup 12.1. New Relic will have plans designed to respond to loss of services, which are tested and reviewed at least annually. This plan will include documented policies and procedures to restore service in the event of a service failure. 12.2. New Relic will establish and follow backup and restore procedures for Customer Data. 12.3. New Relic business continuity plan identifies critical systems. Annual disaster recovery tests are performed to check and restore customer data in the event of an incident. 12.4. New Relic will provide Customer with redacted copies of its plan(s) and evidence of tests/reviews upon request, but not more frequently than once annually, and subject to confidentiality requirements. 12.5. Additional information for Customer Data control and encryption are available on New Relic’s Security Guide. 13. Copies and Removal 13.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of the Agreement for any reason: (a) New Relic will, and will cause its personnel, to cease all access and use of any Customer Data, and (b) New Relic will delete all copies of Customer Data within ninety (90) days. 13.2. New Relic will maintain a process of ensuring secure destruction and deletion of all Customer Data, when reasonably requested by Customer or as otherwise provided in the Agreement. The process will include industry standard processes so that: (i) Customer Data cannot be practicably read or reconstructed, and (ii) the systems that store Customer Data are securely erased and/or decommissioned disks are destroyed. 14. Third Party Vendor Management New Relic may use third party vendors to provide the Services. New Relic performs a security risk-based assessment of prospective vendors before working with them to validate they meet New Relic’s security and business continuity standards, including the type of access and classification of data being accessed (if any), controls necessary to protect data, and legal/regulatory requirements. New Relic enters into written agreements with its vendors that process Customer Data which include confidentiality, privacy, and security obligations that provide an appropriate level of protection for Customer Data that these vendors may process for New Relic to maintain the security posture in this Policy, including following industry security standards. 15. Disclosure by Law In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic will (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. New Relic publishes its law enforcement requests report on New Relic’s Security Guide. 16. Updates As New Relic releases new products, services, functionality, and features, New Relic may update this Policy to account for such products, services, functionality, and features. For additional information, see our Security Guide.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 210.48322,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Security <em>policy</em>",
        "sections": "Security <em>policy</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;<em>referenced</em>-<em>policies</em>&#x2F;security-policy&#x2F;. 2. Security Overview 2.1. New Relic"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    },
    {
      "sections": [
        "New Relic Pre-release policy",
        "New Relic Pre-Release Policy",
        "1. Introduction",
        "2. License and Restrictions",
        "3. Confidential Information",
        "4. Disclaimers and Acknowledgement",
        "5. Limitation of Liability",
        "6. Indemnity",
        "7. Export Restrictions",
        "8. Termination"
      ],
      "title": "New Relic Pre-release policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "5ca324e942906913fc346245cfd80d405e90ad1f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/new-relic-pre-release-policy/",
      "published_at": "2022-01-08T13:08:54Z",
      "updated_at": "2021-03-13T01:17:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Without overriding any express provisions that you may have agreed to in a separate written beta or pre-release agreement executed by you and New Relic, the following terms apply to your use of any New Relic Pre-Release Service(s). New Relic Pre-Release Policy Through your use of any New Relic, Inc. (“New Relic”) Pre-Release Services (as further defined), you signify that you have read, understood, and accept the terms of this Pre-Release Policy (the “Policy”) governing your use and participation in New Relic’s program for making beta, alpha and pre-release versions of New Relic software (i.e. agents, SDKs, APIs, integrations, private locations, code, etc.), services, features, user-interfaces, and platforms made available for evaluation purposes (the “Pre-Release Service(s)”). This Policy is effective on behalf of yourself or the entity(ies) associated with your New Relic account (collectively referred to herein as “you” or “your”) as of your date of first use of the Pre-Release Service(s) (the “Effective Date”), which means that your choice to use the Pre-Release Service(s) means that you have the legal authority to use the Pre-Release Service(s) personally or on behalf of the company or organization associated with your New Relic account in accordance with this Policy. Please read this Policy carefully as your use of Pre-Release Service(s) may have unintended consequences and materially impact your data or use of the New Relic services and products. If you do not agree to this Policy, your sole remedy is to not make use of the Pre-Release Services. 1. Introduction This Policy and any New Relic technical guides and documentation made available with the Pre-Release Service(s) or from the dedicated ‘Documentation’ page of the New Relic website (the “Documentation”) enables you to test experimental features and products before they are made generally available. New Relic reserves the right to withhold or discontinue any Pre-Release Service(s), which may be designated as an alpha, beta, pilot, limited release, developer preview, technology preview, non-production, evaluation, or by a similar description, in its sole discretion and provides no guarantee that any Pre-Release Service(s) will eventually be made commercially or generally available. Pre-Release Service(s) are versions provided before they are generally available and may have bugs, stability issues, or other problems. Use of the Pre-Release Service(s) may result in unexpected results, loss of data, outages, or other damage to your systems or networks. New Relic advises against using the Pre-Release Service(s) in production. No support is offered with or for Pre-Release Service(s). You hereby release New Relic from any liability arising from or related to your use of the Pre-Release Service(s). In connection with your use of the Pre-Release Services, you may provide feedback, comments, and/or suggestions based on or relating to the Pre-Release Services (“Feedback”). The Pre-Release Services and any related information, including without limitation features, functionality, designs, user interface, capabilities, specifications, architectural diagrams, usage data, APIs, deployment schedules, email lists, bug databases, know how, source code, potential features, Feedback, and information related to any business or technical information of New Relic, including without limitation product plans, costs, prices, finances, marketing plans, business opportunities, research, development, and negotiations are considered confidential information of New Relic (“Confidential Information”). 2. License and Restrictions 2.1 Evaluation License. Subject to your compliance with the Policy, New Relic hereby grants you a limited, non-exclusive, non-transferable, revocable license, during the term of this Policy, to use the Pre-Release Service(s) in accordance with the Documentation solely for the purpose of internal evaluation and supplying Feedback to New Relic. 2.2 License Restrictions. You may not (and may not authorize or enable a third party to): (i) reverse engineer, decompile, disassemble, or otherwise attempt to discover the source code or other trade secrets in the Pre-Release Service(s); (ii) access the Pre-Release Service(s) in order to build a similar or competitive application, service, feature or other competitive purpose; (iii) remove or destroy any copyright notices or other proprietary marks contained on or in the Pre-Release Service(s); (iv) use the Pre-Release Service(s) for any benchmarking purposes or in connection with a service bureau, timeshare, service provider, or like activity where you operate the Pre-Release Service(s) on behalf of a third party; (v) mirror, frame, copy, modify, host, rent, lease, sell, commercialize, sublicense, assign or otherwise transfer the Pre-Release Service(s), or the access or use of the Pre-Release Service(s); (vi) use the Pre-Release Service(s) in a manner that may violate or infringe the intellectual property, data protection, or other proprietary right of a third party; (vii) use or access the Pre-Release Service(s) in an unauthorized manner and/or in a manner that violates any applicable law, rule, contract, or guideline (including but not limited to our Documentation or Community Guidelines); (viii) use the Pre-Release Service(s) to transmit worms, viruses, malicious code, security vulnerabilities, or otherwise negatively impact network operations, third parties, or New Relic; (ix) use any data mining or similar data gathering and extraction methods in connection with the Pre-Release Service(s); or (x) use the Pre-Release Service(s) to process sensitive personal information, e.g., “personal information” of children as defined by the Children’s Online Privacy Protection Act, “protected health information” as defined by the Health Insurance Portability and Accountability Act of 1996, government issued identification numbers, financial account information, payment card data, “special categories of data” as described in the EU General Data Protection Regulation (GDPR), or other information subject to regulatory, statutory, or contractual restrictions. New Relic reserves the right, but not the obligation, to monitor or review your use of the Pre-Release Service(s) at any time and may investigate any suspected violations of this Policy. 2.3 Intellectual Property Rights. You acknowledge and agree that, as between you and New Relic, New Relic owns all right, title, and interest in and to the Pre-Release Service(s), Confidential Information, and Feedback, including but not limited to all intellectual property and proprietary rights therein. New Relic and its licensors reserve all rights and licenses not expressly granted herein. 2.4 Feedback. You acknowledge and agree that all Feedback will be the sole and exclusive property of New Relic. You hereby irrevocably transfer and assign to New Relic all right, title, and interest in all Feedback, including but not limited to all intellectual property and proprietary rights therein. 3. Confidential Information You will not use or disclose any Confidential Information, except as necessary for the performance of this Policy, and you will use reasonable efforts to protect Confidential Information from unauthorized use or disclosure. You may disclose Confidential Information only to those employees with a bona fide need to know, provided that each employee has signed a written agreement with nondisclosure restrictions at least as protective of the Confidential Information as those set forth herein. For the purposes of this section, information will not be deemed Confidential Information if it: (a) is or becomes generally known to the public through no fault or breach of this Policy; (b) is rightfully known by you at the time of disclosure without an obligation of confidentiality; (c) is independently developed by you without access to or use of the Confidential Information; or (d) is rightfully obtained by you from a third party without restriction on use or disclosure. Upon New Relic’s request, you agree to destroy all Confidential Information in your possession within 30 days of termination of this Policy. 4. Disclaimers and Acknowledgement 4.1 Acknowledgement of Pre-Release Service(s). You acknowledge and agree that: (a) the Pre-Release Service(s) is not an official product and has not been commercially released for sale by New Relic; (b) the Pre-Release Service(s) may not operate properly, be in final form, or fully function; (c) the Pre-Release Service(s) may contain errors, design flaws, security vulnerabilities, or other problems; (d) it may not be possible to make the Pre-Release Service(s) fully functional or secure; (e) the information obtained using the Pre-Release Service(s) may not be accurate; (f) use of the Pre-Release Service(s) may result in unexpected results, vulnerabilities, loss of data, project delays, or other unpredictable damage or loss, including without limitation to your use of New Relic services and products not governed by this Policy (collectively “Unintended Effects”); (g) New Relic is under no obligation to release a commercial or generally available version of the Pre-Release Service(s); and (h) New Relic has the right unilaterally to abandon development of the Pre-Release Service(s) at any time and without any obligation or liability to you. 4.2 Your Data. You acknowledge and agree that you should not rely on the Pre-Release Service(s) for any reason. You further acknowledge and agree that you are solely responsible for maintaining and protecting all data and information that is stored, retrieved, or otherwise processed by the Pre-Release Service(s) in accordance with your contractual obligations and requirements under applicable law. Without limiting the foregoing, you are responsible for all costs and expenses required to backup and restore any data and information that is lost or corrupted as a result of your use of the Pre-Release Service(s) or any Unintended Effects. You understand and agree that your Data may be transferred to the United States for storage, processing, and use by New Relic to provide the Pre-Release Service(s). For any data that you process using the Pre-Release Service(s), you represent and warrant that you have all necessary rights and consent to do so. New Relic has no obligation to store any data you process using the Pre-Release Service(s) and shall have no liability for the deletion or accuracy of such data. 4.3 Warranty Disclaimers. YOU ACKNOWLEDGE THAT PRE-RELEASE SERVICE(S) ARE PROVIDED “AS IS.” TO THE EXTENT PERMITTED BY APPLICABLE LAW, NEW RELIC DISCLAIMS ALL WARRANTIES, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO ANY WARRANTIES REGARDING MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, SECURITY, AND ANY WARRANTIES ARISING OUT OF THE COURSE OF DEALING OR USAGE OF TRADE. NEW RELIC MAKES NO WARRANTY THAT PRE-RELEASE SERVICE(S) WILL MEET YOUR REQUIREMENTS OR WILL BE ACCURATE, RELIABLE, ERROR-FREE, UNINTERRUPTED, TIMELY, OR SECURE. YOUR USE OF PRE-RELEASE SERVICE(S) IS AT YOUR OWN RISK. 4.4 Modifications. New Relic may change the Policy from time to time as our business changes and technology evolves, and future versions of our generally available services may not be compatible with Pre-Release Service(s) built using previous versions. The most current version of this Policy will be posted in the Documentation. Any changes to this Policy will be effective immediately for all users in instances to comply with applicable law, for new users of a Pre-Release Service(s) and, for all other users, any changes to this Policy will be effective as of fifteen (15) days after posting notice of such changes. If we determine in our sole discretion that an update is material, we will provide notice of such material change to you through your New Relic account, the Pre-Release Service(s), the Documentation, our blogs or community forums, and/or by email to the email address of your account administrator. We may require you to provide consent to the updated Policy in a specified manner before further use of the Pre-Release Service(s) is permitted. If you do not agree to any change(s), your sole remedy is to stop using the Pre-Release Service(s) and any such termination by you shall be without penalty except as specified in this Policy. Otherwise, your use of any Pre-Release Service(s) after any such update shall constitute acceptance of the then-current Policy. 5. Limitation of Liability TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC, ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE FOR ANY INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, EXEMPLARY OR CONSEQUENTIAL (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) DAMAGES, REGARDLESS OF LEGAL THEORY, WHETHER OR NOT NEW RELIC HAS BEEN WARNED OF THE POSSIBILITY OF SUCH DAMAGES, AND EVEN IF A REMEDY FAILS OF ITS ESSENTIAL PURPOSE. NEW RELIC’S AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS POLICY, THE PRE-RELEASE SERVICES, OR ANY UNINTENDED EFFECTS WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE LIMITATIONS OF DAMAGES SET FORTH ABOVE ARE FUNDAMENTAL ELEMENTS OF THE BASIS OF THE BARGAIN BETWEEN NEW RELIC AND YOU. 6. Indemnity You agree to indemnify and hold New Relic, its parents, subsidiaries, affiliates, officers, agents, employees, and licensors harmless from any claims, fees, fines, demands, losses, liabilities, damages, and costs, including reasonable attorney’s fees, arising from or related to your use of the Pre-Release Service(s), including but not limited to allegations arising from your breach of any terms herein and/or allegations that data processed by the Pre-Release Service(s) violates or infringes the privacy, data protection, or intellectual property rights of a third party. 7. Export Restrictions You acknowledge that the software in the Pre-Release Service(s) licensed hereunder may be subject to the export control laws and regulations of the U.S. and other countries. You agree that you will not export or re-export the Pre-Release Service(s), any part thereof, or any process or service that is the direct product of the Pre-Release Service(s) to any country, person or entity subject to U.S. export restrictions. 8. Termination Your access to the Pre-Release Service(s) will terminate upon the earliest of: (i) New Relic making the Pre-Release Service(s) or a successor version of the Pre-Release Service(s) generally available or available for commercial release; (ii) New Relic ceasing to make the Pre-Release Service(s) available to you; or (iii) New Relic terminating this Policy. New Relic may terminate this Policy for any reason upon notice to you. Sections 2.2, 2.3, 2.4, 3, 4, 5, 6, and 7 will survive your use of the Pre-Release Service(s).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 198.41896,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic Pre-release <em>policy</em>",
        "sections": "2. <em>License</em> and Restrictions",
        "tags": "<em>License</em> <em>information</em>",
        "body": " Services and any related <em>information</em>, including without limitation features, functionality, designs, user interface, capabilities, specifications, architectural diagrams, usage data, APIs, deployment schedules, email lists, bug databases, know how, source code, potential features, Feedback"
      },
      "id": "6044e71b28ccbc984d2c60c8"
    }
  ],
  "/docs/licenses/license-information/special-services-licenses/data-collector-licenses": [
    {
      "sections": [
        "New Relic Priority Support"
      ],
      "title": "New Relic Priority Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "f700b8b349627e66540b23ed020c7d9a46e19580",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-priority-support/",
      "published_at": "2022-01-08T13:09:37Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Priority Support document as a PDF (172 KB). The services described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 202.52713,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Priority Support document as a PDF (172 KB). The <em>services</em> described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative."
      },
      "id": "603ea90ae7b9d2e9d42a07d1"
    },
    {
      "sections": [
        "New Relic Diagnostics licenses",
        "Proprietary license",
        "New Relic Diagnostics license terms",
        "Open-source licenses"
      ],
      "title": "New Relic Diagnostics licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "6d86f8f63d372be98270fadc8634f9da8da2a893",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-diagnostics-licenses/",
      "published_at": "2022-01-08T13:10:19Z",
      "updated_at": "2021-03-13T03:25:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Proprietary license New Relic Diagnostics license terms These New Relic Diagnostics License Terms (“Terms”) set forth the terms and conditions under which you (“Customer”) may use New Relic Diagnostics (the “Software”), as made available by New Relic, Inc. (“New Relic”). By clicking “accept” or downloading or using the Software, you agree to be bound by these Terms. If you are agreeing to these Terms on behalf of your company, then “Customer” means your company and you are binding your company to these Terms. 1. License Grant. Subject to all of the terms and conditions of these Terms, New Relic grants Customer a limited, non-exclusive, non-transferable, non-sublicensable license to use the Software in accordance with its documentation in support of Customer’s use of New Relic products to which Customer has a separate subscription (“New Relic Products”). Unless otherwise specified, there is no fee for use of the Software. 2. License Restrictions. Customer will not (a) sell, rent, sublicense, transfer, time-share or otherwise provide access to any copies of the Software, or portions thereof, to a third party; (b) modify, decompile, disassemble or reverse engineer the Software; (c) use the Software to develop services or products for sale or include any components of the Software in any product; (d) remove any product identification, proprietary, copyright or other notices in the Software; or (e) publicly disseminate performance information or analysis (including benchmarks) relating to the Software. 3. Ownership. Except for the limited license rights expressly provided herein, New Relic and its suppliers have and will retain all right, title and interest in and to the Software, and all copies, updates, modifications and derivative works thereof. Customer acknowledges that it is obtaining only a limited license right to the Software and no ownership rights are being conveyed to Customer under these Terms or otherwise. 4. Usage Data. Customer agrees that New Relic and its affiliates have the right to collect Usage Data from Customer through the Software and use Usage Data to support, operate and improve New Relic products and services and for other lawful business purposes. “Usage Data” means diagnostics data related to the use of the Software with New Relic Products, including, without limitation, configuration information, details on diagnostics tasks, New Relic account and application ID numbers and New Relic license keys. For clarity, Usage Data is not considered “Customer Data”, “your Data” or any other similar term as used in any applicable subscription or license agreement for New Relic Products. At Customer’s election, Customer may disable the collection of Usage Data as described in the Software documentation. 5. Disclaimers. New Relic is not obligated to provide any support or maintenance for the Software. ALL USE OF SOFTWARE IS AT CUSTOMER’S OWN RISK. SOFTWARE IS PROVIDED “AS IS,” WITH ALL FAULTS AND WITHOUT WARRANTY OF ANY KIND, EXPRESS, IMPLIED OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, WARRANTIES OF TITLE OR NON-INFRINGEMENT. CUSTOMER MAY HAVE OTHER STATUTORY RIGHTS, HOWEVER, THE DURATION OF STATUTORILY REQUIRED WARRANTIES, IF ANY, WILL BE LIMITED TO THE FULLEST EXTENT PERMITTED BY LAW. 6. Limitation of Liability. TO THE FULLEST EXTENT PERMITTED BY LAW, IN NO EVENT WILL NEW RELIC BE LIABLE FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES OF ANY KIND, REGARDLESS OF THE FORM OF ACTION, WHETHER IN CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY OR OTHERWISE, EVEN IF INFORMED OF THE POSSIBILITY OF SUCH DAMAGES IN ADVANCE. TO THE EXTENT ANY OF THE ABOVE LIMITATIONS ARE NOT ENFORCEABLE AT APPLICABLE LAW, NEW RELIC’S ENTIRE LIABILITY TO CUSTOMER UNDER THESE TERMS WILL NOT EXCEED $50. THESE LIMITATIONS ON LIABILITY ARE A FUNDAMENTAL BASIS OF THE BARGAIN AND NEW RELIC WOULD NOT BE ABLE TO PROVIDE THE SOFTWARE WITHOUT SUCH LIMITATIONS. THESE LIMITATIONS ON LIABILITY WILL APPLY NOTWITHSTANDING ANY FAILURE OF ESSENTIAL PURPOSE OF ANY LIMITED REMEDY. 7. Changes and Termination. At its discretion, without notice or liability to Customer, New Relic may (a) update, modify or discontinue the Software; (b) modify these Terms, effective upon posting; or (c) terminate or suspend these Terms or Customer’s access to the Software. If Customer does not agree with any modification, its sole remedy is to terminate its use of the Software. Upon any termination or suspension, Customer must stop using the Software. 8. Third Party Code. The Software may contain or be provided with third party code (including code which may be made available to Customer in source code form). A list of third party code and ownership, use, warranty and modification rights with respect to such code may be identified in the documentation or provided by New Relic upon Customer’s written request. New Relic is not responsible for applications and services not licensed by New Relic. 9. Export Compliance. Customer acknowledges that the Software is subject to export restrictions by the U.S. government and import restrictions by certain foreign governments. Customer shall not remove or export from the U.S. or allow the export or re-export of any part of the Software or any direct product thereof: (a) into (or to a national or resident of) any embargoed or terrorist-supporting country; (b) to anyone on the U.S. Commerce Department’s Table of Denial Orders or U.S. Treasury Department’s list of Specially Designated Nationals; (c) to any country to which such export or re-export is restricted or prohibited, or as to which the U.S. government or any agency thereof requires an export license or other governmental approval at the time of export or re-export without first obtaining such license or approval; or (d) otherwise in violation of any export or import restrictions, laws or regulations of any U.S. or foreign agency or authority. Customer agrees to the foregoing and warrants that Customer is not located in, under the control of, or a national or resident of any such prohibited country or on any such prohibited party list. 10. Government End-Users. The Software is commercial computer software. If Customer is an entity of the U.S. government, the use, duplication, reproduction, release, modification, disclosure or transfer of the Software, or any related documentation of any kind, is restricted by a license agreement or by these Terms in accordance with Federal Acquisition Regulation 12.212 for civilian purposes and Defense Federal Acquisition Regulation Supplement 227.7202 for military purposes. The Software was developed fully at private expense. All other use is prohibited. 11. General. These Terms will be governed by and construed under the laws of the State of California and the U.S. without regard to conflicts of law provisions thereof, and without regard to the United Nations Convention on the International Sale of Goods. The jurisdiction and venue for actions arising out of or relating to these Terms shall be in the state and federal courts in San Francisco, California. The parties are independent contractors. Customer may not assign these Terms without New Relic’s prior written consent and any attempt to do so will be void; New Relic may assign these Terms freely to any party without Customer’s consent. If any provision of these Terms is held by a court of competent jurisdiction to be unenforceable or invalid for any reason, that provision shall be limited to the minimum extent necessary so that these Terms shall otherwise remain in effect. These Terms are the entire agreement between the parties relating to the Software, and supersede all prior or contemporaneous agreements (oral or written) relating to the Software. Any separate agreement Customer has for New Relic Products does not apply to the Software. Open-source licenses We love open-source software, and use the following in New Relic Diagnostics. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License clbanning/mxj MIT go-yaml/yaml Apache 2.0 StackExchange/wmi MIT go-ole/go-ole MIT shirou/gopsutil BSD-3-Clause shirou/w32 BSD-3-Clause cheggaaa/pb BSD-3-Clause google/uuid BSD-3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 202.45825,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic Diagnostics <em>licenses</em>",
        "sections": "New Relic Diagnostics <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": ") use the Software to develop <em>services</em> or products for sale or include any components of the Software in any product; (d) remove any product identification, proprietary, copyright or other notices in the Software; or (e) publicly disseminate performance <em>information</em> or analysis (including benchmarks"
      },
      "id": "604505ad28ccbc457e2c60b4"
    },
    {
      "sections": [
        "Translation disclaimer"
      ],
      "title": "Translation disclaimer",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Translation"
      ],
      "external_id": "cb23af79521feaaa6193002aef89648a9973cf1d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/translated-documentation/translation-disclaimer/",
      "published_at": "2022-01-08T07:40:57Z",
      "updated_at": "2021-12-09T01:58:23Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The official version of the Documentation is in English. Portions of the New Relic Documentation may be translated for your convenience as shown on this site. Translated content may contain errors, discrepancies, or differences created in the translation, are not binding, and have no legal effect for enforcement or compliance purposes. New Relic does not make any warranty, express or implied, as to the accuracy, reliability, or correctness of any translated Documentation from the English original to any other language. Any guarantees or commitments in your Agreement with New Relic that the Services, products, or other offerings conform or are consistent with the Documentation do not apply to the extent to which the Documentation has been translated.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 142.24936,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " that the <em>Services</em>, products, or other offerings conform or are consistent with the Documentation do not apply to the extent to which the Documentation has been translated."
      },
      "id": "61b162bf196a672b190d46f7"
    }
  ],
  "/docs/licenses/license-information/special-services-licenses/new-relic-diagnostics-licenses": [
    {
      "sections": [
        "Data collector licenses"
      ],
      "title": "Data collector licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "8eeb19ae388c1f4fd6856084084467a037861675",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/data-collector-licenses/",
      "published_at": "2022-01-08T13:09:37Z",
      "updated_at": "2021-03-16T06:20:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Package Licenses annotations Apache 2.0 antlr BSD apache-log4j-extras Apache 2.0 apache-mime4j Apache 2.0 bonecp Apache 2.0 bonecp-provider Apache 2.0 bytelist MIT c3p0 Eclipse 1.0 commons-beanutils Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-digester Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-lang3 Apache 2.0 commons-logging Apache 2.0 commons-math Apache 2.0 commons-pool Apache 2.0 constantine MIT dom4j BSD ehcache Apache 2.0 gson Apache 2.0 guava Apache 2.0 hamcrest-core BSD-3-Clause hibernate-commons-annotations GNU LGPL 2.1 hibernate-core GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 hibernate-c3p0 GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 hystrix Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 jaffl GNU LGPL 3.0 java-driver Apache 2.0 javassist MPL 1.1, GNU GPL 2.1, Apache 2.0 javax.servlet Apache 2.0, Eclipse 1.0 javax.servlet-api CDDL + GPLv2 with classpath exception jboss-logging GNU LGPL 2.1 jboss-transaction-api_1.1_spec GNU LGPL 2.1 jcodings Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 jedis MIT jetty Apache 2.0, Eclipse 1.0 jffi GNU LGPL 3.0 jline BSD 2-Clause joni MIT jnr-netdb GNU LGPL 3.0 jnr-posix Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 joda-time Apache 2.0 jopt-simple MIT json-simple Apache 2.0 jsr305 Apache 2.0 junit Common Public License 1.0 kafka Apache 2.0 libthrift Apache 2.0 log4j Apache 2.0 logback-classic Eclipse 1.0 logback-core Eclipse 1.0 metrics-annotation Apache 2.0 mockito-all MIT msgpack Apache 2.0 mysql-connector-java GNU GPL 2 reflections WTFPL scala-library Apache 2.0 scannotation Apache 2.0 slf4j-api MIT slf4j-over-log4j MIT snakeyaml Apache 2.0 snappy Apache 2.0 syslog4j LGPL xml-apis Apache 2.0 yamlbeans MIT zkclient Apache 2.0 zookeeper Apache 2.0",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 202.52867,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Data collector <em>licenses</em>",
        "sections": "Data collector <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Package <em>Licenses</em> annotations"
      },
      "id": "603eb41d64441fd7f74e8893"
    },
    {
      "sections": [
        "New Relic Priority Support"
      ],
      "title": "New Relic Priority Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "f700b8b349627e66540b23ed020c7d9a46e19580",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-priority-support/",
      "published_at": "2022-01-08T13:09:37Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Priority Support document as a PDF (172 KB). The services described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 202.52713,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Priority Support document as a PDF (172 KB). The <em>services</em> described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative."
      },
      "id": "603ea90ae7b9d2e9d42a07d1"
    },
    {
      "sections": [
        "Translation disclaimer"
      ],
      "title": "Translation disclaimer",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Translation"
      ],
      "external_id": "cb23af79521feaaa6193002aef89648a9973cf1d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/translated-documentation/translation-disclaimer/",
      "published_at": "2022-01-08T07:40:57Z",
      "updated_at": "2021-12-09T01:58:23Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The official version of the Documentation is in English. Portions of the New Relic Documentation may be translated for your convenience as shown on this site. Translated content may contain errors, discrepancies, or differences created in the translation, are not binding, and have no legal effect for enforcement or compliance purposes. New Relic does not make any warranty, express or implied, as to the accuracy, reliability, or correctness of any translated Documentation from the English original to any other language. Any guarantees or commitments in your Agreement with New Relic that the Services, products, or other offerings conform or are consistent with the Documentation do not apply to the extent to which the Documentation has been translated.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 142.24936,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " that the <em>Services</em>, products, or other offerings conform or are consistent with the Documentation do not apply to the extent to which the Documentation has been translated."
      },
      "id": "61b162bf196a672b190d46f7"
    }
  ],
  "/docs/licenses/license-information/special-services-licenses/new-relic-priority-support": [
    {
      "sections": [
        "Data collector licenses"
      ],
      "title": "Data collector licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "8eeb19ae388c1f4fd6856084084467a037861675",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/data-collector-licenses/",
      "published_at": "2022-01-08T13:09:37Z",
      "updated_at": "2021-03-16T06:20:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Package Licenses annotations Apache 2.0 antlr BSD apache-log4j-extras Apache 2.0 apache-mime4j Apache 2.0 bonecp Apache 2.0 bonecp-provider Apache 2.0 bytelist MIT c3p0 Eclipse 1.0 commons-beanutils Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-digester Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-lang3 Apache 2.0 commons-logging Apache 2.0 commons-math Apache 2.0 commons-pool Apache 2.0 constantine MIT dom4j BSD ehcache Apache 2.0 gson Apache 2.0 guava Apache 2.0 hamcrest-core BSD-3-Clause hibernate-commons-annotations GNU LGPL 2.1 hibernate-core GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 hibernate-c3p0 GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 hystrix Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 jaffl GNU LGPL 3.0 java-driver Apache 2.0 javassist MPL 1.1, GNU GPL 2.1, Apache 2.0 javax.servlet Apache 2.0, Eclipse 1.0 javax.servlet-api CDDL + GPLv2 with classpath exception jboss-logging GNU LGPL 2.1 jboss-transaction-api_1.1_spec GNU LGPL 2.1 jcodings Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 jedis MIT jetty Apache 2.0, Eclipse 1.0 jffi GNU LGPL 3.0 jline BSD 2-Clause joni MIT jnr-netdb GNU LGPL 3.0 jnr-posix Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 joda-time Apache 2.0 jopt-simple MIT json-simple Apache 2.0 jsr305 Apache 2.0 junit Common Public License 1.0 kafka Apache 2.0 libthrift Apache 2.0 log4j Apache 2.0 logback-classic Eclipse 1.0 logback-core Eclipse 1.0 metrics-annotation Apache 2.0 mockito-all MIT msgpack Apache 2.0 mysql-connector-java GNU GPL 2 reflections WTFPL scala-library Apache 2.0 scannotation Apache 2.0 slf4j-api MIT slf4j-over-log4j MIT snakeyaml Apache 2.0 snappy Apache 2.0 syslog4j LGPL xml-apis Apache 2.0 yamlbeans MIT zkclient Apache 2.0 zookeeper Apache 2.0",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 202.52867,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Data collector <em>licenses</em>",
        "sections": "Data collector <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Package <em>Licenses</em> annotations"
      },
      "id": "603eb41d64441fd7f74e8893"
    },
    {
      "sections": [
        "New Relic Diagnostics licenses",
        "Proprietary license",
        "New Relic Diagnostics license terms",
        "Open-source licenses"
      ],
      "title": "New Relic Diagnostics licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "6d86f8f63d372be98270fadc8634f9da8da2a893",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-diagnostics-licenses/",
      "published_at": "2022-01-08T13:10:19Z",
      "updated_at": "2021-03-13T03:25:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Proprietary license New Relic Diagnostics license terms These New Relic Diagnostics License Terms (“Terms”) set forth the terms and conditions under which you (“Customer”) may use New Relic Diagnostics (the “Software”), as made available by New Relic, Inc. (“New Relic”). By clicking “accept” or downloading or using the Software, you agree to be bound by these Terms. If you are agreeing to these Terms on behalf of your company, then “Customer” means your company and you are binding your company to these Terms. 1. License Grant. Subject to all of the terms and conditions of these Terms, New Relic grants Customer a limited, non-exclusive, non-transferable, non-sublicensable license to use the Software in accordance with its documentation in support of Customer’s use of New Relic products to which Customer has a separate subscription (“New Relic Products”). Unless otherwise specified, there is no fee for use of the Software. 2. License Restrictions. Customer will not (a) sell, rent, sublicense, transfer, time-share or otherwise provide access to any copies of the Software, or portions thereof, to a third party; (b) modify, decompile, disassemble or reverse engineer the Software; (c) use the Software to develop services or products for sale or include any components of the Software in any product; (d) remove any product identification, proprietary, copyright or other notices in the Software; or (e) publicly disseminate performance information or analysis (including benchmarks) relating to the Software. 3. Ownership. Except for the limited license rights expressly provided herein, New Relic and its suppliers have and will retain all right, title and interest in and to the Software, and all copies, updates, modifications and derivative works thereof. Customer acknowledges that it is obtaining only a limited license right to the Software and no ownership rights are being conveyed to Customer under these Terms or otherwise. 4. Usage Data. Customer agrees that New Relic and its affiliates have the right to collect Usage Data from Customer through the Software and use Usage Data to support, operate and improve New Relic products and services and for other lawful business purposes. “Usage Data” means diagnostics data related to the use of the Software with New Relic Products, including, without limitation, configuration information, details on diagnostics tasks, New Relic account and application ID numbers and New Relic license keys. For clarity, Usage Data is not considered “Customer Data”, “your Data” or any other similar term as used in any applicable subscription or license agreement for New Relic Products. At Customer’s election, Customer may disable the collection of Usage Data as described in the Software documentation. 5. Disclaimers. New Relic is not obligated to provide any support or maintenance for the Software. ALL USE OF SOFTWARE IS AT CUSTOMER’S OWN RISK. SOFTWARE IS PROVIDED “AS IS,” WITH ALL FAULTS AND WITHOUT WARRANTY OF ANY KIND, EXPRESS, IMPLIED OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, WARRANTIES OF TITLE OR NON-INFRINGEMENT. CUSTOMER MAY HAVE OTHER STATUTORY RIGHTS, HOWEVER, THE DURATION OF STATUTORILY REQUIRED WARRANTIES, IF ANY, WILL BE LIMITED TO THE FULLEST EXTENT PERMITTED BY LAW. 6. Limitation of Liability. TO THE FULLEST EXTENT PERMITTED BY LAW, IN NO EVENT WILL NEW RELIC BE LIABLE FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES OF ANY KIND, REGARDLESS OF THE FORM OF ACTION, WHETHER IN CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY OR OTHERWISE, EVEN IF INFORMED OF THE POSSIBILITY OF SUCH DAMAGES IN ADVANCE. TO THE EXTENT ANY OF THE ABOVE LIMITATIONS ARE NOT ENFORCEABLE AT APPLICABLE LAW, NEW RELIC’S ENTIRE LIABILITY TO CUSTOMER UNDER THESE TERMS WILL NOT EXCEED $50. THESE LIMITATIONS ON LIABILITY ARE A FUNDAMENTAL BASIS OF THE BARGAIN AND NEW RELIC WOULD NOT BE ABLE TO PROVIDE THE SOFTWARE WITHOUT SUCH LIMITATIONS. THESE LIMITATIONS ON LIABILITY WILL APPLY NOTWITHSTANDING ANY FAILURE OF ESSENTIAL PURPOSE OF ANY LIMITED REMEDY. 7. Changes and Termination. At its discretion, without notice or liability to Customer, New Relic may (a) update, modify or discontinue the Software; (b) modify these Terms, effective upon posting; or (c) terminate or suspend these Terms or Customer’s access to the Software. If Customer does not agree with any modification, its sole remedy is to terminate its use of the Software. Upon any termination or suspension, Customer must stop using the Software. 8. Third Party Code. The Software may contain or be provided with third party code (including code which may be made available to Customer in source code form). A list of third party code and ownership, use, warranty and modification rights with respect to such code may be identified in the documentation or provided by New Relic upon Customer’s written request. New Relic is not responsible for applications and services not licensed by New Relic. 9. Export Compliance. Customer acknowledges that the Software is subject to export restrictions by the U.S. government and import restrictions by certain foreign governments. Customer shall not remove or export from the U.S. or allow the export or re-export of any part of the Software or any direct product thereof: (a) into (or to a national or resident of) any embargoed or terrorist-supporting country; (b) to anyone on the U.S. Commerce Department’s Table of Denial Orders or U.S. Treasury Department’s list of Specially Designated Nationals; (c) to any country to which such export or re-export is restricted or prohibited, or as to which the U.S. government or any agency thereof requires an export license or other governmental approval at the time of export or re-export without first obtaining such license or approval; or (d) otherwise in violation of any export or import restrictions, laws or regulations of any U.S. or foreign agency or authority. Customer agrees to the foregoing and warrants that Customer is not located in, under the control of, or a national or resident of any such prohibited country or on any such prohibited party list. 10. Government End-Users. The Software is commercial computer software. If Customer is an entity of the U.S. government, the use, duplication, reproduction, release, modification, disclosure or transfer of the Software, or any related documentation of any kind, is restricted by a license agreement or by these Terms in accordance with Federal Acquisition Regulation 12.212 for civilian purposes and Defense Federal Acquisition Regulation Supplement 227.7202 for military purposes. The Software was developed fully at private expense. All other use is prohibited. 11. General. These Terms will be governed by and construed under the laws of the State of California and the U.S. without regard to conflicts of law provisions thereof, and without regard to the United Nations Convention on the International Sale of Goods. The jurisdiction and venue for actions arising out of or relating to these Terms shall be in the state and federal courts in San Francisco, California. The parties are independent contractors. Customer may not assign these Terms without New Relic’s prior written consent and any attempt to do so will be void; New Relic may assign these Terms freely to any party without Customer’s consent. If any provision of these Terms is held by a court of competent jurisdiction to be unenforceable or invalid for any reason, that provision shall be limited to the minimum extent necessary so that these Terms shall otherwise remain in effect. These Terms are the entire agreement between the parties relating to the Software, and supersede all prior or contemporaneous agreements (oral or written) relating to the Software. Any separate agreement Customer has for New Relic Products does not apply to the Software. Open-source licenses We love open-source software, and use the following in New Relic Diagnostics. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License clbanning/mxj MIT go-yaml/yaml Apache 2.0 StackExchange/wmi MIT go-ole/go-ole MIT shirou/gopsutil BSD-3-Clause shirou/w32 BSD-3-Clause cheggaaa/pb BSD-3-Clause google/uuid BSD-3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 202.45825,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic Diagnostics <em>licenses</em>",
        "sections": "New Relic Diagnostics <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": ") use the Software to develop <em>services</em> or products for sale or include any components of the Software in any product; (d) remove any product identification, proprietary, copyright or other notices in the Software; or (e) publicly disseminate performance <em>information</em> or analysis (including benchmarks"
      },
      "id": "604505ad28ccbc457e2c60b4"
    },
    {
      "sections": [
        "Translation disclaimer"
      ],
      "title": "Translation disclaimer",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Translation"
      ],
      "external_id": "cb23af79521feaaa6193002aef89648a9973cf1d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/translated-documentation/translation-disclaimer/",
      "published_at": "2022-01-08T07:40:57Z",
      "updated_at": "2021-12-09T01:58:23Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The official version of the Documentation is in English. Portions of the New Relic Documentation may be translated for your convenience as shown on this site. Translated content may contain errors, discrepancies, or differences created in the translation, are not binding, and have no legal effect for enforcement or compliance purposes. New Relic does not make any warranty, express or implied, as to the accuracy, reliability, or correctness of any translated Documentation from the English original to any other language. Any guarantees or commitments in your Agreement with New Relic that the Services, products, or other offerings conform or are consistent with the Documentation do not apply to the extent to which the Documentation has been translated.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 142.24934,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " that the <em>Services</em>, products, or other offerings conform or are consistent with the Documentation do not apply to the extent to which the Documentation has been translated."
      },
      "id": "61b162bf196a672b190d46f7"
    }
  ],
  "/docs/licenses/license-information/usage-plans/new-relic-one-usage-plan-descriptions": [
    {
      "sections": [
        "Product-based pricing usage and New Relic Platform Pricing Usage Plan",
        "Important",
        "Usage Plan: New Relic Platform Pricing",
        "Eligible Services",
        "Eligible Services (Partners)",
        "Subscriptions with indeterminate pricing or usage quantities",
        "Product Usage Ratio"
      ],
      "title": "Product-based pricing usage and New Relic Platform Pricing Usage Plan",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Usage plans"
      ],
      "external_id": "e2686dc773c4e844544ce633a9a41a16f15edf5a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/usage-plans/product-based-pricing-usage-new-relic-platform-pricing-usage-plan/",
      "published_at": "2022-01-08T09:17:02Z",
      "updated_at": "2021-11-13T14:45:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This doc explains our original product-based pricing model. For more about pricing changes, see Overview of pricing changes. The following provisions are applicable to existing customers (i) that have existing Terms, and (ii) that to the extent its subscription to the New Relic products reference the usage plans set forth below or where a subscription has indeterminate product pricing or usage quantities. New customers are eligible for New Relic One pricing as described here. Capitalized terms not defined below shall take on the meaning set forth in such New Relic order form. Usage Plan: New Relic Platform Pricing The following Eligible Services with New Relic are products that are referenced in specific order forms for New Relic Platform Pricing. For more information, contact your New Relic account representative. Eligible Services Eligible Services Per Unit Unit of Measure Monthly Standard Fee Rate New Relic APM Pro CU Annual* 10,000 Compute Units $166.70 New Relic APM Pro Host Annual* 1 Hosts $149 New Relic Insights Pro Annual 50,000,000 Events $165 New Relic Infrastructure Pro Annual 10,000 Compute Units $12 New Relic Browser Pro Annual 500,000 Page Views $149 New Relic Logs Annual - 8 Days* * 1 Per GB Daily $55 New Relic Logs Annual - 15 Days* * 1 Per GB Daily $65 New Relic Logs Annual - 30 Days* * 1 Per GB Daily $75 New Relic Metrics Annual 1,000 Data Points per Minute $25 New Relic Mobile Enterprise Annual 50,000 Total Users $499 New Relic Serverless for AWS Lambda Annual 1,000,000 AWS Lambda Events $15 New Relic Synthetics Pro Annual 10,000 Checks $69 New Relic Synthetics private locations (for New Relic Synthetics Pro Annual product) Fixed fee N/A $1,000 New Relic Traces Annual 1,000,000 Spans $1 New Relic AI Incident Intelligence Annual 1,000 Incident Events $500 New Relic AI Proactive Detection Annual 1,000,000,000 App Transactions $250 * Customer may utilize either New Relic APM Pro Annual or New Relic APM Pro CUs Annual, but not both. Customer will be provisioned New Relic APM Pro CUs Annual by default, unless otherwise stated in the relevant Order Form. * * Customer may utilize only one of New Relic Logs Annual - 8 Days, New Relic Logs Annual - 15 Days, or New Relic Logs Annual - 30 days. For the avoidance of doubt, Customer may not utilize New Relic Logs Annual with varying numbers of days retention concurrently. Customer will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. For more information about units of measures, see Product definitions. Eligible Services (Partners) Eligible services for New Relic Partners Measurement New Relic APM Pro CU Annual* Sold in Units of 8,900 Compute Units New Relic APM Pro Host Annual* Sold in Units of 1 Host New Relic Browser Pro Annual Sold in Units of 500,000 Page Views New Relic Infrastructure Pro Annual Sold in Units of 125,000 Compute Units New Relic Insights Pro Annual Sold in Units of 45,000,000 Events New Relic Logs Annual - 8 Days* * Sold in Units of 3 GB Daily New Relic Logs Annual - 15 Days* * Sold in Units of 2.5 GB Daily New Relic Logs Annual - 30 Days* * Sold in Units of 2 GB Daily New Relic Metrics Annual Sold in Units of 6,000 Data Points per Minute New Relic Mobile Enterprise Annual Sold in Units of 15,000 Total Users New Relic Serverless for AWS Lambda Annual Sold in Units of 10,000,000 AWS Lambda Events New Relic Synthetics Pro with Private Locations Annual Sold in Units of 22,000 Checks New Relic Traces Annual Sold in Units of 149,000,000 Spans * Customer and/or Partner may utilize either New Relic APM Pro Annual or New Relic APM Pro CUs Annual, but not both. Customer and/or Partner will be provisioned New Relic APM Pro CUs Annual by default, unless otherwise stated in the relevant Order Form. * * Customer and/or Partner may utilize only one of New Relic Logs Annual - 8 Days, New Relic Logs Annual - 15 Days, or New Relic Logs Annual - 30 days. For the avoidance of doubt, Customer may not utilize New Relic Logs Annual with varying numbers of days retention concurrently. Customer and/or Partner will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. Subscriptions with indeterminate pricing or usage quantities Product Usage Ratio Where a Customer subscription to the Products contain no specific pricing or quantities, Product-specific usage for a specific calendar month shall be determined through the calculation of the following ratio (the “Product Usage Ratio”) where: (1) the numerator shall be the Product specific usage for such calendar month multiplied by such Product’s list price, and (2) the denominator shall be the the aggregate Customer usage of all Products during such calendar month multiplied by all such Product(s) list price. The Product Usage Ratio shall be the percentage of usage for a specific Product for a calendar month period, or if usage cannot be measured for a period, the last Product Usage Ratio that can be calculated shall be assumed constant.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.88782,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Product-based pricing <em>usage</em> and New Relic Platform Pricing <em>Usage</em> <em>Plan</em>",
        "sections": "Product-based pricing <em>usage</em> and New Relic Platform Pricing <em>Usage</em> <em>Plan</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " reference the <em>usage</em> <em>plans</em> set forth below or where a subscription has indeterminate product pricing or <em>usage</em> quantities. New customers are eligible for New Relic One pricing as described here. Capitalized terms not defined below shall take on the meaning set forth in such New Relic order form. <em>Usage</em>"
      },
      "id": "603ea32a28ccbc7e22eba768"
    },
    {
      "sections": [
        "Translation disclaimer"
      ],
      "title": "Translation disclaimer",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Translation"
      ],
      "external_id": "cb23af79521feaaa6193002aef89648a9973cf1d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/translated-documentation/translation-disclaimer/",
      "published_at": "2022-01-08T07:40:57Z",
      "updated_at": "2021-12-09T01:58:23Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The official version of the Documentation is in English. Portions of the New Relic Documentation may be translated for your convenience as shown on this site. Translated content may contain errors, discrepancies, or differences created in the translation, are not binding, and have no legal effect for enforcement or compliance purposes. New Relic does not make any warranty, express or implied, as to the accuracy, reliability, or correctness of any translated Documentation from the English original to any other language. Any guarantees or commitments in your Agreement with New Relic that the Services, products, or other offerings conform or are consistent with the Documentation do not apply to the extent to which the Documentation has been translated.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 142.24934,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>"
      },
      "id": "61b162bf196a672b190d46f7"
    },
    {
      "sections": [
        "New Relic One pricing model: Frequently asked questions",
        "Frequently asked questions",
        "Q: Where can I find more information about New Relic One and related products?",
        "Q: Is there a limit on the number of concurrent sessions or IP addresses that may be used with a user account?",
        "Q: What terms govern my use of the Products?",
        "Q: How are the number of full users (also referred to as Monthly Provisioned Users, or full platform users) calculated for each month?",
        "Q: Can I mix and match New Relic One Standard, Pro, and Enterprise users in a billing account?",
        "Q: Can Event extended retention, additional Synthetics Checks, or additional New Relic Edge data be added to my account?",
        "Q: What is included in New Relic's Free Tier of Products?",
        "Q: How does the ‘Free Tier’ impact my pay-as-you-go (PAYG) or Annual Pool of Funds (APOF) subscription?"
      ],
      "title": "New Relic One pricing model: Frequently asked questions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "FAQ"
      ],
      "external_id": "cc702038ec7eb4983faa152108a4a233bd285264",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/faq/new-relic-one-pricing-plan-frequently-asked-questions/",
      "published_at": "2022-01-08T09:13:14Z",
      "updated_at": "2021-11-25T07:37:46Z",
      "document_type": "page",
      "popularity": 1,
      "body": "These are some frequently asked questions about the New Relic One pricing model that arise for agreement-level language. This is meant to serve as a supplement to the main New Relic One pricing docs. Frequently asked questions These FAQs are meant to serve as a supplement to the main New Relic One pricing docs. Q: Where can I find more information about New Relic One and related products? A: For a high level description of user entitlements for our features, see our pricing page. For more in-depth information, see New Relic One pricing. Q: Is there a limit on the number of concurrent sessions or IP addresses that may be used with a user account? A: Yes. See Manage users. Q: What terms govern my use of the Products? A: If a Customer has paid New Relic during a rolling 12-month period, then Customer’s usage of the Products is covered by the Paid Terms of Service. If a Customer has not paid New Relic during a 12-month period, then Customer’s usage of the Products is covered by the Unpaid Terms of Service. Q: How are the number of full users (also referred to as Monthly Provisioned Users, or full platform users) calculated for each month? A: Please see Calculation details. Q: Can I mix and match New Relic One Standard, Pro, and Enterprise users in a billing account? A: No. For the New Relic One pricing model, you can only have one subscription per organization (group of accounts that share the same billing ID). Q: Can Event extended retention, additional Synthetics Checks, or additional New Relic Edge data be added to my account? A: For additional retention for Events or Logs beyond the Standard Data Retention, contact your New Relic account executive. Note: Minimum requirement for Extended Retention requires an Annual Pool of Funds subscription. Q: What is included in New Relic's Free Tier of Products? A: Please see this pricing doc for a description of what's included. Customer’s use of the Free Tier shall be governed by the terms and conditions described in the Unpaid Terms of Service. If Customer’s usage exceeds the Free Tier, Customer is fully responsible for fees incurred in excess of the Free Tier as described in the “Pay-as-you-go” program and as described in the Paid Terms of Service. Q: How does the ‘Free Tier’ impact my pay-as-you-go (PAYG) or Annual Pool of Funds (APOF) subscription? A: The Free Tier usage will be deducted automatically from the Monthly Product Usage for Pay-as-you-go or Annual pool of funds subscriptions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 134.17116,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Q: Where can I find more <em>information</em> about New Relic One and related products?",
        "tags": "<em>License</em> <em>information</em>",
        "body": " pricing docs. Q: Where can I find more <em>information</em> about New Relic One and related products? A: For a high level description of user entitlements for our features, see our pricing page. For more in-depth <em>information</em>, see New Relic One pricing. Q: Is there a limit on the number of concurrent sessions"
      },
      "id": "6044e6e5196a67b568960f3e"
    }
  ],
  "/docs/licenses/license-information/usage-plans/product-based-pricing-usage-new-relic-platform-pricing-usage-plan": [
    {
      "sections": [
        "New Relic One usage plan descriptions",
        "Pay As You Go",
        "Annual Pool of Funds",
        "Applicable Invoicing and Order Terms",
        "User Accounts",
        "New Relic One Pro and Enterprise Service Level Availability Commitment",
        "New Relic One Pro and Enterprise Support Plans",
        "Free tier, ‘lite’, no-charge, preview access, New Relic One - Data, New Relic One - Standard User subscriptions",
        "Separate Platforms"
      ],
      "title": "New Relic One usage plan descriptions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Usage plans"
      ],
      "external_id": "c18e1c6c294914c28bba48f9de025333210ed254",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/usage-plans/new-relic-one-usage-plan-descriptions/",
      "published_at": "2022-01-08T09:17:02Z",
      "updated_at": "2021-11-13T18:17:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document is about the New Relic One pricing model. For an explanation of how that plan works, see New Relic One pricing. The document below goes into license-level details. The Usage Plan applies to (i) your Pay As You Go subscription, or (ii) your Commitment Term for the Annual Pool of Funds subscription (see a description of these two plans). New Relic may modify the Usage Plan from time to time. Any changes to the Usage Plan will become effective immediately for changes that provide a benefit or right to the Customer, all other changes will become effective if Customer assents or upon any new or renewal Commitment Term. Usage Plan - Effective as October 21, 2021: The Order and Usage Plan may contain defined terms that are denoted by capitalization. In the event that a capitalized term is not defined in either the Order or the Usage Plan, such terms shall have the meaning set forth in the New Relic One pricing definitions page. Pay As You Go By electing and subscribing to the Pay As You Go subscription model (“Pay As You Go” or “PAYG”), Customer commits to paying for the New Relic Products on a month-to-month consumption basis. Monthly Product Usage will be invoiced regardless if a PO is required or not. Any Customer dispute to Monthly Product Usage from the prior month must be in good-faith and received by New Relic in writing within three (3) business days of the start of the next month or such dispute notice will be considered invalid. The dispute notice will set forth in reasonable detail the information concerning the disputed charges. The parties will use good-faith efforts to promptly resolve any disputed charges. Customer’s usage of the Products in excess of the Free Tier each month shall be billed in arrears on the first business day of the following month based on the Customer’s Per Unit usage of each Product each month multiplied by the corresponding rates set forth in an Order and summed (“Monthly Product Usage”). Annual Pool of Funds By electing and subscribing to the Annual Pool of Funds subscription model (“Annual Pool of Funds” or “APoF”) for the Commitment Term, Customer commits in an Order to: (i) paying the Commitment Fee amounts described in the Subscription table and any additional commitment fees set forth; and (ii) the Monthly Discounted Fee Rates applying to Customer’s Monthly Product Usage. New Relic will invoice the Commitment Fee as per the ‘Billing Terms’ described in an Order. On a monthly cadence during the Commitment Term, Customer’s Per Unit usage of the Products will be multiplied by the corresponding Monthly Discounted Rate and summed (“Monthly Product Usage”). Monthly Product Usage will be deducted from the Commitment Fee amounts that are paid in advance. If Monthly Product Usage exceeds any such remaining unconsumed amounts, Customer will be invoiced for the difference (“Additional Usage”), including for any Monthly Product Usage during the last month of the Commitment Term. Payment of such invoices will be governed as set forth in the Terms. Any Customer dispute to Monthly Product Usage from the prior month must be in good-faith and received by New Relic in writing within three (3) business days of the start of the next month (including for any Monthly Product Usage for the last month of the Commitment Term) or such dispute notice will be considered invalid. The dispute notice will set forth in reasonable detail the information concerning the disputed charges. The parties will use good-faith efforts to promptly resolve any disputed charges. Any unconsumed balances from the Customer payment of each annual Commitment Fee and any additional payments, if applicable, as set out in an Order will expire and lapse at the end of each year of the Commitment Term. Applicable Invoicing and Order Terms All amounts stated in an Order are non-cancelable payment obligations of the Customer for the Commitment Term regardless of usage. Any fees paid are non-refundable and do not represent a deposit for, or a credit towards, the purchase of other products not specified in an Order or for any purchase after the Commitment Term. Customer acknowledges that a final payment for the full outstanding amount of any remaining unpaid Commitment Fee and/or Monthly Product Usage Fee may be invoiced upon each anniversary date of the term start date. Tax will be added where applicable. New Relic may review Customer's use of the Products at any time. If New Relic identifies any Customer usage of the Product(s) that is not in accordance with the Terms or Documentation, New Relic may suspend such unauthorized usage. All existing purchases and related pricing in effect prior to the execution of an Order shall remain in force. Unless otherwise stated in an Order, an Order does not modify or amend any existing purchases. Additional future products or quantities are not subject to promotional pricing unless otherwise stated in an Order. In the event that a Customer indicates in an Order that it requires a purchase order (“PO”) for its subscription, Customer agrees to provide the required PO prior to the provisioning of the Products. If a Customer does not indicate a PO is required, Customer agrees that New Relic may issue invoice(s) and is entitled to such payment without a PO reference. All Additional Usage fees will be invoiced regardless of a PO requirement. The Product(s) are deemed accepted upon its provisioning. User Accounts Use of the Products require Customer users to create Login Credentials. Customer user Login Credential information must be accurate, current, and complete. New Relic’s use and collection of Login Credentials (in accordance with its General Data Privacy Notice) is for account and product management and support of its customers. Customer and Customer users must abide by the New Relic Acceptable Use Policy (AUP) and Login Credentials may not be shared. Each Customer user must have their own user account. Entry into an Order indicates your agreement that the amount of provisioned users (at the rate specified in the Order) applies in lieu of and supersedes any other amount of users of the Products that may be specified in the agreement between Customer and New Relic. New Relic One Pro and Enterprise Service Level Availability Commitment With a subscription to \"New Relic One - Pro Users\" or \"New Relic One - Enterprise Users,\" you agree that during the Commitment Term the applicable service level availability commitment set forth on the ‘Service level availability commitment’ page in the Documentation shall apply to the Products. For clarity, if your agreement with New Relic contains a different service level availability commitment or remedies, the above does not apply to your subscription to the \"New Relic One - Pro Users\" or \"New Relic One - Enterprise Users\". If you subscribe to any other New Relic Products with New Relic One pricing, any service level availability commitment or related remedies contained within your agreement with New Relic are vacated and nullified, and New Relic will use commercially reasonable efforts to make \"New Relic One - Standard Users\" available in line with industry standards. New Relic One Pro and Enterprise Support Plans With a subscription to New Relic One Users, New Relic provides an updated support plan commitment. By subscribing to New Relic One Users, you agree that during the Commitment Term the applicable Support Plan set forth on the ‘Support plan’ page in the Documentation shall apply in lieu of, and supersedes and replaces, any other support related commitments that may be contained within your agreement with New Relic. For New Relic K.K. customers (Japan), the above does not currently apply to the support offerings provided by New Relic to you. Free tier, ‘lite’, no-charge, preview access, New Relic One - Data, New Relic One - Standard User subscriptions If you are using New Relic’s Products in the free tier only, or on a no-charge, or a ‘lite’ or ‘preview access’ basis you agree that the Unpaid Terms of Service will apply to such Product usage and replace and supersede any other terms. In addition, if your subscription contains the \"New Relic One - Standard Users Product,\" you agree that the Paid Terms of Service will apply to your subscription to the Products set forth in an Order and replace and supersede any other terms. Separate Platforms Customer access to any separate platforms for purposes of applicability of governing terms, such as Community Cloud for Pixie or CodeStream, is subject to their respective terms of service. If you enable a separate platform with New Relic One, New Relic may access and exchange Customer Data with the separate platform on your behalf. For clarity, such other platforms do not form part of the New Relic One offering and, unless otherwise expressly provided in the terms and then only to the extent provided, New Relic One warranties, indemnities, etc., do not apply to use of any other platforms for purposes of applicability of governing terms. Separate platforms are not intended to meet any legal obligations for Prohibited Data uses. Further, unless otherwise agreed to in writing by New Relic, (a) for users of separate platforms, New Relic is neither a Business Associate or a subcontractor (as defined under HIPAA), nor a payment card processor, and (b) separate platforms are neither HIPAA nor PCI DSS compliant.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.96326,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic One <em>usage</em> <em>plan</em> descriptions",
        "sections": "New Relic One <em>usage</em> <em>plan</em> descriptions",
        "tags": "<em>License</em> <em>information</em>",
        "body": "This document is about the New Relic One pricing model. For an explanation of how that <em>plan</em> works, see New Relic One pricing. The document below goes into <em>license</em>-level details. The <em>Usage</em> <em>Plan</em> applies to (i) your Pay As You Go subscription, or (ii) your Commitment Term for the Annual Pool of Funds"
      },
      "id": "6044e74ee7b9d2a4515799c8"
    },
    {
      "sections": [
        "Translation disclaimer"
      ],
      "title": "Translation disclaimer",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Translation"
      ],
      "external_id": "cb23af79521feaaa6193002aef89648a9973cf1d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/translated-documentation/translation-disclaimer/",
      "published_at": "2022-01-08T07:40:57Z",
      "updated_at": "2021-12-09T01:58:23Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The official version of the Documentation is in English. Portions of the New Relic Documentation may be translated for your convenience as shown on this site. Translated content may contain errors, discrepancies, or differences created in the translation, are not binding, and have no legal effect for enforcement or compliance purposes. New Relic does not make any warranty, express or implied, as to the accuracy, reliability, or correctness of any translated Documentation from the English original to any other language. Any guarantees or commitments in your Agreement with New Relic that the Services, products, or other offerings conform or are consistent with the Documentation do not apply to the extent to which the Documentation has been translated.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 142.24934,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>"
      },
      "id": "61b162bf196a672b190d46f7"
    },
    {
      "sections": [
        "New Relic One pricing model: Frequently asked questions",
        "Frequently asked questions",
        "Q: Where can I find more information about New Relic One and related products?",
        "Q: Is there a limit on the number of concurrent sessions or IP addresses that may be used with a user account?",
        "Q: What terms govern my use of the Products?",
        "Q: How are the number of full users (also referred to as Monthly Provisioned Users, or full platform users) calculated for each month?",
        "Q: Can I mix and match New Relic One Standard, Pro, and Enterprise users in a billing account?",
        "Q: Can Event extended retention, additional Synthetics Checks, or additional New Relic Edge data be added to my account?",
        "Q: What is included in New Relic's Free Tier of Products?",
        "Q: How does the ‘Free Tier’ impact my pay-as-you-go (PAYG) or Annual Pool of Funds (APOF) subscription?"
      ],
      "title": "New Relic One pricing model: Frequently asked questions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "FAQ"
      ],
      "external_id": "cc702038ec7eb4983faa152108a4a233bd285264",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/faq/new-relic-one-pricing-plan-frequently-asked-questions/",
      "published_at": "2022-01-08T09:13:14Z",
      "updated_at": "2021-11-25T07:37:46Z",
      "document_type": "page",
      "popularity": 1,
      "body": "These are some frequently asked questions about the New Relic One pricing model that arise for agreement-level language. This is meant to serve as a supplement to the main New Relic One pricing docs. Frequently asked questions These FAQs are meant to serve as a supplement to the main New Relic One pricing docs. Q: Where can I find more information about New Relic One and related products? A: For a high level description of user entitlements for our features, see our pricing page. For more in-depth information, see New Relic One pricing. Q: Is there a limit on the number of concurrent sessions or IP addresses that may be used with a user account? A: Yes. See Manage users. Q: What terms govern my use of the Products? A: If a Customer has paid New Relic during a rolling 12-month period, then Customer’s usage of the Products is covered by the Paid Terms of Service. If a Customer has not paid New Relic during a 12-month period, then Customer’s usage of the Products is covered by the Unpaid Terms of Service. Q: How are the number of full users (also referred to as Monthly Provisioned Users, or full platform users) calculated for each month? A: Please see Calculation details. Q: Can I mix and match New Relic One Standard, Pro, and Enterprise users in a billing account? A: No. For the New Relic One pricing model, you can only have one subscription per organization (group of accounts that share the same billing ID). Q: Can Event extended retention, additional Synthetics Checks, or additional New Relic Edge data be added to my account? A: For additional retention for Events or Logs beyond the Standard Data Retention, contact your New Relic account executive. Note: Minimum requirement for Extended Retention requires an Annual Pool of Funds subscription. Q: What is included in New Relic's Free Tier of Products? A: Please see this pricing doc for a description of what's included. Customer’s use of the Free Tier shall be governed by the terms and conditions described in the Unpaid Terms of Service. If Customer’s usage exceeds the Free Tier, Customer is fully responsible for fees incurred in excess of the Free Tier as described in the “Pay-as-you-go” program and as described in the Paid Terms of Service. Q: How does the ‘Free Tier’ impact my pay-as-you-go (PAYG) or Annual Pool of Funds (APOF) subscription? A: The Free Tier usage will be deducted automatically from the Monthly Product Usage for Pay-as-you-go or Annual pool of funds subscriptions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 134.17116,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Q: Where can I find more <em>information</em> about New Relic One and related products?",
        "tags": "<em>License</em> <em>information</em>",
        "body": " pricing docs. Q: Where can I find more <em>information</em> about New Relic One and related products? A: For a high level description of user entitlements for our features, see our pricing page. For more in-depth <em>information</em>, see New Relic One pricing. Q: Is there a limit on the number of concurrent sessions"
      },
      "id": "6044e6e5196a67b568960f3e"
    }
  ],
  "/docs/licenses/overview": [
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2022-01-08T09:53:40Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0 © 2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0 © 2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0 © 2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0 © 2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0 © 2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright © 2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright © 2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright © 1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright © 1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 61.61016,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Logs plugin <em>licenses</em>",
        "sections": "Logs plugin <em>licenses</em>",
        "tags": "<em>Licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    },
    {
      "sections": [
        "New Relic APM licenses"
      ],
      "title": "New Relic APM licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "e08100d5a48d87c7769f401957826f627bf29779",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses/",
      "published_at": "2022-01-08T08:26:31Z",
      "updated_at": "2021-03-30T21:22:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view licenses for... See... APM agents docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 59.44503,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic APM <em>licenses</em>",
        "sections": "New Relic APM <em>licenses</em>",
        "tags": "<em>Licenses</em>",
        "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view <em>licenses</em> for... See... APM agents docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;product-or-service-<em>licenses</em>&#x2F;new-relic-apm"
      },
      "id": "603e7895e7b9d24e832a07d5"
    },
    {
      "sections": [
        "New Relic Insights licenses"
      ],
      "title": "New Relic Insights licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Insights"
      ],
      "external_id": "3d0f8f4275af8bcf9f6f3ffb02a2e46ff3fc374f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-insights/new-relic-insights-licenses/",
      "published_at": "2022-01-08T09:52:49Z",
      "updated_at": "2021-03-13T04:04:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in New Relic Insights. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Product Licenses New Relic Insights See insights.newrelic.com/licenses",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 58.594967,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic Insights <em>licenses</em>",
        "sections": "New Relic Insights <em>licenses</em>",
        "tags": "<em>Licenses</em>",
        "body": "We love open-source software, and use the following in New Relic Insights. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Product <em>Licenses</em> New Relic Insights See insights.newrelic.com&#x2F;<em>licenses</em>"
      },
      "id": "6044e84428ccbcf6fa2c60c1"
    }
  ],
  "/docs/licenses/product-or-service-licenses/auto-telemetry-with-pixie/auto-telemetry-with-pixie-licenses": [
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-08T09:17:45Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 150.71141,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "CodeStream <em>licenses</em>",
        "sections": "CodeStream <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. CodeStream license on GitHub CodeStream&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    },
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2022-01-08T09:53:40Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0 © 2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0 © 2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0 © 2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0 © 2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0 © 2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright © 2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright © 2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright © 1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright © 1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.25278,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Logs plugin <em>licenses</em>",
        "sections": "Logs plugin <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    },
    {
      "sections": [
        "New Relic APM licenses"
      ],
      "title": "New Relic APM licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "e08100d5a48d87c7769f401957826f627bf29779",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses/",
      "published_at": "2022-01-08T08:26:31Z",
      "updated_at": "2021-03-30T21:22:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view licenses for... See... APM agents docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 137.69243,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic APM <em>licenses</em>",
        "sections": "New Relic APM <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view <em>licenses</em> for... See... APM agents docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>product</em>-or-<em>service</em>-<em>licenses</em>&#x2F;new-relic-apm"
      },
      "id": "603e7895e7b9d24e832a07d5"
    }
  ],
  "/docs/licenses/product-or-service-licenses/codestream/codestream-licenses": [
    {
      "image": "https://docs.newrelic.com/static/cb27400c917f08f6d6fafbc09337440e/432e7/ErrorOnNR1.png",
      "url": "https://docs.newrelic.com/docs/codestream/how-use-codestream/performance-monitoring/",
      "sections": [
        "Performance monitoring with CodeStream",
        "Discover errors on New Relic One",
        "Discover errors via CodeStream",
        "Error details",
        "Collaborate with CodeStream",
        "Use build SHAs or release tags with CodeStream",
        "Other collaboration tools",
        "Associate your repository"
      ],
      "published_at": "2022-01-08T03:16:25Z",
      "title": "Performance monitoring with CodeStream",
      "updated_at": "2022-01-05T01:37:53Z",
      "type": "docs",
      "external_id": "378ad1d91c35b3c33347ee3cf91afb28620b45f7",
      "document_type": "page",
      "popularity": 1,
      "body": "It’s important to know how your code is performing in production and whether or not it’s generating errors. To help you with this, New Relic CodeStream brings performance monitoring right into your IDE. Discover errors on New Relic One Once you’ve connected CodeStream to your New Relic One account, and you've created one or more workloads with errors inbox on New Relic One, use Open in IDE to see APM errors with stack traces directly in your IDE. When you've connected CodeStream to your New Relic One account, in errors inbox click Open in IDE to see the code that caused the error. Once connected, all of your collaboration work in CodeStream (such as the discussion, assignee, and error status) syncs with New Relic One, where you can continue to collaborate. A typical collaboration session could include developers commenting on code in their IDEs, a DevOps engineer assigning errors in errors inbox, and a development manager following along in Slack. By meeting people in the tools they're already using, New Relic CodeStream shortens the amount of time between error discovery and error resolution. Discover errors via CodeStream In addition to errors inbox, discover errors in your IDE in the CodeStream observability section. In addition to recent errors in your repos, see any specific errors assigned to you. Use CodeStream's observability section to keep up to date with recent and assigned stack trace errors. Error details No matter how you've arrived at an error in your IDE, CodeStream presents all of the error’s details, including the stack trace, and you can collaborate with your teammates to resolve the error. Navigate the stack trace to investigate the issue. Click any frame in the stack trace to jump straight to the corresponding file and line number in your IDE. As you navigate the stack trace, if you come across code that seems like the source of your problem, select it and click the comment icon to start collaborating. Collaborate with CodeStream With CodeStream open, once you've identified the problematic code, select it in your editor and click the comment icon that appears next to it in the CodeStream pane. CodeStream automatically mentions the most recent person to touch the code related to the error, making it easy for you to bring the right people into the discussion. Select code in your editor to add a comment. Assign the error and update its status for better tracking an accountability. Once you’ve identified the problem you can assign the error, either to an existing teammate on CodeStream or to a person suggested based on the repository’s Git commit history. You can update the error’s status from unresolved to resolved or ignored. Use build SHAs or release tags with CodeStream You may see this warning if there's no git reference, either a build SHA or release tag, associated with a specific error. CodeStream uses the git reference to match the specific stack trace error with the version of the code running in the environment that triggered the error. The git reference not configured warning message reads: Assocaite a build SHA or release tag with your errors so that CodeStream can help make sure you're looking at the right version of the code. To configure a git reference set the environment variables for your APM agent. Even without the git reference configured, you can still investigate the error, but you may not be looking at the version of the code that caused it. The git reference not found warning message reads: Your version of the code doesn't match the environment that triggered the error. Fetch the following reference to better investigate the error. If you do have git references configured, but the version of the code you're on locally doesn't contain that reference, CodeStream will let you know so that you can more effectively investigate and resolve the error. CodeStream will also let you know if the error doesn’t have a stack trace associated with it. This happens with older errors when the stack trace has aged out on New Relic One. Other collaboration tools In an error discussion, use the ... More actions dropdown to share the discussion on Slack or Microsoft Teams. Associate your repository If there's no repository associated with CodeStream when you click Open in IDE on an error, CodeStream prompts you to do so. All of the repositories you currently have open in your IDE are listed in the select a repo dropdown. If you don’t see the repository you want listed, open it in your IDE and it will automatically get added to the list. If you’re working with a fork, make sure you select the upstream remote. To avoid having to do this manual association every time you open an error, you can make these associations via your APM agent's environment variables.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 156.40605,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Performance monitoring with <em>CodeStream</em>",
        "sections": "Use build SHAs <em>or</em> release tags with <em>CodeStream</em>",
        "body": "It’s important to know how your <em>code</em> is performing in production and whether or not it’s generating errors. To help you with this, New Relic <em>CodeStream</em> brings performance monitoring right into your IDE. Discover errors on New Relic One Once you’ve connected <em>CodeStream</em> to your New Relic One account"
      },
      "id": "617cbd54e7b9d28f12c0535e"
    },
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2022-01-08T09:53:40Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0 © 2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0 © 2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0 © 2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0 © 2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0 © 2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright © 2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright © 2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright © 1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright © 1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.25095,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Logs plugin <em>licenses</em>",
        "sections": "Logs plugin <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    },
    {
      "sections": [
        "New Relic APM licenses"
      ],
      "title": "New Relic APM licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "e08100d5a48d87c7769f401957826f627bf29779",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses/",
      "published_at": "2022-01-08T08:26:31Z",
      "updated_at": "2021-03-30T21:22:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view licenses for... See... APM agents docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 137.6982,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic APM <em>licenses</em>",
        "sections": "New Relic APM <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view <em>licenses</em> for... See... APM agents docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>product</em>-or-<em>service</em>-<em>licenses</em>&#x2F;new-relic-apm"
      },
      "id": "603e7895e7b9d24e832a07d5"
    }
  ],
  "/docs/licenses/product-or-service-licenses/miscellaneous/help-center-documentation-licenses": [
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-08T09:17:45Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 150.70482,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "CodeStream <em>licenses</em>",
        "sections": "CodeStream <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. CodeStream license on GitHub CodeStream&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    },
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2022-01-08T09:53:40Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0 © 2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0 © 2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0 © 2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0 © 2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0 © 2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright © 2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright © 2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright © 1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright © 1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.25095,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Logs plugin <em>licenses</em>",
        "sections": "Logs plugin <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    },
    {
      "sections": [
        "New Relic APM licenses"
      ],
      "title": "New Relic APM licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "e08100d5a48d87c7769f401957826f627bf29779",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses/",
      "published_at": "2022-01-08T08:26:31Z",
      "updated_at": "2021-03-30T21:22:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view licenses for... See... APM agents docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 137.6982,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic APM <em>licenses</em>",
        "sections": "New Relic APM <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view <em>licenses</em> for... See... APM agents docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>product</em>-or-<em>service</em>-<em>licenses</em>&#x2F;new-relic-apm"
      },
      "id": "603e7895e7b9d24e832a07d5"
    }
  ],
  "/docs/licenses/product-or-service-licenses/mobile-app-licenses/android-application-licenses": [
    {
      "sections": [
        "tvOS application licenses"
      ],
      "title": "tvOS application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "0b3ac8ec42cef00f5a4d3ddf354e4be38ad0595f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/tvos-application-licenses/",
      "published_at": "2022-01-08T09:51:19Z",
      "updated_at": "2021-10-24T17:56:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic for TV app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Copyright AFNetworking MIT Copyright © 2011-2020 Alamofire Software Foundation (http://alamofire.org/) Alamofire MIT Copyright © 2014-2021 Alamofire Software Foundation (http://alamofire.org/) CDMarkdownKit MIT Copyright © 2016-2017 Christopher de Haan contact@christopherdehaan.me Charts Apache License, Version 2.0 Copyright 2016-2019 Daniel Cohen Gindi & Philipp Jahoda LoginManagerSDK New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. Mantle MIT Copyright © GitHub, Inc. All rights reserved. Copyright © 2012, Bitswift, Inc MetricMetadata New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NRCharts New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NVActivityIndicatorView MIT Copyright © 2016 Vinh Nguyen NewRelicAgent New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. UICKeyChainStore MIT Copyright © 2011 kishikawa katsumi WidgetLibrary New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. XYPieChart MIT Copyright © 2012 Xiaoyang Feng, XYStudio.cc Yams MIT Copyright © 2016 JP Simard. iOS-fontawesome CC BY 3.0 & MIT Copyright © 2012 Alex Usbergo. All rights reserved. The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 211.79803,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "tvOS <em>application</em> <em>licenses</em>",
        "sections": "tvOS <em>application</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic for TV <em>app</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Copyright"
      },
      "id": "6072d619196a6795b664a75c"
    },
    {
      "sections": [
        "iOS application licenses"
      ],
      "title": "iOS application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "a6df56e363112e7387e6887f04381a36a5457e84",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/ios-application-licenses/",
      "published_at": "2022-01-08T12:36:18Z",
      "updated_at": "2021-09-27T15:25:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Copyright AFNetworking MIT Copyright © 2011-2020 Alamofire Software Foundation http://alamofire.org/ Ace BSD Copyright © 2010, Ajax.org B.V. ActiveLabel MIT Copyright © 2015 Optonaut Alamofire MIT Copyright © 2014-2021 Alamofire Software Foundation Analytics MIT Copyright © 2016 Segment.io, Inc. BBlock MIT Copyright © 2012 David Keegan BigNumber MIT Copyright © 2019 mkrd CDMarkdownKit MIT Copyright © 2016-2017 Christopher de Haan contact@christopherdehaan.me Charts Apache License, Version 2.0 Copyright © 2016-2019 Daniel Cohen Gindi & Philipp Jahoda ECSlidingViewController MIT Copyright © 2013 EdgeCase LoginManagerSDK New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. Mantle MIT Copyright © GitHub, Inc. All rights reserved. Copyright © 2012, Bitswift, Inc MetricMetadata New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NRCharts New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NVActivityIndicatorView MIT Copyright © 2016 Vinh Nguyen NewRelicAgent New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. ObjectMapper MIT Copyright © 2014 Hearst RadarKit New Relic License © 2010-2021 New Relic, Inc. All rights reserved. SSPullToRefresh MIT Copyright © 2012-2014 Sam Soffes, http://soff.es UICKeyChainStore MIT Copyright © 2011 kishikawa katsumi WidgetLibrary New Relic License © 2010-2021 New Relic, Inc. All rights reserved. XYPieChart MIT Copyright © 2012 Xiaoyang Feng, XYStudio.cc Yams MIT Copyright © 2016 JP Simard. jsTimezoneDetect MIT Copyright © 2012 Jon Nylander, project maintained at bitbucket.org lodash MIT Copyright © JS Foundation and other contributors js.foundation The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 205.88844,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "iOS <em>application</em> <em>licenses</em>",
        "sections": "iOS <em>application</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic iOS <em>app</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Copyright"
      },
      "id": "603e9db1196a670e70a83df3"
    },
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-08T09:17:45Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 150.71141,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "CodeStream <em>licenses</em>",
        "sections": "CodeStream <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. CodeStream license on GitHub CodeStream&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    }
  ],
  "/docs/licenses/product-or-service-licenses/mobile-app-licenses/ios-application-licenses": [
    {
      "sections": [
        "tvOS application licenses"
      ],
      "title": "tvOS application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "0b3ac8ec42cef00f5a4d3ddf354e4be38ad0595f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/tvos-application-licenses/",
      "published_at": "2022-01-08T09:51:19Z",
      "updated_at": "2021-10-24T17:56:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic for TV app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Copyright AFNetworking MIT Copyright © 2011-2020 Alamofire Software Foundation (http://alamofire.org/) Alamofire MIT Copyright © 2014-2021 Alamofire Software Foundation (http://alamofire.org/) CDMarkdownKit MIT Copyright © 2016-2017 Christopher de Haan contact@christopherdehaan.me Charts Apache License, Version 2.0 Copyright 2016-2019 Daniel Cohen Gindi & Philipp Jahoda LoginManagerSDK New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. Mantle MIT Copyright © GitHub, Inc. All rights reserved. Copyright © 2012, Bitswift, Inc MetricMetadata New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NRCharts New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NVActivityIndicatorView MIT Copyright © 2016 Vinh Nguyen NewRelicAgent New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. UICKeyChainStore MIT Copyright © 2011 kishikawa katsumi WidgetLibrary New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. XYPieChart MIT Copyright © 2012 Xiaoyang Feng, XYStudio.cc Yams MIT Copyright © 2016 JP Simard. iOS-fontawesome CC BY 3.0 & MIT Copyright © 2012 Alex Usbergo. All rights reserved. The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 211.79803,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "tvOS <em>application</em> <em>licenses</em>",
        "sections": "tvOS <em>application</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic for TV <em>app</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Copyright"
      },
      "id": "6072d619196a6795b664a75c"
    },
    {
      "sections": [
        "Android application licenses"
      ],
      "title": "Android application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "68c9bdc9dec6f02240f002494309519e41619f29",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/android-application-licenses/",
      "published_at": "2022-01-08T09:19:22Z",
      "updated_at": "2021-05-05T16:29:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Android app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Third Party Dependencies License Copyright Android-FlowLayout Apache 2.0 Copyright © 2011, Artem Votincev (apmem.org) AVLoadingIndicatorView Apache 2.0 Copyright © 2015, Jack Wang BottomNavigationViewEx MIT Copyright © 2017, ittianyu Butterknife Apache 2.0 Copyright © 2013, Jake Wharton Crouton Apache 2.0 Copyright © 2012 - 2014, Benjamin Weiss CWAC-SafeRoom Apache 2.0 The copyrights are owned by CommonsWare for things unique to this library and a combination of CommonsWare and the Android Open Source Project for code modified from the Architecture Components' Framework* set of classes. Dagger 2 Apache 2.0 Copyright © 2012, The Dagger Authors Dragtop Layout Apache 2.0 Copyright © 2015, chenupt EventBus Apache 2.0 Copyright © 2012-2017 Markus Junginger, greenrobot FlexibleAdapter Apache 2.0 Copyright © 2015-2018 Davide Steduto, Davidea Solutions Sprl Gson Apache 2.0 Copyright © 2008, Google Inc. markwon Apache 2.0 Copyright © 2019 Dimitry Ivanov (legal@noties.io) mockk Apache 2.0 Copyright © [ 2017] [ github.com/mockk] leakcanary Apache 2.0 Copyright © 2015 Square, Inc. mockito mockito MIT Copyright © 2007 Mockito contributors mosby Apache 2.0 Copyright © 2015 Hannes Dorfmann moshi Apache 2.0 Copyright © 2015 Square, Inc. MPAndroidChart Apache 2.0 Copyright © 2019 Philipp Jahoda New Relic Mobile Agent OKHttp Apache 2.0 Copyright © 2019 Square, Inc. okio Apache 2.0 Copyright © 2013 Square, Inc. Picasso Apache 2.0 Copyright © 2013 Square, Inc. RESTMock Apache 2.0 Copyright © 2016 Appflate.io Retrofit Apache 2.0 Copyright © 2013 Square, Inc. RxJava Apache 2.0 Copyright © 2016-present, RxJava Contributors. Segment IO MIT Copyright © 2016 Segment, Inc. Snackyaml Apache 2.0 Copyright © 2008, www.snakeyaml.org. StickyHeaders Apache 2.0 Copyright © 2014 Emil Sjölander TableView Apache 2.0 Copyright © 2017 Evren Coşkun Transitions-Everywhere Apache 2.0 The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 195.47263,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Android <em>application</em> <em>licenses</em>",
        "sections": "Android <em>application</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic Android <em>app</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Third Party Dependencies"
      },
      "id": "603e9e30196a67b71fa83d96"
    },
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-08T09:17:45Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 150.71141,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "CodeStream <em>licenses</em>",
        "sections": "CodeStream <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. CodeStream license on GitHub CodeStream&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    }
  ],
  "/docs/licenses/product-or-service-licenses/mobile-app-licenses/tvos-application-licenses": [
    {
      "sections": [
        "iOS application licenses"
      ],
      "title": "iOS application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "a6df56e363112e7387e6887f04381a36a5457e84",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/ios-application-licenses/",
      "published_at": "2022-01-08T12:36:18Z",
      "updated_at": "2021-09-27T15:25:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Copyright AFNetworking MIT Copyright © 2011-2020 Alamofire Software Foundation http://alamofire.org/ Ace BSD Copyright © 2010, Ajax.org B.V. ActiveLabel MIT Copyright © 2015 Optonaut Alamofire MIT Copyright © 2014-2021 Alamofire Software Foundation Analytics MIT Copyright © 2016 Segment.io, Inc. BBlock MIT Copyright © 2012 David Keegan BigNumber MIT Copyright © 2019 mkrd CDMarkdownKit MIT Copyright © 2016-2017 Christopher de Haan contact@christopherdehaan.me Charts Apache License, Version 2.0 Copyright © 2016-2019 Daniel Cohen Gindi & Philipp Jahoda ECSlidingViewController MIT Copyright © 2013 EdgeCase LoginManagerSDK New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. Mantle MIT Copyright © GitHub, Inc. All rights reserved. Copyright © 2012, Bitswift, Inc MetricMetadata New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NRCharts New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NVActivityIndicatorView MIT Copyright © 2016 Vinh Nguyen NewRelicAgent New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. ObjectMapper MIT Copyright © 2014 Hearst RadarKit New Relic License © 2010-2021 New Relic, Inc. All rights reserved. SSPullToRefresh MIT Copyright © 2012-2014 Sam Soffes, http://soff.es UICKeyChainStore MIT Copyright © 2011 kishikawa katsumi WidgetLibrary New Relic License © 2010-2021 New Relic, Inc. All rights reserved. XYPieChart MIT Copyright © 2012 Xiaoyang Feng, XYStudio.cc Yams MIT Copyright © 2016 JP Simard. jsTimezoneDetect MIT Copyright © 2012 Jon Nylander, project maintained at bitbucket.org lodash MIT Copyright © JS Foundation and other contributors js.foundation The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 205.88844,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "iOS <em>application</em> <em>licenses</em>",
        "sections": "iOS <em>application</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic iOS <em>app</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Copyright"
      },
      "id": "603e9db1196a670e70a83df3"
    },
    {
      "sections": [
        "Android application licenses"
      ],
      "title": "Android application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "68c9bdc9dec6f02240f002494309519e41619f29",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/android-application-licenses/",
      "published_at": "2022-01-08T09:19:22Z",
      "updated_at": "2021-05-05T16:29:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Android app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Third Party Dependencies License Copyright Android-FlowLayout Apache 2.0 Copyright © 2011, Artem Votincev (apmem.org) AVLoadingIndicatorView Apache 2.0 Copyright © 2015, Jack Wang BottomNavigationViewEx MIT Copyright © 2017, ittianyu Butterknife Apache 2.0 Copyright © 2013, Jake Wharton Crouton Apache 2.0 Copyright © 2012 - 2014, Benjamin Weiss CWAC-SafeRoom Apache 2.0 The copyrights are owned by CommonsWare for things unique to this library and a combination of CommonsWare and the Android Open Source Project for code modified from the Architecture Components' Framework* set of classes. Dagger 2 Apache 2.0 Copyright © 2012, The Dagger Authors Dragtop Layout Apache 2.0 Copyright © 2015, chenupt EventBus Apache 2.0 Copyright © 2012-2017 Markus Junginger, greenrobot FlexibleAdapter Apache 2.0 Copyright © 2015-2018 Davide Steduto, Davidea Solutions Sprl Gson Apache 2.0 Copyright © 2008, Google Inc. markwon Apache 2.0 Copyright © 2019 Dimitry Ivanov (legal@noties.io) mockk Apache 2.0 Copyright © [ 2017] [ github.com/mockk] leakcanary Apache 2.0 Copyright © 2015 Square, Inc. mockito mockito MIT Copyright © 2007 Mockito contributors mosby Apache 2.0 Copyright © 2015 Hannes Dorfmann moshi Apache 2.0 Copyright © 2015 Square, Inc. MPAndroidChart Apache 2.0 Copyright © 2019 Philipp Jahoda New Relic Mobile Agent OKHttp Apache 2.0 Copyright © 2019 Square, Inc. okio Apache 2.0 Copyright © 2013 Square, Inc. Picasso Apache 2.0 Copyright © 2013 Square, Inc. RESTMock Apache 2.0 Copyright © 2016 Appflate.io Retrofit Apache 2.0 Copyright © 2013 Square, Inc. RxJava Apache 2.0 Copyright © 2016-present, RxJava Contributors. Segment IO MIT Copyright © 2016 Segment, Inc. Snackyaml Apache 2.0 Copyright © 2008, www.snakeyaml.org. StickyHeaders Apache 2.0 Copyright © 2014 Emil Sjölander TableView Apache 2.0 Copyright © 2017 Evren Coşkun Transitions-Everywhere Apache 2.0 The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 195.47263,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Android <em>application</em> <em>licenses</em>",
        "sections": "Android <em>application</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic Android <em>app</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Third Party Dependencies"
      },
      "id": "603e9e30196a67b71fa83d96"
    },
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-08T09:17:45Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 150.71141,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "CodeStream <em>licenses</em>",
        "sections": "CodeStream <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. CodeStream license on GitHub CodeStream&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-apm/apm-agent-sdk-licenses": [
    {
      "sections": [
        "New Relic APM licenses"
      ],
      "title": "New Relic APM licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "e08100d5a48d87c7769f401957826f627bf29779",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses/",
      "published_at": "2022-01-08T08:26:31Z",
      "updated_at": "2021-03-30T21:22:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view licenses for... See... APM agents docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 180.58966,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "sections": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We use many third-party libraries and tools to create <em>New</em> <em>Relic</em> <em>APM</em>, including a large number contributed by the open source community. To view <em>licenses</em> for... See... <em>APM</em> agents docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>product</em>-or-<em>service</em>-<em>licenses</em>&#x2F;<em>new</em>-<em>relic</em>-<em>apm</em>"
      },
      "id": "603e7895e7b9d24e832a07d5"
    },
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2022-01-08T08:25:10Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.93378,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    },
    {
      "sections": [
        "Go agent licenses"
      ],
      "title": "Go agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "08a72af2529390cf6296870147ce00b64ea7f633",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/go-agent-licenses/",
      "published_at": "2022-01-08T09:18:32Z",
      "updated_at": "2021-03-16T04:25:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Go Go BSD The remainder of the code is covered by the New Relic agent license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.06311,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Go agent <em>licenses</em>",
        "sections": "Go agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Go Go BSD The remainder of the code is covered by the <em>New</em> <em>Relic</em> agent license agreement."
      },
      "id": "603ea506196a67b2b3a83dee"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-apm/c-sdk-licenses": [
    {
      "sections": [
        "New Relic APM licenses"
      ],
      "title": "New Relic APM licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "e08100d5a48d87c7769f401957826f627bf29779",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses/",
      "published_at": "2022-01-08T08:26:31Z",
      "updated_at": "2021-03-30T21:22:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view licenses for... See... APM agents docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 180.58966,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "sections": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We use many third-party libraries and tools to create <em>New</em> <em>Relic</em> <em>APM</em>, including a large number contributed by the open source community. To view <em>licenses</em> for... See... <em>APM</em> agents docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>product</em>-or-<em>service</em>-<em>licenses</em>&#x2F;<em>new</em>-<em>relic</em>-<em>apm</em>"
      },
      "id": "603e7895e7b9d24e832a07d5"
    },
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2022-01-08T08:25:10Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.93378,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    },
    {
      "sections": [
        "Go agent licenses"
      ],
      "title": "Go agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "08a72af2529390cf6296870147ce00b64ea7f633",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/go-agent-licenses/",
      "published_at": "2022-01-08T09:18:32Z",
      "updated_at": "2021-03-16T04:25:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Go Go BSD The remainder of the code is covered by the New Relic agent license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.06311,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Go agent <em>licenses</em>",
        "sections": "Go agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Go Go BSD The remainder of the code is covered by the <em>New</em> <em>Relic</em> agent license agreement."
      },
      "id": "603ea506196a67b2b3a83dee"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-apm/go-agent-licenses": [
    {
      "sections": [
        "New Relic APM licenses"
      ],
      "title": "New Relic APM licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "e08100d5a48d87c7769f401957826f627bf29779",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses/",
      "published_at": "2022-01-08T08:26:31Z",
      "updated_at": "2021-03-30T21:22:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view licenses for... See... APM agents docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 180.58966,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "sections": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We use many third-party libraries and tools to create <em>New</em> <em>Relic</em> <em>APM</em>, including a large number contributed by the open source community. To view <em>licenses</em> for... See... <em>APM</em> agents docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>product</em>-or-<em>service</em>-<em>licenses</em>&#x2F;<em>new</em>-<em>relic</em>-<em>apm</em>"
      },
      "id": "603e7895e7b9d24e832a07d5"
    },
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2022-01-08T08:25:10Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.93378,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    },
    {
      "sections": [
        ".NET agent: Microsoft Azure Portal Extension licenses"
      ],
      "title": ".NET agent: Microsoft Azure Portal Extension licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "19d61cf14551e1be8895993d42d2c640d7cd238b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/net-agent-microsoft-azure-portal-extension-licenses/",
      "published_at": "2022-01-08T08:26:05Z",
      "updated_at": "2021-03-13T03:35:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the Microsoft Azure Portal Extension. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Antlr BSD JSON.NET MIT PowerArgs MIT The remainder of the code is covered by the New Relic License agreement found in the LICENSE file in the distribution.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.00632,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": ".NET agent: Microsoft Azure Portal Extension <em>licenses</em>",
        "sections": ".NET agent: Microsoft Azure Portal Extension <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": " Antlr BSD JSON.NET MIT PowerArgs MIT The remainder of the code is covered by the <em>New</em> <em>Relic</em> License agreement found in the LICENSE file in the distribution."
      },
      "id": "6044e7bb64441f6e4e378f1a"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses": [
    {
      "sections": [
        "New Relic APM licenses"
      ],
      "title": "New Relic APM licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "e08100d5a48d87c7769f401957826f627bf29779",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses/",
      "published_at": "2022-01-08T08:26:31Z",
      "updated_at": "2021-03-30T21:22:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view licenses for... See... APM agents docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 180.58966,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "sections": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We use many third-party libraries and tools to create <em>New</em> <em>Relic</em> <em>APM</em>, including a large number contributed by the open source community. To view <em>licenses</em> for... See... <em>APM</em> agents docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>product</em>-or-<em>service</em>-<em>licenses</em>&#x2F;<em>new</em>-<em>relic</em>-<em>apm</em>"
      },
      "id": "603e7895e7b9d24e832a07d5"
    },
    {
      "sections": [
        "Go agent licenses"
      ],
      "title": "Go agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "08a72af2529390cf6296870147ce00b64ea7f633",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/go-agent-licenses/",
      "published_at": "2022-01-08T09:18:32Z",
      "updated_at": "2021-03-16T04:25:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Go Go BSD The remainder of the code is covered by the New Relic agent license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.06311,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Go agent <em>licenses</em>",
        "sections": "Go agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Go Go BSD The remainder of the code is covered by the <em>New</em> <em>Relic</em> agent license agreement."
      },
      "id": "603ea506196a67b2b3a83dee"
    },
    {
      "sections": [
        ".NET agent: Microsoft Azure Portal Extension licenses"
      ],
      "title": ".NET agent: Microsoft Azure Portal Extension licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "19d61cf14551e1be8895993d42d2c640d7cd238b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/net-agent-microsoft-azure-portal-extension-licenses/",
      "published_at": "2022-01-08T08:26:05Z",
      "updated_at": "2021-03-13T03:35:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the Microsoft Azure Portal Extension. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Antlr BSD JSON.NET MIT PowerArgs MIT The remainder of the code is covered by the New Relic License agreement found in the LICENSE file in the distribution.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.00632,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": ".NET agent: Microsoft Azure Portal Extension <em>licenses</em>",
        "sections": ".NET agent: Microsoft Azure Portal Extension <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": " Antlr BSD JSON.NET MIT PowerArgs MIT The remainder of the code is covered by the <em>New</em> <em>Relic</em> License agreement found in the LICENSE file in the distribution."
      },
      "id": "6044e7bb64441f6e4e378f1a"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-apm/net-agent-licenses": [
    {
      "sections": [
        "New Relic APM licenses"
      ],
      "title": "New Relic APM licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "e08100d5a48d87c7769f401957826f627bf29779",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses/",
      "published_at": "2022-01-08T08:26:31Z",
      "updated_at": "2021-03-30T21:22:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view licenses for... See... APM agents docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 180.58966,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "sections": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We use many third-party libraries and tools to create <em>New</em> <em>Relic</em> <em>APM</em>, including a large number contributed by the open source community. To view <em>licenses</em> for... See... <em>APM</em> agents docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>product</em>-or-<em>service</em>-<em>licenses</em>&#x2F;<em>new</em>-<em>relic</em>-<em>apm</em>"
      },
      "id": "603e7895e7b9d24e832a07d5"
    },
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2022-01-08T08:25:10Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.93378,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    },
    {
      "sections": [
        "Go agent licenses"
      ],
      "title": "Go agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "08a72af2529390cf6296870147ce00b64ea7f633",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/go-agent-licenses/",
      "published_at": "2022-01-08T09:18:32Z",
      "updated_at": "2021-03-16T04:25:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Go Go BSD The remainder of the code is covered by the New Relic agent license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.06311,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Go agent <em>licenses</em>",
        "sections": "Go agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Go Go BSD The remainder of the code is covered by the <em>New</em> <em>Relic</em> agent license agreement."
      },
      "id": "603ea506196a67b2b3a83dee"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-apm/net-agent-microsoft-azure-portal-extension-licenses": [
    {
      "sections": [
        "New Relic APM licenses"
      ],
      "title": "New Relic APM licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "e08100d5a48d87c7769f401957826f627bf29779",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses/",
      "published_at": "2022-01-08T08:26:31Z",
      "updated_at": "2021-03-30T21:22:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view licenses for... See... APM agents docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 180.58966,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "sections": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We use many third-party libraries and tools to create <em>New</em> <em>Relic</em> <em>APM</em>, including a large number contributed by the open source community. To view <em>licenses</em> for... See... <em>APM</em> agents docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>product</em>-or-<em>service</em>-<em>licenses</em>&#x2F;<em>new</em>-<em>relic</em>-<em>apm</em>"
      },
      "id": "603e7895e7b9d24e832a07d5"
    },
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2022-01-08T08:25:10Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.93378,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    },
    {
      "sections": [
        "Go agent licenses"
      ],
      "title": "Go agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "08a72af2529390cf6296870147ce00b64ea7f633",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/go-agent-licenses/",
      "published_at": "2022-01-08T09:18:32Z",
      "updated_at": "2021-03-16T04:25:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Go Go BSD The remainder of the code is covered by the New Relic agent license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.06311,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Go agent <em>licenses</em>",
        "sections": "Go agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Go Go BSD The remainder of the code is covered by the <em>New</em> <em>Relic</em> agent license agreement."
      },
      "id": "603ea506196a67b2b3a83dee"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-apm/net-agent-microsoft-azure-portal-resource-provider-licenses": [
    {
      "sections": [
        "New Relic APM licenses"
      ],
      "title": "New Relic APM licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "e08100d5a48d87c7769f401957826f627bf29779",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses/",
      "published_at": "2022-01-08T08:26:31Z",
      "updated_at": "2021-03-30T21:22:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view licenses for... See... APM agents docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 180.58966,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "sections": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We use many third-party libraries and tools to create <em>New</em> <em>Relic</em> <em>APM</em>, including a large number contributed by the open source community. To view <em>licenses</em> for... See... <em>APM</em> agents docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>product</em>-or-<em>service</em>-<em>licenses</em>&#x2F;<em>new</em>-<em>relic</em>-<em>apm</em>"
      },
      "id": "603e7895e7b9d24e832a07d5"
    },
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2022-01-08T08:25:10Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.93378,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    },
    {
      "sections": [
        "Go agent licenses"
      ],
      "title": "Go agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "08a72af2529390cf6296870147ce00b64ea7f633",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/go-agent-licenses/",
      "published_at": "2022-01-08T09:18:32Z",
      "updated_at": "2021-03-16T04:25:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Go Go BSD The remainder of the code is covered by the New Relic agent license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.06311,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Go agent <em>licenses</em>",
        "sections": "Go agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Go Go BSD The remainder of the code is covered by the <em>New</em> <em>Relic</em> agent license agreement."
      },
      "id": "603ea506196a67b2b3a83dee"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses": [
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2022-01-08T08:25:10Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.93378,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    },
    {
      "sections": [
        "Go agent licenses"
      ],
      "title": "Go agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "08a72af2529390cf6296870147ce00b64ea7f633",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/go-agent-licenses/",
      "published_at": "2022-01-08T09:18:32Z",
      "updated_at": "2021-03-16T04:25:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Go Go BSD The remainder of the code is covered by the New Relic agent license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.06311,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Go agent <em>licenses</em>",
        "sections": "Go agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Go Go BSD The remainder of the code is covered by the <em>New</em> <em>Relic</em> agent license agreement."
      },
      "id": "603ea506196a67b2b3a83dee"
    },
    {
      "sections": [
        ".NET agent: Microsoft Azure Portal Extension licenses"
      ],
      "title": ".NET agent: Microsoft Azure Portal Extension licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "19d61cf14551e1be8895993d42d2c640d7cd238b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/net-agent-microsoft-azure-portal-extension-licenses/",
      "published_at": "2022-01-08T08:26:05Z",
      "updated_at": "2021-03-13T03:35:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the Microsoft Azure Portal Extension. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Antlr BSD JSON.NET MIT PowerArgs MIT The remainder of the code is covered by the New Relic License agreement found in the LICENSE file in the distribution.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.00632,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": ".NET agent: Microsoft Azure Portal Extension <em>licenses</em>",
        "sections": ".NET agent: Microsoft Azure Portal Extension <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": " Antlr BSD JSON.NET MIT PowerArgs MIT The remainder of the code is covered by the <em>New</em> <em>Relic</em> License agreement found in the LICENSE file in the distribution."
      },
      "id": "6044e7bb64441f6e4e378f1a"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-apm/nodejs-agent-licenses": [
    {
      "sections": [
        "New Relic APM licenses"
      ],
      "title": "New Relic APM licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "e08100d5a48d87c7769f401957826f627bf29779",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses/",
      "published_at": "2022-01-08T08:26:31Z",
      "updated_at": "2021-03-30T21:22:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view licenses for... See... APM agents docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 180.58966,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "sections": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We use many third-party libraries and tools to create <em>New</em> <em>Relic</em> <em>APM</em>, including a large number contributed by the open source community. To view <em>licenses</em> for... See... <em>APM</em> agents docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>product</em>-or-<em>service</em>-<em>licenses</em>&#x2F;<em>new</em>-<em>relic</em>-<em>apm</em>"
      },
      "id": "603e7895e7b9d24e832a07d5"
    },
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2022-01-08T08:25:10Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.93378,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    },
    {
      "sections": [
        "Go agent licenses"
      ],
      "title": "Go agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "08a72af2529390cf6296870147ce00b64ea7f633",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/go-agent-licenses/",
      "published_at": "2022-01-08T09:18:32Z",
      "updated_at": "2021-03-16T04:25:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Go Go BSD The remainder of the code is covered by the New Relic agent license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.06311,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Go agent <em>licenses</em>",
        "sections": "Go agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Go Go BSD The remainder of the code is covered by the <em>New</em> <em>Relic</em> agent license agreement."
      },
      "id": "603ea506196a67b2b3a83dee"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-apm/php-agent-licenses": [
    {
      "sections": [
        "New Relic APM licenses"
      ],
      "title": "New Relic APM licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "e08100d5a48d87c7769f401957826f627bf29779",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses/",
      "published_at": "2022-01-08T08:26:31Z",
      "updated_at": "2021-03-30T21:22:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view licenses for... See... APM agents docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 180.58966,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "sections": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We use many third-party libraries and tools to create <em>New</em> <em>Relic</em> <em>APM</em>, including a large number contributed by the open source community. To view <em>licenses</em> for... See... <em>APM</em> agents docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>product</em>-or-<em>service</em>-<em>licenses</em>&#x2F;<em>new</em>-<em>relic</em>-<em>apm</em>"
      },
      "id": "603e7895e7b9d24e832a07d5"
    },
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2022-01-08T08:25:10Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.93378,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    },
    {
      "sections": [
        "Go agent licenses"
      ],
      "title": "Go agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "08a72af2529390cf6296870147ce00b64ea7f633",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/go-agent-licenses/",
      "published_at": "2022-01-08T09:18:32Z",
      "updated_at": "2021-03-16T04:25:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Go Go BSD The remainder of the code is covered by the New Relic agent license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.06311,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Go agent <em>licenses</em>",
        "sections": "Go agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Go Go BSD The remainder of the code is covered by the <em>New</em> <em>Relic</em> agent license agreement."
      },
      "id": "603ea506196a67b2b3a83dee"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-apm/python-agent-licenses": [
    {
      "sections": [
        "New Relic APM licenses"
      ],
      "title": "New Relic APM licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "e08100d5a48d87c7769f401957826f627bf29779",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses/",
      "published_at": "2022-01-08T08:26:31Z",
      "updated_at": "2021-03-30T21:22:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view licenses for... See... APM agents docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 180.58966,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "sections": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We use many third-party libraries and tools to create <em>New</em> <em>Relic</em> <em>APM</em>, including a large number contributed by the open source community. To view <em>licenses</em> for... See... <em>APM</em> agents docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>product</em>-or-<em>service</em>-<em>licenses</em>&#x2F;<em>new</em>-<em>relic</em>-<em>apm</em>"
      },
      "id": "603e7895e7b9d24e832a07d5"
    },
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2022-01-08T08:25:10Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.93378,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    },
    {
      "sections": [
        "Go agent licenses"
      ],
      "title": "Go agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "08a72af2529390cf6296870147ce00b64ea7f633",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/go-agent-licenses/",
      "published_at": "2022-01-08T09:18:32Z",
      "updated_at": "2021-03-16T04:25:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Go Go BSD The remainder of the code is covered by the New Relic agent license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.06311,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Go agent <em>licenses</em>",
        "sections": "Go agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Go Go BSD The remainder of the code is covered by the <em>New</em> <em>Relic</em> agent license agreement."
      },
      "id": "603ea506196a67b2b3a83dee"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-apm/ruby-agent-licenses": [
    {
      "sections": [
        "New Relic APM licenses"
      ],
      "title": "New Relic APM licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "e08100d5a48d87c7769f401957826f627bf29779",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses/",
      "published_at": "2022-01-08T08:26:31Z",
      "updated_at": "2021-03-30T21:22:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view licenses for... See... APM agents docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 180.58966,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "sections": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We use many third-party libraries and tools to create <em>New</em> <em>Relic</em> <em>APM</em>, including a large number contributed by the open source community. To view <em>licenses</em> for... See... <em>APM</em> agents docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>product</em>-or-<em>service</em>-<em>licenses</em>&#x2F;<em>new</em>-<em>relic</em>-<em>apm</em>"
      },
      "id": "603e7895e7b9d24e832a07d5"
    },
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2022-01-08T08:25:10Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.93378,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    },
    {
      "sections": [
        "Go agent licenses"
      ],
      "title": "Go agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "08a72af2529390cf6296870147ce00b64ea7f633",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/go-agent-licenses/",
      "published_at": "2022-01-08T09:18:32Z",
      "updated_at": "2021-03-16T04:25:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Go Go BSD The remainder of the code is covered by the New Relic agent license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.06311,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Go agent <em>licenses</em>",
        "sections": "Go agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Go Go BSD The remainder of the code is covered by the <em>New</em> <em>Relic</em> agent license agreement."
      },
      "id": "603ea506196a67b2b3a83dee"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-browser/browser-agent-licenses": [
    {
      "sections": [
        "New Relic Browser licenses",
        "UI tier"
      ],
      "title": "New Relic Browser licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Browser"
      ],
      "external_id": "b308c950e31570ee201237a88bfb8891da10a23c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-browser/new-relic-browser-licenses/",
      "published_at": "2022-01-08T09:52:48Z",
      "updated_at": "2021-03-16T04:45:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Browser. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. UI tier Library License DataStax Cassandra Java Driver Apache 2.0 d3 BSD Finagle Apache 2.0 gulp-rename MIT Jackson Apache 2.0 nee ISC nscala-time Apache 2.0 photocopy ISC Rapture Apache 2.0 React Apache 2.0 require.dir MIT Scrooge Apache 2.0 slf4j MIT snappy-java Apache 2.0 TwitterServer Apache 2.0 Typesafe Config Apache 2.0",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.93512,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>New</em> <em>Relic</em> <em>Browser</em> <em>licenses</em>",
        "sections": "<em>New</em> <em>Relic</em> <em>Browser</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> <em>Browser</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. UI tier Library License DataStax"
      },
      "id": "603ece91e7b9d2c9d02a07c2"
    },
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-08T09:17:45Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 150.7048,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "CodeStream <em>licenses</em>",
        "sections": "CodeStream <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. CodeStream license on GitHub CodeStream&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    },
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2022-01-08T09:53:40Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0 © 2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0 © 2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0 © 2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0 © 2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0 © 2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright © 2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright © 2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright © 1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright © 1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.25095,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Logs plugin <em>licenses</em>",
        "sections": "Logs plugin <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-browser/new-relic-browser-licenses": [
    {
      "sections": [
        "Browser agent licenses"
      ],
      "title": "Browser agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Browser"
      ],
      "external_id": "9fac9d2d566767575f22d9458e49410063a83406",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-browser/browser-agent-licenses/",
      "published_at": "2022-01-08T09:52:48Z",
      "updated_at": "2021-03-16T04:45:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Browser agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Browserify MIT Episodes Apache 2.0 Lo-Dash MIT TraceKit MIT",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 177.77339,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Browser</em> agent <em>licenses</em>",
        "sections": "<em>Browser</em> agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> <em>Browser</em> agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Browserify MIT Episodes Apache 2.0 Lo-Dash MIT TraceKit MIT"
      },
      "id": "603eb41ce7b9d298012a080a"
    },
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-08T09:17:45Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 150.7048,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "CodeStream <em>licenses</em>",
        "sections": "CodeStream <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. CodeStream license on GitHub CodeStream&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    },
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2022-01-08T09:53:40Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0 © 2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0 © 2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0 © 2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0 © 2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0 © 2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright © 2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright © 2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright © 1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright © 1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.25095,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Logs plugin <em>licenses</em>",
        "sections": "Logs plugin <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-developer-edition/developer-edition": [
    {
      "sections": [
        "Developer Program Resources"
      ],
      "title": "Developer Program Resources",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Developer edition"
      ],
      "external_id": "8a2f08905c7dcd10e50e975783ca3cf0071324c0",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-developer-edition/developer-program-resources/",
      "published_at": "2022-01-08T09:52:04Z",
      "updated_at": "2021-03-13T03:24:28Z",
      "document_type": "page",
      "popularity": 1,
      "body": "As a customer, you are eligible to participate in New Relic’s Developer Program. Additional information and resources are available at New Relic’s Developer Program site. By downloading, accessing, or using the developer resources (including the CLI), you agree that usage of the developer resources is pursuant to the New Relic Developers Terms and Conditions and that you have the authority to bind your organization. Such terms do not have to be signed in order to be binding. If you do not agree to these terms and conditions, your sole remedy is to not use these developer resources. If your use of the New Relic developer resources are covered under a separate agreement, the above does not apply to you.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.63622,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Developer</em> Program Resources",
        "sections": "<em>Developer</em> Program Resources",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "As a customer, you are eligible to participate in <em>New</em> <em>Relic</em>’s <em>Developer</em> Program. Additional information and resources are available at <em>New</em> <em>Relic</em>’s <em>Developer</em> Program site. By downloading, accessing, or using the <em>developer</em> resources (including the CLI), you agree that usage of the <em>developer</em> resources"
      },
      "id": "6044e7bb196a676d20960f4d"
    },
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-08T09:17:45Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 150.7114,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "CodeStream <em>licenses</em>",
        "sections": "CodeStream <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. CodeStream license on GitHub CodeStream&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    },
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2022-01-08T09:53:40Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0 © 2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0 © 2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0 © 2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0 © 2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0 © 2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright © 2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright © 2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright © 1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright © 1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.25278,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Logs plugin <em>licenses</em>",
        "sections": "Logs plugin <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-developer-edition/developer-program-resources": [
    {
      "sections": [
        "Developer edition",
        "Terms"
      ],
      "title": "Developer edition",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Developer edition"
      ],
      "external_id": "60bc94afd677817a7b7fd7dd471c537090a9f711",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-developer-edition/developer-edition/",
      "published_at": "2022-01-08T09:52:04Z",
      "updated_at": "2021-11-14T03:10:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "All new accounts (outside Japan) created on or after July 30, 2020 are on the New Relic One pricing model and have the newer New Relic One user model as described here. If (i) your account was created prior to July 30, 2020; or (ii) you are a current New Relic K.K. customer (Japan): in addition to the New Relic Pre-release policy, use of the Developer Edition of New Relic to the extent it is available is also subject to the following terms: Terms Non-production use only Up to $500 per month in total product usage across the New Relic platform Up to 2 users",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 233.72397,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Developer</em> <em>edition</em>",
        "sections": "<em>Developer</em> <em>edition</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": " to the <em>New</em> <em>Relic</em> Pre-release policy, use of the <em>Developer</em> <em>Edition</em> of <em>New</em> <em>Relic</em> to the extent it is available is also subject to the following terms: Terms Non-production use only Up to $500 per month in total <em>product</em> usage across the <em>New</em> <em>Relic</em> platform Up to 2 users"
      },
      "id": "604506b9e7b9d22d115799c8"
    },
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-08T09:17:45Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 150.7114,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "CodeStream <em>licenses</em>",
        "sections": "CodeStream <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. CodeStream license on GitHub CodeStream&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    },
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2022-01-08T09:53:40Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0 © 2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0 © 2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0 © 2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0 © 2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0 © 2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright © 2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright © 2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright © 1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright © 1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.25278,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Logs plugin <em>licenses</em>",
        "sections": "Logs plugin <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-infrastructure/new-relic-infrastructure-licenses": [
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-08T09:17:45Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 150.7048,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "CodeStream <em>licenses</em>",
        "sections": "CodeStream <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. CodeStream license on GitHub CodeStream&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    },
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2022-01-08T09:53:40Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0 © 2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0 © 2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0 © 2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0 © 2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0 © 2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright © 2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright © 2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright © 1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright © 1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.25095,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Logs plugin <em>licenses</em>",
        "sections": "Logs plugin <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    },
    {
      "sections": [
        "New Relic APM licenses"
      ],
      "title": "New Relic APM licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "e08100d5a48d87c7769f401957826f627bf29779",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses/",
      "published_at": "2022-01-08T08:26:31Z",
      "updated_at": "2021-03-30T21:22:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view licenses for... See... APM agents docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 137.69818,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>New</em> <em>Relic</em> APM <em>licenses</em>",
        "sections": "<em>New</em> <em>Relic</em> APM <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We use many third-party libraries and tools to create <em>New</em> <em>Relic</em> APM, including a large number contributed by the open source community. To view <em>licenses</em> for... See... APM agents docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>product</em>-or-<em>service</em>-<em>licenses</em>&#x2F;<em>new</em>-<em>relic</em>-apm"
      },
      "id": "603e7895e7b9d24e832a07d5"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-insights/new-relic-insights-licenses": [
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-08T09:17:45Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 150.7048,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "CodeStream <em>licenses</em>",
        "sections": "CodeStream <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. CodeStream license on GitHub CodeStream&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    },
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2022-01-08T09:53:40Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0 © 2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0 © 2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0 © 2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0 © 2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0 © 2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright © 2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright © 2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright © 1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright © 1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.25095,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Logs plugin <em>licenses</em>",
        "sections": "Logs plugin <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    },
    {
      "sections": [
        "New Relic APM licenses"
      ],
      "title": "New Relic APM licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "e08100d5a48d87c7769f401957826f627bf29779",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses/",
      "published_at": "2022-01-08T08:26:31Z",
      "updated_at": "2021-03-30T21:22:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view licenses for... See... APM agents docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 137.69818,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>New</em> <em>Relic</em> APM <em>licenses</em>",
        "sections": "<em>New</em> <em>Relic</em> APM <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We use many third-party libraries and tools to create <em>New</em> <em>Relic</em> APM, including a large number contributed by the open source community. To view <em>licenses</em> for... See... APM agents docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>product</em>-or-<em>service</em>-<em>licenses</em>&#x2F;<em>new</em>-<em>relic</em>-apm"
      },
      "id": "603e7895e7b9d24e832a07d5"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-logs/logs-licenses": [
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2022-01-08T09:53:40Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0 © 2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0 © 2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0 © 2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0 © 2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0 © 2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright © 2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright © 2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright © 1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright © 1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 193.13116,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Logs</em> plugin <em>licenses</em>",
        "sections": "<em>Logs</em> plugin <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": " used for <em>New</em> <em>Relic</em> <em>Logs</em>, see <em>Logs</em> <em>licenses</em>. Plugins for <em>Logs</em> The following <em>licenses</em> are for the plugins used to connects your <em>log</em> data with <em>New</em> <em>Relic</em> <em>Logs</em>. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0 © 2019 <em>New</em> <em>Relic</em>, Inc. Fluentd Library License Copyright Fluentd Apache"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    },
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-08T09:17:45Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 150.7114,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "CodeStream <em>licenses</em>",
        "sections": "CodeStream <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. CodeStream license on GitHub CodeStream&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    },
    {
      "sections": [
        "New Relic APM licenses"
      ],
      "title": "New Relic APM licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "e08100d5a48d87c7769f401957826f627bf29779",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses/",
      "published_at": "2022-01-08T08:26:31Z",
      "updated_at": "2021-03-30T21:22:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view licenses for... See... APM agents docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 137.69243,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>New</em> <em>Relic</em> APM <em>licenses</em>",
        "sections": "<em>New</em> <em>Relic</em> APM <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We use many third-party libraries and tools to create <em>New</em> <em>Relic</em> APM, including a large number contributed by the open source community. To view <em>licenses</em> for... See... APM agents docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>product</em>-or-<em>service</em>-<em>licenses</em>&#x2F;<em>new</em>-<em>relic</em>-apm"
      },
      "id": "603e7895e7b9d24e832a07d5"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses": [
    {
      "sections": [
        "Logs licenses",
        "Logs"
      ],
      "title": "Logs licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "229757bf0e3d8f3518533b11a059b9e4516e4699",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-licenses/",
      "published_at": "2022-01-08T09:53:40Z",
      "updated_at": "2021-03-16T04:46:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for the New Relic Logs plugins, see Logs plugin licenses. Logs axios MIT Copyright © 2014-present Matt Zabriskie Babel-plugin-transform-runtime MIT Copyright © 2014-present Sebastian McKenzie and other contributors Classnames MIT Copyright © 2018 Jed Watson Downshift MIT Copyright © 2017 PayPal Fuzzy-search ISC Copyright © 2016, Wouter Rutgers Immer MIT Copyright © 2017 Michel Weststrate Lodash MIT Copyright © JS Foundation and other contributors Lodash.debounce MIT Copyright © JS Foundation and other contributors Moment MIT Copyright © JS Foundation and other contributors Node-sass MIT Copyright © 2013-2016 Andrew Nesbitt Prop-types MIT Copyright © 2013-present, Facebook, Inc. React MIT Copyright © Facebook, Inc. and its affiliates. React-dom MIT Copyright © Facebook, Inc. and its affiliates. React-highlight-words MIT Copyright © 2015 Treasure Data React-json-view MIT Copyright © 2015 Mac Gainor React-popper MIT Copyright © 2018 React Popper authors React-redux MIT Copyright © 2015-present Dan Abramov React-select MIT Copyright © 2018 Jed Watson React-tooltip MIT Copyright © 2015 Wang Zixiao Redux MIT Copyright © 2015-present Dan Abramov Redux-logger MIT Copyright © 2016 Eugene Rodionov Redux-saga MIT Copyright © 2015 Yassine Elouafi Reselect MIT Copyright © 2015-2018 Reselect Contributors Shortid MIT Copyright © Dylan Greene Snyk Apache License 2.0 Copyright © 2015 Snyk Ltd. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 183.81624,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Logs</em> <em>licenses</em>",
        "sections": "<em>Logs</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> <em>Logs</em> plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea57b28ccbce04ceba77a"
    },
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-08T09:17:45Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 150.7114,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "CodeStream <em>licenses</em>",
        "sections": "CodeStream <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. CodeStream license on GitHub CodeStream&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    },
    {
      "sections": [
        "New Relic APM licenses"
      ],
      "title": "New Relic APM licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "e08100d5a48d87c7769f401957826f627bf29779",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses/",
      "published_at": "2022-01-08T08:26:31Z",
      "updated_at": "2021-03-30T21:22:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view licenses for... See... APM agents docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 137.69243,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>New</em> <em>Relic</em> APM <em>licenses</em>",
        "sections": "<em>New</em> <em>Relic</em> APM <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We use many third-party libraries and tools to create <em>New</em> <em>Relic</em> APM, including a large number contributed by the open source community. To view <em>licenses</em> for... See... APM agents docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>product</em>-or-<em>service</em>-<em>licenses</em>&#x2F;<em>new</em>-<em>relic</em>-apm"
      },
      "id": "603e7895e7b9d24e832a07d5"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-mobile/android-sdk-new-relic-mobile-licenses": [
    {
      "sections": [
        "iOS SDK for mobile monitoring licenses"
      ],
      "title": "iOS SDK for mobile monitoring licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Mobile"
      ],
      "external_id": "71d2df4a922b6728230da4b4e8241f2d458ea66c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-mobile/ios-sdk-new-relic-mobile-licenses/",
      "published_at": "2022-01-08T09:54:31Z",
      "updated_at": "2021-07-22T01:04:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the iOS SDK for mobile monitoring. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apple Reachability Apple Reachability JSON++ MIT mod-pbxproj BSD-3 PLCrashReporter MIT The remainder of the code is covered by the New Relic License agreement found in the LICENSE file in the distribution.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.95552,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "iOS SDK for <em>mobile</em> monitoring <em>licenses</em>",
        "sections": "iOS SDK for <em>mobile</em> monitoring <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the iOS SDK for <em>mobile</em> monitoring. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apple"
      },
      "id": "60450cfde7b9d2c9eb5799c3"
    },
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-08T09:17:45Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 150.7114,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "CodeStream <em>licenses</em>",
        "sections": "CodeStream <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. CodeStream license on GitHub CodeStream&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    },
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2022-01-08T09:53:40Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0 © 2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0 © 2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0 © 2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0 © 2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0 © 2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright © 2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright © 2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright © 1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright © 1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.25278,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Logs plugin <em>licenses</em>",
        "sections": "Logs plugin <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-mobile/ios-sdk-new-relic-mobile-licenses": [
    {
      "sections": [
        "Android SDK for mobile monitoring licenses"
      ],
      "title": "Android SDK for mobile monitoring licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Mobile"
      ],
      "external_id": "7e7fa828754c2ba00d4e2138653c7cdd00ed6c90",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-mobile/android-sdk-new-relic-mobile-licenses/",
      "published_at": "2022-01-08T09:54:32Z",
      "updated_at": "2021-08-21T08:48:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the Android SDK for mobile monitoring. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Commons IO Apache 2.0 Apache Commons Logging Apache 2.0 Apache HttpClient Apache 2.0 ASM ASM (BSD-3 style) Gradle Apache 2.0 Gson Apache 2.0 Guava Apache 2.0 JarJar Apache 2.0 Javassist Apache 2.0 org.json JSON Reflections WTFPL SLF4J MIT Square OkHttp Apache 2.0 The remainder of the code is covered by the New Relic License agreement found in the LICENSE file in the distribution.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.09065,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Android SDK for <em>mobile</em> monitoring <em>licenses</em>",
        "sections": "Android SDK for <em>mobile</em> monitoring <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the Android SDK for <em>mobile</em> monitoring. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License"
      },
      "id": "603e9eb628ccbc117beba796"
    },
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-08T09:17:45Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 150.7114,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "CodeStream <em>licenses</em>",
        "sections": "CodeStream <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. CodeStream license on GitHub CodeStream&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    },
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2022-01-08T09:53:40Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0 © 2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0 © 2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0 © 2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0 © 2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0 © 2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright © 2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright © 2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright © 1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright © 1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.25278,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Logs plugin <em>licenses</em>",
        "sections": "Logs plugin <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-one/preview-access-new-relic-one": [
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-08T09:17:45Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 150.7114,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "CodeStream <em>licenses</em>",
        "sections": "CodeStream <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. CodeStream license on GitHub CodeStream&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    },
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2022-01-08T09:53:40Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0 © 2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0 © 2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0 © 2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0 © 2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0 © 2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright © 2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright © 2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright © 1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright © 1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.25278,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Logs plugin <em>licenses</em>",
        "sections": "Logs plugin <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    },
    {
      "sections": [
        "New Relic APM licenses"
      ],
      "title": "New Relic APM licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "e08100d5a48d87c7769f401957826f627bf29779",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses/",
      "published_at": "2022-01-08T08:26:31Z",
      "updated_at": "2021-03-30T21:22:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view licenses for... See... APM agents docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 137.69243,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>New</em> <em>Relic</em> APM <em>licenses</em>",
        "sections": "<em>New</em> <em>Relic</em> APM <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We use many third-party libraries and tools to create <em>New</em> <em>Relic</em> APM, including a large number contributed by the open source community. To view <em>licenses</em> for... See... APM agents docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>product</em>-or-<em>service</em>-<em>licenses</em>&#x2F;<em>new</em>-<em>relic</em>-apm"
      },
      "id": "603e7895e7b9d24e832a07d5"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-synthetics/new-relic-synthetics-licenses": [
    {
      "sections": [
        "CodeStream licenses"
      ],
      "title": "CodeStream licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "CodeStream"
      ],
      "external_id": "331f3d1cd0897f453f98bc054fda09a9ad2725c1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/codestream/codestream-licenses/",
      "published_at": "2022-01-08T09:17:45Z",
      "updated_at": "2021-10-23T17:05:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. CodeStream license on GitHub CodeStream's third-party software notices on GitHub",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 150.7114,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "CodeStream <em>licenses</em>",
        "sections": "CodeStream <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and we use the following with CodeStream. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. CodeStream license on GitHub CodeStream&#x27;s third-party software notices on GitHub"
      },
      "id": "617440e2196a677ea62f0193"
    },
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2022-01-08T09:53:40Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0 © 2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0 © 2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0 © 2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0 © 2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0 © 2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright © 2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright © 2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright © 1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright © 1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.25278,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Logs plugin <em>licenses</em>",
        "sections": "Logs plugin <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    },
    {
      "sections": [
        "New Relic APM licenses"
      ],
      "title": "New Relic APM licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "e08100d5a48d87c7769f401957826f627bf29779",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses/",
      "published_at": "2022-01-08T08:26:31Z",
      "updated_at": "2021-03-30T21:22:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view licenses for... See... APM agents docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 137.69243,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>New</em> <em>Relic</em> APM <em>licenses</em>",
        "sections": "<em>New</em> <em>Relic</em> APM <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We use many third-party libraries and tools to create <em>New</em> <em>Relic</em> APM, including a large number contributed by the open source community. To view <em>licenses</em> for... See... APM agents docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>product</em>-or-<em>service</em>-<em>licenses</em>&#x2F;<em>new</em>-<em>relic</em>-apm"
      },
      "id": "603e7895e7b9d24e832a07d5"
    }
  ],
  "/docs/licenses/translated-documentation/translation-disclaimer": [
    {
      "sections": [
        "New Relic One pricing model: Frequently asked questions",
        "Frequently asked questions",
        "Q: Where can I find more information about New Relic One and related products?",
        "Q: Is there a limit on the number of concurrent sessions or IP addresses that may be used with a user account?",
        "Q: What terms govern my use of the Products?",
        "Q: How are the number of full users (also referred to as Monthly Provisioned Users, or full platform users) calculated for each month?",
        "Q: Can I mix and match New Relic One Standard, Pro, and Enterprise users in a billing account?",
        "Q: Can Event extended retention, additional Synthetics Checks, or additional New Relic Edge data be added to my account?",
        "Q: What is included in New Relic's Free Tier of Products?",
        "Q: How does the ‘Free Tier’ impact my pay-as-you-go (PAYG) or Annual Pool of Funds (APOF) subscription?"
      ],
      "title": "New Relic One pricing model: Frequently asked questions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "FAQ"
      ],
      "external_id": "cc702038ec7eb4983faa152108a4a233bd285264",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/faq/new-relic-one-pricing-plan-frequently-asked-questions/",
      "published_at": "2022-01-08T09:13:14Z",
      "updated_at": "2021-11-25T07:37:46Z",
      "document_type": "page",
      "popularity": 1,
      "body": "These are some frequently asked questions about the New Relic One pricing model that arise for agreement-level language. This is meant to serve as a supplement to the main New Relic One pricing docs. Frequently asked questions These FAQs are meant to serve as a supplement to the main New Relic One pricing docs. Q: Where can I find more information about New Relic One and related products? A: For a high level description of user entitlements for our features, see our pricing page. For more in-depth information, see New Relic One pricing. Q: Is there a limit on the number of concurrent sessions or IP addresses that may be used with a user account? A: Yes. See Manage users. Q: What terms govern my use of the Products? A: If a Customer has paid New Relic during a rolling 12-month period, then Customer’s usage of the Products is covered by the Paid Terms of Service. If a Customer has not paid New Relic during a 12-month period, then Customer’s usage of the Products is covered by the Unpaid Terms of Service. Q: How are the number of full users (also referred to as Monthly Provisioned Users, or full platform users) calculated for each month? A: Please see Calculation details. Q: Can I mix and match New Relic One Standard, Pro, and Enterprise users in a billing account? A: No. For the New Relic One pricing model, you can only have one subscription per organization (group of accounts that share the same billing ID). Q: Can Event extended retention, additional Synthetics Checks, or additional New Relic Edge data be added to my account? A: For additional retention for Events or Logs beyond the Standard Data Retention, contact your New Relic account executive. Note: Minimum requirement for Extended Retention requires an Annual Pool of Funds subscription. Q: What is included in New Relic's Free Tier of Products? A: Please see this pricing doc for a description of what's included. Customer’s use of the Free Tier shall be governed by the terms and conditions described in the Unpaid Terms of Service. If Customer’s usage exceeds the Free Tier, Customer is fully responsible for fees incurred in excess of the Free Tier as described in the “Pay-as-you-go” program and as described in the Paid Terms of Service. Q: How does the ‘Free Tier’ impact my pay-as-you-go (PAYG) or Annual Pool of Funds (APOF) subscription? A: The Free Tier usage will be deducted automatically from the Monthly Product Usage for Pay-as-you-go or Annual pool of funds subscriptions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 134.17111,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Q: Where can I find more <em>information</em> about New Relic One and related products?",
        "tags": "<em>License</em> <em>information</em>",
        "body": " pricing docs. Q: Where can I find more <em>information</em> about New Relic One and related products? A: For a high level description of user entitlements for our features, see our pricing page. For more in-depth <em>information</em>, see New Relic One pricing. Q: Is there a limit on the number of concurrent sessions"
      },
      "id": "6044e6e5196a67b568960f3e"
    },
    {
      "sections": [
        "Security guide",
        "Tip",
        "Security Program",
        "Security Domains",
        "Security Certifications",
        "Data Control, Facilities, and Encryption",
        "Law Enforcement Request Report"
      ],
      "title": "Security guide",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "356f0d11ffcb62208a743a0a7c127f5f6da9c940",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-guide/",
      "published_at": "2022-01-08T12:36:19Z",
      "updated_at": "2021-12-09T15:28:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Last updated September 17, 2021. This is supplement to our security policy and serves as a guide to New Relic’s description of its Services, functionalities, and features. Tip We may update the URLs in this document without notice. Security Program New Relic follows \"privacy by design\" principles as described here: https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Security Domains New Relic’s policies and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless Security; Configuration Management; Vulnerability Management; Network Protection; Transmission Protection; Password Management; Access Control, Audit Logging & Monitoring; Education, Training, and Awareness; Third Party Assurance; Incident Management; Business Continuity and Disaster Recover; Risk Management; Data Protection & Privacy; and Service Management Systems. Security Certifications New Relic audits its Services against industry standards as described at https://docs.newrelic.com/docs/security/security-privacy/compliance/regulatory-audits-new-relic-services/. Data Control, Facilities, and Encryption New Relic's customers can send data to New Relic's APIs by (1) using New Relic's software, (2) using vendor-neutral software that is managed and maintained by a third-party such as via OpenTelemetry instrumentation provided by opentelemetry.io, or (3) from third-party systems that customer's manage and/or control. New Relic's customers can use New Relic's Services such as NerdGraph to filter out and drop data. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/drop-data-using-nerdgraph/. New Relic's customers can adjust their data retention periods as appropriate for their needs. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/manage-data-retention/#adjust-retention. New Relic Logs obfuscates numbers that match known patterns, such as bank card and social security numbers as described here: https://docs.newrelic.com/docs/logs/log-management/get-started/new-relics-log-management-security-privacy/. New Relic honors requests to delete personal data in accordance with applicable privacy laws. Please see https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Customers may use New Relic's APIs to query data, such as NerdGraph described here, and New Relic Services to export the data to other cloud providers. Customers can configure its log forwarder [https://docs.newrelic.com/docs/logs/enable-log-management-new-relic/enable-log-monitoring-new-relic/forward-your-logs-using-infrastructure-agent/] before sending infrastructure logs to New Relic. For New Relic Customers in New Relic US, FedRAMP and HIPAA-enabled environments, Customer Data is replicated to the off-site backup system via Amazon Simple Storage Service (S3). Category of Customer Description FedRAMP HIPAA-enabled US Gen Pop EU Gen Pop Data is stored in Amazon Web Services (“AWS”). Limited Data is stored in IBM Data for New Relic Incident Intelligence is stored in Google Cloud New Relic regularly tests, assess, and evaluates its measures to ensure the security of processing using industry-recognized standards and uses independent third-party auditors as provided below: Annual SOC 2 Type 2 Annual FedRAMP assessment by an independent third-party pursuant to NIST 800-53 rev 4 Moderate authorization. Annual HITRUST-validated assessment by an independent third-party *Pursuing CY2021 Q4 ISO 27001 TISAX The Services that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed information about AWS security is available at https://aws.amazon.com/security/ and http://aws.amazon.com/security/sharing-the-security-responsibility/. Data encryption at rest utilizes FIPS 140-2 compliant encryption methodology. For AWS SOC Reports, please see https://aws.amazon.com/compliance/soc-faqs/. The Services that operate on Google Cloud Platform (\"GCP\") are protected by the security and environmental controls of GCP. Detailed information about GCP security is available at https://cloud.google.com/docs/tutorials#security. For GCP reports, please see https://cloud.google.com/security/compliance/. IBM Deft Zayo QTS Law Enforcement Request Report New Relic has not to date received any request for customer data from a law enforcement or other government agency (including under any national security process), and has not made any corresponding disclosures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 131.1221,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed <em>information</em> about AWS security is available at https:&#x2F;&#x2F;aws.amazon.com&#x2F;security&#x2F; and http:&#x2F;&#x2F;aws.amazon.com&#x2F;security&#x2F;sharing-the-security-responsibility&#x2F;. Data encryption at rest utilizes"
      },
      "id": "6147558128ccbc973a56a863"
    },
    {
      "sections": [
        "New Relic One pricing: Definitions",
        "Account",
        "Commitment Term",
        "Customer Data",
        "Customer Properties",
        "Documentation",
        "GB Ingested",
        "Incident event",
        "Login Credentials",
        "Monthly Provisioned User",
        "Order",
        "Paid Terms of Service",
        "Product(s)",
        "Software",
        "Terms",
        "Third-Party Services",
        "Unpaid Terms of Service",
        "Usage Plan"
      ],
      "title": "New Relic One pricing: Definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "609575acd671fecf7899378157eabc57bc8d68e2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/new-relic-one-pricing-definitions/",
      "published_at": "2022-01-08T09:16:18Z",
      "updated_at": "2021-11-25T06:51:33Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our New Relic One pricing model (for a glossary for our original pricing, see Original pricing definitions. Account Account refers to the online account or subaccounts that New Relic provides for customers to manage their use of the Products. Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, information, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer Properties Customer Properties means Customer’s websites, infrastructure, networks, mobile applications, or other systems, as well as Customer accounts on Third-Party Services. Documentation Documentation means the New Relic technical guides and documentation made available from the dedicated ‘Documentation’ page of the New Relic website. GB Ingested A GB Ingested is a measurement of the volume of metrics, events, logs, traces, or other telemetry data sent to or generated by the Products for the benefit of the Customer, including from the Software, the Customer Properties, or Third-Party Services. In this context, a GB is defined as 1 billion bytes. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Login Credentials Login Credentials means the username, email address, password, or other personal information that is provided by a Customer user in order to manage an Account. Monthly Provisioned User A Monthly Provisioned User is any user who can log into Customer’s Account(s) and access the New Relic One Product functionality as specified in an Order and the Documentation. In our public docs, this is referred to as a full platform user or a full user. Order Order means the purchasing order for access to the Service or related services that: (1) is either executed by the Parties or entered into by you via self-service, and references this Agreement, or (2) is entered into by you and a Channel Partner. Paid Terms of Service Paid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/paid. Product(s) Product(s) mean the purchase of the New Relic subscription products described in the applicable Order and any updates, corrections, bug fixes, modifications, improvements, related services, new features, and functionality (made generally available to New Relic’s customer base) thereto. Software Software means the distributed software, APIs, scripts, or other code proprietary to New Relic provided with the Products. Terms Terms means the underlying Customer-New Relic agreement and the Order. Third-Party Services Third-Party Services means any third party platform, add-on, service, or product not provided by New Relic and that a user integrates or enables for use with the Products, including third-party applications and plug-ins. Unpaid Terms of Service Unpaid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/unpaid. Usage Plan Usage Plan refers to the Service or Product pricing, invoicing related information, and product-specific terms (e.g. concurrent user account sessions) contained within the Documentation. To learn more about this pricing model, see New Relic One pricing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 123.27106,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": ". Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, <em>information</em>, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer"
      },
      "id": "6044e6e528ccbc26f22c6084"
    }
  ],
  "/docs/logs/forward-logs/aws-firelens-plugin-log-forwarding": [
    {
      "sections": [
        "Fluent Bit plugin for log forwarding",
        "Basic process",
        "Install the Fluent Bit plugin",
        "Tip",
        "Configure the Fluent Bit plugin",
        "Important",
        "Test the Fluent Bit plugin",
        "Associate logs with entities",
        "Optional: Configure plugin attributes",
        "View log data",
        "What's next?"
      ],
      "title": "Fluent Bit plugin for log forwarding",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "dd686f074bb80908ae9de19e543a3bae7d87a466",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/fluent-bit-plugin-log-forwarding/",
      "published_at": "2022-01-08T02:38:07Z",
      "updated_at": "2022-01-08T02:38:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your log data in New Relic. Forwarding your Fluent Bit logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Basic process We have published a container with the plugin installed. It serves as a base image to be used by our Kubernetes integration. We recommend you use this base image and layer your own custom configuration files. To forward your logs from Fluent Bit to New Relic: Make sure you have: A New Relic license key Fluent Bit 1.0 or higher (recommended), although v0.12 or higher is supported Fluent Bit Windows install directions can be found here Fluent Bit Linux install directions can be found here Install the Fluent Bit plugin. Configure the Fluent Bit plugin. Test the Fluent Bit plugin. Generate some traffic and wait a few minutes, then check your account for data. Install the Fluent Bit plugin To install the Fluent Bit plugin: Navigate to New Relic's Fluent Bit plugin repository on GitHub. From the repository page, clone or download the repository. Run the following command to build your plugin: cd newrelic-fluent-bit-output && make all Copy Store out_newrelic.so or out_newrelic_winXX.dll at a location that can be accessed by the fluent-bit daemon. Tip If you'd rather not compile the plugin yourself, you can download pre-compiled versions from our GitHub repository's releases page. Configure the Fluent Bit plugin Fluent Bit needs to know the location of the New Relic plugin and the New Relic license key to output data to New Relic. To configure your Fluent Bit plugin: Important Pay attention to white space when editing your config files. Be sure to use four spaces to indent and one space between keys and values. Locate or create a plugins.conf file in your plugins directory. In the plugins.conf file, add a reference to out_newrelic.so, adjacent to your fluent-bit.conf file: [PLUGINS] Path /PATH/TO/newrelic-fluent-bit-output/out_newrelic.so Copy In the fluent-bit.conf file, add the following line under the service block: [SERVICE] # This is the main configuration block for fluent bit. # Ensure the follow line exists somewhere in the SERVICE block Plugins_File plugins.conf Copy At the bottom of the fluent-bit.conf file, add the following to set up the input and output filters. Replace the placeholder text with your New Relic license key: [INPUT] Name tail Path /PATH/TO/YOUR/LOG/FILE [OUTPUT] Name newrelic Match * licenseKey YOUR_LICENSE_KEY # Optional maxBufferSize 256000 maxRecords 1024 Copy Restart your Fluent Bit instance with the following command: fluent-bit -c /PATH/TO/fluent-bit.conf Copy Test the Fluent Bit plugin To test if your Fluent Bit plugin is receiving input from a log file: Run the following command to append a test log message to your log file: echo \"test message\" >> /PATH/TO/YOUR/LOG/FILE Copy Search the New Relic Logs UI for test message. Associate logs with entities To associate a log line with an entity, such as an infrastructure host, add a FILTER block: [FILTER] Name modify Match * # Or specify a match Add entity.guids <Your Entity GUID ID> # Optional Add hostname <Your hostname> # Optional Copy For more options, see the Fluent Bit modify filter documentation and our documentation to forward your logs using the infrastructure agent. Optional: Configure plugin attributes Once you have installed and configured the Fluent Bit plugin, you can use the following attributes to configure how the plugin sends data to New Relic: Key Description licenseKey The New Relic license key. Use either licenseKey (recommended) or apiKey, not both. Default: none maxBufferSize The maximum size the payloads sent, in bytes. Default: 256000 maxRecords The maximum number of records to send at a time. Default: 1024 apiKey Deprecated. Takes a New Relic Insights insert key, but using the licenseKey attribute is preferred. Use either licenseKey or apiKey, not both. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 374.04916,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Fluent Bit plugin for <em>log</em> forwarding",
        "sections": "Associate <em>logs</em> with entities",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "If your <em>log</em> data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your <em>log</em> data in <em>New</em> <em>Relic</em>. Forwarding your Fluent Bit <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert"
      },
      "id": "603ea7ec28ccbce542eba791"
    },
    {
      "sections": [
        "Forward your logs using the infrastructure agent",
        "Basic process",
        "System requirements",
        "Important",
        "Install the infrastructure agent",
        "Configure the infrastructure agent",
        "Log forwarding parameters",
        "Name (required)",
        "Log source (required)",
        "file",
        "systemd",
        "syslog",
        "tcp",
        "winlog",
        "Optional configuration",
        "attributes",
        "attributes automatically inserted by the infrastructure agent",
        "pattern",
        "max_line_kb",
        "fluentbit",
        "Sample configuration file",
        "logging.d/sample.yaml",
        "View your log data",
        "Troubleshooting",
        "No log data",
        "No data appears when tailing a file",
        "No data appears when capturing via a Syslog socket",
        "No data appears using infrastructure agent proxy",
        "Sending the infrastructure agent's logs to New Relic",
        "Caution",
        "Fluent Bit does not start with the infra agent",
        "Runtime error on Windows",
        "Errors when tailing a large amount of log files (Linux)",
        "What's next?",
        "Uninstall log forwarding"
      ],
      "title": "Forward your logs using the infrastructure agent",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "a1deac410f0eedfb819348524a85a73bbb9d9daf",
      "image": "https://docs.newrelic.com/static/5d4352531871b652878b3cb0768e3f23/c1b63/demotron-host-logs-123021.png",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/forward-your-logs-using-infrastructure-agent/",
      "published_at": "2022-01-09T01:46:14Z",
      "updated_at": "2022-01-07T01:41:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can forward your logs to New Relic using our infrastructure monitoring agent. This makes all of your logging data available in one location and provides deeper visibility into both your application and your platform performance data. Forwarding your logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Basic process To forward your logs through our infrastructure monitoring agent: If you haven't already, create a New Relic account. It's free, forever. Verify the system requirements needed for configuring logs. Ensure you have installed the infrastructure agent, version 1.11.4 or higher. Create a logging.yml configuration file in the infrastructure agent's logging.d directory. Configure your log sources and other parameters. Generate some traffic and wait a few minutes, then check your account for data. Explore your log data in the Logs UI and benefit from the log attributes automatically inserted by the infrastructure agent. Here is an example of logs for your host's UI. You can see logs in context of events for the selected time period, and drill down into detailed data for any of the highlighted attributes. To take advantage of even more capabilities, click Query logs from here to go directly to the Logs UI. Here is an example of a host's logs in context related to an event. System requirements To use the log forwarder of the infrastructure agent, make sure you meet the following requirements: Infrastructure agent version 1.11.4 or higher OpenSSL library 1.1.0 or higher is required by Infrastructure agent starting from 1.16.4. Built-in support for ARM64 architecture on Linux systems (in example, AWS Graviton architecture) added in Infrastructure agent 1.26.0. Important The log forwarding feature is not supported with the Docker container for infrastructure monitoring agents. The log forwarding feature is compatible with the following operating systems: Operating system Supported version Amazon Linux Amazon Linux 2 CentOS Version 7 or higher Debian Version 9 (\"Stretch\") or higher Exception: Version 11 is not supported. Red Hat Enterprise Linux (RHEL) Version 7 or higher SUSE Linux Enterprise Server (SLES) Version 12 Ubuntu Versions 16.04.x, 18.04.x and 20.04.x (LTS versions) Windows Windows Server 2012, 2016, 2019, and 2022, and their service packs. Windows 10 Install the infrastructure agent Starting with version 1.11.4, the infrastructure agent can forward logs to New Relic. To install and run the agent, use a package manager (Linux) or the MSI installer (Windows). Important The log forwarding feature is not included when the infrastructure agent is implemented using Linux tarball or Windows ZIP installations. To use the following links, make sure you are logged to your New Relic account. Amazon Linux CentOS Debian RHEL SLES Ubuntu Windows If you don't have a New Relic account yet, or if you prefer to follow the procedure manually, see our tutorial to install the package manager. Configure the infrastructure agent Configuration files describe which log sources are forwarded. Our infrastructure agent uses .yml files to configure logging. You can add as many config files as you want. To add a new configuration file for the log forwarding feature: Navigate to the log forwarder configuration folder: Linux: /etc/newrelic-infra/logging.d/ Windows: C:\\Program Files\\New Relic\\newrelic-infra\\logging.d\\ Create a logging.yml configuration file, and add the parameters you need. The logging.d directory has various .yml.example files you can use as a reference or starting point. The agent automatically processes new configuration files without having to restart the infrastructure monitoring service. The only exception to this is when configuring a custom Fluent Bit configuration. Log forwarding parameters The infrastructure log forwarding .yml config supports the following parameters: Name (required) To start, define a name of the log or logs you want to forward to New Relic. Log source (required) What you use for the log source will depend on where you want to forward your logs from. Available options include: file Path to the log file or files. The agent tracks changes on the log files in a way similar to tail -f shell. Example: logs: - name: example-log file: /var/log/example.log # Path to a single log file - name: example-log-two file: /var/log/example-two.log # Path to another single log file Copy The file parameter can point to a specific log file or multiple files by using wildcards applied to names and extensions; for example, /logs/*.log. You can use wildcards in place of directories in a file path, which can be used to tail files located in different directories. Example: logs: - name: docker-logs file: /var/lib/docker/containers/*/*.log # Path to multiple folders and files Copy Important Use of wildcards may significantly increase the number of file descriptors and inotify watches the Fluent Bit process keeps open, which can interfere with log collection if the host's file descriptor limit is reached. Tailing a large number of files may require you to increase the maximum number of file descriptors and inotify watchers allowed by the operating system. Please refer to Errors when tailing a large amount of log files for more details on how to increase them. systemd Use the systemd parameter to forward log messages that are collected by the journald daemon in Linux environments. This input type requires the agent to run in root mode. Example: logs: - name: systemd-example systemd: cupsd Copy syslog Syslog data source. Parameters: uri: Syslog socket. Format varies depending on the protocol: TCP/UDP network sockets: [tcp/udp]://LISTEN_ADDRESS:PORT Unix domain sockets: unix_[tcp/udp]:// + /socket/path parser: Syslog parser. Default is rfc3164. Use rfc5424 if your messages include fractional seconds. Note: rfc3164 currently does not work on SuSE. unix_permissions: default is 0644 for domain sockets; this limits entries to processes running as root. You can use 0666 to listen for non-root processes, at your own risk. When running the agent in privileged mode, ports and sockets must be available or owned by nri-agent, with 0666 file permissions, so that other processes can write logs to the sockets. logs: # TCP network socket - name: syslog-tcp-test syslog: uri: tcp://0.0.0.0:5140 # Use the tcp://LISTEN_ADDRESS:PORT format parser: rfc5424 # Default syslog parser is rfc3164 # UDP network socket - name: syslog-udp-test syslog: uri: udp://0.0.0.0:6140 # Use the udp://LISTEN_ADDRESS:PORT format max_line_kb: 35 # Unix TCP domain socket - name: syslog-unix-tcp-test syslog: uri: unix_tcp:///var/unix-tcp-socket-test unix_permissions: 0666 # Default is 0644. Change at your own risk # Unix UDP domain socket - name: syslog-unix-udp-test syslog: uri: unix_udp:///var/unix-udp-socket-test parser: rfc5424 Copy tcp Logs retrieved over TCP connections. Parameters: uri: TCP/IP socket to listen for incoming data. The URI format is tcp://LISTEN_ADDRESS:PORT format: format of the data. It can be json or none. separator: If format: none is used, you can define a separator string for splitting records (default: \\n). logs: - name: tcp-simple-test tcp: uri: tcp://0.0.0.0:1234 # Use the tcp://LISTEN_ADDRESS:PORT format format: none # Raw text - this is default for 'tcp' separator: \\t # String for separating raw text entries max_line_kb: 32 - name: tcp-json-test tcp: uri: tcp://0.0.0.0:2345 # Use the tcp://LISTEN_ADDRESS:PORT format format: json Copy winlog Collect events from Windows log channels. Parameters: channel: name of the channel logs will be collected from. collect-eventids: a list of Windows Event IDs to be collected and forwarded to New Relic. Event ID ranges are supported. exclude-eventids: a list of Windows Event IDs to be excluded from collection. Event ID ranges are supported. All events are collected from the specified channel by default. Configure the collect-eventids and exclude-eventids sections to avoid sending unwanted logs to your New Relic account. Add event IDs or ranges to collect-eventids or exclude-eventids to forward or drop specific events. exclude-eventids takes precedence over collect-eventids if the same event ID is present in both sections. Example: logs: # Winlog log ingestion with eventId filters. - name: windows-security winlog: channel: Security collect-eventids: - 4624 - 4265 - 4700-4800 exclude-eventids: - 4735 # entries for the application, system, powershell, and SCOM channels - name: windows-application winlog: channel: Application - name: windows-system winlog: channel: System - name: windows-pshell winlog: channel: Windows Powershell - name: scom winlog: channel: Operations Manager # Entry for Windows Defender Logs - name: windows-defender winlog: channel: Microsoft-Windows-Windows Defender/Operational # Entry for Windows Clustering Logs - name: windows-clustering winlog: channel: Microsoft-Windows-FailoverClustering/Operational # Entry for IIS logs with logtype attribute for automatic parsing - name: iis-log file: C:\\inetpub\\logs\\LogFiles\\w3svc.log attributes: logtype: iis_w3c Copy Optional configuration The following configuration parameters are not required but are still recommended. attributes List of custom attributes specified as key-value pairs that can be used to send additional data with the logs which you can then query. The attributes configuration parameter can be used with any log source. One common use of the attributes configuration parameter is to specify the logtype attribute. This attribute allows leveraging one of the built-in parsing rules supported by New Relic Logs. Example: logs: - name: example-file-attributes file: /var/log/example.log attributes: logtype: nginx region: example-us-02 team: A-team - name: example-tcp-attributes tcp: uri: tcp://0.0.0.0:2345 format: json attributes: logtype: nginx region: example-us-02 team: B-team Copy attributes automatically inserted by the infrastructure agent The infrastructure agent automatically inserts log attributes for your convenience. Some of them are inserted for any log record, while others depend on the configuration parameters you used while setting up the log forwarder. Attribute name Description entity.guids Always inserted. The infrastructure agent inserts the Entity GUID assigned by New Relic to identify the host where it's running. It is available in the entity.guids field. Note: If the captured logs belong to an application instrumented using APM, the entity.guids field contains both the entity GUID of infrastructure, as well as the GUID of APM, separated by a pipe ( | ) delimiter. fb.input Always inserted. The underlying Fluent Bit input plugin type used to capture the logs. Currently its values are tail, systemd, winlog, syslog, and tcp. filePath Inserted when sing the file input type. Absolute file path of the file being monitored. hostname Always inserted. The hostname of the machine/VM/container executing the infrastructure agent. plugin.type Always inserted. Indicates the utility used to capture the logs. In this case, it is the infrastructure agent itself, so this attribute always has the value nri-agent. pattern Regular expression for filtering records. Only supported for the tail, systemd, syslog, and tcp (only with format none) sources. This field works in a way similar to grep -E in Unix systems. For example, for a given file being captured, you can filter for records containing either WARN or ERROR using: - name: only-records-with-warn-and-error file: /var/log/logFile.log pattern: WARN|ERROR Copy No filtering is applied by default. max_line_kb Maximum size of log entries/lines in KB. If log entries exceed the limit, they are skipped. Default is 128. fluentbit External Fluent Bit configuration and parser files. If defined, they are merged with the existing configuration and parser files generated by the infrastructure agent. The infrastructure agent processes the configuration files located in the logging.d directory and will generate a run-time Fluent Bit configuration file that contains the appropriate [INPUT], [FILTER] and [OUTPUT] sections. Optionally, it will also declare an @INCLUDE in case you provided an external Fluent Bit configuration file via the fluentbit option. The runtime file does not define a [SERVICE] section, leaving all default Fluent Bit configuration values. You can still override Fluent Bit's default settings by defining your own [SERVICE] section in your external Fluent Bit configuration file and include it via the fluentbit option. Parameters: config_file: path to an existing Fluent Bit configuration file. Note that any overlapping source results in duplicate messages in New Relic Logs. parsers_file: path to an existing Fluent Bit parsers file. The following parser names are reserved: rfc3164, rfc3164-local and rfc5424. Sample configuration file Here is an example of a logging.d/ configuration file in YAML format. For more configuration examples, see the infrastructure agent repository. logging.d/sample.yaml # Remember to only use spaces for indentation logs: # Example of 'file' source - name: file-with-attributes file: /var/log/test.log # Path to a single file or pattern attributes: # You can use custom attributes to enrich your data logtype: nginx team: The A Team pattern: Error # Regular expression to filter log entries # Example of 'systemd' source (Linux only) - name: systemd-example systemd: cupsd # Examples of 'syslog' source, one per protocol # TCP network socket - name: syslog-tcp-test syslog: uri: tcp://0.0.0.0:5140 # Use the tcp://LISTEN_ADDRESS:PORT format parser: rfc5424 # Default syslog parser is rfc3164 # UDP network socket - name: syslog-udp-test syslog: uri: udp://0.0.0.0:6140 # Use the udp://LISTEN_ADDRESS:PORT format max_line_kb: 35 # Paths for Unix sockets are defined by combining protocol and path: # unix_udp:// + /path/socket - for example, unix_udp:///tmp/socket # Unix TCP domain socket - name: syslog-unix-tcp-test syslog: uri: unix_tcp:///var/unix-tcp-socket-test unix_permissions: 0666 # Default is 0644. Change at your own risk # Unix UDP domain socket - name: syslog-unix-udp-test syslog: uri: unix_udp:///var/unix-udp-socket-test parser: rfc5424 # Examples of 'tcp' source for formats 'none' and 'json' - name: tcp-simple-test tcp: uri: tcp://0.0.0.0:1234 # Use the tcp://LISTEN_ADDRESS:PORT format format: none # Raw text - this is default for 'tcp' separator: \\t # String for separating raw text entries attributes: # You can add custom attributes to any source of logs tcpFormat: none someOtherAttribute: associatedValue max_line_kb: 32 - name: tcp-json-test tcp: uri: tcp://0.0.0.0:2345 # Use the tcp://LISTEN_ADDRESS:PORT format format: json attributes: tcpFormat: json yetAnotherAttribute: 12345 # Example of Fluent Bit configuration import - name: fluentbit-import fluentbit: config_file: /path/to/fluentbit.config parsers_file: /path/to/fluentbit/parsers.conf Copy View your log data If everything is configured correctly and your data is being collected, you should see logs data in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy Troubleshooting If you encounter problems with configuring your log forwarder, try these troubleshooting tips. No log data If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. No data appears when tailing a file The log forwarding feature requires the agent to have permission to read the data sources. When running the infrastructure agent in privileged or non-privileged modes, make sure that the log files you want to forward (and any intermediary directory in its path) are readable by the user running nri-agent. Example: Check file access under Linux Let's check whether the file /var/log/restrictedLogs/logFile.log can be monitored by the nri-agent user. In Linux, you can do a quick check with the namei command: sudo -u nri-agent namei -ml /var/log/restrictedLogs/logFile.log f: /var/log/restrictedLogs/logFile.log drwxr-xr-x root root / drwxr-xr-x root root var drwxrwxr-x root syslog log drwxr--r-- root root restrictedLogs logFile.log - No such file or directory Copy This command failed because the file is not visible to the nri-agent user. By inspecting the previous output, we can detect that the restrictedLogs directory is missing the execution flag for others. To fix this, execute: sudo chmod 755 /var/log/restrictedLogs Copy And then check for file access again: # sudo -u nri-agent namei -ml /var/log/restrictedLogs/logFile.log f: /var/log/restrictedLogs/logFile.log drwxr-xr-x root root / drwxr-xr-x root root var drwxrwxr-x root syslog log drwxr-xr-x root root restrictedLogs -rw-r----- vagrant vagrant logFile.log Copy The file is now visible to the nri-agent user. You must ensure that the file is also readable by the nri-agent user. To check this, use: # sudo -u nri-agent head /var/log/restrictedLogs/logFile.log head: cannot open '/var/log/restrictedLogs/logFile.log' for reading: Permission denied Copy In this example, the file is missing the read rights for the others group (users other than vagrant and the vagrant user group). You could fix this by granting read permissions to others, but the application could change these permissions upon restart. To avoid this, a better approach is to add the nri-agent user to the vagrant user group. No data appears when capturing via a Syslog socket The log forwarding feature requires that the agent has permission to read the data sources. When running the infrastructure agent in privileged or non-privileged modes: If you're using Unix domain socket files, make sure that the nri-agent user can access these files (please refer to the previous section) and that they have read and write permissions (666) so that other users than nri-agent can write to them. If you're using IP sockets, ensure that the port that you are using is not a system reserved one (like port 80, for example). If no data appears after you enable log management, follow standard log management troubleshooting procedures. No data appears using infrastructure agent proxy As explained in the infrastructure agent configuration guidelines, the proxy parameter must use either HTTP or HTTPS and be in the form https://user:password@hostname:port. The agent can parse the parameter without the HTTP or HTTPS, but the log forwarder cannot. You will see an error like the following in the agent verbose logs: [ERROR] building HTTP transport: parse \\\"hostname:port\\\": first path segment in URL cannot contain colon Copy To solve this problem, check your newrelic-infra.yml file, and ensure the proxy parameter adheres to this form. If you're using caBundleFile or caBundleDir in order to specify any certificate, we recommend to follow the below rules for each OS: Linux For HTTP proxies you don't need to setup any certificates. The plugin loads the system certificates and New Relic sends logs into the logging endpoint. However, you can specify the proxy self-signed certificate (PEM file) using either the caBundleFile or caBundleDir parameters. Windows For HTTP proxies you don't need to setup any certificates. The plugin loads the system certificates. For HTTPS, you can configure it in one of the following ways: Import the proxy certificate to the system pool (Recommended) Import the proxy self-signed certificate (PEM file) by using the MMC tool. Refer to this link, and in Step 2 ensure to import it in your Trusted Root Certification Authorities, instead of in the Intermediate Certification Authorities. Using the caBundleFile and caBundleDir parameters On Windows, we cannot load both the certificates from the system certificate pool and the ones specified with the caBundleFile caBundleDir parameters. So, if you are using caBundleFile or caBundleDir, ensure that the following certificates are placed in the same PEM file (when using caBundleFile) or in the same directory (when using caBundleDir): The Proxy certificate (because it's an HTTPS proxy). The Logging Endpoint certificate (eg. https://log-api.newrelic.com/log/v1). The Infrastructure Agent certificate (eg. https://infra-api.newrelic.com). You can check the certificates by running: # openssl s_client -connect log-api.newrelic.com:443 -servername log-api.newrelic.com Copy Sending the infrastructure agent's logs to New Relic You can configure the infrastructure agent to send its own logs to New Relic. This is useful for troubleshooting issues with log forwarding, the agent, or when contacting support. To forward the infrastructure agent logs to New Relic: Edit your newrelic-infra.yml file. Enable agent logging in troubleshooting mode by adding verbose: 3. On Windows and systems that don't use systemd or where journald is inaccessible, verbose:3 causes the agent to write the logs on the disk. Revert to verbose:0 to prevent this. (Recommended): Enable agent logging in JSON format to log_format: json. Restart the agent to load the new settings. This configuration sets up the agent in troubleshooting mode, but the log forwarder (based on Fluent Bit) will continue in a non-verbose mode. Sometimes you can have issues with the log forwarder itself. For example, there may be problems accessing a specific channel when shipping Windows log events or when accessing a particular log file. In these situations, you can also enable the verbose mode for the log forwarder: Set verbose to a value other than 0. Add the configuration option: trace: [\"log.fw\"]. Caution Check whether you are using the [fluentbit] option. When setting verbose: 3 and trace: [\"log.fw\"], ensure that you don't define any [OUTPUT] section pointing to stdout in an external Fluent Bit configuration file, Fluent Bit does not start with the infra agent Important Fluent Bit's tail plugin does not support network drives. For Linux versions prior to 2016, you may need to update the OpenSSL library to 1.1.0 (or higher). To check if you have this problem: See if infra-agent has started Fluent Bit by running: ps -aux | grep td-agent-bit Copy If it isn't running go to /var/db/newrelic-infra/newrelic-integrations/logging and run: ./fluent-bit -i systemd -o stdout Copy If you get the following error, update OpenSSL to 1.1.0 or higher: error while loading shared libraries: libssl.so.1.1: cannot open shared object file: No such file or directory Copy Runtime error on Windows One of the following error messages may appear when enabling log forwarding on Windows: The code execution cannot proceed because VCRUNTIME140.dll was not found. Copy OR error=\"exit status 3221225781\" process=log-forwarder Copy This is caused by a missing DLL. To solve the issue, install the Microsoft Visual C++ Redistributable as applicable: x64 x86 Errors when tailing a large amount of log files (Linux) It's common to face either one of the following error messages when attempting to tail a large amount of files: Too many open files The user limit on the total number of inotify watches was reached or the kernel failed to allocate a needed resource The operating system defines a maximum amount of allocatable file descriptors (typically 1024 by default), and a maximum amount of allocatable inotify watches (typically 8192 by default). Any process attempting to go above these limits will fail, returning one of the errors above. The underlying technology we use to forward logs, Fluent Bit, opens one file descriptor and sets an inotify watch for each file you configure to be forwarded. Moreover, at the time of writing this section, Fluent Bit uses an extra set of 32 file descriptors for its normal operation, with another extra file descriptor when it shuts down. Therefore, to capture a large amount of files you need to ensure that both the file descriptor and inotify watch limits are slightly greater than the amount of log files you wish to tail. The following instructions summarize how to increase these limits if you want to tail 10,000 log files. Also, it assumes the infrastructure agent is installed in root running mode, and therefore must be run using the root user. Check which is the current hard limit for the amount of file descriptors per process. Typically, this limit should be quite high and should not need to be modified. ulimit -Hn Copy Add the following line to /etc/security/limits.conf. We specified a limit of 10100 here instead of just 10000 to allow Fluent Bit to allocate the extra file descriptors it may need to work. root soft nofile 10100 # replace root by nri-agent for non-root (privileged and unprivileged) installations Copy Add the following line to /etc/pam.d/common-session so that the previous limit is applied upon restart: session required pam_limits.so Copy Add the following line to /etc/sysctl.conf to increase the amount of allowed inotify watchers per user. We specified a limit of 18192 here instead of just 10000 so that the root user will still have 8192 available inotify watches (the default value). fs.inotify.max_user_watches=18192 Copy Restart your system. Ensure that the new limits have been enforced by running: ulimit -Sn # Should return 10100 cat /proc/sys/fs/inotify/max_user_watches # Should return 18192 Copy Learn more on how to increase open file limits, or on how to increase the inotify watches. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards. Uninstall log forwarding To uninstall log forwarding capabilities, go to your logging.d directory, and remove files with the .yml extension that were originally added during the configuration process. Linux: /etc/newrelic-infra/logging.d/ Windows: C:\\Program Files\\New Relic\\newrelic-infra\\logging.d\\",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 353.44617,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward your <em>logs</em> using the <em>infrastructure</em> agent",
        "sections": "Sending the <em>infrastructure</em> agent&#x27;s <em>logs</em> to <em>New</em> <em>Relic</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " <em>management</em> capabilities to collect, process, explore, query, and alert on your <em>log</em> data. Basic process To forward your <em>logs</em> through our infrastructure <em>monitoring</em> agent: If you haven&#x27;t already, create a <em>New</em> <em>Relic</em> account. It&#x27;s free, forever. Verify the system requirements needed for configuring <em>logs</em>"
      },
      "id": "603e9df164441f6b6f4e8843"
    },
    {
      "sections": [
        "Forward your logs to New Relic",
        "How it works",
        "Get started with log forwarders",
        "Log forwarding options",
        "TCP endpoint",
        "Log API",
        "What's next?"
      ],
      "title": "Forward your logs to New Relic",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "b6e203ddc367d5a2b5e002916e49d34f4ba17a87",
      "image": "https://docs.newrelic.com/static/c3d5443b84a1e2b26a4767ce35fa58f3/e5166/new-relic-logs-in-context-diagram.jpg",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/enable-log-management-new-relic/",
      "published_at": "2022-01-08T02:40:08Z",
      "updated_at": "2022-01-04T07:02:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our log management capabilities help you to collect, process, explore, query, and alert on your log data. To get your logs into New Relic, you can: Use your existing log forwarding solution to collect your logs and extend the metadata that is forwarded to New Relic. Use our infrastructure agent as a lightweight data collector, without having to use additional software. Use our Log API to forward your logs via HTTP. Use syslog protocols to forward your logs via a TCP endpoint. How it works The following diagram shows the lifecycle of a log message for an app, from enrichment with APM agent metadata (contextual logging), to formatting and forwarding the log data to New Relic. This diagram illustrates the flow of log messages through New Relic. Standard log formatters transform log events into meaningful output (such as text files) that can be used by downstream people and processes. The NewRelicFormatter transforms log events into the JSON format expected by New Relic. These files contain log information and extended metadata. When you configure your log forwarder (our infrastructure monitoring agent, Fluentd, Logstash, etc.), you can also extend and enrich your log data. By configuring logs in context, the log enricher links the formatted log data with additional transaction information from your application or host. Now your log files are enriched with enhanced metadata and contextual logging data. Your log forwarder sends the files to our logging endpoint for processing. From there you can use our log management capabilities to view, query, set up alerts, and more in New Relic. Here is an example of logs for your host's UI. You can see logs in context of events for the selected time period, and drill down into detailed data for any of the highlighted attributes. To take advantage of even more capabilities, click Query logs from here to go directly to the Logs UI. Here is an example of a host's logs in context related to an event. Get started with log forwarders To forward your logs to New Relic with enriched metadata: If you don't have one already, create a New Relic account. It's free, forever. Have your New Relic account's license key. Install a compatible log forwarder. Ensure that outbound connectivity on TCP port 443 is allowed to the CIDR range that matches your region. Use the DNS name log-api.newrelic.com or log-api.eu.newrelic.com for configuration. For apps monitored by a New Relic APM agent, configure logs in context. Log forwarding options Use any of these solutions to forward your logs to New Relic. Recommended: Infrastructure monitoring agent Amazon: AWS CloudWatch plugin AWS FireLens plugin AWS Kinesis Firehose AWS Lambda for sending logs from S3 Microsoft: Azure ARM template Other log forwarding plugins: Fluent Bit plugin Fluentd plugin Google Cloud Platform Pub/Sub Heroku log streaming Kubernetes plugin Logstash plugin Vector plugin TCP endpoint In some situations you may not have log forwarders; for example, with CDNs, hardware devices, or managed services. You can use syslog protocols such as rsyslog and syslog-ng, and forward your logs to New Relic via a TCP endpoint. Log API If you prefer to connect to New Relic without installing a plugin, we offer an HTTP input integration. This option sends your monitored log data directly to New Relic via the Log API. What's next? After you enable your log forwarder, make the most of your data in New Relic with our log management capabilities: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data if you are forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards. For example, to query and manage your data partition rules, see our NerdGraph tutorial.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 314.5207,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward your <em>logs</em> to <em>New</em> <em>Relic</em>",
        "sections": "Forward your <em>logs</em> to <em>New</em> <em>Relic</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " directly to <em>New</em> <em>Relic</em> via the <em>Log</em> API. What&#x27;s next? After you <em>enable</em> your <em>log</em> forwarder, make the most of your data in <em>New</em> <em>Relic</em> with our <em>log</em> <em>management</em> capabilities: Explore the logging data across your platform with our <em>Logs</em> UI. See your <em>logs</em> in context of your app&#x27;s performance in the APM UI"
      },
      "id": "61571e0e28ccbcbc52f21431"
    }
  ],
  "/docs/logs/forward-logs/aws-lambda-sending-cloudwatch-logs": [
    {
      "sections": [
        "Fluent Bit plugin for log forwarding",
        "Basic process",
        "Install the Fluent Bit plugin",
        "Tip",
        "Configure the Fluent Bit plugin",
        "Important",
        "Test the Fluent Bit plugin",
        "Associate logs with entities",
        "Optional: Configure plugin attributes",
        "View log data",
        "What's next?"
      ],
      "title": "Fluent Bit plugin for log forwarding",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "dd686f074bb80908ae9de19e543a3bae7d87a466",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/fluent-bit-plugin-log-forwarding/",
      "published_at": "2022-01-08T02:38:07Z",
      "updated_at": "2022-01-08T02:38:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your log data in New Relic. Forwarding your Fluent Bit logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Basic process We have published a container with the plugin installed. It serves as a base image to be used by our Kubernetes integration. We recommend you use this base image and layer your own custom configuration files. To forward your logs from Fluent Bit to New Relic: Make sure you have: A New Relic license key Fluent Bit 1.0 or higher (recommended), although v0.12 or higher is supported Fluent Bit Windows install directions can be found here Fluent Bit Linux install directions can be found here Install the Fluent Bit plugin. Configure the Fluent Bit plugin. Test the Fluent Bit plugin. Generate some traffic and wait a few minutes, then check your account for data. Install the Fluent Bit plugin To install the Fluent Bit plugin: Navigate to New Relic's Fluent Bit plugin repository on GitHub. From the repository page, clone or download the repository. Run the following command to build your plugin: cd newrelic-fluent-bit-output && make all Copy Store out_newrelic.so or out_newrelic_winXX.dll at a location that can be accessed by the fluent-bit daemon. Tip If you'd rather not compile the plugin yourself, you can download pre-compiled versions from our GitHub repository's releases page. Configure the Fluent Bit plugin Fluent Bit needs to know the location of the New Relic plugin and the New Relic license key to output data to New Relic. To configure your Fluent Bit plugin: Important Pay attention to white space when editing your config files. Be sure to use four spaces to indent and one space between keys and values. Locate or create a plugins.conf file in your plugins directory. In the plugins.conf file, add a reference to out_newrelic.so, adjacent to your fluent-bit.conf file: [PLUGINS] Path /PATH/TO/newrelic-fluent-bit-output/out_newrelic.so Copy In the fluent-bit.conf file, add the following line under the service block: [SERVICE] # This is the main configuration block for fluent bit. # Ensure the follow line exists somewhere in the SERVICE block Plugins_File plugins.conf Copy At the bottom of the fluent-bit.conf file, add the following to set up the input and output filters. Replace the placeholder text with your New Relic license key: [INPUT] Name tail Path /PATH/TO/YOUR/LOG/FILE [OUTPUT] Name newrelic Match * licenseKey YOUR_LICENSE_KEY # Optional maxBufferSize 256000 maxRecords 1024 Copy Restart your Fluent Bit instance with the following command: fluent-bit -c /PATH/TO/fluent-bit.conf Copy Test the Fluent Bit plugin To test if your Fluent Bit plugin is receiving input from a log file: Run the following command to append a test log message to your log file: echo \"test message\" >> /PATH/TO/YOUR/LOG/FILE Copy Search the New Relic Logs UI for test message. Associate logs with entities To associate a log line with an entity, such as an infrastructure host, add a FILTER block: [FILTER] Name modify Match * # Or specify a match Add entity.guids <Your Entity GUID ID> # Optional Add hostname <Your hostname> # Optional Copy For more options, see the Fluent Bit modify filter documentation and our documentation to forward your logs using the infrastructure agent. Optional: Configure plugin attributes Once you have installed and configured the Fluent Bit plugin, you can use the following attributes to configure how the plugin sends data to New Relic: Key Description licenseKey The New Relic license key. Use either licenseKey (recommended) or apiKey, not both. Default: none maxBufferSize The maximum size the payloads sent, in bytes. Default: 256000 maxRecords The maximum number of records to send at a time. Default: 1024 apiKey Deprecated. Takes a New Relic Insights insert key, but using the licenseKey attribute is preferred. Use either licenseKey or apiKey, not both. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 374.04916,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Fluent Bit plugin for <em>log</em> forwarding",
        "sections": "Associate <em>logs</em> with entities",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "If your <em>log</em> data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your <em>log</em> data in <em>New</em> <em>Relic</em>. Forwarding your Fluent Bit <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert"
      },
      "id": "603ea7ec28ccbce542eba791"
    },
    {
      "sections": [
        "Forward your logs using the infrastructure agent",
        "Basic process",
        "System requirements",
        "Important",
        "Install the infrastructure agent",
        "Configure the infrastructure agent",
        "Log forwarding parameters",
        "Name (required)",
        "Log source (required)",
        "file",
        "systemd",
        "syslog",
        "tcp",
        "winlog",
        "Optional configuration",
        "attributes",
        "attributes automatically inserted by the infrastructure agent",
        "pattern",
        "max_line_kb",
        "fluentbit",
        "Sample configuration file",
        "logging.d/sample.yaml",
        "View your log data",
        "Troubleshooting",
        "No log data",
        "No data appears when tailing a file",
        "No data appears when capturing via a Syslog socket",
        "No data appears using infrastructure agent proxy",
        "Sending the infrastructure agent's logs to New Relic",
        "Caution",
        "Fluent Bit does not start with the infra agent",
        "Runtime error on Windows",
        "Errors when tailing a large amount of log files (Linux)",
        "What's next?",
        "Uninstall log forwarding"
      ],
      "title": "Forward your logs using the infrastructure agent",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "a1deac410f0eedfb819348524a85a73bbb9d9daf",
      "image": "https://docs.newrelic.com/static/5d4352531871b652878b3cb0768e3f23/c1b63/demotron-host-logs-123021.png",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/forward-your-logs-using-infrastructure-agent/",
      "published_at": "2022-01-09T01:46:14Z",
      "updated_at": "2022-01-07T01:41:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can forward your logs to New Relic using our infrastructure monitoring agent. This makes all of your logging data available in one location and provides deeper visibility into both your application and your platform performance data. Forwarding your logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Basic process To forward your logs through our infrastructure monitoring agent: If you haven't already, create a New Relic account. It's free, forever. Verify the system requirements needed for configuring logs. Ensure you have installed the infrastructure agent, version 1.11.4 or higher. Create a logging.yml configuration file in the infrastructure agent's logging.d directory. Configure your log sources and other parameters. Generate some traffic and wait a few minutes, then check your account for data. Explore your log data in the Logs UI and benefit from the log attributes automatically inserted by the infrastructure agent. Here is an example of logs for your host's UI. You can see logs in context of events for the selected time period, and drill down into detailed data for any of the highlighted attributes. To take advantage of even more capabilities, click Query logs from here to go directly to the Logs UI. Here is an example of a host's logs in context related to an event. System requirements To use the log forwarder of the infrastructure agent, make sure you meet the following requirements: Infrastructure agent version 1.11.4 or higher OpenSSL library 1.1.0 or higher is required by Infrastructure agent starting from 1.16.4. Built-in support for ARM64 architecture on Linux systems (in example, AWS Graviton architecture) added in Infrastructure agent 1.26.0. Important The log forwarding feature is not supported with the Docker container for infrastructure monitoring agents. The log forwarding feature is compatible with the following operating systems: Operating system Supported version Amazon Linux Amazon Linux 2 CentOS Version 7 or higher Debian Version 9 (\"Stretch\") or higher Exception: Version 11 is not supported. Red Hat Enterprise Linux (RHEL) Version 7 or higher SUSE Linux Enterprise Server (SLES) Version 12 Ubuntu Versions 16.04.x, 18.04.x and 20.04.x (LTS versions) Windows Windows Server 2012, 2016, 2019, and 2022, and their service packs. Windows 10 Install the infrastructure agent Starting with version 1.11.4, the infrastructure agent can forward logs to New Relic. To install and run the agent, use a package manager (Linux) or the MSI installer (Windows). Important The log forwarding feature is not included when the infrastructure agent is implemented using Linux tarball or Windows ZIP installations. To use the following links, make sure you are logged to your New Relic account. Amazon Linux CentOS Debian RHEL SLES Ubuntu Windows If you don't have a New Relic account yet, or if you prefer to follow the procedure manually, see our tutorial to install the package manager. Configure the infrastructure agent Configuration files describe which log sources are forwarded. Our infrastructure agent uses .yml files to configure logging. You can add as many config files as you want. To add a new configuration file for the log forwarding feature: Navigate to the log forwarder configuration folder: Linux: /etc/newrelic-infra/logging.d/ Windows: C:\\Program Files\\New Relic\\newrelic-infra\\logging.d\\ Create a logging.yml configuration file, and add the parameters you need. The logging.d directory has various .yml.example files you can use as a reference or starting point. The agent automatically processes new configuration files without having to restart the infrastructure monitoring service. The only exception to this is when configuring a custom Fluent Bit configuration. Log forwarding parameters The infrastructure log forwarding .yml config supports the following parameters: Name (required) To start, define a name of the log or logs you want to forward to New Relic. Log source (required) What you use for the log source will depend on where you want to forward your logs from. Available options include: file Path to the log file or files. The agent tracks changes on the log files in a way similar to tail -f shell. Example: logs: - name: example-log file: /var/log/example.log # Path to a single log file - name: example-log-two file: /var/log/example-two.log # Path to another single log file Copy The file parameter can point to a specific log file or multiple files by using wildcards applied to names and extensions; for example, /logs/*.log. You can use wildcards in place of directories in a file path, which can be used to tail files located in different directories. Example: logs: - name: docker-logs file: /var/lib/docker/containers/*/*.log # Path to multiple folders and files Copy Important Use of wildcards may significantly increase the number of file descriptors and inotify watches the Fluent Bit process keeps open, which can interfere with log collection if the host's file descriptor limit is reached. Tailing a large number of files may require you to increase the maximum number of file descriptors and inotify watchers allowed by the operating system. Please refer to Errors when tailing a large amount of log files for more details on how to increase them. systemd Use the systemd parameter to forward log messages that are collected by the journald daemon in Linux environments. This input type requires the agent to run in root mode. Example: logs: - name: systemd-example systemd: cupsd Copy syslog Syslog data source. Parameters: uri: Syslog socket. Format varies depending on the protocol: TCP/UDP network sockets: [tcp/udp]://LISTEN_ADDRESS:PORT Unix domain sockets: unix_[tcp/udp]:// + /socket/path parser: Syslog parser. Default is rfc3164. Use rfc5424 if your messages include fractional seconds. Note: rfc3164 currently does not work on SuSE. unix_permissions: default is 0644 for domain sockets; this limits entries to processes running as root. You can use 0666 to listen for non-root processes, at your own risk. When running the agent in privileged mode, ports and sockets must be available or owned by nri-agent, with 0666 file permissions, so that other processes can write logs to the sockets. logs: # TCP network socket - name: syslog-tcp-test syslog: uri: tcp://0.0.0.0:5140 # Use the tcp://LISTEN_ADDRESS:PORT format parser: rfc5424 # Default syslog parser is rfc3164 # UDP network socket - name: syslog-udp-test syslog: uri: udp://0.0.0.0:6140 # Use the udp://LISTEN_ADDRESS:PORT format max_line_kb: 35 # Unix TCP domain socket - name: syslog-unix-tcp-test syslog: uri: unix_tcp:///var/unix-tcp-socket-test unix_permissions: 0666 # Default is 0644. Change at your own risk # Unix UDP domain socket - name: syslog-unix-udp-test syslog: uri: unix_udp:///var/unix-udp-socket-test parser: rfc5424 Copy tcp Logs retrieved over TCP connections. Parameters: uri: TCP/IP socket to listen for incoming data. The URI format is tcp://LISTEN_ADDRESS:PORT format: format of the data. It can be json or none. separator: If format: none is used, you can define a separator string for splitting records (default: \\n). logs: - name: tcp-simple-test tcp: uri: tcp://0.0.0.0:1234 # Use the tcp://LISTEN_ADDRESS:PORT format format: none # Raw text - this is default for 'tcp' separator: \\t # String for separating raw text entries max_line_kb: 32 - name: tcp-json-test tcp: uri: tcp://0.0.0.0:2345 # Use the tcp://LISTEN_ADDRESS:PORT format format: json Copy winlog Collect events from Windows log channels. Parameters: channel: name of the channel logs will be collected from. collect-eventids: a list of Windows Event IDs to be collected and forwarded to New Relic. Event ID ranges are supported. exclude-eventids: a list of Windows Event IDs to be excluded from collection. Event ID ranges are supported. All events are collected from the specified channel by default. Configure the collect-eventids and exclude-eventids sections to avoid sending unwanted logs to your New Relic account. Add event IDs or ranges to collect-eventids or exclude-eventids to forward or drop specific events. exclude-eventids takes precedence over collect-eventids if the same event ID is present in both sections. Example: logs: # Winlog log ingestion with eventId filters. - name: windows-security winlog: channel: Security collect-eventids: - 4624 - 4265 - 4700-4800 exclude-eventids: - 4735 # entries for the application, system, powershell, and SCOM channels - name: windows-application winlog: channel: Application - name: windows-system winlog: channel: System - name: windows-pshell winlog: channel: Windows Powershell - name: scom winlog: channel: Operations Manager # Entry for Windows Defender Logs - name: windows-defender winlog: channel: Microsoft-Windows-Windows Defender/Operational # Entry for Windows Clustering Logs - name: windows-clustering winlog: channel: Microsoft-Windows-FailoverClustering/Operational # Entry for IIS logs with logtype attribute for automatic parsing - name: iis-log file: C:\\inetpub\\logs\\LogFiles\\w3svc.log attributes: logtype: iis_w3c Copy Optional configuration The following configuration parameters are not required but are still recommended. attributes List of custom attributes specified as key-value pairs that can be used to send additional data with the logs which you can then query. The attributes configuration parameter can be used with any log source. One common use of the attributes configuration parameter is to specify the logtype attribute. This attribute allows leveraging one of the built-in parsing rules supported by New Relic Logs. Example: logs: - name: example-file-attributes file: /var/log/example.log attributes: logtype: nginx region: example-us-02 team: A-team - name: example-tcp-attributes tcp: uri: tcp://0.0.0.0:2345 format: json attributes: logtype: nginx region: example-us-02 team: B-team Copy attributes automatically inserted by the infrastructure agent The infrastructure agent automatically inserts log attributes for your convenience. Some of them are inserted for any log record, while others depend on the configuration parameters you used while setting up the log forwarder. Attribute name Description entity.guids Always inserted. The infrastructure agent inserts the Entity GUID assigned by New Relic to identify the host where it's running. It is available in the entity.guids field. Note: If the captured logs belong to an application instrumented using APM, the entity.guids field contains both the entity GUID of infrastructure, as well as the GUID of APM, separated by a pipe ( | ) delimiter. fb.input Always inserted. The underlying Fluent Bit input plugin type used to capture the logs. Currently its values are tail, systemd, winlog, syslog, and tcp. filePath Inserted when sing the file input type. Absolute file path of the file being monitored. hostname Always inserted. The hostname of the machine/VM/container executing the infrastructure agent. plugin.type Always inserted. Indicates the utility used to capture the logs. In this case, it is the infrastructure agent itself, so this attribute always has the value nri-agent. pattern Regular expression for filtering records. Only supported for the tail, systemd, syslog, and tcp (only with format none) sources. This field works in a way similar to grep -E in Unix systems. For example, for a given file being captured, you can filter for records containing either WARN or ERROR using: - name: only-records-with-warn-and-error file: /var/log/logFile.log pattern: WARN|ERROR Copy No filtering is applied by default. max_line_kb Maximum size of log entries/lines in KB. If log entries exceed the limit, they are skipped. Default is 128. fluentbit External Fluent Bit configuration and parser files. If defined, they are merged with the existing configuration and parser files generated by the infrastructure agent. The infrastructure agent processes the configuration files located in the logging.d directory and will generate a run-time Fluent Bit configuration file that contains the appropriate [INPUT], [FILTER] and [OUTPUT] sections. Optionally, it will also declare an @INCLUDE in case you provided an external Fluent Bit configuration file via the fluentbit option. The runtime file does not define a [SERVICE] section, leaving all default Fluent Bit configuration values. You can still override Fluent Bit's default settings by defining your own [SERVICE] section in your external Fluent Bit configuration file and include it via the fluentbit option. Parameters: config_file: path to an existing Fluent Bit configuration file. Note that any overlapping source results in duplicate messages in New Relic Logs. parsers_file: path to an existing Fluent Bit parsers file. The following parser names are reserved: rfc3164, rfc3164-local and rfc5424. Sample configuration file Here is an example of a logging.d/ configuration file in YAML format. For more configuration examples, see the infrastructure agent repository. logging.d/sample.yaml # Remember to only use spaces for indentation logs: # Example of 'file' source - name: file-with-attributes file: /var/log/test.log # Path to a single file or pattern attributes: # You can use custom attributes to enrich your data logtype: nginx team: The A Team pattern: Error # Regular expression to filter log entries # Example of 'systemd' source (Linux only) - name: systemd-example systemd: cupsd # Examples of 'syslog' source, one per protocol # TCP network socket - name: syslog-tcp-test syslog: uri: tcp://0.0.0.0:5140 # Use the tcp://LISTEN_ADDRESS:PORT format parser: rfc5424 # Default syslog parser is rfc3164 # UDP network socket - name: syslog-udp-test syslog: uri: udp://0.0.0.0:6140 # Use the udp://LISTEN_ADDRESS:PORT format max_line_kb: 35 # Paths for Unix sockets are defined by combining protocol and path: # unix_udp:// + /path/socket - for example, unix_udp:///tmp/socket # Unix TCP domain socket - name: syslog-unix-tcp-test syslog: uri: unix_tcp:///var/unix-tcp-socket-test unix_permissions: 0666 # Default is 0644. Change at your own risk # Unix UDP domain socket - name: syslog-unix-udp-test syslog: uri: unix_udp:///var/unix-udp-socket-test parser: rfc5424 # Examples of 'tcp' source for formats 'none' and 'json' - name: tcp-simple-test tcp: uri: tcp://0.0.0.0:1234 # Use the tcp://LISTEN_ADDRESS:PORT format format: none # Raw text - this is default for 'tcp' separator: \\t # String for separating raw text entries attributes: # You can add custom attributes to any source of logs tcpFormat: none someOtherAttribute: associatedValue max_line_kb: 32 - name: tcp-json-test tcp: uri: tcp://0.0.0.0:2345 # Use the tcp://LISTEN_ADDRESS:PORT format format: json attributes: tcpFormat: json yetAnotherAttribute: 12345 # Example of Fluent Bit configuration import - name: fluentbit-import fluentbit: config_file: /path/to/fluentbit.config parsers_file: /path/to/fluentbit/parsers.conf Copy View your log data If everything is configured correctly and your data is being collected, you should see logs data in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy Troubleshooting If you encounter problems with configuring your log forwarder, try these troubleshooting tips. No log data If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. No data appears when tailing a file The log forwarding feature requires the agent to have permission to read the data sources. When running the infrastructure agent in privileged or non-privileged modes, make sure that the log files you want to forward (and any intermediary directory in its path) are readable by the user running nri-agent. Example: Check file access under Linux Let's check whether the file /var/log/restrictedLogs/logFile.log can be monitored by the nri-agent user. In Linux, you can do a quick check with the namei command: sudo -u nri-agent namei -ml /var/log/restrictedLogs/logFile.log f: /var/log/restrictedLogs/logFile.log drwxr-xr-x root root / drwxr-xr-x root root var drwxrwxr-x root syslog log drwxr--r-- root root restrictedLogs logFile.log - No such file or directory Copy This command failed because the file is not visible to the nri-agent user. By inspecting the previous output, we can detect that the restrictedLogs directory is missing the execution flag for others. To fix this, execute: sudo chmod 755 /var/log/restrictedLogs Copy And then check for file access again: # sudo -u nri-agent namei -ml /var/log/restrictedLogs/logFile.log f: /var/log/restrictedLogs/logFile.log drwxr-xr-x root root / drwxr-xr-x root root var drwxrwxr-x root syslog log drwxr-xr-x root root restrictedLogs -rw-r----- vagrant vagrant logFile.log Copy The file is now visible to the nri-agent user. You must ensure that the file is also readable by the nri-agent user. To check this, use: # sudo -u nri-agent head /var/log/restrictedLogs/logFile.log head: cannot open '/var/log/restrictedLogs/logFile.log' for reading: Permission denied Copy In this example, the file is missing the read rights for the others group (users other than vagrant and the vagrant user group). You could fix this by granting read permissions to others, but the application could change these permissions upon restart. To avoid this, a better approach is to add the nri-agent user to the vagrant user group. No data appears when capturing via a Syslog socket The log forwarding feature requires that the agent has permission to read the data sources. When running the infrastructure agent in privileged or non-privileged modes: If you're using Unix domain socket files, make sure that the nri-agent user can access these files (please refer to the previous section) and that they have read and write permissions (666) so that other users than nri-agent can write to them. If you're using IP sockets, ensure that the port that you are using is not a system reserved one (like port 80, for example). If no data appears after you enable log management, follow standard log management troubleshooting procedures. No data appears using infrastructure agent proxy As explained in the infrastructure agent configuration guidelines, the proxy parameter must use either HTTP or HTTPS and be in the form https://user:password@hostname:port. The agent can parse the parameter without the HTTP or HTTPS, but the log forwarder cannot. You will see an error like the following in the agent verbose logs: [ERROR] building HTTP transport: parse \\\"hostname:port\\\": first path segment in URL cannot contain colon Copy To solve this problem, check your newrelic-infra.yml file, and ensure the proxy parameter adheres to this form. If you're using caBundleFile or caBundleDir in order to specify any certificate, we recommend to follow the below rules for each OS: Linux For HTTP proxies you don't need to setup any certificates. The plugin loads the system certificates and New Relic sends logs into the logging endpoint. However, you can specify the proxy self-signed certificate (PEM file) using either the caBundleFile or caBundleDir parameters. Windows For HTTP proxies you don't need to setup any certificates. The plugin loads the system certificates. For HTTPS, you can configure it in one of the following ways: Import the proxy certificate to the system pool (Recommended) Import the proxy self-signed certificate (PEM file) by using the MMC tool. Refer to this link, and in Step 2 ensure to import it in your Trusted Root Certification Authorities, instead of in the Intermediate Certification Authorities. Using the caBundleFile and caBundleDir parameters On Windows, we cannot load both the certificates from the system certificate pool and the ones specified with the caBundleFile caBundleDir parameters. So, if you are using caBundleFile or caBundleDir, ensure that the following certificates are placed in the same PEM file (when using caBundleFile) or in the same directory (when using caBundleDir): The Proxy certificate (because it's an HTTPS proxy). The Logging Endpoint certificate (eg. https://log-api.newrelic.com/log/v1). The Infrastructure Agent certificate (eg. https://infra-api.newrelic.com). You can check the certificates by running: # openssl s_client -connect log-api.newrelic.com:443 -servername log-api.newrelic.com Copy Sending the infrastructure agent's logs to New Relic You can configure the infrastructure agent to send its own logs to New Relic. This is useful for troubleshooting issues with log forwarding, the agent, or when contacting support. To forward the infrastructure agent logs to New Relic: Edit your newrelic-infra.yml file. Enable agent logging in troubleshooting mode by adding verbose: 3. On Windows and systems that don't use systemd or where journald is inaccessible, verbose:3 causes the agent to write the logs on the disk. Revert to verbose:0 to prevent this. (Recommended): Enable agent logging in JSON format to log_format: json. Restart the agent to load the new settings. This configuration sets up the agent in troubleshooting mode, but the log forwarder (based on Fluent Bit) will continue in a non-verbose mode. Sometimes you can have issues with the log forwarder itself. For example, there may be problems accessing a specific channel when shipping Windows log events or when accessing a particular log file. In these situations, you can also enable the verbose mode for the log forwarder: Set verbose to a value other than 0. Add the configuration option: trace: [\"log.fw\"]. Caution Check whether you are using the [fluentbit] option. When setting verbose: 3 and trace: [\"log.fw\"], ensure that you don't define any [OUTPUT] section pointing to stdout in an external Fluent Bit configuration file, Fluent Bit does not start with the infra agent Important Fluent Bit's tail plugin does not support network drives. For Linux versions prior to 2016, you may need to update the OpenSSL library to 1.1.0 (or higher). To check if you have this problem: See if infra-agent has started Fluent Bit by running: ps -aux | grep td-agent-bit Copy If it isn't running go to /var/db/newrelic-infra/newrelic-integrations/logging and run: ./fluent-bit -i systemd -o stdout Copy If you get the following error, update OpenSSL to 1.1.0 or higher: error while loading shared libraries: libssl.so.1.1: cannot open shared object file: No such file or directory Copy Runtime error on Windows One of the following error messages may appear when enabling log forwarding on Windows: The code execution cannot proceed because VCRUNTIME140.dll was not found. Copy OR error=\"exit status 3221225781\" process=log-forwarder Copy This is caused by a missing DLL. To solve the issue, install the Microsoft Visual C++ Redistributable as applicable: x64 x86 Errors when tailing a large amount of log files (Linux) It's common to face either one of the following error messages when attempting to tail a large amount of files: Too many open files The user limit on the total number of inotify watches was reached or the kernel failed to allocate a needed resource The operating system defines a maximum amount of allocatable file descriptors (typically 1024 by default), and a maximum amount of allocatable inotify watches (typically 8192 by default). Any process attempting to go above these limits will fail, returning one of the errors above. The underlying technology we use to forward logs, Fluent Bit, opens one file descriptor and sets an inotify watch for each file you configure to be forwarded. Moreover, at the time of writing this section, Fluent Bit uses an extra set of 32 file descriptors for its normal operation, with another extra file descriptor when it shuts down. Therefore, to capture a large amount of files you need to ensure that both the file descriptor and inotify watch limits are slightly greater than the amount of log files you wish to tail. The following instructions summarize how to increase these limits if you want to tail 10,000 log files. Also, it assumes the infrastructure agent is installed in root running mode, and therefore must be run using the root user. Check which is the current hard limit for the amount of file descriptors per process. Typically, this limit should be quite high and should not need to be modified. ulimit -Hn Copy Add the following line to /etc/security/limits.conf. We specified a limit of 10100 here instead of just 10000 to allow Fluent Bit to allocate the extra file descriptors it may need to work. root soft nofile 10100 # replace root by nri-agent for non-root (privileged and unprivileged) installations Copy Add the following line to /etc/pam.d/common-session so that the previous limit is applied upon restart: session required pam_limits.so Copy Add the following line to /etc/sysctl.conf to increase the amount of allowed inotify watchers per user. We specified a limit of 18192 here instead of just 10000 so that the root user will still have 8192 available inotify watches (the default value). fs.inotify.max_user_watches=18192 Copy Restart your system. Ensure that the new limits have been enforced by running: ulimit -Sn # Should return 10100 cat /proc/sys/fs/inotify/max_user_watches # Should return 18192 Copy Learn more on how to increase open file limits, or on how to increase the inotify watches. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards. Uninstall log forwarding To uninstall log forwarding capabilities, go to your logging.d directory, and remove files with the .yml extension that were originally added during the configuration process. Linux: /etc/newrelic-infra/logging.d/ Windows: C:\\Program Files\\New Relic\\newrelic-infra\\logging.d\\",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 353.44617,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward your <em>logs</em> using the <em>infrastructure</em> agent",
        "sections": "Sending the <em>infrastructure</em> agent&#x27;s <em>logs</em> to <em>New</em> <em>Relic</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " <em>management</em> capabilities to collect, process, explore, query, and alert on your <em>log</em> data. Basic process To forward your <em>logs</em> through our infrastructure <em>monitoring</em> agent: If you haven&#x27;t already, create a <em>New</em> <em>Relic</em> account. It&#x27;s free, forever. Verify the system requirements needed for configuring <em>logs</em>"
      },
      "id": "603e9df164441f6b6f4e8843"
    },
    {
      "sections": [
        "Forward your logs to New Relic",
        "How it works",
        "Get started with log forwarders",
        "Log forwarding options",
        "TCP endpoint",
        "Log API",
        "What's next?"
      ],
      "title": "Forward your logs to New Relic",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "b6e203ddc367d5a2b5e002916e49d34f4ba17a87",
      "image": "https://docs.newrelic.com/static/c3d5443b84a1e2b26a4767ce35fa58f3/e5166/new-relic-logs-in-context-diagram.jpg",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/enable-log-management-new-relic/",
      "published_at": "2022-01-08T02:40:08Z",
      "updated_at": "2022-01-04T07:02:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our log management capabilities help you to collect, process, explore, query, and alert on your log data. To get your logs into New Relic, you can: Use your existing log forwarding solution to collect your logs and extend the metadata that is forwarded to New Relic. Use our infrastructure agent as a lightweight data collector, without having to use additional software. Use our Log API to forward your logs via HTTP. Use syslog protocols to forward your logs via a TCP endpoint. How it works The following diagram shows the lifecycle of a log message for an app, from enrichment with APM agent metadata (contextual logging), to formatting and forwarding the log data to New Relic. This diagram illustrates the flow of log messages through New Relic. Standard log formatters transform log events into meaningful output (such as text files) that can be used by downstream people and processes. The NewRelicFormatter transforms log events into the JSON format expected by New Relic. These files contain log information and extended metadata. When you configure your log forwarder (our infrastructure monitoring agent, Fluentd, Logstash, etc.), you can also extend and enrich your log data. By configuring logs in context, the log enricher links the formatted log data with additional transaction information from your application or host. Now your log files are enriched with enhanced metadata and contextual logging data. Your log forwarder sends the files to our logging endpoint for processing. From there you can use our log management capabilities to view, query, set up alerts, and more in New Relic. Here is an example of logs for your host's UI. You can see logs in context of events for the selected time period, and drill down into detailed data for any of the highlighted attributes. To take advantage of even more capabilities, click Query logs from here to go directly to the Logs UI. Here is an example of a host's logs in context related to an event. Get started with log forwarders To forward your logs to New Relic with enriched metadata: If you don't have one already, create a New Relic account. It's free, forever. Have your New Relic account's license key. Install a compatible log forwarder. Ensure that outbound connectivity on TCP port 443 is allowed to the CIDR range that matches your region. Use the DNS name log-api.newrelic.com or log-api.eu.newrelic.com for configuration. For apps monitored by a New Relic APM agent, configure logs in context. Log forwarding options Use any of these solutions to forward your logs to New Relic. Recommended: Infrastructure monitoring agent Amazon: AWS CloudWatch plugin AWS FireLens plugin AWS Kinesis Firehose AWS Lambda for sending logs from S3 Microsoft: Azure ARM template Other log forwarding plugins: Fluent Bit plugin Fluentd plugin Google Cloud Platform Pub/Sub Heroku log streaming Kubernetes plugin Logstash plugin Vector plugin TCP endpoint In some situations you may not have log forwarders; for example, with CDNs, hardware devices, or managed services. You can use syslog protocols such as rsyslog and syslog-ng, and forward your logs to New Relic via a TCP endpoint. Log API If you prefer to connect to New Relic without installing a plugin, we offer an HTTP input integration. This option sends your monitored log data directly to New Relic via the Log API. What's next? After you enable your log forwarder, make the most of your data in New Relic with our log management capabilities: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data if you are forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards. For example, to query and manage your data partition rules, see our NerdGraph tutorial.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 314.5207,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward your <em>logs</em> to <em>New</em> <em>Relic</em>",
        "sections": "Forward your <em>logs</em> to <em>New</em> <em>Relic</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " directly to <em>New</em> <em>Relic</em> via the <em>Log</em> API. What&#x27;s next? After you <em>enable</em> your <em>log</em> forwarder, make the most of your data in <em>New</em> <em>Relic</em> with our <em>log</em> <em>management</em> capabilities: Explore the logging data across your platform with our <em>Logs</em> UI. See your <em>logs</em> in context of your app&#x27;s performance in the APM UI"
      },
      "id": "61571e0e28ccbcbc52f21431"
    }
  ],
  "/docs/logs/forward-logs/aws-lambda-sending-logs-s3": [
    {
      "sections": [
        "Fluent Bit plugin for log forwarding",
        "Basic process",
        "Install the Fluent Bit plugin",
        "Tip",
        "Configure the Fluent Bit plugin",
        "Important",
        "Test the Fluent Bit plugin",
        "Associate logs with entities",
        "Optional: Configure plugin attributes",
        "View log data",
        "What's next?"
      ],
      "title": "Fluent Bit plugin for log forwarding",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "dd686f074bb80908ae9de19e543a3bae7d87a466",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/fluent-bit-plugin-log-forwarding/",
      "published_at": "2022-01-08T02:38:07Z",
      "updated_at": "2022-01-08T02:38:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your log data in New Relic. Forwarding your Fluent Bit logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Basic process We have published a container with the plugin installed. It serves as a base image to be used by our Kubernetes integration. We recommend you use this base image and layer your own custom configuration files. To forward your logs from Fluent Bit to New Relic: Make sure you have: A New Relic license key Fluent Bit 1.0 or higher (recommended), although v0.12 or higher is supported Fluent Bit Windows install directions can be found here Fluent Bit Linux install directions can be found here Install the Fluent Bit plugin. Configure the Fluent Bit plugin. Test the Fluent Bit plugin. Generate some traffic and wait a few minutes, then check your account for data. Install the Fluent Bit plugin To install the Fluent Bit plugin: Navigate to New Relic's Fluent Bit plugin repository on GitHub. From the repository page, clone or download the repository. Run the following command to build your plugin: cd newrelic-fluent-bit-output && make all Copy Store out_newrelic.so or out_newrelic_winXX.dll at a location that can be accessed by the fluent-bit daemon. Tip If you'd rather not compile the plugin yourself, you can download pre-compiled versions from our GitHub repository's releases page. Configure the Fluent Bit plugin Fluent Bit needs to know the location of the New Relic plugin and the New Relic license key to output data to New Relic. To configure your Fluent Bit plugin: Important Pay attention to white space when editing your config files. Be sure to use four spaces to indent and one space between keys and values. Locate or create a plugins.conf file in your plugins directory. In the plugins.conf file, add a reference to out_newrelic.so, adjacent to your fluent-bit.conf file: [PLUGINS] Path /PATH/TO/newrelic-fluent-bit-output/out_newrelic.so Copy In the fluent-bit.conf file, add the following line under the service block: [SERVICE] # This is the main configuration block for fluent bit. # Ensure the follow line exists somewhere in the SERVICE block Plugins_File plugins.conf Copy At the bottom of the fluent-bit.conf file, add the following to set up the input and output filters. Replace the placeholder text with your New Relic license key: [INPUT] Name tail Path /PATH/TO/YOUR/LOG/FILE [OUTPUT] Name newrelic Match * licenseKey YOUR_LICENSE_KEY # Optional maxBufferSize 256000 maxRecords 1024 Copy Restart your Fluent Bit instance with the following command: fluent-bit -c /PATH/TO/fluent-bit.conf Copy Test the Fluent Bit plugin To test if your Fluent Bit plugin is receiving input from a log file: Run the following command to append a test log message to your log file: echo \"test message\" >> /PATH/TO/YOUR/LOG/FILE Copy Search the New Relic Logs UI for test message. Associate logs with entities To associate a log line with an entity, such as an infrastructure host, add a FILTER block: [FILTER] Name modify Match * # Or specify a match Add entity.guids <Your Entity GUID ID> # Optional Add hostname <Your hostname> # Optional Copy For more options, see the Fluent Bit modify filter documentation and our documentation to forward your logs using the infrastructure agent. Optional: Configure plugin attributes Once you have installed and configured the Fluent Bit plugin, you can use the following attributes to configure how the plugin sends data to New Relic: Key Description licenseKey The New Relic license key. Use either licenseKey (recommended) or apiKey, not both. Default: none maxBufferSize The maximum size the payloads sent, in bytes. Default: 256000 maxRecords The maximum number of records to send at a time. Default: 1024 apiKey Deprecated. Takes a New Relic Insights insert key, but using the licenseKey attribute is preferred. Use either licenseKey or apiKey, not both. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 374.04892,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Fluent Bit plugin for <em>log</em> forwarding",
        "sections": "Associate <em>logs</em> with entities",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "If your <em>log</em> data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your <em>log</em> data in <em>New</em> <em>Relic</em>. Forwarding your Fluent Bit <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert"
      },
      "id": "603ea7ec28ccbce542eba791"
    },
    {
      "sections": [
        "Forward your logs using the infrastructure agent",
        "Basic process",
        "System requirements",
        "Important",
        "Install the infrastructure agent",
        "Configure the infrastructure agent",
        "Log forwarding parameters",
        "Name (required)",
        "Log source (required)",
        "file",
        "systemd",
        "syslog",
        "tcp",
        "winlog",
        "Optional configuration",
        "attributes",
        "attributes automatically inserted by the infrastructure agent",
        "pattern",
        "max_line_kb",
        "fluentbit",
        "Sample configuration file",
        "logging.d/sample.yaml",
        "View your log data",
        "Troubleshooting",
        "No log data",
        "No data appears when tailing a file",
        "No data appears when capturing via a Syslog socket",
        "No data appears using infrastructure agent proxy",
        "Sending the infrastructure agent's logs to New Relic",
        "Caution",
        "Fluent Bit does not start with the infra agent",
        "Runtime error on Windows",
        "Errors when tailing a large amount of log files (Linux)",
        "What's next?",
        "Uninstall log forwarding"
      ],
      "title": "Forward your logs using the infrastructure agent",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "a1deac410f0eedfb819348524a85a73bbb9d9daf",
      "image": "https://docs.newrelic.com/static/5d4352531871b652878b3cb0768e3f23/c1b63/demotron-host-logs-123021.png",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/forward-your-logs-using-infrastructure-agent/",
      "published_at": "2022-01-09T01:46:14Z",
      "updated_at": "2022-01-07T01:41:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can forward your logs to New Relic using our infrastructure monitoring agent. This makes all of your logging data available in one location and provides deeper visibility into both your application and your platform performance data. Forwarding your logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Basic process To forward your logs through our infrastructure monitoring agent: If you haven't already, create a New Relic account. It's free, forever. Verify the system requirements needed for configuring logs. Ensure you have installed the infrastructure agent, version 1.11.4 or higher. Create a logging.yml configuration file in the infrastructure agent's logging.d directory. Configure your log sources and other parameters. Generate some traffic and wait a few minutes, then check your account for data. Explore your log data in the Logs UI and benefit from the log attributes automatically inserted by the infrastructure agent. Here is an example of logs for your host's UI. You can see logs in context of events for the selected time period, and drill down into detailed data for any of the highlighted attributes. To take advantage of even more capabilities, click Query logs from here to go directly to the Logs UI. Here is an example of a host's logs in context related to an event. System requirements To use the log forwarder of the infrastructure agent, make sure you meet the following requirements: Infrastructure agent version 1.11.4 or higher OpenSSL library 1.1.0 or higher is required by Infrastructure agent starting from 1.16.4. Built-in support for ARM64 architecture on Linux systems (in example, AWS Graviton architecture) added in Infrastructure agent 1.26.0. Important The log forwarding feature is not supported with the Docker container for infrastructure monitoring agents. The log forwarding feature is compatible with the following operating systems: Operating system Supported version Amazon Linux Amazon Linux 2 CentOS Version 7 or higher Debian Version 9 (\"Stretch\") or higher Exception: Version 11 is not supported. Red Hat Enterprise Linux (RHEL) Version 7 or higher SUSE Linux Enterprise Server (SLES) Version 12 Ubuntu Versions 16.04.x, 18.04.x and 20.04.x (LTS versions) Windows Windows Server 2012, 2016, 2019, and 2022, and their service packs. Windows 10 Install the infrastructure agent Starting with version 1.11.4, the infrastructure agent can forward logs to New Relic. To install and run the agent, use a package manager (Linux) or the MSI installer (Windows). Important The log forwarding feature is not included when the infrastructure agent is implemented using Linux tarball or Windows ZIP installations. To use the following links, make sure you are logged to your New Relic account. Amazon Linux CentOS Debian RHEL SLES Ubuntu Windows If you don't have a New Relic account yet, or if you prefer to follow the procedure manually, see our tutorial to install the package manager. Configure the infrastructure agent Configuration files describe which log sources are forwarded. Our infrastructure agent uses .yml files to configure logging. You can add as many config files as you want. To add a new configuration file for the log forwarding feature: Navigate to the log forwarder configuration folder: Linux: /etc/newrelic-infra/logging.d/ Windows: C:\\Program Files\\New Relic\\newrelic-infra\\logging.d\\ Create a logging.yml configuration file, and add the parameters you need. The logging.d directory has various .yml.example files you can use as a reference or starting point. The agent automatically processes new configuration files without having to restart the infrastructure monitoring service. The only exception to this is when configuring a custom Fluent Bit configuration. Log forwarding parameters The infrastructure log forwarding .yml config supports the following parameters: Name (required) To start, define a name of the log or logs you want to forward to New Relic. Log source (required) What you use for the log source will depend on where you want to forward your logs from. Available options include: file Path to the log file or files. The agent tracks changes on the log files in a way similar to tail -f shell. Example: logs: - name: example-log file: /var/log/example.log # Path to a single log file - name: example-log-two file: /var/log/example-two.log # Path to another single log file Copy The file parameter can point to a specific log file or multiple files by using wildcards applied to names and extensions; for example, /logs/*.log. You can use wildcards in place of directories in a file path, which can be used to tail files located in different directories. Example: logs: - name: docker-logs file: /var/lib/docker/containers/*/*.log # Path to multiple folders and files Copy Important Use of wildcards may significantly increase the number of file descriptors and inotify watches the Fluent Bit process keeps open, which can interfere with log collection if the host's file descriptor limit is reached. Tailing a large number of files may require you to increase the maximum number of file descriptors and inotify watchers allowed by the operating system. Please refer to Errors when tailing a large amount of log files for more details on how to increase them. systemd Use the systemd parameter to forward log messages that are collected by the journald daemon in Linux environments. This input type requires the agent to run in root mode. Example: logs: - name: systemd-example systemd: cupsd Copy syslog Syslog data source. Parameters: uri: Syslog socket. Format varies depending on the protocol: TCP/UDP network sockets: [tcp/udp]://LISTEN_ADDRESS:PORT Unix domain sockets: unix_[tcp/udp]:// + /socket/path parser: Syslog parser. Default is rfc3164. Use rfc5424 if your messages include fractional seconds. Note: rfc3164 currently does not work on SuSE. unix_permissions: default is 0644 for domain sockets; this limits entries to processes running as root. You can use 0666 to listen for non-root processes, at your own risk. When running the agent in privileged mode, ports and sockets must be available or owned by nri-agent, with 0666 file permissions, so that other processes can write logs to the sockets. logs: # TCP network socket - name: syslog-tcp-test syslog: uri: tcp://0.0.0.0:5140 # Use the tcp://LISTEN_ADDRESS:PORT format parser: rfc5424 # Default syslog parser is rfc3164 # UDP network socket - name: syslog-udp-test syslog: uri: udp://0.0.0.0:6140 # Use the udp://LISTEN_ADDRESS:PORT format max_line_kb: 35 # Unix TCP domain socket - name: syslog-unix-tcp-test syslog: uri: unix_tcp:///var/unix-tcp-socket-test unix_permissions: 0666 # Default is 0644. Change at your own risk # Unix UDP domain socket - name: syslog-unix-udp-test syslog: uri: unix_udp:///var/unix-udp-socket-test parser: rfc5424 Copy tcp Logs retrieved over TCP connections. Parameters: uri: TCP/IP socket to listen for incoming data. The URI format is tcp://LISTEN_ADDRESS:PORT format: format of the data. It can be json or none. separator: If format: none is used, you can define a separator string for splitting records (default: \\n). logs: - name: tcp-simple-test tcp: uri: tcp://0.0.0.0:1234 # Use the tcp://LISTEN_ADDRESS:PORT format format: none # Raw text - this is default for 'tcp' separator: \\t # String for separating raw text entries max_line_kb: 32 - name: tcp-json-test tcp: uri: tcp://0.0.0.0:2345 # Use the tcp://LISTEN_ADDRESS:PORT format format: json Copy winlog Collect events from Windows log channels. Parameters: channel: name of the channel logs will be collected from. collect-eventids: a list of Windows Event IDs to be collected and forwarded to New Relic. Event ID ranges are supported. exclude-eventids: a list of Windows Event IDs to be excluded from collection. Event ID ranges are supported. All events are collected from the specified channel by default. Configure the collect-eventids and exclude-eventids sections to avoid sending unwanted logs to your New Relic account. Add event IDs or ranges to collect-eventids or exclude-eventids to forward or drop specific events. exclude-eventids takes precedence over collect-eventids if the same event ID is present in both sections. Example: logs: # Winlog log ingestion with eventId filters. - name: windows-security winlog: channel: Security collect-eventids: - 4624 - 4265 - 4700-4800 exclude-eventids: - 4735 # entries for the application, system, powershell, and SCOM channels - name: windows-application winlog: channel: Application - name: windows-system winlog: channel: System - name: windows-pshell winlog: channel: Windows Powershell - name: scom winlog: channel: Operations Manager # Entry for Windows Defender Logs - name: windows-defender winlog: channel: Microsoft-Windows-Windows Defender/Operational # Entry for Windows Clustering Logs - name: windows-clustering winlog: channel: Microsoft-Windows-FailoverClustering/Operational # Entry for IIS logs with logtype attribute for automatic parsing - name: iis-log file: C:\\inetpub\\logs\\LogFiles\\w3svc.log attributes: logtype: iis_w3c Copy Optional configuration The following configuration parameters are not required but are still recommended. attributes List of custom attributes specified as key-value pairs that can be used to send additional data with the logs which you can then query. The attributes configuration parameter can be used with any log source. One common use of the attributes configuration parameter is to specify the logtype attribute. This attribute allows leveraging one of the built-in parsing rules supported by New Relic Logs. Example: logs: - name: example-file-attributes file: /var/log/example.log attributes: logtype: nginx region: example-us-02 team: A-team - name: example-tcp-attributes tcp: uri: tcp://0.0.0.0:2345 format: json attributes: logtype: nginx region: example-us-02 team: B-team Copy attributes automatically inserted by the infrastructure agent The infrastructure agent automatically inserts log attributes for your convenience. Some of them are inserted for any log record, while others depend on the configuration parameters you used while setting up the log forwarder. Attribute name Description entity.guids Always inserted. The infrastructure agent inserts the Entity GUID assigned by New Relic to identify the host where it's running. It is available in the entity.guids field. Note: If the captured logs belong to an application instrumented using APM, the entity.guids field contains both the entity GUID of infrastructure, as well as the GUID of APM, separated by a pipe ( | ) delimiter. fb.input Always inserted. The underlying Fluent Bit input plugin type used to capture the logs. Currently its values are tail, systemd, winlog, syslog, and tcp. filePath Inserted when sing the file input type. Absolute file path of the file being monitored. hostname Always inserted. The hostname of the machine/VM/container executing the infrastructure agent. plugin.type Always inserted. Indicates the utility used to capture the logs. In this case, it is the infrastructure agent itself, so this attribute always has the value nri-agent. pattern Regular expression for filtering records. Only supported for the tail, systemd, syslog, and tcp (only with format none) sources. This field works in a way similar to grep -E in Unix systems. For example, for a given file being captured, you can filter for records containing either WARN or ERROR using: - name: only-records-with-warn-and-error file: /var/log/logFile.log pattern: WARN|ERROR Copy No filtering is applied by default. max_line_kb Maximum size of log entries/lines in KB. If log entries exceed the limit, they are skipped. Default is 128. fluentbit External Fluent Bit configuration and parser files. If defined, they are merged with the existing configuration and parser files generated by the infrastructure agent. The infrastructure agent processes the configuration files located in the logging.d directory and will generate a run-time Fluent Bit configuration file that contains the appropriate [INPUT], [FILTER] and [OUTPUT] sections. Optionally, it will also declare an @INCLUDE in case you provided an external Fluent Bit configuration file via the fluentbit option. The runtime file does not define a [SERVICE] section, leaving all default Fluent Bit configuration values. You can still override Fluent Bit's default settings by defining your own [SERVICE] section in your external Fluent Bit configuration file and include it via the fluentbit option. Parameters: config_file: path to an existing Fluent Bit configuration file. Note that any overlapping source results in duplicate messages in New Relic Logs. parsers_file: path to an existing Fluent Bit parsers file. The following parser names are reserved: rfc3164, rfc3164-local and rfc5424. Sample configuration file Here is an example of a logging.d/ configuration file in YAML format. For more configuration examples, see the infrastructure agent repository. logging.d/sample.yaml # Remember to only use spaces for indentation logs: # Example of 'file' source - name: file-with-attributes file: /var/log/test.log # Path to a single file or pattern attributes: # You can use custom attributes to enrich your data logtype: nginx team: The A Team pattern: Error # Regular expression to filter log entries # Example of 'systemd' source (Linux only) - name: systemd-example systemd: cupsd # Examples of 'syslog' source, one per protocol # TCP network socket - name: syslog-tcp-test syslog: uri: tcp://0.0.0.0:5140 # Use the tcp://LISTEN_ADDRESS:PORT format parser: rfc5424 # Default syslog parser is rfc3164 # UDP network socket - name: syslog-udp-test syslog: uri: udp://0.0.0.0:6140 # Use the udp://LISTEN_ADDRESS:PORT format max_line_kb: 35 # Paths for Unix sockets are defined by combining protocol and path: # unix_udp:// + /path/socket - for example, unix_udp:///tmp/socket # Unix TCP domain socket - name: syslog-unix-tcp-test syslog: uri: unix_tcp:///var/unix-tcp-socket-test unix_permissions: 0666 # Default is 0644. Change at your own risk # Unix UDP domain socket - name: syslog-unix-udp-test syslog: uri: unix_udp:///var/unix-udp-socket-test parser: rfc5424 # Examples of 'tcp' source for formats 'none' and 'json' - name: tcp-simple-test tcp: uri: tcp://0.0.0.0:1234 # Use the tcp://LISTEN_ADDRESS:PORT format format: none # Raw text - this is default for 'tcp' separator: \\t # String for separating raw text entries attributes: # You can add custom attributes to any source of logs tcpFormat: none someOtherAttribute: associatedValue max_line_kb: 32 - name: tcp-json-test tcp: uri: tcp://0.0.0.0:2345 # Use the tcp://LISTEN_ADDRESS:PORT format format: json attributes: tcpFormat: json yetAnotherAttribute: 12345 # Example of Fluent Bit configuration import - name: fluentbit-import fluentbit: config_file: /path/to/fluentbit.config parsers_file: /path/to/fluentbit/parsers.conf Copy View your log data If everything is configured correctly and your data is being collected, you should see logs data in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy Troubleshooting If you encounter problems with configuring your log forwarder, try these troubleshooting tips. No log data If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. No data appears when tailing a file The log forwarding feature requires the agent to have permission to read the data sources. When running the infrastructure agent in privileged or non-privileged modes, make sure that the log files you want to forward (and any intermediary directory in its path) are readable by the user running nri-agent. Example: Check file access under Linux Let's check whether the file /var/log/restrictedLogs/logFile.log can be monitored by the nri-agent user. In Linux, you can do a quick check with the namei command: sudo -u nri-agent namei -ml /var/log/restrictedLogs/logFile.log f: /var/log/restrictedLogs/logFile.log drwxr-xr-x root root / drwxr-xr-x root root var drwxrwxr-x root syslog log drwxr--r-- root root restrictedLogs logFile.log - No such file or directory Copy This command failed because the file is not visible to the nri-agent user. By inspecting the previous output, we can detect that the restrictedLogs directory is missing the execution flag for others. To fix this, execute: sudo chmod 755 /var/log/restrictedLogs Copy And then check for file access again: # sudo -u nri-agent namei -ml /var/log/restrictedLogs/logFile.log f: /var/log/restrictedLogs/logFile.log drwxr-xr-x root root / drwxr-xr-x root root var drwxrwxr-x root syslog log drwxr-xr-x root root restrictedLogs -rw-r----- vagrant vagrant logFile.log Copy The file is now visible to the nri-agent user. You must ensure that the file is also readable by the nri-agent user. To check this, use: # sudo -u nri-agent head /var/log/restrictedLogs/logFile.log head: cannot open '/var/log/restrictedLogs/logFile.log' for reading: Permission denied Copy In this example, the file is missing the read rights for the others group (users other than vagrant and the vagrant user group). You could fix this by granting read permissions to others, but the application could change these permissions upon restart. To avoid this, a better approach is to add the nri-agent user to the vagrant user group. No data appears when capturing via a Syslog socket The log forwarding feature requires that the agent has permission to read the data sources. When running the infrastructure agent in privileged or non-privileged modes: If you're using Unix domain socket files, make sure that the nri-agent user can access these files (please refer to the previous section) and that they have read and write permissions (666) so that other users than nri-agent can write to them. If you're using IP sockets, ensure that the port that you are using is not a system reserved one (like port 80, for example). If no data appears after you enable log management, follow standard log management troubleshooting procedures. No data appears using infrastructure agent proxy As explained in the infrastructure agent configuration guidelines, the proxy parameter must use either HTTP or HTTPS and be in the form https://user:password@hostname:port. The agent can parse the parameter without the HTTP or HTTPS, but the log forwarder cannot. You will see an error like the following in the agent verbose logs: [ERROR] building HTTP transport: parse \\\"hostname:port\\\": first path segment in URL cannot contain colon Copy To solve this problem, check your newrelic-infra.yml file, and ensure the proxy parameter adheres to this form. If you're using caBundleFile or caBundleDir in order to specify any certificate, we recommend to follow the below rules for each OS: Linux For HTTP proxies you don't need to setup any certificates. The plugin loads the system certificates and New Relic sends logs into the logging endpoint. However, you can specify the proxy self-signed certificate (PEM file) using either the caBundleFile or caBundleDir parameters. Windows For HTTP proxies you don't need to setup any certificates. The plugin loads the system certificates. For HTTPS, you can configure it in one of the following ways: Import the proxy certificate to the system pool (Recommended) Import the proxy self-signed certificate (PEM file) by using the MMC tool. Refer to this link, and in Step 2 ensure to import it in your Trusted Root Certification Authorities, instead of in the Intermediate Certification Authorities. Using the caBundleFile and caBundleDir parameters On Windows, we cannot load both the certificates from the system certificate pool and the ones specified with the caBundleFile caBundleDir parameters. So, if you are using caBundleFile or caBundleDir, ensure that the following certificates are placed in the same PEM file (when using caBundleFile) or in the same directory (when using caBundleDir): The Proxy certificate (because it's an HTTPS proxy). The Logging Endpoint certificate (eg. https://log-api.newrelic.com/log/v1). The Infrastructure Agent certificate (eg. https://infra-api.newrelic.com). You can check the certificates by running: # openssl s_client -connect log-api.newrelic.com:443 -servername log-api.newrelic.com Copy Sending the infrastructure agent's logs to New Relic You can configure the infrastructure agent to send its own logs to New Relic. This is useful for troubleshooting issues with log forwarding, the agent, or when contacting support. To forward the infrastructure agent logs to New Relic: Edit your newrelic-infra.yml file. Enable agent logging in troubleshooting mode by adding verbose: 3. On Windows and systems that don't use systemd or where journald is inaccessible, verbose:3 causes the agent to write the logs on the disk. Revert to verbose:0 to prevent this. (Recommended): Enable agent logging in JSON format to log_format: json. Restart the agent to load the new settings. This configuration sets up the agent in troubleshooting mode, but the log forwarder (based on Fluent Bit) will continue in a non-verbose mode. Sometimes you can have issues with the log forwarder itself. For example, there may be problems accessing a specific channel when shipping Windows log events or when accessing a particular log file. In these situations, you can also enable the verbose mode for the log forwarder: Set verbose to a value other than 0. Add the configuration option: trace: [\"log.fw\"]. Caution Check whether you are using the [fluentbit] option. When setting verbose: 3 and trace: [\"log.fw\"], ensure that you don't define any [OUTPUT] section pointing to stdout in an external Fluent Bit configuration file, Fluent Bit does not start with the infra agent Important Fluent Bit's tail plugin does not support network drives. For Linux versions prior to 2016, you may need to update the OpenSSL library to 1.1.0 (or higher). To check if you have this problem: See if infra-agent has started Fluent Bit by running: ps -aux | grep td-agent-bit Copy If it isn't running go to /var/db/newrelic-infra/newrelic-integrations/logging and run: ./fluent-bit -i systemd -o stdout Copy If you get the following error, update OpenSSL to 1.1.0 or higher: error while loading shared libraries: libssl.so.1.1: cannot open shared object file: No such file or directory Copy Runtime error on Windows One of the following error messages may appear when enabling log forwarding on Windows: The code execution cannot proceed because VCRUNTIME140.dll was not found. Copy OR error=\"exit status 3221225781\" process=log-forwarder Copy This is caused by a missing DLL. To solve the issue, install the Microsoft Visual C++ Redistributable as applicable: x64 x86 Errors when tailing a large amount of log files (Linux) It's common to face either one of the following error messages when attempting to tail a large amount of files: Too many open files The user limit on the total number of inotify watches was reached or the kernel failed to allocate a needed resource The operating system defines a maximum amount of allocatable file descriptors (typically 1024 by default), and a maximum amount of allocatable inotify watches (typically 8192 by default). Any process attempting to go above these limits will fail, returning one of the errors above. The underlying technology we use to forward logs, Fluent Bit, opens one file descriptor and sets an inotify watch for each file you configure to be forwarded. Moreover, at the time of writing this section, Fluent Bit uses an extra set of 32 file descriptors for its normal operation, with another extra file descriptor when it shuts down. Therefore, to capture a large amount of files you need to ensure that both the file descriptor and inotify watch limits are slightly greater than the amount of log files you wish to tail. The following instructions summarize how to increase these limits if you want to tail 10,000 log files. Also, it assumes the infrastructure agent is installed in root running mode, and therefore must be run using the root user. Check which is the current hard limit for the amount of file descriptors per process. Typically, this limit should be quite high and should not need to be modified. ulimit -Hn Copy Add the following line to /etc/security/limits.conf. We specified a limit of 10100 here instead of just 10000 to allow Fluent Bit to allocate the extra file descriptors it may need to work. root soft nofile 10100 # replace root by nri-agent for non-root (privileged and unprivileged) installations Copy Add the following line to /etc/pam.d/common-session so that the previous limit is applied upon restart: session required pam_limits.so Copy Add the following line to /etc/sysctl.conf to increase the amount of allowed inotify watchers per user. We specified a limit of 18192 here instead of just 10000 so that the root user will still have 8192 available inotify watches (the default value). fs.inotify.max_user_watches=18192 Copy Restart your system. Ensure that the new limits have been enforced by running: ulimit -Sn # Should return 10100 cat /proc/sys/fs/inotify/max_user_watches # Should return 18192 Copy Learn more on how to increase open file limits, or on how to increase the inotify watches. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards. Uninstall log forwarding To uninstall log forwarding capabilities, go to your logging.d directory, and remove files with the .yml extension that were originally added during the configuration process. Linux: /etc/newrelic-infra/logging.d/ Windows: C:\\Program Files\\New Relic\\newrelic-infra\\logging.d\\",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 353.44598,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward your <em>logs</em> using the <em>infrastructure</em> agent",
        "sections": "Sending the <em>infrastructure</em> agent&#x27;s <em>logs</em> to <em>New</em> <em>Relic</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " <em>management</em> capabilities to collect, process, explore, query, and alert on your <em>log</em> data. Basic process To forward your <em>logs</em> through our infrastructure <em>monitoring</em> agent: If you haven&#x27;t already, create a <em>New</em> <em>Relic</em> account. It&#x27;s free, forever. Verify the system requirements needed for configuring <em>logs</em>"
      },
      "id": "603e9df164441f6b6f4e8843"
    },
    {
      "sections": [
        "Forward your logs to New Relic",
        "How it works",
        "Get started with log forwarders",
        "Log forwarding options",
        "TCP endpoint",
        "Log API",
        "What's next?"
      ],
      "title": "Forward your logs to New Relic",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "b6e203ddc367d5a2b5e002916e49d34f4ba17a87",
      "image": "https://docs.newrelic.com/static/c3d5443b84a1e2b26a4767ce35fa58f3/e5166/new-relic-logs-in-context-diagram.jpg",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/enable-log-management-new-relic/",
      "published_at": "2022-01-08T02:40:08Z",
      "updated_at": "2022-01-04T07:02:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our log management capabilities help you to collect, process, explore, query, and alert on your log data. To get your logs into New Relic, you can: Use your existing log forwarding solution to collect your logs and extend the metadata that is forwarded to New Relic. Use our infrastructure agent as a lightweight data collector, without having to use additional software. Use our Log API to forward your logs via HTTP. Use syslog protocols to forward your logs via a TCP endpoint. How it works The following diagram shows the lifecycle of a log message for an app, from enrichment with APM agent metadata (contextual logging), to formatting and forwarding the log data to New Relic. This diagram illustrates the flow of log messages through New Relic. Standard log formatters transform log events into meaningful output (such as text files) that can be used by downstream people and processes. The NewRelicFormatter transforms log events into the JSON format expected by New Relic. These files contain log information and extended metadata. When you configure your log forwarder (our infrastructure monitoring agent, Fluentd, Logstash, etc.), you can also extend and enrich your log data. By configuring logs in context, the log enricher links the formatted log data with additional transaction information from your application or host. Now your log files are enriched with enhanced metadata and contextual logging data. Your log forwarder sends the files to our logging endpoint for processing. From there you can use our log management capabilities to view, query, set up alerts, and more in New Relic. Here is an example of logs for your host's UI. You can see logs in context of events for the selected time period, and drill down into detailed data for any of the highlighted attributes. To take advantage of even more capabilities, click Query logs from here to go directly to the Logs UI. Here is an example of a host's logs in context related to an event. Get started with log forwarders To forward your logs to New Relic with enriched metadata: If you don't have one already, create a New Relic account. It's free, forever. Have your New Relic account's license key. Install a compatible log forwarder. Ensure that outbound connectivity on TCP port 443 is allowed to the CIDR range that matches your region. Use the DNS name log-api.newrelic.com or log-api.eu.newrelic.com for configuration. For apps monitored by a New Relic APM agent, configure logs in context. Log forwarding options Use any of these solutions to forward your logs to New Relic. Recommended: Infrastructure monitoring agent Amazon: AWS CloudWatch plugin AWS FireLens plugin AWS Kinesis Firehose AWS Lambda for sending logs from S3 Microsoft: Azure ARM template Other log forwarding plugins: Fluent Bit plugin Fluentd plugin Google Cloud Platform Pub/Sub Heroku log streaming Kubernetes plugin Logstash plugin Vector plugin TCP endpoint In some situations you may not have log forwarders; for example, with CDNs, hardware devices, or managed services. You can use syslog protocols such as rsyslog and syslog-ng, and forward your logs to New Relic via a TCP endpoint. Log API If you prefer to connect to New Relic without installing a plugin, we offer an HTTP input integration. This option sends your monitored log data directly to New Relic via the Log API. What's next? After you enable your log forwarder, make the most of your data in New Relic with our log management capabilities: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data if you are forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards. For example, to query and manage your data partition rules, see our NerdGraph tutorial.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 314.52057,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward your <em>logs</em> to <em>New</em> <em>Relic</em>",
        "sections": "Forward your <em>logs</em> to <em>New</em> <em>Relic</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " directly to <em>New</em> <em>Relic</em> via the <em>Log</em> API. What&#x27;s next? After you <em>enable</em> your <em>log</em> forwarder, make the most of your data in <em>New</em> <em>Relic</em> with our <em>log</em> <em>management</em> capabilities: Explore the logging data across your platform with our <em>Logs</em> UI. See your <em>logs</em> in context of your app&#x27;s performance in the APM UI"
      },
      "id": "61571e0e28ccbcbc52f21431"
    }
  ],
  "/docs/logs/forward-logs/azure-log-forwarding": [
    {
      "sections": [
        "Fluent Bit plugin for log forwarding",
        "Basic process",
        "Install the Fluent Bit plugin",
        "Tip",
        "Configure the Fluent Bit plugin",
        "Important",
        "Test the Fluent Bit plugin",
        "Associate logs with entities",
        "Optional: Configure plugin attributes",
        "View log data",
        "What's next?"
      ],
      "title": "Fluent Bit plugin for log forwarding",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "dd686f074bb80908ae9de19e543a3bae7d87a466",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/fluent-bit-plugin-log-forwarding/",
      "published_at": "2022-01-08T02:38:07Z",
      "updated_at": "2022-01-08T02:38:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your log data in New Relic. Forwarding your Fluent Bit logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Basic process We have published a container with the plugin installed. It serves as a base image to be used by our Kubernetes integration. We recommend you use this base image and layer your own custom configuration files. To forward your logs from Fluent Bit to New Relic: Make sure you have: A New Relic license key Fluent Bit 1.0 or higher (recommended), although v0.12 or higher is supported Fluent Bit Windows install directions can be found here Fluent Bit Linux install directions can be found here Install the Fluent Bit plugin. Configure the Fluent Bit plugin. Test the Fluent Bit plugin. Generate some traffic and wait a few minutes, then check your account for data. Install the Fluent Bit plugin To install the Fluent Bit plugin: Navigate to New Relic's Fluent Bit plugin repository on GitHub. From the repository page, clone or download the repository. Run the following command to build your plugin: cd newrelic-fluent-bit-output && make all Copy Store out_newrelic.so or out_newrelic_winXX.dll at a location that can be accessed by the fluent-bit daemon. Tip If you'd rather not compile the plugin yourself, you can download pre-compiled versions from our GitHub repository's releases page. Configure the Fluent Bit plugin Fluent Bit needs to know the location of the New Relic plugin and the New Relic license key to output data to New Relic. To configure your Fluent Bit plugin: Important Pay attention to white space when editing your config files. Be sure to use four spaces to indent and one space between keys and values. Locate or create a plugins.conf file in your plugins directory. In the plugins.conf file, add a reference to out_newrelic.so, adjacent to your fluent-bit.conf file: [PLUGINS] Path /PATH/TO/newrelic-fluent-bit-output/out_newrelic.so Copy In the fluent-bit.conf file, add the following line under the service block: [SERVICE] # This is the main configuration block for fluent bit. # Ensure the follow line exists somewhere in the SERVICE block Plugins_File plugins.conf Copy At the bottom of the fluent-bit.conf file, add the following to set up the input and output filters. Replace the placeholder text with your New Relic license key: [INPUT] Name tail Path /PATH/TO/YOUR/LOG/FILE [OUTPUT] Name newrelic Match * licenseKey YOUR_LICENSE_KEY # Optional maxBufferSize 256000 maxRecords 1024 Copy Restart your Fluent Bit instance with the following command: fluent-bit -c /PATH/TO/fluent-bit.conf Copy Test the Fluent Bit plugin To test if your Fluent Bit plugin is receiving input from a log file: Run the following command to append a test log message to your log file: echo \"test message\" >> /PATH/TO/YOUR/LOG/FILE Copy Search the New Relic Logs UI for test message. Associate logs with entities To associate a log line with an entity, such as an infrastructure host, add a FILTER block: [FILTER] Name modify Match * # Or specify a match Add entity.guids <Your Entity GUID ID> # Optional Add hostname <Your hostname> # Optional Copy For more options, see the Fluent Bit modify filter documentation and our documentation to forward your logs using the infrastructure agent. Optional: Configure plugin attributes Once you have installed and configured the Fluent Bit plugin, you can use the following attributes to configure how the plugin sends data to New Relic: Key Description licenseKey The New Relic license key. Use either licenseKey (recommended) or apiKey, not both. Default: none maxBufferSize The maximum size the payloads sent, in bytes. Default: 256000 maxRecords The maximum number of records to send at a time. Default: 1024 apiKey Deprecated. Takes a New Relic Insights insert key, but using the licenseKey attribute is preferred. Use either licenseKey or apiKey, not both. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 374.09076,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Fluent Bit plugin for <em>log</em> forwarding",
        "sections": "Associate <em>logs</em> with entities",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "If your <em>log</em> data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your <em>log</em> data in <em>New</em> <em>Relic</em>. Forwarding your Fluent Bit <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert"
      },
      "id": "603ea7ec28ccbce542eba791"
    },
    {
      "sections": [
        "Forward your logs using the infrastructure agent",
        "Basic process",
        "System requirements",
        "Important",
        "Install the infrastructure agent",
        "Configure the infrastructure agent",
        "Log forwarding parameters",
        "Name (required)",
        "Log source (required)",
        "file",
        "systemd",
        "syslog",
        "tcp",
        "winlog",
        "Optional configuration",
        "attributes",
        "attributes automatically inserted by the infrastructure agent",
        "pattern",
        "max_line_kb",
        "fluentbit",
        "Sample configuration file",
        "logging.d/sample.yaml",
        "View your log data",
        "Troubleshooting",
        "No log data",
        "No data appears when tailing a file",
        "No data appears when capturing via a Syslog socket",
        "No data appears using infrastructure agent proxy",
        "Sending the infrastructure agent's logs to New Relic",
        "Caution",
        "Fluent Bit does not start with the infra agent",
        "Runtime error on Windows",
        "Errors when tailing a large amount of log files (Linux)",
        "What's next?",
        "Uninstall log forwarding"
      ],
      "title": "Forward your logs using the infrastructure agent",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "a1deac410f0eedfb819348524a85a73bbb9d9daf",
      "image": "https://docs.newrelic.com/static/5d4352531871b652878b3cb0768e3f23/c1b63/demotron-host-logs-123021.png",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/forward-your-logs-using-infrastructure-agent/",
      "published_at": "2022-01-09T01:46:14Z",
      "updated_at": "2022-01-07T01:41:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can forward your logs to New Relic using our infrastructure monitoring agent. This makes all of your logging data available in one location and provides deeper visibility into both your application and your platform performance data. Forwarding your logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Basic process To forward your logs through our infrastructure monitoring agent: If you haven't already, create a New Relic account. It's free, forever. Verify the system requirements needed for configuring logs. Ensure you have installed the infrastructure agent, version 1.11.4 or higher. Create a logging.yml configuration file in the infrastructure agent's logging.d directory. Configure your log sources and other parameters. Generate some traffic and wait a few minutes, then check your account for data. Explore your log data in the Logs UI and benefit from the log attributes automatically inserted by the infrastructure agent. Here is an example of logs for your host's UI. You can see logs in context of events for the selected time period, and drill down into detailed data for any of the highlighted attributes. To take advantage of even more capabilities, click Query logs from here to go directly to the Logs UI. Here is an example of a host's logs in context related to an event. System requirements To use the log forwarder of the infrastructure agent, make sure you meet the following requirements: Infrastructure agent version 1.11.4 or higher OpenSSL library 1.1.0 or higher is required by Infrastructure agent starting from 1.16.4. Built-in support for ARM64 architecture on Linux systems (in example, AWS Graviton architecture) added in Infrastructure agent 1.26.0. Important The log forwarding feature is not supported with the Docker container for infrastructure monitoring agents. The log forwarding feature is compatible with the following operating systems: Operating system Supported version Amazon Linux Amazon Linux 2 CentOS Version 7 or higher Debian Version 9 (\"Stretch\") or higher Exception: Version 11 is not supported. Red Hat Enterprise Linux (RHEL) Version 7 or higher SUSE Linux Enterprise Server (SLES) Version 12 Ubuntu Versions 16.04.x, 18.04.x and 20.04.x (LTS versions) Windows Windows Server 2012, 2016, 2019, and 2022, and their service packs. Windows 10 Install the infrastructure agent Starting with version 1.11.4, the infrastructure agent can forward logs to New Relic. To install and run the agent, use a package manager (Linux) or the MSI installer (Windows). Important The log forwarding feature is not included when the infrastructure agent is implemented using Linux tarball or Windows ZIP installations. To use the following links, make sure you are logged to your New Relic account. Amazon Linux CentOS Debian RHEL SLES Ubuntu Windows If you don't have a New Relic account yet, or if you prefer to follow the procedure manually, see our tutorial to install the package manager. Configure the infrastructure agent Configuration files describe which log sources are forwarded. Our infrastructure agent uses .yml files to configure logging. You can add as many config files as you want. To add a new configuration file for the log forwarding feature: Navigate to the log forwarder configuration folder: Linux: /etc/newrelic-infra/logging.d/ Windows: C:\\Program Files\\New Relic\\newrelic-infra\\logging.d\\ Create a logging.yml configuration file, and add the parameters you need. The logging.d directory has various .yml.example files you can use as a reference or starting point. The agent automatically processes new configuration files without having to restart the infrastructure monitoring service. The only exception to this is when configuring a custom Fluent Bit configuration. Log forwarding parameters The infrastructure log forwarding .yml config supports the following parameters: Name (required) To start, define a name of the log or logs you want to forward to New Relic. Log source (required) What you use for the log source will depend on where you want to forward your logs from. Available options include: file Path to the log file or files. The agent tracks changes on the log files in a way similar to tail -f shell. Example: logs: - name: example-log file: /var/log/example.log # Path to a single log file - name: example-log-two file: /var/log/example-two.log # Path to another single log file Copy The file parameter can point to a specific log file or multiple files by using wildcards applied to names and extensions; for example, /logs/*.log. You can use wildcards in place of directories in a file path, which can be used to tail files located in different directories. Example: logs: - name: docker-logs file: /var/lib/docker/containers/*/*.log # Path to multiple folders and files Copy Important Use of wildcards may significantly increase the number of file descriptors and inotify watches the Fluent Bit process keeps open, which can interfere with log collection if the host's file descriptor limit is reached. Tailing a large number of files may require you to increase the maximum number of file descriptors and inotify watchers allowed by the operating system. Please refer to Errors when tailing a large amount of log files for more details on how to increase them. systemd Use the systemd parameter to forward log messages that are collected by the journald daemon in Linux environments. This input type requires the agent to run in root mode. Example: logs: - name: systemd-example systemd: cupsd Copy syslog Syslog data source. Parameters: uri: Syslog socket. Format varies depending on the protocol: TCP/UDP network sockets: [tcp/udp]://LISTEN_ADDRESS:PORT Unix domain sockets: unix_[tcp/udp]:// + /socket/path parser: Syslog parser. Default is rfc3164. Use rfc5424 if your messages include fractional seconds. Note: rfc3164 currently does not work on SuSE. unix_permissions: default is 0644 for domain sockets; this limits entries to processes running as root. You can use 0666 to listen for non-root processes, at your own risk. When running the agent in privileged mode, ports and sockets must be available or owned by nri-agent, with 0666 file permissions, so that other processes can write logs to the sockets. logs: # TCP network socket - name: syslog-tcp-test syslog: uri: tcp://0.0.0.0:5140 # Use the tcp://LISTEN_ADDRESS:PORT format parser: rfc5424 # Default syslog parser is rfc3164 # UDP network socket - name: syslog-udp-test syslog: uri: udp://0.0.0.0:6140 # Use the udp://LISTEN_ADDRESS:PORT format max_line_kb: 35 # Unix TCP domain socket - name: syslog-unix-tcp-test syslog: uri: unix_tcp:///var/unix-tcp-socket-test unix_permissions: 0666 # Default is 0644. Change at your own risk # Unix UDP domain socket - name: syslog-unix-udp-test syslog: uri: unix_udp:///var/unix-udp-socket-test parser: rfc5424 Copy tcp Logs retrieved over TCP connections. Parameters: uri: TCP/IP socket to listen for incoming data. The URI format is tcp://LISTEN_ADDRESS:PORT format: format of the data. It can be json or none. separator: If format: none is used, you can define a separator string for splitting records (default: \\n). logs: - name: tcp-simple-test tcp: uri: tcp://0.0.0.0:1234 # Use the tcp://LISTEN_ADDRESS:PORT format format: none # Raw text - this is default for 'tcp' separator: \\t # String for separating raw text entries max_line_kb: 32 - name: tcp-json-test tcp: uri: tcp://0.0.0.0:2345 # Use the tcp://LISTEN_ADDRESS:PORT format format: json Copy winlog Collect events from Windows log channels. Parameters: channel: name of the channel logs will be collected from. collect-eventids: a list of Windows Event IDs to be collected and forwarded to New Relic. Event ID ranges are supported. exclude-eventids: a list of Windows Event IDs to be excluded from collection. Event ID ranges are supported. All events are collected from the specified channel by default. Configure the collect-eventids and exclude-eventids sections to avoid sending unwanted logs to your New Relic account. Add event IDs or ranges to collect-eventids or exclude-eventids to forward or drop specific events. exclude-eventids takes precedence over collect-eventids if the same event ID is present in both sections. Example: logs: # Winlog log ingestion with eventId filters. - name: windows-security winlog: channel: Security collect-eventids: - 4624 - 4265 - 4700-4800 exclude-eventids: - 4735 # entries for the application, system, powershell, and SCOM channels - name: windows-application winlog: channel: Application - name: windows-system winlog: channel: System - name: windows-pshell winlog: channel: Windows Powershell - name: scom winlog: channel: Operations Manager # Entry for Windows Defender Logs - name: windows-defender winlog: channel: Microsoft-Windows-Windows Defender/Operational # Entry for Windows Clustering Logs - name: windows-clustering winlog: channel: Microsoft-Windows-FailoverClustering/Operational # Entry for IIS logs with logtype attribute for automatic parsing - name: iis-log file: C:\\inetpub\\logs\\LogFiles\\w3svc.log attributes: logtype: iis_w3c Copy Optional configuration The following configuration parameters are not required but are still recommended. attributes List of custom attributes specified as key-value pairs that can be used to send additional data with the logs which you can then query. The attributes configuration parameter can be used with any log source. One common use of the attributes configuration parameter is to specify the logtype attribute. This attribute allows leveraging one of the built-in parsing rules supported by New Relic Logs. Example: logs: - name: example-file-attributes file: /var/log/example.log attributes: logtype: nginx region: example-us-02 team: A-team - name: example-tcp-attributes tcp: uri: tcp://0.0.0.0:2345 format: json attributes: logtype: nginx region: example-us-02 team: B-team Copy attributes automatically inserted by the infrastructure agent The infrastructure agent automatically inserts log attributes for your convenience. Some of them are inserted for any log record, while others depend on the configuration parameters you used while setting up the log forwarder. Attribute name Description entity.guids Always inserted. The infrastructure agent inserts the Entity GUID assigned by New Relic to identify the host where it's running. It is available in the entity.guids field. Note: If the captured logs belong to an application instrumented using APM, the entity.guids field contains both the entity GUID of infrastructure, as well as the GUID of APM, separated by a pipe ( | ) delimiter. fb.input Always inserted. The underlying Fluent Bit input plugin type used to capture the logs. Currently its values are tail, systemd, winlog, syslog, and tcp. filePath Inserted when sing the file input type. Absolute file path of the file being monitored. hostname Always inserted. The hostname of the machine/VM/container executing the infrastructure agent. plugin.type Always inserted. Indicates the utility used to capture the logs. In this case, it is the infrastructure agent itself, so this attribute always has the value nri-agent. pattern Regular expression for filtering records. Only supported for the tail, systemd, syslog, and tcp (only with format none) sources. This field works in a way similar to grep -E in Unix systems. For example, for a given file being captured, you can filter for records containing either WARN or ERROR using: - name: only-records-with-warn-and-error file: /var/log/logFile.log pattern: WARN|ERROR Copy No filtering is applied by default. max_line_kb Maximum size of log entries/lines in KB. If log entries exceed the limit, they are skipped. Default is 128. fluentbit External Fluent Bit configuration and parser files. If defined, they are merged with the existing configuration and parser files generated by the infrastructure agent. The infrastructure agent processes the configuration files located in the logging.d directory and will generate a run-time Fluent Bit configuration file that contains the appropriate [INPUT], [FILTER] and [OUTPUT] sections. Optionally, it will also declare an @INCLUDE in case you provided an external Fluent Bit configuration file via the fluentbit option. The runtime file does not define a [SERVICE] section, leaving all default Fluent Bit configuration values. You can still override Fluent Bit's default settings by defining your own [SERVICE] section in your external Fluent Bit configuration file and include it via the fluentbit option. Parameters: config_file: path to an existing Fluent Bit configuration file. Note that any overlapping source results in duplicate messages in New Relic Logs. parsers_file: path to an existing Fluent Bit parsers file. The following parser names are reserved: rfc3164, rfc3164-local and rfc5424. Sample configuration file Here is an example of a logging.d/ configuration file in YAML format. For more configuration examples, see the infrastructure agent repository. logging.d/sample.yaml # Remember to only use spaces for indentation logs: # Example of 'file' source - name: file-with-attributes file: /var/log/test.log # Path to a single file or pattern attributes: # You can use custom attributes to enrich your data logtype: nginx team: The A Team pattern: Error # Regular expression to filter log entries # Example of 'systemd' source (Linux only) - name: systemd-example systemd: cupsd # Examples of 'syslog' source, one per protocol # TCP network socket - name: syslog-tcp-test syslog: uri: tcp://0.0.0.0:5140 # Use the tcp://LISTEN_ADDRESS:PORT format parser: rfc5424 # Default syslog parser is rfc3164 # UDP network socket - name: syslog-udp-test syslog: uri: udp://0.0.0.0:6140 # Use the udp://LISTEN_ADDRESS:PORT format max_line_kb: 35 # Paths for Unix sockets are defined by combining protocol and path: # unix_udp:// + /path/socket - for example, unix_udp:///tmp/socket # Unix TCP domain socket - name: syslog-unix-tcp-test syslog: uri: unix_tcp:///var/unix-tcp-socket-test unix_permissions: 0666 # Default is 0644. Change at your own risk # Unix UDP domain socket - name: syslog-unix-udp-test syslog: uri: unix_udp:///var/unix-udp-socket-test parser: rfc5424 # Examples of 'tcp' source for formats 'none' and 'json' - name: tcp-simple-test tcp: uri: tcp://0.0.0.0:1234 # Use the tcp://LISTEN_ADDRESS:PORT format format: none # Raw text - this is default for 'tcp' separator: \\t # String for separating raw text entries attributes: # You can add custom attributes to any source of logs tcpFormat: none someOtherAttribute: associatedValue max_line_kb: 32 - name: tcp-json-test tcp: uri: tcp://0.0.0.0:2345 # Use the tcp://LISTEN_ADDRESS:PORT format format: json attributes: tcpFormat: json yetAnotherAttribute: 12345 # Example of Fluent Bit configuration import - name: fluentbit-import fluentbit: config_file: /path/to/fluentbit.config parsers_file: /path/to/fluentbit/parsers.conf Copy View your log data If everything is configured correctly and your data is being collected, you should see logs data in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy Troubleshooting If you encounter problems with configuring your log forwarder, try these troubleshooting tips. No log data If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. No data appears when tailing a file The log forwarding feature requires the agent to have permission to read the data sources. When running the infrastructure agent in privileged or non-privileged modes, make sure that the log files you want to forward (and any intermediary directory in its path) are readable by the user running nri-agent. Example: Check file access under Linux Let's check whether the file /var/log/restrictedLogs/logFile.log can be monitored by the nri-agent user. In Linux, you can do a quick check with the namei command: sudo -u nri-agent namei -ml /var/log/restrictedLogs/logFile.log f: /var/log/restrictedLogs/logFile.log drwxr-xr-x root root / drwxr-xr-x root root var drwxrwxr-x root syslog log drwxr--r-- root root restrictedLogs logFile.log - No such file or directory Copy This command failed because the file is not visible to the nri-agent user. By inspecting the previous output, we can detect that the restrictedLogs directory is missing the execution flag for others. To fix this, execute: sudo chmod 755 /var/log/restrictedLogs Copy And then check for file access again: # sudo -u nri-agent namei -ml /var/log/restrictedLogs/logFile.log f: /var/log/restrictedLogs/logFile.log drwxr-xr-x root root / drwxr-xr-x root root var drwxrwxr-x root syslog log drwxr-xr-x root root restrictedLogs -rw-r----- vagrant vagrant logFile.log Copy The file is now visible to the nri-agent user. You must ensure that the file is also readable by the nri-agent user. To check this, use: # sudo -u nri-agent head /var/log/restrictedLogs/logFile.log head: cannot open '/var/log/restrictedLogs/logFile.log' for reading: Permission denied Copy In this example, the file is missing the read rights for the others group (users other than vagrant and the vagrant user group). You could fix this by granting read permissions to others, but the application could change these permissions upon restart. To avoid this, a better approach is to add the nri-agent user to the vagrant user group. No data appears when capturing via a Syslog socket The log forwarding feature requires that the agent has permission to read the data sources. When running the infrastructure agent in privileged or non-privileged modes: If you're using Unix domain socket files, make sure that the nri-agent user can access these files (please refer to the previous section) and that they have read and write permissions (666) so that other users than nri-agent can write to them. If you're using IP sockets, ensure that the port that you are using is not a system reserved one (like port 80, for example). If no data appears after you enable log management, follow standard log management troubleshooting procedures. No data appears using infrastructure agent proxy As explained in the infrastructure agent configuration guidelines, the proxy parameter must use either HTTP or HTTPS and be in the form https://user:password@hostname:port. The agent can parse the parameter without the HTTP or HTTPS, but the log forwarder cannot. You will see an error like the following in the agent verbose logs: [ERROR] building HTTP transport: parse \\\"hostname:port\\\": first path segment in URL cannot contain colon Copy To solve this problem, check your newrelic-infra.yml file, and ensure the proxy parameter adheres to this form. If you're using caBundleFile or caBundleDir in order to specify any certificate, we recommend to follow the below rules for each OS: Linux For HTTP proxies you don't need to setup any certificates. The plugin loads the system certificates and New Relic sends logs into the logging endpoint. However, you can specify the proxy self-signed certificate (PEM file) using either the caBundleFile or caBundleDir parameters. Windows For HTTP proxies you don't need to setup any certificates. The plugin loads the system certificates. For HTTPS, you can configure it in one of the following ways: Import the proxy certificate to the system pool (Recommended) Import the proxy self-signed certificate (PEM file) by using the MMC tool. Refer to this link, and in Step 2 ensure to import it in your Trusted Root Certification Authorities, instead of in the Intermediate Certification Authorities. Using the caBundleFile and caBundleDir parameters On Windows, we cannot load both the certificates from the system certificate pool and the ones specified with the caBundleFile caBundleDir parameters. So, if you are using caBundleFile or caBundleDir, ensure that the following certificates are placed in the same PEM file (when using caBundleFile) or in the same directory (when using caBundleDir): The Proxy certificate (because it's an HTTPS proxy). The Logging Endpoint certificate (eg. https://log-api.newrelic.com/log/v1). The Infrastructure Agent certificate (eg. https://infra-api.newrelic.com). You can check the certificates by running: # openssl s_client -connect log-api.newrelic.com:443 -servername log-api.newrelic.com Copy Sending the infrastructure agent's logs to New Relic You can configure the infrastructure agent to send its own logs to New Relic. This is useful for troubleshooting issues with log forwarding, the agent, or when contacting support. To forward the infrastructure agent logs to New Relic: Edit your newrelic-infra.yml file. Enable agent logging in troubleshooting mode by adding verbose: 3. On Windows and systems that don't use systemd or where journald is inaccessible, verbose:3 causes the agent to write the logs on the disk. Revert to verbose:0 to prevent this. (Recommended): Enable agent logging in JSON format to log_format: json. Restart the agent to load the new settings. This configuration sets up the agent in troubleshooting mode, but the log forwarder (based on Fluent Bit) will continue in a non-verbose mode. Sometimes you can have issues with the log forwarder itself. For example, there may be problems accessing a specific channel when shipping Windows log events or when accessing a particular log file. In these situations, you can also enable the verbose mode for the log forwarder: Set verbose to a value other than 0. Add the configuration option: trace: [\"log.fw\"]. Caution Check whether you are using the [fluentbit] option. When setting verbose: 3 and trace: [\"log.fw\"], ensure that you don't define any [OUTPUT] section pointing to stdout in an external Fluent Bit configuration file, Fluent Bit does not start with the infra agent Important Fluent Bit's tail plugin does not support network drives. For Linux versions prior to 2016, you may need to update the OpenSSL library to 1.1.0 (or higher). To check if you have this problem: See if infra-agent has started Fluent Bit by running: ps -aux | grep td-agent-bit Copy If it isn't running go to /var/db/newrelic-infra/newrelic-integrations/logging and run: ./fluent-bit -i systemd -o stdout Copy If you get the following error, update OpenSSL to 1.1.0 or higher: error while loading shared libraries: libssl.so.1.1: cannot open shared object file: No such file or directory Copy Runtime error on Windows One of the following error messages may appear when enabling log forwarding on Windows: The code execution cannot proceed because VCRUNTIME140.dll was not found. Copy OR error=\"exit status 3221225781\" process=log-forwarder Copy This is caused by a missing DLL. To solve the issue, install the Microsoft Visual C++ Redistributable as applicable: x64 x86 Errors when tailing a large amount of log files (Linux) It's common to face either one of the following error messages when attempting to tail a large amount of files: Too many open files The user limit on the total number of inotify watches was reached or the kernel failed to allocate a needed resource The operating system defines a maximum amount of allocatable file descriptors (typically 1024 by default), and a maximum amount of allocatable inotify watches (typically 8192 by default). Any process attempting to go above these limits will fail, returning one of the errors above. The underlying technology we use to forward logs, Fluent Bit, opens one file descriptor and sets an inotify watch for each file you configure to be forwarded. Moreover, at the time of writing this section, Fluent Bit uses an extra set of 32 file descriptors for its normal operation, with another extra file descriptor when it shuts down. Therefore, to capture a large amount of files you need to ensure that both the file descriptor and inotify watch limits are slightly greater than the amount of log files you wish to tail. The following instructions summarize how to increase these limits if you want to tail 10,000 log files. Also, it assumes the infrastructure agent is installed in root running mode, and therefore must be run using the root user. Check which is the current hard limit for the amount of file descriptors per process. Typically, this limit should be quite high and should not need to be modified. ulimit -Hn Copy Add the following line to /etc/security/limits.conf. We specified a limit of 10100 here instead of just 10000 to allow Fluent Bit to allocate the extra file descriptors it may need to work. root soft nofile 10100 # replace root by nri-agent for non-root (privileged and unprivileged) installations Copy Add the following line to /etc/pam.d/common-session so that the previous limit is applied upon restart: session required pam_limits.so Copy Add the following line to /etc/sysctl.conf to increase the amount of allowed inotify watchers per user. We specified a limit of 18192 here instead of just 10000 so that the root user will still have 8192 available inotify watches (the default value). fs.inotify.max_user_watches=18192 Copy Restart your system. Ensure that the new limits have been enforced by running: ulimit -Sn # Should return 10100 cat /proc/sys/fs/inotify/max_user_watches # Should return 18192 Copy Learn more on how to increase open file limits, or on how to increase the inotify watches. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards. Uninstall log forwarding To uninstall log forwarding capabilities, go to your logging.d directory, and remove files with the .yml extension that were originally added during the configuration process. Linux: /etc/newrelic-infra/logging.d/ Windows: C:\\Program Files\\New Relic\\newrelic-infra\\logging.d\\",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 353.48547,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward your <em>logs</em> using the <em>infrastructure</em> agent",
        "sections": "Sending the <em>infrastructure</em> agent&#x27;s <em>logs</em> to <em>New</em> <em>Relic</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " <em>management</em> capabilities to collect, process, explore, query, and alert on your <em>log</em> data. Basic process To forward your <em>logs</em> through our infrastructure <em>monitoring</em> agent: If you haven&#x27;t already, create a <em>New</em> <em>Relic</em> account. It&#x27;s free, forever. Verify the system requirements needed for configuring <em>logs</em>"
      },
      "id": "603e9df164441f6b6f4e8843"
    },
    {
      "sections": [
        "Forward your logs to New Relic",
        "How it works",
        "Get started with log forwarders",
        "Log forwarding options",
        "TCP endpoint",
        "Log API",
        "What's next?"
      ],
      "title": "Forward your logs to New Relic",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "b6e203ddc367d5a2b5e002916e49d34f4ba17a87",
      "image": "https://docs.newrelic.com/static/c3d5443b84a1e2b26a4767ce35fa58f3/e5166/new-relic-logs-in-context-diagram.jpg",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/enable-log-management-new-relic/",
      "published_at": "2022-01-08T02:40:08Z",
      "updated_at": "2022-01-04T07:02:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our log management capabilities help you to collect, process, explore, query, and alert on your log data. To get your logs into New Relic, you can: Use your existing log forwarding solution to collect your logs and extend the metadata that is forwarded to New Relic. Use our infrastructure agent as a lightweight data collector, without having to use additional software. Use our Log API to forward your logs via HTTP. Use syslog protocols to forward your logs via a TCP endpoint. How it works The following diagram shows the lifecycle of a log message for an app, from enrichment with APM agent metadata (contextual logging), to formatting and forwarding the log data to New Relic. This diagram illustrates the flow of log messages through New Relic. Standard log formatters transform log events into meaningful output (such as text files) that can be used by downstream people and processes. The NewRelicFormatter transforms log events into the JSON format expected by New Relic. These files contain log information and extended metadata. When you configure your log forwarder (our infrastructure monitoring agent, Fluentd, Logstash, etc.), you can also extend and enrich your log data. By configuring logs in context, the log enricher links the formatted log data with additional transaction information from your application or host. Now your log files are enriched with enhanced metadata and contextual logging data. Your log forwarder sends the files to our logging endpoint for processing. From there you can use our log management capabilities to view, query, set up alerts, and more in New Relic. Here is an example of logs for your host's UI. You can see logs in context of events for the selected time period, and drill down into detailed data for any of the highlighted attributes. To take advantage of even more capabilities, click Query logs from here to go directly to the Logs UI. Here is an example of a host's logs in context related to an event. Get started with log forwarders To forward your logs to New Relic with enriched metadata: If you don't have one already, create a New Relic account. It's free, forever. Have your New Relic account's license key. Install a compatible log forwarder. Ensure that outbound connectivity on TCP port 443 is allowed to the CIDR range that matches your region. Use the DNS name log-api.newrelic.com or log-api.eu.newrelic.com for configuration. For apps monitored by a New Relic APM agent, configure logs in context. Log forwarding options Use any of these solutions to forward your logs to New Relic. Recommended: Infrastructure monitoring agent Amazon: AWS CloudWatch plugin AWS FireLens plugin AWS Kinesis Firehose AWS Lambda for sending logs from S3 Microsoft: Azure ARM template Other log forwarding plugins: Fluent Bit plugin Fluentd plugin Google Cloud Platform Pub/Sub Heroku log streaming Kubernetes plugin Logstash plugin Vector plugin TCP endpoint In some situations you may not have log forwarders; for example, with CDNs, hardware devices, or managed services. You can use syslog protocols such as rsyslog and syslog-ng, and forward your logs to New Relic via a TCP endpoint. Log API If you prefer to connect to New Relic without installing a plugin, we offer an HTTP input integration. This option sends your monitored log data directly to New Relic via the Log API. What's next? After you enable your log forwarder, make the most of your data in New Relic with our log management capabilities: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data if you are forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards. For example, to query and manage your data partition rules, see our NerdGraph tutorial.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 315.18262,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward your <em>logs</em> to <em>New</em> <em>Relic</em>",
        "sections": "Forward your <em>logs</em> to <em>New</em> <em>Relic</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " for configuration. For apps monitored by a <em>New</em> <em>Relic</em> APM agent, configure <em>logs</em> in context. <em>Log</em> forwarding options Use any of these solutions to forward your <em>logs</em> to <em>New</em> <em>Relic</em>. Recommended: Infrastructure <em>monitoring</em> agent Amazon: AWS <em>Cloud</em>Watch plugin AWS FireLens plugin AWS Kinesis Firehose AWS Lambda for sending"
      },
      "id": "61571e0e28ccbcbc52f21431"
    }
  ],
  "/docs/logs/forward-logs/enable-log-management-new-relic": [
    {
      "sections": [
        "Fluent Bit plugin for log forwarding",
        "Basic process",
        "Install the Fluent Bit plugin",
        "Tip",
        "Configure the Fluent Bit plugin",
        "Important",
        "Test the Fluent Bit plugin",
        "Associate logs with entities",
        "Optional: Configure plugin attributes",
        "View log data",
        "What's next?"
      ],
      "title": "Fluent Bit plugin for log forwarding",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "dd686f074bb80908ae9de19e543a3bae7d87a466",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/fluent-bit-plugin-log-forwarding/",
      "published_at": "2022-01-08T02:38:07Z",
      "updated_at": "2022-01-08T02:38:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your log data in New Relic. Forwarding your Fluent Bit logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Basic process We have published a container with the plugin installed. It serves as a base image to be used by our Kubernetes integration. We recommend you use this base image and layer your own custom configuration files. To forward your logs from Fluent Bit to New Relic: Make sure you have: A New Relic license key Fluent Bit 1.0 or higher (recommended), although v0.12 or higher is supported Fluent Bit Windows install directions can be found here Fluent Bit Linux install directions can be found here Install the Fluent Bit plugin. Configure the Fluent Bit plugin. Test the Fluent Bit plugin. Generate some traffic and wait a few minutes, then check your account for data. Install the Fluent Bit plugin To install the Fluent Bit plugin: Navigate to New Relic's Fluent Bit plugin repository on GitHub. From the repository page, clone or download the repository. Run the following command to build your plugin: cd newrelic-fluent-bit-output && make all Copy Store out_newrelic.so or out_newrelic_winXX.dll at a location that can be accessed by the fluent-bit daemon. Tip If you'd rather not compile the plugin yourself, you can download pre-compiled versions from our GitHub repository's releases page. Configure the Fluent Bit plugin Fluent Bit needs to know the location of the New Relic plugin and the New Relic license key to output data to New Relic. To configure your Fluent Bit plugin: Important Pay attention to white space when editing your config files. Be sure to use four spaces to indent and one space between keys and values. Locate or create a plugins.conf file in your plugins directory. In the plugins.conf file, add a reference to out_newrelic.so, adjacent to your fluent-bit.conf file: [PLUGINS] Path /PATH/TO/newrelic-fluent-bit-output/out_newrelic.so Copy In the fluent-bit.conf file, add the following line under the service block: [SERVICE] # This is the main configuration block for fluent bit. # Ensure the follow line exists somewhere in the SERVICE block Plugins_File plugins.conf Copy At the bottom of the fluent-bit.conf file, add the following to set up the input and output filters. Replace the placeholder text with your New Relic license key: [INPUT] Name tail Path /PATH/TO/YOUR/LOG/FILE [OUTPUT] Name newrelic Match * licenseKey YOUR_LICENSE_KEY # Optional maxBufferSize 256000 maxRecords 1024 Copy Restart your Fluent Bit instance with the following command: fluent-bit -c /PATH/TO/fluent-bit.conf Copy Test the Fluent Bit plugin To test if your Fluent Bit plugin is receiving input from a log file: Run the following command to append a test log message to your log file: echo \"test message\" >> /PATH/TO/YOUR/LOG/FILE Copy Search the New Relic Logs UI for test message. Associate logs with entities To associate a log line with an entity, such as an infrastructure host, add a FILTER block: [FILTER] Name modify Match * # Or specify a match Add entity.guids <Your Entity GUID ID> # Optional Add hostname <Your hostname> # Optional Copy For more options, see the Fluent Bit modify filter documentation and our documentation to forward your logs using the infrastructure agent. Optional: Configure plugin attributes Once you have installed and configured the Fluent Bit plugin, you can use the following attributes to configure how the plugin sends data to New Relic: Key Description licenseKey The New Relic license key. Use either licenseKey (recommended) or apiKey, not both. Default: none maxBufferSize The maximum size the payloads sent, in bytes. Default: 256000 maxRecords The maximum number of records to send at a time. Default: 1024 apiKey Deprecated. Takes a New Relic Insights insert key, but using the licenseKey attribute is preferred. Use either licenseKey or apiKey, not both. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 374.04892,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Fluent Bit plugin for <em>log</em> forwarding",
        "sections": "Associate <em>logs</em> with entities",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "If your <em>log</em> data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your <em>log</em> data in <em>New</em> <em>Relic</em>. Forwarding your Fluent Bit <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert"
      },
      "id": "603ea7ec28ccbce542eba791"
    },
    {
      "sections": [
        "Forward your logs using the infrastructure agent",
        "Basic process",
        "System requirements",
        "Important",
        "Install the infrastructure agent",
        "Configure the infrastructure agent",
        "Log forwarding parameters",
        "Name (required)",
        "Log source (required)",
        "file",
        "systemd",
        "syslog",
        "tcp",
        "winlog",
        "Optional configuration",
        "attributes",
        "attributes automatically inserted by the infrastructure agent",
        "pattern",
        "max_line_kb",
        "fluentbit",
        "Sample configuration file",
        "logging.d/sample.yaml",
        "View your log data",
        "Troubleshooting",
        "No log data",
        "No data appears when tailing a file",
        "No data appears when capturing via a Syslog socket",
        "No data appears using infrastructure agent proxy",
        "Sending the infrastructure agent's logs to New Relic",
        "Caution",
        "Fluent Bit does not start with the infra agent",
        "Runtime error on Windows",
        "Errors when tailing a large amount of log files (Linux)",
        "What's next?",
        "Uninstall log forwarding"
      ],
      "title": "Forward your logs using the infrastructure agent",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "a1deac410f0eedfb819348524a85a73bbb9d9daf",
      "image": "https://docs.newrelic.com/static/5d4352531871b652878b3cb0768e3f23/c1b63/demotron-host-logs-123021.png",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/forward-your-logs-using-infrastructure-agent/",
      "published_at": "2022-01-09T01:46:14Z",
      "updated_at": "2022-01-07T01:41:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can forward your logs to New Relic using our infrastructure monitoring agent. This makes all of your logging data available in one location and provides deeper visibility into both your application and your platform performance data. Forwarding your logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Basic process To forward your logs through our infrastructure monitoring agent: If you haven't already, create a New Relic account. It's free, forever. Verify the system requirements needed for configuring logs. Ensure you have installed the infrastructure agent, version 1.11.4 or higher. Create a logging.yml configuration file in the infrastructure agent's logging.d directory. Configure your log sources and other parameters. Generate some traffic and wait a few minutes, then check your account for data. Explore your log data in the Logs UI and benefit from the log attributes automatically inserted by the infrastructure agent. Here is an example of logs for your host's UI. You can see logs in context of events for the selected time period, and drill down into detailed data for any of the highlighted attributes. To take advantage of even more capabilities, click Query logs from here to go directly to the Logs UI. Here is an example of a host's logs in context related to an event. System requirements To use the log forwarder of the infrastructure agent, make sure you meet the following requirements: Infrastructure agent version 1.11.4 or higher OpenSSL library 1.1.0 or higher is required by Infrastructure agent starting from 1.16.4. Built-in support for ARM64 architecture on Linux systems (in example, AWS Graviton architecture) added in Infrastructure agent 1.26.0. Important The log forwarding feature is not supported with the Docker container for infrastructure monitoring agents. The log forwarding feature is compatible with the following operating systems: Operating system Supported version Amazon Linux Amazon Linux 2 CentOS Version 7 or higher Debian Version 9 (\"Stretch\") or higher Exception: Version 11 is not supported. Red Hat Enterprise Linux (RHEL) Version 7 or higher SUSE Linux Enterprise Server (SLES) Version 12 Ubuntu Versions 16.04.x, 18.04.x and 20.04.x (LTS versions) Windows Windows Server 2012, 2016, 2019, and 2022, and their service packs. Windows 10 Install the infrastructure agent Starting with version 1.11.4, the infrastructure agent can forward logs to New Relic. To install and run the agent, use a package manager (Linux) or the MSI installer (Windows). Important The log forwarding feature is not included when the infrastructure agent is implemented using Linux tarball or Windows ZIP installations. To use the following links, make sure you are logged to your New Relic account. Amazon Linux CentOS Debian RHEL SLES Ubuntu Windows If you don't have a New Relic account yet, or if you prefer to follow the procedure manually, see our tutorial to install the package manager. Configure the infrastructure agent Configuration files describe which log sources are forwarded. Our infrastructure agent uses .yml files to configure logging. You can add as many config files as you want. To add a new configuration file for the log forwarding feature: Navigate to the log forwarder configuration folder: Linux: /etc/newrelic-infra/logging.d/ Windows: C:\\Program Files\\New Relic\\newrelic-infra\\logging.d\\ Create a logging.yml configuration file, and add the parameters you need. The logging.d directory has various .yml.example files you can use as a reference or starting point. The agent automatically processes new configuration files without having to restart the infrastructure monitoring service. The only exception to this is when configuring a custom Fluent Bit configuration. Log forwarding parameters The infrastructure log forwarding .yml config supports the following parameters: Name (required) To start, define a name of the log or logs you want to forward to New Relic. Log source (required) What you use for the log source will depend on where you want to forward your logs from. Available options include: file Path to the log file or files. The agent tracks changes on the log files in a way similar to tail -f shell. Example: logs: - name: example-log file: /var/log/example.log # Path to a single log file - name: example-log-two file: /var/log/example-two.log # Path to another single log file Copy The file parameter can point to a specific log file or multiple files by using wildcards applied to names and extensions; for example, /logs/*.log. You can use wildcards in place of directories in a file path, which can be used to tail files located in different directories. Example: logs: - name: docker-logs file: /var/lib/docker/containers/*/*.log # Path to multiple folders and files Copy Important Use of wildcards may significantly increase the number of file descriptors and inotify watches the Fluent Bit process keeps open, which can interfere with log collection if the host's file descriptor limit is reached. Tailing a large number of files may require you to increase the maximum number of file descriptors and inotify watchers allowed by the operating system. Please refer to Errors when tailing a large amount of log files for more details on how to increase them. systemd Use the systemd parameter to forward log messages that are collected by the journald daemon in Linux environments. This input type requires the agent to run in root mode. Example: logs: - name: systemd-example systemd: cupsd Copy syslog Syslog data source. Parameters: uri: Syslog socket. Format varies depending on the protocol: TCP/UDP network sockets: [tcp/udp]://LISTEN_ADDRESS:PORT Unix domain sockets: unix_[tcp/udp]:// + /socket/path parser: Syslog parser. Default is rfc3164. Use rfc5424 if your messages include fractional seconds. Note: rfc3164 currently does not work on SuSE. unix_permissions: default is 0644 for domain sockets; this limits entries to processes running as root. You can use 0666 to listen for non-root processes, at your own risk. When running the agent in privileged mode, ports and sockets must be available or owned by nri-agent, with 0666 file permissions, so that other processes can write logs to the sockets. logs: # TCP network socket - name: syslog-tcp-test syslog: uri: tcp://0.0.0.0:5140 # Use the tcp://LISTEN_ADDRESS:PORT format parser: rfc5424 # Default syslog parser is rfc3164 # UDP network socket - name: syslog-udp-test syslog: uri: udp://0.0.0.0:6140 # Use the udp://LISTEN_ADDRESS:PORT format max_line_kb: 35 # Unix TCP domain socket - name: syslog-unix-tcp-test syslog: uri: unix_tcp:///var/unix-tcp-socket-test unix_permissions: 0666 # Default is 0644. Change at your own risk # Unix UDP domain socket - name: syslog-unix-udp-test syslog: uri: unix_udp:///var/unix-udp-socket-test parser: rfc5424 Copy tcp Logs retrieved over TCP connections. Parameters: uri: TCP/IP socket to listen for incoming data. The URI format is tcp://LISTEN_ADDRESS:PORT format: format of the data. It can be json or none. separator: If format: none is used, you can define a separator string for splitting records (default: \\n). logs: - name: tcp-simple-test tcp: uri: tcp://0.0.0.0:1234 # Use the tcp://LISTEN_ADDRESS:PORT format format: none # Raw text - this is default for 'tcp' separator: \\t # String for separating raw text entries max_line_kb: 32 - name: tcp-json-test tcp: uri: tcp://0.0.0.0:2345 # Use the tcp://LISTEN_ADDRESS:PORT format format: json Copy winlog Collect events from Windows log channels. Parameters: channel: name of the channel logs will be collected from. collect-eventids: a list of Windows Event IDs to be collected and forwarded to New Relic. Event ID ranges are supported. exclude-eventids: a list of Windows Event IDs to be excluded from collection. Event ID ranges are supported. All events are collected from the specified channel by default. Configure the collect-eventids and exclude-eventids sections to avoid sending unwanted logs to your New Relic account. Add event IDs or ranges to collect-eventids or exclude-eventids to forward or drop specific events. exclude-eventids takes precedence over collect-eventids if the same event ID is present in both sections. Example: logs: # Winlog log ingestion with eventId filters. - name: windows-security winlog: channel: Security collect-eventids: - 4624 - 4265 - 4700-4800 exclude-eventids: - 4735 # entries for the application, system, powershell, and SCOM channels - name: windows-application winlog: channel: Application - name: windows-system winlog: channel: System - name: windows-pshell winlog: channel: Windows Powershell - name: scom winlog: channel: Operations Manager # Entry for Windows Defender Logs - name: windows-defender winlog: channel: Microsoft-Windows-Windows Defender/Operational # Entry for Windows Clustering Logs - name: windows-clustering winlog: channel: Microsoft-Windows-FailoverClustering/Operational # Entry for IIS logs with logtype attribute for automatic parsing - name: iis-log file: C:\\inetpub\\logs\\LogFiles\\w3svc.log attributes: logtype: iis_w3c Copy Optional configuration The following configuration parameters are not required but are still recommended. attributes List of custom attributes specified as key-value pairs that can be used to send additional data with the logs which you can then query. The attributes configuration parameter can be used with any log source. One common use of the attributes configuration parameter is to specify the logtype attribute. This attribute allows leveraging one of the built-in parsing rules supported by New Relic Logs. Example: logs: - name: example-file-attributes file: /var/log/example.log attributes: logtype: nginx region: example-us-02 team: A-team - name: example-tcp-attributes tcp: uri: tcp://0.0.0.0:2345 format: json attributes: logtype: nginx region: example-us-02 team: B-team Copy attributes automatically inserted by the infrastructure agent The infrastructure agent automatically inserts log attributes for your convenience. Some of them are inserted for any log record, while others depend on the configuration parameters you used while setting up the log forwarder. Attribute name Description entity.guids Always inserted. The infrastructure agent inserts the Entity GUID assigned by New Relic to identify the host where it's running. It is available in the entity.guids field. Note: If the captured logs belong to an application instrumented using APM, the entity.guids field contains both the entity GUID of infrastructure, as well as the GUID of APM, separated by a pipe ( | ) delimiter. fb.input Always inserted. The underlying Fluent Bit input plugin type used to capture the logs. Currently its values are tail, systemd, winlog, syslog, and tcp. filePath Inserted when sing the file input type. Absolute file path of the file being monitored. hostname Always inserted. The hostname of the machine/VM/container executing the infrastructure agent. plugin.type Always inserted. Indicates the utility used to capture the logs. In this case, it is the infrastructure agent itself, so this attribute always has the value nri-agent. pattern Regular expression for filtering records. Only supported for the tail, systemd, syslog, and tcp (only with format none) sources. This field works in a way similar to grep -E in Unix systems. For example, for a given file being captured, you can filter for records containing either WARN or ERROR using: - name: only-records-with-warn-and-error file: /var/log/logFile.log pattern: WARN|ERROR Copy No filtering is applied by default. max_line_kb Maximum size of log entries/lines in KB. If log entries exceed the limit, they are skipped. Default is 128. fluentbit External Fluent Bit configuration and parser files. If defined, they are merged with the existing configuration and parser files generated by the infrastructure agent. The infrastructure agent processes the configuration files located in the logging.d directory and will generate a run-time Fluent Bit configuration file that contains the appropriate [INPUT], [FILTER] and [OUTPUT] sections. Optionally, it will also declare an @INCLUDE in case you provided an external Fluent Bit configuration file via the fluentbit option. The runtime file does not define a [SERVICE] section, leaving all default Fluent Bit configuration values. You can still override Fluent Bit's default settings by defining your own [SERVICE] section in your external Fluent Bit configuration file and include it via the fluentbit option. Parameters: config_file: path to an existing Fluent Bit configuration file. Note that any overlapping source results in duplicate messages in New Relic Logs. parsers_file: path to an existing Fluent Bit parsers file. The following parser names are reserved: rfc3164, rfc3164-local and rfc5424. Sample configuration file Here is an example of a logging.d/ configuration file in YAML format. For more configuration examples, see the infrastructure agent repository. logging.d/sample.yaml # Remember to only use spaces for indentation logs: # Example of 'file' source - name: file-with-attributes file: /var/log/test.log # Path to a single file or pattern attributes: # You can use custom attributes to enrich your data logtype: nginx team: The A Team pattern: Error # Regular expression to filter log entries # Example of 'systemd' source (Linux only) - name: systemd-example systemd: cupsd # Examples of 'syslog' source, one per protocol # TCP network socket - name: syslog-tcp-test syslog: uri: tcp://0.0.0.0:5140 # Use the tcp://LISTEN_ADDRESS:PORT format parser: rfc5424 # Default syslog parser is rfc3164 # UDP network socket - name: syslog-udp-test syslog: uri: udp://0.0.0.0:6140 # Use the udp://LISTEN_ADDRESS:PORT format max_line_kb: 35 # Paths for Unix sockets are defined by combining protocol and path: # unix_udp:// + /path/socket - for example, unix_udp:///tmp/socket # Unix TCP domain socket - name: syslog-unix-tcp-test syslog: uri: unix_tcp:///var/unix-tcp-socket-test unix_permissions: 0666 # Default is 0644. Change at your own risk # Unix UDP domain socket - name: syslog-unix-udp-test syslog: uri: unix_udp:///var/unix-udp-socket-test parser: rfc5424 # Examples of 'tcp' source for formats 'none' and 'json' - name: tcp-simple-test tcp: uri: tcp://0.0.0.0:1234 # Use the tcp://LISTEN_ADDRESS:PORT format format: none # Raw text - this is default for 'tcp' separator: \\t # String for separating raw text entries attributes: # You can add custom attributes to any source of logs tcpFormat: none someOtherAttribute: associatedValue max_line_kb: 32 - name: tcp-json-test tcp: uri: tcp://0.0.0.0:2345 # Use the tcp://LISTEN_ADDRESS:PORT format format: json attributes: tcpFormat: json yetAnotherAttribute: 12345 # Example of Fluent Bit configuration import - name: fluentbit-import fluentbit: config_file: /path/to/fluentbit.config parsers_file: /path/to/fluentbit/parsers.conf Copy View your log data If everything is configured correctly and your data is being collected, you should see logs data in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy Troubleshooting If you encounter problems with configuring your log forwarder, try these troubleshooting tips. No log data If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. No data appears when tailing a file The log forwarding feature requires the agent to have permission to read the data sources. When running the infrastructure agent in privileged or non-privileged modes, make sure that the log files you want to forward (and any intermediary directory in its path) are readable by the user running nri-agent. Example: Check file access under Linux Let's check whether the file /var/log/restrictedLogs/logFile.log can be monitored by the nri-agent user. In Linux, you can do a quick check with the namei command: sudo -u nri-agent namei -ml /var/log/restrictedLogs/logFile.log f: /var/log/restrictedLogs/logFile.log drwxr-xr-x root root / drwxr-xr-x root root var drwxrwxr-x root syslog log drwxr--r-- root root restrictedLogs logFile.log - No such file or directory Copy This command failed because the file is not visible to the nri-agent user. By inspecting the previous output, we can detect that the restrictedLogs directory is missing the execution flag for others. To fix this, execute: sudo chmod 755 /var/log/restrictedLogs Copy And then check for file access again: # sudo -u nri-agent namei -ml /var/log/restrictedLogs/logFile.log f: /var/log/restrictedLogs/logFile.log drwxr-xr-x root root / drwxr-xr-x root root var drwxrwxr-x root syslog log drwxr-xr-x root root restrictedLogs -rw-r----- vagrant vagrant logFile.log Copy The file is now visible to the nri-agent user. You must ensure that the file is also readable by the nri-agent user. To check this, use: # sudo -u nri-agent head /var/log/restrictedLogs/logFile.log head: cannot open '/var/log/restrictedLogs/logFile.log' for reading: Permission denied Copy In this example, the file is missing the read rights for the others group (users other than vagrant and the vagrant user group). You could fix this by granting read permissions to others, but the application could change these permissions upon restart. To avoid this, a better approach is to add the nri-agent user to the vagrant user group. No data appears when capturing via a Syslog socket The log forwarding feature requires that the agent has permission to read the data sources. When running the infrastructure agent in privileged or non-privileged modes: If you're using Unix domain socket files, make sure that the nri-agent user can access these files (please refer to the previous section) and that they have read and write permissions (666) so that other users than nri-agent can write to them. If you're using IP sockets, ensure that the port that you are using is not a system reserved one (like port 80, for example). If no data appears after you enable log management, follow standard log management troubleshooting procedures. No data appears using infrastructure agent proxy As explained in the infrastructure agent configuration guidelines, the proxy parameter must use either HTTP or HTTPS and be in the form https://user:password@hostname:port. The agent can parse the parameter without the HTTP or HTTPS, but the log forwarder cannot. You will see an error like the following in the agent verbose logs: [ERROR] building HTTP transport: parse \\\"hostname:port\\\": first path segment in URL cannot contain colon Copy To solve this problem, check your newrelic-infra.yml file, and ensure the proxy parameter adheres to this form. If you're using caBundleFile or caBundleDir in order to specify any certificate, we recommend to follow the below rules for each OS: Linux For HTTP proxies you don't need to setup any certificates. The plugin loads the system certificates and New Relic sends logs into the logging endpoint. However, you can specify the proxy self-signed certificate (PEM file) using either the caBundleFile or caBundleDir parameters. Windows For HTTP proxies you don't need to setup any certificates. The plugin loads the system certificates. For HTTPS, you can configure it in one of the following ways: Import the proxy certificate to the system pool (Recommended) Import the proxy self-signed certificate (PEM file) by using the MMC tool. Refer to this link, and in Step 2 ensure to import it in your Trusted Root Certification Authorities, instead of in the Intermediate Certification Authorities. Using the caBundleFile and caBundleDir parameters On Windows, we cannot load both the certificates from the system certificate pool and the ones specified with the caBundleFile caBundleDir parameters. So, if you are using caBundleFile or caBundleDir, ensure that the following certificates are placed in the same PEM file (when using caBundleFile) or in the same directory (when using caBundleDir): The Proxy certificate (because it's an HTTPS proxy). The Logging Endpoint certificate (eg. https://log-api.newrelic.com/log/v1). The Infrastructure Agent certificate (eg. https://infra-api.newrelic.com). You can check the certificates by running: # openssl s_client -connect log-api.newrelic.com:443 -servername log-api.newrelic.com Copy Sending the infrastructure agent's logs to New Relic You can configure the infrastructure agent to send its own logs to New Relic. This is useful for troubleshooting issues with log forwarding, the agent, or when contacting support. To forward the infrastructure agent logs to New Relic: Edit your newrelic-infra.yml file. Enable agent logging in troubleshooting mode by adding verbose: 3. On Windows and systems that don't use systemd or where journald is inaccessible, verbose:3 causes the agent to write the logs on the disk. Revert to verbose:0 to prevent this. (Recommended): Enable agent logging in JSON format to log_format: json. Restart the agent to load the new settings. This configuration sets up the agent in troubleshooting mode, but the log forwarder (based on Fluent Bit) will continue in a non-verbose mode. Sometimes you can have issues with the log forwarder itself. For example, there may be problems accessing a specific channel when shipping Windows log events or when accessing a particular log file. In these situations, you can also enable the verbose mode for the log forwarder: Set verbose to a value other than 0. Add the configuration option: trace: [\"log.fw\"]. Caution Check whether you are using the [fluentbit] option. When setting verbose: 3 and trace: [\"log.fw\"], ensure that you don't define any [OUTPUT] section pointing to stdout in an external Fluent Bit configuration file, Fluent Bit does not start with the infra agent Important Fluent Bit's tail plugin does not support network drives. For Linux versions prior to 2016, you may need to update the OpenSSL library to 1.1.0 (or higher). To check if you have this problem: See if infra-agent has started Fluent Bit by running: ps -aux | grep td-agent-bit Copy If it isn't running go to /var/db/newrelic-infra/newrelic-integrations/logging and run: ./fluent-bit -i systemd -o stdout Copy If you get the following error, update OpenSSL to 1.1.0 or higher: error while loading shared libraries: libssl.so.1.1: cannot open shared object file: No such file or directory Copy Runtime error on Windows One of the following error messages may appear when enabling log forwarding on Windows: The code execution cannot proceed because VCRUNTIME140.dll was not found. Copy OR error=\"exit status 3221225781\" process=log-forwarder Copy This is caused by a missing DLL. To solve the issue, install the Microsoft Visual C++ Redistributable as applicable: x64 x86 Errors when tailing a large amount of log files (Linux) It's common to face either one of the following error messages when attempting to tail a large amount of files: Too many open files The user limit on the total number of inotify watches was reached or the kernel failed to allocate a needed resource The operating system defines a maximum amount of allocatable file descriptors (typically 1024 by default), and a maximum amount of allocatable inotify watches (typically 8192 by default). Any process attempting to go above these limits will fail, returning one of the errors above. The underlying technology we use to forward logs, Fluent Bit, opens one file descriptor and sets an inotify watch for each file you configure to be forwarded. Moreover, at the time of writing this section, Fluent Bit uses an extra set of 32 file descriptors for its normal operation, with another extra file descriptor when it shuts down. Therefore, to capture a large amount of files you need to ensure that both the file descriptor and inotify watch limits are slightly greater than the amount of log files you wish to tail. The following instructions summarize how to increase these limits if you want to tail 10,000 log files. Also, it assumes the infrastructure agent is installed in root running mode, and therefore must be run using the root user. Check which is the current hard limit for the amount of file descriptors per process. Typically, this limit should be quite high and should not need to be modified. ulimit -Hn Copy Add the following line to /etc/security/limits.conf. We specified a limit of 10100 here instead of just 10000 to allow Fluent Bit to allocate the extra file descriptors it may need to work. root soft nofile 10100 # replace root by nri-agent for non-root (privileged and unprivileged) installations Copy Add the following line to /etc/pam.d/common-session so that the previous limit is applied upon restart: session required pam_limits.so Copy Add the following line to /etc/sysctl.conf to increase the amount of allowed inotify watchers per user. We specified a limit of 18192 here instead of just 10000 so that the root user will still have 8192 available inotify watches (the default value). fs.inotify.max_user_watches=18192 Copy Restart your system. Ensure that the new limits have been enforced by running: ulimit -Sn # Should return 10100 cat /proc/sys/fs/inotify/max_user_watches # Should return 18192 Copy Learn more on how to increase open file limits, or on how to increase the inotify watches. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards. Uninstall log forwarding To uninstall log forwarding capabilities, go to your logging.d directory, and remove files with the .yml extension that were originally added during the configuration process. Linux: /etc/newrelic-infra/logging.d/ Windows: C:\\Program Files\\New Relic\\newrelic-infra\\logging.d\\",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 353.44598,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward your <em>logs</em> using the <em>infrastructure</em> agent",
        "sections": "Sending the <em>infrastructure</em> agent&#x27;s <em>logs</em> to <em>New</em> <em>Relic</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " <em>management</em> capabilities to collect, process, explore, query, and alert on your <em>log</em> data. Basic process To forward your <em>logs</em> through our infrastructure <em>monitoring</em> agent: If you haven&#x27;t already, create a <em>New</em> <em>Relic</em> account. It&#x27;s free, forever. Verify the system requirements needed for configuring <em>logs</em>"
      },
      "id": "603e9df164441f6b6f4e8843"
    },
    {
      "sections": [
        "Logs in context of apps and hosts",
        "See the root cause of issues across your platform",
        "Basic process to enable logs in context",
        "API and other options",
        "What's next?"
      ],
      "title": "Logs in context of apps and hosts",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Configure logs in context"
      ],
      "external_id": "7f77d1e4599c8f7b9b2a44bc817f328f11410651",
      "image": "https://docs.newrelic.com/static/49f9b37d2957292090bdbf226eadacea/c1b63/new-relic-logs-in-context-diagram.png",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/configure-logs-context-apm-agents/",
      "published_at": "2022-01-08T02:05:17Z",
      "updated_at": "2022-01-04T06:00:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "When you need to correlate log data with other telemetry data, enable logs in context in New Relic. Logs in context functionality adds metadata that links your logs with related APM data, like errors or distributed traces, or your platform performance data from infrastructure monitoring logs in New Relic One. To see how logs in context and log patterns can help you find the root cause of an issue, watch this short video (approx. 5 minutes): See the root cause of issues across your platform By bringing all of your application and infrastructure data together in a single solution, you can get to the root cause of issues faster. Logs in context help you quickly see meaningful patterns and trends. The following diagram shows the lifecycle of a log message, from enrichment with agent metadata (contextual logging), to formatting and forwarding the log data to New Relic: This diagram illustrates the flow of log messages through New Relic. Don't spend extra time trying to narrow down all your logs from different parts of your platform. Instead, enable logs in context to see the exact log lines you need to identify and resolve a problem. Basic process to enable logs in context Here is an example of logs for your app's UI. You can see logs in context of events for the selected time period, and drill down into detailed data for any of the highlighted attributes. To take advantage of even more capabilities, click Query logs from here to go directly to the Logs UI. The process to enable logs in context is basically the same, regardless of which APM agent you use to monitor your application: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Update to a supported APM agent version for your app, and enable distributed tracing. Configure logs in context for your APM agent or for your infrastructure monitoring agent. View your logs within the context of your apps or infrastructure in New Relic One. The main differences in this procedure are which log appenders you can use to extend and enrich your log data, and how to configure the log appender you select for your APM agent. For detailed information, see the logs-in-context procedures for: C SDK Go Java .NET Node.js PHP Python Ruby Infrastructure monitoring agent API and other options If our logging solutions don't meet your needs, you can use other options to send your log data to New Relic: Logging extensions via agent API calls HTTP endpoint via our Log API Syslog protocols via TCP endpoint (useful for CDNs, hardware devices, or managed services) What's next? After you set up logs in context for APM or infrastructuring monitoring, make the most of your logging data in the New Relic One UI: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards. Here is an example of logs in context for app trace details, visible from the APM UI.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 233.74223,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Logs</em> <em>in</em> context of apps and hosts",
        "sections": "Basic process to <em>enable</em> <em>logs</em> <em>in</em> context",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "When you need to correlate <em>log</em> data with other telemetry data, <em>enable</em> <em>logs</em> in context in <em>New</em> <em>Relic</em>. <em>Logs</em> in context functionality adds metadata that links your <em>logs</em> with related APM data, like errors or distributed traces, or your platform performance data from infrastructure <em>monitoring</em> <em>logs</em> in <em>New</em>"
      },
      "id": "603ea62e196a6749f8a83dc9"
    }
  ],
  "/docs/logs/forward-logs/fluent-bit-plugin-log-forwarding": [
    {
      "sections": [
        "Forward your logs using the infrastructure agent",
        "Basic process",
        "System requirements",
        "Important",
        "Install the infrastructure agent",
        "Configure the infrastructure agent",
        "Log forwarding parameters",
        "Name (required)",
        "Log source (required)",
        "file",
        "systemd",
        "syslog",
        "tcp",
        "winlog",
        "Optional configuration",
        "attributes",
        "attributes automatically inserted by the infrastructure agent",
        "pattern",
        "max_line_kb",
        "fluentbit",
        "Sample configuration file",
        "logging.d/sample.yaml",
        "View your log data",
        "Troubleshooting",
        "No log data",
        "No data appears when tailing a file",
        "No data appears when capturing via a Syslog socket",
        "No data appears using infrastructure agent proxy",
        "Sending the infrastructure agent's logs to New Relic",
        "Caution",
        "Fluent Bit does not start with the infra agent",
        "Runtime error on Windows",
        "Errors when tailing a large amount of log files (Linux)",
        "What's next?",
        "Uninstall log forwarding"
      ],
      "title": "Forward your logs using the infrastructure agent",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "a1deac410f0eedfb819348524a85a73bbb9d9daf",
      "image": "https://docs.newrelic.com/static/5d4352531871b652878b3cb0768e3f23/c1b63/demotron-host-logs-123021.png",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/forward-your-logs-using-infrastructure-agent/",
      "published_at": "2022-01-09T01:46:14Z",
      "updated_at": "2022-01-07T01:41:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can forward your logs to New Relic using our infrastructure monitoring agent. This makes all of your logging data available in one location and provides deeper visibility into both your application and your platform performance data. Forwarding your logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Basic process To forward your logs through our infrastructure monitoring agent: If you haven't already, create a New Relic account. It's free, forever. Verify the system requirements needed for configuring logs. Ensure you have installed the infrastructure agent, version 1.11.4 or higher. Create a logging.yml configuration file in the infrastructure agent's logging.d directory. Configure your log sources and other parameters. Generate some traffic and wait a few minutes, then check your account for data. Explore your log data in the Logs UI and benefit from the log attributes automatically inserted by the infrastructure agent. Here is an example of logs for your host's UI. You can see logs in context of events for the selected time period, and drill down into detailed data for any of the highlighted attributes. To take advantage of even more capabilities, click Query logs from here to go directly to the Logs UI. Here is an example of a host's logs in context related to an event. System requirements To use the log forwarder of the infrastructure agent, make sure you meet the following requirements: Infrastructure agent version 1.11.4 or higher OpenSSL library 1.1.0 or higher is required by Infrastructure agent starting from 1.16.4. Built-in support for ARM64 architecture on Linux systems (in example, AWS Graviton architecture) added in Infrastructure agent 1.26.0. Important The log forwarding feature is not supported with the Docker container for infrastructure monitoring agents. The log forwarding feature is compatible with the following operating systems: Operating system Supported version Amazon Linux Amazon Linux 2 CentOS Version 7 or higher Debian Version 9 (\"Stretch\") or higher Exception: Version 11 is not supported. Red Hat Enterprise Linux (RHEL) Version 7 or higher SUSE Linux Enterprise Server (SLES) Version 12 Ubuntu Versions 16.04.x, 18.04.x and 20.04.x (LTS versions) Windows Windows Server 2012, 2016, 2019, and 2022, and their service packs. Windows 10 Install the infrastructure agent Starting with version 1.11.4, the infrastructure agent can forward logs to New Relic. To install and run the agent, use a package manager (Linux) or the MSI installer (Windows). Important The log forwarding feature is not included when the infrastructure agent is implemented using Linux tarball or Windows ZIP installations. To use the following links, make sure you are logged to your New Relic account. Amazon Linux CentOS Debian RHEL SLES Ubuntu Windows If you don't have a New Relic account yet, or if you prefer to follow the procedure manually, see our tutorial to install the package manager. Configure the infrastructure agent Configuration files describe which log sources are forwarded. Our infrastructure agent uses .yml files to configure logging. You can add as many config files as you want. To add a new configuration file for the log forwarding feature: Navigate to the log forwarder configuration folder: Linux: /etc/newrelic-infra/logging.d/ Windows: C:\\Program Files\\New Relic\\newrelic-infra\\logging.d\\ Create a logging.yml configuration file, and add the parameters you need. The logging.d directory has various .yml.example files you can use as a reference or starting point. The agent automatically processes new configuration files without having to restart the infrastructure monitoring service. The only exception to this is when configuring a custom Fluent Bit configuration. Log forwarding parameters The infrastructure log forwarding .yml config supports the following parameters: Name (required) To start, define a name of the log or logs you want to forward to New Relic. Log source (required) What you use for the log source will depend on where you want to forward your logs from. Available options include: file Path to the log file or files. The agent tracks changes on the log files in a way similar to tail -f shell. Example: logs: - name: example-log file: /var/log/example.log # Path to a single log file - name: example-log-two file: /var/log/example-two.log # Path to another single log file Copy The file parameter can point to a specific log file or multiple files by using wildcards applied to names and extensions; for example, /logs/*.log. You can use wildcards in place of directories in a file path, which can be used to tail files located in different directories. Example: logs: - name: docker-logs file: /var/lib/docker/containers/*/*.log # Path to multiple folders and files Copy Important Use of wildcards may significantly increase the number of file descriptors and inotify watches the Fluent Bit process keeps open, which can interfere with log collection if the host's file descriptor limit is reached. Tailing a large number of files may require you to increase the maximum number of file descriptors and inotify watchers allowed by the operating system. Please refer to Errors when tailing a large amount of log files for more details on how to increase them. systemd Use the systemd parameter to forward log messages that are collected by the journald daemon in Linux environments. This input type requires the agent to run in root mode. Example: logs: - name: systemd-example systemd: cupsd Copy syslog Syslog data source. Parameters: uri: Syslog socket. Format varies depending on the protocol: TCP/UDP network sockets: [tcp/udp]://LISTEN_ADDRESS:PORT Unix domain sockets: unix_[tcp/udp]:// + /socket/path parser: Syslog parser. Default is rfc3164. Use rfc5424 if your messages include fractional seconds. Note: rfc3164 currently does not work on SuSE. unix_permissions: default is 0644 for domain sockets; this limits entries to processes running as root. You can use 0666 to listen for non-root processes, at your own risk. When running the agent in privileged mode, ports and sockets must be available or owned by nri-agent, with 0666 file permissions, so that other processes can write logs to the sockets. logs: # TCP network socket - name: syslog-tcp-test syslog: uri: tcp://0.0.0.0:5140 # Use the tcp://LISTEN_ADDRESS:PORT format parser: rfc5424 # Default syslog parser is rfc3164 # UDP network socket - name: syslog-udp-test syslog: uri: udp://0.0.0.0:6140 # Use the udp://LISTEN_ADDRESS:PORT format max_line_kb: 35 # Unix TCP domain socket - name: syslog-unix-tcp-test syslog: uri: unix_tcp:///var/unix-tcp-socket-test unix_permissions: 0666 # Default is 0644. Change at your own risk # Unix UDP domain socket - name: syslog-unix-udp-test syslog: uri: unix_udp:///var/unix-udp-socket-test parser: rfc5424 Copy tcp Logs retrieved over TCP connections. Parameters: uri: TCP/IP socket to listen for incoming data. The URI format is tcp://LISTEN_ADDRESS:PORT format: format of the data. It can be json or none. separator: If format: none is used, you can define a separator string for splitting records (default: \\n). logs: - name: tcp-simple-test tcp: uri: tcp://0.0.0.0:1234 # Use the tcp://LISTEN_ADDRESS:PORT format format: none # Raw text - this is default for 'tcp' separator: \\t # String for separating raw text entries max_line_kb: 32 - name: tcp-json-test tcp: uri: tcp://0.0.0.0:2345 # Use the tcp://LISTEN_ADDRESS:PORT format format: json Copy winlog Collect events from Windows log channels. Parameters: channel: name of the channel logs will be collected from. collect-eventids: a list of Windows Event IDs to be collected and forwarded to New Relic. Event ID ranges are supported. exclude-eventids: a list of Windows Event IDs to be excluded from collection. Event ID ranges are supported. All events are collected from the specified channel by default. Configure the collect-eventids and exclude-eventids sections to avoid sending unwanted logs to your New Relic account. Add event IDs or ranges to collect-eventids or exclude-eventids to forward or drop specific events. exclude-eventids takes precedence over collect-eventids if the same event ID is present in both sections. Example: logs: # Winlog log ingestion with eventId filters. - name: windows-security winlog: channel: Security collect-eventids: - 4624 - 4265 - 4700-4800 exclude-eventids: - 4735 # entries for the application, system, powershell, and SCOM channels - name: windows-application winlog: channel: Application - name: windows-system winlog: channel: System - name: windows-pshell winlog: channel: Windows Powershell - name: scom winlog: channel: Operations Manager # Entry for Windows Defender Logs - name: windows-defender winlog: channel: Microsoft-Windows-Windows Defender/Operational # Entry for Windows Clustering Logs - name: windows-clustering winlog: channel: Microsoft-Windows-FailoverClustering/Operational # Entry for IIS logs with logtype attribute for automatic parsing - name: iis-log file: C:\\inetpub\\logs\\LogFiles\\w3svc.log attributes: logtype: iis_w3c Copy Optional configuration The following configuration parameters are not required but are still recommended. attributes List of custom attributes specified as key-value pairs that can be used to send additional data with the logs which you can then query. The attributes configuration parameter can be used with any log source. One common use of the attributes configuration parameter is to specify the logtype attribute. This attribute allows leveraging one of the built-in parsing rules supported by New Relic Logs. Example: logs: - name: example-file-attributes file: /var/log/example.log attributes: logtype: nginx region: example-us-02 team: A-team - name: example-tcp-attributes tcp: uri: tcp://0.0.0.0:2345 format: json attributes: logtype: nginx region: example-us-02 team: B-team Copy attributes automatically inserted by the infrastructure agent The infrastructure agent automatically inserts log attributes for your convenience. Some of them are inserted for any log record, while others depend on the configuration parameters you used while setting up the log forwarder. Attribute name Description entity.guids Always inserted. The infrastructure agent inserts the Entity GUID assigned by New Relic to identify the host where it's running. It is available in the entity.guids field. Note: If the captured logs belong to an application instrumented using APM, the entity.guids field contains both the entity GUID of infrastructure, as well as the GUID of APM, separated by a pipe ( | ) delimiter. fb.input Always inserted. The underlying Fluent Bit input plugin type used to capture the logs. Currently its values are tail, systemd, winlog, syslog, and tcp. filePath Inserted when sing the file input type. Absolute file path of the file being monitored. hostname Always inserted. The hostname of the machine/VM/container executing the infrastructure agent. plugin.type Always inserted. Indicates the utility used to capture the logs. In this case, it is the infrastructure agent itself, so this attribute always has the value nri-agent. pattern Regular expression for filtering records. Only supported for the tail, systemd, syslog, and tcp (only with format none) sources. This field works in a way similar to grep -E in Unix systems. For example, for a given file being captured, you can filter for records containing either WARN or ERROR using: - name: only-records-with-warn-and-error file: /var/log/logFile.log pattern: WARN|ERROR Copy No filtering is applied by default. max_line_kb Maximum size of log entries/lines in KB. If log entries exceed the limit, they are skipped. Default is 128. fluentbit External Fluent Bit configuration and parser files. If defined, they are merged with the existing configuration and parser files generated by the infrastructure agent. The infrastructure agent processes the configuration files located in the logging.d directory and will generate a run-time Fluent Bit configuration file that contains the appropriate [INPUT], [FILTER] and [OUTPUT] sections. Optionally, it will also declare an @INCLUDE in case you provided an external Fluent Bit configuration file via the fluentbit option. The runtime file does not define a [SERVICE] section, leaving all default Fluent Bit configuration values. You can still override Fluent Bit's default settings by defining your own [SERVICE] section in your external Fluent Bit configuration file and include it via the fluentbit option. Parameters: config_file: path to an existing Fluent Bit configuration file. Note that any overlapping source results in duplicate messages in New Relic Logs. parsers_file: path to an existing Fluent Bit parsers file. The following parser names are reserved: rfc3164, rfc3164-local and rfc5424. Sample configuration file Here is an example of a logging.d/ configuration file in YAML format. For more configuration examples, see the infrastructure agent repository. logging.d/sample.yaml # Remember to only use spaces for indentation logs: # Example of 'file' source - name: file-with-attributes file: /var/log/test.log # Path to a single file or pattern attributes: # You can use custom attributes to enrich your data logtype: nginx team: The A Team pattern: Error # Regular expression to filter log entries # Example of 'systemd' source (Linux only) - name: systemd-example systemd: cupsd # Examples of 'syslog' source, one per protocol # TCP network socket - name: syslog-tcp-test syslog: uri: tcp://0.0.0.0:5140 # Use the tcp://LISTEN_ADDRESS:PORT format parser: rfc5424 # Default syslog parser is rfc3164 # UDP network socket - name: syslog-udp-test syslog: uri: udp://0.0.0.0:6140 # Use the udp://LISTEN_ADDRESS:PORT format max_line_kb: 35 # Paths for Unix sockets are defined by combining protocol and path: # unix_udp:// + /path/socket - for example, unix_udp:///tmp/socket # Unix TCP domain socket - name: syslog-unix-tcp-test syslog: uri: unix_tcp:///var/unix-tcp-socket-test unix_permissions: 0666 # Default is 0644. Change at your own risk # Unix UDP domain socket - name: syslog-unix-udp-test syslog: uri: unix_udp:///var/unix-udp-socket-test parser: rfc5424 # Examples of 'tcp' source for formats 'none' and 'json' - name: tcp-simple-test tcp: uri: tcp://0.0.0.0:1234 # Use the tcp://LISTEN_ADDRESS:PORT format format: none # Raw text - this is default for 'tcp' separator: \\t # String for separating raw text entries attributes: # You can add custom attributes to any source of logs tcpFormat: none someOtherAttribute: associatedValue max_line_kb: 32 - name: tcp-json-test tcp: uri: tcp://0.0.0.0:2345 # Use the tcp://LISTEN_ADDRESS:PORT format format: json attributes: tcpFormat: json yetAnotherAttribute: 12345 # Example of Fluent Bit configuration import - name: fluentbit-import fluentbit: config_file: /path/to/fluentbit.config parsers_file: /path/to/fluentbit/parsers.conf Copy View your log data If everything is configured correctly and your data is being collected, you should see logs data in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy Troubleshooting If you encounter problems with configuring your log forwarder, try these troubleshooting tips. No log data If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. No data appears when tailing a file The log forwarding feature requires the agent to have permission to read the data sources. When running the infrastructure agent in privileged or non-privileged modes, make sure that the log files you want to forward (and any intermediary directory in its path) are readable by the user running nri-agent. Example: Check file access under Linux Let's check whether the file /var/log/restrictedLogs/logFile.log can be monitored by the nri-agent user. In Linux, you can do a quick check with the namei command: sudo -u nri-agent namei -ml /var/log/restrictedLogs/logFile.log f: /var/log/restrictedLogs/logFile.log drwxr-xr-x root root / drwxr-xr-x root root var drwxrwxr-x root syslog log drwxr--r-- root root restrictedLogs logFile.log - No such file or directory Copy This command failed because the file is not visible to the nri-agent user. By inspecting the previous output, we can detect that the restrictedLogs directory is missing the execution flag for others. To fix this, execute: sudo chmod 755 /var/log/restrictedLogs Copy And then check for file access again: # sudo -u nri-agent namei -ml /var/log/restrictedLogs/logFile.log f: /var/log/restrictedLogs/logFile.log drwxr-xr-x root root / drwxr-xr-x root root var drwxrwxr-x root syslog log drwxr-xr-x root root restrictedLogs -rw-r----- vagrant vagrant logFile.log Copy The file is now visible to the nri-agent user. You must ensure that the file is also readable by the nri-agent user. To check this, use: # sudo -u nri-agent head /var/log/restrictedLogs/logFile.log head: cannot open '/var/log/restrictedLogs/logFile.log' for reading: Permission denied Copy In this example, the file is missing the read rights for the others group (users other than vagrant and the vagrant user group). You could fix this by granting read permissions to others, but the application could change these permissions upon restart. To avoid this, a better approach is to add the nri-agent user to the vagrant user group. No data appears when capturing via a Syslog socket The log forwarding feature requires that the agent has permission to read the data sources. When running the infrastructure agent in privileged or non-privileged modes: If you're using Unix domain socket files, make sure that the nri-agent user can access these files (please refer to the previous section) and that they have read and write permissions (666) so that other users than nri-agent can write to them. If you're using IP sockets, ensure that the port that you are using is not a system reserved one (like port 80, for example). If no data appears after you enable log management, follow standard log management troubleshooting procedures. No data appears using infrastructure agent proxy As explained in the infrastructure agent configuration guidelines, the proxy parameter must use either HTTP or HTTPS and be in the form https://user:password@hostname:port. The agent can parse the parameter without the HTTP or HTTPS, but the log forwarder cannot. You will see an error like the following in the agent verbose logs: [ERROR] building HTTP transport: parse \\\"hostname:port\\\": first path segment in URL cannot contain colon Copy To solve this problem, check your newrelic-infra.yml file, and ensure the proxy parameter adheres to this form. If you're using caBundleFile or caBundleDir in order to specify any certificate, we recommend to follow the below rules for each OS: Linux For HTTP proxies you don't need to setup any certificates. The plugin loads the system certificates and New Relic sends logs into the logging endpoint. However, you can specify the proxy self-signed certificate (PEM file) using either the caBundleFile or caBundleDir parameters. Windows For HTTP proxies you don't need to setup any certificates. The plugin loads the system certificates. For HTTPS, you can configure it in one of the following ways: Import the proxy certificate to the system pool (Recommended) Import the proxy self-signed certificate (PEM file) by using the MMC tool. Refer to this link, and in Step 2 ensure to import it in your Trusted Root Certification Authorities, instead of in the Intermediate Certification Authorities. Using the caBundleFile and caBundleDir parameters On Windows, we cannot load both the certificates from the system certificate pool and the ones specified with the caBundleFile caBundleDir parameters. So, if you are using caBundleFile or caBundleDir, ensure that the following certificates are placed in the same PEM file (when using caBundleFile) or in the same directory (when using caBundleDir): The Proxy certificate (because it's an HTTPS proxy). The Logging Endpoint certificate (eg. https://log-api.newrelic.com/log/v1). The Infrastructure Agent certificate (eg. https://infra-api.newrelic.com). You can check the certificates by running: # openssl s_client -connect log-api.newrelic.com:443 -servername log-api.newrelic.com Copy Sending the infrastructure agent's logs to New Relic You can configure the infrastructure agent to send its own logs to New Relic. This is useful for troubleshooting issues with log forwarding, the agent, or when contacting support. To forward the infrastructure agent logs to New Relic: Edit your newrelic-infra.yml file. Enable agent logging in troubleshooting mode by adding verbose: 3. On Windows and systems that don't use systemd or where journald is inaccessible, verbose:3 causes the agent to write the logs on the disk. Revert to verbose:0 to prevent this. (Recommended): Enable agent logging in JSON format to log_format: json. Restart the agent to load the new settings. This configuration sets up the agent in troubleshooting mode, but the log forwarder (based on Fluent Bit) will continue in a non-verbose mode. Sometimes you can have issues with the log forwarder itself. For example, there may be problems accessing a specific channel when shipping Windows log events or when accessing a particular log file. In these situations, you can also enable the verbose mode for the log forwarder: Set verbose to a value other than 0. Add the configuration option: trace: [\"log.fw\"]. Caution Check whether you are using the [fluentbit] option. When setting verbose: 3 and trace: [\"log.fw\"], ensure that you don't define any [OUTPUT] section pointing to stdout in an external Fluent Bit configuration file, Fluent Bit does not start with the infra agent Important Fluent Bit's tail plugin does not support network drives. For Linux versions prior to 2016, you may need to update the OpenSSL library to 1.1.0 (or higher). To check if you have this problem: See if infra-agent has started Fluent Bit by running: ps -aux | grep td-agent-bit Copy If it isn't running go to /var/db/newrelic-infra/newrelic-integrations/logging and run: ./fluent-bit -i systemd -o stdout Copy If you get the following error, update OpenSSL to 1.1.0 or higher: error while loading shared libraries: libssl.so.1.1: cannot open shared object file: No such file or directory Copy Runtime error on Windows One of the following error messages may appear when enabling log forwarding on Windows: The code execution cannot proceed because VCRUNTIME140.dll was not found. Copy OR error=\"exit status 3221225781\" process=log-forwarder Copy This is caused by a missing DLL. To solve the issue, install the Microsoft Visual C++ Redistributable as applicable: x64 x86 Errors when tailing a large amount of log files (Linux) It's common to face either one of the following error messages when attempting to tail a large amount of files: Too many open files The user limit on the total number of inotify watches was reached or the kernel failed to allocate a needed resource The operating system defines a maximum amount of allocatable file descriptors (typically 1024 by default), and a maximum amount of allocatable inotify watches (typically 8192 by default). Any process attempting to go above these limits will fail, returning one of the errors above. The underlying technology we use to forward logs, Fluent Bit, opens one file descriptor and sets an inotify watch for each file you configure to be forwarded. Moreover, at the time of writing this section, Fluent Bit uses an extra set of 32 file descriptors for its normal operation, with another extra file descriptor when it shuts down. Therefore, to capture a large amount of files you need to ensure that both the file descriptor and inotify watch limits are slightly greater than the amount of log files you wish to tail. The following instructions summarize how to increase these limits if you want to tail 10,000 log files. Also, it assumes the infrastructure agent is installed in root running mode, and therefore must be run using the root user. Check which is the current hard limit for the amount of file descriptors per process. Typically, this limit should be quite high and should not need to be modified. ulimit -Hn Copy Add the following line to /etc/security/limits.conf. We specified a limit of 10100 here instead of just 10000 to allow Fluent Bit to allocate the extra file descriptors it may need to work. root soft nofile 10100 # replace root by nri-agent for non-root (privileged and unprivileged) installations Copy Add the following line to /etc/pam.d/common-session so that the previous limit is applied upon restart: session required pam_limits.so Copy Add the following line to /etc/sysctl.conf to increase the amount of allowed inotify watchers per user. We specified a limit of 18192 here instead of just 10000 so that the root user will still have 8192 available inotify watches (the default value). fs.inotify.max_user_watches=18192 Copy Restart your system. Ensure that the new limits have been enforced by running: ulimit -Sn # Should return 10100 cat /proc/sys/fs/inotify/max_user_watches # Should return 18192 Copy Learn more on how to increase open file limits, or on how to increase the inotify watches. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards. Uninstall log forwarding To uninstall log forwarding capabilities, go to your logging.d directory, and remove files with the .yml extension that were originally added during the configuration process. Linux: /etc/newrelic-infra/logging.d/ Windows: C:\\Program Files\\New Relic\\newrelic-infra\\logging.d\\",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 353.44574,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward your <em>logs</em> using the <em>infrastructure</em> agent",
        "sections": "Sending the <em>infrastructure</em> agent&#x27;s <em>logs</em> to <em>New</em> <em>Relic</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " <em>management</em> capabilities to collect, process, explore, query, and alert on your <em>log</em> data. Basic process To forward your <em>logs</em> through our infrastructure <em>monitoring</em> agent: If you haven&#x27;t already, create a <em>New</em> <em>Relic</em> account. It&#x27;s free, forever. Verify the system requirements needed for configuring <em>logs</em>"
      },
      "id": "603e9df164441f6b6f4e8843"
    },
    {
      "sections": [
        "Forward your logs to New Relic",
        "How it works",
        "Get started with log forwarders",
        "Log forwarding options",
        "TCP endpoint",
        "Log API",
        "What's next?"
      ],
      "title": "Forward your logs to New Relic",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "b6e203ddc367d5a2b5e002916e49d34f4ba17a87",
      "image": "https://docs.newrelic.com/static/c3d5443b84a1e2b26a4767ce35fa58f3/e5166/new-relic-logs-in-context-diagram.jpg",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/enable-log-management-new-relic/",
      "published_at": "2022-01-08T02:40:08Z",
      "updated_at": "2022-01-04T07:02:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our log management capabilities help you to collect, process, explore, query, and alert on your log data. To get your logs into New Relic, you can: Use your existing log forwarding solution to collect your logs and extend the metadata that is forwarded to New Relic. Use our infrastructure agent as a lightweight data collector, without having to use additional software. Use our Log API to forward your logs via HTTP. Use syslog protocols to forward your logs via a TCP endpoint. How it works The following diagram shows the lifecycle of a log message for an app, from enrichment with APM agent metadata (contextual logging), to formatting and forwarding the log data to New Relic. This diagram illustrates the flow of log messages through New Relic. Standard log formatters transform log events into meaningful output (such as text files) that can be used by downstream people and processes. The NewRelicFormatter transforms log events into the JSON format expected by New Relic. These files contain log information and extended metadata. When you configure your log forwarder (our infrastructure monitoring agent, Fluentd, Logstash, etc.), you can also extend and enrich your log data. By configuring logs in context, the log enricher links the formatted log data with additional transaction information from your application or host. Now your log files are enriched with enhanced metadata and contextual logging data. Your log forwarder sends the files to our logging endpoint for processing. From there you can use our log management capabilities to view, query, set up alerts, and more in New Relic. Here is an example of logs for your host's UI. You can see logs in context of events for the selected time period, and drill down into detailed data for any of the highlighted attributes. To take advantage of even more capabilities, click Query logs from here to go directly to the Logs UI. Here is an example of a host's logs in context related to an event. Get started with log forwarders To forward your logs to New Relic with enriched metadata: If you don't have one already, create a New Relic account. It's free, forever. Have your New Relic account's license key. Install a compatible log forwarder. Ensure that outbound connectivity on TCP port 443 is allowed to the CIDR range that matches your region. Use the DNS name log-api.newrelic.com or log-api.eu.newrelic.com for configuration. For apps monitored by a New Relic APM agent, configure logs in context. Log forwarding options Use any of these solutions to forward your logs to New Relic. Recommended: Infrastructure monitoring agent Amazon: AWS CloudWatch plugin AWS FireLens plugin AWS Kinesis Firehose AWS Lambda for sending logs from S3 Microsoft: Azure ARM template Other log forwarding plugins: Fluent Bit plugin Fluentd plugin Google Cloud Platform Pub/Sub Heroku log streaming Kubernetes plugin Logstash plugin Vector plugin TCP endpoint In some situations you may not have log forwarders; for example, with CDNs, hardware devices, or managed services. You can use syslog protocols such as rsyslog and syslog-ng, and forward your logs to New Relic via a TCP endpoint. Log API If you prefer to connect to New Relic without installing a plugin, we offer an HTTP input integration. This option sends your monitored log data directly to New Relic via the Log API. What's next? After you enable your log forwarder, make the most of your data in New Relic with our log management capabilities: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data if you are forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards. For example, to query and manage your data partition rules, see our NerdGraph tutorial.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 314.52045,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward your <em>logs</em> to <em>New</em> <em>Relic</em>",
        "sections": "Forward your <em>logs</em> to <em>New</em> <em>Relic</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " directly to <em>New</em> <em>Relic</em> via the <em>Log</em> API. What&#x27;s next? After you <em>enable</em> your <em>log</em> forwarder, make the most of your data in <em>New</em> <em>Relic</em> with our <em>log</em> <em>management</em> capabilities: Explore the logging data across your platform with our <em>Logs</em> UI. See your <em>logs</em> in context of your app&#x27;s performance in the APM UI"
      },
      "id": "61571e0e28ccbcbc52f21431"
    },
    {
      "sections": [
        "Logs in context of apps and hosts",
        "See the root cause of issues across your platform",
        "Basic process to enable logs in context",
        "API and other options",
        "What's next?"
      ],
      "title": "Logs in context of apps and hosts",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Configure logs in context"
      ],
      "external_id": "7f77d1e4599c8f7b9b2a44bc817f328f11410651",
      "image": "https://docs.newrelic.com/static/49f9b37d2957292090bdbf226eadacea/c1b63/new-relic-logs-in-context-diagram.png",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/configure-logs-context-apm-agents/",
      "published_at": "2022-01-08T02:05:17Z",
      "updated_at": "2022-01-04T06:00:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "When you need to correlate log data with other telemetry data, enable logs in context in New Relic. Logs in context functionality adds metadata that links your logs with related APM data, like errors or distributed traces, or your platform performance data from infrastructure monitoring logs in New Relic One. To see how logs in context and log patterns can help you find the root cause of an issue, watch this short video (approx. 5 minutes): See the root cause of issues across your platform By bringing all of your application and infrastructure data together in a single solution, you can get to the root cause of issues faster. Logs in context help you quickly see meaningful patterns and trends. The following diagram shows the lifecycle of a log message, from enrichment with agent metadata (contextual logging), to formatting and forwarding the log data to New Relic: This diagram illustrates the flow of log messages through New Relic. Don't spend extra time trying to narrow down all your logs from different parts of your platform. Instead, enable logs in context to see the exact log lines you need to identify and resolve a problem. Basic process to enable logs in context Here is an example of logs for your app's UI. You can see logs in context of events for the selected time period, and drill down into detailed data for any of the highlighted attributes. To take advantage of even more capabilities, click Query logs from here to go directly to the Logs UI. The process to enable logs in context is basically the same, regardless of which APM agent you use to monitor your application: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Update to a supported APM agent version for your app, and enable distributed tracing. Configure logs in context for your APM agent or for your infrastructure monitoring agent. View your logs within the context of your apps or infrastructure in New Relic One. The main differences in this procedure are which log appenders you can use to extend and enrich your log data, and how to configure the log appender you select for your APM agent. For detailed information, see the logs-in-context procedures for: C SDK Go Java .NET Node.js PHP Python Ruby Infrastructure monitoring agent API and other options If our logging solutions don't meet your needs, you can use other options to send your log data to New Relic: Logging extensions via agent API calls HTTP endpoint via our Log API Syslog protocols via TCP endpoint (useful for CDNs, hardware devices, or managed services) What's next? After you set up logs in context for APM or infrastructuring monitoring, make the most of your logging data in the New Relic One UI: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards. Here is an example of logs in context for app trace details, visible from the APM UI.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 233.74214,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Logs</em> <em>in</em> context of apps and hosts",
        "sections": "Basic process to <em>enable</em> <em>logs</em> <em>in</em> context",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "When you need to correlate <em>log</em> data with other telemetry data, <em>enable</em> <em>logs</em> in context in <em>New</em> <em>Relic</em>. <em>Logs</em> in context functionality adds metadata that links your <em>logs</em> with related APM data, like errors or distributed traces, or your platform performance data from infrastructure <em>monitoring</em> <em>logs</em> in <em>New</em>"
      },
      "id": "603ea62e196a6749f8a83dc9"
    }
  ],
  "/docs/logs/forward-logs/fluentd-plugin-log-forwarding": [
    {
      "sections": [
        "Fluent Bit plugin for log forwarding",
        "Basic process",
        "Install the Fluent Bit plugin",
        "Tip",
        "Configure the Fluent Bit plugin",
        "Important",
        "Test the Fluent Bit plugin",
        "Associate logs with entities",
        "Optional: Configure plugin attributes",
        "View log data",
        "What's next?"
      ],
      "title": "Fluent Bit plugin for log forwarding",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "dd686f074bb80908ae9de19e543a3bae7d87a466",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/fluent-bit-plugin-log-forwarding/",
      "published_at": "2022-01-08T02:38:07Z",
      "updated_at": "2022-01-08T02:38:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your log data in New Relic. Forwarding your Fluent Bit logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Basic process We have published a container with the plugin installed. It serves as a base image to be used by our Kubernetes integration. We recommend you use this base image and layer your own custom configuration files. To forward your logs from Fluent Bit to New Relic: Make sure you have: A New Relic license key Fluent Bit 1.0 or higher (recommended), although v0.12 or higher is supported Fluent Bit Windows install directions can be found here Fluent Bit Linux install directions can be found here Install the Fluent Bit plugin. Configure the Fluent Bit plugin. Test the Fluent Bit plugin. Generate some traffic and wait a few minutes, then check your account for data. Install the Fluent Bit plugin To install the Fluent Bit plugin: Navigate to New Relic's Fluent Bit plugin repository on GitHub. From the repository page, clone or download the repository. Run the following command to build your plugin: cd newrelic-fluent-bit-output && make all Copy Store out_newrelic.so or out_newrelic_winXX.dll at a location that can be accessed by the fluent-bit daemon. Tip If you'd rather not compile the plugin yourself, you can download pre-compiled versions from our GitHub repository's releases page. Configure the Fluent Bit plugin Fluent Bit needs to know the location of the New Relic plugin and the New Relic license key to output data to New Relic. To configure your Fluent Bit plugin: Important Pay attention to white space when editing your config files. Be sure to use four spaces to indent and one space between keys and values. Locate or create a plugins.conf file in your plugins directory. In the plugins.conf file, add a reference to out_newrelic.so, adjacent to your fluent-bit.conf file: [PLUGINS] Path /PATH/TO/newrelic-fluent-bit-output/out_newrelic.so Copy In the fluent-bit.conf file, add the following line under the service block: [SERVICE] # This is the main configuration block for fluent bit. # Ensure the follow line exists somewhere in the SERVICE block Plugins_File plugins.conf Copy At the bottom of the fluent-bit.conf file, add the following to set up the input and output filters. Replace the placeholder text with your New Relic license key: [INPUT] Name tail Path /PATH/TO/YOUR/LOG/FILE [OUTPUT] Name newrelic Match * licenseKey YOUR_LICENSE_KEY # Optional maxBufferSize 256000 maxRecords 1024 Copy Restart your Fluent Bit instance with the following command: fluent-bit -c /PATH/TO/fluent-bit.conf Copy Test the Fluent Bit plugin To test if your Fluent Bit plugin is receiving input from a log file: Run the following command to append a test log message to your log file: echo \"test message\" >> /PATH/TO/YOUR/LOG/FILE Copy Search the New Relic Logs UI for test message. Associate logs with entities To associate a log line with an entity, such as an infrastructure host, add a FILTER block: [FILTER] Name modify Match * # Or specify a match Add entity.guids <Your Entity GUID ID> # Optional Add hostname <Your hostname> # Optional Copy For more options, see the Fluent Bit modify filter documentation and our documentation to forward your logs using the infrastructure agent. Optional: Configure plugin attributes Once you have installed and configured the Fluent Bit plugin, you can use the following attributes to configure how the plugin sends data to New Relic: Key Description licenseKey The New Relic license key. Use either licenseKey (recommended) or apiKey, not both. Default: none maxBufferSize The maximum size the payloads sent, in bytes. Default: 256000 maxRecords The maximum number of records to send at a time. Default: 1024 apiKey Deprecated. Takes a New Relic Insights insert key, but using the licenseKey attribute is preferred. Use either licenseKey or apiKey, not both. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 374.04865,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Fluent Bit plugin for <em>log</em> forwarding",
        "sections": "Associate <em>logs</em> with entities",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "If your <em>log</em> data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your <em>log</em> data in <em>New</em> <em>Relic</em>. Forwarding your Fluent Bit <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert"
      },
      "id": "603ea7ec28ccbce542eba791"
    },
    {
      "sections": [
        "Forward your logs using the infrastructure agent",
        "Basic process",
        "System requirements",
        "Important",
        "Install the infrastructure agent",
        "Configure the infrastructure agent",
        "Log forwarding parameters",
        "Name (required)",
        "Log source (required)",
        "file",
        "systemd",
        "syslog",
        "tcp",
        "winlog",
        "Optional configuration",
        "attributes",
        "attributes automatically inserted by the infrastructure agent",
        "pattern",
        "max_line_kb",
        "fluentbit",
        "Sample configuration file",
        "logging.d/sample.yaml",
        "View your log data",
        "Troubleshooting",
        "No log data",
        "No data appears when tailing a file",
        "No data appears when capturing via a Syslog socket",
        "No data appears using infrastructure agent proxy",
        "Sending the infrastructure agent's logs to New Relic",
        "Caution",
        "Fluent Bit does not start with the infra agent",
        "Runtime error on Windows",
        "Errors when tailing a large amount of log files (Linux)",
        "What's next?",
        "Uninstall log forwarding"
      ],
      "title": "Forward your logs using the infrastructure agent",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "a1deac410f0eedfb819348524a85a73bbb9d9daf",
      "image": "https://docs.newrelic.com/static/5d4352531871b652878b3cb0768e3f23/c1b63/demotron-host-logs-123021.png",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/forward-your-logs-using-infrastructure-agent/",
      "published_at": "2022-01-09T01:46:14Z",
      "updated_at": "2022-01-07T01:41:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can forward your logs to New Relic using our infrastructure monitoring agent. This makes all of your logging data available in one location and provides deeper visibility into both your application and your platform performance data. Forwarding your logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Basic process To forward your logs through our infrastructure monitoring agent: If you haven't already, create a New Relic account. It's free, forever. Verify the system requirements needed for configuring logs. Ensure you have installed the infrastructure agent, version 1.11.4 or higher. Create a logging.yml configuration file in the infrastructure agent's logging.d directory. Configure your log sources and other parameters. Generate some traffic and wait a few minutes, then check your account for data. Explore your log data in the Logs UI and benefit from the log attributes automatically inserted by the infrastructure agent. Here is an example of logs for your host's UI. You can see logs in context of events for the selected time period, and drill down into detailed data for any of the highlighted attributes. To take advantage of even more capabilities, click Query logs from here to go directly to the Logs UI. Here is an example of a host's logs in context related to an event. System requirements To use the log forwarder of the infrastructure agent, make sure you meet the following requirements: Infrastructure agent version 1.11.4 or higher OpenSSL library 1.1.0 or higher is required by Infrastructure agent starting from 1.16.4. Built-in support for ARM64 architecture on Linux systems (in example, AWS Graviton architecture) added in Infrastructure agent 1.26.0. Important The log forwarding feature is not supported with the Docker container for infrastructure monitoring agents. The log forwarding feature is compatible with the following operating systems: Operating system Supported version Amazon Linux Amazon Linux 2 CentOS Version 7 or higher Debian Version 9 (\"Stretch\") or higher Exception: Version 11 is not supported. Red Hat Enterprise Linux (RHEL) Version 7 or higher SUSE Linux Enterprise Server (SLES) Version 12 Ubuntu Versions 16.04.x, 18.04.x and 20.04.x (LTS versions) Windows Windows Server 2012, 2016, 2019, and 2022, and their service packs. Windows 10 Install the infrastructure agent Starting with version 1.11.4, the infrastructure agent can forward logs to New Relic. To install and run the agent, use a package manager (Linux) or the MSI installer (Windows). Important The log forwarding feature is not included when the infrastructure agent is implemented using Linux tarball or Windows ZIP installations. To use the following links, make sure you are logged to your New Relic account. Amazon Linux CentOS Debian RHEL SLES Ubuntu Windows If you don't have a New Relic account yet, or if you prefer to follow the procedure manually, see our tutorial to install the package manager. Configure the infrastructure agent Configuration files describe which log sources are forwarded. Our infrastructure agent uses .yml files to configure logging. You can add as many config files as you want. To add a new configuration file for the log forwarding feature: Navigate to the log forwarder configuration folder: Linux: /etc/newrelic-infra/logging.d/ Windows: C:\\Program Files\\New Relic\\newrelic-infra\\logging.d\\ Create a logging.yml configuration file, and add the parameters you need. The logging.d directory has various .yml.example files you can use as a reference or starting point. The agent automatically processes new configuration files without having to restart the infrastructure monitoring service. The only exception to this is when configuring a custom Fluent Bit configuration. Log forwarding parameters The infrastructure log forwarding .yml config supports the following parameters: Name (required) To start, define a name of the log or logs you want to forward to New Relic. Log source (required) What you use for the log source will depend on where you want to forward your logs from. Available options include: file Path to the log file or files. The agent tracks changes on the log files in a way similar to tail -f shell. Example: logs: - name: example-log file: /var/log/example.log # Path to a single log file - name: example-log-two file: /var/log/example-two.log # Path to another single log file Copy The file parameter can point to a specific log file or multiple files by using wildcards applied to names and extensions; for example, /logs/*.log. You can use wildcards in place of directories in a file path, which can be used to tail files located in different directories. Example: logs: - name: docker-logs file: /var/lib/docker/containers/*/*.log # Path to multiple folders and files Copy Important Use of wildcards may significantly increase the number of file descriptors and inotify watches the Fluent Bit process keeps open, which can interfere with log collection if the host's file descriptor limit is reached. Tailing a large number of files may require you to increase the maximum number of file descriptors and inotify watchers allowed by the operating system. Please refer to Errors when tailing a large amount of log files for more details on how to increase them. systemd Use the systemd parameter to forward log messages that are collected by the journald daemon in Linux environments. This input type requires the agent to run in root mode. Example: logs: - name: systemd-example systemd: cupsd Copy syslog Syslog data source. Parameters: uri: Syslog socket. Format varies depending on the protocol: TCP/UDP network sockets: [tcp/udp]://LISTEN_ADDRESS:PORT Unix domain sockets: unix_[tcp/udp]:// + /socket/path parser: Syslog parser. Default is rfc3164. Use rfc5424 if your messages include fractional seconds. Note: rfc3164 currently does not work on SuSE. unix_permissions: default is 0644 for domain sockets; this limits entries to processes running as root. You can use 0666 to listen for non-root processes, at your own risk. When running the agent in privileged mode, ports and sockets must be available or owned by nri-agent, with 0666 file permissions, so that other processes can write logs to the sockets. logs: # TCP network socket - name: syslog-tcp-test syslog: uri: tcp://0.0.0.0:5140 # Use the tcp://LISTEN_ADDRESS:PORT format parser: rfc5424 # Default syslog parser is rfc3164 # UDP network socket - name: syslog-udp-test syslog: uri: udp://0.0.0.0:6140 # Use the udp://LISTEN_ADDRESS:PORT format max_line_kb: 35 # Unix TCP domain socket - name: syslog-unix-tcp-test syslog: uri: unix_tcp:///var/unix-tcp-socket-test unix_permissions: 0666 # Default is 0644. Change at your own risk # Unix UDP domain socket - name: syslog-unix-udp-test syslog: uri: unix_udp:///var/unix-udp-socket-test parser: rfc5424 Copy tcp Logs retrieved over TCP connections. Parameters: uri: TCP/IP socket to listen for incoming data. The URI format is tcp://LISTEN_ADDRESS:PORT format: format of the data. It can be json or none. separator: If format: none is used, you can define a separator string for splitting records (default: \\n). logs: - name: tcp-simple-test tcp: uri: tcp://0.0.0.0:1234 # Use the tcp://LISTEN_ADDRESS:PORT format format: none # Raw text - this is default for 'tcp' separator: \\t # String for separating raw text entries max_line_kb: 32 - name: tcp-json-test tcp: uri: tcp://0.0.0.0:2345 # Use the tcp://LISTEN_ADDRESS:PORT format format: json Copy winlog Collect events from Windows log channels. Parameters: channel: name of the channel logs will be collected from. collect-eventids: a list of Windows Event IDs to be collected and forwarded to New Relic. Event ID ranges are supported. exclude-eventids: a list of Windows Event IDs to be excluded from collection. Event ID ranges are supported. All events are collected from the specified channel by default. Configure the collect-eventids and exclude-eventids sections to avoid sending unwanted logs to your New Relic account. Add event IDs or ranges to collect-eventids or exclude-eventids to forward or drop specific events. exclude-eventids takes precedence over collect-eventids if the same event ID is present in both sections. Example: logs: # Winlog log ingestion with eventId filters. - name: windows-security winlog: channel: Security collect-eventids: - 4624 - 4265 - 4700-4800 exclude-eventids: - 4735 # entries for the application, system, powershell, and SCOM channels - name: windows-application winlog: channel: Application - name: windows-system winlog: channel: System - name: windows-pshell winlog: channel: Windows Powershell - name: scom winlog: channel: Operations Manager # Entry for Windows Defender Logs - name: windows-defender winlog: channel: Microsoft-Windows-Windows Defender/Operational # Entry for Windows Clustering Logs - name: windows-clustering winlog: channel: Microsoft-Windows-FailoverClustering/Operational # Entry for IIS logs with logtype attribute for automatic parsing - name: iis-log file: C:\\inetpub\\logs\\LogFiles\\w3svc.log attributes: logtype: iis_w3c Copy Optional configuration The following configuration parameters are not required but are still recommended. attributes List of custom attributes specified as key-value pairs that can be used to send additional data with the logs which you can then query. The attributes configuration parameter can be used with any log source. One common use of the attributes configuration parameter is to specify the logtype attribute. This attribute allows leveraging one of the built-in parsing rules supported by New Relic Logs. Example: logs: - name: example-file-attributes file: /var/log/example.log attributes: logtype: nginx region: example-us-02 team: A-team - name: example-tcp-attributes tcp: uri: tcp://0.0.0.0:2345 format: json attributes: logtype: nginx region: example-us-02 team: B-team Copy attributes automatically inserted by the infrastructure agent The infrastructure agent automatically inserts log attributes for your convenience. Some of them are inserted for any log record, while others depend on the configuration parameters you used while setting up the log forwarder. Attribute name Description entity.guids Always inserted. The infrastructure agent inserts the Entity GUID assigned by New Relic to identify the host where it's running. It is available in the entity.guids field. Note: If the captured logs belong to an application instrumented using APM, the entity.guids field contains both the entity GUID of infrastructure, as well as the GUID of APM, separated by a pipe ( | ) delimiter. fb.input Always inserted. The underlying Fluent Bit input plugin type used to capture the logs. Currently its values are tail, systemd, winlog, syslog, and tcp. filePath Inserted when sing the file input type. Absolute file path of the file being monitored. hostname Always inserted. The hostname of the machine/VM/container executing the infrastructure agent. plugin.type Always inserted. Indicates the utility used to capture the logs. In this case, it is the infrastructure agent itself, so this attribute always has the value nri-agent. pattern Regular expression for filtering records. Only supported for the tail, systemd, syslog, and tcp (only with format none) sources. This field works in a way similar to grep -E in Unix systems. For example, for a given file being captured, you can filter for records containing either WARN or ERROR using: - name: only-records-with-warn-and-error file: /var/log/logFile.log pattern: WARN|ERROR Copy No filtering is applied by default. max_line_kb Maximum size of log entries/lines in KB. If log entries exceed the limit, they are skipped. Default is 128. fluentbit External Fluent Bit configuration and parser files. If defined, they are merged with the existing configuration and parser files generated by the infrastructure agent. The infrastructure agent processes the configuration files located in the logging.d directory and will generate a run-time Fluent Bit configuration file that contains the appropriate [INPUT], [FILTER] and [OUTPUT] sections. Optionally, it will also declare an @INCLUDE in case you provided an external Fluent Bit configuration file via the fluentbit option. The runtime file does not define a [SERVICE] section, leaving all default Fluent Bit configuration values. You can still override Fluent Bit's default settings by defining your own [SERVICE] section in your external Fluent Bit configuration file and include it via the fluentbit option. Parameters: config_file: path to an existing Fluent Bit configuration file. Note that any overlapping source results in duplicate messages in New Relic Logs. parsers_file: path to an existing Fluent Bit parsers file. The following parser names are reserved: rfc3164, rfc3164-local and rfc5424. Sample configuration file Here is an example of a logging.d/ configuration file in YAML format. For more configuration examples, see the infrastructure agent repository. logging.d/sample.yaml # Remember to only use spaces for indentation logs: # Example of 'file' source - name: file-with-attributes file: /var/log/test.log # Path to a single file or pattern attributes: # You can use custom attributes to enrich your data logtype: nginx team: The A Team pattern: Error # Regular expression to filter log entries # Example of 'systemd' source (Linux only) - name: systemd-example systemd: cupsd # Examples of 'syslog' source, one per protocol # TCP network socket - name: syslog-tcp-test syslog: uri: tcp://0.0.0.0:5140 # Use the tcp://LISTEN_ADDRESS:PORT format parser: rfc5424 # Default syslog parser is rfc3164 # UDP network socket - name: syslog-udp-test syslog: uri: udp://0.0.0.0:6140 # Use the udp://LISTEN_ADDRESS:PORT format max_line_kb: 35 # Paths for Unix sockets are defined by combining protocol and path: # unix_udp:// + /path/socket - for example, unix_udp:///tmp/socket # Unix TCP domain socket - name: syslog-unix-tcp-test syslog: uri: unix_tcp:///var/unix-tcp-socket-test unix_permissions: 0666 # Default is 0644. Change at your own risk # Unix UDP domain socket - name: syslog-unix-udp-test syslog: uri: unix_udp:///var/unix-udp-socket-test parser: rfc5424 # Examples of 'tcp' source for formats 'none' and 'json' - name: tcp-simple-test tcp: uri: tcp://0.0.0.0:1234 # Use the tcp://LISTEN_ADDRESS:PORT format format: none # Raw text - this is default for 'tcp' separator: \\t # String for separating raw text entries attributes: # You can add custom attributes to any source of logs tcpFormat: none someOtherAttribute: associatedValue max_line_kb: 32 - name: tcp-json-test tcp: uri: tcp://0.0.0.0:2345 # Use the tcp://LISTEN_ADDRESS:PORT format format: json attributes: tcpFormat: json yetAnotherAttribute: 12345 # Example of Fluent Bit configuration import - name: fluentbit-import fluentbit: config_file: /path/to/fluentbit.config parsers_file: /path/to/fluentbit/parsers.conf Copy View your log data If everything is configured correctly and your data is being collected, you should see logs data in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy Troubleshooting If you encounter problems with configuring your log forwarder, try these troubleshooting tips. No log data If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. No data appears when tailing a file The log forwarding feature requires the agent to have permission to read the data sources. When running the infrastructure agent in privileged or non-privileged modes, make sure that the log files you want to forward (and any intermediary directory in its path) are readable by the user running nri-agent. Example: Check file access under Linux Let's check whether the file /var/log/restrictedLogs/logFile.log can be monitored by the nri-agent user. In Linux, you can do a quick check with the namei command: sudo -u nri-agent namei -ml /var/log/restrictedLogs/logFile.log f: /var/log/restrictedLogs/logFile.log drwxr-xr-x root root / drwxr-xr-x root root var drwxrwxr-x root syslog log drwxr--r-- root root restrictedLogs logFile.log - No such file or directory Copy This command failed because the file is not visible to the nri-agent user. By inspecting the previous output, we can detect that the restrictedLogs directory is missing the execution flag for others. To fix this, execute: sudo chmod 755 /var/log/restrictedLogs Copy And then check for file access again: # sudo -u nri-agent namei -ml /var/log/restrictedLogs/logFile.log f: /var/log/restrictedLogs/logFile.log drwxr-xr-x root root / drwxr-xr-x root root var drwxrwxr-x root syslog log drwxr-xr-x root root restrictedLogs -rw-r----- vagrant vagrant logFile.log Copy The file is now visible to the nri-agent user. You must ensure that the file is also readable by the nri-agent user. To check this, use: # sudo -u nri-agent head /var/log/restrictedLogs/logFile.log head: cannot open '/var/log/restrictedLogs/logFile.log' for reading: Permission denied Copy In this example, the file is missing the read rights for the others group (users other than vagrant and the vagrant user group). You could fix this by granting read permissions to others, but the application could change these permissions upon restart. To avoid this, a better approach is to add the nri-agent user to the vagrant user group. No data appears when capturing via a Syslog socket The log forwarding feature requires that the agent has permission to read the data sources. When running the infrastructure agent in privileged or non-privileged modes: If you're using Unix domain socket files, make sure that the nri-agent user can access these files (please refer to the previous section) and that they have read and write permissions (666) so that other users than nri-agent can write to them. If you're using IP sockets, ensure that the port that you are using is not a system reserved one (like port 80, for example). If no data appears after you enable log management, follow standard log management troubleshooting procedures. No data appears using infrastructure agent proxy As explained in the infrastructure agent configuration guidelines, the proxy parameter must use either HTTP or HTTPS and be in the form https://user:password@hostname:port. The agent can parse the parameter without the HTTP or HTTPS, but the log forwarder cannot. You will see an error like the following in the agent verbose logs: [ERROR] building HTTP transport: parse \\\"hostname:port\\\": first path segment in URL cannot contain colon Copy To solve this problem, check your newrelic-infra.yml file, and ensure the proxy parameter adheres to this form. If you're using caBundleFile or caBundleDir in order to specify any certificate, we recommend to follow the below rules for each OS: Linux For HTTP proxies you don't need to setup any certificates. The plugin loads the system certificates and New Relic sends logs into the logging endpoint. However, you can specify the proxy self-signed certificate (PEM file) using either the caBundleFile or caBundleDir parameters. Windows For HTTP proxies you don't need to setup any certificates. The plugin loads the system certificates. For HTTPS, you can configure it in one of the following ways: Import the proxy certificate to the system pool (Recommended) Import the proxy self-signed certificate (PEM file) by using the MMC tool. Refer to this link, and in Step 2 ensure to import it in your Trusted Root Certification Authorities, instead of in the Intermediate Certification Authorities. Using the caBundleFile and caBundleDir parameters On Windows, we cannot load both the certificates from the system certificate pool and the ones specified with the caBundleFile caBundleDir parameters. So, if you are using caBundleFile or caBundleDir, ensure that the following certificates are placed in the same PEM file (when using caBundleFile) or in the same directory (when using caBundleDir): The Proxy certificate (because it's an HTTPS proxy). The Logging Endpoint certificate (eg. https://log-api.newrelic.com/log/v1). The Infrastructure Agent certificate (eg. https://infra-api.newrelic.com). You can check the certificates by running: # openssl s_client -connect log-api.newrelic.com:443 -servername log-api.newrelic.com Copy Sending the infrastructure agent's logs to New Relic You can configure the infrastructure agent to send its own logs to New Relic. This is useful for troubleshooting issues with log forwarding, the agent, or when contacting support. To forward the infrastructure agent logs to New Relic: Edit your newrelic-infra.yml file. Enable agent logging in troubleshooting mode by adding verbose: 3. On Windows and systems that don't use systemd or where journald is inaccessible, verbose:3 causes the agent to write the logs on the disk. Revert to verbose:0 to prevent this. (Recommended): Enable agent logging in JSON format to log_format: json. Restart the agent to load the new settings. This configuration sets up the agent in troubleshooting mode, but the log forwarder (based on Fluent Bit) will continue in a non-verbose mode. Sometimes you can have issues with the log forwarder itself. For example, there may be problems accessing a specific channel when shipping Windows log events or when accessing a particular log file. In these situations, you can also enable the verbose mode for the log forwarder: Set verbose to a value other than 0. Add the configuration option: trace: [\"log.fw\"]. Caution Check whether you are using the [fluentbit] option. When setting verbose: 3 and trace: [\"log.fw\"], ensure that you don't define any [OUTPUT] section pointing to stdout in an external Fluent Bit configuration file, Fluent Bit does not start with the infra agent Important Fluent Bit's tail plugin does not support network drives. For Linux versions prior to 2016, you may need to update the OpenSSL library to 1.1.0 (or higher). To check if you have this problem: See if infra-agent has started Fluent Bit by running: ps -aux | grep td-agent-bit Copy If it isn't running go to /var/db/newrelic-infra/newrelic-integrations/logging and run: ./fluent-bit -i systemd -o stdout Copy If you get the following error, update OpenSSL to 1.1.0 or higher: error while loading shared libraries: libssl.so.1.1: cannot open shared object file: No such file or directory Copy Runtime error on Windows One of the following error messages may appear when enabling log forwarding on Windows: The code execution cannot proceed because VCRUNTIME140.dll was not found. Copy OR error=\"exit status 3221225781\" process=log-forwarder Copy This is caused by a missing DLL. To solve the issue, install the Microsoft Visual C++ Redistributable as applicable: x64 x86 Errors when tailing a large amount of log files (Linux) It's common to face either one of the following error messages when attempting to tail a large amount of files: Too many open files The user limit on the total number of inotify watches was reached or the kernel failed to allocate a needed resource The operating system defines a maximum amount of allocatable file descriptors (typically 1024 by default), and a maximum amount of allocatable inotify watches (typically 8192 by default). Any process attempting to go above these limits will fail, returning one of the errors above. The underlying technology we use to forward logs, Fluent Bit, opens one file descriptor and sets an inotify watch for each file you configure to be forwarded. Moreover, at the time of writing this section, Fluent Bit uses an extra set of 32 file descriptors for its normal operation, with another extra file descriptor when it shuts down. Therefore, to capture a large amount of files you need to ensure that both the file descriptor and inotify watch limits are slightly greater than the amount of log files you wish to tail. The following instructions summarize how to increase these limits if you want to tail 10,000 log files. Also, it assumes the infrastructure agent is installed in root running mode, and therefore must be run using the root user. Check which is the current hard limit for the amount of file descriptors per process. Typically, this limit should be quite high and should not need to be modified. ulimit -Hn Copy Add the following line to /etc/security/limits.conf. We specified a limit of 10100 here instead of just 10000 to allow Fluent Bit to allocate the extra file descriptors it may need to work. root soft nofile 10100 # replace root by nri-agent for non-root (privileged and unprivileged) installations Copy Add the following line to /etc/pam.d/common-session so that the previous limit is applied upon restart: session required pam_limits.so Copy Add the following line to /etc/sysctl.conf to increase the amount of allowed inotify watchers per user. We specified a limit of 18192 here instead of just 10000 so that the root user will still have 8192 available inotify watches (the default value). fs.inotify.max_user_watches=18192 Copy Restart your system. Ensure that the new limits have been enforced by running: ulimit -Sn # Should return 10100 cat /proc/sys/fs/inotify/max_user_watches # Should return 18192 Copy Learn more on how to increase open file limits, or on how to increase the inotify watches. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards. Uninstall log forwarding To uninstall log forwarding capabilities, go to your logging.d directory, and remove files with the .yml extension that were originally added during the configuration process. Linux: /etc/newrelic-infra/logging.d/ Windows: C:\\Program Files\\New Relic\\newrelic-infra\\logging.d\\",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 353.44574,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward your <em>logs</em> using the <em>infrastructure</em> agent",
        "sections": "Sending the <em>infrastructure</em> agent&#x27;s <em>logs</em> to <em>New</em> <em>Relic</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " <em>management</em> capabilities to collect, process, explore, query, and alert on your <em>log</em> data. Basic process To forward your <em>logs</em> through our infrastructure <em>monitoring</em> agent: If you haven&#x27;t already, create a <em>New</em> <em>Relic</em> account. It&#x27;s free, forever. Verify the system requirements needed for configuring <em>logs</em>"
      },
      "id": "603e9df164441f6b6f4e8843"
    },
    {
      "sections": [
        "Forward your logs to New Relic",
        "How it works",
        "Get started with log forwarders",
        "Log forwarding options",
        "TCP endpoint",
        "Log API",
        "What's next?"
      ],
      "title": "Forward your logs to New Relic",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "b6e203ddc367d5a2b5e002916e49d34f4ba17a87",
      "image": "https://docs.newrelic.com/static/c3d5443b84a1e2b26a4767ce35fa58f3/e5166/new-relic-logs-in-context-diagram.jpg",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/enable-log-management-new-relic/",
      "published_at": "2022-01-08T02:40:08Z",
      "updated_at": "2022-01-04T07:02:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our log management capabilities help you to collect, process, explore, query, and alert on your log data. To get your logs into New Relic, you can: Use your existing log forwarding solution to collect your logs and extend the metadata that is forwarded to New Relic. Use our infrastructure agent as a lightweight data collector, without having to use additional software. Use our Log API to forward your logs via HTTP. Use syslog protocols to forward your logs via a TCP endpoint. How it works The following diagram shows the lifecycle of a log message for an app, from enrichment with APM agent metadata (contextual logging), to formatting and forwarding the log data to New Relic. This diagram illustrates the flow of log messages through New Relic. Standard log formatters transform log events into meaningful output (such as text files) that can be used by downstream people and processes. The NewRelicFormatter transforms log events into the JSON format expected by New Relic. These files contain log information and extended metadata. When you configure your log forwarder (our infrastructure monitoring agent, Fluentd, Logstash, etc.), you can also extend and enrich your log data. By configuring logs in context, the log enricher links the formatted log data with additional transaction information from your application or host. Now your log files are enriched with enhanced metadata and contextual logging data. Your log forwarder sends the files to our logging endpoint for processing. From there you can use our log management capabilities to view, query, set up alerts, and more in New Relic. Here is an example of logs for your host's UI. You can see logs in context of events for the selected time period, and drill down into detailed data for any of the highlighted attributes. To take advantage of even more capabilities, click Query logs from here to go directly to the Logs UI. Here is an example of a host's logs in context related to an event. Get started with log forwarders To forward your logs to New Relic with enriched metadata: If you don't have one already, create a New Relic account. It's free, forever. Have your New Relic account's license key. Install a compatible log forwarder. Ensure that outbound connectivity on TCP port 443 is allowed to the CIDR range that matches your region. Use the DNS name log-api.newrelic.com or log-api.eu.newrelic.com for configuration. For apps monitored by a New Relic APM agent, configure logs in context. Log forwarding options Use any of these solutions to forward your logs to New Relic. Recommended: Infrastructure monitoring agent Amazon: AWS CloudWatch plugin AWS FireLens plugin AWS Kinesis Firehose AWS Lambda for sending logs from S3 Microsoft: Azure ARM template Other log forwarding plugins: Fluent Bit plugin Fluentd plugin Google Cloud Platform Pub/Sub Heroku log streaming Kubernetes plugin Logstash plugin Vector plugin TCP endpoint In some situations you may not have log forwarders; for example, with CDNs, hardware devices, or managed services. You can use syslog protocols such as rsyslog and syslog-ng, and forward your logs to New Relic via a TCP endpoint. Log API If you prefer to connect to New Relic without installing a plugin, we offer an HTTP input integration. This option sends your monitored log data directly to New Relic via the Log API. What's next? After you enable your log forwarder, make the most of your data in New Relic with our log management capabilities: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data if you are forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards. For example, to query and manage your data partition rules, see our NerdGraph tutorial.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 314.52045,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward your <em>logs</em> to <em>New</em> <em>Relic</em>",
        "sections": "Forward your <em>logs</em> to <em>New</em> <em>Relic</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " directly to <em>New</em> <em>Relic</em> via the <em>Log</em> API. What&#x27;s next? After you <em>enable</em> your <em>log</em> forwarder, make the most of your data in <em>New</em> <em>Relic</em> with our <em>log</em> <em>management</em> capabilities: Explore the logging data across your platform with our <em>Logs</em> UI. See your <em>logs</em> in context of your app&#x27;s performance in the APM UI"
      },
      "id": "61571e0e28ccbcbc52f21431"
    }
  ],
  "/docs/logs/forward-logs/forward-your-logs-using-infrastructure-agent": [
    {
      "sections": [
        "Fluent Bit plugin for log forwarding",
        "Basic process",
        "Install the Fluent Bit plugin",
        "Tip",
        "Configure the Fluent Bit plugin",
        "Important",
        "Test the Fluent Bit plugin",
        "Associate logs with entities",
        "Optional: Configure plugin attributes",
        "View log data",
        "What's next?"
      ],
      "title": "Fluent Bit plugin for log forwarding",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "dd686f074bb80908ae9de19e543a3bae7d87a466",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/fluent-bit-plugin-log-forwarding/",
      "published_at": "2022-01-08T02:38:07Z",
      "updated_at": "2022-01-08T02:38:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your log data in New Relic. Forwarding your Fluent Bit logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Basic process We have published a container with the plugin installed. It serves as a base image to be used by our Kubernetes integration. We recommend you use this base image and layer your own custom configuration files. To forward your logs from Fluent Bit to New Relic: Make sure you have: A New Relic license key Fluent Bit 1.0 or higher (recommended), although v0.12 or higher is supported Fluent Bit Windows install directions can be found here Fluent Bit Linux install directions can be found here Install the Fluent Bit plugin. Configure the Fluent Bit plugin. Test the Fluent Bit plugin. Generate some traffic and wait a few minutes, then check your account for data. Install the Fluent Bit plugin To install the Fluent Bit plugin: Navigate to New Relic's Fluent Bit plugin repository on GitHub. From the repository page, clone or download the repository. Run the following command to build your plugin: cd newrelic-fluent-bit-output && make all Copy Store out_newrelic.so or out_newrelic_winXX.dll at a location that can be accessed by the fluent-bit daemon. Tip If you'd rather not compile the plugin yourself, you can download pre-compiled versions from our GitHub repository's releases page. Configure the Fluent Bit plugin Fluent Bit needs to know the location of the New Relic plugin and the New Relic license key to output data to New Relic. To configure your Fluent Bit plugin: Important Pay attention to white space when editing your config files. Be sure to use four spaces to indent and one space between keys and values. Locate or create a plugins.conf file in your plugins directory. In the plugins.conf file, add a reference to out_newrelic.so, adjacent to your fluent-bit.conf file: [PLUGINS] Path /PATH/TO/newrelic-fluent-bit-output/out_newrelic.so Copy In the fluent-bit.conf file, add the following line under the service block: [SERVICE] # This is the main configuration block for fluent bit. # Ensure the follow line exists somewhere in the SERVICE block Plugins_File plugins.conf Copy At the bottom of the fluent-bit.conf file, add the following to set up the input and output filters. Replace the placeholder text with your New Relic license key: [INPUT] Name tail Path /PATH/TO/YOUR/LOG/FILE [OUTPUT] Name newrelic Match * licenseKey YOUR_LICENSE_KEY # Optional maxBufferSize 256000 maxRecords 1024 Copy Restart your Fluent Bit instance with the following command: fluent-bit -c /PATH/TO/fluent-bit.conf Copy Test the Fluent Bit plugin To test if your Fluent Bit plugin is receiving input from a log file: Run the following command to append a test log message to your log file: echo \"test message\" >> /PATH/TO/YOUR/LOG/FILE Copy Search the New Relic Logs UI for test message. Associate logs with entities To associate a log line with an entity, such as an infrastructure host, add a FILTER block: [FILTER] Name modify Match * # Or specify a match Add entity.guids <Your Entity GUID ID> # Optional Add hostname <Your hostname> # Optional Copy For more options, see the Fluent Bit modify filter documentation and our documentation to forward your logs using the infrastructure agent. Optional: Configure plugin attributes Once you have installed and configured the Fluent Bit plugin, you can use the following attributes to configure how the plugin sends data to New Relic: Key Description licenseKey The New Relic license key. Use either licenseKey (recommended) or apiKey, not both. Default: none maxBufferSize The maximum size the payloads sent, in bytes. Default: 256000 maxRecords The maximum number of records to send at a time. Default: 1024 apiKey Deprecated. Takes a New Relic Insights insert key, but using the licenseKey attribute is preferred. Use either licenseKey or apiKey, not both. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 374.04865,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Fluent Bit plugin for <em>log</em> forwarding",
        "sections": "Associate <em>logs</em> with entities",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "If your <em>log</em> data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your <em>log</em> data in <em>New</em> <em>Relic</em>. Forwarding your Fluent Bit <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert"
      },
      "id": "603ea7ec28ccbce542eba791"
    },
    {
      "sections": [
        "Forward your logs to New Relic",
        "How it works",
        "Get started with log forwarders",
        "Log forwarding options",
        "TCP endpoint",
        "Log API",
        "What's next?"
      ],
      "title": "Forward your logs to New Relic",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "b6e203ddc367d5a2b5e002916e49d34f4ba17a87",
      "image": "https://docs.newrelic.com/static/c3d5443b84a1e2b26a4767ce35fa58f3/e5166/new-relic-logs-in-context-diagram.jpg",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/enable-log-management-new-relic/",
      "published_at": "2022-01-08T02:40:08Z",
      "updated_at": "2022-01-04T07:02:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our log management capabilities help you to collect, process, explore, query, and alert on your log data. To get your logs into New Relic, you can: Use your existing log forwarding solution to collect your logs and extend the metadata that is forwarded to New Relic. Use our infrastructure agent as a lightweight data collector, without having to use additional software. Use our Log API to forward your logs via HTTP. Use syslog protocols to forward your logs via a TCP endpoint. How it works The following diagram shows the lifecycle of a log message for an app, from enrichment with APM agent metadata (contextual logging), to formatting and forwarding the log data to New Relic. This diagram illustrates the flow of log messages through New Relic. Standard log formatters transform log events into meaningful output (such as text files) that can be used by downstream people and processes. The NewRelicFormatter transforms log events into the JSON format expected by New Relic. These files contain log information and extended metadata. When you configure your log forwarder (our infrastructure monitoring agent, Fluentd, Logstash, etc.), you can also extend and enrich your log data. By configuring logs in context, the log enricher links the formatted log data with additional transaction information from your application or host. Now your log files are enriched with enhanced metadata and contextual logging data. Your log forwarder sends the files to our logging endpoint for processing. From there you can use our log management capabilities to view, query, set up alerts, and more in New Relic. Here is an example of logs for your host's UI. You can see logs in context of events for the selected time period, and drill down into detailed data for any of the highlighted attributes. To take advantage of even more capabilities, click Query logs from here to go directly to the Logs UI. Here is an example of a host's logs in context related to an event. Get started with log forwarders To forward your logs to New Relic with enriched metadata: If you don't have one already, create a New Relic account. It's free, forever. Have your New Relic account's license key. Install a compatible log forwarder. Ensure that outbound connectivity on TCP port 443 is allowed to the CIDR range that matches your region. Use the DNS name log-api.newrelic.com or log-api.eu.newrelic.com for configuration. For apps monitored by a New Relic APM agent, configure logs in context. Log forwarding options Use any of these solutions to forward your logs to New Relic. Recommended: Infrastructure monitoring agent Amazon: AWS CloudWatch plugin AWS FireLens plugin AWS Kinesis Firehose AWS Lambda for sending logs from S3 Microsoft: Azure ARM template Other log forwarding plugins: Fluent Bit plugin Fluentd plugin Google Cloud Platform Pub/Sub Heroku log streaming Kubernetes plugin Logstash plugin Vector plugin TCP endpoint In some situations you may not have log forwarders; for example, with CDNs, hardware devices, or managed services. You can use syslog protocols such as rsyslog and syslog-ng, and forward your logs to New Relic via a TCP endpoint. Log API If you prefer to connect to New Relic without installing a plugin, we offer an HTTP input integration. This option sends your monitored log data directly to New Relic via the Log API. What's next? After you enable your log forwarder, make the most of your data in New Relic with our log management capabilities: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data if you are forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards. For example, to query and manage your data partition rules, see our NerdGraph tutorial.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 314.52045,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward your <em>logs</em> to <em>New</em> <em>Relic</em>",
        "sections": "Forward your <em>logs</em> to <em>New</em> <em>Relic</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " directly to <em>New</em> <em>Relic</em> via the <em>Log</em> API. What&#x27;s next? After you <em>enable</em> your <em>log</em> forwarder, make the most of your data in <em>New</em> <em>Relic</em> with our <em>log</em> <em>management</em> capabilities: Explore the logging data across your platform with our <em>Logs</em> UI. See your <em>logs</em> in context of your app&#x27;s performance in the APM UI"
      },
      "id": "61571e0e28ccbcbc52f21431"
    },
    {
      "sections": [
        "Logs in context of apps and hosts",
        "See the root cause of issues across your platform",
        "Basic process to enable logs in context",
        "API and other options",
        "What's next?"
      ],
      "title": "Logs in context of apps and hosts",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Configure logs in context"
      ],
      "external_id": "7f77d1e4599c8f7b9b2a44bc817f328f11410651",
      "image": "https://docs.newrelic.com/static/49f9b37d2957292090bdbf226eadacea/c1b63/new-relic-logs-in-context-diagram.png",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/configure-logs-context-apm-agents/",
      "published_at": "2022-01-08T02:05:17Z",
      "updated_at": "2022-01-04T06:00:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "When you need to correlate log data with other telemetry data, enable logs in context in New Relic. Logs in context functionality adds metadata that links your logs with related APM data, like errors or distributed traces, or your platform performance data from infrastructure monitoring logs in New Relic One. To see how logs in context and log patterns can help you find the root cause of an issue, watch this short video (approx. 5 minutes): See the root cause of issues across your platform By bringing all of your application and infrastructure data together in a single solution, you can get to the root cause of issues faster. Logs in context help you quickly see meaningful patterns and trends. The following diagram shows the lifecycle of a log message, from enrichment with agent metadata (contextual logging), to formatting and forwarding the log data to New Relic: This diagram illustrates the flow of log messages through New Relic. Don't spend extra time trying to narrow down all your logs from different parts of your platform. Instead, enable logs in context to see the exact log lines you need to identify and resolve a problem. Basic process to enable logs in context Here is an example of logs for your app's UI. You can see logs in context of events for the selected time period, and drill down into detailed data for any of the highlighted attributes. To take advantage of even more capabilities, click Query logs from here to go directly to the Logs UI. The process to enable logs in context is basically the same, regardless of which APM agent you use to monitor your application: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Update to a supported APM agent version for your app, and enable distributed tracing. Configure logs in context for your APM agent or for your infrastructure monitoring agent. View your logs within the context of your apps or infrastructure in New Relic One. The main differences in this procedure are which log appenders you can use to extend and enrich your log data, and how to configure the log appender you select for your APM agent. For detailed information, see the logs-in-context procedures for: C SDK Go Java .NET Node.js PHP Python Ruby Infrastructure monitoring agent API and other options If our logging solutions don't meet your needs, you can use other options to send your log data to New Relic: Logging extensions via agent API calls HTTP endpoint via our Log API Syslog protocols via TCP endpoint (useful for CDNs, hardware devices, or managed services) What's next? After you set up logs in context for APM or infrastructuring monitoring, make the most of your logging data in the New Relic One UI: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards. Here is an example of logs in context for app trace details, visible from the APM UI.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 233.74214,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Logs</em> <em>in</em> context of apps and hosts",
        "sections": "Basic process to <em>enable</em> <em>logs</em> <em>in</em> context",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "When you need to correlate <em>log</em> data with other telemetry data, <em>enable</em> <em>logs</em> in context in <em>New</em> <em>Relic</em>. <em>Logs</em> in context functionality adds metadata that links your <em>logs</em> with related APM data, like errors or distributed traces, or your platform performance data from infrastructure <em>monitoring</em> <em>logs</em> in <em>New</em>"
      },
      "id": "603ea62e196a6749f8a83dc9"
    }
  ],
  "/docs/logs/forward-logs/google-cloud-platform-log-forwarding": [
    {
      "sections": [
        "Fluent Bit plugin for log forwarding",
        "Basic process",
        "Install the Fluent Bit plugin",
        "Tip",
        "Configure the Fluent Bit plugin",
        "Important",
        "Test the Fluent Bit plugin",
        "Associate logs with entities",
        "Optional: Configure plugin attributes",
        "View log data",
        "What's next?"
      ],
      "title": "Fluent Bit plugin for log forwarding",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "dd686f074bb80908ae9de19e543a3bae7d87a466",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/fluent-bit-plugin-log-forwarding/",
      "published_at": "2022-01-08T02:38:07Z",
      "updated_at": "2022-01-08T02:38:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your log data in New Relic. Forwarding your Fluent Bit logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Basic process We have published a container with the plugin installed. It serves as a base image to be used by our Kubernetes integration. We recommend you use this base image and layer your own custom configuration files. To forward your logs from Fluent Bit to New Relic: Make sure you have: A New Relic license key Fluent Bit 1.0 or higher (recommended), although v0.12 or higher is supported Fluent Bit Windows install directions can be found here Fluent Bit Linux install directions can be found here Install the Fluent Bit plugin. Configure the Fluent Bit plugin. Test the Fluent Bit plugin. Generate some traffic and wait a few minutes, then check your account for data. Install the Fluent Bit plugin To install the Fluent Bit plugin: Navigate to New Relic's Fluent Bit plugin repository on GitHub. From the repository page, clone or download the repository. Run the following command to build your plugin: cd newrelic-fluent-bit-output && make all Copy Store out_newrelic.so or out_newrelic_winXX.dll at a location that can be accessed by the fluent-bit daemon. Tip If you'd rather not compile the plugin yourself, you can download pre-compiled versions from our GitHub repository's releases page. Configure the Fluent Bit plugin Fluent Bit needs to know the location of the New Relic plugin and the New Relic license key to output data to New Relic. To configure your Fluent Bit plugin: Important Pay attention to white space when editing your config files. Be sure to use four spaces to indent and one space between keys and values. Locate or create a plugins.conf file in your plugins directory. In the plugins.conf file, add a reference to out_newrelic.so, adjacent to your fluent-bit.conf file: [PLUGINS] Path /PATH/TO/newrelic-fluent-bit-output/out_newrelic.so Copy In the fluent-bit.conf file, add the following line under the service block: [SERVICE] # This is the main configuration block for fluent bit. # Ensure the follow line exists somewhere in the SERVICE block Plugins_File plugins.conf Copy At the bottom of the fluent-bit.conf file, add the following to set up the input and output filters. Replace the placeholder text with your New Relic license key: [INPUT] Name tail Path /PATH/TO/YOUR/LOG/FILE [OUTPUT] Name newrelic Match * licenseKey YOUR_LICENSE_KEY # Optional maxBufferSize 256000 maxRecords 1024 Copy Restart your Fluent Bit instance with the following command: fluent-bit -c /PATH/TO/fluent-bit.conf Copy Test the Fluent Bit plugin To test if your Fluent Bit plugin is receiving input from a log file: Run the following command to append a test log message to your log file: echo \"test message\" >> /PATH/TO/YOUR/LOG/FILE Copy Search the New Relic Logs UI for test message. Associate logs with entities To associate a log line with an entity, such as an infrastructure host, add a FILTER block: [FILTER] Name modify Match * # Or specify a match Add entity.guids <Your Entity GUID ID> # Optional Add hostname <Your hostname> # Optional Copy For more options, see the Fluent Bit modify filter documentation and our documentation to forward your logs using the infrastructure agent. Optional: Configure plugin attributes Once you have installed and configured the Fluent Bit plugin, you can use the following attributes to configure how the plugin sends data to New Relic: Key Description licenseKey The New Relic license key. Use either licenseKey (recommended) or apiKey, not both. Default: none maxBufferSize The maximum size the payloads sent, in bytes. Default: 256000 maxRecords The maximum number of records to send at a time. Default: 1024 apiKey Deprecated. Takes a New Relic Insights insert key, but using the licenseKey attribute is preferred. Use either licenseKey or apiKey, not both. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 374.09024,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Fluent Bit plugin for <em>log</em> forwarding",
        "sections": "Associate <em>logs</em> with entities",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "If your <em>log</em> data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your <em>log</em> data in <em>New</em> <em>Relic</em>. Forwarding your Fluent Bit <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert"
      },
      "id": "603ea7ec28ccbce542eba791"
    },
    {
      "sections": [
        "Forward your logs using the infrastructure agent",
        "Basic process",
        "System requirements",
        "Important",
        "Install the infrastructure agent",
        "Configure the infrastructure agent",
        "Log forwarding parameters",
        "Name (required)",
        "Log source (required)",
        "file",
        "systemd",
        "syslog",
        "tcp",
        "winlog",
        "Optional configuration",
        "attributes",
        "attributes automatically inserted by the infrastructure agent",
        "pattern",
        "max_line_kb",
        "fluentbit",
        "Sample configuration file",
        "logging.d/sample.yaml",
        "View your log data",
        "Troubleshooting",
        "No log data",
        "No data appears when tailing a file",
        "No data appears when capturing via a Syslog socket",
        "No data appears using infrastructure agent proxy",
        "Sending the infrastructure agent's logs to New Relic",
        "Caution",
        "Fluent Bit does not start with the infra agent",
        "Runtime error on Windows",
        "Errors when tailing a large amount of log files (Linux)",
        "What's next?",
        "Uninstall log forwarding"
      ],
      "title": "Forward your logs using the infrastructure agent",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "a1deac410f0eedfb819348524a85a73bbb9d9daf",
      "image": "https://docs.newrelic.com/static/5d4352531871b652878b3cb0768e3f23/c1b63/demotron-host-logs-123021.png",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/forward-your-logs-using-infrastructure-agent/",
      "published_at": "2022-01-09T01:46:14Z",
      "updated_at": "2022-01-07T01:41:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can forward your logs to New Relic using our infrastructure monitoring agent. This makes all of your logging data available in one location and provides deeper visibility into both your application and your platform performance data. Forwarding your logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Basic process To forward your logs through our infrastructure monitoring agent: If you haven't already, create a New Relic account. It's free, forever. Verify the system requirements needed for configuring logs. Ensure you have installed the infrastructure agent, version 1.11.4 or higher. Create a logging.yml configuration file in the infrastructure agent's logging.d directory. Configure your log sources and other parameters. Generate some traffic and wait a few minutes, then check your account for data. Explore your log data in the Logs UI and benefit from the log attributes automatically inserted by the infrastructure agent. Here is an example of logs for your host's UI. You can see logs in context of events for the selected time period, and drill down into detailed data for any of the highlighted attributes. To take advantage of even more capabilities, click Query logs from here to go directly to the Logs UI. Here is an example of a host's logs in context related to an event. System requirements To use the log forwarder of the infrastructure agent, make sure you meet the following requirements: Infrastructure agent version 1.11.4 or higher OpenSSL library 1.1.0 or higher is required by Infrastructure agent starting from 1.16.4. Built-in support for ARM64 architecture on Linux systems (in example, AWS Graviton architecture) added in Infrastructure agent 1.26.0. Important The log forwarding feature is not supported with the Docker container for infrastructure monitoring agents. The log forwarding feature is compatible with the following operating systems: Operating system Supported version Amazon Linux Amazon Linux 2 CentOS Version 7 or higher Debian Version 9 (\"Stretch\") or higher Exception: Version 11 is not supported. Red Hat Enterprise Linux (RHEL) Version 7 or higher SUSE Linux Enterprise Server (SLES) Version 12 Ubuntu Versions 16.04.x, 18.04.x and 20.04.x (LTS versions) Windows Windows Server 2012, 2016, 2019, and 2022, and their service packs. Windows 10 Install the infrastructure agent Starting with version 1.11.4, the infrastructure agent can forward logs to New Relic. To install and run the agent, use a package manager (Linux) or the MSI installer (Windows). Important The log forwarding feature is not included when the infrastructure agent is implemented using Linux tarball or Windows ZIP installations. To use the following links, make sure you are logged to your New Relic account. Amazon Linux CentOS Debian RHEL SLES Ubuntu Windows If you don't have a New Relic account yet, or if you prefer to follow the procedure manually, see our tutorial to install the package manager. Configure the infrastructure agent Configuration files describe which log sources are forwarded. Our infrastructure agent uses .yml files to configure logging. You can add as many config files as you want. To add a new configuration file for the log forwarding feature: Navigate to the log forwarder configuration folder: Linux: /etc/newrelic-infra/logging.d/ Windows: C:\\Program Files\\New Relic\\newrelic-infra\\logging.d\\ Create a logging.yml configuration file, and add the parameters you need. The logging.d directory has various .yml.example files you can use as a reference or starting point. The agent automatically processes new configuration files without having to restart the infrastructure monitoring service. The only exception to this is when configuring a custom Fluent Bit configuration. Log forwarding parameters The infrastructure log forwarding .yml config supports the following parameters: Name (required) To start, define a name of the log or logs you want to forward to New Relic. Log source (required) What you use for the log source will depend on where you want to forward your logs from. Available options include: file Path to the log file or files. The agent tracks changes on the log files in a way similar to tail -f shell. Example: logs: - name: example-log file: /var/log/example.log # Path to a single log file - name: example-log-two file: /var/log/example-two.log # Path to another single log file Copy The file parameter can point to a specific log file or multiple files by using wildcards applied to names and extensions; for example, /logs/*.log. You can use wildcards in place of directories in a file path, which can be used to tail files located in different directories. Example: logs: - name: docker-logs file: /var/lib/docker/containers/*/*.log # Path to multiple folders and files Copy Important Use of wildcards may significantly increase the number of file descriptors and inotify watches the Fluent Bit process keeps open, which can interfere with log collection if the host's file descriptor limit is reached. Tailing a large number of files may require you to increase the maximum number of file descriptors and inotify watchers allowed by the operating system. Please refer to Errors when tailing a large amount of log files for more details on how to increase them. systemd Use the systemd parameter to forward log messages that are collected by the journald daemon in Linux environments. This input type requires the agent to run in root mode. Example: logs: - name: systemd-example systemd: cupsd Copy syslog Syslog data source. Parameters: uri: Syslog socket. Format varies depending on the protocol: TCP/UDP network sockets: [tcp/udp]://LISTEN_ADDRESS:PORT Unix domain sockets: unix_[tcp/udp]:// + /socket/path parser: Syslog parser. Default is rfc3164. Use rfc5424 if your messages include fractional seconds. Note: rfc3164 currently does not work on SuSE. unix_permissions: default is 0644 for domain sockets; this limits entries to processes running as root. You can use 0666 to listen for non-root processes, at your own risk. When running the agent in privileged mode, ports and sockets must be available or owned by nri-agent, with 0666 file permissions, so that other processes can write logs to the sockets. logs: # TCP network socket - name: syslog-tcp-test syslog: uri: tcp://0.0.0.0:5140 # Use the tcp://LISTEN_ADDRESS:PORT format parser: rfc5424 # Default syslog parser is rfc3164 # UDP network socket - name: syslog-udp-test syslog: uri: udp://0.0.0.0:6140 # Use the udp://LISTEN_ADDRESS:PORT format max_line_kb: 35 # Unix TCP domain socket - name: syslog-unix-tcp-test syslog: uri: unix_tcp:///var/unix-tcp-socket-test unix_permissions: 0666 # Default is 0644. Change at your own risk # Unix UDP domain socket - name: syslog-unix-udp-test syslog: uri: unix_udp:///var/unix-udp-socket-test parser: rfc5424 Copy tcp Logs retrieved over TCP connections. Parameters: uri: TCP/IP socket to listen for incoming data. The URI format is tcp://LISTEN_ADDRESS:PORT format: format of the data. It can be json or none. separator: If format: none is used, you can define a separator string for splitting records (default: \\n). logs: - name: tcp-simple-test tcp: uri: tcp://0.0.0.0:1234 # Use the tcp://LISTEN_ADDRESS:PORT format format: none # Raw text - this is default for 'tcp' separator: \\t # String for separating raw text entries max_line_kb: 32 - name: tcp-json-test tcp: uri: tcp://0.0.0.0:2345 # Use the tcp://LISTEN_ADDRESS:PORT format format: json Copy winlog Collect events from Windows log channels. Parameters: channel: name of the channel logs will be collected from. collect-eventids: a list of Windows Event IDs to be collected and forwarded to New Relic. Event ID ranges are supported. exclude-eventids: a list of Windows Event IDs to be excluded from collection. Event ID ranges are supported. All events are collected from the specified channel by default. Configure the collect-eventids and exclude-eventids sections to avoid sending unwanted logs to your New Relic account. Add event IDs or ranges to collect-eventids or exclude-eventids to forward or drop specific events. exclude-eventids takes precedence over collect-eventids if the same event ID is present in both sections. Example: logs: # Winlog log ingestion with eventId filters. - name: windows-security winlog: channel: Security collect-eventids: - 4624 - 4265 - 4700-4800 exclude-eventids: - 4735 # entries for the application, system, powershell, and SCOM channels - name: windows-application winlog: channel: Application - name: windows-system winlog: channel: System - name: windows-pshell winlog: channel: Windows Powershell - name: scom winlog: channel: Operations Manager # Entry for Windows Defender Logs - name: windows-defender winlog: channel: Microsoft-Windows-Windows Defender/Operational # Entry for Windows Clustering Logs - name: windows-clustering winlog: channel: Microsoft-Windows-FailoverClustering/Operational # Entry for IIS logs with logtype attribute for automatic parsing - name: iis-log file: C:\\inetpub\\logs\\LogFiles\\w3svc.log attributes: logtype: iis_w3c Copy Optional configuration The following configuration parameters are not required but are still recommended. attributes List of custom attributes specified as key-value pairs that can be used to send additional data with the logs which you can then query. The attributes configuration parameter can be used with any log source. One common use of the attributes configuration parameter is to specify the logtype attribute. This attribute allows leveraging one of the built-in parsing rules supported by New Relic Logs. Example: logs: - name: example-file-attributes file: /var/log/example.log attributes: logtype: nginx region: example-us-02 team: A-team - name: example-tcp-attributes tcp: uri: tcp://0.0.0.0:2345 format: json attributes: logtype: nginx region: example-us-02 team: B-team Copy attributes automatically inserted by the infrastructure agent The infrastructure agent automatically inserts log attributes for your convenience. Some of them are inserted for any log record, while others depend on the configuration parameters you used while setting up the log forwarder. Attribute name Description entity.guids Always inserted. The infrastructure agent inserts the Entity GUID assigned by New Relic to identify the host where it's running. It is available in the entity.guids field. Note: If the captured logs belong to an application instrumented using APM, the entity.guids field contains both the entity GUID of infrastructure, as well as the GUID of APM, separated by a pipe ( | ) delimiter. fb.input Always inserted. The underlying Fluent Bit input plugin type used to capture the logs. Currently its values are tail, systemd, winlog, syslog, and tcp. filePath Inserted when sing the file input type. Absolute file path of the file being monitored. hostname Always inserted. The hostname of the machine/VM/container executing the infrastructure agent. plugin.type Always inserted. Indicates the utility used to capture the logs. In this case, it is the infrastructure agent itself, so this attribute always has the value nri-agent. pattern Regular expression for filtering records. Only supported for the tail, systemd, syslog, and tcp (only with format none) sources. This field works in a way similar to grep -E in Unix systems. For example, for a given file being captured, you can filter for records containing either WARN or ERROR using: - name: only-records-with-warn-and-error file: /var/log/logFile.log pattern: WARN|ERROR Copy No filtering is applied by default. max_line_kb Maximum size of log entries/lines in KB. If log entries exceed the limit, they are skipped. Default is 128. fluentbit External Fluent Bit configuration and parser files. If defined, they are merged with the existing configuration and parser files generated by the infrastructure agent. The infrastructure agent processes the configuration files located in the logging.d directory and will generate a run-time Fluent Bit configuration file that contains the appropriate [INPUT], [FILTER] and [OUTPUT] sections. Optionally, it will also declare an @INCLUDE in case you provided an external Fluent Bit configuration file via the fluentbit option. The runtime file does not define a [SERVICE] section, leaving all default Fluent Bit configuration values. You can still override Fluent Bit's default settings by defining your own [SERVICE] section in your external Fluent Bit configuration file and include it via the fluentbit option. Parameters: config_file: path to an existing Fluent Bit configuration file. Note that any overlapping source results in duplicate messages in New Relic Logs. parsers_file: path to an existing Fluent Bit parsers file. The following parser names are reserved: rfc3164, rfc3164-local and rfc5424. Sample configuration file Here is an example of a logging.d/ configuration file in YAML format. For more configuration examples, see the infrastructure agent repository. logging.d/sample.yaml # Remember to only use spaces for indentation logs: # Example of 'file' source - name: file-with-attributes file: /var/log/test.log # Path to a single file or pattern attributes: # You can use custom attributes to enrich your data logtype: nginx team: The A Team pattern: Error # Regular expression to filter log entries # Example of 'systemd' source (Linux only) - name: systemd-example systemd: cupsd # Examples of 'syslog' source, one per protocol # TCP network socket - name: syslog-tcp-test syslog: uri: tcp://0.0.0.0:5140 # Use the tcp://LISTEN_ADDRESS:PORT format parser: rfc5424 # Default syslog parser is rfc3164 # UDP network socket - name: syslog-udp-test syslog: uri: udp://0.0.0.0:6140 # Use the udp://LISTEN_ADDRESS:PORT format max_line_kb: 35 # Paths for Unix sockets are defined by combining protocol and path: # unix_udp:// + /path/socket - for example, unix_udp:///tmp/socket # Unix TCP domain socket - name: syslog-unix-tcp-test syslog: uri: unix_tcp:///var/unix-tcp-socket-test unix_permissions: 0666 # Default is 0644. Change at your own risk # Unix UDP domain socket - name: syslog-unix-udp-test syslog: uri: unix_udp:///var/unix-udp-socket-test parser: rfc5424 # Examples of 'tcp' source for formats 'none' and 'json' - name: tcp-simple-test tcp: uri: tcp://0.0.0.0:1234 # Use the tcp://LISTEN_ADDRESS:PORT format format: none # Raw text - this is default for 'tcp' separator: \\t # String for separating raw text entries attributes: # You can add custom attributes to any source of logs tcpFormat: none someOtherAttribute: associatedValue max_line_kb: 32 - name: tcp-json-test tcp: uri: tcp://0.0.0.0:2345 # Use the tcp://LISTEN_ADDRESS:PORT format format: json attributes: tcpFormat: json yetAnotherAttribute: 12345 # Example of Fluent Bit configuration import - name: fluentbit-import fluentbit: config_file: /path/to/fluentbit.config parsers_file: /path/to/fluentbit/parsers.conf Copy View your log data If everything is configured correctly and your data is being collected, you should see logs data in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy Troubleshooting If you encounter problems with configuring your log forwarder, try these troubleshooting tips. No log data If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. No data appears when tailing a file The log forwarding feature requires the agent to have permission to read the data sources. When running the infrastructure agent in privileged or non-privileged modes, make sure that the log files you want to forward (and any intermediary directory in its path) are readable by the user running nri-agent. Example: Check file access under Linux Let's check whether the file /var/log/restrictedLogs/logFile.log can be monitored by the nri-agent user. In Linux, you can do a quick check with the namei command: sudo -u nri-agent namei -ml /var/log/restrictedLogs/logFile.log f: /var/log/restrictedLogs/logFile.log drwxr-xr-x root root / drwxr-xr-x root root var drwxrwxr-x root syslog log drwxr--r-- root root restrictedLogs logFile.log - No such file or directory Copy This command failed because the file is not visible to the nri-agent user. By inspecting the previous output, we can detect that the restrictedLogs directory is missing the execution flag for others. To fix this, execute: sudo chmod 755 /var/log/restrictedLogs Copy And then check for file access again: # sudo -u nri-agent namei -ml /var/log/restrictedLogs/logFile.log f: /var/log/restrictedLogs/logFile.log drwxr-xr-x root root / drwxr-xr-x root root var drwxrwxr-x root syslog log drwxr-xr-x root root restrictedLogs -rw-r----- vagrant vagrant logFile.log Copy The file is now visible to the nri-agent user. You must ensure that the file is also readable by the nri-agent user. To check this, use: # sudo -u nri-agent head /var/log/restrictedLogs/logFile.log head: cannot open '/var/log/restrictedLogs/logFile.log' for reading: Permission denied Copy In this example, the file is missing the read rights for the others group (users other than vagrant and the vagrant user group). You could fix this by granting read permissions to others, but the application could change these permissions upon restart. To avoid this, a better approach is to add the nri-agent user to the vagrant user group. No data appears when capturing via a Syslog socket The log forwarding feature requires that the agent has permission to read the data sources. When running the infrastructure agent in privileged or non-privileged modes: If you're using Unix domain socket files, make sure that the nri-agent user can access these files (please refer to the previous section) and that they have read and write permissions (666) so that other users than nri-agent can write to them. If you're using IP sockets, ensure that the port that you are using is not a system reserved one (like port 80, for example). If no data appears after you enable log management, follow standard log management troubleshooting procedures. No data appears using infrastructure agent proxy As explained in the infrastructure agent configuration guidelines, the proxy parameter must use either HTTP or HTTPS and be in the form https://user:password@hostname:port. The agent can parse the parameter without the HTTP or HTTPS, but the log forwarder cannot. You will see an error like the following in the agent verbose logs: [ERROR] building HTTP transport: parse \\\"hostname:port\\\": first path segment in URL cannot contain colon Copy To solve this problem, check your newrelic-infra.yml file, and ensure the proxy parameter adheres to this form. If you're using caBundleFile or caBundleDir in order to specify any certificate, we recommend to follow the below rules for each OS: Linux For HTTP proxies you don't need to setup any certificates. The plugin loads the system certificates and New Relic sends logs into the logging endpoint. However, you can specify the proxy self-signed certificate (PEM file) using either the caBundleFile or caBundleDir parameters. Windows For HTTP proxies you don't need to setup any certificates. The plugin loads the system certificates. For HTTPS, you can configure it in one of the following ways: Import the proxy certificate to the system pool (Recommended) Import the proxy self-signed certificate (PEM file) by using the MMC tool. Refer to this link, and in Step 2 ensure to import it in your Trusted Root Certification Authorities, instead of in the Intermediate Certification Authorities. Using the caBundleFile and caBundleDir parameters On Windows, we cannot load both the certificates from the system certificate pool and the ones specified with the caBundleFile caBundleDir parameters. So, if you are using caBundleFile or caBundleDir, ensure that the following certificates are placed in the same PEM file (when using caBundleFile) or in the same directory (when using caBundleDir): The Proxy certificate (because it's an HTTPS proxy). The Logging Endpoint certificate (eg. https://log-api.newrelic.com/log/v1). The Infrastructure Agent certificate (eg. https://infra-api.newrelic.com). You can check the certificates by running: # openssl s_client -connect log-api.newrelic.com:443 -servername log-api.newrelic.com Copy Sending the infrastructure agent's logs to New Relic You can configure the infrastructure agent to send its own logs to New Relic. This is useful for troubleshooting issues with log forwarding, the agent, or when contacting support. To forward the infrastructure agent logs to New Relic: Edit your newrelic-infra.yml file. Enable agent logging in troubleshooting mode by adding verbose: 3. On Windows and systems that don't use systemd or where journald is inaccessible, verbose:3 causes the agent to write the logs on the disk. Revert to verbose:0 to prevent this. (Recommended): Enable agent logging in JSON format to log_format: json. Restart the agent to load the new settings. This configuration sets up the agent in troubleshooting mode, but the log forwarder (based on Fluent Bit) will continue in a non-verbose mode. Sometimes you can have issues with the log forwarder itself. For example, there may be problems accessing a specific channel when shipping Windows log events or when accessing a particular log file. In these situations, you can also enable the verbose mode for the log forwarder: Set verbose to a value other than 0. Add the configuration option: trace: [\"log.fw\"]. Caution Check whether you are using the [fluentbit] option. When setting verbose: 3 and trace: [\"log.fw\"], ensure that you don't define any [OUTPUT] section pointing to stdout in an external Fluent Bit configuration file, Fluent Bit does not start with the infra agent Important Fluent Bit's tail plugin does not support network drives. For Linux versions prior to 2016, you may need to update the OpenSSL library to 1.1.0 (or higher). To check if you have this problem: See if infra-agent has started Fluent Bit by running: ps -aux | grep td-agent-bit Copy If it isn't running go to /var/db/newrelic-infra/newrelic-integrations/logging and run: ./fluent-bit -i systemd -o stdout Copy If you get the following error, update OpenSSL to 1.1.0 or higher: error while loading shared libraries: libssl.so.1.1: cannot open shared object file: No such file or directory Copy Runtime error on Windows One of the following error messages may appear when enabling log forwarding on Windows: The code execution cannot proceed because VCRUNTIME140.dll was not found. Copy OR error=\"exit status 3221225781\" process=log-forwarder Copy This is caused by a missing DLL. To solve the issue, install the Microsoft Visual C++ Redistributable as applicable: x64 x86 Errors when tailing a large amount of log files (Linux) It's common to face either one of the following error messages when attempting to tail a large amount of files: Too many open files The user limit on the total number of inotify watches was reached or the kernel failed to allocate a needed resource The operating system defines a maximum amount of allocatable file descriptors (typically 1024 by default), and a maximum amount of allocatable inotify watches (typically 8192 by default). Any process attempting to go above these limits will fail, returning one of the errors above. The underlying technology we use to forward logs, Fluent Bit, opens one file descriptor and sets an inotify watch for each file you configure to be forwarded. Moreover, at the time of writing this section, Fluent Bit uses an extra set of 32 file descriptors for its normal operation, with another extra file descriptor when it shuts down. Therefore, to capture a large amount of files you need to ensure that both the file descriptor and inotify watch limits are slightly greater than the amount of log files you wish to tail. The following instructions summarize how to increase these limits if you want to tail 10,000 log files. Also, it assumes the infrastructure agent is installed in root running mode, and therefore must be run using the root user. Check which is the current hard limit for the amount of file descriptors per process. Typically, this limit should be quite high and should not need to be modified. ulimit -Hn Copy Add the following line to /etc/security/limits.conf. We specified a limit of 10100 here instead of just 10000 to allow Fluent Bit to allocate the extra file descriptors it may need to work. root soft nofile 10100 # replace root by nri-agent for non-root (privileged and unprivileged) installations Copy Add the following line to /etc/pam.d/common-session so that the previous limit is applied upon restart: session required pam_limits.so Copy Add the following line to /etc/sysctl.conf to increase the amount of allowed inotify watchers per user. We specified a limit of 18192 here instead of just 10000 so that the root user will still have 8192 available inotify watches (the default value). fs.inotify.max_user_watches=18192 Copy Restart your system. Ensure that the new limits have been enforced by running: ulimit -Sn # Should return 10100 cat /proc/sys/fs/inotify/max_user_watches # Should return 18192 Copy Learn more on how to increase open file limits, or on how to increase the inotify watches. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards. Uninstall log forwarding To uninstall log forwarding capabilities, go to your logging.d directory, and remove files with the .yml extension that were originally added during the configuration process. Linux: /etc/newrelic-infra/logging.d/ Windows: C:\\Program Files\\New Relic\\newrelic-infra\\logging.d\\",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 353.48508,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward your <em>logs</em> using the <em>infrastructure</em> agent",
        "sections": "Sending the <em>infrastructure</em> agent&#x27;s <em>logs</em> to <em>New</em> <em>Relic</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " <em>management</em> capabilities to collect, process, explore, query, and alert on your <em>log</em> data. Basic process To forward your <em>logs</em> through our infrastructure <em>monitoring</em> agent: If you haven&#x27;t already, create a <em>New</em> <em>Relic</em> account. It&#x27;s free, forever. Verify the system requirements needed for configuring <em>logs</em>"
      },
      "id": "603e9df164441f6b6f4e8843"
    },
    {
      "sections": [
        "Forward your logs to New Relic",
        "How it works",
        "Get started with log forwarders",
        "Log forwarding options",
        "TCP endpoint",
        "Log API",
        "What's next?"
      ],
      "title": "Forward your logs to New Relic",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "b6e203ddc367d5a2b5e002916e49d34f4ba17a87",
      "image": "https://docs.newrelic.com/static/c3d5443b84a1e2b26a4767ce35fa58f3/e5166/new-relic-logs-in-context-diagram.jpg",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/enable-log-management-new-relic/",
      "published_at": "2022-01-08T02:40:08Z",
      "updated_at": "2022-01-04T07:02:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our log management capabilities help you to collect, process, explore, query, and alert on your log data. To get your logs into New Relic, you can: Use your existing log forwarding solution to collect your logs and extend the metadata that is forwarded to New Relic. Use our infrastructure agent as a lightweight data collector, without having to use additional software. Use our Log API to forward your logs via HTTP. Use syslog protocols to forward your logs via a TCP endpoint. How it works The following diagram shows the lifecycle of a log message for an app, from enrichment with APM agent metadata (contextual logging), to formatting and forwarding the log data to New Relic. This diagram illustrates the flow of log messages through New Relic. Standard log formatters transform log events into meaningful output (such as text files) that can be used by downstream people and processes. The NewRelicFormatter transforms log events into the JSON format expected by New Relic. These files contain log information and extended metadata. When you configure your log forwarder (our infrastructure monitoring agent, Fluentd, Logstash, etc.), you can also extend and enrich your log data. By configuring logs in context, the log enricher links the formatted log data with additional transaction information from your application or host. Now your log files are enriched with enhanced metadata and contextual logging data. Your log forwarder sends the files to our logging endpoint for processing. From there you can use our log management capabilities to view, query, set up alerts, and more in New Relic. Here is an example of logs for your host's UI. You can see logs in context of events for the selected time period, and drill down into detailed data for any of the highlighted attributes. To take advantage of even more capabilities, click Query logs from here to go directly to the Logs UI. Here is an example of a host's logs in context related to an event. Get started with log forwarders To forward your logs to New Relic with enriched metadata: If you don't have one already, create a New Relic account. It's free, forever. Have your New Relic account's license key. Install a compatible log forwarder. Ensure that outbound connectivity on TCP port 443 is allowed to the CIDR range that matches your region. Use the DNS name log-api.newrelic.com or log-api.eu.newrelic.com for configuration. For apps monitored by a New Relic APM agent, configure logs in context. Log forwarding options Use any of these solutions to forward your logs to New Relic. Recommended: Infrastructure monitoring agent Amazon: AWS CloudWatch plugin AWS FireLens plugin AWS Kinesis Firehose AWS Lambda for sending logs from S3 Microsoft: Azure ARM template Other log forwarding plugins: Fluent Bit plugin Fluentd plugin Google Cloud Platform Pub/Sub Heroku log streaming Kubernetes plugin Logstash plugin Vector plugin TCP endpoint In some situations you may not have log forwarders; for example, with CDNs, hardware devices, or managed services. You can use syslog protocols such as rsyslog and syslog-ng, and forward your logs to New Relic via a TCP endpoint. Log API If you prefer to connect to New Relic without installing a plugin, we offer an HTTP input integration. This option sends your monitored log data directly to New Relic via the Log API. What's next? After you enable your log forwarder, make the most of your data in New Relic with our log management capabilities: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data if you are forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards. For example, to query and manage your data partition rules, see our NerdGraph tutorial.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 314.55548,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward your <em>logs</em> to <em>New</em> <em>Relic</em>",
        "sections": "Forward your <em>logs</em> to <em>New</em> <em>Relic</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " directly to <em>New</em> <em>Relic</em> via the <em>Log</em> API. What&#x27;s next? After you <em>enable</em> your <em>log</em> forwarder, make the most of your data in <em>New</em> <em>Relic</em> with our <em>log</em> <em>management</em> capabilities: Explore the logging data across your platform with our <em>Logs</em> UI. See your <em>logs</em> in context of your app&#x27;s performance in the APM UI"
      },
      "id": "61571e0e28ccbcbc52f21431"
    }
  ],
  "/docs/logs/forward-logs/heroku-log-forwarding": [
    {
      "sections": [
        "Fluent Bit plugin for log forwarding",
        "Basic process",
        "Install the Fluent Bit plugin",
        "Tip",
        "Configure the Fluent Bit plugin",
        "Important",
        "Test the Fluent Bit plugin",
        "Associate logs with entities",
        "Optional: Configure plugin attributes",
        "View log data",
        "What's next?"
      ],
      "title": "Fluent Bit plugin for log forwarding",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "dd686f074bb80908ae9de19e543a3bae7d87a466",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/fluent-bit-plugin-log-forwarding/",
      "published_at": "2022-01-08T02:38:07Z",
      "updated_at": "2022-01-08T02:38:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your log data in New Relic. Forwarding your Fluent Bit logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Basic process We have published a container with the plugin installed. It serves as a base image to be used by our Kubernetes integration. We recommend you use this base image and layer your own custom configuration files. To forward your logs from Fluent Bit to New Relic: Make sure you have: A New Relic license key Fluent Bit 1.0 or higher (recommended), although v0.12 or higher is supported Fluent Bit Windows install directions can be found here Fluent Bit Linux install directions can be found here Install the Fluent Bit plugin. Configure the Fluent Bit plugin. Test the Fluent Bit plugin. Generate some traffic and wait a few minutes, then check your account for data. Install the Fluent Bit plugin To install the Fluent Bit plugin: Navigate to New Relic's Fluent Bit plugin repository on GitHub. From the repository page, clone or download the repository. Run the following command to build your plugin: cd newrelic-fluent-bit-output && make all Copy Store out_newrelic.so or out_newrelic_winXX.dll at a location that can be accessed by the fluent-bit daemon. Tip If you'd rather not compile the plugin yourself, you can download pre-compiled versions from our GitHub repository's releases page. Configure the Fluent Bit plugin Fluent Bit needs to know the location of the New Relic plugin and the New Relic license key to output data to New Relic. To configure your Fluent Bit plugin: Important Pay attention to white space when editing your config files. Be sure to use four spaces to indent and one space between keys and values. Locate or create a plugins.conf file in your plugins directory. In the plugins.conf file, add a reference to out_newrelic.so, adjacent to your fluent-bit.conf file: [PLUGINS] Path /PATH/TO/newrelic-fluent-bit-output/out_newrelic.so Copy In the fluent-bit.conf file, add the following line under the service block: [SERVICE] # This is the main configuration block for fluent bit. # Ensure the follow line exists somewhere in the SERVICE block Plugins_File plugins.conf Copy At the bottom of the fluent-bit.conf file, add the following to set up the input and output filters. Replace the placeholder text with your New Relic license key: [INPUT] Name tail Path /PATH/TO/YOUR/LOG/FILE [OUTPUT] Name newrelic Match * licenseKey YOUR_LICENSE_KEY # Optional maxBufferSize 256000 maxRecords 1024 Copy Restart your Fluent Bit instance with the following command: fluent-bit -c /PATH/TO/fluent-bit.conf Copy Test the Fluent Bit plugin To test if your Fluent Bit plugin is receiving input from a log file: Run the following command to append a test log message to your log file: echo \"test message\" >> /PATH/TO/YOUR/LOG/FILE Copy Search the New Relic Logs UI for test message. Associate logs with entities To associate a log line with an entity, such as an infrastructure host, add a FILTER block: [FILTER] Name modify Match * # Or specify a match Add entity.guids <Your Entity GUID ID> # Optional Add hostname <Your hostname> # Optional Copy For more options, see the Fluent Bit modify filter documentation and our documentation to forward your logs using the infrastructure agent. Optional: Configure plugin attributes Once you have installed and configured the Fluent Bit plugin, you can use the following attributes to configure how the plugin sends data to New Relic: Key Description licenseKey The New Relic license key. Use either licenseKey (recommended) or apiKey, not both. Default: none maxBufferSize The maximum size the payloads sent, in bytes. Default: 256000 maxRecords The maximum number of records to send at a time. Default: 1024 apiKey Deprecated. Takes a New Relic Insights insert key, but using the licenseKey attribute is preferred. Use either licenseKey or apiKey, not both. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 374.0484,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Fluent Bit plugin for <em>log</em> forwarding",
        "sections": "Associate <em>logs</em> with entities",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "If your <em>log</em> data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your <em>log</em> data in <em>New</em> <em>Relic</em>. Forwarding your Fluent Bit <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert"
      },
      "id": "603ea7ec28ccbce542eba791"
    },
    {
      "sections": [
        "Forward your logs using the infrastructure agent",
        "Basic process",
        "System requirements",
        "Important",
        "Install the infrastructure agent",
        "Configure the infrastructure agent",
        "Log forwarding parameters",
        "Name (required)",
        "Log source (required)",
        "file",
        "systemd",
        "syslog",
        "tcp",
        "winlog",
        "Optional configuration",
        "attributes",
        "attributes automatically inserted by the infrastructure agent",
        "pattern",
        "max_line_kb",
        "fluentbit",
        "Sample configuration file",
        "logging.d/sample.yaml",
        "View your log data",
        "Troubleshooting",
        "No log data",
        "No data appears when tailing a file",
        "No data appears when capturing via a Syslog socket",
        "No data appears using infrastructure agent proxy",
        "Sending the infrastructure agent's logs to New Relic",
        "Caution",
        "Fluent Bit does not start with the infra agent",
        "Runtime error on Windows",
        "Errors when tailing a large amount of log files (Linux)",
        "What's next?",
        "Uninstall log forwarding"
      ],
      "title": "Forward your logs using the infrastructure agent",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "a1deac410f0eedfb819348524a85a73bbb9d9daf",
      "image": "https://docs.newrelic.com/static/5d4352531871b652878b3cb0768e3f23/c1b63/demotron-host-logs-123021.png",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/forward-your-logs-using-infrastructure-agent/",
      "published_at": "2022-01-09T01:46:14Z",
      "updated_at": "2022-01-07T01:41:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can forward your logs to New Relic using our infrastructure monitoring agent. This makes all of your logging data available in one location and provides deeper visibility into both your application and your platform performance data. Forwarding your logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Basic process To forward your logs through our infrastructure monitoring agent: If you haven't already, create a New Relic account. It's free, forever. Verify the system requirements needed for configuring logs. Ensure you have installed the infrastructure agent, version 1.11.4 or higher. Create a logging.yml configuration file in the infrastructure agent's logging.d directory. Configure your log sources and other parameters. Generate some traffic and wait a few minutes, then check your account for data. Explore your log data in the Logs UI and benefit from the log attributes automatically inserted by the infrastructure agent. Here is an example of logs for your host's UI. You can see logs in context of events for the selected time period, and drill down into detailed data for any of the highlighted attributes. To take advantage of even more capabilities, click Query logs from here to go directly to the Logs UI. Here is an example of a host's logs in context related to an event. System requirements To use the log forwarder of the infrastructure agent, make sure you meet the following requirements: Infrastructure agent version 1.11.4 or higher OpenSSL library 1.1.0 or higher is required by Infrastructure agent starting from 1.16.4. Built-in support for ARM64 architecture on Linux systems (in example, AWS Graviton architecture) added in Infrastructure agent 1.26.0. Important The log forwarding feature is not supported with the Docker container for infrastructure monitoring agents. The log forwarding feature is compatible with the following operating systems: Operating system Supported version Amazon Linux Amazon Linux 2 CentOS Version 7 or higher Debian Version 9 (\"Stretch\") or higher Exception: Version 11 is not supported. Red Hat Enterprise Linux (RHEL) Version 7 or higher SUSE Linux Enterprise Server (SLES) Version 12 Ubuntu Versions 16.04.x, 18.04.x and 20.04.x (LTS versions) Windows Windows Server 2012, 2016, 2019, and 2022, and their service packs. Windows 10 Install the infrastructure agent Starting with version 1.11.4, the infrastructure agent can forward logs to New Relic. To install and run the agent, use a package manager (Linux) or the MSI installer (Windows). Important The log forwarding feature is not included when the infrastructure agent is implemented using Linux tarball or Windows ZIP installations. To use the following links, make sure you are logged to your New Relic account. Amazon Linux CentOS Debian RHEL SLES Ubuntu Windows If you don't have a New Relic account yet, or if you prefer to follow the procedure manually, see our tutorial to install the package manager. Configure the infrastructure agent Configuration files describe which log sources are forwarded. Our infrastructure agent uses .yml files to configure logging. You can add as many config files as you want. To add a new configuration file for the log forwarding feature: Navigate to the log forwarder configuration folder: Linux: /etc/newrelic-infra/logging.d/ Windows: C:\\Program Files\\New Relic\\newrelic-infra\\logging.d\\ Create a logging.yml configuration file, and add the parameters you need. The logging.d directory has various .yml.example files you can use as a reference or starting point. The agent automatically processes new configuration files without having to restart the infrastructure monitoring service. The only exception to this is when configuring a custom Fluent Bit configuration. Log forwarding parameters The infrastructure log forwarding .yml config supports the following parameters: Name (required) To start, define a name of the log or logs you want to forward to New Relic. Log source (required) What you use for the log source will depend on where you want to forward your logs from. Available options include: file Path to the log file or files. The agent tracks changes on the log files in a way similar to tail -f shell. Example: logs: - name: example-log file: /var/log/example.log # Path to a single log file - name: example-log-two file: /var/log/example-two.log # Path to another single log file Copy The file parameter can point to a specific log file or multiple files by using wildcards applied to names and extensions; for example, /logs/*.log. You can use wildcards in place of directories in a file path, which can be used to tail files located in different directories. Example: logs: - name: docker-logs file: /var/lib/docker/containers/*/*.log # Path to multiple folders and files Copy Important Use of wildcards may significantly increase the number of file descriptors and inotify watches the Fluent Bit process keeps open, which can interfere with log collection if the host's file descriptor limit is reached. Tailing a large number of files may require you to increase the maximum number of file descriptors and inotify watchers allowed by the operating system. Please refer to Errors when tailing a large amount of log files for more details on how to increase them. systemd Use the systemd parameter to forward log messages that are collected by the journald daemon in Linux environments. This input type requires the agent to run in root mode. Example: logs: - name: systemd-example systemd: cupsd Copy syslog Syslog data source. Parameters: uri: Syslog socket. Format varies depending on the protocol: TCP/UDP network sockets: [tcp/udp]://LISTEN_ADDRESS:PORT Unix domain sockets: unix_[tcp/udp]:// + /socket/path parser: Syslog parser. Default is rfc3164. Use rfc5424 if your messages include fractional seconds. Note: rfc3164 currently does not work on SuSE. unix_permissions: default is 0644 for domain sockets; this limits entries to processes running as root. You can use 0666 to listen for non-root processes, at your own risk. When running the agent in privileged mode, ports and sockets must be available or owned by nri-agent, with 0666 file permissions, so that other processes can write logs to the sockets. logs: # TCP network socket - name: syslog-tcp-test syslog: uri: tcp://0.0.0.0:5140 # Use the tcp://LISTEN_ADDRESS:PORT format parser: rfc5424 # Default syslog parser is rfc3164 # UDP network socket - name: syslog-udp-test syslog: uri: udp://0.0.0.0:6140 # Use the udp://LISTEN_ADDRESS:PORT format max_line_kb: 35 # Unix TCP domain socket - name: syslog-unix-tcp-test syslog: uri: unix_tcp:///var/unix-tcp-socket-test unix_permissions: 0666 # Default is 0644. Change at your own risk # Unix UDP domain socket - name: syslog-unix-udp-test syslog: uri: unix_udp:///var/unix-udp-socket-test parser: rfc5424 Copy tcp Logs retrieved over TCP connections. Parameters: uri: TCP/IP socket to listen for incoming data. The URI format is tcp://LISTEN_ADDRESS:PORT format: format of the data. It can be json or none. separator: If format: none is used, you can define a separator string for splitting records (default: \\n). logs: - name: tcp-simple-test tcp: uri: tcp://0.0.0.0:1234 # Use the tcp://LISTEN_ADDRESS:PORT format format: none # Raw text - this is default for 'tcp' separator: \\t # String for separating raw text entries max_line_kb: 32 - name: tcp-json-test tcp: uri: tcp://0.0.0.0:2345 # Use the tcp://LISTEN_ADDRESS:PORT format format: json Copy winlog Collect events from Windows log channels. Parameters: channel: name of the channel logs will be collected from. collect-eventids: a list of Windows Event IDs to be collected and forwarded to New Relic. Event ID ranges are supported. exclude-eventids: a list of Windows Event IDs to be excluded from collection. Event ID ranges are supported. All events are collected from the specified channel by default. Configure the collect-eventids and exclude-eventids sections to avoid sending unwanted logs to your New Relic account. Add event IDs or ranges to collect-eventids or exclude-eventids to forward or drop specific events. exclude-eventids takes precedence over collect-eventids if the same event ID is present in both sections. Example: logs: # Winlog log ingestion with eventId filters. - name: windows-security winlog: channel: Security collect-eventids: - 4624 - 4265 - 4700-4800 exclude-eventids: - 4735 # entries for the application, system, powershell, and SCOM channels - name: windows-application winlog: channel: Application - name: windows-system winlog: channel: System - name: windows-pshell winlog: channel: Windows Powershell - name: scom winlog: channel: Operations Manager # Entry for Windows Defender Logs - name: windows-defender winlog: channel: Microsoft-Windows-Windows Defender/Operational # Entry for Windows Clustering Logs - name: windows-clustering winlog: channel: Microsoft-Windows-FailoverClustering/Operational # Entry for IIS logs with logtype attribute for automatic parsing - name: iis-log file: C:\\inetpub\\logs\\LogFiles\\w3svc.log attributes: logtype: iis_w3c Copy Optional configuration The following configuration parameters are not required but are still recommended. attributes List of custom attributes specified as key-value pairs that can be used to send additional data with the logs which you can then query. The attributes configuration parameter can be used with any log source. One common use of the attributes configuration parameter is to specify the logtype attribute. This attribute allows leveraging one of the built-in parsing rules supported by New Relic Logs. Example: logs: - name: example-file-attributes file: /var/log/example.log attributes: logtype: nginx region: example-us-02 team: A-team - name: example-tcp-attributes tcp: uri: tcp://0.0.0.0:2345 format: json attributes: logtype: nginx region: example-us-02 team: B-team Copy attributes automatically inserted by the infrastructure agent The infrastructure agent automatically inserts log attributes for your convenience. Some of them are inserted for any log record, while others depend on the configuration parameters you used while setting up the log forwarder. Attribute name Description entity.guids Always inserted. The infrastructure agent inserts the Entity GUID assigned by New Relic to identify the host where it's running. It is available in the entity.guids field. Note: If the captured logs belong to an application instrumented using APM, the entity.guids field contains both the entity GUID of infrastructure, as well as the GUID of APM, separated by a pipe ( | ) delimiter. fb.input Always inserted. The underlying Fluent Bit input plugin type used to capture the logs. Currently its values are tail, systemd, winlog, syslog, and tcp. filePath Inserted when sing the file input type. Absolute file path of the file being monitored. hostname Always inserted. The hostname of the machine/VM/container executing the infrastructure agent. plugin.type Always inserted. Indicates the utility used to capture the logs. In this case, it is the infrastructure agent itself, so this attribute always has the value nri-agent. pattern Regular expression for filtering records. Only supported for the tail, systemd, syslog, and tcp (only with format none) sources. This field works in a way similar to grep -E in Unix systems. For example, for a given file being captured, you can filter for records containing either WARN or ERROR using: - name: only-records-with-warn-and-error file: /var/log/logFile.log pattern: WARN|ERROR Copy No filtering is applied by default. max_line_kb Maximum size of log entries/lines in KB. If log entries exceed the limit, they are skipped. Default is 128. fluentbit External Fluent Bit configuration and parser files. If defined, they are merged with the existing configuration and parser files generated by the infrastructure agent. The infrastructure agent processes the configuration files located in the logging.d directory and will generate a run-time Fluent Bit configuration file that contains the appropriate [INPUT], [FILTER] and [OUTPUT] sections. Optionally, it will also declare an @INCLUDE in case you provided an external Fluent Bit configuration file via the fluentbit option. The runtime file does not define a [SERVICE] section, leaving all default Fluent Bit configuration values. You can still override Fluent Bit's default settings by defining your own [SERVICE] section in your external Fluent Bit configuration file and include it via the fluentbit option. Parameters: config_file: path to an existing Fluent Bit configuration file. Note that any overlapping source results in duplicate messages in New Relic Logs. parsers_file: path to an existing Fluent Bit parsers file. The following parser names are reserved: rfc3164, rfc3164-local and rfc5424. Sample configuration file Here is an example of a logging.d/ configuration file in YAML format. For more configuration examples, see the infrastructure agent repository. logging.d/sample.yaml # Remember to only use spaces for indentation logs: # Example of 'file' source - name: file-with-attributes file: /var/log/test.log # Path to a single file or pattern attributes: # You can use custom attributes to enrich your data logtype: nginx team: The A Team pattern: Error # Regular expression to filter log entries # Example of 'systemd' source (Linux only) - name: systemd-example systemd: cupsd # Examples of 'syslog' source, one per protocol # TCP network socket - name: syslog-tcp-test syslog: uri: tcp://0.0.0.0:5140 # Use the tcp://LISTEN_ADDRESS:PORT format parser: rfc5424 # Default syslog parser is rfc3164 # UDP network socket - name: syslog-udp-test syslog: uri: udp://0.0.0.0:6140 # Use the udp://LISTEN_ADDRESS:PORT format max_line_kb: 35 # Paths for Unix sockets are defined by combining protocol and path: # unix_udp:// + /path/socket - for example, unix_udp:///tmp/socket # Unix TCP domain socket - name: syslog-unix-tcp-test syslog: uri: unix_tcp:///var/unix-tcp-socket-test unix_permissions: 0666 # Default is 0644. Change at your own risk # Unix UDP domain socket - name: syslog-unix-udp-test syslog: uri: unix_udp:///var/unix-udp-socket-test parser: rfc5424 # Examples of 'tcp' source for formats 'none' and 'json' - name: tcp-simple-test tcp: uri: tcp://0.0.0.0:1234 # Use the tcp://LISTEN_ADDRESS:PORT format format: none # Raw text - this is default for 'tcp' separator: \\t # String for separating raw text entries attributes: # You can add custom attributes to any source of logs tcpFormat: none someOtherAttribute: associatedValue max_line_kb: 32 - name: tcp-json-test tcp: uri: tcp://0.0.0.0:2345 # Use the tcp://LISTEN_ADDRESS:PORT format format: json attributes: tcpFormat: json yetAnotherAttribute: 12345 # Example of Fluent Bit configuration import - name: fluentbit-import fluentbit: config_file: /path/to/fluentbit.config parsers_file: /path/to/fluentbit/parsers.conf Copy View your log data If everything is configured correctly and your data is being collected, you should see logs data in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy Troubleshooting If you encounter problems with configuring your log forwarder, try these troubleshooting tips. No log data If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. No data appears when tailing a file The log forwarding feature requires the agent to have permission to read the data sources. When running the infrastructure agent in privileged or non-privileged modes, make sure that the log files you want to forward (and any intermediary directory in its path) are readable by the user running nri-agent. Example: Check file access under Linux Let's check whether the file /var/log/restrictedLogs/logFile.log can be monitored by the nri-agent user. In Linux, you can do a quick check with the namei command: sudo -u nri-agent namei -ml /var/log/restrictedLogs/logFile.log f: /var/log/restrictedLogs/logFile.log drwxr-xr-x root root / drwxr-xr-x root root var drwxrwxr-x root syslog log drwxr--r-- root root restrictedLogs logFile.log - No such file or directory Copy This command failed because the file is not visible to the nri-agent user. By inspecting the previous output, we can detect that the restrictedLogs directory is missing the execution flag for others. To fix this, execute: sudo chmod 755 /var/log/restrictedLogs Copy And then check for file access again: # sudo -u nri-agent namei -ml /var/log/restrictedLogs/logFile.log f: /var/log/restrictedLogs/logFile.log drwxr-xr-x root root / drwxr-xr-x root root var drwxrwxr-x root syslog log drwxr-xr-x root root restrictedLogs -rw-r----- vagrant vagrant logFile.log Copy The file is now visible to the nri-agent user. You must ensure that the file is also readable by the nri-agent user. To check this, use: # sudo -u nri-agent head /var/log/restrictedLogs/logFile.log head: cannot open '/var/log/restrictedLogs/logFile.log' for reading: Permission denied Copy In this example, the file is missing the read rights for the others group (users other than vagrant and the vagrant user group). You could fix this by granting read permissions to others, but the application could change these permissions upon restart. To avoid this, a better approach is to add the nri-agent user to the vagrant user group. No data appears when capturing via a Syslog socket The log forwarding feature requires that the agent has permission to read the data sources. When running the infrastructure agent in privileged or non-privileged modes: If you're using Unix domain socket files, make sure that the nri-agent user can access these files (please refer to the previous section) and that they have read and write permissions (666) so that other users than nri-agent can write to them. If you're using IP sockets, ensure that the port that you are using is not a system reserved one (like port 80, for example). If no data appears after you enable log management, follow standard log management troubleshooting procedures. No data appears using infrastructure agent proxy As explained in the infrastructure agent configuration guidelines, the proxy parameter must use either HTTP or HTTPS and be in the form https://user:password@hostname:port. The agent can parse the parameter without the HTTP or HTTPS, but the log forwarder cannot. You will see an error like the following in the agent verbose logs: [ERROR] building HTTP transport: parse \\\"hostname:port\\\": first path segment in URL cannot contain colon Copy To solve this problem, check your newrelic-infra.yml file, and ensure the proxy parameter adheres to this form. If you're using caBundleFile or caBundleDir in order to specify any certificate, we recommend to follow the below rules for each OS: Linux For HTTP proxies you don't need to setup any certificates. The plugin loads the system certificates and New Relic sends logs into the logging endpoint. However, you can specify the proxy self-signed certificate (PEM file) using either the caBundleFile or caBundleDir parameters. Windows For HTTP proxies you don't need to setup any certificates. The plugin loads the system certificates. For HTTPS, you can configure it in one of the following ways: Import the proxy certificate to the system pool (Recommended) Import the proxy self-signed certificate (PEM file) by using the MMC tool. Refer to this link, and in Step 2 ensure to import it in your Trusted Root Certification Authorities, instead of in the Intermediate Certification Authorities. Using the caBundleFile and caBundleDir parameters On Windows, we cannot load both the certificates from the system certificate pool and the ones specified with the caBundleFile caBundleDir parameters. So, if you are using caBundleFile or caBundleDir, ensure that the following certificates are placed in the same PEM file (when using caBundleFile) or in the same directory (when using caBundleDir): The Proxy certificate (because it's an HTTPS proxy). The Logging Endpoint certificate (eg. https://log-api.newrelic.com/log/v1). The Infrastructure Agent certificate (eg. https://infra-api.newrelic.com). You can check the certificates by running: # openssl s_client -connect log-api.newrelic.com:443 -servername log-api.newrelic.com Copy Sending the infrastructure agent's logs to New Relic You can configure the infrastructure agent to send its own logs to New Relic. This is useful for troubleshooting issues with log forwarding, the agent, or when contacting support. To forward the infrastructure agent logs to New Relic: Edit your newrelic-infra.yml file. Enable agent logging in troubleshooting mode by adding verbose: 3. On Windows and systems that don't use systemd or where journald is inaccessible, verbose:3 causes the agent to write the logs on the disk. Revert to verbose:0 to prevent this. (Recommended): Enable agent logging in JSON format to log_format: json. Restart the agent to load the new settings. This configuration sets up the agent in troubleshooting mode, but the log forwarder (based on Fluent Bit) will continue in a non-verbose mode. Sometimes you can have issues with the log forwarder itself. For example, there may be problems accessing a specific channel when shipping Windows log events or when accessing a particular log file. In these situations, you can also enable the verbose mode for the log forwarder: Set verbose to a value other than 0. Add the configuration option: trace: [\"log.fw\"]. Caution Check whether you are using the [fluentbit] option. When setting verbose: 3 and trace: [\"log.fw\"], ensure that you don't define any [OUTPUT] section pointing to stdout in an external Fluent Bit configuration file, Fluent Bit does not start with the infra agent Important Fluent Bit's tail plugin does not support network drives. For Linux versions prior to 2016, you may need to update the OpenSSL library to 1.1.0 (or higher). To check if you have this problem: See if infra-agent has started Fluent Bit by running: ps -aux | grep td-agent-bit Copy If it isn't running go to /var/db/newrelic-infra/newrelic-integrations/logging and run: ./fluent-bit -i systemd -o stdout Copy If you get the following error, update OpenSSL to 1.1.0 or higher: error while loading shared libraries: libssl.so.1.1: cannot open shared object file: No such file or directory Copy Runtime error on Windows One of the following error messages may appear when enabling log forwarding on Windows: The code execution cannot proceed because VCRUNTIME140.dll was not found. Copy OR error=\"exit status 3221225781\" process=log-forwarder Copy This is caused by a missing DLL. To solve the issue, install the Microsoft Visual C++ Redistributable as applicable: x64 x86 Errors when tailing a large amount of log files (Linux) It's common to face either one of the following error messages when attempting to tail a large amount of files: Too many open files The user limit on the total number of inotify watches was reached or the kernel failed to allocate a needed resource The operating system defines a maximum amount of allocatable file descriptors (typically 1024 by default), and a maximum amount of allocatable inotify watches (typically 8192 by default). Any process attempting to go above these limits will fail, returning one of the errors above. The underlying technology we use to forward logs, Fluent Bit, opens one file descriptor and sets an inotify watch for each file you configure to be forwarded. Moreover, at the time of writing this section, Fluent Bit uses an extra set of 32 file descriptors for its normal operation, with another extra file descriptor when it shuts down. Therefore, to capture a large amount of files you need to ensure that both the file descriptor and inotify watch limits are slightly greater than the amount of log files you wish to tail. The following instructions summarize how to increase these limits if you want to tail 10,000 log files. Also, it assumes the infrastructure agent is installed in root running mode, and therefore must be run using the root user. Check which is the current hard limit for the amount of file descriptors per process. Typically, this limit should be quite high and should not need to be modified. ulimit -Hn Copy Add the following line to /etc/security/limits.conf. We specified a limit of 10100 here instead of just 10000 to allow Fluent Bit to allocate the extra file descriptors it may need to work. root soft nofile 10100 # replace root by nri-agent for non-root (privileged and unprivileged) installations Copy Add the following line to /etc/pam.d/common-session so that the previous limit is applied upon restart: session required pam_limits.so Copy Add the following line to /etc/sysctl.conf to increase the amount of allowed inotify watchers per user. We specified a limit of 18192 here instead of just 10000 so that the root user will still have 8192 available inotify watches (the default value). fs.inotify.max_user_watches=18192 Copy Restart your system. Ensure that the new limits have been enforced by running: ulimit -Sn # Should return 10100 cat /proc/sys/fs/inotify/max_user_watches # Should return 18192 Copy Learn more on how to increase open file limits, or on how to increase the inotify watches. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards. Uninstall log forwarding To uninstall log forwarding capabilities, go to your logging.d directory, and remove files with the .yml extension that were originally added during the configuration process. Linux: /etc/newrelic-infra/logging.d/ Windows: C:\\Program Files\\New Relic\\newrelic-infra\\logging.d\\",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 353.44556,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward your <em>logs</em> using the <em>infrastructure</em> agent",
        "sections": "Sending the <em>infrastructure</em> agent&#x27;s <em>logs</em> to <em>New</em> <em>Relic</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " <em>management</em> capabilities to collect, process, explore, query, and alert on your <em>log</em> data. Basic process To forward your <em>logs</em> through our infrastructure <em>monitoring</em> agent: If you haven&#x27;t already, create a <em>New</em> <em>Relic</em> account. It&#x27;s free, forever. Verify the system requirements needed for configuring <em>logs</em>"
      },
      "id": "603e9df164441f6b6f4e8843"
    },
    {
      "sections": [
        "Forward your logs to New Relic",
        "How it works",
        "Get started with log forwarders",
        "Log forwarding options",
        "TCP endpoint",
        "Log API",
        "What's next?"
      ],
      "title": "Forward your logs to New Relic",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "b6e203ddc367d5a2b5e002916e49d34f4ba17a87",
      "image": "https://docs.newrelic.com/static/c3d5443b84a1e2b26a4767ce35fa58f3/e5166/new-relic-logs-in-context-diagram.jpg",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/enable-log-management-new-relic/",
      "published_at": "2022-01-08T02:40:08Z",
      "updated_at": "2022-01-04T07:02:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our log management capabilities help you to collect, process, explore, query, and alert on your log data. To get your logs into New Relic, you can: Use your existing log forwarding solution to collect your logs and extend the metadata that is forwarded to New Relic. Use our infrastructure agent as a lightweight data collector, without having to use additional software. Use our Log API to forward your logs via HTTP. Use syslog protocols to forward your logs via a TCP endpoint. How it works The following diagram shows the lifecycle of a log message for an app, from enrichment with APM agent metadata (contextual logging), to formatting and forwarding the log data to New Relic. This diagram illustrates the flow of log messages through New Relic. Standard log formatters transform log events into meaningful output (such as text files) that can be used by downstream people and processes. The NewRelicFormatter transforms log events into the JSON format expected by New Relic. These files contain log information and extended metadata. When you configure your log forwarder (our infrastructure monitoring agent, Fluentd, Logstash, etc.), you can also extend and enrich your log data. By configuring logs in context, the log enricher links the formatted log data with additional transaction information from your application or host. Now your log files are enriched with enhanced metadata and contextual logging data. Your log forwarder sends the files to our logging endpoint for processing. From there you can use our log management capabilities to view, query, set up alerts, and more in New Relic. Here is an example of logs for your host's UI. You can see logs in context of events for the selected time period, and drill down into detailed data for any of the highlighted attributes. To take advantage of even more capabilities, click Query logs from here to go directly to the Logs UI. Here is an example of a host's logs in context related to an event. Get started with log forwarders To forward your logs to New Relic with enriched metadata: If you don't have one already, create a New Relic account. It's free, forever. Have your New Relic account's license key. Install a compatible log forwarder. Ensure that outbound connectivity on TCP port 443 is allowed to the CIDR range that matches your region. Use the DNS name log-api.newrelic.com or log-api.eu.newrelic.com for configuration. For apps monitored by a New Relic APM agent, configure logs in context. Log forwarding options Use any of these solutions to forward your logs to New Relic. Recommended: Infrastructure monitoring agent Amazon: AWS CloudWatch plugin AWS FireLens plugin AWS Kinesis Firehose AWS Lambda for sending logs from S3 Microsoft: Azure ARM template Other log forwarding plugins: Fluent Bit plugin Fluentd plugin Google Cloud Platform Pub/Sub Heroku log streaming Kubernetes plugin Logstash plugin Vector plugin TCP endpoint In some situations you may not have log forwarders; for example, with CDNs, hardware devices, or managed services. You can use syslog protocols such as rsyslog and syslog-ng, and forward your logs to New Relic via a TCP endpoint. Log API If you prefer to connect to New Relic without installing a plugin, we offer an HTTP input integration. This option sends your monitored log data directly to New Relic via the Log API. What's next? After you enable your log forwarder, make the most of your data in New Relic with our log management capabilities: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data if you are forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards. For example, to query and manage your data partition rules, see our NerdGraph tutorial.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 314.5203,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward your <em>logs</em> to <em>New</em> <em>Relic</em>",
        "sections": "Forward your <em>logs</em> to <em>New</em> <em>Relic</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " directly to <em>New</em> <em>Relic</em> via the <em>Log</em> API. What&#x27;s next? After you <em>enable</em> your <em>log</em> forwarder, make the most of your data in <em>New</em> <em>Relic</em> with our <em>log</em> <em>management</em> capabilities: Explore the logging data across your platform with our <em>Logs</em> UI. See your <em>logs</em> in context of your app&#x27;s performance in the APM UI"
      },
      "id": "61571e0e28ccbcbc52f21431"
    }
  ],
  "/docs/logs/forward-logs/kubernetes-plugin-log-forwarding": [
    {
      "sections": [
        "Fluent Bit plugin for log forwarding",
        "Basic process",
        "Install the Fluent Bit plugin",
        "Tip",
        "Configure the Fluent Bit plugin",
        "Important",
        "Test the Fluent Bit plugin",
        "Associate logs with entities",
        "Optional: Configure plugin attributes",
        "View log data",
        "What's next?"
      ],
      "title": "Fluent Bit plugin for log forwarding",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "dd686f074bb80908ae9de19e543a3bae7d87a466",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/fluent-bit-plugin-log-forwarding/",
      "published_at": "2022-01-08T02:38:07Z",
      "updated_at": "2022-01-08T02:38:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your log data in New Relic. Forwarding your Fluent Bit logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Basic process We have published a container with the plugin installed. It serves as a base image to be used by our Kubernetes integration. We recommend you use this base image and layer your own custom configuration files. To forward your logs from Fluent Bit to New Relic: Make sure you have: A New Relic license key Fluent Bit 1.0 or higher (recommended), although v0.12 or higher is supported Fluent Bit Windows install directions can be found here Fluent Bit Linux install directions can be found here Install the Fluent Bit plugin. Configure the Fluent Bit plugin. Test the Fluent Bit plugin. Generate some traffic and wait a few minutes, then check your account for data. Install the Fluent Bit plugin To install the Fluent Bit plugin: Navigate to New Relic's Fluent Bit plugin repository on GitHub. From the repository page, clone or download the repository. Run the following command to build your plugin: cd newrelic-fluent-bit-output && make all Copy Store out_newrelic.so or out_newrelic_winXX.dll at a location that can be accessed by the fluent-bit daemon. Tip If you'd rather not compile the plugin yourself, you can download pre-compiled versions from our GitHub repository's releases page. Configure the Fluent Bit plugin Fluent Bit needs to know the location of the New Relic plugin and the New Relic license key to output data to New Relic. To configure your Fluent Bit plugin: Important Pay attention to white space when editing your config files. Be sure to use four spaces to indent and one space between keys and values. Locate or create a plugins.conf file in your plugins directory. In the plugins.conf file, add a reference to out_newrelic.so, adjacent to your fluent-bit.conf file: [PLUGINS] Path /PATH/TO/newrelic-fluent-bit-output/out_newrelic.so Copy In the fluent-bit.conf file, add the following line under the service block: [SERVICE] # This is the main configuration block for fluent bit. # Ensure the follow line exists somewhere in the SERVICE block Plugins_File plugins.conf Copy At the bottom of the fluent-bit.conf file, add the following to set up the input and output filters. Replace the placeholder text with your New Relic license key: [INPUT] Name tail Path /PATH/TO/YOUR/LOG/FILE [OUTPUT] Name newrelic Match * licenseKey YOUR_LICENSE_KEY # Optional maxBufferSize 256000 maxRecords 1024 Copy Restart your Fluent Bit instance with the following command: fluent-bit -c /PATH/TO/fluent-bit.conf Copy Test the Fluent Bit plugin To test if your Fluent Bit plugin is receiving input from a log file: Run the following command to append a test log message to your log file: echo \"test message\" >> /PATH/TO/YOUR/LOG/FILE Copy Search the New Relic Logs UI for test message. Associate logs with entities To associate a log line with an entity, such as an infrastructure host, add a FILTER block: [FILTER] Name modify Match * # Or specify a match Add entity.guids <Your Entity GUID ID> # Optional Add hostname <Your hostname> # Optional Copy For more options, see the Fluent Bit modify filter documentation and our documentation to forward your logs using the infrastructure agent. Optional: Configure plugin attributes Once you have installed and configured the Fluent Bit plugin, you can use the following attributes to configure how the plugin sends data to New Relic: Key Description licenseKey The New Relic license key. Use either licenseKey (recommended) or apiKey, not both. Default: none maxBufferSize The maximum size the payloads sent, in bytes. Default: 256000 maxRecords The maximum number of records to send at a time. Default: 1024 apiKey Deprecated. Takes a New Relic Insights insert key, but using the licenseKey attribute is preferred. Use either licenseKey or apiKey, not both. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 374.0484,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Fluent Bit plugin for <em>log</em> forwarding",
        "sections": "Associate <em>logs</em> with entities",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "If your <em>log</em> data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your <em>log</em> data in <em>New</em> <em>Relic</em>. Forwarding your Fluent Bit <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert"
      },
      "id": "603ea7ec28ccbce542eba791"
    },
    {
      "sections": [
        "Forward your logs using the infrastructure agent",
        "Basic process",
        "System requirements",
        "Important",
        "Install the infrastructure agent",
        "Configure the infrastructure agent",
        "Log forwarding parameters",
        "Name (required)",
        "Log source (required)",
        "file",
        "systemd",
        "syslog",
        "tcp",
        "winlog",
        "Optional configuration",
        "attributes",
        "attributes automatically inserted by the infrastructure agent",
        "pattern",
        "max_line_kb",
        "fluentbit",
        "Sample configuration file",
        "logging.d/sample.yaml",
        "View your log data",
        "Troubleshooting",
        "No log data",
        "No data appears when tailing a file",
        "No data appears when capturing via a Syslog socket",
        "No data appears using infrastructure agent proxy",
        "Sending the infrastructure agent's logs to New Relic",
        "Caution",
        "Fluent Bit does not start with the infra agent",
        "Runtime error on Windows",
        "Errors when tailing a large amount of log files (Linux)",
        "What's next?",
        "Uninstall log forwarding"
      ],
      "title": "Forward your logs using the infrastructure agent",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "a1deac410f0eedfb819348524a85a73bbb9d9daf",
      "image": "https://docs.newrelic.com/static/5d4352531871b652878b3cb0768e3f23/c1b63/demotron-host-logs-123021.png",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/forward-your-logs-using-infrastructure-agent/",
      "published_at": "2022-01-09T01:46:14Z",
      "updated_at": "2022-01-07T01:41:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can forward your logs to New Relic using our infrastructure monitoring agent. This makes all of your logging data available in one location and provides deeper visibility into both your application and your platform performance data. Forwarding your logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Basic process To forward your logs through our infrastructure monitoring agent: If you haven't already, create a New Relic account. It's free, forever. Verify the system requirements needed for configuring logs. Ensure you have installed the infrastructure agent, version 1.11.4 or higher. Create a logging.yml configuration file in the infrastructure agent's logging.d directory. Configure your log sources and other parameters. Generate some traffic and wait a few minutes, then check your account for data. Explore your log data in the Logs UI and benefit from the log attributes automatically inserted by the infrastructure agent. Here is an example of logs for your host's UI. You can see logs in context of events for the selected time period, and drill down into detailed data for any of the highlighted attributes. To take advantage of even more capabilities, click Query logs from here to go directly to the Logs UI. Here is an example of a host's logs in context related to an event. System requirements To use the log forwarder of the infrastructure agent, make sure you meet the following requirements: Infrastructure agent version 1.11.4 or higher OpenSSL library 1.1.0 or higher is required by Infrastructure agent starting from 1.16.4. Built-in support for ARM64 architecture on Linux systems (in example, AWS Graviton architecture) added in Infrastructure agent 1.26.0. Important The log forwarding feature is not supported with the Docker container for infrastructure monitoring agents. The log forwarding feature is compatible with the following operating systems: Operating system Supported version Amazon Linux Amazon Linux 2 CentOS Version 7 or higher Debian Version 9 (\"Stretch\") or higher Exception: Version 11 is not supported. Red Hat Enterprise Linux (RHEL) Version 7 or higher SUSE Linux Enterprise Server (SLES) Version 12 Ubuntu Versions 16.04.x, 18.04.x and 20.04.x (LTS versions) Windows Windows Server 2012, 2016, 2019, and 2022, and their service packs. Windows 10 Install the infrastructure agent Starting with version 1.11.4, the infrastructure agent can forward logs to New Relic. To install and run the agent, use a package manager (Linux) or the MSI installer (Windows). Important The log forwarding feature is not included when the infrastructure agent is implemented using Linux tarball or Windows ZIP installations. To use the following links, make sure you are logged to your New Relic account. Amazon Linux CentOS Debian RHEL SLES Ubuntu Windows If you don't have a New Relic account yet, or if you prefer to follow the procedure manually, see our tutorial to install the package manager. Configure the infrastructure agent Configuration files describe which log sources are forwarded. Our infrastructure agent uses .yml files to configure logging. You can add as many config files as you want. To add a new configuration file for the log forwarding feature: Navigate to the log forwarder configuration folder: Linux: /etc/newrelic-infra/logging.d/ Windows: C:\\Program Files\\New Relic\\newrelic-infra\\logging.d\\ Create a logging.yml configuration file, and add the parameters you need. The logging.d directory has various .yml.example files you can use as a reference or starting point. The agent automatically processes new configuration files without having to restart the infrastructure monitoring service. The only exception to this is when configuring a custom Fluent Bit configuration. Log forwarding parameters The infrastructure log forwarding .yml config supports the following parameters: Name (required) To start, define a name of the log or logs you want to forward to New Relic. Log source (required) What you use for the log source will depend on where you want to forward your logs from. Available options include: file Path to the log file or files. The agent tracks changes on the log files in a way similar to tail -f shell. Example: logs: - name: example-log file: /var/log/example.log # Path to a single log file - name: example-log-two file: /var/log/example-two.log # Path to another single log file Copy The file parameter can point to a specific log file or multiple files by using wildcards applied to names and extensions; for example, /logs/*.log. You can use wildcards in place of directories in a file path, which can be used to tail files located in different directories. Example: logs: - name: docker-logs file: /var/lib/docker/containers/*/*.log # Path to multiple folders and files Copy Important Use of wildcards may significantly increase the number of file descriptors and inotify watches the Fluent Bit process keeps open, which can interfere with log collection if the host's file descriptor limit is reached. Tailing a large number of files may require you to increase the maximum number of file descriptors and inotify watchers allowed by the operating system. Please refer to Errors when tailing a large amount of log files for more details on how to increase them. systemd Use the systemd parameter to forward log messages that are collected by the journald daemon in Linux environments. This input type requires the agent to run in root mode. Example: logs: - name: systemd-example systemd: cupsd Copy syslog Syslog data source. Parameters: uri: Syslog socket. Format varies depending on the protocol: TCP/UDP network sockets: [tcp/udp]://LISTEN_ADDRESS:PORT Unix domain sockets: unix_[tcp/udp]:// + /socket/path parser: Syslog parser. Default is rfc3164. Use rfc5424 if your messages include fractional seconds. Note: rfc3164 currently does not work on SuSE. unix_permissions: default is 0644 for domain sockets; this limits entries to processes running as root. You can use 0666 to listen for non-root processes, at your own risk. When running the agent in privileged mode, ports and sockets must be available or owned by nri-agent, with 0666 file permissions, so that other processes can write logs to the sockets. logs: # TCP network socket - name: syslog-tcp-test syslog: uri: tcp://0.0.0.0:5140 # Use the tcp://LISTEN_ADDRESS:PORT format parser: rfc5424 # Default syslog parser is rfc3164 # UDP network socket - name: syslog-udp-test syslog: uri: udp://0.0.0.0:6140 # Use the udp://LISTEN_ADDRESS:PORT format max_line_kb: 35 # Unix TCP domain socket - name: syslog-unix-tcp-test syslog: uri: unix_tcp:///var/unix-tcp-socket-test unix_permissions: 0666 # Default is 0644. Change at your own risk # Unix UDP domain socket - name: syslog-unix-udp-test syslog: uri: unix_udp:///var/unix-udp-socket-test parser: rfc5424 Copy tcp Logs retrieved over TCP connections. Parameters: uri: TCP/IP socket to listen for incoming data. The URI format is tcp://LISTEN_ADDRESS:PORT format: format of the data. It can be json or none. separator: If format: none is used, you can define a separator string for splitting records (default: \\n). logs: - name: tcp-simple-test tcp: uri: tcp://0.0.0.0:1234 # Use the tcp://LISTEN_ADDRESS:PORT format format: none # Raw text - this is default for 'tcp' separator: \\t # String for separating raw text entries max_line_kb: 32 - name: tcp-json-test tcp: uri: tcp://0.0.0.0:2345 # Use the tcp://LISTEN_ADDRESS:PORT format format: json Copy winlog Collect events from Windows log channels. Parameters: channel: name of the channel logs will be collected from. collect-eventids: a list of Windows Event IDs to be collected and forwarded to New Relic. Event ID ranges are supported. exclude-eventids: a list of Windows Event IDs to be excluded from collection. Event ID ranges are supported. All events are collected from the specified channel by default. Configure the collect-eventids and exclude-eventids sections to avoid sending unwanted logs to your New Relic account. Add event IDs or ranges to collect-eventids or exclude-eventids to forward or drop specific events. exclude-eventids takes precedence over collect-eventids if the same event ID is present in both sections. Example: logs: # Winlog log ingestion with eventId filters. - name: windows-security winlog: channel: Security collect-eventids: - 4624 - 4265 - 4700-4800 exclude-eventids: - 4735 # entries for the application, system, powershell, and SCOM channels - name: windows-application winlog: channel: Application - name: windows-system winlog: channel: System - name: windows-pshell winlog: channel: Windows Powershell - name: scom winlog: channel: Operations Manager # Entry for Windows Defender Logs - name: windows-defender winlog: channel: Microsoft-Windows-Windows Defender/Operational # Entry for Windows Clustering Logs - name: windows-clustering winlog: channel: Microsoft-Windows-FailoverClustering/Operational # Entry for IIS logs with logtype attribute for automatic parsing - name: iis-log file: C:\\inetpub\\logs\\LogFiles\\w3svc.log attributes: logtype: iis_w3c Copy Optional configuration The following configuration parameters are not required but are still recommended. attributes List of custom attributes specified as key-value pairs that can be used to send additional data with the logs which you can then query. The attributes configuration parameter can be used with any log source. One common use of the attributes configuration parameter is to specify the logtype attribute. This attribute allows leveraging one of the built-in parsing rules supported by New Relic Logs. Example: logs: - name: example-file-attributes file: /var/log/example.log attributes: logtype: nginx region: example-us-02 team: A-team - name: example-tcp-attributes tcp: uri: tcp://0.0.0.0:2345 format: json attributes: logtype: nginx region: example-us-02 team: B-team Copy attributes automatically inserted by the infrastructure agent The infrastructure agent automatically inserts log attributes for your convenience. Some of them are inserted for any log record, while others depend on the configuration parameters you used while setting up the log forwarder. Attribute name Description entity.guids Always inserted. The infrastructure agent inserts the Entity GUID assigned by New Relic to identify the host where it's running. It is available in the entity.guids field. Note: If the captured logs belong to an application instrumented using APM, the entity.guids field contains both the entity GUID of infrastructure, as well as the GUID of APM, separated by a pipe ( | ) delimiter. fb.input Always inserted. The underlying Fluent Bit input plugin type used to capture the logs. Currently its values are tail, systemd, winlog, syslog, and tcp. filePath Inserted when sing the file input type. Absolute file path of the file being monitored. hostname Always inserted. The hostname of the machine/VM/container executing the infrastructure agent. plugin.type Always inserted. Indicates the utility used to capture the logs. In this case, it is the infrastructure agent itself, so this attribute always has the value nri-agent. pattern Regular expression for filtering records. Only supported for the tail, systemd, syslog, and tcp (only with format none) sources. This field works in a way similar to grep -E in Unix systems. For example, for a given file being captured, you can filter for records containing either WARN or ERROR using: - name: only-records-with-warn-and-error file: /var/log/logFile.log pattern: WARN|ERROR Copy No filtering is applied by default. max_line_kb Maximum size of log entries/lines in KB. If log entries exceed the limit, they are skipped. Default is 128. fluentbit External Fluent Bit configuration and parser files. If defined, they are merged with the existing configuration and parser files generated by the infrastructure agent. The infrastructure agent processes the configuration files located in the logging.d directory and will generate a run-time Fluent Bit configuration file that contains the appropriate [INPUT], [FILTER] and [OUTPUT] sections. Optionally, it will also declare an @INCLUDE in case you provided an external Fluent Bit configuration file via the fluentbit option. The runtime file does not define a [SERVICE] section, leaving all default Fluent Bit configuration values. You can still override Fluent Bit's default settings by defining your own [SERVICE] section in your external Fluent Bit configuration file and include it via the fluentbit option. Parameters: config_file: path to an existing Fluent Bit configuration file. Note that any overlapping source results in duplicate messages in New Relic Logs. parsers_file: path to an existing Fluent Bit parsers file. The following parser names are reserved: rfc3164, rfc3164-local and rfc5424. Sample configuration file Here is an example of a logging.d/ configuration file in YAML format. For more configuration examples, see the infrastructure agent repository. logging.d/sample.yaml # Remember to only use spaces for indentation logs: # Example of 'file' source - name: file-with-attributes file: /var/log/test.log # Path to a single file or pattern attributes: # You can use custom attributes to enrich your data logtype: nginx team: The A Team pattern: Error # Regular expression to filter log entries # Example of 'systemd' source (Linux only) - name: systemd-example systemd: cupsd # Examples of 'syslog' source, one per protocol # TCP network socket - name: syslog-tcp-test syslog: uri: tcp://0.0.0.0:5140 # Use the tcp://LISTEN_ADDRESS:PORT format parser: rfc5424 # Default syslog parser is rfc3164 # UDP network socket - name: syslog-udp-test syslog: uri: udp://0.0.0.0:6140 # Use the udp://LISTEN_ADDRESS:PORT format max_line_kb: 35 # Paths for Unix sockets are defined by combining protocol and path: # unix_udp:// + /path/socket - for example, unix_udp:///tmp/socket # Unix TCP domain socket - name: syslog-unix-tcp-test syslog: uri: unix_tcp:///var/unix-tcp-socket-test unix_permissions: 0666 # Default is 0644. Change at your own risk # Unix UDP domain socket - name: syslog-unix-udp-test syslog: uri: unix_udp:///var/unix-udp-socket-test parser: rfc5424 # Examples of 'tcp' source for formats 'none' and 'json' - name: tcp-simple-test tcp: uri: tcp://0.0.0.0:1234 # Use the tcp://LISTEN_ADDRESS:PORT format format: none # Raw text - this is default for 'tcp' separator: \\t # String for separating raw text entries attributes: # You can add custom attributes to any source of logs tcpFormat: none someOtherAttribute: associatedValue max_line_kb: 32 - name: tcp-json-test tcp: uri: tcp://0.0.0.0:2345 # Use the tcp://LISTEN_ADDRESS:PORT format format: json attributes: tcpFormat: json yetAnotherAttribute: 12345 # Example of Fluent Bit configuration import - name: fluentbit-import fluentbit: config_file: /path/to/fluentbit.config parsers_file: /path/to/fluentbit/parsers.conf Copy View your log data If everything is configured correctly and your data is being collected, you should see logs data in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy Troubleshooting If you encounter problems with configuring your log forwarder, try these troubleshooting tips. No log data If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. No data appears when tailing a file The log forwarding feature requires the agent to have permission to read the data sources. When running the infrastructure agent in privileged or non-privileged modes, make sure that the log files you want to forward (and any intermediary directory in its path) are readable by the user running nri-agent. Example: Check file access under Linux Let's check whether the file /var/log/restrictedLogs/logFile.log can be monitored by the nri-agent user. In Linux, you can do a quick check with the namei command: sudo -u nri-agent namei -ml /var/log/restrictedLogs/logFile.log f: /var/log/restrictedLogs/logFile.log drwxr-xr-x root root / drwxr-xr-x root root var drwxrwxr-x root syslog log drwxr--r-- root root restrictedLogs logFile.log - No such file or directory Copy This command failed because the file is not visible to the nri-agent user. By inspecting the previous output, we can detect that the restrictedLogs directory is missing the execution flag for others. To fix this, execute: sudo chmod 755 /var/log/restrictedLogs Copy And then check for file access again: # sudo -u nri-agent namei -ml /var/log/restrictedLogs/logFile.log f: /var/log/restrictedLogs/logFile.log drwxr-xr-x root root / drwxr-xr-x root root var drwxrwxr-x root syslog log drwxr-xr-x root root restrictedLogs -rw-r----- vagrant vagrant logFile.log Copy The file is now visible to the nri-agent user. You must ensure that the file is also readable by the nri-agent user. To check this, use: # sudo -u nri-agent head /var/log/restrictedLogs/logFile.log head: cannot open '/var/log/restrictedLogs/logFile.log' for reading: Permission denied Copy In this example, the file is missing the read rights for the others group (users other than vagrant and the vagrant user group). You could fix this by granting read permissions to others, but the application could change these permissions upon restart. To avoid this, a better approach is to add the nri-agent user to the vagrant user group. No data appears when capturing via a Syslog socket The log forwarding feature requires that the agent has permission to read the data sources. When running the infrastructure agent in privileged or non-privileged modes: If you're using Unix domain socket files, make sure that the nri-agent user can access these files (please refer to the previous section) and that they have read and write permissions (666) so that other users than nri-agent can write to them. If you're using IP sockets, ensure that the port that you are using is not a system reserved one (like port 80, for example). If no data appears after you enable log management, follow standard log management troubleshooting procedures. No data appears using infrastructure agent proxy As explained in the infrastructure agent configuration guidelines, the proxy parameter must use either HTTP or HTTPS and be in the form https://user:password@hostname:port. The agent can parse the parameter without the HTTP or HTTPS, but the log forwarder cannot. You will see an error like the following in the agent verbose logs: [ERROR] building HTTP transport: parse \\\"hostname:port\\\": first path segment in URL cannot contain colon Copy To solve this problem, check your newrelic-infra.yml file, and ensure the proxy parameter adheres to this form. If you're using caBundleFile or caBundleDir in order to specify any certificate, we recommend to follow the below rules for each OS: Linux For HTTP proxies you don't need to setup any certificates. The plugin loads the system certificates and New Relic sends logs into the logging endpoint. However, you can specify the proxy self-signed certificate (PEM file) using either the caBundleFile or caBundleDir parameters. Windows For HTTP proxies you don't need to setup any certificates. The plugin loads the system certificates. For HTTPS, you can configure it in one of the following ways: Import the proxy certificate to the system pool (Recommended) Import the proxy self-signed certificate (PEM file) by using the MMC tool. Refer to this link, and in Step 2 ensure to import it in your Trusted Root Certification Authorities, instead of in the Intermediate Certification Authorities. Using the caBundleFile and caBundleDir parameters On Windows, we cannot load both the certificates from the system certificate pool and the ones specified with the caBundleFile caBundleDir parameters. So, if you are using caBundleFile or caBundleDir, ensure that the following certificates are placed in the same PEM file (when using caBundleFile) or in the same directory (when using caBundleDir): The Proxy certificate (because it's an HTTPS proxy). The Logging Endpoint certificate (eg. https://log-api.newrelic.com/log/v1). The Infrastructure Agent certificate (eg. https://infra-api.newrelic.com). You can check the certificates by running: # openssl s_client -connect log-api.newrelic.com:443 -servername log-api.newrelic.com Copy Sending the infrastructure agent's logs to New Relic You can configure the infrastructure agent to send its own logs to New Relic. This is useful for troubleshooting issues with log forwarding, the agent, or when contacting support. To forward the infrastructure agent logs to New Relic: Edit your newrelic-infra.yml file. Enable agent logging in troubleshooting mode by adding verbose: 3. On Windows and systems that don't use systemd or where journald is inaccessible, verbose:3 causes the agent to write the logs on the disk. Revert to verbose:0 to prevent this. (Recommended): Enable agent logging in JSON format to log_format: json. Restart the agent to load the new settings. This configuration sets up the agent in troubleshooting mode, but the log forwarder (based on Fluent Bit) will continue in a non-verbose mode. Sometimes you can have issues with the log forwarder itself. For example, there may be problems accessing a specific channel when shipping Windows log events or when accessing a particular log file. In these situations, you can also enable the verbose mode for the log forwarder: Set verbose to a value other than 0. Add the configuration option: trace: [\"log.fw\"]. Caution Check whether you are using the [fluentbit] option. When setting verbose: 3 and trace: [\"log.fw\"], ensure that you don't define any [OUTPUT] section pointing to stdout in an external Fluent Bit configuration file, Fluent Bit does not start with the infra agent Important Fluent Bit's tail plugin does not support network drives. For Linux versions prior to 2016, you may need to update the OpenSSL library to 1.1.0 (or higher). To check if you have this problem: See if infra-agent has started Fluent Bit by running: ps -aux | grep td-agent-bit Copy If it isn't running go to /var/db/newrelic-infra/newrelic-integrations/logging and run: ./fluent-bit -i systemd -o stdout Copy If you get the following error, update OpenSSL to 1.1.0 or higher: error while loading shared libraries: libssl.so.1.1: cannot open shared object file: No such file or directory Copy Runtime error on Windows One of the following error messages may appear when enabling log forwarding on Windows: The code execution cannot proceed because VCRUNTIME140.dll was not found. Copy OR error=\"exit status 3221225781\" process=log-forwarder Copy This is caused by a missing DLL. To solve the issue, install the Microsoft Visual C++ Redistributable as applicable: x64 x86 Errors when tailing a large amount of log files (Linux) It's common to face either one of the following error messages when attempting to tail a large amount of files: Too many open files The user limit on the total number of inotify watches was reached or the kernel failed to allocate a needed resource The operating system defines a maximum amount of allocatable file descriptors (typically 1024 by default), and a maximum amount of allocatable inotify watches (typically 8192 by default). Any process attempting to go above these limits will fail, returning one of the errors above. The underlying technology we use to forward logs, Fluent Bit, opens one file descriptor and sets an inotify watch for each file you configure to be forwarded. Moreover, at the time of writing this section, Fluent Bit uses an extra set of 32 file descriptors for its normal operation, with another extra file descriptor when it shuts down. Therefore, to capture a large amount of files you need to ensure that both the file descriptor and inotify watch limits are slightly greater than the amount of log files you wish to tail. The following instructions summarize how to increase these limits if you want to tail 10,000 log files. Also, it assumes the infrastructure agent is installed in root running mode, and therefore must be run using the root user. Check which is the current hard limit for the amount of file descriptors per process. Typically, this limit should be quite high and should not need to be modified. ulimit -Hn Copy Add the following line to /etc/security/limits.conf. We specified a limit of 10100 here instead of just 10000 to allow Fluent Bit to allocate the extra file descriptors it may need to work. root soft nofile 10100 # replace root by nri-agent for non-root (privileged and unprivileged) installations Copy Add the following line to /etc/pam.d/common-session so that the previous limit is applied upon restart: session required pam_limits.so Copy Add the following line to /etc/sysctl.conf to increase the amount of allowed inotify watchers per user. We specified a limit of 18192 here instead of just 10000 so that the root user will still have 8192 available inotify watches (the default value). fs.inotify.max_user_watches=18192 Copy Restart your system. Ensure that the new limits have been enforced by running: ulimit -Sn # Should return 10100 cat /proc/sys/fs/inotify/max_user_watches # Should return 18192 Copy Learn more on how to increase open file limits, or on how to increase the inotify watches. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards. Uninstall log forwarding To uninstall log forwarding capabilities, go to your logging.d directory, and remove files with the .yml extension that were originally added during the configuration process. Linux: /etc/newrelic-infra/logging.d/ Windows: C:\\Program Files\\New Relic\\newrelic-infra\\logging.d\\",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 353.44556,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward your <em>logs</em> using the <em>infrastructure</em> agent",
        "sections": "Sending the <em>infrastructure</em> agent&#x27;s <em>logs</em> to <em>New</em> <em>Relic</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " <em>management</em> capabilities to collect, process, explore, query, and alert on your <em>log</em> data. Basic process To forward your <em>logs</em> through our infrastructure <em>monitoring</em> agent: If you haven&#x27;t already, create a <em>New</em> <em>Relic</em> account. It&#x27;s free, forever. Verify the system requirements needed for configuring <em>logs</em>"
      },
      "id": "603e9df164441f6b6f4e8843"
    },
    {
      "sections": [
        "Forward your logs to New Relic",
        "How it works",
        "Get started with log forwarders",
        "Log forwarding options",
        "TCP endpoint",
        "Log API",
        "What's next?"
      ],
      "title": "Forward your logs to New Relic",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "b6e203ddc367d5a2b5e002916e49d34f4ba17a87",
      "image": "https://docs.newrelic.com/static/c3d5443b84a1e2b26a4767ce35fa58f3/e5166/new-relic-logs-in-context-diagram.jpg",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/enable-log-management-new-relic/",
      "published_at": "2022-01-08T02:40:08Z",
      "updated_at": "2022-01-04T07:02:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our log management capabilities help you to collect, process, explore, query, and alert on your log data. To get your logs into New Relic, you can: Use your existing log forwarding solution to collect your logs and extend the metadata that is forwarded to New Relic. Use our infrastructure agent as a lightweight data collector, without having to use additional software. Use our Log API to forward your logs via HTTP. Use syslog protocols to forward your logs via a TCP endpoint. How it works The following diagram shows the lifecycle of a log message for an app, from enrichment with APM agent metadata (contextual logging), to formatting and forwarding the log data to New Relic. This diagram illustrates the flow of log messages through New Relic. Standard log formatters transform log events into meaningful output (such as text files) that can be used by downstream people and processes. The NewRelicFormatter transforms log events into the JSON format expected by New Relic. These files contain log information and extended metadata. When you configure your log forwarder (our infrastructure monitoring agent, Fluentd, Logstash, etc.), you can also extend and enrich your log data. By configuring logs in context, the log enricher links the formatted log data with additional transaction information from your application or host. Now your log files are enriched with enhanced metadata and contextual logging data. Your log forwarder sends the files to our logging endpoint for processing. From there you can use our log management capabilities to view, query, set up alerts, and more in New Relic. Here is an example of logs for your host's UI. You can see logs in context of events for the selected time period, and drill down into detailed data for any of the highlighted attributes. To take advantage of even more capabilities, click Query logs from here to go directly to the Logs UI. Here is an example of a host's logs in context related to an event. Get started with log forwarders To forward your logs to New Relic with enriched metadata: If you don't have one already, create a New Relic account. It's free, forever. Have your New Relic account's license key. Install a compatible log forwarder. Ensure that outbound connectivity on TCP port 443 is allowed to the CIDR range that matches your region. Use the DNS name log-api.newrelic.com or log-api.eu.newrelic.com for configuration. For apps monitored by a New Relic APM agent, configure logs in context. Log forwarding options Use any of these solutions to forward your logs to New Relic. Recommended: Infrastructure monitoring agent Amazon: AWS CloudWatch plugin AWS FireLens plugin AWS Kinesis Firehose AWS Lambda for sending logs from S3 Microsoft: Azure ARM template Other log forwarding plugins: Fluent Bit plugin Fluentd plugin Google Cloud Platform Pub/Sub Heroku log streaming Kubernetes plugin Logstash plugin Vector plugin TCP endpoint In some situations you may not have log forwarders; for example, with CDNs, hardware devices, or managed services. You can use syslog protocols such as rsyslog and syslog-ng, and forward your logs to New Relic via a TCP endpoint. Log API If you prefer to connect to New Relic without installing a plugin, we offer an HTTP input integration. This option sends your monitored log data directly to New Relic via the Log API. What's next? After you enable your log forwarder, make the most of your data in New Relic with our log management capabilities: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data if you are forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards. For example, to query and manage your data partition rules, see our NerdGraph tutorial.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 314.5203,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward your <em>logs</em> to <em>New</em> <em>Relic</em>",
        "sections": "Forward your <em>logs</em> to <em>New</em> <em>Relic</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " directly to <em>New</em> <em>Relic</em> via the <em>Log</em> API. What&#x27;s next? After you <em>enable</em> your <em>log</em> forwarder, make the most of your data in <em>New</em> <em>Relic</em> with our <em>log</em> <em>management</em> capabilities: Explore the logging data across your platform with our <em>Logs</em> UI. See your <em>logs</em> in context of your app&#x27;s performance in the APM UI"
      },
      "id": "61571e0e28ccbcbc52f21431"
    }
  ],
  "/docs/logs/forward-logs/logstash-plugin-log-forwarding": [
    {
      "sections": [
        "Fluent Bit plugin for log forwarding",
        "Basic process",
        "Install the Fluent Bit plugin",
        "Tip",
        "Configure the Fluent Bit plugin",
        "Important",
        "Test the Fluent Bit plugin",
        "Associate logs with entities",
        "Optional: Configure plugin attributes",
        "View log data",
        "What's next?"
      ],
      "title": "Fluent Bit plugin for log forwarding",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "dd686f074bb80908ae9de19e543a3bae7d87a466",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/fluent-bit-plugin-log-forwarding/",
      "published_at": "2022-01-08T02:38:07Z",
      "updated_at": "2022-01-08T02:38:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your log data in New Relic. Forwarding your Fluent Bit logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Basic process We have published a container with the plugin installed. It serves as a base image to be used by our Kubernetes integration. We recommend you use this base image and layer your own custom configuration files. To forward your logs from Fluent Bit to New Relic: Make sure you have: A New Relic license key Fluent Bit 1.0 or higher (recommended), although v0.12 or higher is supported Fluent Bit Windows install directions can be found here Fluent Bit Linux install directions can be found here Install the Fluent Bit plugin. Configure the Fluent Bit plugin. Test the Fluent Bit plugin. Generate some traffic and wait a few minutes, then check your account for data. Install the Fluent Bit plugin To install the Fluent Bit plugin: Navigate to New Relic's Fluent Bit plugin repository on GitHub. From the repository page, clone or download the repository. Run the following command to build your plugin: cd newrelic-fluent-bit-output && make all Copy Store out_newrelic.so or out_newrelic_winXX.dll at a location that can be accessed by the fluent-bit daemon. Tip If you'd rather not compile the plugin yourself, you can download pre-compiled versions from our GitHub repository's releases page. Configure the Fluent Bit plugin Fluent Bit needs to know the location of the New Relic plugin and the New Relic license key to output data to New Relic. To configure your Fluent Bit plugin: Important Pay attention to white space when editing your config files. Be sure to use four spaces to indent and one space between keys and values. Locate or create a plugins.conf file in your plugins directory. In the plugins.conf file, add a reference to out_newrelic.so, adjacent to your fluent-bit.conf file: [PLUGINS] Path /PATH/TO/newrelic-fluent-bit-output/out_newrelic.so Copy In the fluent-bit.conf file, add the following line under the service block: [SERVICE] # This is the main configuration block for fluent bit. # Ensure the follow line exists somewhere in the SERVICE block Plugins_File plugins.conf Copy At the bottom of the fluent-bit.conf file, add the following to set up the input and output filters. Replace the placeholder text with your New Relic license key: [INPUT] Name tail Path /PATH/TO/YOUR/LOG/FILE [OUTPUT] Name newrelic Match * licenseKey YOUR_LICENSE_KEY # Optional maxBufferSize 256000 maxRecords 1024 Copy Restart your Fluent Bit instance with the following command: fluent-bit -c /PATH/TO/fluent-bit.conf Copy Test the Fluent Bit plugin To test if your Fluent Bit plugin is receiving input from a log file: Run the following command to append a test log message to your log file: echo \"test message\" >> /PATH/TO/YOUR/LOG/FILE Copy Search the New Relic Logs UI for test message. Associate logs with entities To associate a log line with an entity, such as an infrastructure host, add a FILTER block: [FILTER] Name modify Match * # Or specify a match Add entity.guids <Your Entity GUID ID> # Optional Add hostname <Your hostname> # Optional Copy For more options, see the Fluent Bit modify filter documentation and our documentation to forward your logs using the infrastructure agent. Optional: Configure plugin attributes Once you have installed and configured the Fluent Bit plugin, you can use the following attributes to configure how the plugin sends data to New Relic: Key Description licenseKey The New Relic license key. Use either licenseKey (recommended) or apiKey, not both. Default: none maxBufferSize The maximum size the payloads sent, in bytes. Default: 256000 maxRecords The maximum number of records to send at a time. Default: 1024 apiKey Deprecated. Takes a New Relic Insights insert key, but using the licenseKey attribute is preferred. Use either licenseKey or apiKey, not both. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 374.04816,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Fluent Bit plugin for <em>log</em> forwarding",
        "sections": "Associate <em>logs</em> with entities",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "If your <em>log</em> data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your <em>log</em> data in <em>New</em> <em>Relic</em>. Forwarding your Fluent Bit <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert"
      },
      "id": "603ea7ec28ccbce542eba791"
    },
    {
      "sections": [
        "Forward your logs using the infrastructure agent",
        "Basic process",
        "System requirements",
        "Important",
        "Install the infrastructure agent",
        "Configure the infrastructure agent",
        "Log forwarding parameters",
        "Name (required)",
        "Log source (required)",
        "file",
        "systemd",
        "syslog",
        "tcp",
        "winlog",
        "Optional configuration",
        "attributes",
        "attributes automatically inserted by the infrastructure agent",
        "pattern",
        "max_line_kb",
        "fluentbit",
        "Sample configuration file",
        "logging.d/sample.yaml",
        "View your log data",
        "Troubleshooting",
        "No log data",
        "No data appears when tailing a file",
        "No data appears when capturing via a Syslog socket",
        "No data appears using infrastructure agent proxy",
        "Sending the infrastructure agent's logs to New Relic",
        "Caution",
        "Fluent Bit does not start with the infra agent",
        "Runtime error on Windows",
        "Errors when tailing a large amount of log files (Linux)",
        "What's next?",
        "Uninstall log forwarding"
      ],
      "title": "Forward your logs using the infrastructure agent",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "a1deac410f0eedfb819348524a85a73bbb9d9daf",
      "image": "https://docs.newrelic.com/static/5d4352531871b652878b3cb0768e3f23/c1b63/demotron-host-logs-123021.png",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/forward-your-logs-using-infrastructure-agent/",
      "published_at": "2022-01-09T01:46:14Z",
      "updated_at": "2022-01-07T01:41:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can forward your logs to New Relic using our infrastructure monitoring agent. This makes all of your logging data available in one location and provides deeper visibility into both your application and your platform performance data. Forwarding your logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Basic process To forward your logs through our infrastructure monitoring agent: If you haven't already, create a New Relic account. It's free, forever. Verify the system requirements needed for configuring logs. Ensure you have installed the infrastructure agent, version 1.11.4 or higher. Create a logging.yml configuration file in the infrastructure agent's logging.d directory. Configure your log sources and other parameters. Generate some traffic and wait a few minutes, then check your account for data. Explore your log data in the Logs UI and benefit from the log attributes automatically inserted by the infrastructure agent. Here is an example of logs for your host's UI. You can see logs in context of events for the selected time period, and drill down into detailed data for any of the highlighted attributes. To take advantage of even more capabilities, click Query logs from here to go directly to the Logs UI. Here is an example of a host's logs in context related to an event. System requirements To use the log forwarder of the infrastructure agent, make sure you meet the following requirements: Infrastructure agent version 1.11.4 or higher OpenSSL library 1.1.0 or higher is required by Infrastructure agent starting from 1.16.4. Built-in support for ARM64 architecture on Linux systems (in example, AWS Graviton architecture) added in Infrastructure agent 1.26.0. Important The log forwarding feature is not supported with the Docker container for infrastructure monitoring agents. The log forwarding feature is compatible with the following operating systems: Operating system Supported version Amazon Linux Amazon Linux 2 CentOS Version 7 or higher Debian Version 9 (\"Stretch\") or higher Exception: Version 11 is not supported. Red Hat Enterprise Linux (RHEL) Version 7 or higher SUSE Linux Enterprise Server (SLES) Version 12 Ubuntu Versions 16.04.x, 18.04.x and 20.04.x (LTS versions) Windows Windows Server 2012, 2016, 2019, and 2022, and their service packs. Windows 10 Install the infrastructure agent Starting with version 1.11.4, the infrastructure agent can forward logs to New Relic. To install and run the agent, use a package manager (Linux) or the MSI installer (Windows). Important The log forwarding feature is not included when the infrastructure agent is implemented using Linux tarball or Windows ZIP installations. To use the following links, make sure you are logged to your New Relic account. Amazon Linux CentOS Debian RHEL SLES Ubuntu Windows If you don't have a New Relic account yet, or if you prefer to follow the procedure manually, see our tutorial to install the package manager. Configure the infrastructure agent Configuration files describe which log sources are forwarded. Our infrastructure agent uses .yml files to configure logging. You can add as many config files as you want. To add a new configuration file for the log forwarding feature: Navigate to the log forwarder configuration folder: Linux: /etc/newrelic-infra/logging.d/ Windows: C:\\Program Files\\New Relic\\newrelic-infra\\logging.d\\ Create a logging.yml configuration file, and add the parameters you need. The logging.d directory has various .yml.example files you can use as a reference or starting point. The agent automatically processes new configuration files without having to restart the infrastructure monitoring service. The only exception to this is when configuring a custom Fluent Bit configuration. Log forwarding parameters The infrastructure log forwarding .yml config supports the following parameters: Name (required) To start, define a name of the log or logs you want to forward to New Relic. Log source (required) What you use for the log source will depend on where you want to forward your logs from. Available options include: file Path to the log file or files. The agent tracks changes on the log files in a way similar to tail -f shell. Example: logs: - name: example-log file: /var/log/example.log # Path to a single log file - name: example-log-two file: /var/log/example-two.log # Path to another single log file Copy The file parameter can point to a specific log file or multiple files by using wildcards applied to names and extensions; for example, /logs/*.log. You can use wildcards in place of directories in a file path, which can be used to tail files located in different directories. Example: logs: - name: docker-logs file: /var/lib/docker/containers/*/*.log # Path to multiple folders and files Copy Important Use of wildcards may significantly increase the number of file descriptors and inotify watches the Fluent Bit process keeps open, which can interfere with log collection if the host's file descriptor limit is reached. Tailing a large number of files may require you to increase the maximum number of file descriptors and inotify watchers allowed by the operating system. Please refer to Errors when tailing a large amount of log files for more details on how to increase them. systemd Use the systemd parameter to forward log messages that are collected by the journald daemon in Linux environments. This input type requires the agent to run in root mode. Example: logs: - name: systemd-example systemd: cupsd Copy syslog Syslog data source. Parameters: uri: Syslog socket. Format varies depending on the protocol: TCP/UDP network sockets: [tcp/udp]://LISTEN_ADDRESS:PORT Unix domain sockets: unix_[tcp/udp]:// + /socket/path parser: Syslog parser. Default is rfc3164. Use rfc5424 if your messages include fractional seconds. Note: rfc3164 currently does not work on SuSE. unix_permissions: default is 0644 for domain sockets; this limits entries to processes running as root. You can use 0666 to listen for non-root processes, at your own risk. When running the agent in privileged mode, ports and sockets must be available or owned by nri-agent, with 0666 file permissions, so that other processes can write logs to the sockets. logs: # TCP network socket - name: syslog-tcp-test syslog: uri: tcp://0.0.0.0:5140 # Use the tcp://LISTEN_ADDRESS:PORT format parser: rfc5424 # Default syslog parser is rfc3164 # UDP network socket - name: syslog-udp-test syslog: uri: udp://0.0.0.0:6140 # Use the udp://LISTEN_ADDRESS:PORT format max_line_kb: 35 # Unix TCP domain socket - name: syslog-unix-tcp-test syslog: uri: unix_tcp:///var/unix-tcp-socket-test unix_permissions: 0666 # Default is 0644. Change at your own risk # Unix UDP domain socket - name: syslog-unix-udp-test syslog: uri: unix_udp:///var/unix-udp-socket-test parser: rfc5424 Copy tcp Logs retrieved over TCP connections. Parameters: uri: TCP/IP socket to listen for incoming data. The URI format is tcp://LISTEN_ADDRESS:PORT format: format of the data. It can be json or none. separator: If format: none is used, you can define a separator string for splitting records (default: \\n). logs: - name: tcp-simple-test tcp: uri: tcp://0.0.0.0:1234 # Use the tcp://LISTEN_ADDRESS:PORT format format: none # Raw text - this is default for 'tcp' separator: \\t # String for separating raw text entries max_line_kb: 32 - name: tcp-json-test tcp: uri: tcp://0.0.0.0:2345 # Use the tcp://LISTEN_ADDRESS:PORT format format: json Copy winlog Collect events from Windows log channels. Parameters: channel: name of the channel logs will be collected from. collect-eventids: a list of Windows Event IDs to be collected and forwarded to New Relic. Event ID ranges are supported. exclude-eventids: a list of Windows Event IDs to be excluded from collection. Event ID ranges are supported. All events are collected from the specified channel by default. Configure the collect-eventids and exclude-eventids sections to avoid sending unwanted logs to your New Relic account. Add event IDs or ranges to collect-eventids or exclude-eventids to forward or drop specific events. exclude-eventids takes precedence over collect-eventids if the same event ID is present in both sections. Example: logs: # Winlog log ingestion with eventId filters. - name: windows-security winlog: channel: Security collect-eventids: - 4624 - 4265 - 4700-4800 exclude-eventids: - 4735 # entries for the application, system, powershell, and SCOM channels - name: windows-application winlog: channel: Application - name: windows-system winlog: channel: System - name: windows-pshell winlog: channel: Windows Powershell - name: scom winlog: channel: Operations Manager # Entry for Windows Defender Logs - name: windows-defender winlog: channel: Microsoft-Windows-Windows Defender/Operational # Entry for Windows Clustering Logs - name: windows-clustering winlog: channel: Microsoft-Windows-FailoverClustering/Operational # Entry for IIS logs with logtype attribute for automatic parsing - name: iis-log file: C:\\inetpub\\logs\\LogFiles\\w3svc.log attributes: logtype: iis_w3c Copy Optional configuration The following configuration parameters are not required but are still recommended. attributes List of custom attributes specified as key-value pairs that can be used to send additional data with the logs which you can then query. The attributes configuration parameter can be used with any log source. One common use of the attributes configuration parameter is to specify the logtype attribute. This attribute allows leveraging one of the built-in parsing rules supported by New Relic Logs. Example: logs: - name: example-file-attributes file: /var/log/example.log attributes: logtype: nginx region: example-us-02 team: A-team - name: example-tcp-attributes tcp: uri: tcp://0.0.0.0:2345 format: json attributes: logtype: nginx region: example-us-02 team: B-team Copy attributes automatically inserted by the infrastructure agent The infrastructure agent automatically inserts log attributes for your convenience. Some of them are inserted for any log record, while others depend on the configuration parameters you used while setting up the log forwarder. Attribute name Description entity.guids Always inserted. The infrastructure agent inserts the Entity GUID assigned by New Relic to identify the host where it's running. It is available in the entity.guids field. Note: If the captured logs belong to an application instrumented using APM, the entity.guids field contains both the entity GUID of infrastructure, as well as the GUID of APM, separated by a pipe ( | ) delimiter. fb.input Always inserted. The underlying Fluent Bit input plugin type used to capture the logs. Currently its values are tail, systemd, winlog, syslog, and tcp. filePath Inserted when sing the file input type. Absolute file path of the file being monitored. hostname Always inserted. The hostname of the machine/VM/container executing the infrastructure agent. plugin.type Always inserted. Indicates the utility used to capture the logs. In this case, it is the infrastructure agent itself, so this attribute always has the value nri-agent. pattern Regular expression for filtering records. Only supported for the tail, systemd, syslog, and tcp (only with format none) sources. This field works in a way similar to grep -E in Unix systems. For example, for a given file being captured, you can filter for records containing either WARN or ERROR using: - name: only-records-with-warn-and-error file: /var/log/logFile.log pattern: WARN|ERROR Copy No filtering is applied by default. max_line_kb Maximum size of log entries/lines in KB. If log entries exceed the limit, they are skipped. Default is 128. fluentbit External Fluent Bit configuration and parser files. If defined, they are merged with the existing configuration and parser files generated by the infrastructure agent. The infrastructure agent processes the configuration files located in the logging.d directory and will generate a run-time Fluent Bit configuration file that contains the appropriate [INPUT], [FILTER] and [OUTPUT] sections. Optionally, it will also declare an @INCLUDE in case you provided an external Fluent Bit configuration file via the fluentbit option. The runtime file does not define a [SERVICE] section, leaving all default Fluent Bit configuration values. You can still override Fluent Bit's default settings by defining your own [SERVICE] section in your external Fluent Bit configuration file and include it via the fluentbit option. Parameters: config_file: path to an existing Fluent Bit configuration file. Note that any overlapping source results in duplicate messages in New Relic Logs. parsers_file: path to an existing Fluent Bit parsers file. The following parser names are reserved: rfc3164, rfc3164-local and rfc5424. Sample configuration file Here is an example of a logging.d/ configuration file in YAML format. For more configuration examples, see the infrastructure agent repository. logging.d/sample.yaml # Remember to only use spaces for indentation logs: # Example of 'file' source - name: file-with-attributes file: /var/log/test.log # Path to a single file or pattern attributes: # You can use custom attributes to enrich your data logtype: nginx team: The A Team pattern: Error # Regular expression to filter log entries # Example of 'systemd' source (Linux only) - name: systemd-example systemd: cupsd # Examples of 'syslog' source, one per protocol # TCP network socket - name: syslog-tcp-test syslog: uri: tcp://0.0.0.0:5140 # Use the tcp://LISTEN_ADDRESS:PORT format parser: rfc5424 # Default syslog parser is rfc3164 # UDP network socket - name: syslog-udp-test syslog: uri: udp://0.0.0.0:6140 # Use the udp://LISTEN_ADDRESS:PORT format max_line_kb: 35 # Paths for Unix sockets are defined by combining protocol and path: # unix_udp:// + /path/socket - for example, unix_udp:///tmp/socket # Unix TCP domain socket - name: syslog-unix-tcp-test syslog: uri: unix_tcp:///var/unix-tcp-socket-test unix_permissions: 0666 # Default is 0644. Change at your own risk # Unix UDP domain socket - name: syslog-unix-udp-test syslog: uri: unix_udp:///var/unix-udp-socket-test parser: rfc5424 # Examples of 'tcp' source for formats 'none' and 'json' - name: tcp-simple-test tcp: uri: tcp://0.0.0.0:1234 # Use the tcp://LISTEN_ADDRESS:PORT format format: none # Raw text - this is default for 'tcp' separator: \\t # String for separating raw text entries attributes: # You can add custom attributes to any source of logs tcpFormat: none someOtherAttribute: associatedValue max_line_kb: 32 - name: tcp-json-test tcp: uri: tcp://0.0.0.0:2345 # Use the tcp://LISTEN_ADDRESS:PORT format format: json attributes: tcpFormat: json yetAnotherAttribute: 12345 # Example of Fluent Bit configuration import - name: fluentbit-import fluentbit: config_file: /path/to/fluentbit.config parsers_file: /path/to/fluentbit/parsers.conf Copy View your log data If everything is configured correctly and your data is being collected, you should see logs data in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy Troubleshooting If you encounter problems with configuring your log forwarder, try these troubleshooting tips. No log data If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. No data appears when tailing a file The log forwarding feature requires the agent to have permission to read the data sources. When running the infrastructure agent in privileged or non-privileged modes, make sure that the log files you want to forward (and any intermediary directory in its path) are readable by the user running nri-agent. Example: Check file access under Linux Let's check whether the file /var/log/restrictedLogs/logFile.log can be monitored by the nri-agent user. In Linux, you can do a quick check with the namei command: sudo -u nri-agent namei -ml /var/log/restrictedLogs/logFile.log f: /var/log/restrictedLogs/logFile.log drwxr-xr-x root root / drwxr-xr-x root root var drwxrwxr-x root syslog log drwxr--r-- root root restrictedLogs logFile.log - No such file or directory Copy This command failed because the file is not visible to the nri-agent user. By inspecting the previous output, we can detect that the restrictedLogs directory is missing the execution flag for others. To fix this, execute: sudo chmod 755 /var/log/restrictedLogs Copy And then check for file access again: # sudo -u nri-agent namei -ml /var/log/restrictedLogs/logFile.log f: /var/log/restrictedLogs/logFile.log drwxr-xr-x root root / drwxr-xr-x root root var drwxrwxr-x root syslog log drwxr-xr-x root root restrictedLogs -rw-r----- vagrant vagrant logFile.log Copy The file is now visible to the nri-agent user. You must ensure that the file is also readable by the nri-agent user. To check this, use: # sudo -u nri-agent head /var/log/restrictedLogs/logFile.log head: cannot open '/var/log/restrictedLogs/logFile.log' for reading: Permission denied Copy In this example, the file is missing the read rights for the others group (users other than vagrant and the vagrant user group). You could fix this by granting read permissions to others, but the application could change these permissions upon restart. To avoid this, a better approach is to add the nri-agent user to the vagrant user group. No data appears when capturing via a Syslog socket The log forwarding feature requires that the agent has permission to read the data sources. When running the infrastructure agent in privileged or non-privileged modes: If you're using Unix domain socket files, make sure that the nri-agent user can access these files (please refer to the previous section) and that they have read and write permissions (666) so that other users than nri-agent can write to them. If you're using IP sockets, ensure that the port that you are using is not a system reserved one (like port 80, for example). If no data appears after you enable log management, follow standard log management troubleshooting procedures. No data appears using infrastructure agent proxy As explained in the infrastructure agent configuration guidelines, the proxy parameter must use either HTTP or HTTPS and be in the form https://user:password@hostname:port. The agent can parse the parameter without the HTTP or HTTPS, but the log forwarder cannot. You will see an error like the following in the agent verbose logs: [ERROR] building HTTP transport: parse \\\"hostname:port\\\": first path segment in URL cannot contain colon Copy To solve this problem, check your newrelic-infra.yml file, and ensure the proxy parameter adheres to this form. If you're using caBundleFile or caBundleDir in order to specify any certificate, we recommend to follow the below rules for each OS: Linux For HTTP proxies you don't need to setup any certificates. The plugin loads the system certificates and New Relic sends logs into the logging endpoint. However, you can specify the proxy self-signed certificate (PEM file) using either the caBundleFile or caBundleDir parameters. Windows For HTTP proxies you don't need to setup any certificates. The plugin loads the system certificates. For HTTPS, you can configure it in one of the following ways: Import the proxy certificate to the system pool (Recommended) Import the proxy self-signed certificate (PEM file) by using the MMC tool. Refer to this link, and in Step 2 ensure to import it in your Trusted Root Certification Authorities, instead of in the Intermediate Certification Authorities. Using the caBundleFile and caBundleDir parameters On Windows, we cannot load both the certificates from the system certificate pool and the ones specified with the caBundleFile caBundleDir parameters. So, if you are using caBundleFile or caBundleDir, ensure that the following certificates are placed in the same PEM file (when using caBundleFile) or in the same directory (when using caBundleDir): The Proxy certificate (because it's an HTTPS proxy). The Logging Endpoint certificate (eg. https://log-api.newrelic.com/log/v1). The Infrastructure Agent certificate (eg. https://infra-api.newrelic.com). You can check the certificates by running: # openssl s_client -connect log-api.newrelic.com:443 -servername log-api.newrelic.com Copy Sending the infrastructure agent's logs to New Relic You can configure the infrastructure agent to send its own logs to New Relic. This is useful for troubleshooting issues with log forwarding, the agent, or when contacting support. To forward the infrastructure agent logs to New Relic: Edit your newrelic-infra.yml file. Enable agent logging in troubleshooting mode by adding verbose: 3. On Windows and systems that don't use systemd or where journald is inaccessible, verbose:3 causes the agent to write the logs on the disk. Revert to verbose:0 to prevent this. (Recommended): Enable agent logging in JSON format to log_format: json. Restart the agent to load the new settings. This configuration sets up the agent in troubleshooting mode, but the log forwarder (based on Fluent Bit) will continue in a non-verbose mode. Sometimes you can have issues with the log forwarder itself. For example, there may be problems accessing a specific channel when shipping Windows log events or when accessing a particular log file. In these situations, you can also enable the verbose mode for the log forwarder: Set verbose to a value other than 0. Add the configuration option: trace: [\"log.fw\"]. Caution Check whether you are using the [fluentbit] option. When setting verbose: 3 and trace: [\"log.fw\"], ensure that you don't define any [OUTPUT] section pointing to stdout in an external Fluent Bit configuration file, Fluent Bit does not start with the infra agent Important Fluent Bit's tail plugin does not support network drives. For Linux versions prior to 2016, you may need to update the OpenSSL library to 1.1.0 (or higher). To check if you have this problem: See if infra-agent has started Fluent Bit by running: ps -aux | grep td-agent-bit Copy If it isn't running go to /var/db/newrelic-infra/newrelic-integrations/logging and run: ./fluent-bit -i systemd -o stdout Copy If you get the following error, update OpenSSL to 1.1.0 or higher: error while loading shared libraries: libssl.so.1.1: cannot open shared object file: No such file or directory Copy Runtime error on Windows One of the following error messages may appear when enabling log forwarding on Windows: The code execution cannot proceed because VCRUNTIME140.dll was not found. Copy OR error=\"exit status 3221225781\" process=log-forwarder Copy This is caused by a missing DLL. To solve the issue, install the Microsoft Visual C++ Redistributable as applicable: x64 x86 Errors when tailing a large amount of log files (Linux) It's common to face either one of the following error messages when attempting to tail a large amount of files: Too many open files The user limit on the total number of inotify watches was reached or the kernel failed to allocate a needed resource The operating system defines a maximum amount of allocatable file descriptors (typically 1024 by default), and a maximum amount of allocatable inotify watches (typically 8192 by default). Any process attempting to go above these limits will fail, returning one of the errors above. The underlying technology we use to forward logs, Fluent Bit, opens one file descriptor and sets an inotify watch for each file you configure to be forwarded. Moreover, at the time of writing this section, Fluent Bit uses an extra set of 32 file descriptors for its normal operation, with another extra file descriptor when it shuts down. Therefore, to capture a large amount of files you need to ensure that both the file descriptor and inotify watch limits are slightly greater than the amount of log files you wish to tail. The following instructions summarize how to increase these limits if you want to tail 10,000 log files. Also, it assumes the infrastructure agent is installed in root running mode, and therefore must be run using the root user. Check which is the current hard limit for the amount of file descriptors per process. Typically, this limit should be quite high and should not need to be modified. ulimit -Hn Copy Add the following line to /etc/security/limits.conf. We specified a limit of 10100 here instead of just 10000 to allow Fluent Bit to allocate the extra file descriptors it may need to work. root soft nofile 10100 # replace root by nri-agent for non-root (privileged and unprivileged) installations Copy Add the following line to /etc/pam.d/common-session so that the previous limit is applied upon restart: session required pam_limits.so Copy Add the following line to /etc/sysctl.conf to increase the amount of allowed inotify watchers per user. We specified a limit of 18192 here instead of just 10000 so that the root user will still have 8192 available inotify watches (the default value). fs.inotify.max_user_watches=18192 Copy Restart your system. Ensure that the new limits have been enforced by running: ulimit -Sn # Should return 10100 cat /proc/sys/fs/inotify/max_user_watches # Should return 18192 Copy Learn more on how to increase open file limits, or on how to increase the inotify watches. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards. Uninstall log forwarding To uninstall log forwarding capabilities, go to your logging.d directory, and remove files with the .yml extension that were originally added during the configuration process. Linux: /etc/newrelic-infra/logging.d/ Windows: C:\\Program Files\\New Relic\\newrelic-infra\\logging.d\\",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 353.44534,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward your <em>logs</em> using the <em>infrastructure</em> agent",
        "sections": "Sending the <em>infrastructure</em> agent&#x27;s <em>logs</em> to <em>New</em> <em>Relic</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " <em>management</em> capabilities to collect, process, explore, query, and alert on your <em>log</em> data. Basic process To forward your <em>logs</em> through our infrastructure <em>monitoring</em> agent: If you haven&#x27;t already, create a <em>New</em> <em>Relic</em> account. It&#x27;s free, forever. Verify the system requirements needed for configuring <em>logs</em>"
      },
      "id": "603e9df164441f6b6f4e8843"
    },
    {
      "sections": [
        "Forward your logs to New Relic",
        "How it works",
        "Get started with log forwarders",
        "Log forwarding options",
        "TCP endpoint",
        "Log API",
        "What's next?"
      ],
      "title": "Forward your logs to New Relic",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "b6e203ddc367d5a2b5e002916e49d34f4ba17a87",
      "image": "https://docs.newrelic.com/static/c3d5443b84a1e2b26a4767ce35fa58f3/e5166/new-relic-logs-in-context-diagram.jpg",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/enable-log-management-new-relic/",
      "published_at": "2022-01-08T02:40:08Z",
      "updated_at": "2022-01-04T07:02:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our log management capabilities help you to collect, process, explore, query, and alert on your log data. To get your logs into New Relic, you can: Use your existing log forwarding solution to collect your logs and extend the metadata that is forwarded to New Relic. Use our infrastructure agent as a lightweight data collector, without having to use additional software. Use our Log API to forward your logs via HTTP. Use syslog protocols to forward your logs via a TCP endpoint. How it works The following diagram shows the lifecycle of a log message for an app, from enrichment with APM agent metadata (contextual logging), to formatting and forwarding the log data to New Relic. This diagram illustrates the flow of log messages through New Relic. Standard log formatters transform log events into meaningful output (such as text files) that can be used by downstream people and processes. The NewRelicFormatter transforms log events into the JSON format expected by New Relic. These files contain log information and extended metadata. When you configure your log forwarder (our infrastructure monitoring agent, Fluentd, Logstash, etc.), you can also extend and enrich your log data. By configuring logs in context, the log enricher links the formatted log data with additional transaction information from your application or host. Now your log files are enriched with enhanced metadata and contextual logging data. Your log forwarder sends the files to our logging endpoint for processing. From there you can use our log management capabilities to view, query, set up alerts, and more in New Relic. Here is an example of logs for your host's UI. You can see logs in context of events for the selected time period, and drill down into detailed data for any of the highlighted attributes. To take advantage of even more capabilities, click Query logs from here to go directly to the Logs UI. Here is an example of a host's logs in context related to an event. Get started with log forwarders To forward your logs to New Relic with enriched metadata: If you don't have one already, create a New Relic account. It's free, forever. Have your New Relic account's license key. Install a compatible log forwarder. Ensure that outbound connectivity on TCP port 443 is allowed to the CIDR range that matches your region. Use the DNS name log-api.newrelic.com or log-api.eu.newrelic.com for configuration. For apps monitored by a New Relic APM agent, configure logs in context. Log forwarding options Use any of these solutions to forward your logs to New Relic. Recommended: Infrastructure monitoring agent Amazon: AWS CloudWatch plugin AWS FireLens plugin AWS Kinesis Firehose AWS Lambda for sending logs from S3 Microsoft: Azure ARM template Other log forwarding plugins: Fluent Bit plugin Fluentd plugin Google Cloud Platform Pub/Sub Heroku log streaming Kubernetes plugin Logstash plugin Vector plugin TCP endpoint In some situations you may not have log forwarders; for example, with CDNs, hardware devices, or managed services. You can use syslog protocols such as rsyslog and syslog-ng, and forward your logs to New Relic via a TCP endpoint. Log API If you prefer to connect to New Relic without installing a plugin, we offer an HTTP input integration. This option sends your monitored log data directly to New Relic via the Log API. What's next? After you enable your log forwarder, make the most of your data in New Relic with our log management capabilities: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data if you are forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards. For example, to query and manage your data partition rules, see our NerdGraph tutorial.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 314.52017,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward your <em>logs</em> to <em>New</em> <em>Relic</em>",
        "sections": "Forward your <em>logs</em> to <em>New</em> <em>Relic</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " directly to <em>New</em> <em>Relic</em> via the <em>Log</em> API. What&#x27;s next? After you <em>enable</em> your <em>log</em> forwarder, make the most of your data in <em>New</em> <em>Relic</em> with our <em>log</em> <em>management</em> capabilities: Explore the logging data across your platform with our <em>Logs</em> UI. See your <em>logs</em> in context of your app&#x27;s performance in the APM UI"
      },
      "id": "61571e0e28ccbcbc52f21431"
    }
  ],
  "/docs/logs/forward-logs/stream-logs-using-kinesis-data-firehose": [
    {
      "sections": [
        "Fluent Bit plugin for log forwarding",
        "Basic process",
        "Install the Fluent Bit plugin",
        "Tip",
        "Configure the Fluent Bit plugin",
        "Important",
        "Test the Fluent Bit plugin",
        "Associate logs with entities",
        "Optional: Configure plugin attributes",
        "View log data",
        "What's next?"
      ],
      "title": "Fluent Bit plugin for log forwarding",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "dd686f074bb80908ae9de19e543a3bae7d87a466",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/fluent-bit-plugin-log-forwarding/",
      "published_at": "2022-01-08T02:38:07Z",
      "updated_at": "2022-01-08T02:38:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your log data in New Relic. Forwarding your Fluent Bit logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Basic process We have published a container with the plugin installed. It serves as a base image to be used by our Kubernetes integration. We recommend you use this base image and layer your own custom configuration files. To forward your logs from Fluent Bit to New Relic: Make sure you have: A New Relic license key Fluent Bit 1.0 or higher (recommended), although v0.12 or higher is supported Fluent Bit Windows install directions can be found here Fluent Bit Linux install directions can be found here Install the Fluent Bit plugin. Configure the Fluent Bit plugin. Test the Fluent Bit plugin. Generate some traffic and wait a few minutes, then check your account for data. Install the Fluent Bit plugin To install the Fluent Bit plugin: Navigate to New Relic's Fluent Bit plugin repository on GitHub. From the repository page, clone or download the repository. Run the following command to build your plugin: cd newrelic-fluent-bit-output && make all Copy Store out_newrelic.so or out_newrelic_winXX.dll at a location that can be accessed by the fluent-bit daemon. Tip If you'd rather not compile the plugin yourself, you can download pre-compiled versions from our GitHub repository's releases page. Configure the Fluent Bit plugin Fluent Bit needs to know the location of the New Relic plugin and the New Relic license key to output data to New Relic. To configure your Fluent Bit plugin: Important Pay attention to white space when editing your config files. Be sure to use four spaces to indent and one space between keys and values. Locate or create a plugins.conf file in your plugins directory. In the plugins.conf file, add a reference to out_newrelic.so, adjacent to your fluent-bit.conf file: [PLUGINS] Path /PATH/TO/newrelic-fluent-bit-output/out_newrelic.so Copy In the fluent-bit.conf file, add the following line under the service block: [SERVICE] # This is the main configuration block for fluent bit. # Ensure the follow line exists somewhere in the SERVICE block Plugins_File plugins.conf Copy At the bottom of the fluent-bit.conf file, add the following to set up the input and output filters. Replace the placeholder text with your New Relic license key: [INPUT] Name tail Path /PATH/TO/YOUR/LOG/FILE [OUTPUT] Name newrelic Match * licenseKey YOUR_LICENSE_KEY # Optional maxBufferSize 256000 maxRecords 1024 Copy Restart your Fluent Bit instance with the following command: fluent-bit -c /PATH/TO/fluent-bit.conf Copy Test the Fluent Bit plugin To test if your Fluent Bit plugin is receiving input from a log file: Run the following command to append a test log message to your log file: echo \"test message\" >> /PATH/TO/YOUR/LOG/FILE Copy Search the New Relic Logs UI for test message. Associate logs with entities To associate a log line with an entity, such as an infrastructure host, add a FILTER block: [FILTER] Name modify Match * # Or specify a match Add entity.guids <Your Entity GUID ID> # Optional Add hostname <Your hostname> # Optional Copy For more options, see the Fluent Bit modify filter documentation and our documentation to forward your logs using the infrastructure agent. Optional: Configure plugin attributes Once you have installed and configured the Fluent Bit plugin, you can use the following attributes to configure how the plugin sends data to New Relic: Key Description licenseKey The New Relic license key. Use either licenseKey (recommended) or apiKey, not both. Default: none maxBufferSize The maximum size the payloads sent, in bytes. Default: 256000 maxRecords The maximum number of records to send at a time. Default: 1024 apiKey Deprecated. Takes a New Relic Insights insert key, but using the licenseKey attribute is preferred. Use either licenseKey or apiKey, not both. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 374.04816,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Fluent Bit plugin for <em>log</em> forwarding",
        "sections": "Associate <em>logs</em> with entities",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "If your <em>log</em> data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your <em>log</em> data in <em>New</em> <em>Relic</em>. Forwarding your Fluent Bit <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert"
      },
      "id": "603ea7ec28ccbce542eba791"
    },
    {
      "sections": [
        "Forward your logs using the infrastructure agent",
        "Basic process",
        "System requirements",
        "Important",
        "Install the infrastructure agent",
        "Configure the infrastructure agent",
        "Log forwarding parameters",
        "Name (required)",
        "Log source (required)",
        "file",
        "systemd",
        "syslog",
        "tcp",
        "winlog",
        "Optional configuration",
        "attributes",
        "attributes automatically inserted by the infrastructure agent",
        "pattern",
        "max_line_kb",
        "fluentbit",
        "Sample configuration file",
        "logging.d/sample.yaml",
        "View your log data",
        "Troubleshooting",
        "No log data",
        "No data appears when tailing a file",
        "No data appears when capturing via a Syslog socket",
        "No data appears using infrastructure agent proxy",
        "Sending the infrastructure agent's logs to New Relic",
        "Caution",
        "Fluent Bit does not start with the infra agent",
        "Runtime error on Windows",
        "Errors when tailing a large amount of log files (Linux)",
        "What's next?",
        "Uninstall log forwarding"
      ],
      "title": "Forward your logs using the infrastructure agent",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "a1deac410f0eedfb819348524a85a73bbb9d9daf",
      "image": "https://docs.newrelic.com/static/5d4352531871b652878b3cb0768e3f23/c1b63/demotron-host-logs-123021.png",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/forward-your-logs-using-infrastructure-agent/",
      "published_at": "2022-01-09T01:46:14Z",
      "updated_at": "2022-01-07T01:41:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can forward your logs to New Relic using our infrastructure monitoring agent. This makes all of your logging data available in one location and provides deeper visibility into both your application and your platform performance data. Forwarding your logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Basic process To forward your logs through our infrastructure monitoring agent: If you haven't already, create a New Relic account. It's free, forever. Verify the system requirements needed for configuring logs. Ensure you have installed the infrastructure agent, version 1.11.4 or higher. Create a logging.yml configuration file in the infrastructure agent's logging.d directory. Configure your log sources and other parameters. Generate some traffic and wait a few minutes, then check your account for data. Explore your log data in the Logs UI and benefit from the log attributes automatically inserted by the infrastructure agent. Here is an example of logs for your host's UI. You can see logs in context of events for the selected time period, and drill down into detailed data for any of the highlighted attributes. To take advantage of even more capabilities, click Query logs from here to go directly to the Logs UI. Here is an example of a host's logs in context related to an event. System requirements To use the log forwarder of the infrastructure agent, make sure you meet the following requirements: Infrastructure agent version 1.11.4 or higher OpenSSL library 1.1.0 or higher is required by Infrastructure agent starting from 1.16.4. Built-in support for ARM64 architecture on Linux systems (in example, AWS Graviton architecture) added in Infrastructure agent 1.26.0. Important The log forwarding feature is not supported with the Docker container for infrastructure monitoring agents. The log forwarding feature is compatible with the following operating systems: Operating system Supported version Amazon Linux Amazon Linux 2 CentOS Version 7 or higher Debian Version 9 (\"Stretch\") or higher Exception: Version 11 is not supported. Red Hat Enterprise Linux (RHEL) Version 7 or higher SUSE Linux Enterprise Server (SLES) Version 12 Ubuntu Versions 16.04.x, 18.04.x and 20.04.x (LTS versions) Windows Windows Server 2012, 2016, 2019, and 2022, and their service packs. Windows 10 Install the infrastructure agent Starting with version 1.11.4, the infrastructure agent can forward logs to New Relic. To install and run the agent, use a package manager (Linux) or the MSI installer (Windows). Important The log forwarding feature is not included when the infrastructure agent is implemented using Linux tarball or Windows ZIP installations. To use the following links, make sure you are logged to your New Relic account. Amazon Linux CentOS Debian RHEL SLES Ubuntu Windows If you don't have a New Relic account yet, or if you prefer to follow the procedure manually, see our tutorial to install the package manager. Configure the infrastructure agent Configuration files describe which log sources are forwarded. Our infrastructure agent uses .yml files to configure logging. You can add as many config files as you want. To add a new configuration file for the log forwarding feature: Navigate to the log forwarder configuration folder: Linux: /etc/newrelic-infra/logging.d/ Windows: C:\\Program Files\\New Relic\\newrelic-infra\\logging.d\\ Create a logging.yml configuration file, and add the parameters you need. The logging.d directory has various .yml.example files you can use as a reference or starting point. The agent automatically processes new configuration files without having to restart the infrastructure monitoring service. The only exception to this is when configuring a custom Fluent Bit configuration. Log forwarding parameters The infrastructure log forwarding .yml config supports the following parameters: Name (required) To start, define a name of the log or logs you want to forward to New Relic. Log source (required) What you use for the log source will depend on where you want to forward your logs from. Available options include: file Path to the log file or files. The agent tracks changes on the log files in a way similar to tail -f shell. Example: logs: - name: example-log file: /var/log/example.log # Path to a single log file - name: example-log-two file: /var/log/example-two.log # Path to another single log file Copy The file parameter can point to a specific log file or multiple files by using wildcards applied to names and extensions; for example, /logs/*.log. You can use wildcards in place of directories in a file path, which can be used to tail files located in different directories. Example: logs: - name: docker-logs file: /var/lib/docker/containers/*/*.log # Path to multiple folders and files Copy Important Use of wildcards may significantly increase the number of file descriptors and inotify watches the Fluent Bit process keeps open, which can interfere with log collection if the host's file descriptor limit is reached. Tailing a large number of files may require you to increase the maximum number of file descriptors and inotify watchers allowed by the operating system. Please refer to Errors when tailing a large amount of log files for more details on how to increase them. systemd Use the systemd parameter to forward log messages that are collected by the journald daemon in Linux environments. This input type requires the agent to run in root mode. Example: logs: - name: systemd-example systemd: cupsd Copy syslog Syslog data source. Parameters: uri: Syslog socket. Format varies depending on the protocol: TCP/UDP network sockets: [tcp/udp]://LISTEN_ADDRESS:PORT Unix domain sockets: unix_[tcp/udp]:// + /socket/path parser: Syslog parser. Default is rfc3164. Use rfc5424 if your messages include fractional seconds. Note: rfc3164 currently does not work on SuSE. unix_permissions: default is 0644 for domain sockets; this limits entries to processes running as root. You can use 0666 to listen for non-root processes, at your own risk. When running the agent in privileged mode, ports and sockets must be available or owned by nri-agent, with 0666 file permissions, so that other processes can write logs to the sockets. logs: # TCP network socket - name: syslog-tcp-test syslog: uri: tcp://0.0.0.0:5140 # Use the tcp://LISTEN_ADDRESS:PORT format parser: rfc5424 # Default syslog parser is rfc3164 # UDP network socket - name: syslog-udp-test syslog: uri: udp://0.0.0.0:6140 # Use the udp://LISTEN_ADDRESS:PORT format max_line_kb: 35 # Unix TCP domain socket - name: syslog-unix-tcp-test syslog: uri: unix_tcp:///var/unix-tcp-socket-test unix_permissions: 0666 # Default is 0644. Change at your own risk # Unix UDP domain socket - name: syslog-unix-udp-test syslog: uri: unix_udp:///var/unix-udp-socket-test parser: rfc5424 Copy tcp Logs retrieved over TCP connections. Parameters: uri: TCP/IP socket to listen for incoming data. The URI format is tcp://LISTEN_ADDRESS:PORT format: format of the data. It can be json or none. separator: If format: none is used, you can define a separator string for splitting records (default: \\n). logs: - name: tcp-simple-test tcp: uri: tcp://0.0.0.0:1234 # Use the tcp://LISTEN_ADDRESS:PORT format format: none # Raw text - this is default for 'tcp' separator: \\t # String for separating raw text entries max_line_kb: 32 - name: tcp-json-test tcp: uri: tcp://0.0.0.0:2345 # Use the tcp://LISTEN_ADDRESS:PORT format format: json Copy winlog Collect events from Windows log channels. Parameters: channel: name of the channel logs will be collected from. collect-eventids: a list of Windows Event IDs to be collected and forwarded to New Relic. Event ID ranges are supported. exclude-eventids: a list of Windows Event IDs to be excluded from collection. Event ID ranges are supported. All events are collected from the specified channel by default. Configure the collect-eventids and exclude-eventids sections to avoid sending unwanted logs to your New Relic account. Add event IDs or ranges to collect-eventids or exclude-eventids to forward or drop specific events. exclude-eventids takes precedence over collect-eventids if the same event ID is present in both sections. Example: logs: # Winlog log ingestion with eventId filters. - name: windows-security winlog: channel: Security collect-eventids: - 4624 - 4265 - 4700-4800 exclude-eventids: - 4735 # entries for the application, system, powershell, and SCOM channels - name: windows-application winlog: channel: Application - name: windows-system winlog: channel: System - name: windows-pshell winlog: channel: Windows Powershell - name: scom winlog: channel: Operations Manager # Entry for Windows Defender Logs - name: windows-defender winlog: channel: Microsoft-Windows-Windows Defender/Operational # Entry for Windows Clustering Logs - name: windows-clustering winlog: channel: Microsoft-Windows-FailoverClustering/Operational # Entry for IIS logs with logtype attribute for automatic parsing - name: iis-log file: C:\\inetpub\\logs\\LogFiles\\w3svc.log attributes: logtype: iis_w3c Copy Optional configuration The following configuration parameters are not required but are still recommended. attributes List of custom attributes specified as key-value pairs that can be used to send additional data with the logs which you can then query. The attributes configuration parameter can be used with any log source. One common use of the attributes configuration parameter is to specify the logtype attribute. This attribute allows leveraging one of the built-in parsing rules supported by New Relic Logs. Example: logs: - name: example-file-attributes file: /var/log/example.log attributes: logtype: nginx region: example-us-02 team: A-team - name: example-tcp-attributes tcp: uri: tcp://0.0.0.0:2345 format: json attributes: logtype: nginx region: example-us-02 team: B-team Copy attributes automatically inserted by the infrastructure agent The infrastructure agent automatically inserts log attributes for your convenience. Some of them are inserted for any log record, while others depend on the configuration parameters you used while setting up the log forwarder. Attribute name Description entity.guids Always inserted. The infrastructure agent inserts the Entity GUID assigned by New Relic to identify the host where it's running. It is available in the entity.guids field. Note: If the captured logs belong to an application instrumented using APM, the entity.guids field contains both the entity GUID of infrastructure, as well as the GUID of APM, separated by a pipe ( | ) delimiter. fb.input Always inserted. The underlying Fluent Bit input plugin type used to capture the logs. Currently its values are tail, systemd, winlog, syslog, and tcp. filePath Inserted when sing the file input type. Absolute file path of the file being monitored. hostname Always inserted. The hostname of the machine/VM/container executing the infrastructure agent. plugin.type Always inserted. Indicates the utility used to capture the logs. In this case, it is the infrastructure agent itself, so this attribute always has the value nri-agent. pattern Regular expression for filtering records. Only supported for the tail, systemd, syslog, and tcp (only with format none) sources. This field works in a way similar to grep -E in Unix systems. For example, for a given file being captured, you can filter for records containing either WARN or ERROR using: - name: only-records-with-warn-and-error file: /var/log/logFile.log pattern: WARN|ERROR Copy No filtering is applied by default. max_line_kb Maximum size of log entries/lines in KB. If log entries exceed the limit, they are skipped. Default is 128. fluentbit External Fluent Bit configuration and parser files. If defined, they are merged with the existing configuration and parser files generated by the infrastructure agent. The infrastructure agent processes the configuration files located in the logging.d directory and will generate a run-time Fluent Bit configuration file that contains the appropriate [INPUT], [FILTER] and [OUTPUT] sections. Optionally, it will also declare an @INCLUDE in case you provided an external Fluent Bit configuration file via the fluentbit option. The runtime file does not define a [SERVICE] section, leaving all default Fluent Bit configuration values. You can still override Fluent Bit's default settings by defining your own [SERVICE] section in your external Fluent Bit configuration file and include it via the fluentbit option. Parameters: config_file: path to an existing Fluent Bit configuration file. Note that any overlapping source results in duplicate messages in New Relic Logs. parsers_file: path to an existing Fluent Bit parsers file. The following parser names are reserved: rfc3164, rfc3164-local and rfc5424. Sample configuration file Here is an example of a logging.d/ configuration file in YAML format. For more configuration examples, see the infrastructure agent repository. logging.d/sample.yaml # Remember to only use spaces for indentation logs: # Example of 'file' source - name: file-with-attributes file: /var/log/test.log # Path to a single file or pattern attributes: # You can use custom attributes to enrich your data logtype: nginx team: The A Team pattern: Error # Regular expression to filter log entries # Example of 'systemd' source (Linux only) - name: systemd-example systemd: cupsd # Examples of 'syslog' source, one per protocol # TCP network socket - name: syslog-tcp-test syslog: uri: tcp://0.0.0.0:5140 # Use the tcp://LISTEN_ADDRESS:PORT format parser: rfc5424 # Default syslog parser is rfc3164 # UDP network socket - name: syslog-udp-test syslog: uri: udp://0.0.0.0:6140 # Use the udp://LISTEN_ADDRESS:PORT format max_line_kb: 35 # Paths for Unix sockets are defined by combining protocol and path: # unix_udp:// + /path/socket - for example, unix_udp:///tmp/socket # Unix TCP domain socket - name: syslog-unix-tcp-test syslog: uri: unix_tcp:///var/unix-tcp-socket-test unix_permissions: 0666 # Default is 0644. Change at your own risk # Unix UDP domain socket - name: syslog-unix-udp-test syslog: uri: unix_udp:///var/unix-udp-socket-test parser: rfc5424 # Examples of 'tcp' source for formats 'none' and 'json' - name: tcp-simple-test tcp: uri: tcp://0.0.0.0:1234 # Use the tcp://LISTEN_ADDRESS:PORT format format: none # Raw text - this is default for 'tcp' separator: \\t # String for separating raw text entries attributes: # You can add custom attributes to any source of logs tcpFormat: none someOtherAttribute: associatedValue max_line_kb: 32 - name: tcp-json-test tcp: uri: tcp://0.0.0.0:2345 # Use the tcp://LISTEN_ADDRESS:PORT format format: json attributes: tcpFormat: json yetAnotherAttribute: 12345 # Example of Fluent Bit configuration import - name: fluentbit-import fluentbit: config_file: /path/to/fluentbit.config parsers_file: /path/to/fluentbit/parsers.conf Copy View your log data If everything is configured correctly and your data is being collected, you should see logs data in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy Troubleshooting If you encounter problems with configuring your log forwarder, try these troubleshooting tips. No log data If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. No data appears when tailing a file The log forwarding feature requires the agent to have permission to read the data sources. When running the infrastructure agent in privileged or non-privileged modes, make sure that the log files you want to forward (and any intermediary directory in its path) are readable by the user running nri-agent. Example: Check file access under Linux Let's check whether the file /var/log/restrictedLogs/logFile.log can be monitored by the nri-agent user. In Linux, you can do a quick check with the namei command: sudo -u nri-agent namei -ml /var/log/restrictedLogs/logFile.log f: /var/log/restrictedLogs/logFile.log drwxr-xr-x root root / drwxr-xr-x root root var drwxrwxr-x root syslog log drwxr--r-- root root restrictedLogs logFile.log - No such file or directory Copy This command failed because the file is not visible to the nri-agent user. By inspecting the previous output, we can detect that the restrictedLogs directory is missing the execution flag for others. To fix this, execute: sudo chmod 755 /var/log/restrictedLogs Copy And then check for file access again: # sudo -u nri-agent namei -ml /var/log/restrictedLogs/logFile.log f: /var/log/restrictedLogs/logFile.log drwxr-xr-x root root / drwxr-xr-x root root var drwxrwxr-x root syslog log drwxr-xr-x root root restrictedLogs -rw-r----- vagrant vagrant logFile.log Copy The file is now visible to the nri-agent user. You must ensure that the file is also readable by the nri-agent user. To check this, use: # sudo -u nri-agent head /var/log/restrictedLogs/logFile.log head: cannot open '/var/log/restrictedLogs/logFile.log' for reading: Permission denied Copy In this example, the file is missing the read rights for the others group (users other than vagrant and the vagrant user group). You could fix this by granting read permissions to others, but the application could change these permissions upon restart. To avoid this, a better approach is to add the nri-agent user to the vagrant user group. No data appears when capturing via a Syslog socket The log forwarding feature requires that the agent has permission to read the data sources. When running the infrastructure agent in privileged or non-privileged modes: If you're using Unix domain socket files, make sure that the nri-agent user can access these files (please refer to the previous section) and that they have read and write permissions (666) so that other users than nri-agent can write to them. If you're using IP sockets, ensure that the port that you are using is not a system reserved one (like port 80, for example). If no data appears after you enable log management, follow standard log management troubleshooting procedures. No data appears using infrastructure agent proxy As explained in the infrastructure agent configuration guidelines, the proxy parameter must use either HTTP or HTTPS and be in the form https://user:password@hostname:port. The agent can parse the parameter without the HTTP or HTTPS, but the log forwarder cannot. You will see an error like the following in the agent verbose logs: [ERROR] building HTTP transport: parse \\\"hostname:port\\\": first path segment in URL cannot contain colon Copy To solve this problem, check your newrelic-infra.yml file, and ensure the proxy parameter adheres to this form. If you're using caBundleFile or caBundleDir in order to specify any certificate, we recommend to follow the below rules for each OS: Linux For HTTP proxies you don't need to setup any certificates. The plugin loads the system certificates and New Relic sends logs into the logging endpoint. However, you can specify the proxy self-signed certificate (PEM file) using either the caBundleFile or caBundleDir parameters. Windows For HTTP proxies you don't need to setup any certificates. The plugin loads the system certificates. For HTTPS, you can configure it in one of the following ways: Import the proxy certificate to the system pool (Recommended) Import the proxy self-signed certificate (PEM file) by using the MMC tool. Refer to this link, and in Step 2 ensure to import it in your Trusted Root Certification Authorities, instead of in the Intermediate Certification Authorities. Using the caBundleFile and caBundleDir parameters On Windows, we cannot load both the certificates from the system certificate pool and the ones specified with the caBundleFile caBundleDir parameters. So, if you are using caBundleFile or caBundleDir, ensure that the following certificates are placed in the same PEM file (when using caBundleFile) or in the same directory (when using caBundleDir): The Proxy certificate (because it's an HTTPS proxy). The Logging Endpoint certificate (eg. https://log-api.newrelic.com/log/v1). The Infrastructure Agent certificate (eg. https://infra-api.newrelic.com). You can check the certificates by running: # openssl s_client -connect log-api.newrelic.com:443 -servername log-api.newrelic.com Copy Sending the infrastructure agent's logs to New Relic You can configure the infrastructure agent to send its own logs to New Relic. This is useful for troubleshooting issues with log forwarding, the agent, or when contacting support. To forward the infrastructure agent logs to New Relic: Edit your newrelic-infra.yml file. Enable agent logging in troubleshooting mode by adding verbose: 3. On Windows and systems that don't use systemd or where journald is inaccessible, verbose:3 causes the agent to write the logs on the disk. Revert to verbose:0 to prevent this. (Recommended): Enable agent logging in JSON format to log_format: json. Restart the agent to load the new settings. This configuration sets up the agent in troubleshooting mode, but the log forwarder (based on Fluent Bit) will continue in a non-verbose mode. Sometimes you can have issues with the log forwarder itself. For example, there may be problems accessing a specific channel when shipping Windows log events or when accessing a particular log file. In these situations, you can also enable the verbose mode for the log forwarder: Set verbose to a value other than 0. Add the configuration option: trace: [\"log.fw\"]. Caution Check whether you are using the [fluentbit] option. When setting verbose: 3 and trace: [\"log.fw\"], ensure that you don't define any [OUTPUT] section pointing to stdout in an external Fluent Bit configuration file, Fluent Bit does not start with the infra agent Important Fluent Bit's tail plugin does not support network drives. For Linux versions prior to 2016, you may need to update the OpenSSL library to 1.1.0 (or higher). To check if you have this problem: See if infra-agent has started Fluent Bit by running: ps -aux | grep td-agent-bit Copy If it isn't running go to /var/db/newrelic-infra/newrelic-integrations/logging and run: ./fluent-bit -i systemd -o stdout Copy If you get the following error, update OpenSSL to 1.1.0 or higher: error while loading shared libraries: libssl.so.1.1: cannot open shared object file: No such file or directory Copy Runtime error on Windows One of the following error messages may appear when enabling log forwarding on Windows: The code execution cannot proceed because VCRUNTIME140.dll was not found. Copy OR error=\"exit status 3221225781\" process=log-forwarder Copy This is caused by a missing DLL. To solve the issue, install the Microsoft Visual C++ Redistributable as applicable: x64 x86 Errors when tailing a large amount of log files (Linux) It's common to face either one of the following error messages when attempting to tail a large amount of files: Too many open files The user limit on the total number of inotify watches was reached or the kernel failed to allocate a needed resource The operating system defines a maximum amount of allocatable file descriptors (typically 1024 by default), and a maximum amount of allocatable inotify watches (typically 8192 by default). Any process attempting to go above these limits will fail, returning one of the errors above. The underlying technology we use to forward logs, Fluent Bit, opens one file descriptor and sets an inotify watch for each file you configure to be forwarded. Moreover, at the time of writing this section, Fluent Bit uses an extra set of 32 file descriptors for its normal operation, with another extra file descriptor when it shuts down. Therefore, to capture a large amount of files you need to ensure that both the file descriptor and inotify watch limits are slightly greater than the amount of log files you wish to tail. The following instructions summarize how to increase these limits if you want to tail 10,000 log files. Also, it assumes the infrastructure agent is installed in root running mode, and therefore must be run using the root user. Check which is the current hard limit for the amount of file descriptors per process. Typically, this limit should be quite high and should not need to be modified. ulimit -Hn Copy Add the following line to /etc/security/limits.conf. We specified a limit of 10100 here instead of just 10000 to allow Fluent Bit to allocate the extra file descriptors it may need to work. root soft nofile 10100 # replace root by nri-agent for non-root (privileged and unprivileged) installations Copy Add the following line to /etc/pam.d/common-session so that the previous limit is applied upon restart: session required pam_limits.so Copy Add the following line to /etc/sysctl.conf to increase the amount of allowed inotify watchers per user. We specified a limit of 18192 here instead of just 10000 so that the root user will still have 8192 available inotify watches (the default value). fs.inotify.max_user_watches=18192 Copy Restart your system. Ensure that the new limits have been enforced by running: ulimit -Sn # Should return 10100 cat /proc/sys/fs/inotify/max_user_watches # Should return 18192 Copy Learn more on how to increase open file limits, or on how to increase the inotify watches. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards. Uninstall log forwarding To uninstall log forwarding capabilities, go to your logging.d directory, and remove files with the .yml extension that were originally added during the configuration process. Linux: /etc/newrelic-infra/logging.d/ Windows: C:\\Program Files\\New Relic\\newrelic-infra\\logging.d\\",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 353.44534,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward your <em>logs</em> using the <em>infrastructure</em> agent",
        "sections": "Sending the <em>infrastructure</em> agent&#x27;s <em>logs</em> to <em>New</em> <em>Relic</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " <em>management</em> capabilities to collect, process, explore, query, and alert on your <em>log</em> data. Basic process To forward your <em>logs</em> through our infrastructure <em>monitoring</em> agent: If you haven&#x27;t already, create a <em>New</em> <em>Relic</em> account. It&#x27;s free, forever. Verify the system requirements needed for configuring <em>logs</em>"
      },
      "id": "603e9df164441f6b6f4e8843"
    },
    {
      "sections": [
        "Forward your logs to New Relic",
        "How it works",
        "Get started with log forwarders",
        "Log forwarding options",
        "TCP endpoint",
        "Log API",
        "What's next?"
      ],
      "title": "Forward your logs to New Relic",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "b6e203ddc367d5a2b5e002916e49d34f4ba17a87",
      "image": "https://docs.newrelic.com/static/c3d5443b84a1e2b26a4767ce35fa58f3/e5166/new-relic-logs-in-context-diagram.jpg",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/enable-log-management-new-relic/",
      "published_at": "2022-01-08T02:40:08Z",
      "updated_at": "2022-01-04T07:02:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our log management capabilities help you to collect, process, explore, query, and alert on your log data. To get your logs into New Relic, you can: Use your existing log forwarding solution to collect your logs and extend the metadata that is forwarded to New Relic. Use our infrastructure agent as a lightweight data collector, without having to use additional software. Use our Log API to forward your logs via HTTP. Use syslog protocols to forward your logs via a TCP endpoint. How it works The following diagram shows the lifecycle of a log message for an app, from enrichment with APM agent metadata (contextual logging), to formatting and forwarding the log data to New Relic. This diagram illustrates the flow of log messages through New Relic. Standard log formatters transform log events into meaningful output (such as text files) that can be used by downstream people and processes. The NewRelicFormatter transforms log events into the JSON format expected by New Relic. These files contain log information and extended metadata. When you configure your log forwarder (our infrastructure monitoring agent, Fluentd, Logstash, etc.), you can also extend and enrich your log data. By configuring logs in context, the log enricher links the formatted log data with additional transaction information from your application or host. Now your log files are enriched with enhanced metadata and contextual logging data. Your log forwarder sends the files to our logging endpoint for processing. From there you can use our log management capabilities to view, query, set up alerts, and more in New Relic. Here is an example of logs for your host's UI. You can see logs in context of events for the selected time period, and drill down into detailed data for any of the highlighted attributes. To take advantage of even more capabilities, click Query logs from here to go directly to the Logs UI. Here is an example of a host's logs in context related to an event. Get started with log forwarders To forward your logs to New Relic with enriched metadata: If you don't have one already, create a New Relic account. It's free, forever. Have your New Relic account's license key. Install a compatible log forwarder. Ensure that outbound connectivity on TCP port 443 is allowed to the CIDR range that matches your region. Use the DNS name log-api.newrelic.com or log-api.eu.newrelic.com for configuration. For apps monitored by a New Relic APM agent, configure logs in context. Log forwarding options Use any of these solutions to forward your logs to New Relic. Recommended: Infrastructure monitoring agent Amazon: AWS CloudWatch plugin AWS FireLens plugin AWS Kinesis Firehose AWS Lambda for sending logs from S3 Microsoft: Azure ARM template Other log forwarding plugins: Fluent Bit plugin Fluentd plugin Google Cloud Platform Pub/Sub Heroku log streaming Kubernetes plugin Logstash plugin Vector plugin TCP endpoint In some situations you may not have log forwarders; for example, with CDNs, hardware devices, or managed services. You can use syslog protocols such as rsyslog and syslog-ng, and forward your logs to New Relic via a TCP endpoint. Log API If you prefer to connect to New Relic without installing a plugin, we offer an HTTP input integration. This option sends your monitored log data directly to New Relic via the Log API. What's next? After you enable your log forwarder, make the most of your data in New Relic with our log management capabilities: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data if you are forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards. For example, to query and manage your data partition rules, see our NerdGraph tutorial.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 314.52017,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward your <em>logs</em> to <em>New</em> <em>Relic</em>",
        "sections": "Forward your <em>logs</em> to <em>New</em> <em>Relic</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " directly to <em>New</em> <em>Relic</em> via the <em>Log</em> API. What&#x27;s next? After you <em>enable</em> your <em>log</em> forwarder, make the most of your data in <em>New</em> <em>Relic</em> with our <em>log</em> <em>management</em> capabilities: Explore the logging data across your platform with our <em>Logs</em> UI. See your <em>logs</em> in context of your app&#x27;s performance in the APM UI"
      },
      "id": "61571e0e28ccbcbc52f21431"
    }
  ],
  "/docs/logs/forward-logs/vector-output-sink-log-forwarding": [
    {
      "sections": [
        "Fluent Bit plugin for log forwarding",
        "Basic process",
        "Install the Fluent Bit plugin",
        "Tip",
        "Configure the Fluent Bit plugin",
        "Important",
        "Test the Fluent Bit plugin",
        "Associate logs with entities",
        "Optional: Configure plugin attributes",
        "View log data",
        "What's next?"
      ],
      "title": "Fluent Bit plugin for log forwarding",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "dd686f074bb80908ae9de19e543a3bae7d87a466",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/fluent-bit-plugin-log-forwarding/",
      "published_at": "2022-01-08T02:38:07Z",
      "updated_at": "2022-01-08T02:38:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your log data in New Relic. Forwarding your Fluent Bit logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Basic process We have published a container with the plugin installed. It serves as a base image to be used by our Kubernetes integration. We recommend you use this base image and layer your own custom configuration files. To forward your logs from Fluent Bit to New Relic: Make sure you have: A New Relic license key Fluent Bit 1.0 or higher (recommended), although v0.12 or higher is supported Fluent Bit Windows install directions can be found here Fluent Bit Linux install directions can be found here Install the Fluent Bit plugin. Configure the Fluent Bit plugin. Test the Fluent Bit plugin. Generate some traffic and wait a few minutes, then check your account for data. Install the Fluent Bit plugin To install the Fluent Bit plugin: Navigate to New Relic's Fluent Bit plugin repository on GitHub. From the repository page, clone or download the repository. Run the following command to build your plugin: cd newrelic-fluent-bit-output && make all Copy Store out_newrelic.so or out_newrelic_winXX.dll at a location that can be accessed by the fluent-bit daemon. Tip If you'd rather not compile the plugin yourself, you can download pre-compiled versions from our GitHub repository's releases page. Configure the Fluent Bit plugin Fluent Bit needs to know the location of the New Relic plugin and the New Relic license key to output data to New Relic. To configure your Fluent Bit plugin: Important Pay attention to white space when editing your config files. Be sure to use four spaces to indent and one space between keys and values. Locate or create a plugins.conf file in your plugins directory. In the plugins.conf file, add a reference to out_newrelic.so, adjacent to your fluent-bit.conf file: [PLUGINS] Path /PATH/TO/newrelic-fluent-bit-output/out_newrelic.so Copy In the fluent-bit.conf file, add the following line under the service block: [SERVICE] # This is the main configuration block for fluent bit. # Ensure the follow line exists somewhere in the SERVICE block Plugins_File plugins.conf Copy At the bottom of the fluent-bit.conf file, add the following to set up the input and output filters. Replace the placeholder text with your New Relic license key: [INPUT] Name tail Path /PATH/TO/YOUR/LOG/FILE [OUTPUT] Name newrelic Match * licenseKey YOUR_LICENSE_KEY # Optional maxBufferSize 256000 maxRecords 1024 Copy Restart your Fluent Bit instance with the following command: fluent-bit -c /PATH/TO/fluent-bit.conf Copy Test the Fluent Bit plugin To test if your Fluent Bit plugin is receiving input from a log file: Run the following command to append a test log message to your log file: echo \"test message\" >> /PATH/TO/YOUR/LOG/FILE Copy Search the New Relic Logs UI for test message. Associate logs with entities To associate a log line with an entity, such as an infrastructure host, add a FILTER block: [FILTER] Name modify Match * # Or specify a match Add entity.guids <Your Entity GUID ID> # Optional Add hostname <Your hostname> # Optional Copy For more options, see the Fluent Bit modify filter documentation and our documentation to forward your logs using the infrastructure agent. Optional: Configure plugin attributes Once you have installed and configured the Fluent Bit plugin, you can use the following attributes to configure how the plugin sends data to New Relic: Key Description licenseKey The New Relic license key. Use either licenseKey (recommended) or apiKey, not both. Default: none maxBufferSize The maximum size the payloads sent, in bytes. Default: 256000 maxRecords The maximum number of records to send at a time. Default: 1024 apiKey Deprecated. Takes a New Relic Insights insert key, but using the licenseKey attribute is preferred. Use either licenseKey or apiKey, not both. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 374.04816,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Fluent Bit plugin for <em>log</em> forwarding",
        "sections": "Associate <em>logs</em> with entities",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "If your <em>log</em> data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your <em>log</em> data in <em>New</em> <em>Relic</em>. Forwarding your Fluent Bit <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert"
      },
      "id": "603ea7ec28ccbce542eba791"
    },
    {
      "sections": [
        "Forward your logs using the infrastructure agent",
        "Basic process",
        "System requirements",
        "Important",
        "Install the infrastructure agent",
        "Configure the infrastructure agent",
        "Log forwarding parameters",
        "Name (required)",
        "Log source (required)",
        "file",
        "systemd",
        "syslog",
        "tcp",
        "winlog",
        "Optional configuration",
        "attributes",
        "attributes automatically inserted by the infrastructure agent",
        "pattern",
        "max_line_kb",
        "fluentbit",
        "Sample configuration file",
        "logging.d/sample.yaml",
        "View your log data",
        "Troubleshooting",
        "No log data",
        "No data appears when tailing a file",
        "No data appears when capturing via a Syslog socket",
        "No data appears using infrastructure agent proxy",
        "Sending the infrastructure agent's logs to New Relic",
        "Caution",
        "Fluent Bit does not start with the infra agent",
        "Runtime error on Windows",
        "Errors when tailing a large amount of log files (Linux)",
        "What's next?",
        "Uninstall log forwarding"
      ],
      "title": "Forward your logs using the infrastructure agent",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "a1deac410f0eedfb819348524a85a73bbb9d9daf",
      "image": "https://docs.newrelic.com/static/5d4352531871b652878b3cb0768e3f23/c1b63/demotron-host-logs-123021.png",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/forward-your-logs-using-infrastructure-agent/",
      "published_at": "2022-01-09T01:46:14Z",
      "updated_at": "2022-01-07T01:41:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can forward your logs to New Relic using our infrastructure monitoring agent. This makes all of your logging data available in one location and provides deeper visibility into both your application and your platform performance data. Forwarding your logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Basic process To forward your logs through our infrastructure monitoring agent: If you haven't already, create a New Relic account. It's free, forever. Verify the system requirements needed for configuring logs. Ensure you have installed the infrastructure agent, version 1.11.4 or higher. Create a logging.yml configuration file in the infrastructure agent's logging.d directory. Configure your log sources and other parameters. Generate some traffic and wait a few minutes, then check your account for data. Explore your log data in the Logs UI and benefit from the log attributes automatically inserted by the infrastructure agent. Here is an example of logs for your host's UI. You can see logs in context of events for the selected time period, and drill down into detailed data for any of the highlighted attributes. To take advantage of even more capabilities, click Query logs from here to go directly to the Logs UI. Here is an example of a host's logs in context related to an event. System requirements To use the log forwarder of the infrastructure agent, make sure you meet the following requirements: Infrastructure agent version 1.11.4 or higher OpenSSL library 1.1.0 or higher is required by Infrastructure agent starting from 1.16.4. Built-in support for ARM64 architecture on Linux systems (in example, AWS Graviton architecture) added in Infrastructure agent 1.26.0. Important The log forwarding feature is not supported with the Docker container for infrastructure monitoring agents. The log forwarding feature is compatible with the following operating systems: Operating system Supported version Amazon Linux Amazon Linux 2 CentOS Version 7 or higher Debian Version 9 (\"Stretch\") or higher Exception: Version 11 is not supported. Red Hat Enterprise Linux (RHEL) Version 7 or higher SUSE Linux Enterprise Server (SLES) Version 12 Ubuntu Versions 16.04.x, 18.04.x and 20.04.x (LTS versions) Windows Windows Server 2012, 2016, 2019, and 2022, and their service packs. Windows 10 Install the infrastructure agent Starting with version 1.11.4, the infrastructure agent can forward logs to New Relic. To install and run the agent, use a package manager (Linux) or the MSI installer (Windows). Important The log forwarding feature is not included when the infrastructure agent is implemented using Linux tarball or Windows ZIP installations. To use the following links, make sure you are logged to your New Relic account. Amazon Linux CentOS Debian RHEL SLES Ubuntu Windows If you don't have a New Relic account yet, or if you prefer to follow the procedure manually, see our tutorial to install the package manager. Configure the infrastructure agent Configuration files describe which log sources are forwarded. Our infrastructure agent uses .yml files to configure logging. You can add as many config files as you want. To add a new configuration file for the log forwarding feature: Navigate to the log forwarder configuration folder: Linux: /etc/newrelic-infra/logging.d/ Windows: C:\\Program Files\\New Relic\\newrelic-infra\\logging.d\\ Create a logging.yml configuration file, and add the parameters you need. The logging.d directory has various .yml.example files you can use as a reference or starting point. The agent automatically processes new configuration files without having to restart the infrastructure monitoring service. The only exception to this is when configuring a custom Fluent Bit configuration. Log forwarding parameters The infrastructure log forwarding .yml config supports the following parameters: Name (required) To start, define a name of the log or logs you want to forward to New Relic. Log source (required) What you use for the log source will depend on where you want to forward your logs from. Available options include: file Path to the log file or files. The agent tracks changes on the log files in a way similar to tail -f shell. Example: logs: - name: example-log file: /var/log/example.log # Path to a single log file - name: example-log-two file: /var/log/example-two.log # Path to another single log file Copy The file parameter can point to a specific log file or multiple files by using wildcards applied to names and extensions; for example, /logs/*.log. You can use wildcards in place of directories in a file path, which can be used to tail files located in different directories. Example: logs: - name: docker-logs file: /var/lib/docker/containers/*/*.log # Path to multiple folders and files Copy Important Use of wildcards may significantly increase the number of file descriptors and inotify watches the Fluent Bit process keeps open, which can interfere with log collection if the host's file descriptor limit is reached. Tailing a large number of files may require you to increase the maximum number of file descriptors and inotify watchers allowed by the operating system. Please refer to Errors when tailing a large amount of log files for more details on how to increase them. systemd Use the systemd parameter to forward log messages that are collected by the journald daemon in Linux environments. This input type requires the agent to run in root mode. Example: logs: - name: systemd-example systemd: cupsd Copy syslog Syslog data source. Parameters: uri: Syslog socket. Format varies depending on the protocol: TCP/UDP network sockets: [tcp/udp]://LISTEN_ADDRESS:PORT Unix domain sockets: unix_[tcp/udp]:// + /socket/path parser: Syslog parser. Default is rfc3164. Use rfc5424 if your messages include fractional seconds. Note: rfc3164 currently does not work on SuSE. unix_permissions: default is 0644 for domain sockets; this limits entries to processes running as root. You can use 0666 to listen for non-root processes, at your own risk. When running the agent in privileged mode, ports and sockets must be available or owned by nri-agent, with 0666 file permissions, so that other processes can write logs to the sockets. logs: # TCP network socket - name: syslog-tcp-test syslog: uri: tcp://0.0.0.0:5140 # Use the tcp://LISTEN_ADDRESS:PORT format parser: rfc5424 # Default syslog parser is rfc3164 # UDP network socket - name: syslog-udp-test syslog: uri: udp://0.0.0.0:6140 # Use the udp://LISTEN_ADDRESS:PORT format max_line_kb: 35 # Unix TCP domain socket - name: syslog-unix-tcp-test syslog: uri: unix_tcp:///var/unix-tcp-socket-test unix_permissions: 0666 # Default is 0644. Change at your own risk # Unix UDP domain socket - name: syslog-unix-udp-test syslog: uri: unix_udp:///var/unix-udp-socket-test parser: rfc5424 Copy tcp Logs retrieved over TCP connections. Parameters: uri: TCP/IP socket to listen for incoming data. The URI format is tcp://LISTEN_ADDRESS:PORT format: format of the data. It can be json or none. separator: If format: none is used, you can define a separator string for splitting records (default: \\n). logs: - name: tcp-simple-test tcp: uri: tcp://0.0.0.0:1234 # Use the tcp://LISTEN_ADDRESS:PORT format format: none # Raw text - this is default for 'tcp' separator: \\t # String for separating raw text entries max_line_kb: 32 - name: tcp-json-test tcp: uri: tcp://0.0.0.0:2345 # Use the tcp://LISTEN_ADDRESS:PORT format format: json Copy winlog Collect events from Windows log channels. Parameters: channel: name of the channel logs will be collected from. collect-eventids: a list of Windows Event IDs to be collected and forwarded to New Relic. Event ID ranges are supported. exclude-eventids: a list of Windows Event IDs to be excluded from collection. Event ID ranges are supported. All events are collected from the specified channel by default. Configure the collect-eventids and exclude-eventids sections to avoid sending unwanted logs to your New Relic account. Add event IDs or ranges to collect-eventids or exclude-eventids to forward or drop specific events. exclude-eventids takes precedence over collect-eventids if the same event ID is present in both sections. Example: logs: # Winlog log ingestion with eventId filters. - name: windows-security winlog: channel: Security collect-eventids: - 4624 - 4265 - 4700-4800 exclude-eventids: - 4735 # entries for the application, system, powershell, and SCOM channels - name: windows-application winlog: channel: Application - name: windows-system winlog: channel: System - name: windows-pshell winlog: channel: Windows Powershell - name: scom winlog: channel: Operations Manager # Entry for Windows Defender Logs - name: windows-defender winlog: channel: Microsoft-Windows-Windows Defender/Operational # Entry for Windows Clustering Logs - name: windows-clustering winlog: channel: Microsoft-Windows-FailoverClustering/Operational # Entry for IIS logs with logtype attribute for automatic parsing - name: iis-log file: C:\\inetpub\\logs\\LogFiles\\w3svc.log attributes: logtype: iis_w3c Copy Optional configuration The following configuration parameters are not required but are still recommended. attributes List of custom attributes specified as key-value pairs that can be used to send additional data with the logs which you can then query. The attributes configuration parameter can be used with any log source. One common use of the attributes configuration parameter is to specify the logtype attribute. This attribute allows leveraging one of the built-in parsing rules supported by New Relic Logs. Example: logs: - name: example-file-attributes file: /var/log/example.log attributes: logtype: nginx region: example-us-02 team: A-team - name: example-tcp-attributes tcp: uri: tcp://0.0.0.0:2345 format: json attributes: logtype: nginx region: example-us-02 team: B-team Copy attributes automatically inserted by the infrastructure agent The infrastructure agent automatically inserts log attributes for your convenience. Some of them are inserted for any log record, while others depend on the configuration parameters you used while setting up the log forwarder. Attribute name Description entity.guids Always inserted. The infrastructure agent inserts the Entity GUID assigned by New Relic to identify the host where it's running. It is available in the entity.guids field. Note: If the captured logs belong to an application instrumented using APM, the entity.guids field contains both the entity GUID of infrastructure, as well as the GUID of APM, separated by a pipe ( | ) delimiter. fb.input Always inserted. The underlying Fluent Bit input plugin type used to capture the logs. Currently its values are tail, systemd, winlog, syslog, and tcp. filePath Inserted when sing the file input type. Absolute file path of the file being monitored. hostname Always inserted. The hostname of the machine/VM/container executing the infrastructure agent. plugin.type Always inserted. Indicates the utility used to capture the logs. In this case, it is the infrastructure agent itself, so this attribute always has the value nri-agent. pattern Regular expression for filtering records. Only supported for the tail, systemd, syslog, and tcp (only with format none) sources. This field works in a way similar to grep -E in Unix systems. For example, for a given file being captured, you can filter for records containing either WARN or ERROR using: - name: only-records-with-warn-and-error file: /var/log/logFile.log pattern: WARN|ERROR Copy No filtering is applied by default. max_line_kb Maximum size of log entries/lines in KB. If log entries exceed the limit, they are skipped. Default is 128. fluentbit External Fluent Bit configuration and parser files. If defined, they are merged with the existing configuration and parser files generated by the infrastructure agent. The infrastructure agent processes the configuration files located in the logging.d directory and will generate a run-time Fluent Bit configuration file that contains the appropriate [INPUT], [FILTER] and [OUTPUT] sections. Optionally, it will also declare an @INCLUDE in case you provided an external Fluent Bit configuration file via the fluentbit option. The runtime file does not define a [SERVICE] section, leaving all default Fluent Bit configuration values. You can still override Fluent Bit's default settings by defining your own [SERVICE] section in your external Fluent Bit configuration file and include it via the fluentbit option. Parameters: config_file: path to an existing Fluent Bit configuration file. Note that any overlapping source results in duplicate messages in New Relic Logs. parsers_file: path to an existing Fluent Bit parsers file. The following parser names are reserved: rfc3164, rfc3164-local and rfc5424. Sample configuration file Here is an example of a logging.d/ configuration file in YAML format. For more configuration examples, see the infrastructure agent repository. logging.d/sample.yaml # Remember to only use spaces for indentation logs: # Example of 'file' source - name: file-with-attributes file: /var/log/test.log # Path to a single file or pattern attributes: # You can use custom attributes to enrich your data logtype: nginx team: The A Team pattern: Error # Regular expression to filter log entries # Example of 'systemd' source (Linux only) - name: systemd-example systemd: cupsd # Examples of 'syslog' source, one per protocol # TCP network socket - name: syslog-tcp-test syslog: uri: tcp://0.0.0.0:5140 # Use the tcp://LISTEN_ADDRESS:PORT format parser: rfc5424 # Default syslog parser is rfc3164 # UDP network socket - name: syslog-udp-test syslog: uri: udp://0.0.0.0:6140 # Use the udp://LISTEN_ADDRESS:PORT format max_line_kb: 35 # Paths for Unix sockets are defined by combining protocol and path: # unix_udp:// + /path/socket - for example, unix_udp:///tmp/socket # Unix TCP domain socket - name: syslog-unix-tcp-test syslog: uri: unix_tcp:///var/unix-tcp-socket-test unix_permissions: 0666 # Default is 0644. Change at your own risk # Unix UDP domain socket - name: syslog-unix-udp-test syslog: uri: unix_udp:///var/unix-udp-socket-test parser: rfc5424 # Examples of 'tcp' source for formats 'none' and 'json' - name: tcp-simple-test tcp: uri: tcp://0.0.0.0:1234 # Use the tcp://LISTEN_ADDRESS:PORT format format: none # Raw text - this is default for 'tcp' separator: \\t # String for separating raw text entries attributes: # You can add custom attributes to any source of logs tcpFormat: none someOtherAttribute: associatedValue max_line_kb: 32 - name: tcp-json-test tcp: uri: tcp://0.0.0.0:2345 # Use the tcp://LISTEN_ADDRESS:PORT format format: json attributes: tcpFormat: json yetAnotherAttribute: 12345 # Example of Fluent Bit configuration import - name: fluentbit-import fluentbit: config_file: /path/to/fluentbit.config parsers_file: /path/to/fluentbit/parsers.conf Copy View your log data If everything is configured correctly and your data is being collected, you should see logs data in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy Troubleshooting If you encounter problems with configuring your log forwarder, try these troubleshooting tips. No log data If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. No data appears when tailing a file The log forwarding feature requires the agent to have permission to read the data sources. When running the infrastructure agent in privileged or non-privileged modes, make sure that the log files you want to forward (and any intermediary directory in its path) are readable by the user running nri-agent. Example: Check file access under Linux Let's check whether the file /var/log/restrictedLogs/logFile.log can be monitored by the nri-agent user. In Linux, you can do a quick check with the namei command: sudo -u nri-agent namei -ml /var/log/restrictedLogs/logFile.log f: /var/log/restrictedLogs/logFile.log drwxr-xr-x root root / drwxr-xr-x root root var drwxrwxr-x root syslog log drwxr--r-- root root restrictedLogs logFile.log - No such file or directory Copy This command failed because the file is not visible to the nri-agent user. By inspecting the previous output, we can detect that the restrictedLogs directory is missing the execution flag for others. To fix this, execute: sudo chmod 755 /var/log/restrictedLogs Copy And then check for file access again: # sudo -u nri-agent namei -ml /var/log/restrictedLogs/logFile.log f: /var/log/restrictedLogs/logFile.log drwxr-xr-x root root / drwxr-xr-x root root var drwxrwxr-x root syslog log drwxr-xr-x root root restrictedLogs -rw-r----- vagrant vagrant logFile.log Copy The file is now visible to the nri-agent user. You must ensure that the file is also readable by the nri-agent user. To check this, use: # sudo -u nri-agent head /var/log/restrictedLogs/logFile.log head: cannot open '/var/log/restrictedLogs/logFile.log' for reading: Permission denied Copy In this example, the file is missing the read rights for the others group (users other than vagrant and the vagrant user group). You could fix this by granting read permissions to others, but the application could change these permissions upon restart. To avoid this, a better approach is to add the nri-agent user to the vagrant user group. No data appears when capturing via a Syslog socket The log forwarding feature requires that the agent has permission to read the data sources. When running the infrastructure agent in privileged or non-privileged modes: If you're using Unix domain socket files, make sure that the nri-agent user can access these files (please refer to the previous section) and that they have read and write permissions (666) so that other users than nri-agent can write to them. If you're using IP sockets, ensure that the port that you are using is not a system reserved one (like port 80, for example). If no data appears after you enable log management, follow standard log management troubleshooting procedures. No data appears using infrastructure agent proxy As explained in the infrastructure agent configuration guidelines, the proxy parameter must use either HTTP or HTTPS and be in the form https://user:password@hostname:port. The agent can parse the parameter without the HTTP or HTTPS, but the log forwarder cannot. You will see an error like the following in the agent verbose logs: [ERROR] building HTTP transport: parse \\\"hostname:port\\\": first path segment in URL cannot contain colon Copy To solve this problem, check your newrelic-infra.yml file, and ensure the proxy parameter adheres to this form. If you're using caBundleFile or caBundleDir in order to specify any certificate, we recommend to follow the below rules for each OS: Linux For HTTP proxies you don't need to setup any certificates. The plugin loads the system certificates and New Relic sends logs into the logging endpoint. However, you can specify the proxy self-signed certificate (PEM file) using either the caBundleFile or caBundleDir parameters. Windows For HTTP proxies you don't need to setup any certificates. The plugin loads the system certificates. For HTTPS, you can configure it in one of the following ways: Import the proxy certificate to the system pool (Recommended) Import the proxy self-signed certificate (PEM file) by using the MMC tool. Refer to this link, and in Step 2 ensure to import it in your Trusted Root Certification Authorities, instead of in the Intermediate Certification Authorities. Using the caBundleFile and caBundleDir parameters On Windows, we cannot load both the certificates from the system certificate pool and the ones specified with the caBundleFile caBundleDir parameters. So, if you are using caBundleFile or caBundleDir, ensure that the following certificates are placed in the same PEM file (when using caBundleFile) or in the same directory (when using caBundleDir): The Proxy certificate (because it's an HTTPS proxy). The Logging Endpoint certificate (eg. https://log-api.newrelic.com/log/v1). The Infrastructure Agent certificate (eg. https://infra-api.newrelic.com). You can check the certificates by running: # openssl s_client -connect log-api.newrelic.com:443 -servername log-api.newrelic.com Copy Sending the infrastructure agent's logs to New Relic You can configure the infrastructure agent to send its own logs to New Relic. This is useful for troubleshooting issues with log forwarding, the agent, or when contacting support. To forward the infrastructure agent logs to New Relic: Edit your newrelic-infra.yml file. Enable agent logging in troubleshooting mode by adding verbose: 3. On Windows and systems that don't use systemd or where journald is inaccessible, verbose:3 causes the agent to write the logs on the disk. Revert to verbose:0 to prevent this. (Recommended): Enable agent logging in JSON format to log_format: json. Restart the agent to load the new settings. This configuration sets up the agent in troubleshooting mode, but the log forwarder (based on Fluent Bit) will continue in a non-verbose mode. Sometimes you can have issues with the log forwarder itself. For example, there may be problems accessing a specific channel when shipping Windows log events or when accessing a particular log file. In these situations, you can also enable the verbose mode for the log forwarder: Set verbose to a value other than 0. Add the configuration option: trace: [\"log.fw\"]. Caution Check whether you are using the [fluentbit] option. When setting verbose: 3 and trace: [\"log.fw\"], ensure that you don't define any [OUTPUT] section pointing to stdout in an external Fluent Bit configuration file, Fluent Bit does not start with the infra agent Important Fluent Bit's tail plugin does not support network drives. For Linux versions prior to 2016, you may need to update the OpenSSL library to 1.1.0 (or higher). To check if you have this problem: See if infra-agent has started Fluent Bit by running: ps -aux | grep td-agent-bit Copy If it isn't running go to /var/db/newrelic-infra/newrelic-integrations/logging and run: ./fluent-bit -i systemd -o stdout Copy If you get the following error, update OpenSSL to 1.1.0 or higher: error while loading shared libraries: libssl.so.1.1: cannot open shared object file: No such file or directory Copy Runtime error on Windows One of the following error messages may appear when enabling log forwarding on Windows: The code execution cannot proceed because VCRUNTIME140.dll was not found. Copy OR error=\"exit status 3221225781\" process=log-forwarder Copy This is caused by a missing DLL. To solve the issue, install the Microsoft Visual C++ Redistributable as applicable: x64 x86 Errors when tailing a large amount of log files (Linux) It's common to face either one of the following error messages when attempting to tail a large amount of files: Too many open files The user limit on the total number of inotify watches was reached or the kernel failed to allocate a needed resource The operating system defines a maximum amount of allocatable file descriptors (typically 1024 by default), and a maximum amount of allocatable inotify watches (typically 8192 by default). Any process attempting to go above these limits will fail, returning one of the errors above. The underlying technology we use to forward logs, Fluent Bit, opens one file descriptor and sets an inotify watch for each file you configure to be forwarded. Moreover, at the time of writing this section, Fluent Bit uses an extra set of 32 file descriptors for its normal operation, with another extra file descriptor when it shuts down. Therefore, to capture a large amount of files you need to ensure that both the file descriptor and inotify watch limits are slightly greater than the amount of log files you wish to tail. The following instructions summarize how to increase these limits if you want to tail 10,000 log files. Also, it assumes the infrastructure agent is installed in root running mode, and therefore must be run using the root user. Check which is the current hard limit for the amount of file descriptors per process. Typically, this limit should be quite high and should not need to be modified. ulimit -Hn Copy Add the following line to /etc/security/limits.conf. We specified a limit of 10100 here instead of just 10000 to allow Fluent Bit to allocate the extra file descriptors it may need to work. root soft nofile 10100 # replace root by nri-agent for non-root (privileged and unprivileged) installations Copy Add the following line to /etc/pam.d/common-session so that the previous limit is applied upon restart: session required pam_limits.so Copy Add the following line to /etc/sysctl.conf to increase the amount of allowed inotify watchers per user. We specified a limit of 18192 here instead of just 10000 so that the root user will still have 8192 available inotify watches (the default value). fs.inotify.max_user_watches=18192 Copy Restart your system. Ensure that the new limits have been enforced by running: ulimit -Sn # Should return 10100 cat /proc/sys/fs/inotify/max_user_watches # Should return 18192 Copy Learn more on how to increase open file limits, or on how to increase the inotify watches. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards. Uninstall log forwarding To uninstall log forwarding capabilities, go to your logging.d directory, and remove files with the .yml extension that were originally added during the configuration process. Linux: /etc/newrelic-infra/logging.d/ Windows: C:\\Program Files\\New Relic\\newrelic-infra\\logging.d\\",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 353.44534,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward your <em>logs</em> using the <em>infrastructure</em> agent",
        "sections": "Sending the <em>infrastructure</em> agent&#x27;s <em>logs</em> to <em>New</em> <em>Relic</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " <em>management</em> capabilities to collect, process, explore, query, and alert on your <em>log</em> data. Basic process To forward your <em>logs</em> through our infrastructure <em>monitoring</em> agent: If you haven&#x27;t already, create a <em>New</em> <em>Relic</em> account. It&#x27;s free, forever. Verify the system requirements needed for configuring <em>logs</em>"
      },
      "id": "603e9df164441f6b6f4e8843"
    },
    {
      "sections": [
        "Forward your logs to New Relic",
        "How it works",
        "Get started with log forwarders",
        "Log forwarding options",
        "TCP endpoint",
        "Log API",
        "What's next?"
      ],
      "title": "Forward your logs to New Relic",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "b6e203ddc367d5a2b5e002916e49d34f4ba17a87",
      "image": "https://docs.newrelic.com/static/c3d5443b84a1e2b26a4767ce35fa58f3/e5166/new-relic-logs-in-context-diagram.jpg",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/enable-log-management-new-relic/",
      "published_at": "2022-01-08T02:40:08Z",
      "updated_at": "2022-01-04T07:02:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our log management capabilities help you to collect, process, explore, query, and alert on your log data. To get your logs into New Relic, you can: Use your existing log forwarding solution to collect your logs and extend the metadata that is forwarded to New Relic. Use our infrastructure agent as a lightweight data collector, without having to use additional software. Use our Log API to forward your logs via HTTP. Use syslog protocols to forward your logs via a TCP endpoint. How it works The following diagram shows the lifecycle of a log message for an app, from enrichment with APM agent metadata (contextual logging), to formatting and forwarding the log data to New Relic. This diagram illustrates the flow of log messages through New Relic. Standard log formatters transform log events into meaningful output (such as text files) that can be used by downstream people and processes. The NewRelicFormatter transforms log events into the JSON format expected by New Relic. These files contain log information and extended metadata. When you configure your log forwarder (our infrastructure monitoring agent, Fluentd, Logstash, etc.), you can also extend and enrich your log data. By configuring logs in context, the log enricher links the formatted log data with additional transaction information from your application or host. Now your log files are enriched with enhanced metadata and contextual logging data. Your log forwarder sends the files to our logging endpoint for processing. From there you can use our log management capabilities to view, query, set up alerts, and more in New Relic. Here is an example of logs for your host's UI. You can see logs in context of events for the selected time period, and drill down into detailed data for any of the highlighted attributes. To take advantage of even more capabilities, click Query logs from here to go directly to the Logs UI. Here is an example of a host's logs in context related to an event. Get started with log forwarders To forward your logs to New Relic with enriched metadata: If you don't have one already, create a New Relic account. It's free, forever. Have your New Relic account's license key. Install a compatible log forwarder. Ensure that outbound connectivity on TCP port 443 is allowed to the CIDR range that matches your region. Use the DNS name log-api.newrelic.com or log-api.eu.newrelic.com for configuration. For apps monitored by a New Relic APM agent, configure logs in context. Log forwarding options Use any of these solutions to forward your logs to New Relic. Recommended: Infrastructure monitoring agent Amazon: AWS CloudWatch plugin AWS FireLens plugin AWS Kinesis Firehose AWS Lambda for sending logs from S3 Microsoft: Azure ARM template Other log forwarding plugins: Fluent Bit plugin Fluentd plugin Google Cloud Platform Pub/Sub Heroku log streaming Kubernetes plugin Logstash plugin Vector plugin TCP endpoint In some situations you may not have log forwarders; for example, with CDNs, hardware devices, or managed services. You can use syslog protocols such as rsyslog and syslog-ng, and forward your logs to New Relic via a TCP endpoint. Log API If you prefer to connect to New Relic without installing a plugin, we offer an HTTP input integration. This option sends your monitored log data directly to New Relic via the Log API. What's next? After you enable your log forwarder, make the most of your data in New Relic with our log management capabilities: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data if you are forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards. For example, to query and manage your data partition rules, see our NerdGraph tutorial.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 314.52017,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward your <em>logs</em> to <em>New</em> <em>Relic</em>",
        "sections": "Forward your <em>logs</em> to <em>New</em> <em>Relic</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " directly to <em>New</em> <em>Relic</em> via the <em>Log</em> API. What&#x27;s next? After you <em>enable</em> your <em>log</em> forwarder, make the most of your data in <em>New</em> <em>Relic</em> with our <em>log</em> <em>management</em> capabilities: Explore the logging data across your platform with our <em>Logs</em> UI. See your <em>logs</em> in context of your app&#x27;s performance in the APM UI"
      },
      "id": "61571e0e28ccbcbc52f21431"
    }
  ],
  "/docs/logs/get-started/get-started-log-management": [
    {
      "sections": [
        "Introduction to the Log API",
        "HTTP endpoint",
        "HTTP setup",
        "HTTP headers",
        "HTTP query parameters",
        "JSON body",
        "Simplified JSON body message",
        "Detailed JSON body message",
        "Limits and restricted characters",
        "Caution",
        "Important",
        "Rate limit violations",
        "HTTP requests per minute",
        "JSON bytes per minute",
        "Log payload format",
        "JSON message attributes",
        "Common block attributes",
        "Logs block attributes",
        "JSON message attribute parsing",
        "Log JSON examples",
        "Log POST message example",
        "JSON POST request example",
        "What's next?"
      ],
      "title": "Introduction to the Log API",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "Log API"
      ],
      "external_id": "576e06ac550560552b78c0960b22cafcb5df3dde",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/log-api/introduction-log-api/",
      "published_at": "2022-01-08T10:19:42Z",
      "updated_at": "2022-01-08T10:19:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If our log forwarding solutions don't meet your needs, you can use our Log API to send log data directly to New Relic via an HTTP endpoint. Want to try out our Log API? Create a New Relic account for free! No credit card required. HTTP endpoint Use the endpoint that's applicable for your New Relic account: United States (US) endpoint: https://log-api.newrelic.com/log/v1 Copy European Union (EU) endpoint: https://log-api.eu.newrelic.com/log/v1 Copy HTTP setup To send log data to your New Relic account via the Log API: Get your New Relic license key. Review the limits and restricted characters for your JSON payload. Generate the JSON message using the required headers and body fields. Ensure that your Api-Key or License-Key is included in your headers or query parameters. Refer to the log JSON examples. Send your JSON message to the appropriate HTTP endpoint for your New Relic account in a POST request. US: https://log-api.newrelic.com/log/v1 EU: https://log-api.eu.newrelic.com/log/v1 Generate some traffic and wait a few minutes, then check your account for data. If no data appears after you enable our log management capabilities, follow our troubleshooting procedures. HTTP headers When creating your HTTP headers, use these guidelines: Header Supported values Content-Type Required application/json json application/gzip gzip Api-Key Required A New Relic license key. You can also send this via query parameter. Gzipped JSON formatting is accepted. If sending compressed JSON, please include the Content-Type: application/json and Content-Encoding: gzip headers. HTTP query parameters The license key can also be passed as a query string parameter. This can be useful when sending logs from cloud-based sources that don't allow custom HTTP request headers. Query parameter Value Api-Key Your license key. Use this key whenever you send a header. JSON body You can send your JSON message using either a simplified or detailed set of attributes: Simplified JSON body message When using the simplified format to create your JSON message, send a single JSON object with the following: Field Value type Format Required Notes \"timestamp\" Integer Either milliseconds or seconds since epoch No If the field is not specific as millisecond or seconds since epoch, the message will be timestamped using the ingest time \"message\" String any string No This is the main log message field that is searched by default \"logtype\" String any string No Primary field for identifying logs and matching parsing rules other_fields (must not contain white space) String any string No These will become attributes of the log message. Note: Log management does not support white space in attribute names. Detailed JSON body message When using the detailed format to create your body, it must be a JSON array containing one or more JSON objects, each of which with the following format: Field Value type Format Required Notes \"common\" Object See common. No Any attributes that are common to all log messages \"logs\" Array See logs. Yes Array with the log entries Limits and restricted characters Caution Avoid calling our API from within the code of a customer-facing application. This can cause performance issues or block your application if response time is slow. If you need to do it this way, call our API asynchronously to avoid these performance issues. Restrictions on logs sent to the Log API: Payload total size: 1MB(10^6 bytes) maximum per POST. We highly recommend using compression. The payload must be encoded as UTF-8. Number of attributes per event: 255 maximum. Length of attribute name: 255 characters. Length of attribute value: The first 4,094 characters are stored in NRDB as a Log event field with the same name, such as message. If the string value exceeds 4,094 characters, we store the long string as a blob. Some specific attributes have additional restrictions: accountId: This is a reserved attribute name. If it is included, it will be dropped during ingest. entity.guid, entity.name, and entity.type: These attributes are used internally to identify entities. Any values submitted with these keys in the attributes section of a metric data point may cause undefined behavior such as missing entities in the UI or telemetry not associating with the expected entities. For more information please refer to Entity synthesis. eventType: This is a reserved attribute name. If it is included, it will be dropped during ingest. timestamp: Must be a Unix epoch timestamp. You can define timestamps either in seconds or in milliseconds. Important Payloads with timestamps older than 48 hours may be dropped. Rate limits on logs sent to the Log API: Maximum rate for HTTP requests sent to the Log API: 300,000 requests per minute Maximum rate of uncompressed Log JSON bytes sent to the Log API: 10 GB per minute Rate limit violations Exceeding rate limits affects how the Log API behaves. Follow these instructions if this happens. HTTP requests per minute When the maximum request rate limit is exceeded for an account, the New Relic Log API returns a 429 response for the remainder of the minute. This response includes a Retry-After header indicating how long to wait in seconds before resubmitting or sending new data. To resolve this issue, either reduce the number of data points you are sending, or request a rate limit change. Subsequent subscription changes do not impact modified rate limits. If an account change impacts your rate limit, you must notify us to adjust your rate limit. To request rate limit changes, contact your New Relic account representative, or visit our Support portal. JSON bytes per minute When the maximum Log JSON byte limit is exceeded for an account, the New Relic Log API returns a 429 response for the remainder of the minute. This response includes a Retry-After header indicating how long to wait in seconds before resubmitting or sending new data. To resolve this issue, try to reduce the amount of log data you are sending, or spread it out over a larger period of time. To request rate limit changes, contact your New Relic account representative, or visit our Support portal. Log payload format We accept any valid JSON payload. The payload must encoded as UTF-8. Important Log management does not support white space in attribute names. For example, {\"Sample Attribute\": \"Value\"} would cause errors. JSON message attributes Common block attributes This is a block containing attributes that will be common to all log entries in logs: Field Value type Format Required Notes \"timestamp\" Integer Milliseconds or seconds since epoch No Message timestamp default to ingest time \"attributes\" Object JSON No This sub-object contains all other attributes of the message Logs block attributes This is an array containing log entries with the following format: Field Value type Format Required Notes \"timestamp\" Integer Milliseconds or seconds since epoch No Message timestamp default to ingest time \"attributes\" Object JSON No This sub-object contains all other attributes of the message \"message\" String (any string) Yes This is the main log message field that is searched by default \"log\" String (any string) No We will rewrite this string as the field message on ingest \"LOG\" String (any string) No We will rewrite this string as the field message on ingest \"MESSAGE\" String (any string) No We will rewrite this string as the field message on ingest JSON message attribute parsing Our log management capabilities will parse any message attribute as JSON. The resulting JSON attributes in the parsed message will be added to the event. If the message attribute is not JSON, it is left as is. Important White space in an attribute name is not supported. For example, {\"Sample Attribute\": \"Value\"} would cause errors. Instead, use something like {\"Sample_Attribute\":\"Value\"}. Here is an example message attribute: { \"timestamp\": 1562767499238, \"message\": \"{\\\"service-name\\\": \\\"login-service\\\", \\\"user\\\": {\\\"id\\\": 123, \\\"name\\\": \\\"alice\\\"}}\" } Copy This will be treated as: { \"timestamp\": 1562767499238, \"message\": \"{\\\"service-name\\\": \\\"my-service\\\", \\\"user\\\": {\\\"id\\\": 123, \\\"name\\\": \\\"alice\\\"}}\", \"service-name\": \"my-service\", \"user\": { \"id\": 123, \"name\": \"alice\" } } Copy Log JSON examples Attributes can be scalar JSON types like string and number. They can also be compound (or nested) objects. Compound attributes will have their associated attributes stored with flattened names. For example, here is a compound user attribute in a log entry's attributes: \"attributes\": { \"action\": \"login\", \"user\": { \"id\": 123, \"name\": \"alice\" } } Copy This will result in the following attributes being stored with the log event: Attribute Value \"action\" \"login\" \"user.id\" 123 \"user.name\" \"alice\" Log POST message example Log POST message example: POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: <YOUR_LICENSE_KEY> Accept: */* Content-Length: 319 [{ \"common\": { \"attributes\": { \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } }, \"logs\": [{ \"timestamp\": <TIMESTAMP_IN_UNIX_EPOCH>, \"message\": \"User 'xyz' logged in\" },{ \"timestamp\": <TIMESTAMP_IN_UNIX_EPOCH>, \"message\": \"User 'xyz' logged out\", \"attributes\": { \"auditId\": 123 } }] }] Copy This POST message would result in the following log messages being stored in New Relic: Attribute Value \"logtype\" \"accesslogs\" \"service\" \"login-service\" \"hostname\" \"login.example.com\" Here's an example of stored logs block attributes: Attribute Value \"timestamp\" 1550086450124 \"message\" \"User 'xyz' logged out\" \"auditId\" 123 JSON POST request example Here's an example of a JSON POST request: POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: <YOUR_LICENSE_KEY> Accept: */* Content-Length: 133 { \"timestamp\": <TIMESTAMP_IN_UNIX_EPOCH>, \"message\": \"User 'xyz' logged in\", \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } Copy What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 205.5166,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to the <em>Log</em> API",
        "sections": "<em>Logs</em> block attributes",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " that is searched by default &quot;logtype&quot; String any string No Primary field for identifying <em>logs</em> and matching parsing rules other_fields (must not contain white space) String any string No These will become attributes of the <em>log</em> message. Note: <em>Log</em> <em>management</em> does not support white space in attribute names"
      },
      "id": "61902eb9196a6776c0e7277b"
    },
    {
      "sections": [
        "Drop log data with drop filter rules",
        "Savings, security, speed",
        "Caution",
        "How drop filter rules work",
        "Cautions when dropping data",
        "Create drop filter rules",
        "Manage drop filter rules via NerdGraph API",
        "Types of drop filter rules",
        "Drop log events",
        "Drop attributes",
        "Tip",
        "View or delete drop filter rules"
      ],
      "title": "Drop log data with drop filter rules",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "41ef69e9d8d23b2ab732b489bb5e0cb47b8c16b6",
      "image": "https://docs.newrelic.com/static/db4b077fafd911b9f5019b022b3048ab/b04e4/ingest-pipeline.png",
      "url": "https://docs.newrelic.com/docs/logs/ui-data/drop-data-drop-filter-rules/",
      "published_at": "2022-01-08T10:21:46Z",
      "updated_at": "2022-01-08T10:21:45Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After log event data has been sent to New Relic, it can either be stored in our NRDB database or dropped (discarded). To drop log data, you can use the logs management UI: that is explained in this doc. Alternatively, you can use NerdGraph to drop data. Savings, security, speed Drop filter rules help you accomplish several important goals: Lower costs by storing only the logs relevant to your account. Protect privacy and security by removing personal identifiable information (PII). Reduce noise by removing irrelevant events and attributes. Caution Use caution when deciding to drop data. The data you drop is not recoverable. Before using this feature, review the responsibilities and considerations for dropping data. How drop filter rules work A drop filter rule matches data based on a query. When triggered, the drop filter rule removes the matching data from the ingestion pipeline before it is written to NRDB. This creates an explicit demarcation between the logs being forwarded from your domain and the data that New Relic collects. Since the data removed by the drop filter rule doesn't reach our backend, it cannot be queried: the data is gone and cannot be restored. During the ingestion process, customer log data can be parsed, transformed, or dropped before being stored in New Relic's database. Cautions when dropping data When creating drop rules, you are responsible for ensuring that the rules accurately identify and discard the data that meets the conditions that you have established. You are also responsible for monitoring the rule, as well as the data you disclose to New Relic. New Relic cannot guarantee that this functionality will completely resolve data disclosure concerns you may have. New Relic doesn't review or monitor how effective the rules you develop are. Creating rules about sensitive data can leak information about what kinds of data you maintain, including the format of your data or systems (for example, through referencing email addresses or specific credit card numbers). Any user with the relevant role-based access control permissions can view and edit all information in the rules you create. Create drop filter rules For permissions-related requirements, see Drop data requirements. Once a drop filter rule is active, it's applied to all log events ingested from that point onwards. Rules are not applied retroactively. Logs collected before creating a rule are not filtered by that rule. Filter or query the set of logs that contain the data you want to drop. Then, from Manage data on the left nav of the Logs UI, click Create drop filter. To create a new drop filter rule, you can use new or existing log queries. Go to one.newrelic.com > Logs. Filter or query to the specific set of logs that contain the data to be dropped. Once the query is active, from Manage data on the left nav of the Logs UI, click Create drop filter. Recommendation: Change the drop rule's default name to a meaningful name. Choose to either drop the entire log event that matches the query or just a specific subset of attributes in the matching events. Review the log partitions where this drop rule applies. Save the drop filter rule. Manage drop filter rules via NerdGraph API To manage your drop filter rules programmatically, you can use NerdGraph to create, query, and delete your drop filter rules. If you create drop filter rules for log events using NerdGraph, set the source attribute to Logging. The logging UI only displays drop filters with the source attribute. Types of drop filter rules The drop filters UI prompts you to select whether to drop logs based on the query or on specific attributes. Drop log events The default type of drop filter rule is to drop logs. This option drops the entire log events that match the filter or query. When creating a rule, try to provide a specific query that only matches log data that should be dropped. Our drop filters process won't let you create drop filter rules without values in the matching query. This prevents badly formed rules from dropping all log data. Drop attributes You can specify attributes to be dropped in a log event that matches your query. At least one or more attributes must be selected. Any attribute which is selected will be dropped; all remaining attributes will be kept and stored in NRDB. Tip We recommend this method for removing fields that could contain personal identifiable information (PII) or other sensitive attributes without losing valuable monitoring data. View or delete drop filter rules To view or delete a drop filter rule: Go to one.newrelic.com > Logs. From Manage data on the left nav of the Logs UI, click Drop filters. Click the delete icon next to the drop filter rule you want to remove. Once deleted, rules no longer filter ingested log events.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.0758,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Drop <em>log</em> data with drop filter rules",
        "sections": "Drop <em>log</em> data with drop filter rules",
        "tags": "<em>Log</em> <em>management</em>",
        "body": "After <em>log</em> event data has been sent to New Relic, it can either be stored in our NRDB database or dropped (discarded). To drop <em>log</em> data, you can use the <em>logs</em> <em>management</em> UI: that is explained in this doc. Alternatively, you can use NerdGraph to drop data. Savings, security, speed Drop filter rules"
      },
      "id": "603e813f28ccbc08c1eba787"
    },
    {
      "sections": [
        "Parsing log data",
        "Parsing example",
        "How log parsing works",
        "Parse attributes using Grok",
        "Organizing by logtype",
        "Important",
        "Limits",
        "Tip",
        "Built-in parsing rules",
        "List of built-in rules",
        "Add the logtype attribute",
        "New Relic infrastructure agent example",
        "Fluentd example",
        "Fluent Bit example",
        "Logstash example",
        "Logs API example",
        "Create custom parsing rules",
        "Troubleshooting"
      ],
      "title": "Parsing log data",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "67ff268a2d145b790fa501dd4eca7907d50b1912",
      "image": "https://docs.newrelic.com/static/dc392bb7142d2fdb253a649daf4ebe6d/c1b63/log-parsing-rule-ui.png",
      "url": "https://docs.newrelic.com/docs/logs/ui-data/parsing/",
      "published_at": "2022-01-08T02:41:29Z",
      "updated_at": "2022-01-04T06:04:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Parsing is the process of splitting unstructured log data into attributes (key/value pairs). You can use these attributes to facet or filter logs in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial on YouTube (approx. 4-1/2 minutes). New Relic parses log data according to rules. This document describes how logs parsing works, how to use built-in rules, and how to create custom rules. You can also create, query, and manage your log parsing rules by using NerdGraph, our GraphQL API, at api.newrelic.com/graphiql. For more information, see our NerdGraph tutorial for parsing. Parsing example A good example is a default NGINX access log containing unstructured text. It is useful for searching but not much else. Here's an example of a typical line: 127.180.71.3 - - [10/May/1997:08:05:32 +0000] \"GET /downloads/product_1 HTTP/1.1\" 304 0 \"-\" \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" Copy In an unparsed format, you would need to do a full text search to answer most questions. After parsing, the log is organized into attributes, like response code and request URL: { \"remote_addr\":\"93.180.71.3\", \"time\":\"1586514731\", \"method\":\"GET\", \"path\":\"/downloads/product_1\", \"version\":\"HTTP/1.1\", \"response\":\"304\", \"bytesSent\": 0, \"user_agent\": \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" } Copy Parsing makes it easier to create custom queries that facet on those values. This helps you understand the distribution of response codes per request URL and quickly find problematic pages. How log parsing works Here's an overview of how New Relic implements parsing of logs: Log parsing How it works What All parsing takes place against the message field; no other fields can be parsed. Each parsing rule is created with matching criteria that determines which logs the rule will attempt to parse. To simplify the matching process, we recommend adding a logtype attribute to your logs. However, you are not limited to using logtype; any attribute can be used as matching criteria. When Parsing will only be applied once to each log message. If multiple parsing rules match the log, only the first that succeeds will be applied. Parsing takes place during log ingestion, before data is written to NRDB. Once data has been written to storage, it can no longer be parsed. Parsing occurs in the pipeline before data enrichments take place. Be careful when defining the matching criteria for a parsing rule. If the criteria is based on an attribute that doesn't exist untail after parsing or enrichment take place, that data won't be present in the logs when matching occurs. As a result, no parsing will happen. How Rules can be written in Grok, regex, or a mixture of the two. Grok is a collection of patterns that abstract away complicated regular expressions. If the content of the message field is JSON, it will be parsed automatically. Parse attributes using Grok Parsing patterns are specified using Grok, an industry standard for parsing log messages. Grok is a superset of regular expressions that adds built-in named patterns to be used in place of literal complex regular expressions. For instance, instead of having to remember that an integer can be matched with the regular expression (?:[+-]?(?:[0-9]+)), you can just write %{INT} to use the Grok pattern INT, which represents the same regular expression. You can always use a mix of regular expressions and Grok pattern names in your matching string. Grok patterns have the syntax: %{PATTERN_NAME[:OPTIONAL_EXTRACTED_ATTRIBUTE_NAME[:OPTIONAL_TYPE]]} Where: PATTERN_NAME is one of the Grok patterns. Click grok-patterns to see the most commonly-used patterns. The pattern name is just a user-friendly name representing a regular expression. They are exactly equal to the corresponding regular expression. OPTIONAL_EXTRACTED_ATTRIBUTE_NAME, if provided, is the name of the attribute that will be added to your log message with the value matched by the pattern name. It's equivalent to using a named capture group using regular expressions. If this is not provided, then the parsing rule will just match a region of your string, but not extract an attribute with its value. OPTIONAL_TYPE specifies the type of attribute value to extract. If omitted, values are extracted as strings. For instance, to extract the value 123 from \"File Size: 123\" as a number into attribute file_size, use value: %{INT:file_size:int}. Supported types are: Type specified in Grok Type stored in the New Relic database boolean boolean byte short int integer integer long long float float double double string (default) text string date datetime ISO 8601 time as a long See our blog post for more details. Organizing by logtype New Relic's log ingestion pipeline can parse data by matching a log event to a rule that describes how the log should be parsed. There are two ways log events can be parsed: Use a built-in rule. Define a custom rule. Rules are a combination of matching logic and parsing logic. Matching is done by defining a query match on an attribute of the logs. Rules are not applied retroactively. Logs collected before a rule is created are not parsed by that rule. The simplest way to organize your logs and how they are parsed is to include the logtype field in your log event. This tells New Relic what built-in rule to apply to the logs. Important Once a parsing rule is active, data parsed by the rule is permanently changed. This cannot be reverted. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. Limits Parsing is computationally expensive, which introduces risk. Parsing is done for custom rules defined in an account and for matching patterns to a log. A large number of patterns or poorly defined custom rules will consume a huge amount of memory and CPU resources while also taking a very long time to complete. In order to prevent problems, we apply two parsing limits: per-message-per-rule and per-account. Limit Description Per-message-per-rule The per-message-per-rule limit prevents the time spent parsing any single message from being greater than 100 ms. If that limit is reached, the system will cease attempting to parse the log message with that rule. The ingestion pipeline will attempt to run any other applicable on that message, and the message will still be passed through the ingestion pipeline and stored in NRDB. The log message will be in its original, unparsed format. Per-account The per-account limit exists to prevent accounts from using more than their fair share of resources. The limit considers the total time spent processing all log messages for an account per-minute. The limit is not a fixed value; it scales up or down proportionally to the volume of data stored daily by the account and the environment size that is subsequently allocated to support that customer. Tip To easily check if your rate limits have been reached, go to your system Limits page in the New Relic UI. Built-in parsing rules Common log formats have well-established parsing rules already created for them. To get the benefit of built-in parsing rules, add the logtype attribute when forwarding logs. Set the value to something listed in the following table, and the rules for that type of log will be applied automatically. List of built-in rules The following logtype attribute values map to a predefined parsing rule. For example, to query the Application Load Balancer: From the New Relic UI, use the format logtype: alb. From NerdGraph, use the format logtype = 'alb'. To learn what fields are parsed for each rule, see our documentation about built-in parsing rules. logtype Example matching query alb AWS Application Load Balancer logtype:alb apache Apache Access logtype:apache cloudfront-web CloudFront Web logtype:cloudfront-web elb Amazon Elastic Load Balancer logtype:elb iis_w3c Microsoft IIS server logs - W3C format logtype:iis_w3c monit Monit logs logtype:monit mysql-error MySQL Error logtype:mysql-error nginx NGINX access logs logtype:nginx nginx-error NGINX error logs logtype:nginx-error route-53 Amazon Route 53 logs logtype:route-53 syslog-rfc5424 Syslog logtype:syslog-rfc5424 Add the logtype attribute When aggregating logs, it's important to provide metadata that makes it easy to organize, search, and parse those logs. One simple way of doing this is to add the attribute logtype to the log messages when they are shipped. Built-in parsing rules are applied by default to certain logtype values. Here are some examples of how to add logtype to logs sent by some of our supported shipping methods. New Relic infrastructure agent example Add logtype as an attribute. You must set the logtype for each named source. logs: - name: file-simple file: /path/to/file attributes: logtype: fileRaw - name: nginx-example file: /var/log/nginx.log attributes: logtype: nginx Copy Fluentd example Add a filter block to the .conf file, which uses a record_transformer to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluentd examples. <filter containers> @type record_transformer enable_ruby true <record> #Add logtype to trigger a built-in parsing rule for nginx access logs logtype nginx #Set timestamp from the value contained in the field \"time\" timestamp record[\"time\"] #Add hostname and tag fields to all records hostname \"#{Socket.gethostname}\" tag ${tag} </record> </filter> Copy Fluent Bit example Add a filter block to the .conf file that uses a record_modifier to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluent Bit examples. [FILTER] Name record_modifier Match * Record logtype nginx Record hostname ${HOSTNAME} Record service_name Sample-App-Name Copy Logstash example Add a filter block to the Logstash configuration which uses an add_field mutate filter to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Logstash examples. filter { mutate { add_field => { \"logtype\" => \"nginx\" \"service_name\" => \"myservicename\" \"hostname\" => \"%{host}\" } } } Copy Logs API example You can add attributes to the JSON request sent to New Relic. In this example we add a logtype attribute of value nginx to trigger the built-in NGINX parsing rule. Learn more about using the Logs API. POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: YOUR_LICENSE_KEY Accept: */* Content-Length: 133 { \"timestamp\": TIMESTAMP_IN_UNIX_EPOCH, \"message\": \"User 'xyz' logged in\", \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } Copy Create custom parsing rules Many logs are formatted or structured in a unique way. In order to parse them, custom logic must be built and applied. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. To create and manage your own, custom parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing, then click Create parsing rule. Enter the parsing rule's name. Choose an attribute and value to match on. Write your Grok pattern and test the rule. To learn about Grok and custom parsing rules, read our blog post about how to parse logs with Grok patterns. Enable and save the custom parsing rule. To view the list of custom parsing rules: From Manage Data on the left nav of the Logs UI, click Parsing. To view existing parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing. Troubleshooting If parsing is not working the way you intended, it may be due to: Logic: The parsing rule matching logic does not match the logs you want. Timing: If your parsing matching rule targets a value that doesn't exist yet, it will fail. This can occur if the value is added later in the pipeline as part of the enrichment process. Limits: There is a fixed amount of time available every minute to process logs via parsing, patterns, drop filters, etc. If the maximum amount of time has been spent, parsing will be skipped for additional log event records. To resolve these problems, create or adjust your custom parsing rules.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 170.27258,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Parsing <em>log</em> data",
        "sections": "<em>Logstash</em> example",
        "tags": "<em>Log</em> <em>management</em>",
        "body": "Parsing is the process of splitting unstructured <em>log</em> data into attributes (key&#x2F;value pairs). You can use these attributes to facet or filter <em>logs</em> in useful ways. This in turn helps you build better charts and alerts. To <em>get</em> <em>started</em> with parsing, you may want to watch the following video tutorial"
      },
      "id": "619030c5e7b9d20cf0386b1c"
    }
  ],
  "/docs/logs/get-started/new-relics-log-management-security-privacy": [
    {
      "sections": [
        "Get started with log management",
        "Find problems faster, reduce context switching",
        "Bring in your logging data",
        "View your logging data in New Relic",
        "What's next"
      ],
      "title": "Get started with log management",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "Get started"
      ],
      "external_id": "1335d59906e840e38d6ef8572c31baea338f0b49",
      "image": "https://docs.newrelic.com/static/aa6a747a9c8b7ebb8ea64f44456e3c67/c1b63/demotron-services-logs-123021c.png",
      "url": "https://docs.newrelic.com/docs/logs/get-started/get-started-log-management/",
      "published_at": "2022-01-09T01:42:38Z",
      "updated_at": "2022-01-04T05:58:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "As applications move towards the cloud, microservices architecture is becoming more dispersed, making the ability to monitor logs essential. New Relic offers a fast, scalable log management platform so you can connect your logs with the rest of your telemetry and infrastructure data in a single place. See how it works with this video (approx. 2 minutes). Our log management solution provides deeper visibility into application and infrastructure performance data (events and errors) to reduce mean-time-to-resolve (MTTR) and quickly troubleshoot production incidents. Find problems faster, reduce context switching Log management provides a way to connect your log data with the rest of your application and infrastructure data. You can get to the root cause of problems quickly, without losing context switching between tools. Log management features include: Instantly search through your logs. Visualize your log data directly from the Logs UI. Use logging data to create custom charts, dashboards, and alerts. Troubleshoot performance issues without switching between tools. Visualize everything in a single place. Bring in your logging data To forward your log data to New Relic, you can: Use our infrastructure monitoring agent as a lightweight data collector, without having to install additional software. Select from a wide range of log forwarding plugins, including Amazon, Microsoft, Fluentd, Fluent Bit, Kubernetes, Logstash, and more. Use our OpenTelemetry solutions. Send your log data by using the Log API or TCP endpoint. Once log management is enabled, you can also connect your logs with your APM agent, Kubernetes clusters, or distributed tracing to get additional contextual logging data with our logs in context extensions. View your logging data in New Relic You can explore your logging data in the UI or by API: Logs UI at one.newrelic.com Logs UI for EU region data center if applicable: one.eu.newrelic.com You can also query the Log data type. For example, use NRQL to run: SELECT * FROM Log Copy You can also use NerdGraph, our GraphQL-format API, to request the exact data you need. Here is an example of log data for one of an app's related entities (a workload service in this example): You can also see relevant logs in context, directly from the app in the APM UI or from the host in the Hosts UI in New Relic One. From here you can drill down into detailed data, run queries, set alerts, and more. What's next Ready to get started with our log management solutions? If you don't have one already, create a New Relic account. It's free, forever. Enable log management by forwarding your logs to New Relic. Recommendation: Use our infrastructure agent as your log forwarder, so you can get logs in context of your platform and services directly in our UI. For apps monitored by a New Relic APM agent, configure logs in context. Explore the logging data across your platform with our Logs UI in New Relic One, where you can add alerts, query your data, and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.33298,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Get</em> <em>started</em> with <em>log</em> <em>management</em>",
        "sections": "<em>Get</em> <em>started</em> with <em>log</em> <em>management</em>",
        "tags": "<em>Log</em> <em>management</em>",
        "body": ", run queries, set alerts, and more. What&#x27;s next Ready to <em>get</em> <em>started</em> with our <em>log</em> <em>management</em> solutions? If you don&#x27;t have one already, create a New Relic account. It&#x27;s free, forever. Enable <em>log</em> <em>management</em> by forwarding your <em>logs</em> to New Relic. Recommendation: Use our infrastructure agent as your <em>log</em>"
      },
      "id": "603ea62ee7b9d249432a07e2"
    },
    {
      "sections": [
        "Introduction to the Log API",
        "HTTP endpoint",
        "HTTP setup",
        "HTTP headers",
        "HTTP query parameters",
        "JSON body",
        "Simplified JSON body message",
        "Detailed JSON body message",
        "Limits and restricted characters",
        "Caution",
        "Important",
        "Rate limit violations",
        "HTTP requests per minute",
        "JSON bytes per minute",
        "Log payload format",
        "JSON message attributes",
        "Common block attributes",
        "Logs block attributes",
        "JSON message attribute parsing",
        "Log JSON examples",
        "Log POST message example",
        "JSON POST request example",
        "What's next?"
      ],
      "title": "Introduction to the Log API",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "Log API"
      ],
      "external_id": "576e06ac550560552b78c0960b22cafcb5df3dde",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/log-api/introduction-log-api/",
      "published_at": "2022-01-08T10:19:42Z",
      "updated_at": "2022-01-08T10:19:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If our log forwarding solutions don't meet your needs, you can use our Log API to send log data directly to New Relic via an HTTP endpoint. Want to try out our Log API? Create a New Relic account for free! No credit card required. HTTP endpoint Use the endpoint that's applicable for your New Relic account: United States (US) endpoint: https://log-api.newrelic.com/log/v1 Copy European Union (EU) endpoint: https://log-api.eu.newrelic.com/log/v1 Copy HTTP setup To send log data to your New Relic account via the Log API: Get your New Relic license key. Review the limits and restricted characters for your JSON payload. Generate the JSON message using the required headers and body fields. Ensure that your Api-Key or License-Key is included in your headers or query parameters. Refer to the log JSON examples. Send your JSON message to the appropriate HTTP endpoint for your New Relic account in a POST request. US: https://log-api.newrelic.com/log/v1 EU: https://log-api.eu.newrelic.com/log/v1 Generate some traffic and wait a few minutes, then check your account for data. If no data appears after you enable our log management capabilities, follow our troubleshooting procedures. HTTP headers When creating your HTTP headers, use these guidelines: Header Supported values Content-Type Required application/json json application/gzip gzip Api-Key Required A New Relic license key. You can also send this via query parameter. Gzipped JSON formatting is accepted. If sending compressed JSON, please include the Content-Type: application/json and Content-Encoding: gzip headers. HTTP query parameters The license key can also be passed as a query string parameter. This can be useful when sending logs from cloud-based sources that don't allow custom HTTP request headers. Query parameter Value Api-Key Your license key. Use this key whenever you send a header. JSON body You can send your JSON message using either a simplified or detailed set of attributes: Simplified JSON body message When using the simplified format to create your JSON message, send a single JSON object with the following: Field Value type Format Required Notes \"timestamp\" Integer Either milliseconds or seconds since epoch No If the field is not specific as millisecond or seconds since epoch, the message will be timestamped using the ingest time \"message\" String any string No This is the main log message field that is searched by default \"logtype\" String any string No Primary field for identifying logs and matching parsing rules other_fields (must not contain white space) String any string No These will become attributes of the log message. Note: Log management does not support white space in attribute names. Detailed JSON body message When using the detailed format to create your body, it must be a JSON array containing one or more JSON objects, each of which with the following format: Field Value type Format Required Notes \"common\" Object See common. No Any attributes that are common to all log messages \"logs\" Array See logs. Yes Array with the log entries Limits and restricted characters Caution Avoid calling our API from within the code of a customer-facing application. This can cause performance issues or block your application if response time is slow. If you need to do it this way, call our API asynchronously to avoid these performance issues. Restrictions on logs sent to the Log API: Payload total size: 1MB(10^6 bytes) maximum per POST. We highly recommend using compression. The payload must be encoded as UTF-8. Number of attributes per event: 255 maximum. Length of attribute name: 255 characters. Length of attribute value: The first 4,094 characters are stored in NRDB as a Log event field with the same name, such as message. If the string value exceeds 4,094 characters, we store the long string as a blob. Some specific attributes have additional restrictions: accountId: This is a reserved attribute name. If it is included, it will be dropped during ingest. entity.guid, entity.name, and entity.type: These attributes are used internally to identify entities. Any values submitted with these keys in the attributes section of a metric data point may cause undefined behavior such as missing entities in the UI or telemetry not associating with the expected entities. For more information please refer to Entity synthesis. eventType: This is a reserved attribute name. If it is included, it will be dropped during ingest. timestamp: Must be a Unix epoch timestamp. You can define timestamps either in seconds or in milliseconds. Important Payloads with timestamps older than 48 hours may be dropped. Rate limits on logs sent to the Log API: Maximum rate for HTTP requests sent to the Log API: 300,000 requests per minute Maximum rate of uncompressed Log JSON bytes sent to the Log API: 10 GB per minute Rate limit violations Exceeding rate limits affects how the Log API behaves. Follow these instructions if this happens. HTTP requests per minute When the maximum request rate limit is exceeded for an account, the New Relic Log API returns a 429 response for the remainder of the minute. This response includes a Retry-After header indicating how long to wait in seconds before resubmitting or sending new data. To resolve this issue, either reduce the number of data points you are sending, or request a rate limit change. Subsequent subscription changes do not impact modified rate limits. If an account change impacts your rate limit, you must notify us to adjust your rate limit. To request rate limit changes, contact your New Relic account representative, or visit our Support portal. JSON bytes per minute When the maximum Log JSON byte limit is exceeded for an account, the New Relic Log API returns a 429 response for the remainder of the minute. This response includes a Retry-After header indicating how long to wait in seconds before resubmitting or sending new data. To resolve this issue, try to reduce the amount of log data you are sending, or spread it out over a larger period of time. To request rate limit changes, contact your New Relic account representative, or visit our Support portal. Log payload format We accept any valid JSON payload. The payload must encoded as UTF-8. Important Log management does not support white space in attribute names. For example, {\"Sample Attribute\": \"Value\"} would cause errors. JSON message attributes Common block attributes This is a block containing attributes that will be common to all log entries in logs: Field Value type Format Required Notes \"timestamp\" Integer Milliseconds or seconds since epoch No Message timestamp default to ingest time \"attributes\" Object JSON No This sub-object contains all other attributes of the message Logs block attributes This is an array containing log entries with the following format: Field Value type Format Required Notes \"timestamp\" Integer Milliseconds or seconds since epoch No Message timestamp default to ingest time \"attributes\" Object JSON No This sub-object contains all other attributes of the message \"message\" String (any string) Yes This is the main log message field that is searched by default \"log\" String (any string) No We will rewrite this string as the field message on ingest \"LOG\" String (any string) No We will rewrite this string as the field message on ingest \"MESSAGE\" String (any string) No We will rewrite this string as the field message on ingest JSON message attribute parsing Our log management capabilities will parse any message attribute as JSON. The resulting JSON attributes in the parsed message will be added to the event. If the message attribute is not JSON, it is left as is. Important White space in an attribute name is not supported. For example, {\"Sample Attribute\": \"Value\"} would cause errors. Instead, use something like {\"Sample_Attribute\":\"Value\"}. Here is an example message attribute: { \"timestamp\": 1562767499238, \"message\": \"{\\\"service-name\\\": \\\"login-service\\\", \\\"user\\\": {\\\"id\\\": 123, \\\"name\\\": \\\"alice\\\"}}\" } Copy This will be treated as: { \"timestamp\": 1562767499238, \"message\": \"{\\\"service-name\\\": \\\"my-service\\\", \\\"user\\\": {\\\"id\\\": 123, \\\"name\\\": \\\"alice\\\"}}\", \"service-name\": \"my-service\", \"user\": { \"id\": 123, \"name\": \"alice\" } } Copy Log JSON examples Attributes can be scalar JSON types like string and number. They can also be compound (or nested) objects. Compound attributes will have their associated attributes stored with flattened names. For example, here is a compound user attribute in a log entry's attributes: \"attributes\": { \"action\": \"login\", \"user\": { \"id\": 123, \"name\": \"alice\" } } Copy This will result in the following attributes being stored with the log event: Attribute Value \"action\" \"login\" \"user.id\" 123 \"user.name\" \"alice\" Log POST message example Log POST message example: POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: <YOUR_LICENSE_KEY> Accept: */* Content-Length: 319 [{ \"common\": { \"attributes\": { \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } }, \"logs\": [{ \"timestamp\": <TIMESTAMP_IN_UNIX_EPOCH>, \"message\": \"User 'xyz' logged in\" },{ \"timestamp\": <TIMESTAMP_IN_UNIX_EPOCH>, \"message\": \"User 'xyz' logged out\", \"attributes\": { \"auditId\": 123 } }] }] Copy This POST message would result in the following log messages being stored in New Relic: Attribute Value \"logtype\" \"accesslogs\" \"service\" \"login-service\" \"hostname\" \"login.example.com\" Here's an example of stored logs block attributes: Attribute Value \"timestamp\" 1550086450124 \"message\" \"User 'xyz' logged out\" \"auditId\" 123 JSON POST request example Here's an example of a JSON POST request: POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: <YOUR_LICENSE_KEY> Accept: */* Content-Length: 133 { \"timestamp\": <TIMESTAMP_IN_UNIX_EPOCH>, \"message\": \"User 'xyz' logged in\", \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } Copy What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 205.5166,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to the <em>Log</em> API",
        "sections": "<em>Logs</em> block attributes",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " that is searched by default &quot;logtype&quot; String any string No Primary field for identifying <em>logs</em> and matching parsing rules other_fields (must not contain white space) String any string No These will become attributes of the <em>log</em> message. Note: <em>Log</em> <em>management</em> does not support white space in attribute names"
      },
      "id": "61902eb9196a6776c0e7277b"
    },
    {
      "sections": [
        "Drop log data with drop filter rules",
        "Savings, security, speed",
        "Caution",
        "How drop filter rules work",
        "Cautions when dropping data",
        "Create drop filter rules",
        "Manage drop filter rules via NerdGraph API",
        "Types of drop filter rules",
        "Drop log events",
        "Drop attributes",
        "Tip",
        "View or delete drop filter rules"
      ],
      "title": "Drop log data with drop filter rules",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "41ef69e9d8d23b2ab732b489bb5e0cb47b8c16b6",
      "image": "https://docs.newrelic.com/static/db4b077fafd911b9f5019b022b3048ab/b04e4/ingest-pipeline.png",
      "url": "https://docs.newrelic.com/docs/logs/ui-data/drop-data-drop-filter-rules/",
      "published_at": "2022-01-08T10:21:46Z",
      "updated_at": "2022-01-08T10:21:45Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After log event data has been sent to New Relic, it can either be stored in our NRDB database or dropped (discarded). To drop log data, you can use the logs management UI: that is explained in this doc. Alternatively, you can use NerdGraph to drop data. Savings, security, speed Drop filter rules help you accomplish several important goals: Lower costs by storing only the logs relevant to your account. Protect privacy and security by removing personal identifiable information (PII). Reduce noise by removing irrelevant events and attributes. Caution Use caution when deciding to drop data. The data you drop is not recoverable. Before using this feature, review the responsibilities and considerations for dropping data. How drop filter rules work A drop filter rule matches data based on a query. When triggered, the drop filter rule removes the matching data from the ingestion pipeline before it is written to NRDB. This creates an explicit demarcation between the logs being forwarded from your domain and the data that New Relic collects. Since the data removed by the drop filter rule doesn't reach our backend, it cannot be queried: the data is gone and cannot be restored. During the ingestion process, customer log data can be parsed, transformed, or dropped before being stored in New Relic's database. Cautions when dropping data When creating drop rules, you are responsible for ensuring that the rules accurately identify and discard the data that meets the conditions that you have established. You are also responsible for monitoring the rule, as well as the data you disclose to New Relic. New Relic cannot guarantee that this functionality will completely resolve data disclosure concerns you may have. New Relic doesn't review or monitor how effective the rules you develop are. Creating rules about sensitive data can leak information about what kinds of data you maintain, including the format of your data or systems (for example, through referencing email addresses or specific credit card numbers). Any user with the relevant role-based access control permissions can view and edit all information in the rules you create. Create drop filter rules For permissions-related requirements, see Drop data requirements. Once a drop filter rule is active, it's applied to all log events ingested from that point onwards. Rules are not applied retroactively. Logs collected before creating a rule are not filtered by that rule. Filter or query the set of logs that contain the data you want to drop. Then, from Manage data on the left nav of the Logs UI, click Create drop filter. To create a new drop filter rule, you can use new or existing log queries. Go to one.newrelic.com > Logs. Filter or query to the specific set of logs that contain the data to be dropped. Once the query is active, from Manage data on the left nav of the Logs UI, click Create drop filter. Recommendation: Change the drop rule's default name to a meaningful name. Choose to either drop the entire log event that matches the query or just a specific subset of attributes in the matching events. Review the log partitions where this drop rule applies. Save the drop filter rule. Manage drop filter rules via NerdGraph API To manage your drop filter rules programmatically, you can use NerdGraph to create, query, and delete your drop filter rules. If you create drop filter rules for log events using NerdGraph, set the source attribute to Logging. The logging UI only displays drop filters with the source attribute. Types of drop filter rules The drop filters UI prompts you to select whether to drop logs based on the query or on specific attributes. Drop log events The default type of drop filter rule is to drop logs. This option drops the entire log events that match the filter or query. When creating a rule, try to provide a specific query that only matches log data that should be dropped. Our drop filters process won't let you create drop filter rules without values in the matching query. This prevents badly formed rules from dropping all log data. Drop attributes You can specify attributes to be dropped in a log event that matches your query. At least one or more attributes must be selected. Any attribute which is selected will be dropped; all remaining attributes will be kept and stored in NRDB. Tip We recommend this method for removing fields that could contain personal identifiable information (PII) or other sensitive attributes without losing valuable monitoring data. View or delete drop filter rules To view or delete a drop filter rule: Go to one.newrelic.com > Logs. From Manage data on the left nav of the Logs UI, click Drop filters. Click the delete icon next to the drop filter rule you want to remove. Once deleted, rules no longer filter ingested log events.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.0758,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Drop <em>log</em> data with drop filter rules",
        "sections": "Drop <em>log</em> data with drop filter rules",
        "tags": "<em>Log</em> <em>management</em>",
        "body": "After <em>log</em> event data has been sent to New Relic, it can either be stored in our NRDB database or dropped (discarded). To drop <em>log</em> data, you can use the <em>logs</em> <em>management</em> UI: that is explained in this doc. Alternatively, you can use NerdGraph to drop data. Savings, security, speed Drop filter rules"
      },
      "id": "603e813f28ccbc08c1eba787"
    }
  ],
  "/docs/logs/log-api/introduction-log-api": [
    {
      "sections": [
        "Get started with log management",
        "Find problems faster, reduce context switching",
        "Bring in your logging data",
        "View your logging data in New Relic",
        "What's next"
      ],
      "title": "Get started with log management",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "Get started"
      ],
      "external_id": "1335d59906e840e38d6ef8572c31baea338f0b49",
      "image": "https://docs.newrelic.com/static/aa6a747a9c8b7ebb8ea64f44456e3c67/c1b63/demotron-services-logs-123021c.png",
      "url": "https://docs.newrelic.com/docs/logs/get-started/get-started-log-management/",
      "published_at": "2022-01-09T01:42:38Z",
      "updated_at": "2022-01-04T05:58:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "As applications move towards the cloud, microservices architecture is becoming more dispersed, making the ability to monitor logs essential. New Relic offers a fast, scalable log management platform so you can connect your logs with the rest of your telemetry and infrastructure data in a single place. See how it works with this video (approx. 2 minutes). Our log management solution provides deeper visibility into application and infrastructure performance data (events and errors) to reduce mean-time-to-resolve (MTTR) and quickly troubleshoot production incidents. Find problems faster, reduce context switching Log management provides a way to connect your log data with the rest of your application and infrastructure data. You can get to the root cause of problems quickly, without losing context switching between tools. Log management features include: Instantly search through your logs. Visualize your log data directly from the Logs UI. Use logging data to create custom charts, dashboards, and alerts. Troubleshoot performance issues without switching between tools. Visualize everything in a single place. Bring in your logging data To forward your log data to New Relic, you can: Use our infrastructure monitoring agent as a lightweight data collector, without having to install additional software. Select from a wide range of log forwarding plugins, including Amazon, Microsoft, Fluentd, Fluent Bit, Kubernetes, Logstash, and more. Use our OpenTelemetry solutions. Send your log data by using the Log API or TCP endpoint. Once log management is enabled, you can also connect your logs with your APM agent, Kubernetes clusters, or distributed tracing to get additional contextual logging data with our logs in context extensions. View your logging data in New Relic You can explore your logging data in the UI or by API: Logs UI at one.newrelic.com Logs UI for EU region data center if applicable: one.eu.newrelic.com You can also query the Log data type. For example, use NRQL to run: SELECT * FROM Log Copy You can also use NerdGraph, our GraphQL-format API, to request the exact data you need. Here is an example of log data for one of an app's related entities (a workload service in this example): You can also see relevant logs in context, directly from the app in the APM UI or from the host in the Hosts UI in New Relic One. From here you can drill down into detailed data, run queries, set alerts, and more. What's next Ready to get started with our log management solutions? If you don't have one already, create a New Relic account. It's free, forever. Enable log management by forwarding your logs to New Relic. Recommendation: Use our infrastructure agent as your log forwarder, so you can get logs in context of your platform and services directly in our UI. For apps monitored by a New Relic APM agent, configure logs in context. Explore the logging data across your platform with our Logs UI in New Relic One, where you can add alerts, query your data, and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.75851,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Get started with <em>log</em> <em>management</em>",
        "sections": "Get started with <em>log</em> <em>management</em>",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " data by using the <em>Log</em> <em>API</em> or TCP endpoint. Once <em>log</em> <em>management</em> is enabled, you can also connect your <em>logs</em> with your APM agent, Kubernetes clusters, or distributed tracing to get additional contextual logging data with our <em>logs</em> in context extensions. View your logging data in New Relic You can explore"
      },
      "id": "603ea62ee7b9d249432a07e2"
    },
    {
      "sections": [
        "Drop log data with drop filter rules",
        "Savings, security, speed",
        "Caution",
        "How drop filter rules work",
        "Cautions when dropping data",
        "Create drop filter rules",
        "Manage drop filter rules via NerdGraph API",
        "Types of drop filter rules",
        "Drop log events",
        "Drop attributes",
        "Tip",
        "View or delete drop filter rules"
      ],
      "title": "Drop log data with drop filter rules",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "41ef69e9d8d23b2ab732b489bb5e0cb47b8c16b6",
      "image": "https://docs.newrelic.com/static/db4b077fafd911b9f5019b022b3048ab/b04e4/ingest-pipeline.png",
      "url": "https://docs.newrelic.com/docs/logs/ui-data/drop-data-drop-filter-rules/",
      "published_at": "2022-01-08T10:21:46Z",
      "updated_at": "2022-01-08T10:21:45Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After log event data has been sent to New Relic, it can either be stored in our NRDB database or dropped (discarded). To drop log data, you can use the logs management UI: that is explained in this doc. Alternatively, you can use NerdGraph to drop data. Savings, security, speed Drop filter rules help you accomplish several important goals: Lower costs by storing only the logs relevant to your account. Protect privacy and security by removing personal identifiable information (PII). Reduce noise by removing irrelevant events and attributes. Caution Use caution when deciding to drop data. The data you drop is not recoverable. Before using this feature, review the responsibilities and considerations for dropping data. How drop filter rules work A drop filter rule matches data based on a query. When triggered, the drop filter rule removes the matching data from the ingestion pipeline before it is written to NRDB. This creates an explicit demarcation between the logs being forwarded from your domain and the data that New Relic collects. Since the data removed by the drop filter rule doesn't reach our backend, it cannot be queried: the data is gone and cannot be restored. During the ingestion process, customer log data can be parsed, transformed, or dropped before being stored in New Relic's database. Cautions when dropping data When creating drop rules, you are responsible for ensuring that the rules accurately identify and discard the data that meets the conditions that you have established. You are also responsible for monitoring the rule, as well as the data you disclose to New Relic. New Relic cannot guarantee that this functionality will completely resolve data disclosure concerns you may have. New Relic doesn't review or monitor how effective the rules you develop are. Creating rules about sensitive data can leak information about what kinds of data you maintain, including the format of your data or systems (for example, through referencing email addresses or specific credit card numbers). Any user with the relevant role-based access control permissions can view and edit all information in the rules you create. Create drop filter rules For permissions-related requirements, see Drop data requirements. Once a drop filter rule is active, it's applied to all log events ingested from that point onwards. Rules are not applied retroactively. Logs collected before creating a rule are not filtered by that rule. Filter or query the set of logs that contain the data you want to drop. Then, from Manage data on the left nav of the Logs UI, click Create drop filter. To create a new drop filter rule, you can use new or existing log queries. Go to one.newrelic.com > Logs. Filter or query to the specific set of logs that contain the data to be dropped. Once the query is active, from Manage data on the left nav of the Logs UI, click Create drop filter. Recommendation: Change the drop rule's default name to a meaningful name. Choose to either drop the entire log event that matches the query or just a specific subset of attributes in the matching events. Review the log partitions where this drop rule applies. Save the drop filter rule. Manage drop filter rules via NerdGraph API To manage your drop filter rules programmatically, you can use NerdGraph to create, query, and delete your drop filter rules. If you create drop filter rules for log events using NerdGraph, set the source attribute to Logging. The logging UI only displays drop filters with the source attribute. Types of drop filter rules The drop filters UI prompts you to select whether to drop logs based on the query or on specific attributes. Drop log events The default type of drop filter rule is to drop logs. This option drops the entire log events that match the filter or query. When creating a rule, try to provide a specific query that only matches log data that should be dropped. Our drop filters process won't let you create drop filter rules without values in the matching query. This prevents badly formed rules from dropping all log data. Drop attributes You can specify attributes to be dropped in a log event that matches your query. At least one or more attributes must be selected. Any attribute which is selected will be dropped; all remaining attributes will be kept and stored in NRDB. Tip We recommend this method for removing fields that could contain personal identifiable information (PII) or other sensitive attributes without losing valuable monitoring data. View or delete drop filter rules To view or delete a drop filter rule: Go to one.newrelic.com > Logs. From Manage data on the left nav of the Logs UI, click Drop filters. Click the delete icon next to the drop filter rule you want to remove. Once deleted, rules no longer filter ingested log events.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.08096,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Drop <em>log</em> data with drop filter rules",
        "sections": "<em>Manage</em> drop filter rules via NerdGraph <em>API</em>",
        "tags": "<em>Log</em> <em>management</em>",
        "body": "After <em>log</em> event data has been sent to New Relic, it can either be stored in our NRDB database or dropped (discarded). To drop <em>log</em> data, you can use the <em>logs</em> <em>management</em> UI: that is explained in this doc. Alternatively, you can use NerdGraph to drop data. Savings, security, speed Drop filter rules"
      },
      "id": "603e813f28ccbc08c1eba787"
    },
    {
      "sections": [
        "Parsing log data",
        "Parsing example",
        "How log parsing works",
        "Parse attributes using Grok",
        "Organizing by logtype",
        "Important",
        "Limits",
        "Tip",
        "Built-in parsing rules",
        "List of built-in rules",
        "Add the logtype attribute",
        "New Relic infrastructure agent example",
        "Fluentd example",
        "Fluent Bit example",
        "Logstash example",
        "Logs API example",
        "Create custom parsing rules",
        "Troubleshooting"
      ],
      "title": "Parsing log data",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "67ff268a2d145b790fa501dd4eca7907d50b1912",
      "image": "https://docs.newrelic.com/static/dc392bb7142d2fdb253a649daf4ebe6d/c1b63/log-parsing-rule-ui.png",
      "url": "https://docs.newrelic.com/docs/logs/ui-data/parsing/",
      "published_at": "2022-01-08T02:41:29Z",
      "updated_at": "2022-01-04T06:04:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Parsing is the process of splitting unstructured log data into attributes (key/value pairs). You can use these attributes to facet or filter logs in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial on YouTube (approx. 4-1/2 minutes). New Relic parses log data according to rules. This document describes how logs parsing works, how to use built-in rules, and how to create custom rules. You can also create, query, and manage your log parsing rules by using NerdGraph, our GraphQL API, at api.newrelic.com/graphiql. For more information, see our NerdGraph tutorial for parsing. Parsing example A good example is a default NGINX access log containing unstructured text. It is useful for searching but not much else. Here's an example of a typical line: 127.180.71.3 - - [10/May/1997:08:05:32 +0000] \"GET /downloads/product_1 HTTP/1.1\" 304 0 \"-\" \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" Copy In an unparsed format, you would need to do a full text search to answer most questions. After parsing, the log is organized into attributes, like response code and request URL: { \"remote_addr\":\"93.180.71.3\", \"time\":\"1586514731\", \"method\":\"GET\", \"path\":\"/downloads/product_1\", \"version\":\"HTTP/1.1\", \"response\":\"304\", \"bytesSent\": 0, \"user_agent\": \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" } Copy Parsing makes it easier to create custom queries that facet on those values. This helps you understand the distribution of response codes per request URL and quickly find problematic pages. How log parsing works Here's an overview of how New Relic implements parsing of logs: Log parsing How it works What All parsing takes place against the message field; no other fields can be parsed. Each parsing rule is created with matching criteria that determines which logs the rule will attempt to parse. To simplify the matching process, we recommend adding a logtype attribute to your logs. However, you are not limited to using logtype; any attribute can be used as matching criteria. When Parsing will only be applied once to each log message. If multiple parsing rules match the log, only the first that succeeds will be applied. Parsing takes place during log ingestion, before data is written to NRDB. Once data has been written to storage, it can no longer be parsed. Parsing occurs in the pipeline before data enrichments take place. Be careful when defining the matching criteria for a parsing rule. If the criteria is based on an attribute that doesn't exist untail after parsing or enrichment take place, that data won't be present in the logs when matching occurs. As a result, no parsing will happen. How Rules can be written in Grok, regex, or a mixture of the two. Grok is a collection of patterns that abstract away complicated regular expressions. If the content of the message field is JSON, it will be parsed automatically. Parse attributes using Grok Parsing patterns are specified using Grok, an industry standard for parsing log messages. Grok is a superset of regular expressions that adds built-in named patterns to be used in place of literal complex regular expressions. For instance, instead of having to remember that an integer can be matched with the regular expression (?:[+-]?(?:[0-9]+)), you can just write %{INT} to use the Grok pattern INT, which represents the same regular expression. You can always use a mix of regular expressions and Grok pattern names in your matching string. Grok patterns have the syntax: %{PATTERN_NAME[:OPTIONAL_EXTRACTED_ATTRIBUTE_NAME[:OPTIONAL_TYPE]]} Where: PATTERN_NAME is one of the Grok patterns. Click grok-patterns to see the most commonly-used patterns. The pattern name is just a user-friendly name representing a regular expression. They are exactly equal to the corresponding regular expression. OPTIONAL_EXTRACTED_ATTRIBUTE_NAME, if provided, is the name of the attribute that will be added to your log message with the value matched by the pattern name. It's equivalent to using a named capture group using regular expressions. If this is not provided, then the parsing rule will just match a region of your string, but not extract an attribute with its value. OPTIONAL_TYPE specifies the type of attribute value to extract. If omitted, values are extracted as strings. For instance, to extract the value 123 from \"File Size: 123\" as a number into attribute file_size, use value: %{INT:file_size:int}. Supported types are: Type specified in Grok Type stored in the New Relic database boolean boolean byte short int integer integer long long float float double double string (default) text string date datetime ISO 8601 time as a long See our blog post for more details. Organizing by logtype New Relic's log ingestion pipeline can parse data by matching a log event to a rule that describes how the log should be parsed. There are two ways log events can be parsed: Use a built-in rule. Define a custom rule. Rules are a combination of matching logic and parsing logic. Matching is done by defining a query match on an attribute of the logs. Rules are not applied retroactively. Logs collected before a rule is created are not parsed by that rule. The simplest way to organize your logs and how they are parsed is to include the logtype field in your log event. This tells New Relic what built-in rule to apply to the logs. Important Once a parsing rule is active, data parsed by the rule is permanently changed. This cannot be reverted. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. Limits Parsing is computationally expensive, which introduces risk. Parsing is done for custom rules defined in an account and for matching patterns to a log. A large number of patterns or poorly defined custom rules will consume a huge amount of memory and CPU resources while also taking a very long time to complete. In order to prevent problems, we apply two parsing limits: per-message-per-rule and per-account. Limit Description Per-message-per-rule The per-message-per-rule limit prevents the time spent parsing any single message from being greater than 100 ms. If that limit is reached, the system will cease attempting to parse the log message with that rule. The ingestion pipeline will attempt to run any other applicable on that message, and the message will still be passed through the ingestion pipeline and stored in NRDB. The log message will be in its original, unparsed format. Per-account The per-account limit exists to prevent accounts from using more than their fair share of resources. The limit considers the total time spent processing all log messages for an account per-minute. The limit is not a fixed value; it scales up or down proportionally to the volume of data stored daily by the account and the environment size that is subsequently allocated to support that customer. Tip To easily check if your rate limits have been reached, go to your system Limits page in the New Relic UI. Built-in parsing rules Common log formats have well-established parsing rules already created for them. To get the benefit of built-in parsing rules, add the logtype attribute when forwarding logs. Set the value to something listed in the following table, and the rules for that type of log will be applied automatically. List of built-in rules The following logtype attribute values map to a predefined parsing rule. For example, to query the Application Load Balancer: From the New Relic UI, use the format logtype: alb. From NerdGraph, use the format logtype = 'alb'. To learn what fields are parsed for each rule, see our documentation about built-in parsing rules. logtype Example matching query alb AWS Application Load Balancer logtype:alb apache Apache Access logtype:apache cloudfront-web CloudFront Web logtype:cloudfront-web elb Amazon Elastic Load Balancer logtype:elb iis_w3c Microsoft IIS server logs - W3C format logtype:iis_w3c monit Monit logs logtype:monit mysql-error MySQL Error logtype:mysql-error nginx NGINX access logs logtype:nginx nginx-error NGINX error logs logtype:nginx-error route-53 Amazon Route 53 logs logtype:route-53 syslog-rfc5424 Syslog logtype:syslog-rfc5424 Add the logtype attribute When aggregating logs, it's important to provide metadata that makes it easy to organize, search, and parse those logs. One simple way of doing this is to add the attribute logtype to the log messages when they are shipped. Built-in parsing rules are applied by default to certain logtype values. Here are some examples of how to add logtype to logs sent by some of our supported shipping methods. New Relic infrastructure agent example Add logtype as an attribute. You must set the logtype for each named source. logs: - name: file-simple file: /path/to/file attributes: logtype: fileRaw - name: nginx-example file: /var/log/nginx.log attributes: logtype: nginx Copy Fluentd example Add a filter block to the .conf file, which uses a record_transformer to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluentd examples. <filter containers> @type record_transformer enable_ruby true <record> #Add logtype to trigger a built-in parsing rule for nginx access logs logtype nginx #Set timestamp from the value contained in the field \"time\" timestamp record[\"time\"] #Add hostname and tag fields to all records hostname \"#{Socket.gethostname}\" tag ${tag} </record> </filter> Copy Fluent Bit example Add a filter block to the .conf file that uses a record_modifier to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluent Bit examples. [FILTER] Name record_modifier Match * Record logtype nginx Record hostname ${HOSTNAME} Record service_name Sample-App-Name Copy Logstash example Add a filter block to the Logstash configuration which uses an add_field mutate filter to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Logstash examples. filter { mutate { add_field => { \"logtype\" => \"nginx\" \"service_name\" => \"myservicename\" \"hostname\" => \"%{host}\" } } } Copy Logs API example You can add attributes to the JSON request sent to New Relic. In this example we add a logtype attribute of value nginx to trigger the built-in NGINX parsing rule. Learn more about using the Logs API. POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: YOUR_LICENSE_KEY Accept: */* Content-Length: 133 { \"timestamp\": TIMESTAMP_IN_UNIX_EPOCH, \"message\": \"User 'xyz' logged in\", \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } Copy Create custom parsing rules Many logs are formatted or structured in a unique way. In order to parse them, custom logic must be built and applied. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. To create and manage your own, custom parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing, then click Create parsing rule. Enter the parsing rule's name. Choose an attribute and value to match on. Write your Grok pattern and test the rule. To learn about Grok and custom parsing rules, read our blog post about how to parse logs with Grok patterns. Enable and save the custom parsing rule. To view the list of custom parsing rules: From Manage Data on the left nav of the Logs UI, click Parsing. To view existing parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing. Troubleshooting If parsing is not working the way you intended, it may be due to: Logic: The parsing rule matching logic does not match the logs you want. Timing: If your parsing matching rule targets a value that doesn't exist yet, it will fail. This can occur if the value is added later in the pipeline as part of the enrichment process. Limits: There is a fixed amount of time available every minute to process logs via parsing, patterns, drop filters, etc. If the maximum amount of time has been spent, parsing will be skipped for additional log event records. To resolve these problems, create or adjust your custom parsing rules.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.9596,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Parsing <em>log</em> data",
        "sections": "<em>Logs</em> <em>API</em> example",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " on YouTube (approx. 4-1&#x2F;2 minutes). New Relic parses <em>log</em> data according to rules. This document describes how <em>logs</em> parsing works, how to use built-in rules, and how to create custom rules. You can also create, query, and manage your <em>log</em> parsing rules by using NerdGraph, our GraphQL <em>API</em>"
      },
      "id": "619030c5e7b9d20cf0386b1c"
    }
  ],
  "/docs/logs/log-api/log-event-data": [
    {
      "sections": [
        "Drop log data with drop filter rules",
        "Savings, security, speed",
        "Caution",
        "How drop filter rules work",
        "Cautions when dropping data",
        "Create drop filter rules",
        "Manage drop filter rules via NerdGraph API",
        "Types of drop filter rules",
        "Drop log events",
        "Drop attributes",
        "Tip",
        "View or delete drop filter rules"
      ],
      "title": "Drop log data with drop filter rules",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "41ef69e9d8d23b2ab732b489bb5e0cb47b8c16b6",
      "image": "https://docs.newrelic.com/static/db4b077fafd911b9f5019b022b3048ab/b04e4/ingest-pipeline.png",
      "url": "https://docs.newrelic.com/docs/logs/ui-data/drop-data-drop-filter-rules/",
      "published_at": "2022-01-08T10:21:46Z",
      "updated_at": "2022-01-08T10:21:45Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After log event data has been sent to New Relic, it can either be stored in our NRDB database or dropped (discarded). To drop log data, you can use the logs management UI: that is explained in this doc. Alternatively, you can use NerdGraph to drop data. Savings, security, speed Drop filter rules help you accomplish several important goals: Lower costs by storing only the logs relevant to your account. Protect privacy and security by removing personal identifiable information (PII). Reduce noise by removing irrelevant events and attributes. Caution Use caution when deciding to drop data. The data you drop is not recoverable. Before using this feature, review the responsibilities and considerations for dropping data. How drop filter rules work A drop filter rule matches data based on a query. When triggered, the drop filter rule removes the matching data from the ingestion pipeline before it is written to NRDB. This creates an explicit demarcation between the logs being forwarded from your domain and the data that New Relic collects. Since the data removed by the drop filter rule doesn't reach our backend, it cannot be queried: the data is gone and cannot be restored. During the ingestion process, customer log data can be parsed, transformed, or dropped before being stored in New Relic's database. Cautions when dropping data When creating drop rules, you are responsible for ensuring that the rules accurately identify and discard the data that meets the conditions that you have established. You are also responsible for monitoring the rule, as well as the data you disclose to New Relic. New Relic cannot guarantee that this functionality will completely resolve data disclosure concerns you may have. New Relic doesn't review or monitor how effective the rules you develop are. Creating rules about sensitive data can leak information about what kinds of data you maintain, including the format of your data or systems (for example, through referencing email addresses or specific credit card numbers). Any user with the relevant role-based access control permissions can view and edit all information in the rules you create. Create drop filter rules For permissions-related requirements, see Drop data requirements. Once a drop filter rule is active, it's applied to all log events ingested from that point onwards. Rules are not applied retroactively. Logs collected before creating a rule are not filtered by that rule. Filter or query the set of logs that contain the data you want to drop. Then, from Manage data on the left nav of the Logs UI, click Create drop filter. To create a new drop filter rule, you can use new or existing log queries. Go to one.newrelic.com > Logs. Filter or query to the specific set of logs that contain the data to be dropped. Once the query is active, from Manage data on the left nav of the Logs UI, click Create drop filter. Recommendation: Change the drop rule's default name to a meaningful name. Choose to either drop the entire log event that matches the query or just a specific subset of attributes in the matching events. Review the log partitions where this drop rule applies. Save the drop filter rule. Manage drop filter rules via NerdGraph API To manage your drop filter rules programmatically, you can use NerdGraph to create, query, and delete your drop filter rules. If you create drop filter rules for log events using NerdGraph, set the source attribute to Logging. The logging UI only displays drop filters with the source attribute. Types of drop filter rules The drop filters UI prompts you to select whether to drop logs based on the query or on specific attributes. Drop log events The default type of drop filter rule is to drop logs. This option drops the entire log events that match the filter or query. When creating a rule, try to provide a specific query that only matches log data that should be dropped. Our drop filters process won't let you create drop filter rules without values in the matching query. This prevents badly formed rules from dropping all log data. Drop attributes You can specify attributes to be dropped in a log event that matches your query. At least one or more attributes must be selected. Any attribute which is selected will be dropped; all remaining attributes will be kept and stored in NRDB. Tip We recommend this method for removing fields that could contain personal identifiable information (PII) or other sensitive attributes without losing valuable monitoring data. View or delete drop filter rules To view or delete a drop filter rule: Go to one.newrelic.com > Logs. From Manage data on the left nav of the Logs UI, click Drop filters. Click the delete icon next to the drop filter rule you want to remove. Once deleted, rules no longer filter ingested log events.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 351.6852,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Drop <em>log</em> <em>data</em> with drop filter rules",
        "sections": "Drop <em>log</em> <em>data</em> with drop filter rules",
        "tags": "<em>Log</em> <em>management</em>",
        "body": "After <em>log</em> event <em>data</em> has been sent to New Relic, it can either be stored in our NRDB database or dropped (discarded). To drop <em>log</em> <em>data</em>, you can use the <em>logs</em> <em>management</em> <em>UI</em>: that is explained in this doc. Alternatively, you can use NerdGraph to drop <em>data</em>. Savings, security, speed Drop filter rules"
      },
      "id": "603e813f28ccbc08c1eba787"
    },
    {
      "sections": [
        "Use Logs UI",
        "Explore your log data",
        "Tip",
        "Save your views",
        "Examples",
        "Create an alert from log data",
        "Add log volume chart to a dashboard",
        "Troubleshoot an error (logs in context)",
        "Troubleshoot latency (logs in context)",
        "Links to logs in New Relic"
      ],
      "title": "Use Logs UI",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "c5ff41f3e659ed1e83243e9c4d1dfbe9ebba2811",
      "image": "https://docs.newrelic.com/static/939df456a92cd602106bbfe9bbc4e4ce/c1b63/logs-ui-123021.png",
      "url": "https://docs.newrelic.com/docs/logs/ui-data/use-logs-ui/",
      "published_at": "2022-01-08T02:42:17Z",
      "updated_at": "2022-01-04T07:03:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use the Logs UI at one.newrelic.com or our EU region data center (if applicable) at one.eu.newrelic.com/ to: Spot interesting or significant patterns in your logs. Examine more context around a particular log line. Explore and manipulate your logging data with filters and parsing rules. Query and share the data with charts, add to dashboards, etc. Organize your account's log data, and optimize query performance with data partitions. Set up alert conditions for problems you want to prevent. To stay up to date with new capabilities and improvements, subscribe to our RSS feed for Logs release notes. Explore your log data Use the left nav in the Logs UI as an easy workflow through all logs, attributes, patterns, live-tail logging, and queries. Manage your log data by dropping or parsing data, creating data partitions or alert conditions. Get more details about specific logs and their attributes from the center nav. To explore your logging data, follow this basic workflow. If you have not customized your New Relic One navigation bar, go to one.newrelic.com, click Browse data and select Logs. Look for patterns: To spot suspicious spikes or drops in log messages, click the patterns icon on the left nav. To look at logs for a specific time period, click that point (or click and drag an area) on the chart, or use the time picker. Narrow your focus: To narrow the focus of your initial search results or quickly find outliers, expand any attributes in the log details to view the ten most common values within the results. For example, if a host listed under the hostname attribute is generating significantly more error messages than the others, select that value to apply it to your search. To make your log messages easier to query and understand, use our built-in parsing rules, or create your own parsing rules for attributes that match a particular value. To manage the amount of log data collected and to store fewer logs, create drop filter rules that avoid collecting data you don't need. Examine log details: Select a log message to view its details as a table of attributes or as JSON. To see which attributes are included in a log message, click the log line. Add or remove attributes as needed to help your query focus on the details you need. To help troubleshoot problems related to a specific value in the log details, click the Show surrounding logs icon for the attribute's details. To control which attributes appear in the results, click any highlighted value in the log's log_summary column. To get more details in extremely long messages, expand the data stored as blobs. Search: By default, the Logs UI shows all your logs, but you can also search with keywords or phrases to find the results you want; for example, process failed. OR From the search field, use the type-ahead dropdowns to select an attribute, operator, and value; for example: service_name equals my service Copy For more information, see the logs query syntax documentation. Tip To organize data within an account and to optimize query performance, create data partition rules. Get related logs: For example: To immediately see how your system responds to deployments or other app changes, enable live-tail logging. To view all the logs for a specific value, review the attributes list in the selected log's Log details. To help identify an issue's root cause before it occurred or its impact after an event, click the Show surrounding logs icon. Share your findings: Use any of the core New Relic One functions (specific account, time range, data explorer, query builder, etc.) to share the data with charts, add to dashboards, etc. For more information, see the examples in this document. Save your views You can save your logs query, table configuration, time range, and attribute grouping in a saved view, so that you can quickly return to it later. To save a log analytics view after you've configured the view: Click the Saved Views tab in the Logs UI left nav. Click Save current view. Give your saved view the name you want for it to be listed in the Saved Views tab. Select which aspects from the current view you want to save. Select the permission level: Private, Public (Read-only), and Public (Read and write). Public means that any user with access to the account is able to see the saved view. Click Save view. Examples Here are a few examples of how you can use the Logs UI to get detailed information. To use some of these examples, you must be able to see logs in context. Create an alert from log data You can create alert conditions directly in the Logs UI: Go to one.newrelic.com > Logs. Search for results that you want to alert on; for example, service_name:\"your service\" \"fatal error\". From the Manage data section on the left nav, click Create alert condition. Complete the Create an alert condition section that slides out, then review the NRQL query that will power the alert condition. After you save the Logs alert condition, you can view it in the Alerts UI, where you can make additional changes as needed. Add log volume chart to a dashboard You can add log charts to a dashboard directly from the Logs UI. Go to one.newrelic.com > Logs. Search for results you want to plot; for example, service_name:\"checkout service\" \"process failed\". OR Select a saved view. Click Add to dashboard, then fill out the details to add to an existing or new dashboard. You can also create charts with the data explorer or the query builder in New Relic One. Troubleshoot an error (logs in context) To troubleshoot errors this way, you must be able to see [logs in context] /docs/logs/logs-context/configure-logs-context-apm-agents/). Then, to have a better understanding of what was happening on the host at the time an error occurred in your app: Go to APM > (select an app) > Events > Error analytics and select an error trace. From the error trace Details, click See logs. From the Logs UI, browse the related log details. To identify the host generating the error, click Show surrounding logs. Troubleshoot latency (logs in context) To troubleshoot latency this way, you must be able to see logs in context. Then, to have a better understanding of how your systems were operating when performance noticeably slowed: Go to one.newrelic.com > Distributed tracing. Select a particularly slow trace. From the trace Details, click See logs for this trace. Browse related logs in the Logs UI. Links to logs in New Relic Depending on your New Relic subscription, you can access your logs from several places in the New Relic UI. For some of these options, you must be able to see logs in context. To view logs... Do this... Directly from the Logs UI Go to one.newrelic.com > Logs. EU region data center (if available): Go to one.eu.newrelic.com/ > Logs. From distributed tracing Go to one.newrelic.com > Distributed tracing > (select a trace > Logs (if available). From a host in your infrastructure Go to one.newrelic.com > Infrastructure > Hosts > (select a host) > Events explorer > Logs (if available). From Kubernetes Go to one.newrelic.com > Kubernetes cluster explorer > (select a cluster) > (select a pod or container) > See logs (if available). From an entity Go to one.newrelic.com > Explorer > (select an entity) > Logs (if available). From your app in APM (logs in context) Go to one.newrelic.com > APM > (select an app) > Events > Logs (if available). From an error trace in APM (logs in context) Go to one.newrelic.com > APM > (select an app) > Error analytics > (select an error trace) > See logs (if available).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.0178,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Use <em>Logs</em> <em>UI</em>",
        "sections": "Use <em>Logs</em> <em>UI</em>",
        "tags": "<em>Log</em> <em>management</em>",
        "body": "Use the <em>Logs</em> <em>UI</em> at one.newrelic.com or our EU region <em>data</em> center (if applicable) at one.eu.newrelic.com&#x2F; to: Spot interesting or significant patterns in your <em>logs</em>. Examine more context around a particular <em>log</em> line. Explore and manipulate your logging <em>data</em> with filters and parsing rules. Query"
      },
      "id": "603ea62e64441ff7ba4e8854"
    },
    {
      "sections": [
        "Parsing log data",
        "Parsing example",
        "How log parsing works",
        "Parse attributes using Grok",
        "Organizing by logtype",
        "Important",
        "Limits",
        "Tip",
        "Built-in parsing rules",
        "List of built-in rules",
        "Add the logtype attribute",
        "New Relic infrastructure agent example",
        "Fluentd example",
        "Fluent Bit example",
        "Logstash example",
        "Logs API example",
        "Create custom parsing rules",
        "Troubleshooting"
      ],
      "title": "Parsing log data",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "67ff268a2d145b790fa501dd4eca7907d50b1912",
      "image": "https://docs.newrelic.com/static/dc392bb7142d2fdb253a649daf4ebe6d/c1b63/log-parsing-rule-ui.png",
      "url": "https://docs.newrelic.com/docs/logs/ui-data/parsing/",
      "published_at": "2022-01-08T02:41:29Z",
      "updated_at": "2022-01-04T06:04:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Parsing is the process of splitting unstructured log data into attributes (key/value pairs). You can use these attributes to facet or filter logs in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial on YouTube (approx. 4-1/2 minutes). New Relic parses log data according to rules. This document describes how logs parsing works, how to use built-in rules, and how to create custom rules. You can also create, query, and manage your log parsing rules by using NerdGraph, our GraphQL API, at api.newrelic.com/graphiql. For more information, see our NerdGraph tutorial for parsing. Parsing example A good example is a default NGINX access log containing unstructured text. It is useful for searching but not much else. Here's an example of a typical line: 127.180.71.3 - - [10/May/1997:08:05:32 +0000] \"GET /downloads/product_1 HTTP/1.1\" 304 0 \"-\" \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" Copy In an unparsed format, you would need to do a full text search to answer most questions. After parsing, the log is organized into attributes, like response code and request URL: { \"remote_addr\":\"93.180.71.3\", \"time\":\"1586514731\", \"method\":\"GET\", \"path\":\"/downloads/product_1\", \"version\":\"HTTP/1.1\", \"response\":\"304\", \"bytesSent\": 0, \"user_agent\": \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" } Copy Parsing makes it easier to create custom queries that facet on those values. This helps you understand the distribution of response codes per request URL and quickly find problematic pages. How log parsing works Here's an overview of how New Relic implements parsing of logs: Log parsing How it works What All parsing takes place against the message field; no other fields can be parsed. Each parsing rule is created with matching criteria that determines which logs the rule will attempt to parse. To simplify the matching process, we recommend adding a logtype attribute to your logs. However, you are not limited to using logtype; any attribute can be used as matching criteria. When Parsing will only be applied once to each log message. If multiple parsing rules match the log, only the first that succeeds will be applied. Parsing takes place during log ingestion, before data is written to NRDB. Once data has been written to storage, it can no longer be parsed. Parsing occurs in the pipeline before data enrichments take place. Be careful when defining the matching criteria for a parsing rule. If the criteria is based on an attribute that doesn't exist untail after parsing or enrichment take place, that data won't be present in the logs when matching occurs. As a result, no parsing will happen. How Rules can be written in Grok, regex, or a mixture of the two. Grok is a collection of patterns that abstract away complicated regular expressions. If the content of the message field is JSON, it will be parsed automatically. Parse attributes using Grok Parsing patterns are specified using Grok, an industry standard for parsing log messages. Grok is a superset of regular expressions that adds built-in named patterns to be used in place of literal complex regular expressions. For instance, instead of having to remember that an integer can be matched with the regular expression (?:[+-]?(?:[0-9]+)), you can just write %{INT} to use the Grok pattern INT, which represents the same regular expression. You can always use a mix of regular expressions and Grok pattern names in your matching string. Grok patterns have the syntax: %{PATTERN_NAME[:OPTIONAL_EXTRACTED_ATTRIBUTE_NAME[:OPTIONAL_TYPE]]} Where: PATTERN_NAME is one of the Grok patterns. Click grok-patterns to see the most commonly-used patterns. The pattern name is just a user-friendly name representing a regular expression. They are exactly equal to the corresponding regular expression. OPTIONAL_EXTRACTED_ATTRIBUTE_NAME, if provided, is the name of the attribute that will be added to your log message with the value matched by the pattern name. It's equivalent to using a named capture group using regular expressions. If this is not provided, then the parsing rule will just match a region of your string, but not extract an attribute with its value. OPTIONAL_TYPE specifies the type of attribute value to extract. If omitted, values are extracted as strings. For instance, to extract the value 123 from \"File Size: 123\" as a number into attribute file_size, use value: %{INT:file_size:int}. Supported types are: Type specified in Grok Type stored in the New Relic database boolean boolean byte short int integer integer long long float float double double string (default) text string date datetime ISO 8601 time as a long See our blog post for more details. Organizing by logtype New Relic's log ingestion pipeline can parse data by matching a log event to a rule that describes how the log should be parsed. There are two ways log events can be parsed: Use a built-in rule. Define a custom rule. Rules are a combination of matching logic and parsing logic. Matching is done by defining a query match on an attribute of the logs. Rules are not applied retroactively. Logs collected before a rule is created are not parsed by that rule. The simplest way to organize your logs and how they are parsed is to include the logtype field in your log event. This tells New Relic what built-in rule to apply to the logs. Important Once a parsing rule is active, data parsed by the rule is permanently changed. This cannot be reverted. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. Limits Parsing is computationally expensive, which introduces risk. Parsing is done for custom rules defined in an account and for matching patterns to a log. A large number of patterns or poorly defined custom rules will consume a huge amount of memory and CPU resources while also taking a very long time to complete. In order to prevent problems, we apply two parsing limits: per-message-per-rule and per-account. Limit Description Per-message-per-rule The per-message-per-rule limit prevents the time spent parsing any single message from being greater than 100 ms. If that limit is reached, the system will cease attempting to parse the log message with that rule. The ingestion pipeline will attempt to run any other applicable on that message, and the message will still be passed through the ingestion pipeline and stored in NRDB. The log message will be in its original, unparsed format. Per-account The per-account limit exists to prevent accounts from using more than their fair share of resources. The limit considers the total time spent processing all log messages for an account per-minute. The limit is not a fixed value; it scales up or down proportionally to the volume of data stored daily by the account and the environment size that is subsequently allocated to support that customer. Tip To easily check if your rate limits have been reached, go to your system Limits page in the New Relic UI. Built-in parsing rules Common log formats have well-established parsing rules already created for them. To get the benefit of built-in parsing rules, add the logtype attribute when forwarding logs. Set the value to something listed in the following table, and the rules for that type of log will be applied automatically. List of built-in rules The following logtype attribute values map to a predefined parsing rule. For example, to query the Application Load Balancer: From the New Relic UI, use the format logtype: alb. From NerdGraph, use the format logtype = 'alb'. To learn what fields are parsed for each rule, see our documentation about built-in parsing rules. logtype Example matching query alb AWS Application Load Balancer logtype:alb apache Apache Access logtype:apache cloudfront-web CloudFront Web logtype:cloudfront-web elb Amazon Elastic Load Balancer logtype:elb iis_w3c Microsoft IIS server logs - W3C format logtype:iis_w3c monit Monit logs logtype:monit mysql-error MySQL Error logtype:mysql-error nginx NGINX access logs logtype:nginx nginx-error NGINX error logs logtype:nginx-error route-53 Amazon Route 53 logs logtype:route-53 syslog-rfc5424 Syslog logtype:syslog-rfc5424 Add the logtype attribute When aggregating logs, it's important to provide metadata that makes it easy to organize, search, and parse those logs. One simple way of doing this is to add the attribute logtype to the log messages when they are shipped. Built-in parsing rules are applied by default to certain logtype values. Here are some examples of how to add logtype to logs sent by some of our supported shipping methods. New Relic infrastructure agent example Add logtype as an attribute. You must set the logtype for each named source. logs: - name: file-simple file: /path/to/file attributes: logtype: fileRaw - name: nginx-example file: /var/log/nginx.log attributes: logtype: nginx Copy Fluentd example Add a filter block to the .conf file, which uses a record_transformer to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluentd examples. <filter containers> @type record_transformer enable_ruby true <record> #Add logtype to trigger a built-in parsing rule for nginx access logs logtype nginx #Set timestamp from the value contained in the field \"time\" timestamp record[\"time\"] #Add hostname and tag fields to all records hostname \"#{Socket.gethostname}\" tag ${tag} </record> </filter> Copy Fluent Bit example Add a filter block to the .conf file that uses a record_modifier to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluent Bit examples. [FILTER] Name record_modifier Match * Record logtype nginx Record hostname ${HOSTNAME} Record service_name Sample-App-Name Copy Logstash example Add a filter block to the Logstash configuration which uses an add_field mutate filter to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Logstash examples. filter { mutate { add_field => { \"logtype\" => \"nginx\" \"service_name\" => \"myservicename\" \"hostname\" => \"%{host}\" } } } Copy Logs API example You can add attributes to the JSON request sent to New Relic. In this example we add a logtype attribute of value nginx to trigger the built-in NGINX parsing rule. Learn more about using the Logs API. POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: YOUR_LICENSE_KEY Accept: */* Content-Length: 133 { \"timestamp\": TIMESTAMP_IN_UNIX_EPOCH, \"message\": \"User 'xyz' logged in\", \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } Copy Create custom parsing rules Many logs are formatted or structured in a unique way. In order to parse them, custom logic must be built and applied. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. To create and manage your own, custom parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing, then click Create parsing rule. Enter the parsing rule's name. Choose an attribute and value to match on. Write your Grok pattern and test the rule. To learn about Grok and custom parsing rules, read our blog post about how to parse logs with Grok patterns. Enable and save the custom parsing rule. To view the list of custom parsing rules: From Manage Data on the left nav of the Logs UI, click Parsing. To view existing parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing. Troubleshooting If parsing is not working the way you intended, it may be due to: Logic: The parsing rule matching logic does not match the logs you want. Timing: If your parsing matching rule targets a value that doesn't exist yet, it will fail. This can occur if the value is added later in the pipeline as part of the enrichment process. Limits: There is a fixed amount of time available every minute to process logs via parsing, patterns, drop filters, etc. If the maximum amount of time has been spent, parsing will be skipped for additional log event records. To resolve these problems, create or adjust your custom parsing rules.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 289.6018,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Parsing <em>log</em> <em>data</em>",
        "sections": "Parsing <em>log</em> <em>data</em>",
        "tags": "<em>Log</em> <em>management</em>",
        "body": "Parsing is the process of splitting unstructured <em>log</em> <em>data</em> into attributes (key&#x2F;value pairs). You can use these attributes to facet or filter <em>logs</em> in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial"
      },
      "id": "619030c5e7b9d20cf0386b1c"
    }
  ]
}