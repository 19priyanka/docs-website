{
  "/docs/licenses/license-information/general-usage-licenses/new-relics-provision-services": [
    {
      "sections": [
        "New Relic data usage limits and policies",
        "View limits and manage data"
      ],
      "title": "New Relic data usage limits and policies",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "fc32c25b40a030ffa0fad6bfc95be7fca1360ee1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies/",
      "published_at": "2021-10-19T04:02:17Z",
      "updated_at": "2021-10-19T04:02:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers' accounts, we have various data volume and rate limits in place. We reserve the right to enforce these limits to protect our system and to avoid issues for you and other customers. If your New Relic account, whether by configuration or by error, exceeds one of these limits, it or its child accounts might experience one or both of the following: Sampling of data Temporary pause or cessation of data collection To learn more about how hitting a limit can affect your data, see View limits. If you have further questions about these limits, your contract, or a limit you've reached, contact your New Relic account representative. We can work with you to adjust any rate limits to meet your needs. View limits and manage data For information about system and account limits, and for links to data ingest API limits, go to View limits. To manage your data ingest, storage, and limits for organization or billing purposes, go to Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 418.12622,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic data <em>usage</em> limits and policies",
        "sections": "New Relic data <em>usage</em> limits and policies",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data <em>usage</em> spikes in one New Relic account from impacting other customers&#x27; accounts, we have various data volume and rate limits in place. We reserve the right to enforce"
      },
      "id": "603eb1c528ccbc0311eba7c7"
    },
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Important",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-10-18T12:23:05Z",
      "updated_at": "2021-09-07T23:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Important NOTE: If you are a New Relic HIPAA customer, please be advised that you must follow the requirements specified in the Global Technical Support Section of HIPAA enablement - what you need to know and do when requesting support and engaging with the New Relic Global Technical Support team for assistance. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customer’s paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customer’s paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relic’s distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relic’s Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and we’re here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relic’s Open Source Community Plus Projects. New Relic’s Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relic’s Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relic’s Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered “generally available.” If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we can’t help with things we didn’t build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and we’ll help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. We’ll do our best to determine whether an issue is with New Relic’s Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the “account owner,” such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relic’s Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Important As of March 10, 2021, you're no longer able to create a New Relic support ticket by emailing support@newrelic.com. If you have ticketed support as part of your subscription, when logged into your New Relic account, go to one.newrelic.com to create a support ticket. Once you're there, follow these steps: Click the ? icon in the upper right hand corner. Click I need more help. Select Create a Support Ticket. By creating a support ticket directly within your account, it will speed up the support process and improve our team’s ability to troubleshoot your issue more effectively. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relic’s Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 231.66289,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " environments Troubleshooting problems with ingesting data into New Relic <em>General</em> <em>usage</em> and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments &amp; frameworks"
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "The People's Republic of China"
      ],
      "title": "The People's Republic of China",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "c3c078b8139d695b928d2001cd6c6c9318c43599",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/peoples-republic-china/",
      "published_at": "2021-10-18T14:36:06Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important Information Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory requirements in the PRC. New Relic does not provide support for, including but not limited to, the deployment, access, or use of the Service(s) or Software in the PRC, or otherwise in respect of assets in the PRC (“PRC Use”). Without overriding any express prohibitions that you may have agreed to as part of your agreement with New Relic, you (and you as an agent of the respective New Relic Customer (\"Customer\")) acknowledge that any PRC use is subject to the PRC national firewall system and may be subject to outages and other interference outside of the control of New Relic. Accordingly, you acknowledge and agree (on behalf of yourself and including any Customer on whose behalf you use the Service(s) or Software) that any PRC Use is at your sole risk and is fully excepted from all representations and warranties, including, but not limited to, any terms that assure the confidentiality, integrity, availability or privacy of your data. New Relic makes no other representations and hereby expressly disclaims any and all warranties in respect of PRC Use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 199.39331,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "Important <em>Information</em> Regarding the People’s Republic of China and the Use of New Relic New Relic is not authorized to do business in the People’s Republic of China (“PRC”). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory"
      },
      "id": "603eb3a328ccbcb488eba76b"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/peoples-republic-china": [
    {
      "sections": [
        "New Relic data usage limits and policies",
        "View limits and manage data"
      ],
      "title": "New Relic data usage limits and policies",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "fc32c25b40a030ffa0fad6bfc95be7fca1360ee1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies/",
      "published_at": "2021-10-19T04:02:17Z",
      "updated_at": "2021-10-19T04:02:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers' accounts, we have various data volume and rate limits in place. We reserve the right to enforce these limits to protect our system and to avoid issues for you and other customers. If your New Relic account, whether by configuration or by error, exceeds one of these limits, it or its child accounts might experience one or both of the following: Sampling of data Temporary pause or cessation of data collection To learn more about how hitting a limit can affect your data, see View limits. If you have further questions about these limits, your contract, or a limit you've reached, contact your New Relic account representative. We can work with you to adjust any rate limits to meet your needs. View limits and manage data For information about system and account limits, and for links to data ingest API limits, go to View limits. To manage your data ingest, storage, and limits for organization or billing purposes, go to Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 418.12595,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic data <em>usage</em> limits and policies",
        "sections": "New Relic data <em>usage</em> limits and policies",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data <em>usage</em> spikes in one New Relic account from impacting other customers&#x27; accounts, we have various data volume and rate limits in place. We reserve the right to enforce"
      },
      "id": "603eb1c528ccbc0311eba7c7"
    },
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Important",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-10-18T12:23:05Z",
      "updated_at": "2021-09-07T23:56:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Important NOTE: If you are a New Relic HIPAA customer, please be advised that you must follow the requirements specified in the Global Technical Support Section of HIPAA enablement - what you need to know and do when requesting support and engaging with the New Relic Global Technical Support team for assistance. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customer’s paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customer’s paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customer’s business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relic’s distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relic’s Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and we’re here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relic’s Open Source Community Plus Projects. New Relic’s Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relic’s Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relic’s Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relic’s Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered “generally available.” If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we can’t help with things we didn’t build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and we’ll help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. We’ll do our best to determine whether an issue is with New Relic’s Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the “account owner,” such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relic’s Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Important As of March 10, 2021, you're no longer able to create a New Relic support ticket by emailing support@newrelic.com. If you have ticketed support as part of your subscription, when logged into your New Relic account, go to one.newrelic.com to create a support ticket. Once you're there, follow these steps: Click the ? icon in the upper right hand corner. Click I need more help. Select Create a Support Ticket. By creating a support ticket directly within your account, it will speed up the support process and improve our team’s ability to troubleshoot your issue more effectively. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relic’s Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 231.66287,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " environments Troubleshooting problems with ingesting data into New Relic <em>General</em> <em>usage</em> and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments &amp; frameworks"
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "Government addendum",
        "New Relic, Inc.Government Addendum"
      ],
      "title": "Government addendum",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "cfed51b7f4f7583476f56b2b574204d38493e882",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/government-addendum/",
      "published_at": "2021-10-18T15:31:42Z",
      "updated_at": "2021-03-16T04:21:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The below Government Addendum applies only to customers that are United States federal, state, or local government customers with an existing New Relic agreement in place that explicitly references this Government Addendum applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic, Inc.Government Addendum For purposes of this Agreement and to the extent applicable, the Service is \"commercial computer software\" and a \"commercially available off-the-shelf (COTS) item\" as defined at FAR 2.101 developed at the private expense of New Relic. If acquired by or on behalf of a civilian agency, the U.S. Government acquires this commercial computer software and/or commercial computer software documentation and other technical data subject to the terms of the Agreement as specified in 48 C.F.R. 12.212 (Computer Software) and 12.211 (Technical Data) of the Federal Acquisition Regulation (\"FAR\") and its successors. If acquired by or on behalf of any agency within the Department of Defense (\"DOD\"), the U.S. Government acquires this commercial computer software and/or commercial computer software documentation subject to the terms of the Agreement as specified in 48 C.F.R. 227.7202-3 of the DOD FAR Supplement (\"DFARS\") and its successors. This addendum is in lieu of and supersedes any other FAR, DFARS, or other clause or provision that addresses government rights in computer software or technical data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 199.39267,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>"
      },
      "id": "603ea47ee7b9d2dd0d2a07ef"
    }
  ],
  "/docs/licenses/license-information/other-licenses/services-licenses": [
    {
      "sections": [
        "New Relic data usage limits and policies",
        "View limits and manage data"
      ],
      "title": "New Relic data usage limits and policies",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "fc32c25b40a030ffa0fad6bfc95be7fca1360ee1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies/",
      "published_at": "2021-10-19T04:02:17Z",
      "updated_at": "2021-10-19T04:02:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers' accounts, we have various data volume and rate limits in place. We reserve the right to enforce these limits to protect our system and to avoid issues for you and other customers. If your New Relic account, whether by configuration or by error, exceeds one of these limits, it or its child accounts might experience one or both of the following: Sampling of data Temporary pause or cessation of data collection To learn more about how hitting a limit can affect your data, see View limits. If you have further questions about these limits, your contract, or a limit you've reached, contact your New Relic account representative. We can work with you to adjust any rate limits to meet your needs. View limits and manage data For information about system and account limits, and for links to data ingest API limits, go to View limits. To manage your data ingest, storage, and limits for organization or billing purposes, go to Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 213.52281,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " limits and manage data For <em>information</em> about system and account limits, and for links to data ingest API limits, go to View limits. To manage your data ingest, storage, and limits for organization or billing purposes, go to Manage data."
      },
      "id": "603eb1c528ccbc0311eba7c7"
    },
    {
      "sections": [
        "Add-on end user license agreement",
        "New Relic, Inc. Add-on End User License Agreement"
      ],
      "title": "Add-on end user license agreement",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "a01c225ca30f95dab7db856cd946c76de557c31f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/add-end-user-license-agreement/",
      "published_at": "2021-10-18T15:29:13Z",
      "updated_at": "2021-03-16T06:19:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic, Inc. Add-on End User License Agreement In connection with the work provided by New Relic's Expert Services, Field Instrumentation Team, or Sales team, you may be provided with certain custom-created software to visualize, enable, optimize, or enhance your use of New Relic's Services. By downloading, installing, authorizing installation, or using the Add-on (defined below) with your Underlying Software (defined below), you (“Customer”) agree to the terms and conditions herein (“Agreement”) with New Relic, Inc., a Delaware corporation with offices located at 188 Spear Street, Suite 1200 San Francisco, CA 94105 (“New Relic\"), (collectively the “Parties”). Capitalized terms not defined herein shall have the meanings set forth in the New Relic Terms of Service (the “Terms of Service”) available here: newrelic.com/termsandconditions/terms. IF YOU DO NOT AGREE TO THIS AGREEMENT, YOUR SOLE REMEDY IS TO NOT USE THE ADD-ON. 1. DEFINITIONS “Add-on” means the New Relic applications and/or software, including but not limited to connectors, extensions, UI extension, and plugins, provided by New Relic to Customer to enable the Services to operate with certain third party or Customer software or systems (“Underlying Software”). For the purposes of the Terms of Service, the Add-on shall be treated like an Agent, subject to the separate terms herein. 2. USE OF THE PROGRAMMABILITY ADD-ON 2.1 Software Evaluation License. Subject to the terms herein, New Relic grants to Customer a limited, non-exclusive, non-transferrable, non-sublicensable right to install, use, and configure the Add-on solely as needed to enable the Services for internal evaluation purposes. New Relic reserves all rights and licenses not expressly granted herein. 2.2 Ownership. As between the Parties, New Relic owns all right, title, and interest to the Add-on and Feedback, including but not limited to any intellectual property and proprietary rights therein. Customer retains all right, title, and interest in any Customer Data processed by the Add-on in connection with the Services. 2.3 Feedback. Customer agrees to provide feedback, suggestions, ideas, requests or recommendations (“Feedback”) regarding the Add-on, and hereby irrevocably assigns all intellectual property and proprietary rights it holds in the Feedback to New Relic. 2.4 Notice. Customer acknowledges and agrees that the Add-on is made to work with the Underlying Software as configured at the time of creation and there is no guarantee that Add-on will continue to work in the event Customer changes, replaces, upgrades versions of, updates, or otherwise changes the Underlying Software (an “Upgrade Event”). Customer shall hold New Relic harmless from any claims or damages arising from Customer’s Upgrade Event. 2.5 Restrictions. Customer will not: (i) use the Add-on except as permitted hereunder; (ii) distribute, sell, sublicense, or otherwise transfer the Add-on; (iii) decompile, disassemble or reverse engineer any software underlying the Add-on; (iv) use the Add-on to damage, disable, overburden or impair any New Relic server or network(s) connected to any New Relic server or interfere with any other party’s use and enjoyment of the Services; (v) use the Add-on to defraud, defame, abuse, harass, stalk, threaten or infringe the rights of privacy or other intellectual property rights of others or otherwise violate any applicable law; (vi) circumvent or modify any security technologies included as part of the Add-on; or (vii) attempt or permit any third party to do any of the foregoing. New Relic may suspend Customer usage of the Services, without notice, pending any investigation of misuse. These restrictions may be supplemented or superceded (to the extent they conflict) by the New Relic Acceptable Use Policy as may be published and updated from time to time on the New Relic web site at docs.newrelic.com/docs/licenses/license-information/acceptable-use-policy/acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential Information” means: (a) the Programmability Add-on, and any features, results or output produced by, and other information relating to the Add-on (including, without limitation, all Feedback); and (b) any business or technical information of New Relic including but not limited to any technical information, research, development, know-how that a reasonable person would understand to be confidential. 3.2 Restrictions. Customer will not use or disclose any Confidential Information, except as necessary for the performance of this Agreement. Customer will use all reasonable efforts to protect Confidential Information from unauthorized use or disclosure, but in no event less than the efforts that it ordinarily uses with respect to its own proprietary information. Customer may disclose Confidential Information to those of its employees who have a bona fide need to know such Confidential Information for the performance of this Agreement; provided that each such employee first executes a written agreement that contains use and nondisclosure restrictions at least as protective as those set forth herein. Confidential Information shall not include any information that: (a) is or becomes generally known to the public through no fault or breach of this Agreement by Customer; (b) is rightfully known by Customer at the time of disclosure without an obligation of confidentiality; (c) is independently developed by Customer without access or use of any Confidential Information; or (d) is rightfully obtained from a third party without restriction on use or disclosure. 4. DISCLAIMER 4.1 DISCLAIMER. THE ADD-ON IS PROVIDED AS-IS AND AS-AVAILABLE AND NEW RELIC DISCLAIMS AND MAKES NO WARRANTIES, EITHER EXPRESS OR IMPLIED, INCLUDING ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR NON-INFRINGEMENT AND WHETHER OR NOT ARISING THROUGH A COURSE OF DEALING. THE ADD-ON IS NOT GUARANTEED TO BE ERROR-FREE, COMPATIBLE WITH THE UNDERLYING SOFTWARE, OR THAT CUSTOMER WILL ACHIEVE ANY RESULTS FROM USE OF THE ADD-ON THEREFROM. 4.2 LIMITATION OF LIABILITY. TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC, ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE TO CUSTOMER OR ANY THIRD PARTY FOR DAMAGES OF ANY KIND, INCLUDING, WITHOUT LIMITATION, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE INSTALLATION, USE OR INABILITY TO USE THE ADD-ON OR FOR ANY ERROR OR DEFECT IN THE ADD-ON OR THE SERVICES, REGARDLESS OF LEGAL THEORY, WHETHER OR NOT NEW RELIC HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSS OR DAMAGE. NEW RELIC’S AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS AGREEMENT WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE PARTIES HAVE AGREED THAT THESE LIMITATIONS WILL SURVIVE AND APPLY EVEN IF ANY LIMITED REMEDY SPECIFIED IN THIS AGREEMENT IS FOUND TO HAVE FAILED OF ITS ESSENTIAL PURPOSE. THE LIMITATIONS OF DAMAGES SET FORTH ABOVE ARE FUNDAMENTAL ELEMENTS OF THE BASIS OF THE BARGAIN BETWEEN NEW RELIC AND CUSTOMER. 4.3 EVALUATION VERSION. Customer agrees and acknowledges that: (a) the Add-on is not an official product and has not been commercially released for sale by New Relic; (b) the Add-on may not operate properly, being in final form, or fully functional; (c) the Add-on may contain errors, security vulnerabilities, design flaws, or other problems; (d) it may not be possible to make the Add-on fully functional; (e) the information obtained using the Add-on may not be accurate; (f) use of the Add-on may result in unexpected results, loss of data, delays or other unpredictable damages or loss; (g) New Relic is under no obligation to release a commercial version of the Add-on; and (h) New Relic has the right unilaterally to abandon development of the Add-on, at any time and without any obligation or liability to Customer. 5. GENERAL PROVISIONS 5.1 Terms of Service; Documentation. This Agreement shall be considered a part of the New Relic documentation, located at: https://docs.newrelic.com (the “Documentation). This Agreement and the Terms of Service constitute the entire and exclusive agreement between New Relic and Customer with respect to the Add-on. In the absence of a separate agreement, upon agreement between New Relic and Customer with respect to the Add-On, the terms and conditions of this Agreement shall govern the relationship between New Relic and Customer with respect to such Add-On. To the extent of a conflict between the Agreement and the Terms of Service, this Agreement shall govern with respect to the Add-on only.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.69788,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Add-on end user <em>license</em> agreement",
        "sections": "Add-on end user <em>license</em> agreement",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": "&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;acceptable-use-policy&#x2F;acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential <em>Information</em>” means: (a) the Programmability Add-on, and any features, results or output produced by, and other <em>information</em> relating to the Add-on (including, without limitation"
      },
      "id": "603ec23328ccbccf1beba79a"
    },
    {
      "sections": [
        "FIT instrumentation end user license agreement"
      ],
      "title": "FIT instrumentation end user license agreement",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "e8e10baf120678407d08c9f78ab708d271cf0223",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/fit-instrumentation-end-user-license-agreement/",
      "published_at": "2021-10-18T15:30:04Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "NEW RELIC, INC. FIT INSTRUMENTATION END USER LICENSE AGREEMENT In connection with the work provided by New Relic’s Field Instrumentation Team, you may be provided with certain custom-created software to enable, optimize, or enhance your use of New Relic’s Services. By downloading, installing, authorizing installation, or using the FIT Instrumentation with your Underlying Software (defined below), you (“Customer”) agree to the terms and conditions herein (“Agreement”) with New Relic, Inc., a Delaware corporation with offices located at 188 Spear Street, Suite 1200 San Francisco, CA 94105 (“New Relic”), (collectively the “Parties”). Capitalized terms not defined herein shall have the meanings set forth in the New Relic Terms of Service (the “Terms of Service”) available here: http://newrelic.com/terms IF YOU DO NOT AGREE TO THIS AGREEMENT, PLEASE DO NOT USE THE FIT INSTRUMENTATION. 1. DEFINITIONS “FIT Instrumentation” means the New Relic custom-made software, including but not limited to connectors, extensions, and plugins, provided by New Relic to Customer to enable the Services to operate with certain third party or Customer software or systems (“Underlying Software”). For the purposes of the Terms of Service, the FIT Instrumentation shall be treated like an “Agent”, subject to the separate terms herein. 2. USE OF THE FIT INSTRUMENTATION 2.1 Software Evaluation License. Subject to the terms herein, New Relic grants to Customer a limited, non-exclusive, non-transferrable, non-sublicensable right to install, use, and configure the FIT Instrumentation solely as needed to enable the Services for internal evaluation purposes. New Relic reserves all rights and licenses not expressly granted herein. 2.2 Ownership. As between the Parties, New Relic owns all right, title, and interest to the FIT Instrumentation and Feedback, including but not limited to any intellectual property and proprietary rights therein. Customer retains all right, title, and interest in any Customer Data processed by the FIT Instrumentation in connection with the Services. 2.3 Feedback. Customer agrees to provide feedback, suggestions, ideas, requests or recommendations (“Feedback”) regarding the FIT Instrumentation, and hereby irrevocably assigns all intellectual property and proprietary rights it holds in the Feedback to New Relic. 2.4 Notice. Customer acknowledges and agrees that the FIT Instrumentation is made to work with the Underlying Software as configured at the time of creation and there is no guarantee that FIT Instrumentation will continue to work in the event Customer changes, replaces, upgrades versions of, updates, or otherwise changes the Underlying Software (an “Upgrade Event”). Customer shall hold New Relic harmless from any claims or damages arising from Customer’s Upgrade Event. 2.5 Restrictions. Customer will not: (i) use the FIT Instrumentation except as permitted hereunder; (ii) distribute, sell, sublicense, or otherwise transfer the FIT Instrumentation; (iii) decompile, disassemble or reverse engineer any software underlying the FIT Instrumentation; (iv) use the FIT Instrumentation to damage, disable, overburden or impair any New Relic server or network(s) connected to any New Relic server or interfere with any other party’s use and enjoyment of the Services; (v) use the FIT Instrumentation to defraud, defame, abuse, harass, stalk, threaten or infringe the rights of privacy or other intellectual property rights of others or otherwise violate any applicable law; (vi) circumvent or modify any security technologies included as part of the FIT Instrumentation; or (vii) attempt or permit any third party to do any of the foregoing. New Relic may suspend usage of the Services by any user, without notice, pending any investigation of misuse.  These restrictions may be supplemented or superceded (to the extent they conflict) by the New Relic Acceptable Use Policy as may be published and updated from time to time at https://docs.newrelic.com/docs/licenses/license-information/acceptable-use-policy/acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential Information” means: (a) the FIT Instrumentation, and any features, results or output produced by, and other information relating to the FIT Instrumentation (including, without limitation, all Feedback); and (b) any business or technical information of New Relic including but not limited to any technical information, research, development, know-how that a reasonable person would understand to be confidential. 3.2 Restrictions. Customer will not use or disclose any Confidential Information, except as necessary for the performance of this Agreement. Customer will use all reasonable efforts to protect Confidential Information from unauthorized use or disclosure, but in no event less than the efforts that it ordinarily uses with respect to its own proprietary information. Customer may disclose Confidential Information to those of its employees who have a bona fide need to know such Confidential Information for the performance of this Agreement; provided that each such employee first executes a written agreement that contains use and nondisclosure restrictions at least as protective as those set forth herein. Confidential Information shall not include any information that: (a) is or becomes generally known to the public through no fault or breach of this Agreement by Customer; (b) is rightfully known by Customer at the time of disclosure without an obligation of confidentiality; (c) is independently developed by Customer without access or use of any Confidential Information; or (d) is rightfully obtained from a third party without restriction on use or disclosure. 4. DISCLAIMER 4.1 DISCLAIMER. FIT INSTRUMENTATION IS PROVIDED AS-IS AND AS-AVAILABLE AND NEW RELIC DISCLAIMS AND MAKES NO WARRANTIES, EITHER EXPRESS OR IMPLIED, INCLUDING ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR NON-INFRINGEMENT AND WHETHER OR NOT ARISING THROUGH A COURSE OF DEALING. FIT INSTRUMENTATION IS NOT GUARANTEED TO BE ERROR-FREE, COMPATIBLE WITH THE UNDERLYING SOFTWARE. , OR THAT CUSTOMER WILL ACHIEVE ANY RESULTS FROM USE OF THE FIT INSTRUMENTATION THEREFROM. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OF IMPLIED WARRANTIES OR LIMITATIONS ON APPLICABLE STATUTORY RIGHTS OF A CONSUMER, SO THE ABOVE EXCLUSION AND LIMITATIONS MAY NOT APPLY TO THE CUSTOMER. 4.2 LIMITATION OF LIABILITY. TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE TO CUSTOMER OR ANY THIRD PARTY FOR DAMAGES OF ANY KIND, INCLUDING, WITHOUT LIMITATION, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE INSTALLATION, USE OR INABILITY TO USE THE FIT INSTRUMENTATION OR FOR ANY ERROR OR DEFECT IN THE FIT INSTRUMENTATION OR THE SERVICES, WHETHER SUCH LIABILITY ARISES FROM ANY CLAIM BASED UPON CONTRACT, WARRANTY, TORT, STRICT LIABILITY, OR OTHERWISE, WHETHER OR NOT NEW RELIC HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSS OR DAMAGE. NEW RELIC'S AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS AGREEMENT WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE PARTIES HAVE AGREED THAT THESE LIMITATIONS WILL SURVIVE AND APPLY EVEN IF ANY LIMITED REMEDY SPECIFIED IN THIS AGREEMENT IS FOUND TO HAVE FAILED OF ITS ESSENTIAL PURPOSE. 4.3 EVALUATION VERSION. Customer agrees and acknowledges that: (a) the FIT Instrumentation is not an official product and has not been commercially released for sale by New Relic; (b) the FIT Instrumentation may not operate properly, being in final form, or fully functional; (c) the FIT Instrumentation may contain errors, security vulnerabilities, design flaws, or other problems; (d) it may not be possible to make the FIT Instrumentation fully functional; (e) the information obtained using the FIT Instrumentation may not be accurate; (f) use of the FIT Instrumentation may result in unexpected results, loss of data, delays or other unpredictable damages or loss; (g) New Relic is under no obligation to release a commercial version of the FIT Instrumentation; and (h) New Relic has the right unilaterally to abandon development of the FIT Instrumentation, at any time and without any obligation or liability to Customer. 5. GENERAL PROVISIONS 5.1 Terms of Service; Documentation. This Agreement shall be considered a part of the New Relic documentation, located at: https://docs.newrelic.com (the \"Documentation\").This Agreement and the Terms of Service constitute the entire and exclusive agreement between New Relic and Customer with respect to the FIT Instrumentation. In the absence of a separate agreement between New Relic and Customer with respect to the FIT Instrumentation, the terms and conditions of this Agreement shall govern the relationship between New Relic and Customer with respect to such FIT Instrumentation.  To the extent of a conflict between the Agreement and the Terms of Service, this Agreement shall govern with respect to the FIT Instrumentation only.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.69484,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "FIT instrumentation end user <em>license</em> agreement",
        "sections": "FIT instrumentation end user <em>license</em> agreement",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": " Acceptable Use Policy as may be published and updated from time to time at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;acceptable-use-policy&#x2F;acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. “Confidential <em>Information</em>” means: (a) the FIT Instrumentation, and any features, results"
      },
      "id": "603e9f3ee7b9d206e32a0800"
    }
  ],
  "/docs/licenses/license-information/product-definitions/legacy-product-definitions": [
    {
      "sections": [
        "New Relic data usage limits and policies",
        "View limits and manage data"
      ],
      "title": "New Relic data usage limits and policies",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "fc32c25b40a030ffa0fad6bfc95be7fca1360ee1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies/",
      "published_at": "2021-10-19T04:02:17Z",
      "updated_at": "2021-10-19T04:02:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers' accounts, we have various data volume and rate limits in place. We reserve the right to enforce these limits to protect our system and to avoid issues for you and other customers. If your New Relic account, whether by configuration or by error, exceeds one of these limits, it or its child accounts might experience one or both of the following: Sampling of data Temporary pause or cessation of data collection To learn more about how hitting a limit can affect your data, see View limits. If you have further questions about these limits, your contract, or a limit you've reached, contact your New Relic account representative. We can work with you to adjust any rate limits to meet your needs. View limits and manage data For information about system and account limits, and for links to data ingest API limits, go to View limits. To manage your data ingest, storage, and limits for organization or billing purposes, go to Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 213.52267,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " limits and manage data For <em>information</em> about system and account limits, and for links to data ingest API limits, go to View limits. To manage your data ingest, storage, and limits for organization or billing purposes, go to Manage data."
      },
      "id": "603eb1c528ccbc0311eba7c7"
    },
    {
      "sections": [
        "New Relic One pricing: Definitions",
        "Account",
        "Commitment Term",
        "Customer Data",
        "Customer Properties",
        "Documentation",
        "GB Ingested",
        "Incident event",
        "Login Credentials",
        "Monthly Provisioned User",
        "Order",
        "Paid Terms of Service",
        "Product(s)",
        "Software",
        "Terms",
        "Third-Party Services",
        "Unpaid Terms of Service",
        "Usage Plan"
      ],
      "title": "New Relic One pricing: Definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "609575acd671fecf7899378157eabc57bc8d68e2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/new-relic-one-pricing-definitions/",
      "published_at": "2021-10-18T14:36:56Z",
      "updated_at": "2021-05-22T17:25:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our New Relic One pricing plan (for a glossary for our original pricing, see Original pricing definitions. Account Account refers to the online account or subaccounts that New Relic provides for customers to manage their use of the Products. Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, information, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer Properties Customer Properties means Customer’s websites, infrastructure, networks, mobile applications, or other systems, as well as Customer accounts on Third-Party Services. Documentation Documentation means the New Relic technical guides and documentation made available from the dedicated ‘Documentation’ page of the New Relic website. GB Ingested A GB Ingested is a measurement of the volume of metrics, events, logs, traces, or other telemetry data sent to or generated by the Products for the benefit of the Customer, including from the Software, the Customer Properties, or Third-Party Services. In this context, a GB is defined as 1 billion bytes. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Login Credentials Login Credentials means the username, email address, password, or other personal information that is provided by a Customer user in order to manage an Account. Monthly Provisioned User A Monthly Provisioned User is any user who can log into Customer’s Account(s) and access the New Relic One Product functionality as specified in an Order and the Documentation. In our public docs, this is referred to as a full user. Order Order means the purchasing order for access to the Service or related services that: (1) is either executed by the Parties or entered into by you via self-service, and references this Agreement, or (2) is entered into by you and a Channel Partner. Paid Terms of Service Paid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/paid. Product(s) Product(s) mean the purchase of the New Relic subscription products described in the applicable Order and any updates, corrections, bug fixes, modifications, improvements, related services, new features, and functionality (made generally available to New Relic’s customer base) thereto. Software Software means the distributed software, APIs, scripts, or other code proprietary to New Relic provided with the Products. Terms Terms means the underlying Customer-New Relic agreement and the Order. Third-Party Services Third-Party Services means any third party platform, add-on, service, or product not provided by New Relic and that a user integrates or enables for use with the Products, including third-party applications and plug-ins. Unpaid Terms of Service Unpaid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/unpaid. Usage Plan Usage Plan refers to the Service or Product pricing, invoicing related information, and product-specific terms (e.g. concurrent user account sessions) contained within the Documentation. To learn more about this pricing plan, see New Relic One pricing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 195.00294,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic One pricing: <em>Definitions</em>",
        "sections": "New Relic One pricing: <em>Definitions</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " pricing, invoicing related <em>information</em>, and <em>product</em>-specific terms (e.g. concurrent user account sessions) contained within the Documentation. To learn more about this pricing plan, see New Relic One pricing."
      },
      "id": "6044e6e528ccbc26f22c6084"
    },
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Purpose",
        "2. Security Overview",
        "3.Security Certifications",
        "4. Data Control and Encryption",
        "5. Facilities",
        "6. Employee Access, Screening and Controls.",
        "7. Security Incident and Data Breach Response",
        "8. Network and Systems Security",
        "9. Authentication and Access Management",
        "10. Vulnerability Management",
        "11. System Access and Logging",
        "12. Disaster Recovery and Data Backup",
        "13. Copies and Removal",
        "14. Third Party Vendor Management",
        "15. Disclosure by Law",
        "16. Updates"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2021-10-18T14:36:56Z",
      "updated_at": "2021-09-27T15:05:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Updated 23 September 2021. The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Purpose This Policy describes New Relic's security program and the technical and organizational security controls to protect New Relic’s systems as shown in New Relic’s third-party audits and certifications. New Relic may update this Policy from time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/. 2. Security Overview 2.1. New Relic maintains a comprehensive Information Protection Program to manage information security within New Relic that includes administrative, technical, and physical safeguards designed to protect the confidentiality, integrity, and availability of Customer Data and that is appropriate to the nature, size, and complexity of New Relic's business operations. New Relic’s Information Protection Program includes: 2.1.1. executive review, support and accountability for all security related policies and practices; 2.1.2. formal written policies and procedures that are designed to protect against the loss, theft, or other unauthorized access or alteration of Customer Data, meet or exceed applicable industry standards, outlines a definition of information security and its overall objectives, include defined information security roles and responsibilities, a framework for setting control objectives and controls, a formal and effective risk mitigation program and a service provider security management program; 2.1.3. periodic risk assessments, including internal audits and/or independent 3rd party audits, to measure the effectiveness and appropriateness of controls for all systems processing Customer Data; 2.1.4. in-depth review of security incidents, including effective determination of root cause and corrective action; formal, industry-recognized controls frameworks based on an external audit standard; 2.1.5. employee screening and access management; and 2.1.6. comprehensive security testing, vulnerability identification, and mitigation methodology that consists of a variety of independent approaches that, when combined, are designed to maximize coverage for a diverse set of attack vectors. 2.2. New Relic's Chief Information Security Officer (CISO) leads New Relic's Information Security Program and develops, reviews, and approves, with appropriate stakeholders, New Relic's security policies and procedures. New Relic has appointed data protection officer(s) as described in New Relic’s Privacy Notices who are consulted as necessary under applicable laws. 2.3. Information security objectives, approach, scope, importance, goals, and principles for the organization’s security program are formally identified, communicated throughout the organization to users in a form that is relevant, accessible, and understandable to the intended reader; and supported by a controls framework that considers legislative, regulatory, contractual requirements, and other policy-related requirements. 2.4. New Relic will maintain written policies and procedures to review, test, and approve (as appropriate) changes affecting New Relic infrastructure and systems that process Customer Data including, but not limited to, peer reviews prior to introducing new code into production. New Relic will establish acceptance criteria for new information systems, upgrades, and new versions and will carry out suitable tests of the system(s) during development and prior to acceptance. Any changes or updates will not materially decrease the security of the systems. 3.Security Certifications 3.1. New Relic regularly tests, assesses, and evaluates its security measures for protecting Customer Data using industry-recognized standards and uses independent third-party auditors to verify such controls. 3.2. New Relic agrees to provide Customer, upon request, with applicable certifications or reports about New Relic systems. All information exchanged in connection with the audit activities described in this section is deemed to be the Confidential Information of New Relic. 3.3. Additional information about New Relic’s security certifications are available on New Relic’s Security Guide. 4. Data Control and Encryption 4.1. The Service and related features are designed to provide Customer with control over Customer’s data sources and Customer’s environments that are monitored and sending data to New Relic. 4.2. New Relic will have established methods to: (i) enable Customer to use encryption on Customer Data in transit, and (ii) securely hash passwords in storage following industry standard practices (e.g. scrypt) as described in the Documentation. Server certificate-based authentication is used as part of the TLS encryption with a trusted certificate authority. 4.3. New Relic receives and processes data in accordance with the Agreement and the Services as described in the Documentation. New Relic permits customers to delete personal data in accordance with applicable privacy laws as further described in the Documentation. In the event of error in personal data sent in Customer Data Customers may request personal data deletion and re-send data that is accurate. 4.4. Additional information for Customer data control and encryption, including encryption of data at rest, is available on New Relic’s Security Guide. 5. Facilities 5.1. All physical locations that process Customer Data are co-locations and third party data centers. All physical locations have security measures in place that are designed to prevent unauthorized physical access to data processing facilities and unlawful access, modification, or destruction of Customer Data. 5.2. New Relic will seek assurances (e.g., in the form of an independent 3rd party audit report such as the SOC 2 Type 2, ISO 27001, and vendor security evaluations) from its data processing facilities vendors that store or process Customer Data: 5.2.1. secure its data process facilities in an access-controlled location and have protections in place to prevent unauthorized access, damage, and interference; 5.2.2 employ physical security appropriate to the classification of the assets and information being managed which may include card key access, security cameras, and solid wall construction for all exterior walls; 5.2.3 limit and screen all entrants, which may include measures such as on-site security guard, badge reader, electronic lock, or a monitored closed caption television (CCTV); and 5.2.4 maintain relevant access logs. 5.3. Additional information about New Relic’s third party data centers are available on New Relic’s Security Guide. 6. Employee Access, Screening and Controls. New Relic will have and maintain policies and practices that include, at a minimum, the following controls and safeguards applied to New Relic employees and contractors who may access Customer Data: 6.1. For U.S. New Relic employees and subject to applicable law, a criminal background screening of each of its new employees to whom it gives access to Customer Data at the federal, state, and county levels. For non-U.S. New Relic employees, New Relic will use commercially reasonable efforts to meet the same criteria as established for U.S.-based New Relic employees, subject to general business practices in the respective country and in compliance with applicable local law requirements; 6.2. All New Relic employees with access to Customer Data will undergo adequate training, such as annual security awareness training, in the care, protection and handling of Customer Data, and will align with the privacy and security measures set out in this Addendum by following the guidance provided in their welcome package, which includes New Relic’s security policies and a security acknowledgement; 6.3. A disciplinary policy and process, to be used when New Relic employees violate New Relic security or privacy policies or access Customer Data without prior authorization; 6.4. Administrative or remote access to New Relic systems that process Customer Data will align with industry standard practices, including multi-factor authentication; 6.5. Restricted access to New Relic proprietary source code to prevent unauthorized access; 6.6. Controls designed to ensure that only those New Relic employees with an actual need-to-know will have access to New Relic systems including, but not limited to, the use of a formal access management process for the request, review, approval, and provisioning; 6.7. Controls designed to ensure that New Relic employees are granted access to New Relic systems based on least-privilege principles; and 6.8. Revoke access to New Relic employees no longer requiring access. 7. Security Incident and Data Breach Response 7.1. New Relic will take appropriate physical, technical, and administrative security measures that are commercially reasonable and consistent with industry standards to prevent a Data Breach, and as required by any applicable law or regulation. “Data Breach” means the theft, loss, or unauthorized access of Customer Data. Without limiting the foregoing, New Relic will implement security measures at least as stringent as those set out in this Addendum. New Relic will designate a senior representative to provide incident briefings, as needed in case of a Data Breach, and to respond to reasonable requests by Customer pertaining to privacy and data security issues within a commercially reasonable time frame. 7.2. New Relic will: 7.2.1. Notify Customer without undue delay if New Relic becomes aware of a Data Breach; 7.2.2. Maintain a security incident response plan, including procedures and means to respond in a manner consistent with industry standards; 7.2.3. Reasonably cooperate with Customer to investigate and remediate a Data Breach and mitigate any further risk to the Customer Data, or risk to data subjects and/or Customer’s reputation or brand; 7.2.4. Provide reasonable assistance to Customer at New Relic’s sole cost and expense; and 7.2.5. Make commercially reasonable efforts to preserve evidence and reasonably cooperate with Customer and legal authorities (as applicable and legally permissible) during an investigation of a Data Breach. 8. Network and Systems Security 8.1. All extranet connectivity by New Relic personnel to systems processing Customer Data will be through secure remote connections. 8.2. Network segments connected to the Internet will be protected by secure access control mechanisms, such as a firewall configured to secure all devices behind it and properly addresses security concerns according to industry standard practices. 8.3. New Relic will have industry standard measures in place to actively monitor its systems and help detect a potential hostile attack, such as Network Intrusion Detection (NID) or Host Intrusion Detection (HID)/Prevention. 8.4. Applications, ports, services, and similar access points installed on a computer or network facility, which are not specifically required for business functionality, will be disabled or removed. 8.5. New Relic maintains configuration standards for authorized operating systems and software for systems that support processing of Customer Data. New Relic will establish and follow server configuration guidelines and processes for preventing unauthorized access to Customer Data. New Relic maintains secure images or templates for systems based on the organization’s approved configuration standards. 8.6. Development, test, and operational environments will be logically separated to reduce the risks of unauthorized access or changes to the operational system. 8.7. New Relic network architecture will be designed to limit site access and restrict the availability of information systems that are considered to be vulnerable to attack. 8.8. New Relic provides appropriate TLS certificates when users access and view Customer Data in the Service. New Relic’s software for sending Customer Data to New Relic will encrypt Customer Data in transit by default. 9. Authentication and Access Management If you subscribe to the requisite New Relic One Service, New Relic will provide industry standard authentication and access controls to protect Customer Data, including industry standard authentication methods utilized to help prevent unauthorized access to the Service. New Relic’s access control methods will clearly state the rules and rights for each user or group of users including applications and information sharing and will include a process for granting and removing access to all information systems processing Customer Data. A record of all privileges allocated will be maintained pursuant to the requirements herein. 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. 10. Vulnerability Management New Relic will have and maintain the following vulnerability management processes for all devices used to connect to the New Relic network and Services. 10.1. New Relic will align to industry standard practices for build out, minimization of services and secure configuration, in accordance with, industry-recognized minimum security baselines for the New Relic platform and subcontractor systems connected to the New Relic platform in relation to the provision of the Services. 10.2. New Relic will employ industry-recognized standards and tools to conduct frequent infrastructure vulnerability scanning to test New Relic’s network and infrastructure and application penetration testing to test the New Relic Services. New Relic applies “Medium”, “High” and “Critical” security patches for all components in production and development environments as soon as commercially possible in accordance with its vulnerability management protocol, and consistent with industry standard practices and standards; 10.3. New Relic will have processes in place designed to ensure adherence to industry standard security development practices for development and testing for all code, APIs, and applications deployed and implemented in support of the Service; 10.4. New Relic will have and maintain solutions to identify and prevent malicious attackers or code from accessing or compromising Customer Data or systems that process Customer Data. These include, but are not limited to, software that identifies and removes malware and detects attempted intrusions. New Relic will have a security event and incident monitoring system and supporting processes to notify appropriate personnel in response to threats. 11. System Access and Logging 11.1. Access to New Relic’s systems will not be granted to employees of New Relic unless they have been uniquely identified and have sufficient credentials. 11.2. Access to New Relic’s systems will be logged and retained for no less than 6 months to assist in investigations and access control monitoring, including, but not limited to, end user access and activities, and information security events. 11.3. New Relic agrees to provide Customer the capability to access log records relating to Customer Accounts and the New Relic systems that process Customer Data in the event of a Data Breach or if required in connection with a law enforcement request. 12. Disaster Recovery and Data Backup 12.1. New Relic will have plans designed to respond to loss of services, which are tested and reviewed at least annually. This plan will include documented policies and procedures to restore service in the event of a service failure. 12.2. New Relic will establish and follow backup and restore procedures for Customer Data. 12.3. New Relic business continuity plan identifies critical systems. Annual disaster recovery tests are performed to check and restore customer data in the event of an incident. 12.4. New Relic will provide Customer with redacted copies of its plan(s) and evidence of tests/reviews upon request, but not more frequently than once annually, and subject to confidentiality requirements. 12.5. Additional information for Customer Data control and encryption are available on New Relic’s Security Guide. 13. Copies and Removal 13.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of the Agreement for any reason: (a) New Relic will, and will cause its personnel, to cease all access and use of any Customer Data, and (b) New Relic will delete all copies of Customer Data within ninety (90) days. 13.2. New Relic will maintain a process of ensuring secure destruction and deletion of all Customer Data, when reasonably requested by Customer or as otherwise provided in the Agreement. The process will include industry standard processes so that: (i) Customer Data cannot be practicably read or reconstructed, and (ii) the systems that store Customer Data are securely erased and/or decommissioned disks are destroyed. 14. Third Party Vendor Management New Relic may use third party vendors to provide the Services. New Relic performs a security risk-based assessment of prospective vendors before working with them to validate they meet New Relic’s security and business continuity standards, including the type of access and classification of data being accessed (if any), controls necessary to protect data, and legal/regulatory requirements. New Relic enters into written agreements with its vendors that process Customer Data which include confidentiality, privacy, and security obligations that provide an appropriate level of protection for Customer Data that these vendors may process for New Relic to maintain the security posture in this Policy, including following industry security standards. 15. Disclosure by Law In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic will (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. New Relic publishes its law enforcement requests report on New Relic’s Security Guide. 16. Updates As New Relic releases new products, services, functionality, and features, New Relic may update this Policy to account for such products, services, functionality, and features. For additional information, see our Security Guide.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 139.3138,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;referenced-policies&#x2F;security-policy&#x2F;. 2. Security Overview 2.1. New Relic"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    }
  ],
  "/docs/licenses/license-information/product-definitions/new-relic-one-pricing-definitions": [
    {
      "sections": [
        "Original product-based pricing definitions",
        "App",
        "App transaction",
        "AWS Lambda event",
        "Check",
        "Compute unit",
        "Datapoints per minute",
        "Event",
        "Host",
        "Incident event",
        "Page view",
        "Per GB daily",
        "Span",
        "User"
      ],
      "title": "Original product-based pricing definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "42087e53167736831855bf9a4c2967c465677b45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/legacy-product-definitions/",
      "published_at": "2021-10-18T14:36:06Z",
      "updated_at": "2021-09-14T07:27:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our original product-based pricing. For New Relic One pricing plan terms, see New Relic One pricing definitions. App An app means the application software designed to run on smartphones, tablet computers, and other mobile devices. Each app includes a predefined number of users. App transaction An app transaction is an APM application's attempt to process a web or non-web request. In New Relic APM these manifest as throughput TIMESERIES or individually as events in the Transaction event type. AWS Lambda event An AWS Lambda event means the row of data collected from the customer's AWS Lambda function by the New Relic agent or sent from an external service into the New Relic platform. It consists of the AwsLambdaInvocation, AwsLambdaInvocationError, or custom event types. Check A check means the single instance of a Synthetics monitor running in New Relic's monitoring network and reporting back response time, and whether the check was a success or failure. Compute unit A compute unit means the measure of resources associated with a unit of computation on a physical or virtual host. Datapoints per minute Datapoints per minute (DPM) refers to the per-minute rate at which individual metric values are sent to the New Relic Metric Ingest API. For billing purposes, datapoints per minute are calculated as a monthly average value by summing the datapoints ingested during a 30 day period and dividing by the number of minutes in that period (43,200). Event An event means the row of data collected from the customer's application by the New Relic agent or sent from an external service into our database. Host A host means the physical computer or virtual machine instance running a single copy of an operating system. Host usage is tracked monthly by summing the hours that every host in the account is connected to New Relic and dividing by 750. A host is counted if it is connected any time during an hour. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Page view A page view means the full page load (triggering an onLoad event) or a recorded URL change (state change). Per GB daily Per GB daily represents a daily average of Log data sent to New Relic over a 30 day period. Span A span represents an operation summary collected from the customer's application via the New Relic APM agent or New Relic Serverless for AWS Lambda agent, or sent from other tracing tools to the New Relic Trace API. User A user means the individual that connects to your app from a single device. Each unique device is considered as a unique user.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.88754,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Original <em>product</em>-based pricing <em>definitions</em>",
        "sections": "Original <em>product</em>-based pricing <em>definitions</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": "This is a glossary of terms that appear in contracts for our original <em>product</em>-based pricing. For New Relic One pricing plan terms, see New Relic One pricing <em>definitions</em>. App An app means the application software designed to run on smartphones, tablet computers, and other mobile devices. Each app"
      },
      "id": "603ebacc64441f77774e8872"
    },
    {
      "sections": [
        "New Relic data usage limits and policies",
        "View limits and manage data"
      ],
      "title": "New Relic data usage limits and policies",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "fc32c25b40a030ffa0fad6bfc95be7fca1360ee1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies/",
      "published_at": "2021-10-19T04:02:17Z",
      "updated_at": "2021-10-19T04:02:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers' accounts, we have various data volume and rate limits in place. We reserve the right to enforce these limits to protect our system and to avoid issues for you and other customers. If your New Relic account, whether by configuration or by error, exceeds one of these limits, it or its child accounts might experience one or both of the following: Sampling of data Temporary pause or cessation of data collection To learn more about how hitting a limit can affect your data, see View limits. If you have further questions about these limits, your contract, or a limit you've reached, contact your New Relic account representative. We can work with you to adjust any rate limits to meet your needs. View limits and manage data For information about system and account limits, and for links to data ingest API limits, go to View limits. To manage your data ingest, storage, and limits for organization or billing purposes, go to Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 213.52267,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " limits and manage data For <em>information</em> about system and account limits, and for links to data ingest API limits, go to View limits. To manage your data ingest, storage, and limits for organization or billing purposes, go to Manage data."
      },
      "id": "603eb1c528ccbc0311eba7c7"
    },
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Purpose",
        "2. Security Overview",
        "3.Security Certifications",
        "4. Data Control and Encryption",
        "5. Facilities",
        "6. Employee Access, Screening and Controls.",
        "7. Security Incident and Data Breach Response",
        "8. Network and Systems Security",
        "9. Authentication and Access Management",
        "10. Vulnerability Management",
        "11. System Access and Logging",
        "12. Disaster Recovery and Data Backup",
        "13. Copies and Removal",
        "14. Third Party Vendor Management",
        "15. Disclosure by Law",
        "16. Updates"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2021-10-18T14:36:56Z",
      "updated_at": "2021-09-27T15:05:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Updated 23 September 2021. The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Purpose This Policy describes New Relic's security program and the technical and organizational security controls to protect New Relic’s systems as shown in New Relic’s third-party audits and certifications. New Relic may update this Policy from time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/. 2. Security Overview 2.1. New Relic maintains a comprehensive Information Protection Program to manage information security within New Relic that includes administrative, technical, and physical safeguards designed to protect the confidentiality, integrity, and availability of Customer Data and that is appropriate to the nature, size, and complexity of New Relic's business operations. New Relic’s Information Protection Program includes: 2.1.1. executive review, support and accountability for all security related policies and practices; 2.1.2. formal written policies and procedures that are designed to protect against the loss, theft, or other unauthorized access or alteration of Customer Data, meet or exceed applicable industry standards, outlines a definition of information security and its overall objectives, include defined information security roles and responsibilities, a framework for setting control objectives and controls, a formal and effective risk mitigation program and a service provider security management program; 2.1.3. periodic risk assessments, including internal audits and/or independent 3rd party audits, to measure the effectiveness and appropriateness of controls for all systems processing Customer Data; 2.1.4. in-depth review of security incidents, including effective determination of root cause and corrective action; formal, industry-recognized controls frameworks based on an external audit standard; 2.1.5. employee screening and access management; and 2.1.6. comprehensive security testing, vulnerability identification, and mitigation methodology that consists of a variety of independent approaches that, when combined, are designed to maximize coverage for a diverse set of attack vectors. 2.2. New Relic's Chief Information Security Officer (CISO) leads New Relic's Information Security Program and develops, reviews, and approves, with appropriate stakeholders, New Relic's security policies and procedures. New Relic has appointed data protection officer(s) as described in New Relic’s Privacy Notices who are consulted as necessary under applicable laws. 2.3. Information security objectives, approach, scope, importance, goals, and principles for the organization’s security program are formally identified, communicated throughout the organization to users in a form that is relevant, accessible, and understandable to the intended reader; and supported by a controls framework that considers legislative, regulatory, contractual requirements, and other policy-related requirements. 2.4. New Relic will maintain written policies and procedures to review, test, and approve (as appropriate) changes affecting New Relic infrastructure and systems that process Customer Data including, but not limited to, peer reviews prior to introducing new code into production. New Relic will establish acceptance criteria for new information systems, upgrades, and new versions and will carry out suitable tests of the system(s) during development and prior to acceptance. Any changes or updates will not materially decrease the security of the systems. 3.Security Certifications 3.1. New Relic regularly tests, assesses, and evaluates its security measures for protecting Customer Data using industry-recognized standards and uses independent third-party auditors to verify such controls. 3.2. New Relic agrees to provide Customer, upon request, with applicable certifications or reports about New Relic systems. All information exchanged in connection with the audit activities described in this section is deemed to be the Confidential Information of New Relic. 3.3. Additional information about New Relic’s security certifications are available on New Relic’s Security Guide. 4. Data Control and Encryption 4.1. The Service and related features are designed to provide Customer with control over Customer’s data sources and Customer’s environments that are monitored and sending data to New Relic. 4.2. New Relic will have established methods to: (i) enable Customer to use encryption on Customer Data in transit, and (ii) securely hash passwords in storage following industry standard practices (e.g. scrypt) as described in the Documentation. Server certificate-based authentication is used as part of the TLS encryption with a trusted certificate authority. 4.3. New Relic receives and processes data in accordance with the Agreement and the Services as described in the Documentation. New Relic permits customers to delete personal data in accordance with applicable privacy laws as further described in the Documentation. In the event of error in personal data sent in Customer Data Customers may request personal data deletion and re-send data that is accurate. 4.4. Additional information for Customer data control and encryption, including encryption of data at rest, is available on New Relic’s Security Guide. 5. Facilities 5.1. All physical locations that process Customer Data are co-locations and third party data centers. All physical locations have security measures in place that are designed to prevent unauthorized physical access to data processing facilities and unlawful access, modification, or destruction of Customer Data. 5.2. New Relic will seek assurances (e.g., in the form of an independent 3rd party audit report such as the SOC 2 Type 2, ISO 27001, and vendor security evaluations) from its data processing facilities vendors that store or process Customer Data: 5.2.1. secure its data process facilities in an access-controlled location and have protections in place to prevent unauthorized access, damage, and interference; 5.2.2 employ physical security appropriate to the classification of the assets and information being managed which may include card key access, security cameras, and solid wall construction for all exterior walls; 5.2.3 limit and screen all entrants, which may include measures such as on-site security guard, badge reader, electronic lock, or a monitored closed caption television (CCTV); and 5.2.4 maintain relevant access logs. 5.3. Additional information about New Relic’s third party data centers are available on New Relic’s Security Guide. 6. Employee Access, Screening and Controls. New Relic will have and maintain policies and practices that include, at a minimum, the following controls and safeguards applied to New Relic employees and contractors who may access Customer Data: 6.1. For U.S. New Relic employees and subject to applicable law, a criminal background screening of each of its new employees to whom it gives access to Customer Data at the federal, state, and county levels. For non-U.S. New Relic employees, New Relic will use commercially reasonable efforts to meet the same criteria as established for U.S.-based New Relic employees, subject to general business practices in the respective country and in compliance with applicable local law requirements; 6.2. All New Relic employees with access to Customer Data will undergo adequate training, such as annual security awareness training, in the care, protection and handling of Customer Data, and will align with the privacy and security measures set out in this Addendum by following the guidance provided in their welcome package, which includes New Relic’s security policies and a security acknowledgement; 6.3. A disciplinary policy and process, to be used when New Relic employees violate New Relic security or privacy policies or access Customer Data without prior authorization; 6.4. Administrative or remote access to New Relic systems that process Customer Data will align with industry standard practices, including multi-factor authentication; 6.5. Restricted access to New Relic proprietary source code to prevent unauthorized access; 6.6. Controls designed to ensure that only those New Relic employees with an actual need-to-know will have access to New Relic systems including, but not limited to, the use of a formal access management process for the request, review, approval, and provisioning; 6.7. Controls designed to ensure that New Relic employees are granted access to New Relic systems based on least-privilege principles; and 6.8. Revoke access to New Relic employees no longer requiring access. 7. Security Incident and Data Breach Response 7.1. New Relic will take appropriate physical, technical, and administrative security measures that are commercially reasonable and consistent with industry standards to prevent a Data Breach, and as required by any applicable law or regulation. “Data Breach” means the theft, loss, or unauthorized access of Customer Data. Without limiting the foregoing, New Relic will implement security measures at least as stringent as those set out in this Addendum. New Relic will designate a senior representative to provide incident briefings, as needed in case of a Data Breach, and to respond to reasonable requests by Customer pertaining to privacy and data security issues within a commercially reasonable time frame. 7.2. New Relic will: 7.2.1. Notify Customer without undue delay if New Relic becomes aware of a Data Breach; 7.2.2. Maintain a security incident response plan, including procedures and means to respond in a manner consistent with industry standards; 7.2.3. Reasonably cooperate with Customer to investigate and remediate a Data Breach and mitigate any further risk to the Customer Data, or risk to data subjects and/or Customer’s reputation or brand; 7.2.4. Provide reasonable assistance to Customer at New Relic’s sole cost and expense; and 7.2.5. Make commercially reasonable efforts to preserve evidence and reasonably cooperate with Customer and legal authorities (as applicable and legally permissible) during an investigation of a Data Breach. 8. Network and Systems Security 8.1. All extranet connectivity by New Relic personnel to systems processing Customer Data will be through secure remote connections. 8.2. Network segments connected to the Internet will be protected by secure access control mechanisms, such as a firewall configured to secure all devices behind it and properly addresses security concerns according to industry standard practices. 8.3. New Relic will have industry standard measures in place to actively monitor its systems and help detect a potential hostile attack, such as Network Intrusion Detection (NID) or Host Intrusion Detection (HID)/Prevention. 8.4. Applications, ports, services, and similar access points installed on a computer or network facility, which are not specifically required for business functionality, will be disabled or removed. 8.5. New Relic maintains configuration standards for authorized operating systems and software for systems that support processing of Customer Data. New Relic will establish and follow server configuration guidelines and processes for preventing unauthorized access to Customer Data. New Relic maintains secure images or templates for systems based on the organization’s approved configuration standards. 8.6. Development, test, and operational environments will be logically separated to reduce the risks of unauthorized access or changes to the operational system. 8.7. New Relic network architecture will be designed to limit site access and restrict the availability of information systems that are considered to be vulnerable to attack. 8.8. New Relic provides appropriate TLS certificates when users access and view Customer Data in the Service. New Relic’s software for sending Customer Data to New Relic will encrypt Customer Data in transit by default. 9. Authentication and Access Management If you subscribe to the requisite New Relic One Service, New Relic will provide industry standard authentication and access controls to protect Customer Data, including industry standard authentication methods utilized to help prevent unauthorized access to the Service. New Relic’s access control methods will clearly state the rules and rights for each user or group of users including applications and information sharing and will include a process for granting and removing access to all information systems processing Customer Data. A record of all privileges allocated will be maintained pursuant to the requirements herein. 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. 10. Vulnerability Management New Relic will have and maintain the following vulnerability management processes for all devices used to connect to the New Relic network and Services. 10.1. New Relic will align to industry standard practices for build out, minimization of services and secure configuration, in accordance with, industry-recognized minimum security baselines for the New Relic platform and subcontractor systems connected to the New Relic platform in relation to the provision of the Services. 10.2. New Relic will employ industry-recognized standards and tools to conduct frequent infrastructure vulnerability scanning to test New Relic’s network and infrastructure and application penetration testing to test the New Relic Services. New Relic applies “Medium”, “High” and “Critical” security patches for all components in production and development environments as soon as commercially possible in accordance with its vulnerability management protocol, and consistent with industry standard practices and standards; 10.3. New Relic will have processes in place designed to ensure adherence to industry standard security development practices for development and testing for all code, APIs, and applications deployed and implemented in support of the Service; 10.4. New Relic will have and maintain solutions to identify and prevent malicious attackers or code from accessing or compromising Customer Data or systems that process Customer Data. These include, but are not limited to, software that identifies and removes malware and detects attempted intrusions. New Relic will have a security event and incident monitoring system and supporting processes to notify appropriate personnel in response to threats. 11. System Access and Logging 11.1. Access to New Relic’s systems will not be granted to employees of New Relic unless they have been uniquely identified and have sufficient credentials. 11.2. Access to New Relic’s systems will be logged and retained for no less than 6 months to assist in investigations and access control monitoring, including, but not limited to, end user access and activities, and information security events. 11.3. New Relic agrees to provide Customer the capability to access log records relating to Customer Accounts and the New Relic systems that process Customer Data in the event of a Data Breach or if required in connection with a law enforcement request. 12. Disaster Recovery and Data Backup 12.1. New Relic will have plans designed to respond to loss of services, which are tested and reviewed at least annually. This plan will include documented policies and procedures to restore service in the event of a service failure. 12.2. New Relic will establish and follow backup and restore procedures for Customer Data. 12.3. New Relic business continuity plan identifies critical systems. Annual disaster recovery tests are performed to check and restore customer data in the event of an incident. 12.4. New Relic will provide Customer with redacted copies of its plan(s) and evidence of tests/reviews upon request, but not more frequently than once annually, and subject to confidentiality requirements. 12.5. Additional information for Customer Data control and encryption are available on New Relic’s Security Guide. 13. Copies and Removal 13.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of the Agreement for any reason: (a) New Relic will, and will cause its personnel, to cease all access and use of any Customer Data, and (b) New Relic will delete all copies of Customer Data within ninety (90) days. 13.2. New Relic will maintain a process of ensuring secure destruction and deletion of all Customer Data, when reasonably requested by Customer or as otherwise provided in the Agreement. The process will include industry standard processes so that: (i) Customer Data cannot be practicably read or reconstructed, and (ii) the systems that store Customer Data are securely erased and/or decommissioned disks are destroyed. 14. Third Party Vendor Management New Relic may use third party vendors to provide the Services. New Relic performs a security risk-based assessment of prospective vendors before working with them to validate they meet New Relic’s security and business continuity standards, including the type of access and classification of data being accessed (if any), controls necessary to protect data, and legal/regulatory requirements. New Relic enters into written agreements with its vendors that process Customer Data which include confidentiality, privacy, and security obligations that provide an appropriate level of protection for Customer Data that these vendors may process for New Relic to maintain the security posture in this Policy, including following industry security standards. 15. Disclosure by Law In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic will (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. New Relic publishes its law enforcement requests report on New Relic’s Security Guide. 16. Updates As New Relic releases new products, services, functionality, and features, New Relic may update this Policy to account for such products, services, functionality, and features. For additional information, see our Security Guide.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 139.3138,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;referenced-policies&#x2F;security-policy&#x2F;. 2. Security Overview 2.1. New Relic"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    }
  ],
  "/docs/licenses/license-information/referenced-policies/new-relic-pre-release-policy": [
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Purpose",
        "2. Security Overview",
        "3.Security Certifications",
        "4. Data Control and Encryption",
        "5. Facilities",
        "6. Employee Access, Screening and Controls.",
        "7. Security Incident and Data Breach Response",
        "8. Network and Systems Security",
        "9. Authentication and Access Management",
        "10. Vulnerability Management",
        "11. System Access and Logging",
        "12. Disaster Recovery and Data Backup",
        "13. Copies and Removal",
        "14. Third Party Vendor Management",
        "15. Disclosure by Law",
        "16. Updates"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2021-10-18T14:36:56Z",
      "updated_at": "2021-09-27T15:05:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Updated 23 September 2021. The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Purpose This Policy describes New Relic's security program and the technical and organizational security controls to protect New Relic’s systems as shown in New Relic’s third-party audits and certifications. New Relic may update this Policy from time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/. 2. Security Overview 2.1. New Relic maintains a comprehensive Information Protection Program to manage information security within New Relic that includes administrative, technical, and physical safeguards designed to protect the confidentiality, integrity, and availability of Customer Data and that is appropriate to the nature, size, and complexity of New Relic's business operations. New Relic’s Information Protection Program includes: 2.1.1. executive review, support and accountability for all security related policies and practices; 2.1.2. formal written policies and procedures that are designed to protect against the loss, theft, or other unauthorized access or alteration of Customer Data, meet or exceed applicable industry standards, outlines a definition of information security and its overall objectives, include defined information security roles and responsibilities, a framework for setting control objectives and controls, a formal and effective risk mitigation program and a service provider security management program; 2.1.3. periodic risk assessments, including internal audits and/or independent 3rd party audits, to measure the effectiveness and appropriateness of controls for all systems processing Customer Data; 2.1.4. in-depth review of security incidents, including effective determination of root cause and corrective action; formal, industry-recognized controls frameworks based on an external audit standard; 2.1.5. employee screening and access management; and 2.1.6. comprehensive security testing, vulnerability identification, and mitigation methodology that consists of a variety of independent approaches that, when combined, are designed to maximize coverage for a diverse set of attack vectors. 2.2. New Relic's Chief Information Security Officer (CISO) leads New Relic's Information Security Program and develops, reviews, and approves, with appropriate stakeholders, New Relic's security policies and procedures. New Relic has appointed data protection officer(s) as described in New Relic’s Privacy Notices who are consulted as necessary under applicable laws. 2.3. Information security objectives, approach, scope, importance, goals, and principles for the organization’s security program are formally identified, communicated throughout the organization to users in a form that is relevant, accessible, and understandable to the intended reader; and supported by a controls framework that considers legislative, regulatory, contractual requirements, and other policy-related requirements. 2.4. New Relic will maintain written policies and procedures to review, test, and approve (as appropriate) changes affecting New Relic infrastructure and systems that process Customer Data including, but not limited to, peer reviews prior to introducing new code into production. New Relic will establish acceptance criteria for new information systems, upgrades, and new versions and will carry out suitable tests of the system(s) during development and prior to acceptance. Any changes or updates will not materially decrease the security of the systems. 3.Security Certifications 3.1. New Relic regularly tests, assesses, and evaluates its security measures for protecting Customer Data using industry-recognized standards and uses independent third-party auditors to verify such controls. 3.2. New Relic agrees to provide Customer, upon request, with applicable certifications or reports about New Relic systems. All information exchanged in connection with the audit activities described in this section is deemed to be the Confidential Information of New Relic. 3.3. Additional information about New Relic’s security certifications are available on New Relic’s Security Guide. 4. Data Control and Encryption 4.1. The Service and related features are designed to provide Customer with control over Customer’s data sources and Customer’s environments that are monitored and sending data to New Relic. 4.2. New Relic will have established methods to: (i) enable Customer to use encryption on Customer Data in transit, and (ii) securely hash passwords in storage following industry standard practices (e.g. scrypt) as described in the Documentation. Server certificate-based authentication is used as part of the TLS encryption with a trusted certificate authority. 4.3. New Relic receives and processes data in accordance with the Agreement and the Services as described in the Documentation. New Relic permits customers to delete personal data in accordance with applicable privacy laws as further described in the Documentation. In the event of error in personal data sent in Customer Data Customers may request personal data deletion and re-send data that is accurate. 4.4. Additional information for Customer data control and encryption, including encryption of data at rest, is available on New Relic’s Security Guide. 5. Facilities 5.1. All physical locations that process Customer Data are co-locations and third party data centers. All physical locations have security measures in place that are designed to prevent unauthorized physical access to data processing facilities and unlawful access, modification, or destruction of Customer Data. 5.2. New Relic will seek assurances (e.g., in the form of an independent 3rd party audit report such as the SOC 2 Type 2, ISO 27001, and vendor security evaluations) from its data processing facilities vendors that store or process Customer Data: 5.2.1. secure its data process facilities in an access-controlled location and have protections in place to prevent unauthorized access, damage, and interference; 5.2.2 employ physical security appropriate to the classification of the assets and information being managed which may include card key access, security cameras, and solid wall construction for all exterior walls; 5.2.3 limit and screen all entrants, which may include measures such as on-site security guard, badge reader, electronic lock, or a monitored closed caption television (CCTV); and 5.2.4 maintain relevant access logs. 5.3. Additional information about New Relic’s third party data centers are available on New Relic’s Security Guide. 6. Employee Access, Screening and Controls. New Relic will have and maintain policies and practices that include, at a minimum, the following controls and safeguards applied to New Relic employees and contractors who may access Customer Data: 6.1. For U.S. New Relic employees and subject to applicable law, a criminal background screening of each of its new employees to whom it gives access to Customer Data at the federal, state, and county levels. For non-U.S. New Relic employees, New Relic will use commercially reasonable efforts to meet the same criteria as established for U.S.-based New Relic employees, subject to general business practices in the respective country and in compliance with applicable local law requirements; 6.2. All New Relic employees with access to Customer Data will undergo adequate training, such as annual security awareness training, in the care, protection and handling of Customer Data, and will align with the privacy and security measures set out in this Addendum by following the guidance provided in their welcome package, which includes New Relic’s security policies and a security acknowledgement; 6.3. A disciplinary policy and process, to be used when New Relic employees violate New Relic security or privacy policies or access Customer Data without prior authorization; 6.4. Administrative or remote access to New Relic systems that process Customer Data will align with industry standard practices, including multi-factor authentication; 6.5. Restricted access to New Relic proprietary source code to prevent unauthorized access; 6.6. Controls designed to ensure that only those New Relic employees with an actual need-to-know will have access to New Relic systems including, but not limited to, the use of a formal access management process for the request, review, approval, and provisioning; 6.7. Controls designed to ensure that New Relic employees are granted access to New Relic systems based on least-privilege principles; and 6.8. Revoke access to New Relic employees no longer requiring access. 7. Security Incident and Data Breach Response 7.1. New Relic will take appropriate physical, technical, and administrative security measures that are commercially reasonable and consistent with industry standards to prevent a Data Breach, and as required by any applicable law or regulation. “Data Breach” means the theft, loss, or unauthorized access of Customer Data. Without limiting the foregoing, New Relic will implement security measures at least as stringent as those set out in this Addendum. New Relic will designate a senior representative to provide incident briefings, as needed in case of a Data Breach, and to respond to reasonable requests by Customer pertaining to privacy and data security issues within a commercially reasonable time frame. 7.2. New Relic will: 7.2.1. Notify Customer without undue delay if New Relic becomes aware of a Data Breach; 7.2.2. Maintain a security incident response plan, including procedures and means to respond in a manner consistent with industry standards; 7.2.3. Reasonably cooperate with Customer to investigate and remediate a Data Breach and mitigate any further risk to the Customer Data, or risk to data subjects and/or Customer’s reputation or brand; 7.2.4. Provide reasonable assistance to Customer at New Relic’s sole cost and expense; and 7.2.5. Make commercially reasonable efforts to preserve evidence and reasonably cooperate with Customer and legal authorities (as applicable and legally permissible) during an investigation of a Data Breach. 8. Network and Systems Security 8.1. All extranet connectivity by New Relic personnel to systems processing Customer Data will be through secure remote connections. 8.2. Network segments connected to the Internet will be protected by secure access control mechanisms, such as a firewall configured to secure all devices behind it and properly addresses security concerns according to industry standard practices. 8.3. New Relic will have industry standard measures in place to actively monitor its systems and help detect a potential hostile attack, such as Network Intrusion Detection (NID) or Host Intrusion Detection (HID)/Prevention. 8.4. Applications, ports, services, and similar access points installed on a computer or network facility, which are not specifically required for business functionality, will be disabled or removed. 8.5. New Relic maintains configuration standards for authorized operating systems and software for systems that support processing of Customer Data. New Relic will establish and follow server configuration guidelines and processes for preventing unauthorized access to Customer Data. New Relic maintains secure images or templates for systems based on the organization’s approved configuration standards. 8.6. Development, test, and operational environments will be logically separated to reduce the risks of unauthorized access or changes to the operational system. 8.7. New Relic network architecture will be designed to limit site access and restrict the availability of information systems that are considered to be vulnerable to attack. 8.8. New Relic provides appropriate TLS certificates when users access and view Customer Data in the Service. New Relic’s software for sending Customer Data to New Relic will encrypt Customer Data in transit by default. 9. Authentication and Access Management If you subscribe to the requisite New Relic One Service, New Relic will provide industry standard authentication and access controls to protect Customer Data, including industry standard authentication methods utilized to help prevent unauthorized access to the Service. New Relic’s access control methods will clearly state the rules and rights for each user or group of users including applications and information sharing and will include a process for granting and removing access to all information systems processing Customer Data. A record of all privileges allocated will be maintained pursuant to the requirements herein. 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. 10. Vulnerability Management New Relic will have and maintain the following vulnerability management processes for all devices used to connect to the New Relic network and Services. 10.1. New Relic will align to industry standard practices for build out, minimization of services and secure configuration, in accordance with, industry-recognized minimum security baselines for the New Relic platform and subcontractor systems connected to the New Relic platform in relation to the provision of the Services. 10.2. New Relic will employ industry-recognized standards and tools to conduct frequent infrastructure vulnerability scanning to test New Relic’s network and infrastructure and application penetration testing to test the New Relic Services. New Relic applies “Medium”, “High” and “Critical” security patches for all components in production and development environments as soon as commercially possible in accordance with its vulnerability management protocol, and consistent with industry standard practices and standards; 10.3. New Relic will have processes in place designed to ensure adherence to industry standard security development practices for development and testing for all code, APIs, and applications deployed and implemented in support of the Service; 10.4. New Relic will have and maintain solutions to identify and prevent malicious attackers or code from accessing or compromising Customer Data or systems that process Customer Data. These include, but are not limited to, software that identifies and removes malware and detects attempted intrusions. New Relic will have a security event and incident monitoring system and supporting processes to notify appropriate personnel in response to threats. 11. System Access and Logging 11.1. Access to New Relic’s systems will not be granted to employees of New Relic unless they have been uniquely identified and have sufficient credentials. 11.2. Access to New Relic’s systems will be logged and retained for no less than 6 months to assist in investigations and access control monitoring, including, but not limited to, end user access and activities, and information security events. 11.3. New Relic agrees to provide Customer the capability to access log records relating to Customer Accounts and the New Relic systems that process Customer Data in the event of a Data Breach or if required in connection with a law enforcement request. 12. Disaster Recovery and Data Backup 12.1. New Relic will have plans designed to respond to loss of services, which are tested and reviewed at least annually. This plan will include documented policies and procedures to restore service in the event of a service failure. 12.2. New Relic will establish and follow backup and restore procedures for Customer Data. 12.3. New Relic business continuity plan identifies critical systems. Annual disaster recovery tests are performed to check and restore customer data in the event of an incident. 12.4. New Relic will provide Customer with redacted copies of its plan(s) and evidence of tests/reviews upon request, but not more frequently than once annually, and subject to confidentiality requirements. 12.5. Additional information for Customer Data control and encryption are available on New Relic’s Security Guide. 13. Copies and Removal 13.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of the Agreement for any reason: (a) New Relic will, and will cause its personnel, to cease all access and use of any Customer Data, and (b) New Relic will delete all copies of Customer Data within ninety (90) days. 13.2. New Relic will maintain a process of ensuring secure destruction and deletion of all Customer Data, when reasonably requested by Customer or as otherwise provided in the Agreement. The process will include industry standard processes so that: (i) Customer Data cannot be practicably read or reconstructed, and (ii) the systems that store Customer Data are securely erased and/or decommissioned disks are destroyed. 14. Third Party Vendor Management New Relic may use third party vendors to provide the Services. New Relic performs a security risk-based assessment of prospective vendors before working with them to validate they meet New Relic’s security and business continuity standards, including the type of access and classification of data being accessed (if any), controls necessary to protect data, and legal/regulatory requirements. New Relic enters into written agreements with its vendors that process Customer Data which include confidentiality, privacy, and security obligations that provide an appropriate level of protection for Customer Data that these vendors may process for New Relic to maintain the security posture in this Policy, including following industry security standards. 15. Disclosure by Law In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic will (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. New Relic publishes its law enforcement requests report on New Relic’s Security Guide. 16. Updates As New Relic releases new products, services, functionality, and features, New Relic may update this Policy to account for such products, services, functionality, and features. For additional information, see our Security Guide.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.29898,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Security <em>policy</em>",
        "sections": "Security <em>policy</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;<em>referenced</em>-<em>policies</em>&#x2F;security-policy&#x2F;. 2. Security Overview 2.1. New Relic"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    },
    {
      "sections": [
        "Security guide",
        "Tip",
        "Security Program",
        "Security Domains",
        "Security Certifications",
        "Data Control, Facilities, and Encryption",
        "Law Enforcement Request Report"
      ],
      "title": "Security guide",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "356f0d11ffcb62208a743a0a7c127f5f6da9c940",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-guide/",
      "published_at": "2021-10-18T14:37:41Z",
      "updated_at": "2021-09-19T15:21:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Last updated September 17, 2021. This is supplement to our security policy and serves as a guide to New Relic’s description of its Services, functionalities, and features. Tip We may update the URLs in this document without notice. Security Program New Relic follows \"privacy by design\" principles as described here: https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Security Domains New Relic’s policies and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless Security; Configuration Management; Vulnerability Management; Network Protection; Transmission Protection; Password Management; Access Control, Audit Logging & Monitoring; Education, Training, and Awareness; Third Party Assurance; Incident Management; Business Continuity and Disaster Recover; Risk Management; Data Protection & Privacy; and Service Management Systems. Security Certifications New Relic audits its Services against industry standards as described at https://docs.newrelic.com/docs/security/security-privacy/compliance/regulatory-audits-new-relic-services/. Data Control, Facilities, and Encryption New Relic's customers can send data to New Relic's APIs by (1) using New Relic's software, (2) using vendor-neutral software that is managed and maintained by a third-party such as via OpenTelemetry instrumentation provided by opentelemetry.io, or (3) from third-party systems that customer's manage and/or control. New Relic's customers can use New Relic's Services such as NerdGraph to filter out and drop data. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/drop-data-using-nerdgraph/. New Relic's customers can adjust their data retention periods as appropriate for their needs. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/manage-data-retention/#adjust-retention. New Relic Logs obfuscates numbers that match known patterns, such as bank card and social security numbers as described here: https://docs.newrelic.com/docs/logs/log-management/get-started/new-relics-log-management-security-privacy/. New Relic honors requests to delete personal data in accordance with applicable privacy laws. Please see https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Customers may use New Relic's APIs to query data, such as NerdGraph described here, and New Relic Services to export the data to other cloud providers. Customers can configure its log forwarder [https://docs.newrelic.com/docs/logs/enable-log-management-new-relic/enable-log-monitoring-new-relic/forward-your-logs-using-infrastructure-agent/] before sending infrastructure logs to New Relic. For New Relic Customers in New Relic US, FedRAMP and HIPAA-enabled environments, Customer Data is replicated to the off-site backup system via Amazon Simple Storage Service (S3). Category of Customer Description FedRAMP HIPAA-enabled US Gen Pop EU Gen Pop Data is stored in Amazon Web Services (“AWS”). Limited Data is stored in IBM Data for New Relic Incident Intelligence is stored in Google Cloud New Relic regularly tests, assess, and evaluates its measures to ensure the security of processing using industry-recognized standards and uses independent third-party auditors as provided below: Annual SOC 2 Type 2 Annual FedRAMP assessment by an independent third-party pursuant to NIST 800-53 rev 4 Moderate authorization. Annual HITRUST-validated assessment by an independent third-party *Pursuing CY2021 Q4 ISO 27001 *Pursuing CY2021 TISAX *Pursuing CY2021 The Services that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed information about AWS security is available at https://aws.amazon.com/security/ and http://aws.amazon.com/security/sharing-the-security-responsibility/. Data encryption at rest utilizes FIPS 140-2 compliant encryption methodology. For AWS SOC Reports, please see https://aws.amazon.com/compliance/soc-faqs/. The Services that operate on Google Cloud Platform (\"GCP\") are protected by the security and environmental controls of GCP. Detailed information about GCP security is available at https://cloud.google.com/docs/tutorials#security. For GCP reports, please see https://cloud.google.com/security/compliance/. IBM Deft Zayo QTS Law Enforcement Request Report New Relic has not to date received any request for customer data from a law enforcement or other government agency (including under any national security process), and has not made any corresponding disclosures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 255.59146,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " as described here: https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;security&#x2F;security-privacy&#x2F;data-privacy&#x2F;data-privacy-new-relic&#x2F;. Security Domains New Relic’s <em>policies</em> and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless"
      },
      "id": "6147558128ccbc973a56a863"
    },
    {
      "sections": [
        "New Relic data usage limits and policies",
        "View limits and manage data"
      ],
      "title": "New Relic data usage limits and policies",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "fc32c25b40a030ffa0fad6bfc95be7fca1360ee1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies/",
      "published_at": "2021-10-19T04:02:17Z",
      "updated_at": "2021-10-19T04:02:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers' accounts, we have various data volume and rate limits in place. We reserve the right to enforce these limits to protect our system and to avoid issues for you and other customers. If your New Relic account, whether by configuration or by error, exceeds one of these limits, it or its child accounts might experience one or both of the following: Sampling of data Temporary pause or cessation of data collection To learn more about how hitting a limit can affect your data, see View limits. If you have further questions about these limits, your contract, or a limit you've reached, contact your New Relic account representative. We can work with you to adjust any rate limits to meet your needs. View limits and manage data For information about system and account limits, and for links to data ingest API limits, go to View limits. To manage your data ingest, storage, and limits for organization or billing purposes, go to Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 216.88243,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic data usage limits and <em>policies</em>",
        "sections": "New Relic data usage limits and <em>policies</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " limits and manage data For <em>information</em> about system and account limits, and for links to data ingest API limits, go to View limits. To manage your data ingest, storage, and limits for organization or billing purposes, go to Manage data."
      },
      "id": "603eb1c528ccbc0311eba7c7"
    }
  ],
  "/docs/licenses/license-information/referenced-policies/security-guide": [
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Purpose",
        "2. Security Overview",
        "3.Security Certifications",
        "4. Data Control and Encryption",
        "5. Facilities",
        "6. Employee Access, Screening and Controls.",
        "7. Security Incident and Data Breach Response",
        "8. Network and Systems Security",
        "9. Authentication and Access Management",
        "10. Vulnerability Management",
        "11. System Access and Logging",
        "12. Disaster Recovery and Data Backup",
        "13. Copies and Removal",
        "14. Third Party Vendor Management",
        "15. Disclosure by Law",
        "16. Updates"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2021-10-18T14:36:56Z",
      "updated_at": "2021-09-27T15:05:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Updated 23 September 2021. The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Purpose This Policy describes New Relic's security program and the technical and organizational security controls to protect New Relic’s systems as shown in New Relic’s third-party audits and certifications. New Relic may update this Policy from time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/. 2. Security Overview 2.1. New Relic maintains a comprehensive Information Protection Program to manage information security within New Relic that includes administrative, technical, and physical safeguards designed to protect the confidentiality, integrity, and availability of Customer Data and that is appropriate to the nature, size, and complexity of New Relic's business operations. New Relic’s Information Protection Program includes: 2.1.1. executive review, support and accountability for all security related policies and practices; 2.1.2. formal written policies and procedures that are designed to protect against the loss, theft, or other unauthorized access or alteration of Customer Data, meet or exceed applicable industry standards, outlines a definition of information security and its overall objectives, include defined information security roles and responsibilities, a framework for setting control objectives and controls, a formal and effective risk mitigation program and a service provider security management program; 2.1.3. periodic risk assessments, including internal audits and/or independent 3rd party audits, to measure the effectiveness and appropriateness of controls for all systems processing Customer Data; 2.1.4. in-depth review of security incidents, including effective determination of root cause and corrective action; formal, industry-recognized controls frameworks based on an external audit standard; 2.1.5. employee screening and access management; and 2.1.6. comprehensive security testing, vulnerability identification, and mitigation methodology that consists of a variety of independent approaches that, when combined, are designed to maximize coverage for a diverse set of attack vectors. 2.2. New Relic's Chief Information Security Officer (CISO) leads New Relic's Information Security Program and develops, reviews, and approves, with appropriate stakeholders, New Relic's security policies and procedures. New Relic has appointed data protection officer(s) as described in New Relic’s Privacy Notices who are consulted as necessary under applicable laws. 2.3. Information security objectives, approach, scope, importance, goals, and principles for the organization’s security program are formally identified, communicated throughout the organization to users in a form that is relevant, accessible, and understandable to the intended reader; and supported by a controls framework that considers legislative, regulatory, contractual requirements, and other policy-related requirements. 2.4. New Relic will maintain written policies and procedures to review, test, and approve (as appropriate) changes affecting New Relic infrastructure and systems that process Customer Data including, but not limited to, peer reviews prior to introducing new code into production. New Relic will establish acceptance criteria for new information systems, upgrades, and new versions and will carry out suitable tests of the system(s) during development and prior to acceptance. Any changes or updates will not materially decrease the security of the systems. 3.Security Certifications 3.1. New Relic regularly tests, assesses, and evaluates its security measures for protecting Customer Data using industry-recognized standards and uses independent third-party auditors to verify such controls. 3.2. New Relic agrees to provide Customer, upon request, with applicable certifications or reports about New Relic systems. All information exchanged in connection with the audit activities described in this section is deemed to be the Confidential Information of New Relic. 3.3. Additional information about New Relic’s security certifications are available on New Relic’s Security Guide. 4. Data Control and Encryption 4.1. The Service and related features are designed to provide Customer with control over Customer’s data sources and Customer’s environments that are monitored and sending data to New Relic. 4.2. New Relic will have established methods to: (i) enable Customer to use encryption on Customer Data in transit, and (ii) securely hash passwords in storage following industry standard practices (e.g. scrypt) as described in the Documentation. Server certificate-based authentication is used as part of the TLS encryption with a trusted certificate authority. 4.3. New Relic receives and processes data in accordance with the Agreement and the Services as described in the Documentation. New Relic permits customers to delete personal data in accordance with applicable privacy laws as further described in the Documentation. In the event of error in personal data sent in Customer Data Customers may request personal data deletion and re-send data that is accurate. 4.4. Additional information for Customer data control and encryption, including encryption of data at rest, is available on New Relic’s Security Guide. 5. Facilities 5.1. All physical locations that process Customer Data are co-locations and third party data centers. All physical locations have security measures in place that are designed to prevent unauthorized physical access to data processing facilities and unlawful access, modification, or destruction of Customer Data. 5.2. New Relic will seek assurances (e.g., in the form of an independent 3rd party audit report such as the SOC 2 Type 2, ISO 27001, and vendor security evaluations) from its data processing facilities vendors that store or process Customer Data: 5.2.1. secure its data process facilities in an access-controlled location and have protections in place to prevent unauthorized access, damage, and interference; 5.2.2 employ physical security appropriate to the classification of the assets and information being managed which may include card key access, security cameras, and solid wall construction for all exterior walls; 5.2.3 limit and screen all entrants, which may include measures such as on-site security guard, badge reader, electronic lock, or a monitored closed caption television (CCTV); and 5.2.4 maintain relevant access logs. 5.3. Additional information about New Relic’s third party data centers are available on New Relic’s Security Guide. 6. Employee Access, Screening and Controls. New Relic will have and maintain policies and practices that include, at a minimum, the following controls and safeguards applied to New Relic employees and contractors who may access Customer Data: 6.1. For U.S. New Relic employees and subject to applicable law, a criminal background screening of each of its new employees to whom it gives access to Customer Data at the federal, state, and county levels. For non-U.S. New Relic employees, New Relic will use commercially reasonable efforts to meet the same criteria as established for U.S.-based New Relic employees, subject to general business practices in the respective country and in compliance with applicable local law requirements; 6.2. All New Relic employees with access to Customer Data will undergo adequate training, such as annual security awareness training, in the care, protection and handling of Customer Data, and will align with the privacy and security measures set out in this Addendum by following the guidance provided in their welcome package, which includes New Relic’s security policies and a security acknowledgement; 6.3. A disciplinary policy and process, to be used when New Relic employees violate New Relic security or privacy policies or access Customer Data without prior authorization; 6.4. Administrative or remote access to New Relic systems that process Customer Data will align with industry standard practices, including multi-factor authentication; 6.5. Restricted access to New Relic proprietary source code to prevent unauthorized access; 6.6. Controls designed to ensure that only those New Relic employees with an actual need-to-know will have access to New Relic systems including, but not limited to, the use of a formal access management process for the request, review, approval, and provisioning; 6.7. Controls designed to ensure that New Relic employees are granted access to New Relic systems based on least-privilege principles; and 6.8. Revoke access to New Relic employees no longer requiring access. 7. Security Incident and Data Breach Response 7.1. New Relic will take appropriate physical, technical, and administrative security measures that are commercially reasonable and consistent with industry standards to prevent a Data Breach, and as required by any applicable law or regulation. “Data Breach” means the theft, loss, or unauthorized access of Customer Data. Without limiting the foregoing, New Relic will implement security measures at least as stringent as those set out in this Addendum. New Relic will designate a senior representative to provide incident briefings, as needed in case of a Data Breach, and to respond to reasonable requests by Customer pertaining to privacy and data security issues within a commercially reasonable time frame. 7.2. New Relic will: 7.2.1. Notify Customer without undue delay if New Relic becomes aware of a Data Breach; 7.2.2. Maintain a security incident response plan, including procedures and means to respond in a manner consistent with industry standards; 7.2.3. Reasonably cooperate with Customer to investigate and remediate a Data Breach and mitigate any further risk to the Customer Data, or risk to data subjects and/or Customer’s reputation or brand; 7.2.4. Provide reasonable assistance to Customer at New Relic’s sole cost and expense; and 7.2.5. Make commercially reasonable efforts to preserve evidence and reasonably cooperate with Customer and legal authorities (as applicable and legally permissible) during an investigation of a Data Breach. 8. Network and Systems Security 8.1. All extranet connectivity by New Relic personnel to systems processing Customer Data will be through secure remote connections. 8.2. Network segments connected to the Internet will be protected by secure access control mechanisms, such as a firewall configured to secure all devices behind it and properly addresses security concerns according to industry standard practices. 8.3. New Relic will have industry standard measures in place to actively monitor its systems and help detect a potential hostile attack, such as Network Intrusion Detection (NID) or Host Intrusion Detection (HID)/Prevention. 8.4. Applications, ports, services, and similar access points installed on a computer or network facility, which are not specifically required for business functionality, will be disabled or removed. 8.5. New Relic maintains configuration standards for authorized operating systems and software for systems that support processing of Customer Data. New Relic will establish and follow server configuration guidelines and processes for preventing unauthorized access to Customer Data. New Relic maintains secure images or templates for systems based on the organization’s approved configuration standards. 8.6. Development, test, and operational environments will be logically separated to reduce the risks of unauthorized access or changes to the operational system. 8.7. New Relic network architecture will be designed to limit site access and restrict the availability of information systems that are considered to be vulnerable to attack. 8.8. New Relic provides appropriate TLS certificates when users access and view Customer Data in the Service. New Relic’s software for sending Customer Data to New Relic will encrypt Customer Data in transit by default. 9. Authentication and Access Management If you subscribe to the requisite New Relic One Service, New Relic will provide industry standard authentication and access controls to protect Customer Data, including industry standard authentication methods utilized to help prevent unauthorized access to the Service. New Relic’s access control methods will clearly state the rules and rights for each user or group of users including applications and information sharing and will include a process for granting and removing access to all information systems processing Customer Data. A record of all privileges allocated will be maintained pursuant to the requirements herein. 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. 10. Vulnerability Management New Relic will have and maintain the following vulnerability management processes for all devices used to connect to the New Relic network and Services. 10.1. New Relic will align to industry standard practices for build out, minimization of services and secure configuration, in accordance with, industry-recognized minimum security baselines for the New Relic platform and subcontractor systems connected to the New Relic platform in relation to the provision of the Services. 10.2. New Relic will employ industry-recognized standards and tools to conduct frequent infrastructure vulnerability scanning to test New Relic’s network and infrastructure and application penetration testing to test the New Relic Services. New Relic applies “Medium”, “High” and “Critical” security patches for all components in production and development environments as soon as commercially possible in accordance with its vulnerability management protocol, and consistent with industry standard practices and standards; 10.3. New Relic will have processes in place designed to ensure adherence to industry standard security development practices for development and testing for all code, APIs, and applications deployed and implemented in support of the Service; 10.4. New Relic will have and maintain solutions to identify and prevent malicious attackers or code from accessing or compromising Customer Data or systems that process Customer Data. These include, but are not limited to, software that identifies and removes malware and detects attempted intrusions. New Relic will have a security event and incident monitoring system and supporting processes to notify appropriate personnel in response to threats. 11. System Access and Logging 11.1. Access to New Relic’s systems will not be granted to employees of New Relic unless they have been uniquely identified and have sufficient credentials. 11.2. Access to New Relic’s systems will be logged and retained for no less than 6 months to assist in investigations and access control monitoring, including, but not limited to, end user access and activities, and information security events. 11.3. New Relic agrees to provide Customer the capability to access log records relating to Customer Accounts and the New Relic systems that process Customer Data in the event of a Data Breach or if required in connection with a law enforcement request. 12. Disaster Recovery and Data Backup 12.1. New Relic will have plans designed to respond to loss of services, which are tested and reviewed at least annually. This plan will include documented policies and procedures to restore service in the event of a service failure. 12.2. New Relic will establish and follow backup and restore procedures for Customer Data. 12.3. New Relic business continuity plan identifies critical systems. Annual disaster recovery tests are performed to check and restore customer data in the event of an incident. 12.4. New Relic will provide Customer with redacted copies of its plan(s) and evidence of tests/reviews upon request, but not more frequently than once annually, and subject to confidentiality requirements. 12.5. Additional information for Customer Data control and encryption are available on New Relic’s Security Guide. 13. Copies and Removal 13.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of the Agreement for any reason: (a) New Relic will, and will cause its personnel, to cease all access and use of any Customer Data, and (b) New Relic will delete all copies of Customer Data within ninety (90) days. 13.2. New Relic will maintain a process of ensuring secure destruction and deletion of all Customer Data, when reasonably requested by Customer or as otherwise provided in the Agreement. The process will include industry standard processes so that: (i) Customer Data cannot be practicably read or reconstructed, and (ii) the systems that store Customer Data are securely erased and/or decommissioned disks are destroyed. 14. Third Party Vendor Management New Relic may use third party vendors to provide the Services. New Relic performs a security risk-based assessment of prospective vendors before working with them to validate they meet New Relic’s security and business continuity standards, including the type of access and classification of data being accessed (if any), controls necessary to protect data, and legal/regulatory requirements. New Relic enters into written agreements with its vendors that process Customer Data which include confidentiality, privacy, and security obligations that provide an appropriate level of protection for Customer Data that these vendors may process for New Relic to maintain the security posture in this Policy, including following industry security standards. 15. Disclosure by Law In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic will (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. New Relic publishes its law enforcement requests report on New Relic’s Security Guide. 16. Updates As New Relic releases new products, services, functionality, and features, New Relic may update this Policy to account for such products, services, functionality, and features. For additional information, see our Security Guide.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.29898,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Security <em>policy</em>",
        "sections": "Security <em>policy</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;<em>referenced</em>-<em>policies</em>&#x2F;security-policy&#x2F;. 2. Security Overview 2.1. New Relic"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    },
    {
      "sections": [
        "New Relic data usage limits and policies",
        "View limits and manage data"
      ],
      "title": "New Relic data usage limits and policies",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "fc32c25b40a030ffa0fad6bfc95be7fca1360ee1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies/",
      "published_at": "2021-10-19T04:02:17Z",
      "updated_at": "2021-10-19T04:02:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers' accounts, we have various data volume and rate limits in place. We reserve the right to enforce these limits to protect our system and to avoid issues for you and other customers. If your New Relic account, whether by configuration or by error, exceeds one of these limits, it or its child accounts might experience one or both of the following: Sampling of data Temporary pause or cessation of data collection To learn more about how hitting a limit can affect your data, see View limits. If you have further questions about these limits, your contract, or a limit you've reached, contact your New Relic account representative. We can work with you to adjust any rate limits to meet your needs. View limits and manage data For information about system and account limits, and for links to data ingest API limits, go to View limits. To manage your data ingest, storage, and limits for organization or billing purposes, go to Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 216.88243,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic data usage limits and <em>policies</em>",
        "sections": "New Relic data usage limits and <em>policies</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " limits and manage data For <em>information</em> about system and account limits, and for links to data ingest API limits, go to View limits. To manage your data ingest, storage, and limits for organization or billing purposes, go to Manage data."
      },
      "id": "603eb1c528ccbc0311eba7c7"
    },
    {
      "sections": [
        "Service level availability commitment",
        "New Relic Service Level Availability"
      ],
      "title": "Service level availability commitment",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "5489f268010887c72446f12059a1a0e4279ad960",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/service-level-availability-commitment/",
      "published_at": "2021-10-18T14:37:41Z",
      "updated_at": "2021-03-16T04:22:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you subscribe to New Relic Full Stack Observability Pro or Enterprise Products, the service level availability commitments available to you are those set forth (i) in the Terms, or (ii) as set forth on this page. If you subscribe to New Relic Full Stack Observability Standard, any service level availability commitment or related remedies contained in your Terms are vacated and nullified, and New Relic will use commercially reasonable efforts to make New Relic Full Stack Observability Standard and Telemetry Data Platform available in line with industry standards. If you subscribe to any other New Relic Products on a product-based pricing basis, any service level availability commitment are contained in (i) your Terms, or (ii) if your Terms explicitly references this Service Level Availability commitment applying to the Service purchased in an Order, this page for the Services you use. Capitalized terms not defined below shall take on the meaning set forth in your Terms. New Relic Service Level Availability The Service will be considered available so long as Customer is able to log in to its interface and view Customer Data (\"Service Availability\"). The applicable Service Availability will be calculated as a percentage of: (1) the total number of minutes in a month after (2) subtracting any periods of unavailability during such month from the total number of minutes in a month. New Relic will use commercially reasonable efforts to maintain Service Availability of at least 99.8% during any calendar month. In the event the Service Availability drops below: (i) 98.5% for two consecutive calendar months during the Subscription Term, or (ii) 96.5% in any single calendar month, Customer may request to terminate the relevant Service with no penalty. Such termination will be effective as of the end of the then-current billing period and no additional fees will be charged. The service level within New Relic's control is the Service Availability, not, for example, the transmission of data over the public Internet. Service Availability calculations will exclude unavailability arising from any: (a) planned maintenance periods; (b) emergency maintenance that is necessary to prevent imminent harm to the Service; (c) force majeure events; (d) Third-Party Services, Customer application, equipment, software or other technology, or Customer or its User's use of the Service, in violation of the Agreement or not in accordance with the Documentation; or (e) suspension, limitation, and/or termination of Customer’s access or use of the Service in accordance with this Agreement. This describes Customer’s sole and exclusive remedy for failures of Service Availability. Customer may request the Service Availability attainment for the previous month by filing a support ticket on the New Relic support site.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 208.73428,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>"
      },
      "id": "603ea44828ccbcd9c8eba7b7"
    }
  ],
  "/docs/licenses/license-information/referenced-policies/security-policy": [
    {
      "sections": [
        "Security guide",
        "Tip",
        "Security Program",
        "Security Domains",
        "Security Certifications",
        "Data Control, Facilities, and Encryption",
        "Law Enforcement Request Report"
      ],
      "title": "Security guide",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "356f0d11ffcb62208a743a0a7c127f5f6da9c940",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-guide/",
      "published_at": "2021-10-18T14:37:41Z",
      "updated_at": "2021-09-19T15:21:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Last updated September 17, 2021. This is supplement to our security policy and serves as a guide to New Relic’s description of its Services, functionalities, and features. Tip We may update the URLs in this document without notice. Security Program New Relic follows \"privacy by design\" principles as described here: https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Security Domains New Relic’s policies and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless Security; Configuration Management; Vulnerability Management; Network Protection; Transmission Protection; Password Management; Access Control, Audit Logging & Monitoring; Education, Training, and Awareness; Third Party Assurance; Incident Management; Business Continuity and Disaster Recover; Risk Management; Data Protection & Privacy; and Service Management Systems. Security Certifications New Relic audits its Services against industry standards as described at https://docs.newrelic.com/docs/security/security-privacy/compliance/regulatory-audits-new-relic-services/. Data Control, Facilities, and Encryption New Relic's customers can send data to New Relic's APIs by (1) using New Relic's software, (2) using vendor-neutral software that is managed and maintained by a third-party such as via OpenTelemetry instrumentation provided by opentelemetry.io, or (3) from third-party systems that customer's manage and/or control. New Relic's customers can use New Relic's Services such as NerdGraph to filter out and drop data. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/drop-data-using-nerdgraph/. New Relic's customers can adjust their data retention periods as appropriate for their needs. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/manage-data-retention/#adjust-retention. New Relic Logs obfuscates numbers that match known patterns, such as bank card and social security numbers as described here: https://docs.newrelic.com/docs/logs/log-management/get-started/new-relics-log-management-security-privacy/. New Relic honors requests to delete personal data in accordance with applicable privacy laws. Please see https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Customers may use New Relic's APIs to query data, such as NerdGraph described here, and New Relic Services to export the data to other cloud providers. Customers can configure its log forwarder [https://docs.newrelic.com/docs/logs/enable-log-management-new-relic/enable-log-monitoring-new-relic/forward-your-logs-using-infrastructure-agent/] before sending infrastructure logs to New Relic. For New Relic Customers in New Relic US, FedRAMP and HIPAA-enabled environments, Customer Data is replicated to the off-site backup system via Amazon Simple Storage Service (S3). Category of Customer Description FedRAMP HIPAA-enabled US Gen Pop EU Gen Pop Data is stored in Amazon Web Services (“AWS”). Limited Data is stored in IBM Data for New Relic Incident Intelligence is stored in Google Cloud New Relic regularly tests, assess, and evaluates its measures to ensure the security of processing using industry-recognized standards and uses independent third-party auditors as provided below: Annual SOC 2 Type 2 Annual FedRAMP assessment by an independent third-party pursuant to NIST 800-53 rev 4 Moderate authorization. Annual HITRUST-validated assessment by an independent third-party *Pursuing CY2021 Q4 ISO 27001 *Pursuing CY2021 TISAX *Pursuing CY2021 The Services that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed information about AWS security is available at https://aws.amazon.com/security/ and http://aws.amazon.com/security/sharing-the-security-responsibility/. Data encryption at rest utilizes FIPS 140-2 compliant encryption methodology. For AWS SOC Reports, please see https://aws.amazon.com/compliance/soc-faqs/. The Services that operate on Google Cloud Platform (\"GCP\") are protected by the security and environmental controls of GCP. Detailed information about GCP security is available at https://cloud.google.com/docs/tutorials#security. For GCP reports, please see https://cloud.google.com/security/compliance/. IBM Deft Zayo QTS Law Enforcement Request Report New Relic has not to date received any request for customer data from a law enforcement or other government agency (including under any national security process), and has not made any corresponding disclosures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 255.59143,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " as described here: https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;security&#x2F;security-privacy&#x2F;data-privacy&#x2F;data-privacy-new-relic&#x2F;. Security Domains New Relic’s <em>policies</em> and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless"
      },
      "id": "6147558128ccbc973a56a863"
    },
    {
      "sections": [
        "New Relic data usage limits and policies",
        "View limits and manage data"
      ],
      "title": "New Relic data usage limits and policies",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "fc32c25b40a030ffa0fad6bfc95be7fca1360ee1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies/",
      "published_at": "2021-10-19T04:02:17Z",
      "updated_at": "2021-10-19T04:02:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers' accounts, we have various data volume and rate limits in place. We reserve the right to enforce these limits to protect our system and to avoid issues for you and other customers. If your New Relic account, whether by configuration or by error, exceeds one of these limits, it or its child accounts might experience one or both of the following: Sampling of data Temporary pause or cessation of data collection To learn more about how hitting a limit can affect your data, see View limits. If you have further questions about these limits, your contract, or a limit you've reached, contact your New Relic account representative. We can work with you to adjust any rate limits to meet your needs. View limits and manage data For information about system and account limits, and for links to data ingest API limits, go to View limits. To manage your data ingest, storage, and limits for organization or billing purposes, go to Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 216.88228,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic data usage limits and <em>policies</em>",
        "sections": "New Relic data usage limits and <em>policies</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " limits and manage data For <em>information</em> about system and account limits, and for links to data ingest API limits, go to View limits. To manage your data ingest, storage, and limits for organization or billing purposes, go to Manage data."
      },
      "id": "603eb1c528ccbc0311eba7c7"
    },
    {
      "sections": [
        "Service level availability commitment",
        "New Relic Service Level Availability"
      ],
      "title": "Service level availability commitment",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "5489f268010887c72446f12059a1a0e4279ad960",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/service-level-availability-commitment/",
      "published_at": "2021-10-18T14:37:41Z",
      "updated_at": "2021-03-16T04:22:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you subscribe to New Relic Full Stack Observability Pro or Enterprise Products, the service level availability commitments available to you are those set forth (i) in the Terms, or (ii) as set forth on this page. If you subscribe to New Relic Full Stack Observability Standard, any service level availability commitment or related remedies contained in your Terms are vacated and nullified, and New Relic will use commercially reasonable efforts to make New Relic Full Stack Observability Standard and Telemetry Data Platform available in line with industry standards. If you subscribe to any other New Relic Products on a product-based pricing basis, any service level availability commitment are contained in (i) your Terms, or (ii) if your Terms explicitly references this Service Level Availability commitment applying to the Service purchased in an Order, this page for the Services you use. Capitalized terms not defined below shall take on the meaning set forth in your Terms. New Relic Service Level Availability The Service will be considered available so long as Customer is able to log in to its interface and view Customer Data (\"Service Availability\"). The applicable Service Availability will be calculated as a percentage of: (1) the total number of minutes in a month after (2) subtracting any periods of unavailability during such month from the total number of minutes in a month. New Relic will use commercially reasonable efforts to maintain Service Availability of at least 99.8% during any calendar month. In the event the Service Availability drops below: (i) 98.5% for two consecutive calendar months during the Subscription Term, or (ii) 96.5% in any single calendar month, Customer may request to terminate the relevant Service with no penalty. Such termination will be effective as of the end of the then-current billing period and no additional fees will be charged. The service level within New Relic's control is the Service Availability, not, for example, the transmission of data over the public Internet. Service Availability calculations will exclude unavailability arising from any: (a) planned maintenance periods; (b) emergency maintenance that is necessary to prevent imminent harm to the Service; (c) force majeure events; (d) Third-Party Services, Customer application, equipment, software or other technology, or Customer or its User's use of the Service, in violation of the Agreement or not in accordance with the Documentation; or (e) suspension, limitation, and/or termination of Customer’s access or use of the Service in accordance with this Agreement. This describes Customer’s sole and exclusive remedy for failures of Service Availability. Customer may request the Service Availability attainment for the previous month by filing a support ticket on the New Relic support site.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 208.73428,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>"
      },
      "id": "603ea44828ccbcd9c8eba7b7"
    }
  ],
  "/docs/licenses/license-information/referenced-policies/service-level-availability-commitment": [
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Purpose",
        "2. Security Overview",
        "3.Security Certifications",
        "4. Data Control and Encryption",
        "5. Facilities",
        "6. Employee Access, Screening and Controls.",
        "7. Security Incident and Data Breach Response",
        "8. Network and Systems Security",
        "9. Authentication and Access Management",
        "10. Vulnerability Management",
        "11. System Access and Logging",
        "12. Disaster Recovery and Data Backup",
        "13. Copies and Removal",
        "14. Third Party Vendor Management",
        "15. Disclosure by Law",
        "16. Updates"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2021-10-18T14:36:56Z",
      "updated_at": "2021-09-27T15:05:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Updated 23 September 2021. The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Purpose This Policy describes New Relic's security program and the technical and organizational security controls to protect New Relic’s systems as shown in New Relic’s third-party audits and certifications. New Relic may update this Policy from time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/. 2. Security Overview 2.1. New Relic maintains a comprehensive Information Protection Program to manage information security within New Relic that includes administrative, technical, and physical safeguards designed to protect the confidentiality, integrity, and availability of Customer Data and that is appropriate to the nature, size, and complexity of New Relic's business operations. New Relic’s Information Protection Program includes: 2.1.1. executive review, support and accountability for all security related policies and practices; 2.1.2. formal written policies and procedures that are designed to protect against the loss, theft, or other unauthorized access or alteration of Customer Data, meet or exceed applicable industry standards, outlines a definition of information security and its overall objectives, include defined information security roles and responsibilities, a framework for setting control objectives and controls, a formal and effective risk mitigation program and a service provider security management program; 2.1.3. periodic risk assessments, including internal audits and/or independent 3rd party audits, to measure the effectiveness and appropriateness of controls for all systems processing Customer Data; 2.1.4. in-depth review of security incidents, including effective determination of root cause and corrective action; formal, industry-recognized controls frameworks based on an external audit standard; 2.1.5. employee screening and access management; and 2.1.6. comprehensive security testing, vulnerability identification, and mitigation methodology that consists of a variety of independent approaches that, when combined, are designed to maximize coverage for a diverse set of attack vectors. 2.2. New Relic's Chief Information Security Officer (CISO) leads New Relic's Information Security Program and develops, reviews, and approves, with appropriate stakeholders, New Relic's security policies and procedures. New Relic has appointed data protection officer(s) as described in New Relic’s Privacy Notices who are consulted as necessary under applicable laws. 2.3. Information security objectives, approach, scope, importance, goals, and principles for the organization’s security program are formally identified, communicated throughout the organization to users in a form that is relevant, accessible, and understandable to the intended reader; and supported by a controls framework that considers legislative, regulatory, contractual requirements, and other policy-related requirements. 2.4. New Relic will maintain written policies and procedures to review, test, and approve (as appropriate) changes affecting New Relic infrastructure and systems that process Customer Data including, but not limited to, peer reviews prior to introducing new code into production. New Relic will establish acceptance criteria for new information systems, upgrades, and new versions and will carry out suitable tests of the system(s) during development and prior to acceptance. Any changes or updates will not materially decrease the security of the systems. 3.Security Certifications 3.1. New Relic regularly tests, assesses, and evaluates its security measures for protecting Customer Data using industry-recognized standards and uses independent third-party auditors to verify such controls. 3.2. New Relic agrees to provide Customer, upon request, with applicable certifications or reports about New Relic systems. All information exchanged in connection with the audit activities described in this section is deemed to be the Confidential Information of New Relic. 3.3. Additional information about New Relic’s security certifications are available on New Relic’s Security Guide. 4. Data Control and Encryption 4.1. The Service and related features are designed to provide Customer with control over Customer’s data sources and Customer’s environments that are monitored and sending data to New Relic. 4.2. New Relic will have established methods to: (i) enable Customer to use encryption on Customer Data in transit, and (ii) securely hash passwords in storage following industry standard practices (e.g. scrypt) as described in the Documentation. Server certificate-based authentication is used as part of the TLS encryption with a trusted certificate authority. 4.3. New Relic receives and processes data in accordance with the Agreement and the Services as described in the Documentation. New Relic permits customers to delete personal data in accordance with applicable privacy laws as further described in the Documentation. In the event of error in personal data sent in Customer Data Customers may request personal data deletion and re-send data that is accurate. 4.4. Additional information for Customer data control and encryption, including encryption of data at rest, is available on New Relic’s Security Guide. 5. Facilities 5.1. All physical locations that process Customer Data are co-locations and third party data centers. All physical locations have security measures in place that are designed to prevent unauthorized physical access to data processing facilities and unlawful access, modification, or destruction of Customer Data. 5.2. New Relic will seek assurances (e.g., in the form of an independent 3rd party audit report such as the SOC 2 Type 2, ISO 27001, and vendor security evaluations) from its data processing facilities vendors that store or process Customer Data: 5.2.1. secure its data process facilities in an access-controlled location and have protections in place to prevent unauthorized access, damage, and interference; 5.2.2 employ physical security appropriate to the classification of the assets and information being managed which may include card key access, security cameras, and solid wall construction for all exterior walls; 5.2.3 limit and screen all entrants, which may include measures such as on-site security guard, badge reader, electronic lock, or a monitored closed caption television (CCTV); and 5.2.4 maintain relevant access logs. 5.3. Additional information about New Relic’s third party data centers are available on New Relic’s Security Guide. 6. Employee Access, Screening and Controls. New Relic will have and maintain policies and practices that include, at a minimum, the following controls and safeguards applied to New Relic employees and contractors who may access Customer Data: 6.1. For U.S. New Relic employees and subject to applicable law, a criminal background screening of each of its new employees to whom it gives access to Customer Data at the federal, state, and county levels. For non-U.S. New Relic employees, New Relic will use commercially reasonable efforts to meet the same criteria as established for U.S.-based New Relic employees, subject to general business practices in the respective country and in compliance with applicable local law requirements; 6.2. All New Relic employees with access to Customer Data will undergo adequate training, such as annual security awareness training, in the care, protection and handling of Customer Data, and will align with the privacy and security measures set out in this Addendum by following the guidance provided in their welcome package, which includes New Relic’s security policies and a security acknowledgement; 6.3. A disciplinary policy and process, to be used when New Relic employees violate New Relic security or privacy policies or access Customer Data without prior authorization; 6.4. Administrative or remote access to New Relic systems that process Customer Data will align with industry standard practices, including multi-factor authentication; 6.5. Restricted access to New Relic proprietary source code to prevent unauthorized access; 6.6. Controls designed to ensure that only those New Relic employees with an actual need-to-know will have access to New Relic systems including, but not limited to, the use of a formal access management process for the request, review, approval, and provisioning; 6.7. Controls designed to ensure that New Relic employees are granted access to New Relic systems based on least-privilege principles; and 6.8. Revoke access to New Relic employees no longer requiring access. 7. Security Incident and Data Breach Response 7.1. New Relic will take appropriate physical, technical, and administrative security measures that are commercially reasonable and consistent with industry standards to prevent a Data Breach, and as required by any applicable law or regulation. “Data Breach” means the theft, loss, or unauthorized access of Customer Data. Without limiting the foregoing, New Relic will implement security measures at least as stringent as those set out in this Addendum. New Relic will designate a senior representative to provide incident briefings, as needed in case of a Data Breach, and to respond to reasonable requests by Customer pertaining to privacy and data security issues within a commercially reasonable time frame. 7.2. New Relic will: 7.2.1. Notify Customer without undue delay if New Relic becomes aware of a Data Breach; 7.2.2. Maintain a security incident response plan, including procedures and means to respond in a manner consistent with industry standards; 7.2.3. Reasonably cooperate with Customer to investigate and remediate a Data Breach and mitigate any further risk to the Customer Data, or risk to data subjects and/or Customer’s reputation or brand; 7.2.4. Provide reasonable assistance to Customer at New Relic’s sole cost and expense; and 7.2.5. Make commercially reasonable efforts to preserve evidence and reasonably cooperate with Customer and legal authorities (as applicable and legally permissible) during an investigation of a Data Breach. 8. Network and Systems Security 8.1. All extranet connectivity by New Relic personnel to systems processing Customer Data will be through secure remote connections. 8.2. Network segments connected to the Internet will be protected by secure access control mechanisms, such as a firewall configured to secure all devices behind it and properly addresses security concerns according to industry standard practices. 8.3. New Relic will have industry standard measures in place to actively monitor its systems and help detect a potential hostile attack, such as Network Intrusion Detection (NID) or Host Intrusion Detection (HID)/Prevention. 8.4. Applications, ports, services, and similar access points installed on a computer or network facility, which are not specifically required for business functionality, will be disabled or removed. 8.5. New Relic maintains configuration standards for authorized operating systems and software for systems that support processing of Customer Data. New Relic will establish and follow server configuration guidelines and processes for preventing unauthorized access to Customer Data. New Relic maintains secure images or templates for systems based on the organization’s approved configuration standards. 8.6. Development, test, and operational environments will be logically separated to reduce the risks of unauthorized access or changes to the operational system. 8.7. New Relic network architecture will be designed to limit site access and restrict the availability of information systems that are considered to be vulnerable to attack. 8.8. New Relic provides appropriate TLS certificates when users access and view Customer Data in the Service. New Relic’s software for sending Customer Data to New Relic will encrypt Customer Data in transit by default. 9. Authentication and Access Management If you subscribe to the requisite New Relic One Service, New Relic will provide industry standard authentication and access controls to protect Customer Data, including industry standard authentication methods utilized to help prevent unauthorized access to the Service. New Relic’s access control methods will clearly state the rules and rights for each user or group of users including applications and information sharing and will include a process for granting and removing access to all information systems processing Customer Data. A record of all privileges allocated will be maintained pursuant to the requirements herein. 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. 10. Vulnerability Management New Relic will have and maintain the following vulnerability management processes for all devices used to connect to the New Relic network and Services. 10.1. New Relic will align to industry standard practices for build out, minimization of services and secure configuration, in accordance with, industry-recognized minimum security baselines for the New Relic platform and subcontractor systems connected to the New Relic platform in relation to the provision of the Services. 10.2. New Relic will employ industry-recognized standards and tools to conduct frequent infrastructure vulnerability scanning to test New Relic’s network and infrastructure and application penetration testing to test the New Relic Services. New Relic applies “Medium”, “High” and “Critical” security patches for all components in production and development environments as soon as commercially possible in accordance with its vulnerability management protocol, and consistent with industry standard practices and standards; 10.3. New Relic will have processes in place designed to ensure adherence to industry standard security development practices for development and testing for all code, APIs, and applications deployed and implemented in support of the Service; 10.4. New Relic will have and maintain solutions to identify and prevent malicious attackers or code from accessing or compromising Customer Data or systems that process Customer Data. These include, but are not limited to, software that identifies and removes malware and detects attempted intrusions. New Relic will have a security event and incident monitoring system and supporting processes to notify appropriate personnel in response to threats. 11. System Access and Logging 11.1. Access to New Relic’s systems will not be granted to employees of New Relic unless they have been uniquely identified and have sufficient credentials. 11.2. Access to New Relic’s systems will be logged and retained for no less than 6 months to assist in investigations and access control monitoring, including, but not limited to, end user access and activities, and information security events. 11.3. New Relic agrees to provide Customer the capability to access log records relating to Customer Accounts and the New Relic systems that process Customer Data in the event of a Data Breach or if required in connection with a law enforcement request. 12. Disaster Recovery and Data Backup 12.1. New Relic will have plans designed to respond to loss of services, which are tested and reviewed at least annually. This plan will include documented policies and procedures to restore service in the event of a service failure. 12.2. New Relic will establish and follow backup and restore procedures for Customer Data. 12.3. New Relic business continuity plan identifies critical systems. Annual disaster recovery tests are performed to check and restore customer data in the event of an incident. 12.4. New Relic will provide Customer with redacted copies of its plan(s) and evidence of tests/reviews upon request, but not more frequently than once annually, and subject to confidentiality requirements. 12.5. Additional information for Customer Data control and encryption are available on New Relic’s Security Guide. 13. Copies and Removal 13.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of the Agreement for any reason: (a) New Relic will, and will cause its personnel, to cease all access and use of any Customer Data, and (b) New Relic will delete all copies of Customer Data within ninety (90) days. 13.2. New Relic will maintain a process of ensuring secure destruction and deletion of all Customer Data, when reasonably requested by Customer or as otherwise provided in the Agreement. The process will include industry standard processes so that: (i) Customer Data cannot be practicably read or reconstructed, and (ii) the systems that store Customer Data are securely erased and/or decommissioned disks are destroyed. 14. Third Party Vendor Management New Relic may use third party vendors to provide the Services. New Relic performs a security risk-based assessment of prospective vendors before working with them to validate they meet New Relic’s security and business continuity standards, including the type of access and classification of data being accessed (if any), controls necessary to protect data, and legal/regulatory requirements. New Relic enters into written agreements with its vendors that process Customer Data which include confidentiality, privacy, and security obligations that provide an appropriate level of protection for Customer Data that these vendors may process for New Relic to maintain the security posture in this Policy, including following industry security standards. 15. Disclosure by Law In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic will (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. New Relic publishes its law enforcement requests report on New Relic’s Security Guide. 16. Updates As New Relic releases new products, services, functionality, and features, New Relic may update this Policy to account for such products, services, functionality, and features. For additional information, see our Security Guide.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.29892,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Security <em>policy</em>",
        "sections": "Security <em>policy</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;<em>referenced</em>-<em>policies</em>&#x2F;security-policy&#x2F;. 2. Security Overview 2.1. New Relic"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    },
    {
      "sections": [
        "Security guide",
        "Tip",
        "Security Program",
        "Security Domains",
        "Security Certifications",
        "Data Control, Facilities, and Encryption",
        "Law Enforcement Request Report"
      ],
      "title": "Security guide",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "356f0d11ffcb62208a743a0a7c127f5f6da9c940",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-guide/",
      "published_at": "2021-10-18T14:37:41Z",
      "updated_at": "2021-09-19T15:21:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Last updated September 17, 2021. This is supplement to our security policy and serves as a guide to New Relic’s description of its Services, functionalities, and features. Tip We may update the URLs in this document without notice. Security Program New Relic follows \"privacy by design\" principles as described here: https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Security Domains New Relic’s policies and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless Security; Configuration Management; Vulnerability Management; Network Protection; Transmission Protection; Password Management; Access Control, Audit Logging & Monitoring; Education, Training, and Awareness; Third Party Assurance; Incident Management; Business Continuity and Disaster Recover; Risk Management; Data Protection & Privacy; and Service Management Systems. Security Certifications New Relic audits its Services against industry standards as described at https://docs.newrelic.com/docs/security/security-privacy/compliance/regulatory-audits-new-relic-services/. Data Control, Facilities, and Encryption New Relic's customers can send data to New Relic's APIs by (1) using New Relic's software, (2) using vendor-neutral software that is managed and maintained by a third-party such as via OpenTelemetry instrumentation provided by opentelemetry.io, or (3) from third-party systems that customer's manage and/or control. New Relic's customers can use New Relic's Services such as NerdGraph to filter out and drop data. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/drop-data-using-nerdgraph/. New Relic's customers can adjust their data retention periods as appropriate for their needs. See https://docs.newrelic.com/docs/telemetry-data-platform/manage-data/manage-data-retention/#adjust-retention. New Relic Logs obfuscates numbers that match known patterns, such as bank card and social security numbers as described here: https://docs.newrelic.com/docs/logs/log-management/get-started/new-relics-log-management-security-privacy/. New Relic honors requests to delete personal data in accordance with applicable privacy laws. Please see https://docs.newrelic.com/docs/security/security-privacy/data-privacy/data-privacy-new-relic/. Customers may use New Relic's APIs to query data, such as NerdGraph described here, and New Relic Services to export the data to other cloud providers. Customers can configure its log forwarder [https://docs.newrelic.com/docs/logs/enable-log-management-new-relic/enable-log-monitoring-new-relic/forward-your-logs-using-infrastructure-agent/] before sending infrastructure logs to New Relic. For New Relic Customers in New Relic US, FedRAMP and HIPAA-enabled environments, Customer Data is replicated to the off-site backup system via Amazon Simple Storage Service (S3). Category of Customer Description FedRAMP HIPAA-enabled US Gen Pop EU Gen Pop Data is stored in Amazon Web Services (“AWS”). Limited Data is stored in IBM Data for New Relic Incident Intelligence is stored in Google Cloud New Relic regularly tests, assess, and evaluates its measures to ensure the security of processing using industry-recognized standards and uses independent third-party auditors as provided below: Annual SOC 2 Type 2 Annual FedRAMP assessment by an independent third-party pursuant to NIST 800-53 rev 4 Moderate authorization. Annual HITRUST-validated assessment by an independent third-party *Pursuing CY2021 Q4 ISO 27001 *Pursuing CY2021 TISAX *Pursuing CY2021 The Services that operate on Amazon Web Services (“AWS”) are protected by the security and environmental controls of AWS. Detailed information about AWS security is available at https://aws.amazon.com/security/ and http://aws.amazon.com/security/sharing-the-security-responsibility/. Data encryption at rest utilizes FIPS 140-2 compliant encryption methodology. For AWS SOC Reports, please see https://aws.amazon.com/compliance/soc-faqs/. The Services that operate on Google Cloud Platform (\"GCP\") are protected by the security and environmental controls of GCP. Detailed information about GCP security is available at https://cloud.google.com/docs/tutorials#security. For GCP reports, please see https://cloud.google.com/security/compliance/. IBM Deft Zayo QTS Law Enforcement Request Report New Relic has not to date received any request for customer data from a law enforcement or other government agency (including under any national security process), and has not made any corresponding disclosures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 255.59142,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " as described here: https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;security&#x2F;security-privacy&#x2F;data-privacy&#x2F;data-privacy-new-relic&#x2F;. Security Domains New Relic’s <em>policies</em> and procedures cover industry-recognized security domains such as Endpoint Protection; Portable Media Security; Mobile Device Security; Wireless"
      },
      "id": "6147558128ccbc973a56a863"
    },
    {
      "sections": [
        "New Relic data usage limits and policies",
        "View limits and manage data"
      ],
      "title": "New Relic data usage limits and policies",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "fc32c25b40a030ffa0fad6bfc95be7fca1360ee1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies/",
      "published_at": "2021-10-19T04:02:17Z",
      "updated_at": "2021-10-19T04:02:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers' accounts, we have various data volume and rate limits in place. We reserve the right to enforce these limits to protect our system and to avoid issues for you and other customers. If your New Relic account, whether by configuration or by error, exceeds one of these limits, it or its child accounts might experience one or both of the following: Sampling of data Temporary pause or cessation of data collection To learn more about how hitting a limit can affect your data, see View limits. If you have further questions about these limits, your contract, or a limit you've reached, contact your New Relic account representative. We can work with you to adjust any rate limits to meet your needs. View limits and manage data For information about system and account limits, and for links to data ingest API limits, go to View limits. To manage your data ingest, storage, and limits for organization or billing purposes, go to Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 216.88214,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic data usage limits and <em>policies</em>",
        "sections": "New Relic data usage limits and <em>policies</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " limits and manage data For <em>information</em> about system and account limits, and for links to data ingest API limits, go to View limits. To manage your data ingest, storage, and limits for organization or billing purposes, go to Manage data."
      },
      "id": "603eb1c528ccbc0311eba7c7"
    }
  ],
  "/docs/licenses/license-information/special-services-licenses/data-collector-licenses": [
    {
      "sections": [
        "New Relic data usage limits and policies",
        "View limits and manage data"
      ],
      "title": "New Relic data usage limits and policies",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "fc32c25b40a030ffa0fad6bfc95be7fca1360ee1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies/",
      "published_at": "2021-10-19T04:02:17Z",
      "updated_at": "2021-10-19T04:02:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers' accounts, we have various data volume and rate limits in place. We reserve the right to enforce these limits to protect our system and to avoid issues for you and other customers. If your New Relic account, whether by configuration or by error, exceeds one of these limits, it or its child accounts might experience one or both of the following: Sampling of data Temporary pause or cessation of data collection To learn more about how hitting a limit can affect your data, see View limits. If you have further questions about these limits, your contract, or a limit you've reached, contact your New Relic account representative. We can work with you to adjust any rate limits to meet your needs. View limits and manage data For information about system and account limits, and for links to data ingest API limits, go to View limits. To manage your data ingest, storage, and limits for organization or billing purposes, go to Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 213.52222,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": "We strive to keep our resources operating efficiently so that our <em>services</em> are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers&#x27; accounts, we have various data volume and rate limits in place. We reserve the right to enforce"
      },
      "id": "603eb1c528ccbc0311eba7c7"
    },
    {
      "sections": [
        "New Relic Priority Support"
      ],
      "title": "New Relic Priority Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "f700b8b349627e66540b23ed020c7d9a46e19580",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-priority-support/",
      "published_at": "2021-10-19T05:41:55Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Priority Support document as a PDF (172 KB). The services described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 193.1074,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Priority Support document as a PDF (172 KB). The <em>services</em> described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative."
      },
      "id": "603ea90ae7b9d2e9d42a07d1"
    },
    {
      "sections": [
        "New Relic Premium Support"
      ],
      "title": "New Relic Premium Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "db095214148ca053cdabd9815dc74b04299dbe7b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-premium-support/",
      "published_at": "2021-10-19T05:40:44Z",
      "updated_at": "2021-03-16T04:14:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Premium Support document as a PDF (248 KB). The services described in this document only apply to initial orders entered after April 1, 2018. If you have questions about New Relic Premium Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 193.10661,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Premium Support document as a PDF (248 KB). The <em>services</em> described in this document only apply to initial orders entered after April 1, 2018. If you have questions about New Relic Premium Support, contact your New Relic account representative."
      },
      "id": "603ea8c964441fe48b4e88af"
    }
  ],
  "/docs/licenses/license-information/special-services-licenses/new-relic-diagnostics-licenses": [
    {
      "sections": [
        "New Relic data usage limits and policies",
        "View limits and manage data"
      ],
      "title": "New Relic data usage limits and policies",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "fc32c25b40a030ffa0fad6bfc95be7fca1360ee1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies/",
      "published_at": "2021-10-19T04:02:17Z",
      "updated_at": "2021-10-19T04:02:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers' accounts, we have various data volume and rate limits in place. We reserve the right to enforce these limits to protect our system and to avoid issues for you and other customers. If your New Relic account, whether by configuration or by error, exceeds one of these limits, it or its child accounts might experience one or both of the following: Sampling of data Temporary pause or cessation of data collection To learn more about how hitting a limit can affect your data, see View limits. If you have further questions about these limits, your contract, or a limit you've reached, contact your New Relic account representative. We can work with you to adjust any rate limits to meet your needs. View limits and manage data For information about system and account limits, and for links to data ingest API limits, go to View limits. To manage your data ingest, storage, and limits for organization or billing purposes, go to Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 213.52208,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": "We strive to keep our resources operating efficiently so that our <em>services</em> are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers&#x27; accounts, we have various data volume and rate limits in place. We reserve the right to enforce"
      },
      "id": "603eb1c528ccbc0311eba7c7"
    },
    {
      "sections": [
        "Data collector licenses"
      ],
      "title": "Data collector licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "8eeb19ae388c1f4fd6856084084467a037861675",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/data-collector-licenses/",
      "published_at": "2021-10-19T05:39:16Z",
      "updated_at": "2021-03-16T06:20:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Package Licenses annotations Apache 2.0 antlr BSD apache-log4j-extras Apache 2.0 apache-mime4j Apache 2.0 bonecp Apache 2.0 bonecp-provider Apache 2.0 bytelist MIT c3p0 Eclipse 1.0 commons-beanutils Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-digester Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-lang3 Apache 2.0 commons-logging Apache 2.0 commons-math Apache 2.0 commons-pool Apache 2.0 constantine MIT dom4j BSD ehcache Apache 2.0 gson Apache 2.0 guava Apache 2.0 hamcrest-core BSD-3-Clause hibernate-commons-annotations GNU LGPL 2.1 hibernate-core GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 hibernate-c3p0 GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 hystrix Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 jaffl GNU LGPL 3.0 java-driver Apache 2.0 javassist MPL 1.1, GNU GPL 2.1, Apache 2.0 javax.servlet Apache 2.0, Eclipse 1.0 javax.servlet-api CDDL + GPLv2 with classpath exception jboss-logging GNU LGPL 2.1 jboss-transaction-api_1.1_spec GNU LGPL 2.1 jcodings Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 jedis MIT jetty Apache 2.0, Eclipse 1.0 jffi GNU LGPL 3.0 jline BSD 2-Clause joni MIT jnr-netdb GNU LGPL 3.0 jnr-posix Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 joda-time Apache 2.0 jopt-simple MIT json-simple Apache 2.0 jsr305 Apache 2.0 junit Common Public License 1.0 kafka Apache 2.0 libthrift Apache 2.0 log4j Apache 2.0 logback-classic Eclipse 1.0 logback-core Eclipse 1.0 metrics-annotation Apache 2.0 mockito-all MIT msgpack Apache 2.0 mysql-connector-java GNU GPL 2 reflections WTFPL scala-library Apache 2.0 scannotation Apache 2.0 slf4j-api MIT slf4j-over-log4j MIT snakeyaml Apache 2.0 snappy Apache 2.0 syslog4j LGPL xml-apis Apache 2.0 yamlbeans MIT zkclient Apache 2.0 zookeeper Apache 2.0",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 193.11008,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Data collector <em>licenses</em>",
        "sections": "Data collector <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Package <em>Licenses</em> annotations"
      },
      "id": "603eb41d64441fd7f74e8893"
    },
    {
      "sections": [
        "New Relic Priority Support"
      ],
      "title": "New Relic Priority Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "f700b8b349627e66540b23ed020c7d9a46e19580",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-priority-support/",
      "published_at": "2021-10-19T05:41:55Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Priority Support document as a PDF (172 KB). The services described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 193.1074,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Priority Support document as a PDF (172 KB). The <em>services</em> described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative."
      },
      "id": "603ea90ae7b9d2e9d42a07d1"
    }
  ],
  "/docs/licenses/license-information/special-services-licenses/new-relic-premium-support": [
    {
      "sections": [
        "New Relic data usage limits and policies",
        "View limits and manage data"
      ],
      "title": "New Relic data usage limits and policies",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "fc32c25b40a030ffa0fad6bfc95be7fca1360ee1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies/",
      "published_at": "2021-10-19T04:02:17Z",
      "updated_at": "2021-10-19T04:02:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers' accounts, we have various data volume and rate limits in place. We reserve the right to enforce these limits to protect our system and to avoid issues for you and other customers. If your New Relic account, whether by configuration or by error, exceeds one of these limits, it or its child accounts might experience one or both of the following: Sampling of data Temporary pause or cessation of data collection To learn more about how hitting a limit can affect your data, see View limits. If you have further questions about these limits, your contract, or a limit you've reached, contact your New Relic account representative. We can work with you to adjust any rate limits to meet your needs. View limits and manage data For information about system and account limits, and for links to data ingest API limits, go to View limits. To manage your data ingest, storage, and limits for organization or billing purposes, go to Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 213.52208,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": "We strive to keep our resources operating efficiently so that our <em>services</em> are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers&#x27; accounts, we have various data volume and rate limits in place. We reserve the right to enforce"
      },
      "id": "603eb1c528ccbc0311eba7c7"
    },
    {
      "sections": [
        "Data collector licenses"
      ],
      "title": "Data collector licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "8eeb19ae388c1f4fd6856084084467a037861675",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/data-collector-licenses/",
      "published_at": "2021-10-19T05:39:16Z",
      "updated_at": "2021-03-16T06:20:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Package Licenses annotations Apache 2.0 antlr BSD apache-log4j-extras Apache 2.0 apache-mime4j Apache 2.0 bonecp Apache 2.0 bonecp-provider Apache 2.0 bytelist MIT c3p0 Eclipse 1.0 commons-beanutils Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-digester Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-lang3 Apache 2.0 commons-logging Apache 2.0 commons-math Apache 2.0 commons-pool Apache 2.0 constantine MIT dom4j BSD ehcache Apache 2.0 gson Apache 2.0 guava Apache 2.0 hamcrest-core BSD-3-Clause hibernate-commons-annotations GNU LGPL 2.1 hibernate-core GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 hibernate-c3p0 GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 hystrix Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 jaffl GNU LGPL 3.0 java-driver Apache 2.0 javassist MPL 1.1, GNU GPL 2.1, Apache 2.0 javax.servlet Apache 2.0, Eclipse 1.0 javax.servlet-api CDDL + GPLv2 with classpath exception jboss-logging GNU LGPL 2.1 jboss-transaction-api_1.1_spec GNU LGPL 2.1 jcodings Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 jedis MIT jetty Apache 2.0, Eclipse 1.0 jffi GNU LGPL 3.0 jline BSD 2-Clause joni MIT jnr-netdb GNU LGPL 3.0 jnr-posix Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 joda-time Apache 2.0 jopt-simple MIT json-simple Apache 2.0 jsr305 Apache 2.0 junit Common Public License 1.0 kafka Apache 2.0 libthrift Apache 2.0 log4j Apache 2.0 logback-classic Eclipse 1.0 logback-core Eclipse 1.0 metrics-annotation Apache 2.0 mockito-all MIT msgpack Apache 2.0 mysql-connector-java GNU GPL 2 reflections WTFPL scala-library Apache 2.0 scannotation Apache 2.0 slf4j-api MIT slf4j-over-log4j MIT snakeyaml Apache 2.0 snappy Apache 2.0 syslog4j LGPL xml-apis Apache 2.0 yamlbeans MIT zkclient Apache 2.0 zookeeper Apache 2.0",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 193.11008,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Data collector <em>licenses</em>",
        "sections": "Data collector <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Package <em>Licenses</em> annotations"
      },
      "id": "603eb41d64441fd7f74e8893"
    },
    {
      "sections": [
        "New Relic Priority Support"
      ],
      "title": "New Relic Priority Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "f700b8b349627e66540b23ed020c7d9a46e19580",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-priority-support/",
      "published_at": "2021-10-19T05:41:55Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Priority Support document as a PDF (172 KB). The services described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 193.1074,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Priority Support document as a PDF (172 KB). The <em>services</em> described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative."
      },
      "id": "603ea90ae7b9d2e9d42a07d1"
    }
  ],
  "/docs/licenses/license-information/special-services-licenses/new-relic-priority-support": [
    {
      "sections": [
        "New Relic data usage limits and policies",
        "View limits and manage data"
      ],
      "title": "New Relic data usage limits and policies",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "fc32c25b40a030ffa0fad6bfc95be7fca1360ee1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies/",
      "published_at": "2021-10-19T04:02:17Z",
      "updated_at": "2021-10-19T04:02:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers' accounts, we have various data volume and rate limits in place. We reserve the right to enforce these limits to protect our system and to avoid issues for you and other customers. If your New Relic account, whether by configuration or by error, exceeds one of these limits, it or its child accounts might experience one or both of the following: Sampling of data Temporary pause or cessation of data collection To learn more about how hitting a limit can affect your data, see View limits. If you have further questions about these limits, your contract, or a limit you've reached, contact your New Relic account representative. We can work with you to adjust any rate limits to meet your needs. View limits and manage data For information about system and account limits, and for links to data ingest API limits, go to View limits. To manage your data ingest, storage, and limits for organization or billing purposes, go to Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 213.52208,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": "We strive to keep our resources operating efficiently so that our <em>services</em> are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers&#x27; accounts, we have various data volume and rate limits in place. We reserve the right to enforce"
      },
      "id": "603eb1c528ccbc0311eba7c7"
    },
    {
      "sections": [
        "Data collector licenses"
      ],
      "title": "Data collector licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "8eeb19ae388c1f4fd6856084084467a037861675",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/data-collector-licenses/",
      "published_at": "2021-10-19T05:39:16Z",
      "updated_at": "2021-03-16T06:20:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Package Licenses annotations Apache 2.0 antlr BSD apache-log4j-extras Apache 2.0 apache-mime4j Apache 2.0 bonecp Apache 2.0 bonecp-provider Apache 2.0 bytelist MIT c3p0 Eclipse 1.0 commons-beanutils Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-digester Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-lang3 Apache 2.0 commons-logging Apache 2.0 commons-math Apache 2.0 commons-pool Apache 2.0 constantine MIT dom4j BSD ehcache Apache 2.0 gson Apache 2.0 guava Apache 2.0 hamcrest-core BSD-3-Clause hibernate-commons-annotations GNU LGPL 2.1 hibernate-core GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 hibernate-c3p0 GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 hystrix Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 jaffl GNU LGPL 3.0 java-driver Apache 2.0 javassist MPL 1.1, GNU GPL 2.1, Apache 2.0 javax.servlet Apache 2.0, Eclipse 1.0 javax.servlet-api CDDL + GPLv2 with classpath exception jboss-logging GNU LGPL 2.1 jboss-transaction-api_1.1_spec GNU LGPL 2.1 jcodings Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 jedis MIT jetty Apache 2.0, Eclipse 1.0 jffi GNU LGPL 3.0 jline BSD 2-Clause joni MIT jnr-netdb GNU LGPL 3.0 jnr-posix Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 joda-time Apache 2.0 jopt-simple MIT json-simple Apache 2.0 jsr305 Apache 2.0 junit Common Public License 1.0 kafka Apache 2.0 libthrift Apache 2.0 log4j Apache 2.0 logback-classic Eclipse 1.0 logback-core Eclipse 1.0 metrics-annotation Apache 2.0 mockito-all MIT msgpack Apache 2.0 mysql-connector-java GNU GPL 2 reflections WTFPL scala-library Apache 2.0 scannotation Apache 2.0 slf4j-api MIT slf4j-over-log4j MIT snakeyaml Apache 2.0 snappy Apache 2.0 syslog4j LGPL xml-apis Apache 2.0 yamlbeans MIT zkclient Apache 2.0 zookeeper Apache 2.0",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 193.11008,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Data collector <em>licenses</em>",
        "sections": "Data collector <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Package <em>Licenses</em> annotations"
      },
      "id": "603eb41d64441fd7f74e8893"
    },
    {
      "sections": [
        "New Relic Premium Support"
      ],
      "title": "New Relic Premium Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "db095214148ca053cdabd9815dc74b04299dbe7b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-premium-support/",
      "published_at": "2021-10-19T05:40:44Z",
      "updated_at": "2021-03-16T04:14:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Premium Support document as a PDF (248 KB). The services described in this document only apply to initial orders entered after April 1, 2018. If you have questions about New Relic Premium Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 193.1066,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Premium Support document as a PDF (248 KB). The <em>services</em> described in this document only apply to initial orders entered after April 1, 2018. If you have questions about New Relic Premium Support, contact your New Relic account representative."
      },
      "id": "603ea8c964441fe48b4e88af"
    }
  ],
  "/docs/licenses/license-information/usage-plans/new-relic-one-usage-plan-descriptions": [
    {
      "sections": [
        "New Relic data usage limits and policies",
        "View limits and manage data"
      ],
      "title": "New Relic data usage limits and policies",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "fc32c25b40a030ffa0fad6bfc95be7fca1360ee1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies/",
      "published_at": "2021-10-19T04:02:17Z",
      "updated_at": "2021-10-19T04:02:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers' accounts, we have various data volume and rate limits in place. We reserve the right to enforce these limits to protect our system and to avoid issues for you and other customers. If your New Relic account, whether by configuration or by error, exceeds one of these limits, it or its child accounts might experience one or both of the following: Sampling of data Temporary pause or cessation of data collection To learn more about how hitting a limit can affect your data, see View limits. If you have further questions about these limits, your contract, or a limit you've reached, contact your New Relic account representative. We can work with you to adjust any rate limits to meet your needs. View limits and manage data For information about system and account limits, and for links to data ingest API limits, go to View limits. To manage your data ingest, storage, and limits for organization or billing purposes, go to Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 213.52194,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic data <em>usage</em> limits and policies",
        "sections": "New Relic data <em>usage</em> limits and policies",
        "tags": "<em>License</em> <em>information</em>",
        "body": "We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data <em>usage</em> spikes in one New Relic account from impacting other customers&#x27; accounts, we have various data volume and rate limits in place. We reserve the right to enforce"
      },
      "id": "603eb1c528ccbc0311eba7c7"
    },
    {
      "sections": [
        "Product-based pricing usage and New Relic Platform Pricing Usage Plan",
        "Important",
        "Usage Plan: New Relic Platform Pricing",
        "Eligible Services",
        "Eligible Services (Partners)",
        "Subscriptions with indeterminate pricing or usage quantities",
        "Product Usage Ratio"
      ],
      "title": "Product-based pricing usage and New Relic Platform Pricing Usage Plan",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Usage plans"
      ],
      "external_id": "e2686dc773c4e844544ce633a9a41a16f15edf5a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/usage-plans/product-based-pricing-usage-new-relic-platform-pricing-usage-plan/",
      "published_at": "2021-10-18T14:38:24Z",
      "updated_at": "2021-03-16T04:22:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This doc explains our original product-based pricing plan. For more about pricing changes, see Overview of pricing changes. The following provisions are applicable to existing customers (i) that have existing Terms, and (ii) that to the extent its subscription to the New Relic products reference the usage plans set forth below or where a subscription has indeterminate product pricing or usage quantities. New customers are eligible for New Relic One pricing as described here. Capitalized terms not defined below shall take on the meaning set forth in such New Relic order form. Usage Plan: New Relic Platform Pricing The following Eligible Services with New Relic are products that are referenced in specific order forms for New Relic Platform Pricing. For more information, contact your New Relic account representative. Eligible Services Eligible Services Per Unit Unit of Measure Monthly Standard Fee Rate New Relic APM Pro CU Annual* 10,000 Compute Units $166.70 New Relic APM Pro Host Annual* 1 Hosts $149 New Relic Insights Pro Annual 50,000,000 Events $165 New Relic Infrastructure Pro Annual 10,000 Compute Units $12 New Relic Browser Pro Annual 500,000 Page Views $149 New Relic Logs Annual - 8 Days* * 1 Per GB Daily $55 New Relic Logs Annual - 15 Days* * 1 Per GB Daily $65 New Relic Logs Annual - 30 Days* * 1 Per GB Daily $75 New Relic Metrics Annual 1,000 Data Points per Minute $25 New Relic Mobile Enterprise Annual 50,000 Total Users $499 New Relic Serverless for AWS Lambda Annual 1,000,000 AWS Lambda Events $15 New Relic Synthetics Pro Annual 10,000 Checks $69 New Relic Synthetics private locations (for New Relic Synthetics Pro Annual product) Fixed fee N/A $1,000 New Relic Traces Annual 1,000,000 Spans $1 New Relic AI Incident Intelligence Annual 1,000 Incident Events $500 New Relic AI Proactive Detection Annual 1,000,000,000 App Transactions $250 * Customer may utilize either New Relic APM Pro Annual or New Relic APM Pro CUs Annual, but not both. Customer will be provisioned New Relic APM Pro CUs Annual by default, unless otherwise stated in the relevant Order Form. * * Customer may utilize only one of New Relic Logs Annual - 8 Days, New Relic Logs Annual - 15 Days, or New Relic Logs Annual - 30 days. For the avoidance of doubt, Customer may not utilize New Relic Logs Annual with varying numbers of days retention concurrently. Customer will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. For more information about units of measures, see Product definitions. Eligible Services (Partners) Eligible services for New Relic Partners Measurement New Relic APM Pro CU Annual* Sold in Units of 8,900 Compute Units New Relic APM Pro Host Annual* Sold in Units of 1 Host New Relic Browser Pro Annual Sold in Units of 500,000 Page Views New Relic Infrastructure Pro Annual Sold in Units of 125,000 Compute Units New Relic Insights Pro Annual Sold in Units of 45,000,000 Events New Relic Logs Annual - 8 Days* * Sold in Units of 3 GB Daily New Relic Logs Annual - 15 Days* * Sold in Units of 2.5 GB Daily New Relic Logs Annual - 30 Days* * Sold in Units of 2 GB Daily New Relic Metrics Annual Sold in Units of 6,000 Data Points per Minute New Relic Mobile Enterprise Annual Sold in Units of 15,000 Total Users New Relic Serverless for AWS Lambda Annual Sold in Units of 10,000,000 AWS Lambda Events New Relic Synthetics Pro with Private Locations Annual Sold in Units of 22,000 Checks New Relic Traces Annual Sold in Units of 149,000,000 Spans * Customer and/or Partner may utilize either New Relic APM Pro Annual or New Relic APM Pro CUs Annual, but not both. Customer and/or Partner will be provisioned New Relic APM Pro CUs Annual by default, unless otherwise stated in the relevant Order Form. * * Customer and/or Partner may utilize only one of New Relic Logs Annual - 8 Days, New Relic Logs Annual - 15 Days, or New Relic Logs Annual - 30 days. For the avoidance of doubt, Customer may not utilize New Relic Logs Annual with varying numbers of days retention concurrently. Customer and/or Partner will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. Subscriptions with indeterminate pricing or usage quantities Product Usage Ratio Where a Customer subscription to the Products contain no specific pricing or quantities, Product-specific usage for a specific calendar month shall be determined through the calculation of the following ratio (the “Product Usage Ratio”) where: (1) the numerator shall be the Product specific usage for such calendar month multiplied by such Product’s list price, and (2) the denominator shall be the the aggregate Customer usage of all Products during such calendar month multiplied by all such Product(s) list price. The Product Usage Ratio shall be the percentage of usage for a specific Product for a calendar month period, or if usage cannot be measured for a period, the last Product Usage Ratio that can be calculated shall be assumed constant.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 191.28838,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Product-based pricing <em>usage</em> and New Relic Platform Pricing <em>Usage</em> <em>Plan</em>",
        "sections": "Product-based pricing <em>usage</em> and New Relic Platform Pricing <em>Usage</em> <em>Plan</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " reference the <em>usage</em> <em>plans</em> set forth below or where a subscription has indeterminate product pricing or <em>usage</em> quantities. New customers are eligible for New Relic One pricing as described here. Capitalized terms not defined below shall take on the meaning set forth in such New Relic order form. <em>Usage</em>"
      },
      "id": "603ea32a28ccbc7e22eba768"
    },
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Purpose",
        "2. Security Overview",
        "3.Security Certifications",
        "4. Data Control and Encryption",
        "5. Facilities",
        "6. Employee Access, Screening and Controls.",
        "7. Security Incident and Data Breach Response",
        "8. Network and Systems Security",
        "9. Authentication and Access Management",
        "10. Vulnerability Management",
        "11. System Access and Logging",
        "12. Disaster Recovery and Data Backup",
        "13. Copies and Removal",
        "14. Third Party Vendor Management",
        "15. Disclosure by Law",
        "16. Updates"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2021-10-18T14:36:56Z",
      "updated_at": "2021-09-27T15:05:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Updated 23 September 2021. The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Purpose This Policy describes New Relic's security program and the technical and organizational security controls to protect New Relic’s systems as shown in New Relic’s third-party audits and certifications. New Relic may update this Policy from time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/. 2. Security Overview 2.1. New Relic maintains a comprehensive Information Protection Program to manage information security within New Relic that includes administrative, technical, and physical safeguards designed to protect the confidentiality, integrity, and availability of Customer Data and that is appropriate to the nature, size, and complexity of New Relic's business operations. New Relic’s Information Protection Program includes: 2.1.1. executive review, support and accountability for all security related policies and practices; 2.1.2. formal written policies and procedures that are designed to protect against the loss, theft, or other unauthorized access or alteration of Customer Data, meet or exceed applicable industry standards, outlines a definition of information security and its overall objectives, include defined information security roles and responsibilities, a framework for setting control objectives and controls, a formal and effective risk mitigation program and a service provider security management program; 2.1.3. periodic risk assessments, including internal audits and/or independent 3rd party audits, to measure the effectiveness and appropriateness of controls for all systems processing Customer Data; 2.1.4. in-depth review of security incidents, including effective determination of root cause and corrective action; formal, industry-recognized controls frameworks based on an external audit standard; 2.1.5. employee screening and access management; and 2.1.6. comprehensive security testing, vulnerability identification, and mitigation methodology that consists of a variety of independent approaches that, when combined, are designed to maximize coverage for a diverse set of attack vectors. 2.2. New Relic's Chief Information Security Officer (CISO) leads New Relic's Information Security Program and develops, reviews, and approves, with appropriate stakeholders, New Relic's security policies and procedures. New Relic has appointed data protection officer(s) as described in New Relic’s Privacy Notices who are consulted as necessary under applicable laws. 2.3. Information security objectives, approach, scope, importance, goals, and principles for the organization’s security program are formally identified, communicated throughout the organization to users in a form that is relevant, accessible, and understandable to the intended reader; and supported by a controls framework that considers legislative, regulatory, contractual requirements, and other policy-related requirements. 2.4. New Relic will maintain written policies and procedures to review, test, and approve (as appropriate) changes affecting New Relic infrastructure and systems that process Customer Data including, but not limited to, peer reviews prior to introducing new code into production. New Relic will establish acceptance criteria for new information systems, upgrades, and new versions and will carry out suitable tests of the system(s) during development and prior to acceptance. Any changes or updates will not materially decrease the security of the systems. 3.Security Certifications 3.1. New Relic regularly tests, assesses, and evaluates its security measures for protecting Customer Data using industry-recognized standards and uses independent third-party auditors to verify such controls. 3.2. New Relic agrees to provide Customer, upon request, with applicable certifications or reports about New Relic systems. All information exchanged in connection with the audit activities described in this section is deemed to be the Confidential Information of New Relic. 3.3. Additional information about New Relic’s security certifications are available on New Relic’s Security Guide. 4. Data Control and Encryption 4.1. The Service and related features are designed to provide Customer with control over Customer’s data sources and Customer’s environments that are monitored and sending data to New Relic. 4.2. New Relic will have established methods to: (i) enable Customer to use encryption on Customer Data in transit, and (ii) securely hash passwords in storage following industry standard practices (e.g. scrypt) as described in the Documentation. Server certificate-based authentication is used as part of the TLS encryption with a trusted certificate authority. 4.3. New Relic receives and processes data in accordance with the Agreement and the Services as described in the Documentation. New Relic permits customers to delete personal data in accordance with applicable privacy laws as further described in the Documentation. In the event of error in personal data sent in Customer Data Customers may request personal data deletion and re-send data that is accurate. 4.4. Additional information for Customer data control and encryption, including encryption of data at rest, is available on New Relic’s Security Guide. 5. Facilities 5.1. All physical locations that process Customer Data are co-locations and third party data centers. All physical locations have security measures in place that are designed to prevent unauthorized physical access to data processing facilities and unlawful access, modification, or destruction of Customer Data. 5.2. New Relic will seek assurances (e.g., in the form of an independent 3rd party audit report such as the SOC 2 Type 2, ISO 27001, and vendor security evaluations) from its data processing facilities vendors that store or process Customer Data: 5.2.1. secure its data process facilities in an access-controlled location and have protections in place to prevent unauthorized access, damage, and interference; 5.2.2 employ physical security appropriate to the classification of the assets and information being managed which may include card key access, security cameras, and solid wall construction for all exterior walls; 5.2.3 limit and screen all entrants, which may include measures such as on-site security guard, badge reader, electronic lock, or a monitored closed caption television (CCTV); and 5.2.4 maintain relevant access logs. 5.3. Additional information about New Relic’s third party data centers are available on New Relic’s Security Guide. 6. Employee Access, Screening and Controls. New Relic will have and maintain policies and practices that include, at a minimum, the following controls and safeguards applied to New Relic employees and contractors who may access Customer Data: 6.1. For U.S. New Relic employees and subject to applicable law, a criminal background screening of each of its new employees to whom it gives access to Customer Data at the federal, state, and county levels. For non-U.S. New Relic employees, New Relic will use commercially reasonable efforts to meet the same criteria as established for U.S.-based New Relic employees, subject to general business practices in the respective country and in compliance with applicable local law requirements; 6.2. All New Relic employees with access to Customer Data will undergo adequate training, such as annual security awareness training, in the care, protection and handling of Customer Data, and will align with the privacy and security measures set out in this Addendum by following the guidance provided in their welcome package, which includes New Relic’s security policies and a security acknowledgement; 6.3. A disciplinary policy and process, to be used when New Relic employees violate New Relic security or privacy policies or access Customer Data without prior authorization; 6.4. Administrative or remote access to New Relic systems that process Customer Data will align with industry standard practices, including multi-factor authentication; 6.5. Restricted access to New Relic proprietary source code to prevent unauthorized access; 6.6. Controls designed to ensure that only those New Relic employees with an actual need-to-know will have access to New Relic systems including, but not limited to, the use of a formal access management process for the request, review, approval, and provisioning; 6.7. Controls designed to ensure that New Relic employees are granted access to New Relic systems based on least-privilege principles; and 6.8. Revoke access to New Relic employees no longer requiring access. 7. Security Incident and Data Breach Response 7.1. New Relic will take appropriate physical, technical, and administrative security measures that are commercially reasonable and consistent with industry standards to prevent a Data Breach, and as required by any applicable law or regulation. “Data Breach” means the theft, loss, or unauthorized access of Customer Data. Without limiting the foregoing, New Relic will implement security measures at least as stringent as those set out in this Addendum. New Relic will designate a senior representative to provide incident briefings, as needed in case of a Data Breach, and to respond to reasonable requests by Customer pertaining to privacy and data security issues within a commercially reasonable time frame. 7.2. New Relic will: 7.2.1. Notify Customer without undue delay if New Relic becomes aware of a Data Breach; 7.2.2. Maintain a security incident response plan, including procedures and means to respond in a manner consistent with industry standards; 7.2.3. Reasonably cooperate with Customer to investigate and remediate a Data Breach and mitigate any further risk to the Customer Data, or risk to data subjects and/or Customer’s reputation or brand; 7.2.4. Provide reasonable assistance to Customer at New Relic’s sole cost and expense; and 7.2.5. Make commercially reasonable efforts to preserve evidence and reasonably cooperate with Customer and legal authorities (as applicable and legally permissible) during an investigation of a Data Breach. 8. Network and Systems Security 8.1. All extranet connectivity by New Relic personnel to systems processing Customer Data will be through secure remote connections. 8.2. Network segments connected to the Internet will be protected by secure access control mechanisms, such as a firewall configured to secure all devices behind it and properly addresses security concerns according to industry standard practices. 8.3. New Relic will have industry standard measures in place to actively monitor its systems and help detect a potential hostile attack, such as Network Intrusion Detection (NID) or Host Intrusion Detection (HID)/Prevention. 8.4. Applications, ports, services, and similar access points installed on a computer or network facility, which are not specifically required for business functionality, will be disabled or removed. 8.5. New Relic maintains configuration standards for authorized operating systems and software for systems that support processing of Customer Data. New Relic will establish and follow server configuration guidelines and processes for preventing unauthorized access to Customer Data. New Relic maintains secure images or templates for systems based on the organization’s approved configuration standards. 8.6. Development, test, and operational environments will be logically separated to reduce the risks of unauthorized access or changes to the operational system. 8.7. New Relic network architecture will be designed to limit site access and restrict the availability of information systems that are considered to be vulnerable to attack. 8.8. New Relic provides appropriate TLS certificates when users access and view Customer Data in the Service. New Relic’s software for sending Customer Data to New Relic will encrypt Customer Data in transit by default. 9. Authentication and Access Management If you subscribe to the requisite New Relic One Service, New Relic will provide industry standard authentication and access controls to protect Customer Data, including industry standard authentication methods utilized to help prevent unauthorized access to the Service. New Relic’s access control methods will clearly state the rules and rights for each user or group of users including applications and information sharing and will include a process for granting and removing access to all information systems processing Customer Data. A record of all privileges allocated will be maintained pursuant to the requirements herein. 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. 10. Vulnerability Management New Relic will have and maintain the following vulnerability management processes for all devices used to connect to the New Relic network and Services. 10.1. New Relic will align to industry standard practices for build out, minimization of services and secure configuration, in accordance with, industry-recognized minimum security baselines for the New Relic platform and subcontractor systems connected to the New Relic platform in relation to the provision of the Services. 10.2. New Relic will employ industry-recognized standards and tools to conduct frequent infrastructure vulnerability scanning to test New Relic’s network and infrastructure and application penetration testing to test the New Relic Services. New Relic applies “Medium”, “High” and “Critical” security patches for all components in production and development environments as soon as commercially possible in accordance with its vulnerability management protocol, and consistent with industry standard practices and standards; 10.3. New Relic will have processes in place designed to ensure adherence to industry standard security development practices for development and testing for all code, APIs, and applications deployed and implemented in support of the Service; 10.4. New Relic will have and maintain solutions to identify and prevent malicious attackers or code from accessing or compromising Customer Data or systems that process Customer Data. These include, but are not limited to, software that identifies and removes malware and detects attempted intrusions. New Relic will have a security event and incident monitoring system and supporting processes to notify appropriate personnel in response to threats. 11. System Access and Logging 11.1. Access to New Relic’s systems will not be granted to employees of New Relic unless they have been uniquely identified and have sufficient credentials. 11.2. Access to New Relic’s systems will be logged and retained for no less than 6 months to assist in investigations and access control monitoring, including, but not limited to, end user access and activities, and information security events. 11.3. New Relic agrees to provide Customer the capability to access log records relating to Customer Accounts and the New Relic systems that process Customer Data in the event of a Data Breach or if required in connection with a law enforcement request. 12. Disaster Recovery and Data Backup 12.1. New Relic will have plans designed to respond to loss of services, which are tested and reviewed at least annually. This plan will include documented policies and procedures to restore service in the event of a service failure. 12.2. New Relic will establish and follow backup and restore procedures for Customer Data. 12.3. New Relic business continuity plan identifies critical systems. Annual disaster recovery tests are performed to check and restore customer data in the event of an incident. 12.4. New Relic will provide Customer with redacted copies of its plan(s) and evidence of tests/reviews upon request, but not more frequently than once annually, and subject to confidentiality requirements. 12.5. Additional information for Customer Data control and encryption are available on New Relic’s Security Guide. 13. Copies and Removal 13.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of the Agreement for any reason: (a) New Relic will, and will cause its personnel, to cease all access and use of any Customer Data, and (b) New Relic will delete all copies of Customer Data within ninety (90) days. 13.2. New Relic will maintain a process of ensuring secure destruction and deletion of all Customer Data, when reasonably requested by Customer or as otherwise provided in the Agreement. The process will include industry standard processes so that: (i) Customer Data cannot be practicably read or reconstructed, and (ii) the systems that store Customer Data are securely erased and/or decommissioned disks are destroyed. 14. Third Party Vendor Management New Relic may use third party vendors to provide the Services. New Relic performs a security risk-based assessment of prospective vendors before working with them to validate they meet New Relic’s security and business continuity standards, including the type of access and classification of data being accessed (if any), controls necessary to protect data, and legal/regulatory requirements. New Relic enters into written agreements with its vendors that process Customer Data which include confidentiality, privacy, and security obligations that provide an appropriate level of protection for Customer Data that these vendors may process for New Relic to maintain the security posture in this Policy, including following industry security standards. 15. Disclosure by Law In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic will (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. New Relic publishes its law enforcement requests report on New Relic’s Security Guide. 16. Updates As New Relic releases new products, services, functionality, and features, New Relic may update this Policy to account for such products, services, functionality, and features. For additional information, see our Security Guide.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 139.31372,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;referenced-policies&#x2F;security-policy&#x2F;. 2. Security Overview 2.1. New Relic"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    }
  ],
  "/docs/licenses/license-information/usage-plans/product-based-pricing-usage-new-relic-platform-pricing-usage-plan": [
    {
      "sections": [
        "New Relic data usage limits and policies",
        "View limits and manage data"
      ],
      "title": "New Relic data usage limits and policies",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "fc32c25b40a030ffa0fad6bfc95be7fca1360ee1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies/",
      "published_at": "2021-10-19T04:02:17Z",
      "updated_at": "2021-10-19T04:02:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers' accounts, we have various data volume and rate limits in place. We reserve the right to enforce these limits to protect our system and to avoid issues for you and other customers. If your New Relic account, whether by configuration or by error, exceeds one of these limits, it or its child accounts might experience one or both of the following: Sampling of data Temporary pause or cessation of data collection To learn more about how hitting a limit can affect your data, see View limits. If you have further questions about these limits, your contract, or a limit you've reached, contact your New Relic account representative. We can work with you to adjust any rate limits to meet your needs. View limits and manage data For information about system and account limits, and for links to data ingest API limits, go to View limits. To manage your data ingest, storage, and limits for organization or billing purposes, go to Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 213.52194,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic data <em>usage</em> limits and policies",
        "sections": "New Relic data <em>usage</em> limits and policies",
        "tags": "<em>License</em> <em>information</em>",
        "body": "We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data <em>usage</em> spikes in one New Relic account from impacting other customers&#x27; accounts, we have various data volume and rate limits in place. We reserve the right to enforce"
      },
      "id": "603eb1c528ccbc0311eba7c7"
    },
    {
      "sections": [
        "New Relic One usage plan descriptions",
        "Pay As You Go",
        "Annual Pool of Funds",
        "Applicable Invoicing and Order Terms",
        "User Accounts",
        "New Relic One Pro and Enterprise Service Level Availability Commitment",
        "New Relic One Pro and Enterprise Support Plans",
        "Free tier, ‘lite’, no-charge, preview access, New Relic One - Data, New Relic One - Standard User subscriptions",
        "Separate Platforms"
      ],
      "title": "New Relic One usage plan descriptions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Usage plans"
      ],
      "external_id": "c18e1c6c294914c28bba48f9de025333210ed254",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/usage-plans/new-relic-one-usage-plan-descriptions/",
      "published_at": "2021-10-18T14:38:24Z",
      "updated_at": "2021-08-08T23:13:31Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document is about the New Relic One pricing plan. For an explanation of how that plan works, see New Relic One pricing. The document below goes into license-level details. The Usage Plan applies to (i) your Pay As You Go subscription, or (ii) your Commitment Term for the Annual Pool of Funds subscription (see a description of these two plans). New Relic may modify the Usage Plan from time to time. Any changes to the Usage Plan will become effective immediately for changes that provide a benefit or right to the Customer, all other changes will become effective if Customer assents or upon any new or renewal Commitment Term. Usage Plan - Effective as February 19, 2021: The Order and Usage Plan may contain defined terms that are denoted by capitalization. In the event that a capitalized term is not defined in either the Order or the Usage Plan, such terms shall have the meaning set forth in the New Relic One pricing definitions page. Pay As You Go By electing and subscribing to the Pay As You Go subscription model (“Pay As You Go” or “PAYG”), Customer commits to paying for the New Relic Products on a month-to-month consumption basis. Monthly Product Usage will be invoiced regardless if a PO is required or not. Any Customer dispute to Monthly Product Usage from the prior month must be in good-faith and received by New Relic in writing within three (3) business days of the start of the next month or such dispute notice will be considered invalid. The dispute notice will set forth in reasonable detail the information concerning the disputed charges. The parties will use good-faith efforts to promptly resolve any disputed charges. Customer’s usage of the Products in excess of the Free Tier each month shall be billed in arrears on the first business day of the following month based on the Customer’s Per Unit usage of each Product each month multiplied by the corresponding rates set forth in an Order and summed (“Monthly Product Usage”). Annual Pool of Funds By electing and subscribing to the Annual Pool of Funds subscription model (“Annual Pool of Funds” or “APoF”) for the Commitment Term, Customer commits in an Order to: (i) paying the Commitment Fee amounts described in the Subscription table and any additional commitment fees set forth; and (ii) the Monthly Discounted Fee Rates applying to Customer’s Monthly Product Usage. New Relic will invoice the Commitment Fee as per the ‘Billing Terms’ described in an Order. On a monthly cadence during the Commitment Term, Customer’s Per Unit usage of the Products will be multiplied by the corresponding Monthly Discounted Rate and summed (“Monthly Product Usage”). Monthly Product Usage will be deducted from the Commitment Fee amounts that are paid in advance. If Monthly Product Usage exceeds any such remaining unconsumed amounts, Customer will be invoiced for the difference (“Additional Usage”), including for any Monthly Product Usage during the last month of the Commitment Term. Payment of such invoices will be governed as set forth in the Terms. Any Customer dispute to Monthly Product Usage from the prior month must be in good-faith and received by New Relic in writing within three (3) business days of the start of the next month (including for any Monthly Product Usage for the last month of the Commitment Term) or such dispute notice will be considered invalid. The dispute notice will set forth in reasonable detail the information concerning the disputed charges. The parties will use good-faith efforts to promptly resolve any disputed charges. Any unconsumed balances from the Customer payment of each annual Commitment Fee and any additional payments, if applicable, as set out in an Order will expire and lapse at the end of each year of the Commitment Term. Applicable Invoicing and Order Terms All amounts stated in an Order are non-cancelable payment obligations of the Customer for the Commitment Term regardless of usage. Any fees paid are non-refundable and do not represent a deposit for, or a credit towards, the purchase of other products not specified in an Order or for any purchase after the Commitment Term. Customer acknowledges that a final payment for the full outstanding amount of any remaining unpaid Commitment Fee and/or Monthly Product Usage Fee may be invoiced upon each anniversary date of the term start date. Tax will be added where applicable. New Relic may review Customer's use of the Products at any time. If New Relic identifies any Customer usage of the Product(s) that is not in accordance with the Terms or Documentation, New Relic may suspend such unauthorized usage. All existing purchases and related pricing in effect prior to the execution of an Order shall remain in force. Unless otherwise stated in an Order, an Order does not modify or amend any existing purchases. Additional future products or quantities are not subject to promotional pricing unless otherwise stated in an Order. In the event that a Customer indicates in an Order that it requires a purchase order (“PO”) for its subscription, Customer agrees to provide the required PO prior to the provisioning of the Products. If a Customer does not indicate a PO is required, Customer agrees that New Relic may issue invoice(s) and is entitled to such payment without a PO reference. All Additional Usage fees will be invoiced regardless of a PO requirement. The Product(s) are deemed accepted upon its provisioning. User Accounts Use of the Products require Customer users to create Login Credentials. Customer user Login Credential information must be accurate, current, and complete. New Relic’s use and collection of Login Credentials (in accordance with its General Data Privacy Notice) is for account and product management and support of its customers. Customer and Customer users must abide by the New Relic Acceptable Use Policy (AUP) and Login Credentials may not be shared. Each Customer user must have their own user account. Entry into an Order indicates your agreement that the amount of provisioned users (at the rate specified in the Order) applies in lieu of and supersedes any other amount of users of the Products that may be specified in the agreement between Customer and New Relic. New Relic One Pro and Enterprise Service Level Availability Commitment With a subscription to New Relic Full Stack Observability Pro or Enterprise Products, you agree that during the Commitment Term the applicable service level availability commitment set forth on the ‘Service level availability commitment’ page in the Documentation shall apply to the Products. For clarity, if your agreement with New Relic contains a different service level availability commitment or remedies, the above does not apply to your subscription to the New Relic Full Stack Observability Pro or Enterprise Products. If you subscribe to any other New Relic Products with New Relic One pricing, any service level availability commitment or related remedies contained within your agreement with New Relic are vacated and nullified, and New Relic will use commercially reasonable efforts to make New Relic Full Stack Observability Standard and Telemetry Data Platform available in line with industry standards. New Relic One Pro and Enterprise Support Plans With a subscription to New Relic One Users (Full Stack Observability), New Relic provides an updated support plan commitment. By subscribing to New Relic One Users (Full Stack Observability), you agree that during the Commitment Term the applicable Support Plan set forth on the ‘Support plan’ page in the Documentation shall apply in lieu of, and supersedes and replaces, any other support related commitments that may be contained within your agreement with New Relic. For New Relic K.K. customers (Japan), the above does not currently apply to the support offerings provided by New Relic to you. Free tier, ‘lite’, no-charge, preview access, New Relic One - Data, New Relic One - Standard User subscriptions If you are using New Relic’s Products in the free tier only, or on a no-charge, or a ‘lite’ or ‘preview access’ basis you agree that the Unpaid Terms of Service will apply to such Product usage and replace and supersede any other terms. In addition, if your subscription contains the New Relic One - Standard User (Full Stack Observability Standard) Product, you agree that the Paid Terms of Service will apply to your subscription to the Products set forth in an Order and replace and supersede any other terms. Separate Platforms Customer access to any separate platforms for purposes of applicability of governing terms, such as Community Cloud for Pixie, is subject to their own terms of service. For clarity, such other platforms do not form part of the New Relic One offering and New Relic One warranties, indemnities, etc., do not apply to use of any other platforms for purposes of applicability of governing terms.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.36697,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic One <em>usage</em> <em>plan</em> descriptions",
        "sections": "New Relic One <em>usage</em> <em>plan</em> descriptions",
        "tags": "<em>License</em> <em>information</em>",
        "body": "This document is about the New Relic One pricing <em>plan</em>. For an explanation of how that <em>plan</em> works, see New Relic One pricing. The document below goes into <em>license</em>-level details. The <em>Usage</em> <em>Plan</em> applies to (i) your Pay As You Go subscription, or (ii) your Commitment Term for the Annual Pool of Funds"
      },
      "id": "6044e74ee7b9d2a4515799c8"
    },
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Purpose",
        "2. Security Overview",
        "3.Security Certifications",
        "4. Data Control and Encryption",
        "5. Facilities",
        "6. Employee Access, Screening and Controls.",
        "7. Security Incident and Data Breach Response",
        "8. Network and Systems Security",
        "9. Authentication and Access Management",
        "10. Vulnerability Management",
        "11. System Access and Logging",
        "12. Disaster Recovery and Data Backup",
        "13. Copies and Removal",
        "14. Third Party Vendor Management",
        "15. Disclosure by Law",
        "16. Updates"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2021-10-18T14:36:56Z",
      "updated_at": "2021-09-27T15:05:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Updated 23 September 2021. The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Purpose This Policy describes New Relic's security program and the technical and organizational security controls to protect New Relic’s systems as shown in New Relic’s third-party audits and certifications. New Relic may update this Policy from time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/. 2. Security Overview 2.1. New Relic maintains a comprehensive Information Protection Program to manage information security within New Relic that includes administrative, technical, and physical safeguards designed to protect the confidentiality, integrity, and availability of Customer Data and that is appropriate to the nature, size, and complexity of New Relic's business operations. New Relic’s Information Protection Program includes: 2.1.1. executive review, support and accountability for all security related policies and practices; 2.1.2. formal written policies and procedures that are designed to protect against the loss, theft, or other unauthorized access or alteration of Customer Data, meet or exceed applicable industry standards, outlines a definition of information security and its overall objectives, include defined information security roles and responsibilities, a framework for setting control objectives and controls, a formal and effective risk mitigation program and a service provider security management program; 2.1.3. periodic risk assessments, including internal audits and/or independent 3rd party audits, to measure the effectiveness and appropriateness of controls for all systems processing Customer Data; 2.1.4. in-depth review of security incidents, including effective determination of root cause and corrective action; formal, industry-recognized controls frameworks based on an external audit standard; 2.1.5. employee screening and access management; and 2.1.6. comprehensive security testing, vulnerability identification, and mitigation methodology that consists of a variety of independent approaches that, when combined, are designed to maximize coverage for a diverse set of attack vectors. 2.2. New Relic's Chief Information Security Officer (CISO) leads New Relic's Information Security Program and develops, reviews, and approves, with appropriate stakeholders, New Relic's security policies and procedures. New Relic has appointed data protection officer(s) as described in New Relic’s Privacy Notices who are consulted as necessary under applicable laws. 2.3. Information security objectives, approach, scope, importance, goals, and principles for the organization’s security program are formally identified, communicated throughout the organization to users in a form that is relevant, accessible, and understandable to the intended reader; and supported by a controls framework that considers legislative, regulatory, contractual requirements, and other policy-related requirements. 2.4. New Relic will maintain written policies and procedures to review, test, and approve (as appropriate) changes affecting New Relic infrastructure and systems that process Customer Data including, but not limited to, peer reviews prior to introducing new code into production. New Relic will establish acceptance criteria for new information systems, upgrades, and new versions and will carry out suitable tests of the system(s) during development and prior to acceptance. Any changes or updates will not materially decrease the security of the systems. 3.Security Certifications 3.1. New Relic regularly tests, assesses, and evaluates its security measures for protecting Customer Data using industry-recognized standards and uses independent third-party auditors to verify such controls. 3.2. New Relic agrees to provide Customer, upon request, with applicable certifications or reports about New Relic systems. All information exchanged in connection with the audit activities described in this section is deemed to be the Confidential Information of New Relic. 3.3. Additional information about New Relic’s security certifications are available on New Relic’s Security Guide. 4. Data Control and Encryption 4.1. The Service and related features are designed to provide Customer with control over Customer’s data sources and Customer’s environments that are monitored and sending data to New Relic. 4.2. New Relic will have established methods to: (i) enable Customer to use encryption on Customer Data in transit, and (ii) securely hash passwords in storage following industry standard practices (e.g. scrypt) as described in the Documentation. Server certificate-based authentication is used as part of the TLS encryption with a trusted certificate authority. 4.3. New Relic receives and processes data in accordance with the Agreement and the Services as described in the Documentation. New Relic permits customers to delete personal data in accordance with applicable privacy laws as further described in the Documentation. In the event of error in personal data sent in Customer Data Customers may request personal data deletion and re-send data that is accurate. 4.4. Additional information for Customer data control and encryption, including encryption of data at rest, is available on New Relic’s Security Guide. 5. Facilities 5.1. All physical locations that process Customer Data are co-locations and third party data centers. All physical locations have security measures in place that are designed to prevent unauthorized physical access to data processing facilities and unlawful access, modification, or destruction of Customer Data. 5.2. New Relic will seek assurances (e.g., in the form of an independent 3rd party audit report such as the SOC 2 Type 2, ISO 27001, and vendor security evaluations) from its data processing facilities vendors that store or process Customer Data: 5.2.1. secure its data process facilities in an access-controlled location and have protections in place to prevent unauthorized access, damage, and interference; 5.2.2 employ physical security appropriate to the classification of the assets and information being managed which may include card key access, security cameras, and solid wall construction for all exterior walls; 5.2.3 limit and screen all entrants, which may include measures such as on-site security guard, badge reader, electronic lock, or a monitored closed caption television (CCTV); and 5.2.4 maintain relevant access logs. 5.3. Additional information about New Relic’s third party data centers are available on New Relic’s Security Guide. 6. Employee Access, Screening and Controls. New Relic will have and maintain policies and practices that include, at a minimum, the following controls and safeguards applied to New Relic employees and contractors who may access Customer Data: 6.1. For U.S. New Relic employees and subject to applicable law, a criminal background screening of each of its new employees to whom it gives access to Customer Data at the federal, state, and county levels. For non-U.S. New Relic employees, New Relic will use commercially reasonable efforts to meet the same criteria as established for U.S.-based New Relic employees, subject to general business practices in the respective country and in compliance with applicable local law requirements; 6.2. All New Relic employees with access to Customer Data will undergo adequate training, such as annual security awareness training, in the care, protection and handling of Customer Data, and will align with the privacy and security measures set out in this Addendum by following the guidance provided in their welcome package, which includes New Relic’s security policies and a security acknowledgement; 6.3. A disciplinary policy and process, to be used when New Relic employees violate New Relic security or privacy policies or access Customer Data without prior authorization; 6.4. Administrative or remote access to New Relic systems that process Customer Data will align with industry standard practices, including multi-factor authentication; 6.5. Restricted access to New Relic proprietary source code to prevent unauthorized access; 6.6. Controls designed to ensure that only those New Relic employees with an actual need-to-know will have access to New Relic systems including, but not limited to, the use of a formal access management process for the request, review, approval, and provisioning; 6.7. Controls designed to ensure that New Relic employees are granted access to New Relic systems based on least-privilege principles; and 6.8. Revoke access to New Relic employees no longer requiring access. 7. Security Incident and Data Breach Response 7.1. New Relic will take appropriate physical, technical, and administrative security measures that are commercially reasonable and consistent with industry standards to prevent a Data Breach, and as required by any applicable law or regulation. “Data Breach” means the theft, loss, or unauthorized access of Customer Data. Without limiting the foregoing, New Relic will implement security measures at least as stringent as those set out in this Addendum. New Relic will designate a senior representative to provide incident briefings, as needed in case of a Data Breach, and to respond to reasonable requests by Customer pertaining to privacy and data security issues within a commercially reasonable time frame. 7.2. New Relic will: 7.2.1. Notify Customer without undue delay if New Relic becomes aware of a Data Breach; 7.2.2. Maintain a security incident response plan, including procedures and means to respond in a manner consistent with industry standards; 7.2.3. Reasonably cooperate with Customer to investigate and remediate a Data Breach and mitigate any further risk to the Customer Data, or risk to data subjects and/or Customer’s reputation or brand; 7.2.4. Provide reasonable assistance to Customer at New Relic’s sole cost and expense; and 7.2.5. Make commercially reasonable efforts to preserve evidence and reasonably cooperate with Customer and legal authorities (as applicable and legally permissible) during an investigation of a Data Breach. 8. Network and Systems Security 8.1. All extranet connectivity by New Relic personnel to systems processing Customer Data will be through secure remote connections. 8.2. Network segments connected to the Internet will be protected by secure access control mechanisms, such as a firewall configured to secure all devices behind it and properly addresses security concerns according to industry standard practices. 8.3. New Relic will have industry standard measures in place to actively monitor its systems and help detect a potential hostile attack, such as Network Intrusion Detection (NID) or Host Intrusion Detection (HID)/Prevention. 8.4. Applications, ports, services, and similar access points installed on a computer or network facility, which are not specifically required for business functionality, will be disabled or removed. 8.5. New Relic maintains configuration standards for authorized operating systems and software for systems that support processing of Customer Data. New Relic will establish and follow server configuration guidelines and processes for preventing unauthorized access to Customer Data. New Relic maintains secure images or templates for systems based on the organization’s approved configuration standards. 8.6. Development, test, and operational environments will be logically separated to reduce the risks of unauthorized access or changes to the operational system. 8.7. New Relic network architecture will be designed to limit site access and restrict the availability of information systems that are considered to be vulnerable to attack. 8.8. New Relic provides appropriate TLS certificates when users access and view Customer Data in the Service. New Relic’s software for sending Customer Data to New Relic will encrypt Customer Data in transit by default. 9. Authentication and Access Management If you subscribe to the requisite New Relic One Service, New Relic will provide industry standard authentication and access controls to protect Customer Data, including industry standard authentication methods utilized to help prevent unauthorized access to the Service. New Relic’s access control methods will clearly state the rules and rights for each user or group of users including applications and information sharing and will include a process for granting and removing access to all information systems processing Customer Data. A record of all privileges allocated will be maintained pursuant to the requirements herein. 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. 10. Vulnerability Management New Relic will have and maintain the following vulnerability management processes for all devices used to connect to the New Relic network and Services. 10.1. New Relic will align to industry standard practices for build out, minimization of services and secure configuration, in accordance with, industry-recognized minimum security baselines for the New Relic platform and subcontractor systems connected to the New Relic platform in relation to the provision of the Services. 10.2. New Relic will employ industry-recognized standards and tools to conduct frequent infrastructure vulnerability scanning to test New Relic’s network and infrastructure and application penetration testing to test the New Relic Services. New Relic applies “Medium”, “High” and “Critical” security patches for all components in production and development environments as soon as commercially possible in accordance with its vulnerability management protocol, and consistent with industry standard practices and standards; 10.3. New Relic will have processes in place designed to ensure adherence to industry standard security development practices for development and testing for all code, APIs, and applications deployed and implemented in support of the Service; 10.4. New Relic will have and maintain solutions to identify and prevent malicious attackers or code from accessing or compromising Customer Data or systems that process Customer Data. These include, but are not limited to, software that identifies and removes malware and detects attempted intrusions. New Relic will have a security event and incident monitoring system and supporting processes to notify appropriate personnel in response to threats. 11. System Access and Logging 11.1. Access to New Relic’s systems will not be granted to employees of New Relic unless they have been uniquely identified and have sufficient credentials. 11.2. Access to New Relic’s systems will be logged and retained for no less than 6 months to assist in investigations and access control monitoring, including, but not limited to, end user access and activities, and information security events. 11.3. New Relic agrees to provide Customer the capability to access log records relating to Customer Accounts and the New Relic systems that process Customer Data in the event of a Data Breach or if required in connection with a law enforcement request. 12. Disaster Recovery and Data Backup 12.1. New Relic will have plans designed to respond to loss of services, which are tested and reviewed at least annually. This plan will include documented policies and procedures to restore service in the event of a service failure. 12.2. New Relic will establish and follow backup and restore procedures for Customer Data. 12.3. New Relic business continuity plan identifies critical systems. Annual disaster recovery tests are performed to check and restore customer data in the event of an incident. 12.4. New Relic will provide Customer with redacted copies of its plan(s) and evidence of tests/reviews upon request, but not more frequently than once annually, and subject to confidentiality requirements. 12.5. Additional information for Customer Data control and encryption are available on New Relic’s Security Guide. 13. Copies and Removal 13.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of the Agreement for any reason: (a) New Relic will, and will cause its personnel, to cease all access and use of any Customer Data, and (b) New Relic will delete all copies of Customer Data within ninety (90) days. 13.2. New Relic will maintain a process of ensuring secure destruction and deletion of all Customer Data, when reasonably requested by Customer or as otherwise provided in the Agreement. The process will include industry standard processes so that: (i) Customer Data cannot be practicably read or reconstructed, and (ii) the systems that store Customer Data are securely erased and/or decommissioned disks are destroyed. 14. Third Party Vendor Management New Relic may use third party vendors to provide the Services. New Relic performs a security risk-based assessment of prospective vendors before working with them to validate they meet New Relic’s security and business continuity standards, including the type of access and classification of data being accessed (if any), controls necessary to protect data, and legal/regulatory requirements. New Relic enters into written agreements with its vendors that process Customer Data which include confidentiality, privacy, and security obligations that provide an appropriate level of protection for Customer Data that these vendors may process for New Relic to maintain the security posture in this Policy, including following industry security standards. 15. Disclosure by Law In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic will (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed. New Relic publishes its law enforcement requests report on New Relic’s Security Guide. 16. Updates As New Relic releases new products, services, functionality, and features, New Relic may update this Policy to account for such products, services, functionality, and features. For additional information, see our Security Guide.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 139.31372,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": " time to time, and such updates will not materially reduce the overall protections set forth in this Policy. The then-current terms of this Policy are available at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;referenced-policies&#x2F;security-policy&#x2F;. 2. Security Overview 2.1. New Relic"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    }
  ],
  "/docs/licenses/product-or-service-licenses/miscellaneous/help-center-documentation-licenses": [
    {
      "sections": [
        "iOS application licenses"
      ],
      "title": "iOS application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "a6df56e363112e7387e6887f04381a36a5457e84",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/ios-application-licenses/",
      "published_at": "2021-10-18T14:40:40Z",
      "updated_at": "2021-09-27T15:25:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Copyright AFNetworking MIT Copyright © 2011-2020 Alamofire Software Foundation http://alamofire.org/ Ace BSD Copyright © 2010, Ajax.org B.V. ActiveLabel MIT Copyright © 2015 Optonaut Alamofire MIT Copyright © 2014-2021 Alamofire Software Foundation Analytics MIT Copyright © 2016 Segment.io, Inc. BBlock MIT Copyright © 2012 David Keegan BigNumber MIT Copyright © 2019 mkrd CDMarkdownKit MIT Copyright © 2016-2017 Christopher de Haan contact@christopherdehaan.me Charts Apache License, Version 2.0 Copyright © 2016-2019 Daniel Cohen Gindi & Philipp Jahoda ECSlidingViewController MIT Copyright © 2013 EdgeCase LoginManagerSDK New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. Mantle MIT Copyright © GitHub, Inc. All rights reserved. Copyright © 2012, Bitswift, Inc MetricMetadata New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NRCharts New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NVActivityIndicatorView MIT Copyright © 2016 Vinh Nguyen NewRelicAgent New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. ObjectMapper MIT Copyright © 2014 Hearst RadarKit New Relic License © 2010-2021 New Relic, Inc. All rights reserved. SSPullToRefresh MIT Copyright © 2012-2014 Sam Soffes, http://soff.es UICKeyChainStore MIT Copyright © 2011 kishikawa katsumi WidgetLibrary New Relic License © 2010-2021 New Relic, Inc. All rights reserved. XYPieChart MIT Copyright © 2012 Xiaoyang Feng, XYStudio.cc Yams MIT Copyright © 2016 JP Simard. jsTimezoneDetect MIT Copyright © 2012 Jon Nylander, project maintained at bitbucket.org lodash MIT Copyright © JS Foundation and other contributors js.foundation The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.92891,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "iOS application <em>licenses</em>",
        "sections": "iOS application <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Copyright"
      },
      "id": "603e9db1196a670e70a83df3"
    },
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2021-10-18T14:42:09Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 160.39954,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    },
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2021-10-18T14:45:13Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0 © 2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0 © 2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0 © 2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0 © 2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0 © 2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright © 2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright © 2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright © 1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright © 1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.73653,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Logs plugin <em>licenses</em>",
        "sections": "Logs plugin <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    }
  ],
  "/docs/licenses/product-or-service-licenses/mobile-app-licenses/android-application-licenses": [
    {
      "sections": [
        "iOS application licenses"
      ],
      "title": "iOS application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "a6df56e363112e7387e6887f04381a36a5457e84",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/ios-application-licenses/",
      "published_at": "2021-10-18T14:40:40Z",
      "updated_at": "2021-09-27T15:25:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Copyright AFNetworking MIT Copyright © 2011-2020 Alamofire Software Foundation http://alamofire.org/ Ace BSD Copyright © 2010, Ajax.org B.V. ActiveLabel MIT Copyright © 2015 Optonaut Alamofire MIT Copyright © 2014-2021 Alamofire Software Foundation Analytics MIT Copyright © 2016 Segment.io, Inc. BBlock MIT Copyright © 2012 David Keegan BigNumber MIT Copyright © 2019 mkrd CDMarkdownKit MIT Copyright © 2016-2017 Christopher de Haan contact@christopherdehaan.me Charts Apache License, Version 2.0 Copyright © 2016-2019 Daniel Cohen Gindi & Philipp Jahoda ECSlidingViewController MIT Copyright © 2013 EdgeCase LoginManagerSDK New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. Mantle MIT Copyright © GitHub, Inc. All rights reserved. Copyright © 2012, Bitswift, Inc MetricMetadata New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NRCharts New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NVActivityIndicatorView MIT Copyright © 2016 Vinh Nguyen NewRelicAgent New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. ObjectMapper MIT Copyright © 2014 Hearst RadarKit New Relic License © 2010-2021 New Relic, Inc. All rights reserved. SSPullToRefresh MIT Copyright © 2012-2014 Sam Soffes, http://soff.es UICKeyChainStore MIT Copyright © 2011 kishikawa katsumi WidgetLibrary New Relic License © 2010-2021 New Relic, Inc. All rights reserved. XYPieChart MIT Copyright © 2012 Xiaoyang Feng, XYStudio.cc Yams MIT Copyright © 2016 JP Simard. jsTimezoneDetect MIT Copyright © 2012 Jon Nylander, project maintained at bitbucket.org lodash MIT Copyright © JS Foundation and other contributors js.foundation The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 269.6194,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "iOS <em>application</em> <em>licenses</em>",
        "sections": "iOS <em>application</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic iOS <em>app</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Copyright"
      },
      "id": "603e9db1196a670e70a83df3"
    },
    {
      "sections": [
        "tvOS application licenses"
      ],
      "title": "tvOS application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "0b3ac8ec42cef00f5a4d3ddf354e4be38ad0595f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/tvos-application-licenses/",
      "published_at": "2021-10-18T14:39:53Z",
      "updated_at": "2021-05-05T16:26:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic for TV app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Copyright AFNetworking MIT Copyright © 2011-2020 Alamofire Software Foundation (http://alamofire.org/) Analytics MIT Copyright © 2016 Segment.io, Inc. CDMarkdownKit MIT Copyright © 2016-2017 Christopher de Haan contact@christopherdehaan.me LoginManagerSDK New Relic License © 2010-2021 New Relic, Inc. All rights reserved. Mantle MIT Copyright © GitHub, Inc. All rights reserved. Copyright © 2012, Bitswift, Inc MetricMetadata New Relic License © 2010-2021 New Relic, Inc. All rights reserved. NRCharts New Relic License © 2010-2021 New Relic, Inc. All rights reserved. NVActivityIndicatorView MIT Copyright © 2016 Vinh Nguyen NewRelicAgent New Relic License © 2010-2021 New Relic, Inc. All rights reserved. UICKeyChainStore MIT Copyright © 2011 kishikawa katsumi WidgetLibrary New Relic License © 2010-2021 New Relic, Inc. All rights reserved. XYPieChart MIT Copyright © 2012 Xiaoyang Feng, XYStudio.cc Yams MIT Copyright © 2016 JP Simard. iOS-fontawesome CC BY 3.0 & MIT Copyright © 2012 Alex Usbergo. All rights reserved. The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 210.95815,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "tvOS <em>application</em> <em>licenses</em>",
        "sections": "tvOS <em>application</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic for TV <em>app</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Copyright"
      },
      "id": "6072d619196a6795b664a75c"
    },
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2021-10-18T14:42:09Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.04977,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    }
  ],
  "/docs/licenses/product-or-service-licenses/mobile-app-licenses/ios-application-licenses": [
    {
      "sections": [
        "Android application licenses"
      ],
      "title": "Android application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "68c9bdc9dec6f02240f002494309519e41619f29",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/android-application-licenses/",
      "published_at": "2021-10-18T14:39:53Z",
      "updated_at": "2021-05-05T16:29:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Android app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Third Party Dependencies License Copyright Android-FlowLayout Apache 2.0 Copyright © 2011, Artem Votincev (apmem.org) AVLoadingIndicatorView Apache 2.0 Copyright © 2015, Jack Wang BottomNavigationViewEx MIT Copyright © 2017, ittianyu Butterknife Apache 2.0 Copyright © 2013, Jake Wharton Crouton Apache 2.0 Copyright © 2012 - 2014, Benjamin Weiss CWAC-SafeRoom Apache 2.0 The copyrights are owned by CommonsWare for things unique to this library and a combination of CommonsWare and the Android Open Source Project for code modified from the Architecture Components' Framework* set of classes. Dagger 2 Apache 2.0 Copyright © 2012, The Dagger Authors Dragtop Layout Apache 2.0 Copyright © 2015, chenupt EventBus Apache 2.0 Copyright © 2012-2017 Markus Junginger, greenrobot FlexibleAdapter Apache 2.0 Copyright © 2015-2018 Davide Steduto, Davidea Solutions Sprl Gson Apache 2.0 Copyright © 2008, Google Inc. markwon Apache 2.0 Copyright © 2019 Dimitry Ivanov (legal@noties.io) mockk Apache 2.0 Copyright © [ 2017] [ github.com/mockk] leakcanary Apache 2.0 Copyright © 2015 Square, Inc. mockito mockito MIT Copyright © 2007 Mockito contributors mosby Apache 2.0 Copyright © 2015 Hannes Dorfmann moshi Apache 2.0 Copyright © 2015 Square, Inc. MPAndroidChart Apache 2.0 Copyright © 2019 Philipp Jahoda New Relic Mobile Agent OKHttp Apache 2.0 Copyright © 2019 Square, Inc. okio Apache 2.0 Copyright © 2013 Square, Inc. Picasso Apache 2.0 Copyright © 2013 Square, Inc. RESTMock Apache 2.0 Copyright © 2016 Appflate.io Retrofit Apache 2.0 Copyright © 2013 Square, Inc. RxJava Apache 2.0 Copyright © 2016-present, RxJava Contributors. Segment IO MIT Copyright © 2016 Segment, Inc. Snackyaml Apache 2.0 Copyright © 2008, www.snakeyaml.org. StickyHeaders Apache 2.0 Copyright © 2014 Emil Sjölander TableView Apache 2.0 Copyright © 2017 Evren Coşkun Transitions-Everywhere Apache 2.0 The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 210.95825,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Android <em>application</em> <em>licenses</em>",
        "sections": "Android <em>application</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic Android <em>app</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Third Party Dependencies"
      },
      "id": "603e9e30196a67b71fa83d96"
    },
    {
      "sections": [
        "tvOS application licenses"
      ],
      "title": "tvOS application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "0b3ac8ec42cef00f5a4d3ddf354e4be38ad0595f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/tvos-application-licenses/",
      "published_at": "2021-10-18T14:39:53Z",
      "updated_at": "2021-05-05T16:26:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic for TV app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Copyright AFNetworking MIT Copyright © 2011-2020 Alamofire Software Foundation (http://alamofire.org/) Analytics MIT Copyright © 2016 Segment.io, Inc. CDMarkdownKit MIT Copyright © 2016-2017 Christopher de Haan contact@christopherdehaan.me LoginManagerSDK New Relic License © 2010-2021 New Relic, Inc. All rights reserved. Mantle MIT Copyright © GitHub, Inc. All rights reserved. Copyright © 2012, Bitswift, Inc MetricMetadata New Relic License © 2010-2021 New Relic, Inc. All rights reserved. NRCharts New Relic License © 2010-2021 New Relic, Inc. All rights reserved. NVActivityIndicatorView MIT Copyright © 2016 Vinh Nguyen NewRelicAgent New Relic License © 2010-2021 New Relic, Inc. All rights reserved. UICKeyChainStore MIT Copyright © 2011 kishikawa katsumi WidgetLibrary New Relic License © 2010-2021 New Relic, Inc. All rights reserved. XYPieChart MIT Copyright © 2012 Xiaoyang Feng, XYStudio.cc Yams MIT Copyright © 2016 JP Simard. iOS-fontawesome CC BY 3.0 & MIT Copyright © 2012 Alex Usbergo. All rights reserved. The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 210.95815,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "tvOS <em>application</em> <em>licenses</em>",
        "sections": "tvOS <em>application</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic for TV <em>app</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Copyright"
      },
      "id": "6072d619196a6795b664a75c"
    },
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2021-10-18T14:42:09Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.04976,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    }
  ],
  "/docs/licenses/product-or-service-licenses/mobile-app-licenses/tvos-application-licenses": [
    {
      "sections": [
        "iOS application licenses"
      ],
      "title": "iOS application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "a6df56e363112e7387e6887f04381a36a5457e84",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/ios-application-licenses/",
      "published_at": "2021-10-18T14:40:40Z",
      "updated_at": "2021-09-27T15:25:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Copyright AFNetworking MIT Copyright © 2011-2020 Alamofire Software Foundation http://alamofire.org/ Ace BSD Copyright © 2010, Ajax.org B.V. ActiveLabel MIT Copyright © 2015 Optonaut Alamofire MIT Copyright © 2014-2021 Alamofire Software Foundation Analytics MIT Copyright © 2016 Segment.io, Inc. BBlock MIT Copyright © 2012 David Keegan BigNumber MIT Copyright © 2019 mkrd CDMarkdownKit MIT Copyright © 2016-2017 Christopher de Haan contact@christopherdehaan.me Charts Apache License, Version 2.0 Copyright © 2016-2019 Daniel Cohen Gindi & Philipp Jahoda ECSlidingViewController MIT Copyright © 2013 EdgeCase LoginManagerSDK New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. Mantle MIT Copyright © GitHub, Inc. All rights reserved. Copyright © 2012, Bitswift, Inc MetricMetadata New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NRCharts New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NVActivityIndicatorView MIT Copyright © 2016 Vinh Nguyen NewRelicAgent New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. ObjectMapper MIT Copyright © 2014 Hearst RadarKit New Relic License © 2010-2021 New Relic, Inc. All rights reserved. SSPullToRefresh MIT Copyright © 2012-2014 Sam Soffes, http://soff.es UICKeyChainStore MIT Copyright © 2011 kishikawa katsumi WidgetLibrary New Relic License © 2010-2021 New Relic, Inc. All rights reserved. XYPieChart MIT Copyright © 2012 Xiaoyang Feng, XYStudio.cc Yams MIT Copyright © 2016 JP Simard. jsTimezoneDetect MIT Copyright © 2012 Jon Nylander, project maintained at bitbucket.org lodash MIT Copyright © JS Foundation and other contributors js.foundation The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 269.61932,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "iOS <em>application</em> <em>licenses</em>",
        "sections": "iOS <em>application</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic iOS <em>app</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Copyright"
      },
      "id": "603e9db1196a670e70a83df3"
    },
    {
      "sections": [
        "Android application licenses"
      ],
      "title": "Android application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "68c9bdc9dec6f02240f002494309519e41619f29",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/android-application-licenses/",
      "published_at": "2021-10-18T14:39:53Z",
      "updated_at": "2021-05-05T16:29:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Android app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Third Party Dependencies License Copyright Android-FlowLayout Apache 2.0 Copyright © 2011, Artem Votincev (apmem.org) AVLoadingIndicatorView Apache 2.0 Copyright © 2015, Jack Wang BottomNavigationViewEx MIT Copyright © 2017, ittianyu Butterknife Apache 2.0 Copyright © 2013, Jake Wharton Crouton Apache 2.0 Copyright © 2012 - 2014, Benjamin Weiss CWAC-SafeRoom Apache 2.0 The copyrights are owned by CommonsWare for things unique to this library and a combination of CommonsWare and the Android Open Source Project for code modified from the Architecture Components' Framework* set of classes. Dagger 2 Apache 2.0 Copyright © 2012, The Dagger Authors Dragtop Layout Apache 2.0 Copyright © 2015, chenupt EventBus Apache 2.0 Copyright © 2012-2017 Markus Junginger, greenrobot FlexibleAdapter Apache 2.0 Copyright © 2015-2018 Davide Steduto, Davidea Solutions Sprl Gson Apache 2.0 Copyright © 2008, Google Inc. markwon Apache 2.0 Copyright © 2019 Dimitry Ivanov (legal@noties.io) mockk Apache 2.0 Copyright © [ 2017] [ github.com/mockk] leakcanary Apache 2.0 Copyright © 2015 Square, Inc. mockito mockito MIT Copyright © 2007 Mockito contributors mosby Apache 2.0 Copyright © 2015 Hannes Dorfmann moshi Apache 2.0 Copyright © 2015 Square, Inc. MPAndroidChart Apache 2.0 Copyright © 2019 Philipp Jahoda New Relic Mobile Agent OKHttp Apache 2.0 Copyright © 2019 Square, Inc. okio Apache 2.0 Copyright © 2013 Square, Inc. Picasso Apache 2.0 Copyright © 2013 Square, Inc. RESTMock Apache 2.0 Copyright © 2016 Appflate.io Retrofit Apache 2.0 Copyright © 2013 Square, Inc. RxJava Apache 2.0 Copyright © 2016-present, RxJava Contributors. Segment IO MIT Copyright © 2016 Segment, Inc. Snackyaml Apache 2.0 Copyright © 2008, www.snakeyaml.org. StickyHeaders Apache 2.0 Copyright © 2014 Emil Sjölander TableView Apache 2.0 Copyright © 2017 Evren Coşkun Transitions-Everywhere Apache 2.0 The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 210.95825,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Android <em>application</em> <em>licenses</em>",
        "sections": "Android <em>application</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic Android <em>app</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Third Party Dependencies"
      },
      "id": "603e9e30196a67b71fa83d96"
    },
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2021-10-18T14:42:09Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.04974,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-apm/apm-agent-sdk-licenses": [
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2021-10-18T14:42:09Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 228.7847,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    },
    {
      "sections": [
        "New Relic APM licenses"
      ],
      "title": "New Relic APM licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "e08100d5a48d87c7769f401957826f627bf29779",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses/",
      "published_at": "2021-10-18T14:42:52Z",
      "updated_at": "2021-03-30T21:22:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view licenses for... See... APM agents docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.36266,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "sections": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We use many third-party libraries and tools to create <em>New</em> <em>Relic</em> <em>APM</em>, including a large number contributed by the open source community. To view <em>licenses</em> for... See... <em>APM</em> agents docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>product</em>-or-<em>service</em>-<em>licenses</em>&#x2F;<em>new</em>-<em>relic</em>-<em>apm</em>"
      },
      "id": "603e7895e7b9d24e832a07d5"
    },
    {
      "sections": [
        "Go agent licenses"
      ],
      "title": "Go agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "08a72af2529390cf6296870147ce00b64ea7f633",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/go-agent-licenses/",
      "published_at": "2021-10-18T14:41:24Z",
      "updated_at": "2021-03-16T04:25:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Go Go BSD The remainder of the code is covered by the New Relic agent license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.46028,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Go agent <em>licenses</em>",
        "sections": "Go agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Go Go BSD The remainder of the code is covered by the <em>New</em> <em>Relic</em> agent license agreement."
      },
      "id": "603ea506196a67b2b3a83dee"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-apm/c-sdk-licenses": [
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2021-10-18T14:42:09Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 228.78468,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    },
    {
      "sections": [
        "New Relic APM licenses"
      ],
      "title": "New Relic APM licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "e08100d5a48d87c7769f401957826f627bf29779",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses/",
      "published_at": "2021-10-18T14:42:52Z",
      "updated_at": "2021-03-30T21:22:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view licenses for... See... APM agents docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.36266,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "sections": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We use many third-party libraries and tools to create <em>New</em> <em>Relic</em> <em>APM</em>, including a large number contributed by the open source community. To view <em>licenses</em> for... See... <em>APM</em> agents docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>product</em>-or-<em>service</em>-<em>licenses</em>&#x2F;<em>new</em>-<em>relic</em>-<em>apm</em>"
      },
      "id": "603e7895e7b9d24e832a07d5"
    },
    {
      "sections": [
        "Go agent licenses"
      ],
      "title": "Go agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "08a72af2529390cf6296870147ce00b64ea7f633",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/go-agent-licenses/",
      "published_at": "2021-10-18T14:41:24Z",
      "updated_at": "2021-03-16T04:25:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Go Go BSD The remainder of the code is covered by the New Relic agent license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.46028,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Go agent <em>licenses</em>",
        "sections": "Go agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Go Go BSD The remainder of the code is covered by the <em>New</em> <em>Relic</em> agent license agreement."
      },
      "id": "603ea506196a67b2b3a83dee"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-apm/go-agent-licenses": [
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2021-10-18T14:42:09Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 228.78468,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    },
    {
      "sections": [
        "New Relic APM licenses"
      ],
      "title": "New Relic APM licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "e08100d5a48d87c7769f401957826f627bf29779",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses/",
      "published_at": "2021-10-18T14:42:52Z",
      "updated_at": "2021-03-30T21:22:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view licenses for... See... APM agents docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.36266,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "sections": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We use many third-party libraries and tools to create <em>New</em> <em>Relic</em> <em>APM</em>, including a large number contributed by the open source community. To view <em>licenses</em> for... See... <em>APM</em> agents docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>product</em>-or-<em>service</em>-<em>licenses</em>&#x2F;<em>new</em>-<em>relic</em>-<em>apm</em>"
      },
      "id": "603e7895e7b9d24e832a07d5"
    },
    {
      "sections": [
        ".NET agent: Microsoft Azure Portal Extension licenses"
      ],
      "title": ".NET agent: Microsoft Azure Portal Extension licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "19d61cf14551e1be8895993d42d2c640d7cd238b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/net-agent-microsoft-azure-portal-extension-licenses/",
      "published_at": "2021-10-18T14:42:10Z",
      "updated_at": "2021-03-13T03:35:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the Microsoft Azure Portal Extension. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Antlr BSD JSON.NET MIT PowerArgs MIT The remainder of the code is covered by the New Relic License agreement found in the LICENSE file in the distribution.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.35324,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": ".NET agent: Microsoft Azure Portal Extension <em>licenses</em>",
        "sections": ".NET agent: Microsoft Azure Portal Extension <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": " Antlr BSD JSON.NET MIT PowerArgs MIT The remainder of the code is covered by the <em>New</em> <em>Relic</em> License agreement found in the LICENSE file in the distribution."
      },
      "id": "6044e7bb64441f6e4e378f1a"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses": [
    {
      "sections": [
        "New Relic APM licenses"
      ],
      "title": "New Relic APM licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "e08100d5a48d87c7769f401957826f627bf29779",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses/",
      "published_at": "2021-10-18T14:42:52Z",
      "updated_at": "2021-03-30T21:22:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view licenses for... See... APM agents docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.36266,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "sections": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We use many third-party libraries and tools to create <em>New</em> <em>Relic</em> <em>APM</em>, including a large number contributed by the open source community. To view <em>licenses</em> for... See... <em>APM</em> agents docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>product</em>-or-<em>service</em>-<em>licenses</em>&#x2F;<em>new</em>-<em>relic</em>-<em>apm</em>"
      },
      "id": "603e7895e7b9d24e832a07d5"
    },
    {
      "sections": [
        "Go agent licenses"
      ],
      "title": "Go agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "08a72af2529390cf6296870147ce00b64ea7f633",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/go-agent-licenses/",
      "published_at": "2021-10-18T14:41:24Z",
      "updated_at": "2021-03-16T04:25:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Go Go BSD The remainder of the code is covered by the New Relic agent license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.46028,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Go agent <em>licenses</em>",
        "sections": "Go agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Go Go BSD The remainder of the code is covered by the <em>New</em> <em>Relic</em> agent license agreement."
      },
      "id": "603ea506196a67b2b3a83dee"
    },
    {
      "sections": [
        ".NET agent: Microsoft Azure Portal Extension licenses"
      ],
      "title": ".NET agent: Microsoft Azure Portal Extension licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "19d61cf14551e1be8895993d42d2c640d7cd238b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/net-agent-microsoft-azure-portal-extension-licenses/",
      "published_at": "2021-10-18T14:42:10Z",
      "updated_at": "2021-03-13T03:35:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the Microsoft Azure Portal Extension. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Antlr BSD JSON.NET MIT PowerArgs MIT The remainder of the code is covered by the New Relic License agreement found in the LICENSE file in the distribution.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.35324,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": ".NET agent: Microsoft Azure Portal Extension <em>licenses</em>",
        "sections": ".NET agent: Microsoft Azure Portal Extension <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": " Antlr BSD JSON.NET MIT PowerArgs MIT The remainder of the code is covered by the <em>New</em> <em>Relic</em> License agreement found in the LICENSE file in the distribution."
      },
      "id": "6044e7bb64441f6e4e378f1a"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-apm/net-agent-licenses": [
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2021-10-18T14:42:09Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 228.78465,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    },
    {
      "sections": [
        "New Relic APM licenses"
      ],
      "title": "New Relic APM licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "e08100d5a48d87c7769f401957826f627bf29779",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses/",
      "published_at": "2021-10-18T14:42:52Z",
      "updated_at": "2021-03-30T21:22:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view licenses for... See... APM agents docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.36266,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "sections": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We use many third-party libraries and tools to create <em>New</em> <em>Relic</em> <em>APM</em>, including a large number contributed by the open source community. To view <em>licenses</em> for... See... <em>APM</em> agents docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>product</em>-or-<em>service</em>-<em>licenses</em>&#x2F;<em>new</em>-<em>relic</em>-<em>apm</em>"
      },
      "id": "603e7895e7b9d24e832a07d5"
    },
    {
      "sections": [
        "Go agent licenses"
      ],
      "title": "Go agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "08a72af2529390cf6296870147ce00b64ea7f633",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/go-agent-licenses/",
      "published_at": "2021-10-18T14:41:24Z",
      "updated_at": "2021-03-16T04:25:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Go Go BSD The remainder of the code is covered by the New Relic agent license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.46028,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Go agent <em>licenses</em>",
        "sections": "Go agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Go Go BSD The remainder of the code is covered by the <em>New</em> <em>Relic</em> agent license agreement."
      },
      "id": "603ea506196a67b2b3a83dee"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-apm/net-agent-microsoft-azure-portal-extension-licenses": [
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2021-10-18T14:42:09Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 228.78465,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    },
    {
      "sections": [
        "New Relic APM licenses"
      ],
      "title": "New Relic APM licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "e08100d5a48d87c7769f401957826f627bf29779",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses/",
      "published_at": "2021-10-18T14:42:52Z",
      "updated_at": "2021-03-30T21:22:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view licenses for... See... APM agents docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.36266,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "sections": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We use many third-party libraries and tools to create <em>New</em> <em>Relic</em> <em>APM</em>, including a large number contributed by the open source community. To view <em>licenses</em> for... See... <em>APM</em> agents docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>product</em>-or-<em>service</em>-<em>licenses</em>&#x2F;<em>new</em>-<em>relic</em>-<em>apm</em>"
      },
      "id": "603e7895e7b9d24e832a07d5"
    },
    {
      "sections": [
        "Go agent licenses"
      ],
      "title": "Go agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "08a72af2529390cf6296870147ce00b64ea7f633",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/go-agent-licenses/",
      "published_at": "2021-10-18T14:41:24Z",
      "updated_at": "2021-03-16T04:25:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Go Go BSD The remainder of the code is covered by the New Relic agent license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.46028,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Go agent <em>licenses</em>",
        "sections": "Go agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Go Go BSD The remainder of the code is covered by the <em>New</em> <em>Relic</em> agent license agreement."
      },
      "id": "603ea506196a67b2b3a83dee"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-apm/net-agent-microsoft-azure-portal-resource-provider-licenses": [
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2021-10-18T14:42:09Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 228.78462,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    },
    {
      "sections": [
        "New Relic APM licenses"
      ],
      "title": "New Relic APM licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "e08100d5a48d87c7769f401957826f627bf29779",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses/",
      "published_at": "2021-10-18T14:42:52Z",
      "updated_at": "2021-03-30T21:22:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view licenses for... See... APM agents docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.36266,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "sections": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We use many third-party libraries and tools to create <em>New</em> <em>Relic</em> <em>APM</em>, including a large number contributed by the open source community. To view <em>licenses</em> for... See... <em>APM</em> agents docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>product</em>-or-<em>service</em>-<em>licenses</em>&#x2F;<em>new</em>-<em>relic</em>-<em>apm</em>"
      },
      "id": "603e7895e7b9d24e832a07d5"
    },
    {
      "sections": [
        "Go agent licenses"
      ],
      "title": "Go agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "08a72af2529390cf6296870147ce00b64ea7f633",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/go-agent-licenses/",
      "published_at": "2021-10-18T14:41:24Z",
      "updated_at": "2021-03-16T04:25:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Go Go BSD The remainder of the code is covered by the New Relic agent license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.46028,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Go agent <em>licenses</em>",
        "sections": "Go agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Go Go BSD The remainder of the code is covered by the <em>New</em> <em>Relic</em> agent license agreement."
      },
      "id": "603ea506196a67b2b3a83dee"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses": [
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2021-10-18T14:42:09Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 228.78462,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    },
    {
      "sections": [
        "Go agent licenses"
      ],
      "title": "Go agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "08a72af2529390cf6296870147ce00b64ea7f633",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/go-agent-licenses/",
      "published_at": "2021-10-18T14:41:24Z",
      "updated_at": "2021-03-16T04:25:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Go Go BSD The remainder of the code is covered by the New Relic agent license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.46028,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Go agent <em>licenses</em>",
        "sections": "Go agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Go Go BSD The remainder of the code is covered by the <em>New</em> <em>Relic</em> agent license agreement."
      },
      "id": "603ea506196a67b2b3a83dee"
    },
    {
      "sections": [
        ".NET agent: Microsoft Azure Portal Extension licenses"
      ],
      "title": ".NET agent: Microsoft Azure Portal Extension licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "19d61cf14551e1be8895993d42d2c640d7cd238b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/net-agent-microsoft-azure-portal-extension-licenses/",
      "published_at": "2021-10-18T14:42:10Z",
      "updated_at": "2021-03-13T03:35:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the Microsoft Azure Portal Extension. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Antlr BSD JSON.NET MIT PowerArgs MIT The remainder of the code is covered by the New Relic License agreement found in the LICENSE file in the distribution.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.35324,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": ".NET agent: Microsoft Azure Portal Extension <em>licenses</em>",
        "sections": ".NET agent: Microsoft Azure Portal Extension <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": " Antlr BSD JSON.NET MIT PowerArgs MIT The remainder of the code is covered by the <em>New</em> <em>Relic</em> License agreement found in the LICENSE file in the distribution."
      },
      "id": "6044e7bb64441f6e4e378f1a"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-apm/nodejs-agent-licenses": [
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2021-10-18T14:42:09Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 228.78459,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    },
    {
      "sections": [
        "New Relic APM licenses"
      ],
      "title": "New Relic APM licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "e08100d5a48d87c7769f401957826f627bf29779",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses/",
      "published_at": "2021-10-18T14:42:52Z",
      "updated_at": "2021-03-30T21:22:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view licenses for... See... APM agents docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.36266,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "sections": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We use many third-party libraries and tools to create <em>New</em> <em>Relic</em> <em>APM</em>, including a large number contributed by the open source community. To view <em>licenses</em> for... See... <em>APM</em> agents docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>product</em>-or-<em>service</em>-<em>licenses</em>&#x2F;<em>new</em>-<em>relic</em>-<em>apm</em>"
      },
      "id": "603e7895e7b9d24e832a07d5"
    },
    {
      "sections": [
        "Go agent licenses"
      ],
      "title": "Go agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "08a72af2529390cf6296870147ce00b64ea7f633",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/go-agent-licenses/",
      "published_at": "2021-10-18T14:41:24Z",
      "updated_at": "2021-03-16T04:25:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Go Go BSD The remainder of the code is covered by the New Relic agent license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.46028,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Go agent <em>licenses</em>",
        "sections": "Go agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Go Go BSD The remainder of the code is covered by the <em>New</em> <em>Relic</em> agent license agreement."
      },
      "id": "603ea506196a67b2b3a83dee"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-apm/php-agent-licenses": [
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2021-10-18T14:42:09Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 228.78459,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    },
    {
      "sections": [
        "New Relic APM licenses"
      ],
      "title": "New Relic APM licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "e08100d5a48d87c7769f401957826f627bf29779",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses/",
      "published_at": "2021-10-18T14:42:52Z",
      "updated_at": "2021-03-30T21:22:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view licenses for... See... APM agents docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.36266,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "sections": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We use many third-party libraries and tools to create <em>New</em> <em>Relic</em> <em>APM</em>, including a large number contributed by the open source community. To view <em>licenses</em> for... See... <em>APM</em> agents docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>product</em>-or-<em>service</em>-<em>licenses</em>&#x2F;<em>new</em>-<em>relic</em>-<em>apm</em>"
      },
      "id": "603e7895e7b9d24e832a07d5"
    },
    {
      "sections": [
        "Go agent licenses"
      ],
      "title": "Go agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "08a72af2529390cf6296870147ce00b64ea7f633",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/go-agent-licenses/",
      "published_at": "2021-10-18T14:41:24Z",
      "updated_at": "2021-03-16T04:25:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Go Go BSD The remainder of the code is covered by the New Relic agent license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.46028,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Go agent <em>licenses</em>",
        "sections": "Go agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Go Go BSD The remainder of the code is covered by the <em>New</em> <em>Relic</em> agent license agreement."
      },
      "id": "603ea506196a67b2b3a83dee"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-apm/python-agent-licenses": [
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2021-10-18T14:42:09Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 228.78456,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    },
    {
      "sections": [
        "New Relic APM licenses"
      ],
      "title": "New Relic APM licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "e08100d5a48d87c7769f401957826f627bf29779",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses/",
      "published_at": "2021-10-18T14:42:52Z",
      "updated_at": "2021-03-30T21:22:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view licenses for... See... APM agents docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.36266,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "sections": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We use many third-party libraries and tools to create <em>New</em> <em>Relic</em> <em>APM</em>, including a large number contributed by the open source community. To view <em>licenses</em> for... See... <em>APM</em> agents docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>product</em>-or-<em>service</em>-<em>licenses</em>&#x2F;<em>new</em>-<em>relic</em>-<em>apm</em>"
      },
      "id": "603e7895e7b9d24e832a07d5"
    },
    {
      "sections": [
        "Go agent licenses"
      ],
      "title": "Go agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "08a72af2529390cf6296870147ce00b64ea7f633",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/go-agent-licenses/",
      "published_at": "2021-10-18T14:41:24Z",
      "updated_at": "2021-03-16T04:25:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Go Go BSD The remainder of the code is covered by the New Relic agent license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.46028,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Go agent <em>licenses</em>",
        "sections": "Go agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Go Go BSD The remainder of the code is covered by the <em>New</em> <em>Relic</em> agent license agreement."
      },
      "id": "603ea506196a67b2b3a83dee"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-apm/ruby-agent-licenses": [
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2021-10-18T14:42:09Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 228.78456,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    },
    {
      "sections": [
        "New Relic APM licenses"
      ],
      "title": "New Relic APM licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "e08100d5a48d87c7769f401957826f627bf29779",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses/",
      "published_at": "2021-10-18T14:42:52Z",
      "updated_at": "2021-03-30T21:22:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view licenses for... See... APM agents docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.36266,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "sections": "<em>New</em> <em>Relic</em> <em>APM</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We use many third-party libraries and tools to create <em>New</em> <em>Relic</em> <em>APM</em>, including a large number contributed by the open source community. To view <em>licenses</em> for... See... <em>APM</em> agents docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>product</em>-or-<em>service</em>-<em>licenses</em>&#x2F;<em>new</em>-<em>relic</em>-<em>apm</em>"
      },
      "id": "603e7895e7b9d24e832a07d5"
    },
    {
      "sections": [
        "Go agent licenses"
      ],
      "title": "Go agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "08a72af2529390cf6296870147ce00b64ea7f633",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/go-agent-licenses/",
      "published_at": "2021-10-18T14:41:24Z",
      "updated_at": "2021-03-16T04:25:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Go Go BSD The remainder of the code is covered by the New Relic agent license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.46028,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Go agent <em>licenses</em>",
        "sections": "Go agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Go agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Go Go BSD The remainder of the code is covered by the <em>New</em> <em>Relic</em> agent license agreement."
      },
      "id": "603ea506196a67b2b3a83dee"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-browser/browser-agent-licenses": [
    {
      "sections": [
        "New Relic Browser licenses",
        "UI tier"
      ],
      "title": "New Relic Browser licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Browser"
      ],
      "external_id": "b308c950e31570ee201237a88bfb8891da10a23c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-browser/new-relic-browser-licenses/",
      "published_at": "2021-10-18T14:39:11Z",
      "updated_at": "2021-03-16T04:45:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Browser. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. UI tier Library License DataStax Cassandra Java Driver Apache 2.0 d3 BSD Finagle Apache 2.0 gulp-rename MIT Jackson Apache 2.0 nee ISC nscala-time Apache 2.0 photocopy ISC Rapture Apache 2.0 React Apache 2.0 require.dir MIT Scrooge Apache 2.0 slf4j MIT snappy-java Apache 2.0 TwitterServer Apache 2.0 Typesafe Config Apache 2.0",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.91907,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>New</em> <em>Relic</em> <em>Browser</em> <em>licenses</em>",
        "sections": "<em>New</em> <em>Relic</em> <em>Browser</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> <em>Browser</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. UI tier Library License DataStax"
      },
      "id": "603ece91e7b9d2c9d02a07c2"
    },
    {
      "sections": [
        "iOS application licenses"
      ],
      "title": "iOS application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "a6df56e363112e7387e6887f04381a36a5457e84",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/ios-application-licenses/",
      "published_at": "2021-10-18T14:40:40Z",
      "updated_at": "2021-09-27T15:25:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Copyright AFNetworking MIT Copyright © 2011-2020 Alamofire Software Foundation http://alamofire.org/ Ace BSD Copyright © 2010, Ajax.org B.V. ActiveLabel MIT Copyright © 2015 Optonaut Alamofire MIT Copyright © 2014-2021 Alamofire Software Foundation Analytics MIT Copyright © 2016 Segment.io, Inc. BBlock MIT Copyright © 2012 David Keegan BigNumber MIT Copyright © 2019 mkrd CDMarkdownKit MIT Copyright © 2016-2017 Christopher de Haan contact@christopherdehaan.me Charts Apache License, Version 2.0 Copyright © 2016-2019 Daniel Cohen Gindi & Philipp Jahoda ECSlidingViewController MIT Copyright © 2013 EdgeCase LoginManagerSDK New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. Mantle MIT Copyright © GitHub, Inc. All rights reserved. Copyright © 2012, Bitswift, Inc MetricMetadata New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NRCharts New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NVActivityIndicatorView MIT Copyright © 2016 Vinh Nguyen NewRelicAgent New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. ObjectMapper MIT Copyright © 2014 Hearst RadarKit New Relic License © 2010-2021 New Relic, Inc. All rights reserved. SSPullToRefresh MIT Copyright © 2012-2014 Sam Soffes, http://soff.es UICKeyChainStore MIT Copyright © 2011 kishikawa katsumi WidgetLibrary New Relic License © 2010-2021 New Relic, Inc. All rights reserved. XYPieChart MIT Copyright © 2012 Xiaoyang Feng, XYStudio.cc Yams MIT Copyright © 2016 JP Simard. jsTimezoneDetect MIT Copyright © 2012 Jon Nylander, project maintained at bitbucket.org lodash MIT Copyright © JS Foundation and other contributors js.foundation The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.92877,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "iOS application <em>licenses</em>",
        "sections": "iOS application <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Copyright"
      },
      "id": "603e9db1196a670e70a83df3"
    },
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2021-10-18T14:42:09Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 160.39937,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-browser/new-relic-browser-licenses": [
    {
      "sections": [
        "Browser agent licenses"
      ],
      "title": "Browser agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Browser"
      ],
      "external_id": "9fac9d2d566767575f22d9458e49410063a83406",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-browser/browser-agent-licenses/",
      "published_at": "2021-10-18T14:39:11Z",
      "updated_at": "2021-03-16T04:45:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Browser agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Browserify MIT Episodes Apache 2.0 Lo-Dash MIT TraceKit MIT",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.62791,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Browser</em> agent <em>licenses</em>",
        "sections": "<em>Browser</em> agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> <em>Browser</em> agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Browserify MIT Episodes Apache 2.0 Lo-Dash MIT TraceKit MIT"
      },
      "id": "603eb41ce7b9d298012a080a"
    },
    {
      "sections": [
        "iOS application licenses"
      ],
      "title": "iOS application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "a6df56e363112e7387e6887f04381a36a5457e84",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/ios-application-licenses/",
      "published_at": "2021-10-18T14:40:40Z",
      "updated_at": "2021-09-27T15:25:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Copyright AFNetworking MIT Copyright © 2011-2020 Alamofire Software Foundation http://alamofire.org/ Ace BSD Copyright © 2010, Ajax.org B.V. ActiveLabel MIT Copyright © 2015 Optonaut Alamofire MIT Copyright © 2014-2021 Alamofire Software Foundation Analytics MIT Copyright © 2016 Segment.io, Inc. BBlock MIT Copyright © 2012 David Keegan BigNumber MIT Copyright © 2019 mkrd CDMarkdownKit MIT Copyright © 2016-2017 Christopher de Haan contact@christopherdehaan.me Charts Apache License, Version 2.0 Copyright © 2016-2019 Daniel Cohen Gindi & Philipp Jahoda ECSlidingViewController MIT Copyright © 2013 EdgeCase LoginManagerSDK New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. Mantle MIT Copyright © GitHub, Inc. All rights reserved. Copyright © 2012, Bitswift, Inc MetricMetadata New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NRCharts New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NVActivityIndicatorView MIT Copyright © 2016 Vinh Nguyen NewRelicAgent New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. ObjectMapper MIT Copyright © 2014 Hearst RadarKit New Relic License © 2010-2021 New Relic, Inc. All rights reserved. SSPullToRefresh MIT Copyright © 2012-2014 Sam Soffes, http://soff.es UICKeyChainStore MIT Copyright © 2011 kishikawa katsumi WidgetLibrary New Relic License © 2010-2021 New Relic, Inc. All rights reserved. XYPieChart MIT Copyright © 2012 Xiaoyang Feng, XYStudio.cc Yams MIT Copyright © 2016 JP Simard. jsTimezoneDetect MIT Copyright © 2012 Jon Nylander, project maintained at bitbucket.org lodash MIT Copyright © JS Foundation and other contributors js.foundation The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.92877,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "iOS application <em>licenses</em>",
        "sections": "iOS application <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Copyright"
      },
      "id": "603e9db1196a670e70a83df3"
    },
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2021-10-18T14:42:09Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 160.39937,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-developer-edition/developer-edition": [
    {
      "sections": [
        "Developer Program Resources"
      ],
      "title": "Developer Program Resources",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Developer edition"
      ],
      "external_id": "8a2f08905c7dcd10e50e975783ca3cf0071324c0",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-developer-edition/developer-program-resources/",
      "published_at": "2021-10-18T14:44:19Z",
      "updated_at": "2021-03-13T03:24:28Z",
      "document_type": "page",
      "popularity": 1,
      "body": "As a customer, you are eligible to participate in New Relic’s Developer Program. Additional information and resources are available at New Relic’s Developer Program site. By downloading, accessing, or using the developer resources (including the CLI), you agree that usage of the developer resources is pursuant to the New Relic Developers Terms and Conditions and that you have the authority to bind your organization. Such terms do not have to be signed in order to be binding. If you do not agree to these terms and conditions, your sole remedy is to not use these developer resources. If your use of the New Relic developer resources are covered under a separate agreement, the above does not apply to you.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 219.66754,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Developer</em> Program Resources",
        "sections": "<em>Developer</em> Program Resources",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "As a customer, you are eligible to participate in <em>New</em> <em>Relic</em>’s <em>Developer</em> Program. Additional information and resources are available at <em>New</em> <em>Relic</em>’s <em>Developer</em> Program site. By downloading, accessing, or using the <em>developer</em> resources (including the CLI), you agree that usage of the <em>developer</em> resources"
      },
      "id": "6044e7bb196a676d20960f4d"
    },
    {
      "sections": [
        "iOS application licenses"
      ],
      "title": "iOS application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "a6df56e363112e7387e6887f04381a36a5457e84",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/ios-application-licenses/",
      "published_at": "2021-10-18T14:40:40Z",
      "updated_at": "2021-09-27T15:25:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Copyright AFNetworking MIT Copyright © 2011-2020 Alamofire Software Foundation http://alamofire.org/ Ace BSD Copyright © 2010, Ajax.org B.V. ActiveLabel MIT Copyright © 2015 Optonaut Alamofire MIT Copyright © 2014-2021 Alamofire Software Foundation Analytics MIT Copyright © 2016 Segment.io, Inc. BBlock MIT Copyright © 2012 David Keegan BigNumber MIT Copyright © 2019 mkrd CDMarkdownKit MIT Copyright © 2016-2017 Christopher de Haan contact@christopherdehaan.me Charts Apache License, Version 2.0 Copyright © 2016-2019 Daniel Cohen Gindi & Philipp Jahoda ECSlidingViewController MIT Copyright © 2013 EdgeCase LoginManagerSDK New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. Mantle MIT Copyright © GitHub, Inc. All rights reserved. Copyright © 2012, Bitswift, Inc MetricMetadata New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NRCharts New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NVActivityIndicatorView MIT Copyright © 2016 Vinh Nguyen NewRelicAgent New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. ObjectMapper MIT Copyright © 2014 Hearst RadarKit New Relic License © 2010-2021 New Relic, Inc. All rights reserved. SSPullToRefresh MIT Copyright © 2012-2014 Sam Soffes, http://soff.es UICKeyChainStore MIT Copyright © 2011 kishikawa katsumi WidgetLibrary New Relic License © 2010-2021 New Relic, Inc. All rights reserved. XYPieChart MIT Copyright © 2012 Xiaoyang Feng, XYStudio.cc Yams MIT Copyright © 2016 JP Simard. jsTimezoneDetect MIT Copyright © 2012 Jon Nylander, project maintained at bitbucket.org lodash MIT Copyright © JS Foundation and other contributors js.foundation The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 169.54697,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "iOS application <em>licenses</em>",
        "sections": "iOS application <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Copyright"
      },
      "id": "603e9db1196a670e70a83df3"
    },
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2021-10-18T14:42:09Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.04959,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-developer-edition/developer-program-resources": [
    {
      "sections": [
        "Developer edition",
        "Terms"
      ],
      "title": "Developer edition",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Developer edition"
      ],
      "external_id": "60bc94afd677817a7b7fd7dd471c537090a9f711",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-developer-edition/developer-edition/",
      "published_at": "2021-10-18T14:44:19Z",
      "updated_at": "2021-03-13T02:26:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "All new accounts (outside Japan) created on or after July 30, 2020 are on the New Relic One pricing plan and have the newer New Relic One user model as described here. If (i) your account was created prior to July 30, 2020; or (ii) you are a current New Relic K.K. customer (Japan): in addition to the New Relic Pre-release policy, use of the Developer Edition of New Relic to the extent it is available is also subject to the following terms: Terms Non-production use only Up to $500 per month in total product usage across the New Relic platform Up to 2 users",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 219.66577,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Developer</em> <em>edition</em>",
        "sections": "<em>Developer</em> <em>edition</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": " to the <em>New</em> <em>Relic</em> Pre-release policy, use of the <em>Developer</em> <em>Edition</em> of <em>New</em> <em>Relic</em> to the extent it is available is also subject to the following terms: Terms Non-production use only Up to $500 per month in total <em>product</em> usage across the <em>New</em> <em>Relic</em> platform Up to 2 users"
      },
      "id": "604506b9e7b9d22d115799c8"
    },
    {
      "sections": [
        "iOS application licenses"
      ],
      "title": "iOS application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "a6df56e363112e7387e6887f04381a36a5457e84",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/ios-application-licenses/",
      "published_at": "2021-10-18T14:40:40Z",
      "updated_at": "2021-09-27T15:25:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Copyright AFNetworking MIT Copyright © 2011-2020 Alamofire Software Foundation http://alamofire.org/ Ace BSD Copyright © 2010, Ajax.org B.V. ActiveLabel MIT Copyright © 2015 Optonaut Alamofire MIT Copyright © 2014-2021 Alamofire Software Foundation Analytics MIT Copyright © 2016 Segment.io, Inc. BBlock MIT Copyright © 2012 David Keegan BigNumber MIT Copyright © 2019 mkrd CDMarkdownKit MIT Copyright © 2016-2017 Christopher de Haan contact@christopherdehaan.me Charts Apache License, Version 2.0 Copyright © 2016-2019 Daniel Cohen Gindi & Philipp Jahoda ECSlidingViewController MIT Copyright © 2013 EdgeCase LoginManagerSDK New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. Mantle MIT Copyright © GitHub, Inc. All rights reserved. Copyright © 2012, Bitswift, Inc MetricMetadata New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NRCharts New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NVActivityIndicatorView MIT Copyright © 2016 Vinh Nguyen NewRelicAgent New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. ObjectMapper MIT Copyright © 2014 Hearst RadarKit New Relic License © 2010-2021 New Relic, Inc. All rights reserved. SSPullToRefresh MIT Copyright © 2012-2014 Sam Soffes, http://soff.es UICKeyChainStore MIT Copyright © 2011 kishikawa katsumi WidgetLibrary New Relic License © 2010-2021 New Relic, Inc. All rights reserved. XYPieChart MIT Copyright © 2012 Xiaoyang Feng, XYStudio.cc Yams MIT Copyright © 2016 JP Simard. jsTimezoneDetect MIT Copyright © 2012 Jon Nylander, project maintained at bitbucket.org lodash MIT Copyright © JS Foundation and other contributors js.foundation The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 169.54697,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "iOS application <em>licenses</em>",
        "sections": "iOS application <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Copyright"
      },
      "id": "603e9db1196a670e70a83df3"
    },
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2021-10-18T14:42:09Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.04959,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-infrastructure/new-relic-infrastructure-licenses": [
    {
      "sections": [
        "iOS application licenses"
      ],
      "title": "iOS application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "a6df56e363112e7387e6887f04381a36a5457e84",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/ios-application-licenses/",
      "published_at": "2021-10-18T14:40:40Z",
      "updated_at": "2021-09-27T15:25:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Copyright AFNetworking MIT Copyright © 2011-2020 Alamofire Software Foundation http://alamofire.org/ Ace BSD Copyright © 2010, Ajax.org B.V. ActiveLabel MIT Copyright © 2015 Optonaut Alamofire MIT Copyright © 2014-2021 Alamofire Software Foundation Analytics MIT Copyright © 2016 Segment.io, Inc. BBlock MIT Copyright © 2012 David Keegan BigNumber MIT Copyright © 2019 mkrd CDMarkdownKit MIT Copyright © 2016-2017 Christopher de Haan contact@christopherdehaan.me Charts Apache License, Version 2.0 Copyright © 2016-2019 Daniel Cohen Gindi & Philipp Jahoda ECSlidingViewController MIT Copyright © 2013 EdgeCase LoginManagerSDK New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. Mantle MIT Copyright © GitHub, Inc. All rights reserved. Copyright © 2012, Bitswift, Inc MetricMetadata New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NRCharts New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NVActivityIndicatorView MIT Copyright © 2016 Vinh Nguyen NewRelicAgent New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. ObjectMapper MIT Copyright © 2014 Hearst RadarKit New Relic License © 2010-2021 New Relic, Inc. All rights reserved. SSPullToRefresh MIT Copyright © 2012-2014 Sam Soffes, http://soff.es UICKeyChainStore MIT Copyright © 2011 kishikawa katsumi WidgetLibrary New Relic License © 2010-2021 New Relic, Inc. All rights reserved. XYPieChart MIT Copyright © 2012 Xiaoyang Feng, XYStudio.cc Yams MIT Copyright © 2016 JP Simard. jsTimezoneDetect MIT Copyright © 2012 Jon Nylander, project maintained at bitbucket.org lodash MIT Copyright © JS Foundation and other contributors js.foundation The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.92874,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "iOS application <em>licenses</em>",
        "sections": "iOS application <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Copyright"
      },
      "id": "603e9db1196a670e70a83df3"
    },
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2021-10-18T14:42:09Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 160.39932,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    },
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2021-10-18T14:45:13Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0 © 2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0 © 2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0 © 2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0 © 2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0 © 2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright © 2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright © 2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright © 1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright © 1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.73653,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Logs plugin <em>licenses</em>",
        "sections": "Logs plugin <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-insights/new-relic-insights-licenses": [
    {
      "sections": [
        "iOS application licenses"
      ],
      "title": "iOS application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "a6df56e363112e7387e6887f04381a36a5457e84",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/ios-application-licenses/",
      "published_at": "2021-10-18T14:40:40Z",
      "updated_at": "2021-09-27T15:25:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Copyright AFNetworking MIT Copyright © 2011-2020 Alamofire Software Foundation http://alamofire.org/ Ace BSD Copyright © 2010, Ajax.org B.V. ActiveLabel MIT Copyright © 2015 Optonaut Alamofire MIT Copyright © 2014-2021 Alamofire Software Foundation Analytics MIT Copyright © 2016 Segment.io, Inc. BBlock MIT Copyright © 2012 David Keegan BigNumber MIT Copyright © 2019 mkrd CDMarkdownKit MIT Copyright © 2016-2017 Christopher de Haan contact@christopherdehaan.me Charts Apache License, Version 2.0 Copyright © 2016-2019 Daniel Cohen Gindi & Philipp Jahoda ECSlidingViewController MIT Copyright © 2013 EdgeCase LoginManagerSDK New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. Mantle MIT Copyright © GitHub, Inc. All rights reserved. Copyright © 2012, Bitswift, Inc MetricMetadata New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NRCharts New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NVActivityIndicatorView MIT Copyright © 2016 Vinh Nguyen NewRelicAgent New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. ObjectMapper MIT Copyright © 2014 Hearst RadarKit New Relic License © 2010-2021 New Relic, Inc. All rights reserved. SSPullToRefresh MIT Copyright © 2012-2014 Sam Soffes, http://soff.es UICKeyChainStore MIT Copyright © 2011 kishikawa katsumi WidgetLibrary New Relic License © 2010-2021 New Relic, Inc. All rights reserved. XYPieChart MIT Copyright © 2012 Xiaoyang Feng, XYStudio.cc Yams MIT Copyright © 2016 JP Simard. jsTimezoneDetect MIT Copyright © 2012 Jon Nylander, project maintained at bitbucket.org lodash MIT Copyright © JS Foundation and other contributors js.foundation The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.92874,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "iOS application <em>licenses</em>",
        "sections": "iOS application <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Copyright"
      },
      "id": "603e9db1196a670e70a83df3"
    },
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2021-10-18T14:42:09Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 160.39932,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    },
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2021-10-18T14:45:13Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0 © 2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0 © 2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0 © 2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0 © 2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0 © 2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright © 2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright © 2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright © 1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright © 1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.73653,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Logs plugin <em>licenses</em>",
        "sections": "Logs plugin <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-logs/logs-licenses": [
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2021-10-18T14:45:13Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0 © 2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0 © 2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0 © 2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0 © 2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0 © 2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright © 2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright © 2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright © 1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright © 1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 200.79065,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Logs</em> plugin <em>licenses</em>",
        "sections": "<em>Logs</em> plugin <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": " used for <em>New</em> <em>Relic</em> <em>Logs</em>, see <em>Logs</em> <em>licenses</em>. Plugins for <em>Logs</em> The following <em>licenses</em> are for the plugins used to connects your <em>log</em> data with <em>New</em> <em>Relic</em> <em>Logs</em>. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0 © 2019 <em>New</em> <em>Relic</em>, Inc. Fluentd Library License Copyright Fluentd Apache"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    },
    {
      "sections": [
        "iOS application licenses"
      ],
      "title": "iOS application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "a6df56e363112e7387e6887f04381a36a5457e84",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/ios-application-licenses/",
      "published_at": "2021-10-18T14:40:40Z",
      "updated_at": "2021-09-27T15:25:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Copyright AFNetworking MIT Copyright © 2011-2020 Alamofire Software Foundation http://alamofire.org/ Ace BSD Copyright © 2010, Ajax.org B.V. ActiveLabel MIT Copyright © 2015 Optonaut Alamofire MIT Copyright © 2014-2021 Alamofire Software Foundation Analytics MIT Copyright © 2016 Segment.io, Inc. BBlock MIT Copyright © 2012 David Keegan BigNumber MIT Copyright © 2019 mkrd CDMarkdownKit MIT Copyright © 2016-2017 Christopher de Haan contact@christopherdehaan.me Charts Apache License, Version 2.0 Copyright © 2016-2019 Daniel Cohen Gindi & Philipp Jahoda ECSlidingViewController MIT Copyright © 2013 EdgeCase LoginManagerSDK New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. Mantle MIT Copyright © GitHub, Inc. All rights reserved. Copyright © 2012, Bitswift, Inc MetricMetadata New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NRCharts New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NVActivityIndicatorView MIT Copyright © 2016 Vinh Nguyen NewRelicAgent New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. ObjectMapper MIT Copyright © 2014 Hearst RadarKit New Relic License © 2010-2021 New Relic, Inc. All rights reserved. SSPullToRefresh MIT Copyright © 2012-2014 Sam Soffes, http://soff.es UICKeyChainStore MIT Copyright © 2011 kishikawa katsumi WidgetLibrary New Relic License © 2010-2021 New Relic, Inc. All rights reserved. XYPieChart MIT Copyright © 2012 Xiaoyang Feng, XYStudio.cc Yams MIT Copyright © 2016 JP Simard. jsTimezoneDetect MIT Copyright © 2012 Jon Nylander, project maintained at bitbucket.org lodash MIT Copyright © JS Foundation and other contributors js.foundation The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 169.54694,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "iOS application <em>licenses</em>",
        "sections": "iOS application <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Copyright"
      },
      "id": "603e9db1196a670e70a83df3"
    },
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2021-10-18T14:42:09Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.04955,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses": [
    {
      "sections": [
        "Logs licenses",
        "Logs"
      ],
      "title": "Logs licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "229757bf0e3d8f3518533b11a059b9e4516e4699",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-licenses/",
      "published_at": "2021-10-18T14:45:13Z",
      "updated_at": "2021-03-16T04:46:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for the New Relic Logs plugins, see Logs plugin licenses. Logs axios MIT Copyright © 2014-present Matt Zabriskie Babel-plugin-transform-runtime MIT Copyright © 2014-present Sebastian McKenzie and other contributors Classnames MIT Copyright © 2018 Jed Watson Downshift MIT Copyright © 2017 PayPal Fuzzy-search ISC Copyright © 2016, Wouter Rutgers Immer MIT Copyright © 2017 Michel Weststrate Lodash MIT Copyright © JS Foundation and other contributors Lodash.debounce MIT Copyright © JS Foundation and other contributors Moment MIT Copyright © JS Foundation and other contributors Node-sass MIT Copyright © 2013-2016 Andrew Nesbitt Prop-types MIT Copyright © 2013-present, Facebook, Inc. React MIT Copyright © Facebook, Inc. and its affiliates. React-dom MIT Copyright © Facebook, Inc. and its affiliates. React-highlight-words MIT Copyright © 2015 Treasure Data React-json-view MIT Copyright © 2015 Mac Gainor React-popper MIT Copyright © 2018 React Popper authors React-redux MIT Copyright © 2015-present Dan Abramov React-select MIT Copyright © 2018 Jed Watson React-tooltip MIT Copyright © 2015 Wang Zixiao Redux MIT Copyright © 2015-present Dan Abramov Redux-logger MIT Copyright © 2016 Eugene Rodionov Redux-saga MIT Copyright © 2015 Yassine Elouafi Reselect MIT Copyright © 2015-2018 Reselect Contributors Shortid MIT Copyright © Dylan Greene Snyk Apache License 2.0 Copyright © 2015 Snyk Ltd. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 190.75214,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Logs</em> <em>licenses</em>",
        "sections": "<em>Logs</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> <em>Logs</em> plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea57b28ccbce04ceba77a"
    },
    {
      "sections": [
        "iOS application licenses"
      ],
      "title": "iOS application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "a6df56e363112e7387e6887f04381a36a5457e84",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/ios-application-licenses/",
      "published_at": "2021-10-18T14:40:40Z",
      "updated_at": "2021-09-27T15:25:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Copyright AFNetworking MIT Copyright © 2011-2020 Alamofire Software Foundation http://alamofire.org/ Ace BSD Copyright © 2010, Ajax.org B.V. ActiveLabel MIT Copyright © 2015 Optonaut Alamofire MIT Copyright © 2014-2021 Alamofire Software Foundation Analytics MIT Copyright © 2016 Segment.io, Inc. BBlock MIT Copyright © 2012 David Keegan BigNumber MIT Copyright © 2019 mkrd CDMarkdownKit MIT Copyright © 2016-2017 Christopher de Haan contact@christopherdehaan.me Charts Apache License, Version 2.0 Copyright © 2016-2019 Daniel Cohen Gindi & Philipp Jahoda ECSlidingViewController MIT Copyright © 2013 EdgeCase LoginManagerSDK New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. Mantle MIT Copyright © GitHub, Inc. All rights reserved. Copyright © 2012, Bitswift, Inc MetricMetadata New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NRCharts New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NVActivityIndicatorView MIT Copyright © 2016 Vinh Nguyen NewRelicAgent New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. ObjectMapper MIT Copyright © 2014 Hearst RadarKit New Relic License © 2010-2021 New Relic, Inc. All rights reserved. SSPullToRefresh MIT Copyright © 2012-2014 Sam Soffes, http://soff.es UICKeyChainStore MIT Copyright © 2011 kishikawa katsumi WidgetLibrary New Relic License © 2010-2021 New Relic, Inc. All rights reserved. XYPieChart MIT Copyright © 2012 Xiaoyang Feng, XYStudio.cc Yams MIT Copyright © 2016 JP Simard. jsTimezoneDetect MIT Copyright © 2012 Jon Nylander, project maintained at bitbucket.org lodash MIT Copyright © JS Foundation and other contributors js.foundation The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 169.54694,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "iOS application <em>licenses</em>",
        "sections": "iOS application <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Copyright"
      },
      "id": "603e9db1196a670e70a83df3"
    },
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2021-10-18T14:42:09Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.04955,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-mobile/android-sdk-new-relic-mobile-licenses": [
    {
      "sections": [
        "iOS SDK for mobile monitoring licenses"
      ],
      "title": "iOS SDK for mobile monitoring licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Mobile"
      ],
      "external_id": "71d2df4a922b6728230da4b4e8241f2d458ea66c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-mobile/ios-sdk-new-relic-mobile-licenses/",
      "published_at": "2021-10-18T14:45:59Z",
      "updated_at": "2021-07-22T01:04:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the iOS SDK for mobile monitoring. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apple Reachability Apple Reachability JSON++ MIT mod-pbxproj BSD-3 PLCrashReporter MIT The remainder of the code is covered by the New Relic License agreement found in the LICENSE file in the distribution.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 190.06651,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "iOS SDK for <em>mobile</em> monitoring <em>licenses</em>",
        "sections": "iOS SDK for <em>mobile</em> monitoring <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the iOS SDK for <em>mobile</em> monitoring. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apple"
      },
      "id": "60450cfde7b9d2c9eb5799c3"
    },
    {
      "sections": [
        "iOS application licenses"
      ],
      "title": "iOS application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "a6df56e363112e7387e6887f04381a36a5457e84",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/ios-application-licenses/",
      "published_at": "2021-10-18T14:40:40Z",
      "updated_at": "2021-09-27T15:25:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Copyright AFNetworking MIT Copyright © 2011-2020 Alamofire Software Foundation http://alamofire.org/ Ace BSD Copyright © 2010, Ajax.org B.V. ActiveLabel MIT Copyright © 2015 Optonaut Alamofire MIT Copyright © 2014-2021 Alamofire Software Foundation Analytics MIT Copyright © 2016 Segment.io, Inc. BBlock MIT Copyright © 2012 David Keegan BigNumber MIT Copyright © 2019 mkrd CDMarkdownKit MIT Copyright © 2016-2017 Christopher de Haan contact@christopherdehaan.me Charts Apache License, Version 2.0 Copyright © 2016-2019 Daniel Cohen Gindi & Philipp Jahoda ECSlidingViewController MIT Copyright © 2013 EdgeCase LoginManagerSDK New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. Mantle MIT Copyright © GitHub, Inc. All rights reserved. Copyright © 2012, Bitswift, Inc MetricMetadata New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NRCharts New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NVActivityIndicatorView MIT Copyright © 2016 Vinh Nguyen NewRelicAgent New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. ObjectMapper MIT Copyright © 2014 Hearst RadarKit New Relic License © 2010-2021 New Relic, Inc. All rights reserved. SSPullToRefresh MIT Copyright © 2012-2014 Sam Soffes, http://soff.es UICKeyChainStore MIT Copyright © 2011 kishikawa katsumi WidgetLibrary New Relic License © 2010-2021 New Relic, Inc. All rights reserved. XYPieChart MIT Copyright © 2012 Xiaoyang Feng, XYStudio.cc Yams MIT Copyright © 2016 JP Simard. jsTimezoneDetect MIT Copyright © 2012 Jon Nylander, project maintained at bitbucket.org lodash MIT Copyright © JS Foundation and other contributors js.foundation The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 169.5469,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "iOS application <em>licenses</em>",
        "sections": "iOS application <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Copyright"
      },
      "id": "603e9db1196a670e70a83df3"
    },
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2021-10-18T14:42:09Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.04953,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-mobile/ios-sdk-new-relic-mobile-licenses": [
    {
      "sections": [
        "Android SDK for mobile monitoring licenses"
      ],
      "title": "Android SDK for mobile monitoring licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Mobile"
      ],
      "external_id": "7e7fa828754c2ba00d4e2138653c7cdd00ed6c90",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-mobile/android-sdk-new-relic-mobile-licenses/",
      "published_at": "2021-10-18T14:45:59Z",
      "updated_at": "2021-08-21T08:48:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the Android SDK for mobile monitoring. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Commons IO Apache 2.0 Apache Commons Logging Apache 2.0 Apache HttpClient Apache 2.0 ASM ASM (BSD-3 style) Gradle Apache 2.0 Gson Apache 2.0 Guava Apache 2.0 JarJar Apache 2.0 Javassist Apache 2.0 org.json JSON Reflections WTFPL SLF4J MIT Square OkHttp Apache 2.0 The remainder of the code is covered by the New Relic License agreement found in the LICENSE file in the distribution.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 195.71065,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Android SDK for <em>mobile</em> monitoring <em>licenses</em>",
        "sections": "Android SDK for <em>mobile</em> monitoring <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the Android SDK for <em>mobile</em> monitoring. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License"
      },
      "id": "603e9eb628ccbc117beba796"
    },
    {
      "sections": [
        "iOS application licenses"
      ],
      "title": "iOS application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "a6df56e363112e7387e6887f04381a36a5457e84",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/ios-application-licenses/",
      "published_at": "2021-10-18T14:40:40Z",
      "updated_at": "2021-09-27T15:25:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Copyright AFNetworking MIT Copyright © 2011-2020 Alamofire Software Foundation http://alamofire.org/ Ace BSD Copyright © 2010, Ajax.org B.V. ActiveLabel MIT Copyright © 2015 Optonaut Alamofire MIT Copyright © 2014-2021 Alamofire Software Foundation Analytics MIT Copyright © 2016 Segment.io, Inc. BBlock MIT Copyright © 2012 David Keegan BigNumber MIT Copyright © 2019 mkrd CDMarkdownKit MIT Copyright © 2016-2017 Christopher de Haan contact@christopherdehaan.me Charts Apache License, Version 2.0 Copyright © 2016-2019 Daniel Cohen Gindi & Philipp Jahoda ECSlidingViewController MIT Copyright © 2013 EdgeCase LoginManagerSDK New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. Mantle MIT Copyright © GitHub, Inc. All rights reserved. Copyright © 2012, Bitswift, Inc MetricMetadata New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NRCharts New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NVActivityIndicatorView MIT Copyright © 2016 Vinh Nguyen NewRelicAgent New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. ObjectMapper MIT Copyright © 2014 Hearst RadarKit New Relic License © 2010-2021 New Relic, Inc. All rights reserved. SSPullToRefresh MIT Copyright © 2012-2014 Sam Soffes, http://soff.es UICKeyChainStore MIT Copyright © 2011 kishikawa katsumi WidgetLibrary New Relic License © 2010-2021 New Relic, Inc. All rights reserved. XYPieChart MIT Copyright © 2012 Xiaoyang Feng, XYStudio.cc Yams MIT Copyright © 2016 JP Simard. jsTimezoneDetect MIT Copyright © 2012 Jon Nylander, project maintained at bitbucket.org lodash MIT Copyright © JS Foundation and other contributors js.foundation The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 169.5469,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "iOS application <em>licenses</em>",
        "sections": "iOS application <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Copyright"
      },
      "id": "603e9db1196a670e70a83df3"
    },
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2021-10-18T14:42:09Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.04953,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-one/preview-access-new-relic-one": [
    {
      "sections": [
        "iOS application licenses"
      ],
      "title": "iOS application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "a6df56e363112e7387e6887f04381a36a5457e84",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/ios-application-licenses/",
      "published_at": "2021-10-18T14:40:40Z",
      "updated_at": "2021-09-27T15:25:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Copyright AFNetworking MIT Copyright © 2011-2020 Alamofire Software Foundation http://alamofire.org/ Ace BSD Copyright © 2010, Ajax.org B.V. ActiveLabel MIT Copyright © 2015 Optonaut Alamofire MIT Copyright © 2014-2021 Alamofire Software Foundation Analytics MIT Copyright © 2016 Segment.io, Inc. BBlock MIT Copyright © 2012 David Keegan BigNumber MIT Copyright © 2019 mkrd CDMarkdownKit MIT Copyright © 2016-2017 Christopher de Haan contact@christopherdehaan.me Charts Apache License, Version 2.0 Copyright © 2016-2019 Daniel Cohen Gindi & Philipp Jahoda ECSlidingViewController MIT Copyright © 2013 EdgeCase LoginManagerSDK New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. Mantle MIT Copyright © GitHub, Inc. All rights reserved. Copyright © 2012, Bitswift, Inc MetricMetadata New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NRCharts New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NVActivityIndicatorView MIT Copyright © 2016 Vinh Nguyen NewRelicAgent New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. ObjectMapper MIT Copyright © 2014 Hearst RadarKit New Relic License © 2010-2021 New Relic, Inc. All rights reserved. SSPullToRefresh MIT Copyright © 2012-2014 Sam Soffes, http://soff.es UICKeyChainStore MIT Copyright © 2011 kishikawa katsumi WidgetLibrary New Relic License © 2010-2021 New Relic, Inc. All rights reserved. XYPieChart MIT Copyright © 2012 Xiaoyang Feng, XYStudio.cc Yams MIT Copyright © 2016 JP Simard. jsTimezoneDetect MIT Copyright © 2012 Jon Nylander, project maintained at bitbucket.org lodash MIT Copyright © JS Foundation and other contributors js.foundation The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 169.5469,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "iOS application <em>licenses</em>",
        "sections": "iOS application <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Copyright"
      },
      "id": "603e9db1196a670e70a83df3"
    },
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2021-10-18T14:42:09Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.0495,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    },
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2021-10-18T14:45:13Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0 © 2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0 © 2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0 © 2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0 © 2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0 © 2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright © 2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright © 2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright © 1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright © 1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 144.39459,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Logs plugin <em>licenses</em>",
        "sections": "Logs plugin <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-plugins/plugins-licenses": [
    {
      "sections": [
        "iOS application licenses"
      ],
      "title": "iOS application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "a6df56e363112e7387e6887f04381a36a5457e84",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/ios-application-licenses/",
      "published_at": "2021-10-18T14:40:40Z",
      "updated_at": "2021-09-27T15:25:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Copyright AFNetworking MIT Copyright © 2011-2020 Alamofire Software Foundation http://alamofire.org/ Ace BSD Copyright © 2010, Ajax.org B.V. ActiveLabel MIT Copyright © 2015 Optonaut Alamofire MIT Copyright © 2014-2021 Alamofire Software Foundation Analytics MIT Copyright © 2016 Segment.io, Inc. BBlock MIT Copyright © 2012 David Keegan BigNumber MIT Copyright © 2019 mkrd CDMarkdownKit MIT Copyright © 2016-2017 Christopher de Haan contact@christopherdehaan.me Charts Apache License, Version 2.0 Copyright © 2016-2019 Daniel Cohen Gindi & Philipp Jahoda ECSlidingViewController MIT Copyright © 2013 EdgeCase LoginManagerSDK New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. Mantle MIT Copyright © GitHub, Inc. All rights reserved. Copyright © 2012, Bitswift, Inc MetricMetadata New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NRCharts New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NVActivityIndicatorView MIT Copyright © 2016 Vinh Nguyen NewRelicAgent New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. ObjectMapper MIT Copyright © 2014 Hearst RadarKit New Relic License © 2010-2021 New Relic, Inc. All rights reserved. SSPullToRefresh MIT Copyright © 2012-2014 Sam Soffes, http://soff.es UICKeyChainStore MIT Copyright © 2011 kishikawa katsumi WidgetLibrary New Relic License © 2010-2021 New Relic, Inc. All rights reserved. XYPieChart MIT Copyright © 2012 Xiaoyang Feng, XYStudio.cc Yams MIT Copyright © 2016 JP Simard. jsTimezoneDetect MIT Copyright © 2012 Jon Nylander, project maintained at bitbucket.org lodash MIT Copyright © JS Foundation and other contributors js.foundation The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 169.5469,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "iOS application <em>licenses</em>",
        "sections": "iOS application <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Copyright"
      },
      "id": "603e9db1196a670e70a83df3"
    },
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2021-10-18T14:42:09Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.0495,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    },
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2021-10-18T14:45:13Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0 © 2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0 © 2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0 © 2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0 © 2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0 © 2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright © 2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright © 2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright © 1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright © 1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.94124,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Logs <em>plugin</em> <em>licenses</em>",
        "sections": "Logs <em>plugin</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Logs <em>plugins</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    }
  ],
  "/docs/licenses/product-or-service-licenses/new-relic-synthetics/new-relic-synthetics-licenses": [
    {
      "sections": [
        "iOS application licenses"
      ],
      "title": "iOS application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "a6df56e363112e7387e6887f04381a36a5457e84",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/ios-application-licenses/",
      "published_at": "2021-10-18T14:40:40Z",
      "updated_at": "2021-09-27T15:25:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Copyright AFNetworking MIT Copyright © 2011-2020 Alamofire Software Foundation http://alamofire.org/ Ace BSD Copyright © 2010, Ajax.org B.V. ActiveLabel MIT Copyright © 2015 Optonaut Alamofire MIT Copyright © 2014-2021 Alamofire Software Foundation Analytics MIT Copyright © 2016 Segment.io, Inc. BBlock MIT Copyright © 2012 David Keegan BigNumber MIT Copyright © 2019 mkrd CDMarkdownKit MIT Copyright © 2016-2017 Christopher de Haan contact@christopherdehaan.me Charts Apache License, Version 2.0 Copyright © 2016-2019 Daniel Cohen Gindi & Philipp Jahoda ECSlidingViewController MIT Copyright © 2013 EdgeCase LoginManagerSDK New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. Mantle MIT Copyright © GitHub, Inc. All rights reserved. Copyright © 2012, Bitswift, Inc MetricMetadata New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NRCharts New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. NVActivityIndicatorView MIT Copyright © 2016 Vinh Nguyen NewRelicAgent New Relic License Copyright © 2010-2021 New Relic, Inc. All rights reserved. ObjectMapper MIT Copyright © 2014 Hearst RadarKit New Relic License © 2010-2021 New Relic, Inc. All rights reserved. SSPullToRefresh MIT Copyright © 2012-2014 Sam Soffes, http://soff.es UICKeyChainStore MIT Copyright © 2011 kishikawa katsumi WidgetLibrary New Relic License © 2010-2021 New Relic, Inc. All rights reserved. XYPieChart MIT Copyright © 2012 Xiaoyang Feng, XYStudio.cc Yams MIT Copyright © 2016 JP Simard. jsTimezoneDetect MIT Copyright © 2012 Jon Nylander, project maintained at bitbucket.org lodash MIT Copyright © JS Foundation and other contributors js.foundation The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 169.54688,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "iOS application <em>licenses</em>",
        "sections": "iOS application <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Copyright"
      },
      "id": "603e9db1196a670e70a83df3"
    },
    {
      "sections": [
        "Java agent licenses"
      ],
      "title": "Java agent licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "37b0e6a62ed17e39eb39c2c673f5b3c4127cc6ab",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/java-agent-licenses/",
      "published_at": "2021-10-18T14:42:09Z",
      "updated_at": "2021-10-01T20:32:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Apache Cassandra Apache License 2.0 Apache Harmony Apache License 2.0 ASM ASM (BSD-3) Caffeine Apache License 2.0 Checker Qual The MIT License (MIT) Guava Apache License 2.0 gRPC Apache License 2.0 Gson Apache License 2.0 HttpClient Apache License 2.0 Jackson Apache License 2.0 JRegex BSD-3 JSON.simple Apache License 2.0 Log4j2 Apache License 2.0 Netty Apache License 2.0 OkHttp Apache License 2.0 Okio Apache License 2.0 OpenTracing Apache License 2.0 Protobuf The Protocol Buffers License SLF4J The MIT License (MIT) SnakeYAML Apache License 2.0 Remaining code in versions prior to v6.0.0 is licensed under the New Relic agent license agreement. License terms for v6.0.0 and later can be found on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.04948,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java agent <em>licenses</em>",
        "sections": "Java agent <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Java agent. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Apache"
      },
      "id": "604506b964441ff5e1378f19"
    },
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2021-10-18T14:45:13Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0 © 2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0 © 2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0 © 2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0 © 2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0 © 2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright © 2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright © 1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright © 2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright © 1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright © 1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 144.39459,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Logs plugin <em>licenses</em>",
        "sections": "Logs plugin <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the <em>New</em> <em>Relic</em> Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    }
  ],
  "/docs/logs/forward-logs/aws-firelens-plugin-log-forwarding": [
    {
      "sections": [
        "Kubernetes plugin for log forwarding",
        "Enable Kubernetes for log management",
        "View log data",
        "What's next?"
      ],
      "title": "Kubernetes plugin for log forwarding",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "c55c6a084a1de057aac4f85ed3807c41317f63e1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/kubernetes-plugin-log-forwarding/",
      "published_at": "2021-10-18T05:22:43Z",
      "updated_at": "2021-10-13T02:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your log data in New Relic. This plugin is also provided in a standalone Docker image that can be installed in a Kubernetes cluster in the form of a DaemonSet. We refer to this as our Kubernetes plugin. Forwarding your Kubernetes logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Enable Kubernetes for log management To forward your logs to New Relic with our Kubernetes plugin for Fluent Bit: Make sure you have: A New Relic license key Kubernetes cluster deployed Follow the procedures in GitHub to install the Kubernetes plugin. Optional: If you are installing the plugin as a Helm chart, you can set numerous configurations. However, we recommend the standard setup, as it is valid for most users. Generate some traffic and wait a few minutes, then check your account for data. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 297.0804,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kubernetes plugin for <em>log</em> forwarding",
        "sections": "<em>Enable</em> Kubernetes for <em>log</em> <em>management</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " to this as our Kubernetes plugin. Forwarding your Kubernetes <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert on your <em>log</em> data. <em>Enable</em> Kubernetes for <em>log</em> <em>management</em> To forward your <em>logs</em> to <em>New</em> <em>Relic</em> with our Kubernetes plugin for Fluent Bit"
      },
      "id": "603ec19128ccbcad24eba748"
    },
    {
      "sections": [
        "Ruby: Configure logs in context",
        "Set up your Ruby app",
        "Standard Ruby on Rails configuration",
        "Incompatible gems",
        "Rails advanced configuration",
        "Ruby configuration without Rails",
        "Lograge configuration",
        "Other logging extensions",
        "Troubleshooting",
        "What's next?"
      ],
      "title": "Ruby: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Logs in context for Ruby"
      ],
      "external_id": "d2100c0d6ddffe6b52a1f968f61f777a1e56a13c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/configure-logs-context-ruby/",
      "published_at": "2021-10-19T03:54:54Z",
      "updated_at": "2021-10-19T03:54:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Ruby agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Ruby app To enable logs in context for APM apps monitored by Ruby: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Ruby agent version, and enable distributed tracing. Use Ruby agent version 6.7.0 or higher for logs in context. For Rails applications, use a supported Rails version. Configure logs in context for Ruby. Standard Ruby on Rails configuration Rails logging is controlled by two components: A logger you can customize by setting config.logger A log formatter you can customize by setting config.log_formatter) In most cases, you should configure logs in context by setting config.log_formatter to the DecoratingFormatter in your Rails application. For more information about Rails configuration, see the rubyonrails.org documentation. In your application's config, require newrelic_rpm, then add the following line: module ________ class Application < Rails::Application ... config.log_formatter = ::NewRelic::Agent::Logging::DecoratingFormatter.new end end Copy This configuration uses the New Relic formatter for log messages, but the remaining configuration is provided by the other Rails settings. Incompatible gems New Relic's decorating logger is known to be incompatible with the following gems: logging semantic logger rails_stdout_logger rails_12factor Rails advanced configuration If setting the log_formatter option doesn't meet your needs, replace the entire Rails logger with an instance of the New Relic logger. Provide the parameters to the logger's constructor, like this: module ________ class Application < Rails::Application ... config.logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( \"log/application.log\", #etc... ) end end Copy Ruby configuration without Rails For non-Rails applications, use the DecoratingLogger in place of the Ruby standard ::Logger, like this: logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( 'log/application.log', #etc... ) ... logger.info(...) Copy The DecoratingLogger is a drop-in replacement for the Ruby standard ::Logger. Their constructors accept the same parameters. Lograge configuration To configure this extension with the lograge gem, follow standard procedures in this doc for Ruby on Rails configuration. No additional configuration is required for the lograge gem. Other logging extensions To use our logging extension with a different logging implementation or with your own custom logger, use the DecoratingFormatter. For example: module ________ class Application < Rails::Application ... config.logger = ::YourCustomLoggerImplementation.new( $stdout, formatter: ::NewRelic::Agent::Logging::DecoratingFormatter.new ) end end Copy To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you have configured your logging in /config/application.rb or in /config/environments/development.rb, run your application locally and check its logging output. You should see some output like this: {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"00fc7d46\",\"timestamp\":1567701375543,\"message\":\"example log message one\",\"log.level\":\"DEBUG\"} {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"6754870b\",\"timestamp\":1567702843604,\"message\":\"example log message two\",\"log.level\":\"DEBUG\"} Copy Troubleshooting If you don't see log data in the UI, follow the troubleshooting procedures. Also, if you see JSON logs in your application's output, but your query does not return logs, check your log forwarder. If the logs from your application are not formatted in JSON with fields like trace.id and span.id, there may be a problem with your log forwarding extension. In this situation: Check that the application is using a supported logging framework. Check that your logging configuration has been applied to all the environments where you want to forward and enrich the log data being sent to New Relic. Check that another logger is not configured later in your application's configuration. Logs in context for Ruby does not support tagged logging. If you are initializing your logger with a log_tags argument, your custom tags may not appear on the final version of your logs. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 282.5148,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Ruby: Configure <em>logs</em> <em>in</em> context",
        "sections": "Ruby: Configure <em>logs</em> <em>in</em> context",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in context for the Ruby agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Ruby app To <em>enable</em> <em>logs</em>"
      },
      "id": "612d460128ccbc17bf56a863"
    },
    {
      "sections": [
        "Forward logs and activity logs from Azure",
        "Azure Resources Manager logs",
        "Use the Azure Resource Manager (ARM) template",
        "View log data",
        "Send logs from Azure resources",
        "Azure activity logs",
        "What's next?"
      ],
      "title": "Forward logs and activity logs from Azure",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic",
        "Azure",
        "Cloud logs"
      ],
      "external_id": "793bb2faa9c693f007a632d4faf24b1dbaa0c906",
      "image": "https://docs.newrelic.com/static/da32e66650336026701ab338f5afbb60/5f1d2/azure-tile.png",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/azure-log-forwarding/",
      "published_at": "2021-10-18T20:17:19Z",
      "updated_at": "2021-10-07T16:10:48Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Microsoft Azure (ARM), you can use our template to forward and enrich your log data in New Relic. Forwarding your Azure EventHub logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Azure Resources Manager logs The Microsoft Azure Resources Manager (ARM) template provided by New Relic One helps you set up Azure to: Forward logs from EventHub to New Relic One. Forward activity logs to New Relic One through EventHub. The setup process is almost the same for both use cases. As part of the setup process, you can select which Azure activity logs you want to forward to New Relic, including: Administrative Azure Activity Logs Alert Azure Activity Logs Autoscale Azure Activity Logs Policy Azure Activity Logs Recommendation Azure Activity Logs Resource Health Azure Activity Logs Security Azure Activity Logs Service Health Azure Activity Logs Use the Azure Resource Manager (ARM) template The template is idempotent. You can start forwarding logs from EventHub and then rerun the same template to configure Azure activity logs forwarding by completing step 10. Make sure you have a New Relic license key. Log in to New Relic Logs and click Add more data sources on the top right of the page. Under Log ingestion, click the tile. Select the account you want to send the logs, and click Continue. Click Generate API Key and copy the generated API Key. Click Deploy to Azure and a new tab will be open with the ARM template loaded in Azure. Select the Resource Group where you want to create the necessary resources, and a Region. In the New Relic License Key field, paste the previously copied API Key. Ensure the New Relic One endpoint set is the one corresponding to your account. Optional: Set to true the Azure activity logs you want to forward. Click Review + create, review the data you've inserted, and click Create. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If you want to only query for logs coming from Azure, run the following query: SELECT * FROM Log where plugin.type='azure' Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. Send logs from Azure resources By default, this template only configures the needed function and resources to forward logs to New Relic One. We can also configure the activity logs to be forwarded, but there isn't a default log forwarding from your Azure resources. If you want to forward logs from any resource that produces them, you need to configure it by creating a diagnostic setting for the given resource. For example, if you have a function running on Azure and you want to forward the logs to New Relic One, you'll need to configure a diagnostic setting to forward the logs to EventHub. For more information, see the Microsoft documentation to create diagnostic settings for sendig platform logs and metrics to different destinations. Azure activity logs Activating the Azure activity logs forwarding is optional. It provides: More visibility of your Azure resources Activity of the Azure resources Information about performed actions Events and their timestamps The user who performed an action, if applicable These logs give your organization more control over the resources. However, be aware of wrong or unintentional changes on your resources and even unexpected actions. For more information about this kind of event, see the Microsoft Azure Activity Log event schema. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.66534,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward <em>logs</em> and activity <em>logs</em> from Azure",
        "sections": "Azure Resources <em>Manager</em> <em>logs</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "If your <em>log</em> data is already being monitored by Microsoft Azure (ARM), you can use our template to forward and enrich your <em>log</em> data in <em>New</em> <em>Relic</em>. Forwarding your Azure EventHub <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert on your"
      },
      "id": "6150545de7b9d2ae518de37f"
    }
  ],
  "/docs/logs/forward-logs/aws-lambda-sending-cloudwatch-logs": [
    {
      "sections": [
        "Kubernetes plugin for log forwarding",
        "Enable Kubernetes for log management",
        "View log data",
        "What's next?"
      ],
      "title": "Kubernetes plugin for log forwarding",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "c55c6a084a1de057aac4f85ed3807c41317f63e1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/kubernetes-plugin-log-forwarding/",
      "published_at": "2021-10-18T05:22:43Z",
      "updated_at": "2021-10-13T02:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your log data in New Relic. This plugin is also provided in a standalone Docker image that can be installed in a Kubernetes cluster in the form of a DaemonSet. We refer to this as our Kubernetes plugin. Forwarding your Kubernetes logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Enable Kubernetes for log management To forward your logs to New Relic with our Kubernetes plugin for Fluent Bit: Make sure you have: A New Relic license key Kubernetes cluster deployed Follow the procedures in GitHub to install the Kubernetes plugin. Optional: If you are installing the plugin as a Helm chart, you can set numerous configurations. However, we recommend the standard setup, as it is valid for most users. Generate some traffic and wait a few minutes, then check your account for data. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 297.08032,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kubernetes plugin for <em>log</em> forwarding",
        "sections": "<em>Enable</em> Kubernetes for <em>log</em> <em>management</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " to this as our Kubernetes plugin. Forwarding your Kubernetes <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert on your <em>log</em> data. <em>Enable</em> Kubernetes for <em>log</em> <em>management</em> To forward your <em>logs</em> to <em>New</em> <em>Relic</em> with our Kubernetes plugin for Fluent Bit"
      },
      "id": "603ec19128ccbcad24eba748"
    },
    {
      "sections": [
        "Ruby: Configure logs in context",
        "Set up your Ruby app",
        "Standard Ruby on Rails configuration",
        "Incompatible gems",
        "Rails advanced configuration",
        "Ruby configuration without Rails",
        "Lograge configuration",
        "Other logging extensions",
        "Troubleshooting",
        "What's next?"
      ],
      "title": "Ruby: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Logs in context for Ruby"
      ],
      "external_id": "d2100c0d6ddffe6b52a1f968f61f777a1e56a13c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/configure-logs-context-ruby/",
      "published_at": "2021-10-19T03:54:54Z",
      "updated_at": "2021-10-19T03:54:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Ruby agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Ruby app To enable logs in context for APM apps monitored by Ruby: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Ruby agent version, and enable distributed tracing. Use Ruby agent version 6.7.0 or higher for logs in context. For Rails applications, use a supported Rails version. Configure logs in context for Ruby. Standard Ruby on Rails configuration Rails logging is controlled by two components: A logger you can customize by setting config.logger A log formatter you can customize by setting config.log_formatter) In most cases, you should configure logs in context by setting config.log_formatter to the DecoratingFormatter in your Rails application. For more information about Rails configuration, see the rubyonrails.org documentation. In your application's config, require newrelic_rpm, then add the following line: module ________ class Application < Rails::Application ... config.log_formatter = ::NewRelic::Agent::Logging::DecoratingFormatter.new end end Copy This configuration uses the New Relic formatter for log messages, but the remaining configuration is provided by the other Rails settings. Incompatible gems New Relic's decorating logger is known to be incompatible with the following gems: logging semantic logger rails_stdout_logger rails_12factor Rails advanced configuration If setting the log_formatter option doesn't meet your needs, replace the entire Rails logger with an instance of the New Relic logger. Provide the parameters to the logger's constructor, like this: module ________ class Application < Rails::Application ... config.logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( \"log/application.log\", #etc... ) end end Copy Ruby configuration without Rails For non-Rails applications, use the DecoratingLogger in place of the Ruby standard ::Logger, like this: logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( 'log/application.log', #etc... ) ... logger.info(...) Copy The DecoratingLogger is a drop-in replacement for the Ruby standard ::Logger. Their constructors accept the same parameters. Lograge configuration To configure this extension with the lograge gem, follow standard procedures in this doc for Ruby on Rails configuration. No additional configuration is required for the lograge gem. Other logging extensions To use our logging extension with a different logging implementation or with your own custom logger, use the DecoratingFormatter. For example: module ________ class Application < Rails::Application ... config.logger = ::YourCustomLoggerImplementation.new( $stdout, formatter: ::NewRelic::Agent::Logging::DecoratingFormatter.new ) end end Copy To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you have configured your logging in /config/application.rb or in /config/environments/development.rb, run your application locally and check its logging output. You should see some output like this: {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"00fc7d46\",\"timestamp\":1567701375543,\"message\":\"example log message one\",\"log.level\":\"DEBUG\"} {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"6754870b\",\"timestamp\":1567702843604,\"message\":\"example log message two\",\"log.level\":\"DEBUG\"} Copy Troubleshooting If you don't see log data in the UI, follow the troubleshooting procedures. Also, if you see JSON logs in your application's output, but your query does not return logs, check your log forwarder. If the logs from your application are not formatted in JSON with fields like trace.id and span.id, there may be a problem with your log forwarding extension. In this situation: Check that the application is using a supported logging framework. Check that your logging configuration has been applied to all the environments where you want to forward and enrich the log data being sent to New Relic. Check that another logger is not configured later in your application's configuration. Logs in context for Ruby does not support tagged logging. If you are initializing your logger with a log_tags argument, your custom tags may not appear on the final version of your logs. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 282.5146,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Ruby: Configure <em>logs</em> <em>in</em> context",
        "sections": "Ruby: Configure <em>logs</em> <em>in</em> context",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in context for the Ruby agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Ruby app To <em>enable</em> <em>logs</em>"
      },
      "id": "612d460128ccbc17bf56a863"
    },
    {
      "sections": [
        "Forward logs and activity logs from Azure",
        "Azure Resources Manager logs",
        "Use the Azure Resource Manager (ARM) template",
        "View log data",
        "Send logs from Azure resources",
        "Azure activity logs",
        "What's next?"
      ],
      "title": "Forward logs and activity logs from Azure",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic",
        "Azure",
        "Cloud logs"
      ],
      "external_id": "793bb2faa9c693f007a632d4faf24b1dbaa0c906",
      "image": "https://docs.newrelic.com/static/da32e66650336026701ab338f5afbb60/5f1d2/azure-tile.png",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/azure-log-forwarding/",
      "published_at": "2021-10-18T20:17:19Z",
      "updated_at": "2021-10-07T16:10:48Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Microsoft Azure (ARM), you can use our template to forward and enrich your log data in New Relic. Forwarding your Azure EventHub logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Azure Resources Manager logs The Microsoft Azure Resources Manager (ARM) template provided by New Relic One helps you set up Azure to: Forward logs from EventHub to New Relic One. Forward activity logs to New Relic One through EventHub. The setup process is almost the same for both use cases. As part of the setup process, you can select which Azure activity logs you want to forward to New Relic, including: Administrative Azure Activity Logs Alert Azure Activity Logs Autoscale Azure Activity Logs Policy Azure Activity Logs Recommendation Azure Activity Logs Resource Health Azure Activity Logs Security Azure Activity Logs Service Health Azure Activity Logs Use the Azure Resource Manager (ARM) template The template is idempotent. You can start forwarding logs from EventHub and then rerun the same template to configure Azure activity logs forwarding by completing step 10. Make sure you have a New Relic license key. Log in to New Relic Logs and click Add more data sources on the top right of the page. Under Log ingestion, click the tile. Select the account you want to send the logs, and click Continue. Click Generate API Key and copy the generated API Key. Click Deploy to Azure and a new tab will be open with the ARM template loaded in Azure. Select the Resource Group where you want to create the necessary resources, and a Region. In the New Relic License Key field, paste the previously copied API Key. Ensure the New Relic One endpoint set is the one corresponding to your account. Optional: Set to true the Azure activity logs you want to forward. Click Review + create, review the data you've inserted, and click Create. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If you want to only query for logs coming from Azure, run the following query: SELECT * FROM Log where plugin.type='azure' Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. Send logs from Azure resources By default, this template only configures the needed function and resources to forward logs to New Relic One. We can also configure the activity logs to be forwarded, but there isn't a default log forwarding from your Azure resources. If you want to forward logs from any resource that produces them, you need to configure it by creating a diagnostic setting for the given resource. For example, if you have a function running on Azure and you want to forward the logs to New Relic One, you'll need to configure a diagnostic setting to forward the logs to EventHub. For more information, see the Microsoft documentation to create diagnostic settings for sendig platform logs and metrics to different destinations. Azure activity logs Activating the Azure activity logs forwarding is optional. It provides: More visibility of your Azure resources Activity of the Azure resources Information about performed actions Events and their timestamps The user who performed an action, if applicable These logs give your organization more control over the resources. However, be aware of wrong or unintentional changes on your resources and even unexpected actions. For more information about this kind of event, see the Microsoft Azure Activity Log event schema. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.66528,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward <em>logs</em> and activity <em>logs</em> from Azure",
        "sections": "Azure Resources <em>Manager</em> <em>logs</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "If your <em>log</em> data is already being monitored by Microsoft Azure (ARM), you can use our template to forward and enrich your <em>log</em> data in <em>New</em> <em>Relic</em>. Forwarding your Azure EventHub <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert on your"
      },
      "id": "6150545de7b9d2ae518de37f"
    }
  ],
  "/docs/logs/forward-logs/aws-lambda-sending-logs-s3": [
    {
      "sections": [
        "Kubernetes plugin for log forwarding",
        "Enable Kubernetes for log management",
        "View log data",
        "What's next?"
      ],
      "title": "Kubernetes plugin for log forwarding",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "c55c6a084a1de057aac4f85ed3807c41317f63e1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/kubernetes-plugin-log-forwarding/",
      "published_at": "2021-10-18T05:22:43Z",
      "updated_at": "2021-10-13T02:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your log data in New Relic. This plugin is also provided in a standalone Docker image that can be installed in a Kubernetes cluster in the form of a DaemonSet. We refer to this as our Kubernetes plugin. Forwarding your Kubernetes logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Enable Kubernetes for log management To forward your logs to New Relic with our Kubernetes plugin for Fluent Bit: Make sure you have: A New Relic license key Kubernetes cluster deployed Follow the procedures in GitHub to install the Kubernetes plugin. Optional: If you are installing the plugin as a Helm chart, you can set numerous configurations. However, we recommend the standard setup, as it is valid for most users. Generate some traffic and wait a few minutes, then check your account for data. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 297.08032,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kubernetes plugin for <em>log</em> forwarding",
        "sections": "<em>Enable</em> Kubernetes for <em>log</em> <em>management</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " to this as our Kubernetes plugin. Forwarding your Kubernetes <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert on your <em>log</em> data. <em>Enable</em> Kubernetes for <em>log</em> <em>management</em> To forward your <em>logs</em> to <em>New</em> <em>Relic</em> with our Kubernetes plugin for Fluent Bit"
      },
      "id": "603ec19128ccbcad24eba748"
    },
    {
      "sections": [
        "Ruby: Configure logs in context",
        "Set up your Ruby app",
        "Standard Ruby on Rails configuration",
        "Incompatible gems",
        "Rails advanced configuration",
        "Ruby configuration without Rails",
        "Lograge configuration",
        "Other logging extensions",
        "Troubleshooting",
        "What's next?"
      ],
      "title": "Ruby: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Logs in context for Ruby"
      ],
      "external_id": "d2100c0d6ddffe6b52a1f968f61f777a1e56a13c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/configure-logs-context-ruby/",
      "published_at": "2021-10-19T03:54:54Z",
      "updated_at": "2021-10-19T03:54:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Ruby agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Ruby app To enable logs in context for APM apps monitored by Ruby: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Ruby agent version, and enable distributed tracing. Use Ruby agent version 6.7.0 or higher for logs in context. For Rails applications, use a supported Rails version. Configure logs in context for Ruby. Standard Ruby on Rails configuration Rails logging is controlled by two components: A logger you can customize by setting config.logger A log formatter you can customize by setting config.log_formatter) In most cases, you should configure logs in context by setting config.log_formatter to the DecoratingFormatter in your Rails application. For more information about Rails configuration, see the rubyonrails.org documentation. In your application's config, require newrelic_rpm, then add the following line: module ________ class Application < Rails::Application ... config.log_formatter = ::NewRelic::Agent::Logging::DecoratingFormatter.new end end Copy This configuration uses the New Relic formatter for log messages, but the remaining configuration is provided by the other Rails settings. Incompatible gems New Relic's decorating logger is known to be incompatible with the following gems: logging semantic logger rails_stdout_logger rails_12factor Rails advanced configuration If setting the log_formatter option doesn't meet your needs, replace the entire Rails logger with an instance of the New Relic logger. Provide the parameters to the logger's constructor, like this: module ________ class Application < Rails::Application ... config.logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( \"log/application.log\", #etc... ) end end Copy Ruby configuration without Rails For non-Rails applications, use the DecoratingLogger in place of the Ruby standard ::Logger, like this: logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( 'log/application.log', #etc... ) ... logger.info(...) Copy The DecoratingLogger is a drop-in replacement for the Ruby standard ::Logger. Their constructors accept the same parameters. Lograge configuration To configure this extension with the lograge gem, follow standard procedures in this doc for Ruby on Rails configuration. No additional configuration is required for the lograge gem. Other logging extensions To use our logging extension with a different logging implementation or with your own custom logger, use the DecoratingFormatter. For example: module ________ class Application < Rails::Application ... config.logger = ::YourCustomLoggerImplementation.new( $stdout, formatter: ::NewRelic::Agent::Logging::DecoratingFormatter.new ) end end Copy To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you have configured your logging in /config/application.rb or in /config/environments/development.rb, run your application locally and check its logging output. You should see some output like this: {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"00fc7d46\",\"timestamp\":1567701375543,\"message\":\"example log message one\",\"log.level\":\"DEBUG\"} {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"6754870b\",\"timestamp\":1567702843604,\"message\":\"example log message two\",\"log.level\":\"DEBUG\"} Copy Troubleshooting If you don't see log data in the UI, follow the troubleshooting procedures. Also, if you see JSON logs in your application's output, but your query does not return logs, check your log forwarder. If the logs from your application are not formatted in JSON with fields like trace.id and span.id, there may be a problem with your log forwarding extension. In this situation: Check that the application is using a supported logging framework. Check that your logging configuration has been applied to all the environments where you want to forward and enrich the log data being sent to New Relic. Check that another logger is not configured later in your application's configuration. Logs in context for Ruby does not support tagged logging. If you are initializing your logger with a log_tags argument, your custom tags may not appear on the final version of your logs. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 282.5146,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Ruby: Configure <em>logs</em> <em>in</em> context",
        "sections": "Ruby: Configure <em>logs</em> <em>in</em> context",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in context for the Ruby agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Ruby app To <em>enable</em> <em>logs</em>"
      },
      "id": "612d460128ccbc17bf56a863"
    },
    {
      "sections": [
        "Forward logs and activity logs from Azure",
        "Azure Resources Manager logs",
        "Use the Azure Resource Manager (ARM) template",
        "View log data",
        "Send logs from Azure resources",
        "Azure activity logs",
        "What's next?"
      ],
      "title": "Forward logs and activity logs from Azure",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic",
        "Azure",
        "Cloud logs"
      ],
      "external_id": "793bb2faa9c693f007a632d4faf24b1dbaa0c906",
      "image": "https://docs.newrelic.com/static/da32e66650336026701ab338f5afbb60/5f1d2/azure-tile.png",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/azure-log-forwarding/",
      "published_at": "2021-10-18T20:17:19Z",
      "updated_at": "2021-10-07T16:10:48Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Microsoft Azure (ARM), you can use our template to forward and enrich your log data in New Relic. Forwarding your Azure EventHub logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Azure Resources Manager logs The Microsoft Azure Resources Manager (ARM) template provided by New Relic One helps you set up Azure to: Forward logs from EventHub to New Relic One. Forward activity logs to New Relic One through EventHub. The setup process is almost the same for both use cases. As part of the setup process, you can select which Azure activity logs you want to forward to New Relic, including: Administrative Azure Activity Logs Alert Azure Activity Logs Autoscale Azure Activity Logs Policy Azure Activity Logs Recommendation Azure Activity Logs Resource Health Azure Activity Logs Security Azure Activity Logs Service Health Azure Activity Logs Use the Azure Resource Manager (ARM) template The template is idempotent. You can start forwarding logs from EventHub and then rerun the same template to configure Azure activity logs forwarding by completing step 10. Make sure you have a New Relic license key. Log in to New Relic Logs and click Add more data sources on the top right of the page. Under Log ingestion, click the tile. Select the account you want to send the logs, and click Continue. Click Generate API Key and copy the generated API Key. Click Deploy to Azure and a new tab will be open with the ARM template loaded in Azure. Select the Resource Group where you want to create the necessary resources, and a Region. In the New Relic License Key field, paste the previously copied API Key. Ensure the New Relic One endpoint set is the one corresponding to your account. Optional: Set to true the Azure activity logs you want to forward. Click Review + create, review the data you've inserted, and click Create. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If you want to only query for logs coming from Azure, run the following query: SELECT * FROM Log where plugin.type='azure' Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. Send logs from Azure resources By default, this template only configures the needed function and resources to forward logs to New Relic One. We can also configure the activity logs to be forwarded, but there isn't a default log forwarding from your Azure resources. If you want to forward logs from any resource that produces them, you need to configure it by creating a diagnostic setting for the given resource. For example, if you have a function running on Azure and you want to forward the logs to New Relic One, you'll need to configure a diagnostic setting to forward the logs to EventHub. For more information, see the Microsoft documentation to create diagnostic settings for sendig platform logs and metrics to different destinations. Azure activity logs Activating the Azure activity logs forwarding is optional. It provides: More visibility of your Azure resources Activity of the Azure resources Information about performed actions Events and their timestamps The user who performed an action, if applicable These logs give your organization more control over the resources. However, be aware of wrong or unintentional changes on your resources and even unexpected actions. For more information about this kind of event, see the Microsoft Azure Activity Log event schema. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.66528,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward <em>logs</em> and activity <em>logs</em> from Azure",
        "sections": "Azure Resources <em>Manager</em> <em>logs</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "If your <em>log</em> data is already being monitored by Microsoft Azure (ARM), you can use our template to forward and enrich your <em>log</em> data in <em>New</em> <em>Relic</em>. Forwarding your Azure EventHub <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert on your"
      },
      "id": "6150545de7b9d2ae518de37f"
    }
  ],
  "/docs/logs/forward-logs/azure-log-forwarding": [
    {
      "sections": [
        "Kubernetes plugin for log forwarding",
        "Enable Kubernetes for log management",
        "View log data",
        "What's next?"
      ],
      "title": "Kubernetes plugin for log forwarding",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "c55c6a084a1de057aac4f85ed3807c41317f63e1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/kubernetes-plugin-log-forwarding/",
      "published_at": "2021-10-18T05:22:43Z",
      "updated_at": "2021-10-13T02:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your log data in New Relic. This plugin is also provided in a standalone Docker image that can be installed in a Kubernetes cluster in the form of a DaemonSet. We refer to this as our Kubernetes plugin. Forwarding your Kubernetes logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Enable Kubernetes for log management To forward your logs to New Relic with our Kubernetes plugin for Fluent Bit: Make sure you have: A New Relic license key Kubernetes cluster deployed Follow the procedures in GitHub to install the Kubernetes plugin. Optional: If you are installing the plugin as a Helm chart, you can set numerous configurations. However, we recommend the standard setup, as it is valid for most users. Generate some traffic and wait a few minutes, then check your account for data. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 300.8494,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kubernetes plugin for <em>log</em> forwarding",
        "sections": "<em>Enable</em> Kubernetes for <em>log</em> <em>management</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " to this as our Kubernetes plugin. Forwarding your Kubernetes <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert on your <em>log</em> data. <em>Enable</em> Kubernetes for <em>log</em> <em>management</em> To forward your <em>logs</em> to <em>New</em> <em>Relic</em> with our Kubernetes plugin for Fluent Bit"
      },
      "id": "603ec19128ccbcad24eba748"
    },
    {
      "sections": [
        "AWS Lambda for sending CloudWatch logs",
        "Install and configure the Cloudwatch logs Lambda function",
        "Create a Lambda trigger",
        "Configure retries (optional)",
        "Tip",
        "Resources created by the SAM template",
        "View log data",
        "What's next?"
      ],
      "title": "AWS Lambda for sending CloudWatch logs",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "69c310375d48a667779ffabead6f920eb6a34004",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/aws-lambda-sending-cloudwatch-logs/",
      "published_at": "2021-10-18T20:17:19Z",
      "updated_at": "2021-10-07T18:51:52Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can send your Amazon CloudWatch logs to New Relic using our AWS Lambda function, newrelic-log-ingestion. This can be easily deployed from the AWS Serverless application repository. Forwarding your CloudWatch logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Install and configure the Cloudwatch logs Lambda function The following setup shows one approach for configuring environment variables. You can also configure them from the Functions page. Complete the following: Make sure you have a New Relic license key. Open the AWS Serverless Application Repository in your browser. Search for newrelic and check Show apps that create custom IAM roles or resource policies to find newrelic-log-ingestion. Open the newrelic-log-ingestion details and click Deploy. Scroll to the Application settings and configure log forwarding using the following environment variables: Key Description DEBUG_LOGGING_ENABLED A boolean to determine if you want to output debug messages in the CloudWatch console. Optional. To turn on debug logs, set this to true. Default is false. LICENSE_KEY New Relic license key is used for sending data to New Relic. Required. LOGGING_ENABLED Determines if logs are forwarded to New Relic. Required. To turn on logging, set this to true. NR_LOGGING_ENDPOINT New Relic ingestion endpoint for logs. Required. Two endpoints are available: US: https://log-api.newrelic.com/log/v1 EU: https://log-api.eu.newrelic.com/log/v1 NR_TAGS Specify tags to be added to all log events. Optional. Each tag is composed of a colon-delimited key and value. Multiple key-value pairs are semicolon-delimited; for example, env:prod;team:myTeam. Acknowledge that the app creates custom IAM roles, and then click Deploy. Once the process completes, create a Lambda trigger to link your Lambda function to CloudWatch logs. Create a Lambda trigger To get your logs streaming to New Relic, attach a trigger to the Lambda: From the left side menu, select Functions. Find and select the previously created newrelic-log-ingestion function. Under Designer, click Add Triggers, and select Cloudwatch Logs from the dropdown. Select the the appropriate Log group for your application. Enter a name for your filter. Optional: Enter a filter pattern. Check the Enable trigger checkbox, then click Add to create the trigger. Configure retries (optional) You can configure the number of retries you want to perform in case the function fails to send the data in case of communication issues. Recommended number is 3 retries, but you can change the retry behavior by changing the below parameters: Tip The more the number of retries there are can make the function run for longer times. This increases the probability of having higher costs for Lambda. However, decreasing the number of retries could increase the probability of data loss. MAX_RETRIES = 3 # Defines the number of retries after lambda failure to deliver data INITIAL_BACKOFF = 1 # Defines the initial wait seconds until next retry is executed BACKOFF_MULTIPLIER = 2 # Time multiplier between the retries As an example, in default above configuration, first retry will happen after 1 second, second retry after 2 seconds and third retry will happen after 4 seconds. Copy Resources created by the SAM template When you create the application from the repository, the following resources are also created: The Lambda function itself A role used to give execution permissions to the Lambda function based in CloudWatch Logs. All other Lambda configurations not listed can be left as the defaults. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.08124,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "AWS Lambda for sending <em>CloudWatch</em> <em>logs</em>",
        "sections": "<em>Install</em> and configure the <em>Cloudwatch</em> <em>logs</em> Lambda function",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "You can send your Amazon <em>Cloud</em>Watch <em>logs</em> to <em>New</em> <em>Relic</em> using our AWS Lambda function, newrelic-<em>log</em>-ingestion. This can be easily deployed from the AWS Serverless application repository. Forwarding your <em>Cloud</em>Watch <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect"
      },
      "id": "603ea6bb28ccbc228deba74c"
    },
    {
      "sections": [
        "Ruby: Configure logs in context",
        "Set up your Ruby app",
        "Standard Ruby on Rails configuration",
        "Incompatible gems",
        "Rails advanced configuration",
        "Ruby configuration without Rails",
        "Lograge configuration",
        "Other logging extensions",
        "Troubleshooting",
        "What's next?"
      ],
      "title": "Ruby: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Logs in context for Ruby"
      ],
      "external_id": "d2100c0d6ddffe6b52a1f968f61f777a1e56a13c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/configure-logs-context-ruby/",
      "published_at": "2021-10-19T03:54:54Z",
      "updated_at": "2021-10-19T03:54:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Ruby agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Ruby app To enable logs in context for APM apps monitored by Ruby: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Ruby agent version, and enable distributed tracing. Use Ruby agent version 6.7.0 or higher for logs in context. For Rails applications, use a supported Rails version. Configure logs in context for Ruby. Standard Ruby on Rails configuration Rails logging is controlled by two components: A logger you can customize by setting config.logger A log formatter you can customize by setting config.log_formatter) In most cases, you should configure logs in context by setting config.log_formatter to the DecoratingFormatter in your Rails application. For more information about Rails configuration, see the rubyonrails.org documentation. In your application's config, require newrelic_rpm, then add the following line: module ________ class Application < Rails::Application ... config.log_formatter = ::NewRelic::Agent::Logging::DecoratingFormatter.new end end Copy This configuration uses the New Relic formatter for log messages, but the remaining configuration is provided by the other Rails settings. Incompatible gems New Relic's decorating logger is known to be incompatible with the following gems: logging semantic logger rails_stdout_logger rails_12factor Rails advanced configuration If setting the log_formatter option doesn't meet your needs, replace the entire Rails logger with an instance of the New Relic logger. Provide the parameters to the logger's constructor, like this: module ________ class Application < Rails::Application ... config.logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( \"log/application.log\", #etc... ) end end Copy Ruby configuration without Rails For non-Rails applications, use the DecoratingLogger in place of the Ruby standard ::Logger, like this: logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( 'log/application.log', #etc... ) ... logger.info(...) Copy The DecoratingLogger is a drop-in replacement for the Ruby standard ::Logger. Their constructors accept the same parameters. Lograge configuration To configure this extension with the lograge gem, follow standard procedures in this doc for Ruby on Rails configuration. No additional configuration is required for the lograge gem. Other logging extensions To use our logging extension with a different logging implementation or with your own custom logger, use the DecoratingFormatter. For example: module ________ class Application < Rails::Application ... config.logger = ::YourCustomLoggerImplementation.new( $stdout, formatter: ::NewRelic::Agent::Logging::DecoratingFormatter.new ) end end Copy To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you have configured your logging in /config/application.rb or in /config/environments/development.rb, run your application locally and check its logging output. You should see some output like this: {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"00fc7d46\",\"timestamp\":1567701375543,\"message\":\"example log message one\",\"log.level\":\"DEBUG\"} {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"6754870b\",\"timestamp\":1567702843604,\"message\":\"example log message two\",\"log.level\":\"DEBUG\"} Copy Troubleshooting If you don't see log data in the UI, follow the troubleshooting procedures. Also, if you see JSON logs in your application's output, but your query does not return logs, check your log forwarder. If the logs from your application are not formatted in JSON with fields like trace.id and span.id, there may be a problem with your log forwarding extension. In this situation: Check that the application is using a supported logging framework. Check that your logging configuration has been applied to all the environments where you want to forward and enrich the log data being sent to New Relic. Check that another logger is not configured later in your application's configuration. Logs in context for Ruby does not support tagged logging. If you are initializing your logger with a log_tags argument, your custom tags may not appear on the final version of your logs. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 285.71515,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Ruby: Configure <em>logs</em> <em>in</em> context",
        "sections": "Ruby: Configure <em>logs</em> <em>in</em> context",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in context for the Ruby agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Ruby app To <em>enable</em> <em>logs</em>"
      },
      "id": "612d460128ccbc17bf56a863"
    }
  ],
  "/docs/logs/forward-logs/enable-log-management-new-relic": [
    {
      "sections": [
        "Kubernetes plugin for log forwarding",
        "Enable Kubernetes for log management",
        "View log data",
        "What's next?"
      ],
      "title": "Kubernetes plugin for log forwarding",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "c55c6a084a1de057aac4f85ed3807c41317f63e1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/kubernetes-plugin-log-forwarding/",
      "published_at": "2021-10-18T05:22:43Z",
      "updated_at": "2021-10-13T02:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your log data in New Relic. This plugin is also provided in a standalone Docker image that can be installed in a Kubernetes cluster in the form of a DaemonSet. We refer to this as our Kubernetes plugin. Forwarding your Kubernetes logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Enable Kubernetes for log management To forward your logs to New Relic with our Kubernetes plugin for Fluent Bit: Make sure you have: A New Relic license key Kubernetes cluster deployed Follow the procedures in GitHub to install the Kubernetes plugin. Optional: If you are installing the plugin as a Helm chart, you can set numerous configurations. However, we recommend the standard setup, as it is valid for most users. Generate some traffic and wait a few minutes, then check your account for data. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 297.08023,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kubernetes plugin for <em>log</em> forwarding",
        "sections": "<em>Enable</em> Kubernetes for <em>log</em> <em>management</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " to this as our Kubernetes plugin. Forwarding your Kubernetes <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert on your <em>log</em> data. <em>Enable</em> Kubernetes for <em>log</em> <em>management</em> To forward your <em>logs</em> to <em>New</em> <em>Relic</em> with our Kubernetes plugin for Fluent Bit"
      },
      "id": "603ec19128ccbcad24eba748"
    },
    {
      "sections": [
        "Ruby: Configure logs in context",
        "Set up your Ruby app",
        "Standard Ruby on Rails configuration",
        "Incompatible gems",
        "Rails advanced configuration",
        "Ruby configuration without Rails",
        "Lograge configuration",
        "Other logging extensions",
        "Troubleshooting",
        "What's next?"
      ],
      "title": "Ruby: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Logs in context for Ruby"
      ],
      "external_id": "d2100c0d6ddffe6b52a1f968f61f777a1e56a13c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/configure-logs-context-ruby/",
      "published_at": "2021-10-19T03:54:54Z",
      "updated_at": "2021-10-19T03:54:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Ruby agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Ruby app To enable logs in context for APM apps monitored by Ruby: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Ruby agent version, and enable distributed tracing. Use Ruby agent version 6.7.0 or higher for logs in context. For Rails applications, use a supported Rails version. Configure logs in context for Ruby. Standard Ruby on Rails configuration Rails logging is controlled by two components: A logger you can customize by setting config.logger A log formatter you can customize by setting config.log_formatter) In most cases, you should configure logs in context by setting config.log_formatter to the DecoratingFormatter in your Rails application. For more information about Rails configuration, see the rubyonrails.org documentation. In your application's config, require newrelic_rpm, then add the following line: module ________ class Application < Rails::Application ... config.log_formatter = ::NewRelic::Agent::Logging::DecoratingFormatter.new end end Copy This configuration uses the New Relic formatter for log messages, but the remaining configuration is provided by the other Rails settings. Incompatible gems New Relic's decorating logger is known to be incompatible with the following gems: logging semantic logger rails_stdout_logger rails_12factor Rails advanced configuration If setting the log_formatter option doesn't meet your needs, replace the entire Rails logger with an instance of the New Relic logger. Provide the parameters to the logger's constructor, like this: module ________ class Application < Rails::Application ... config.logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( \"log/application.log\", #etc... ) end end Copy Ruby configuration without Rails For non-Rails applications, use the DecoratingLogger in place of the Ruby standard ::Logger, like this: logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( 'log/application.log', #etc... ) ... logger.info(...) Copy The DecoratingLogger is a drop-in replacement for the Ruby standard ::Logger. Their constructors accept the same parameters. Lograge configuration To configure this extension with the lograge gem, follow standard procedures in this doc for Ruby on Rails configuration. No additional configuration is required for the lograge gem. Other logging extensions To use our logging extension with a different logging implementation or with your own custom logger, use the DecoratingFormatter. For example: module ________ class Application < Rails::Application ... config.logger = ::YourCustomLoggerImplementation.new( $stdout, formatter: ::NewRelic::Agent::Logging::DecoratingFormatter.new ) end end Copy To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you have configured your logging in /config/application.rb or in /config/environments/development.rb, run your application locally and check its logging output. You should see some output like this: {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"00fc7d46\",\"timestamp\":1567701375543,\"message\":\"example log message one\",\"log.level\":\"DEBUG\"} {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"6754870b\",\"timestamp\":1567702843604,\"message\":\"example log message two\",\"log.level\":\"DEBUG\"} Copy Troubleshooting If you don't see log data in the UI, follow the troubleshooting procedures. Also, if you see JSON logs in your application's output, but your query does not return logs, check your log forwarder. If the logs from your application are not formatted in JSON with fields like trace.id and span.id, there may be a problem with your log forwarding extension. In this situation: Check that the application is using a supported logging framework. Check that your logging configuration has been applied to all the environments where you want to forward and enrich the log data being sent to New Relic. Check that another logger is not configured later in your application's configuration. Logs in context for Ruby does not support tagged logging. If you are initializing your logger with a log_tags argument, your custom tags may not appear on the final version of your logs. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 282.5144,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Ruby: Configure <em>logs</em> <em>in</em> context",
        "sections": "Ruby: Configure <em>logs</em> <em>in</em> context",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in context for the Ruby agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Ruby app To <em>enable</em> <em>logs</em>"
      },
      "id": "612d460128ccbc17bf56a863"
    },
    {
      "sections": [
        "Forward logs and activity logs from Azure",
        "Azure Resources Manager logs",
        "Use the Azure Resource Manager (ARM) template",
        "View log data",
        "Send logs from Azure resources",
        "Azure activity logs",
        "What's next?"
      ],
      "title": "Forward logs and activity logs from Azure",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic",
        "Azure",
        "Cloud logs"
      ],
      "external_id": "793bb2faa9c693f007a632d4faf24b1dbaa0c906",
      "image": "https://docs.newrelic.com/static/da32e66650336026701ab338f5afbb60/5f1d2/azure-tile.png",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/azure-log-forwarding/",
      "published_at": "2021-10-18T20:17:19Z",
      "updated_at": "2021-10-07T16:10:48Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Microsoft Azure (ARM), you can use our template to forward and enrich your log data in New Relic. Forwarding your Azure EventHub logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Azure Resources Manager logs The Microsoft Azure Resources Manager (ARM) template provided by New Relic One helps you set up Azure to: Forward logs from EventHub to New Relic One. Forward activity logs to New Relic One through EventHub. The setup process is almost the same for both use cases. As part of the setup process, you can select which Azure activity logs you want to forward to New Relic, including: Administrative Azure Activity Logs Alert Azure Activity Logs Autoscale Azure Activity Logs Policy Azure Activity Logs Recommendation Azure Activity Logs Resource Health Azure Activity Logs Security Azure Activity Logs Service Health Azure Activity Logs Use the Azure Resource Manager (ARM) template The template is idempotent. You can start forwarding logs from EventHub and then rerun the same template to configure Azure activity logs forwarding by completing step 10. Make sure you have a New Relic license key. Log in to New Relic Logs and click Add more data sources on the top right of the page. Under Log ingestion, click the tile. Select the account you want to send the logs, and click Continue. Click Generate API Key and copy the generated API Key. Click Deploy to Azure and a new tab will be open with the ARM template loaded in Azure. Select the Resource Group where you want to create the necessary resources, and a Region. In the New Relic License Key field, paste the previously copied API Key. Ensure the New Relic One endpoint set is the one corresponding to your account. Optional: Set to true the Azure activity logs you want to forward. Click Review + create, review the data you've inserted, and click Create. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If you want to only query for logs coming from Azure, run the following query: SELECT * FROM Log where plugin.type='azure' Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. Send logs from Azure resources By default, this template only configures the needed function and resources to forward logs to New Relic One. We can also configure the activity logs to be forwarded, but there isn't a default log forwarding from your Azure resources. If you want to forward logs from any resource that produces them, you need to configure it by creating a diagnostic setting for the given resource. For example, if you have a function running on Azure and you want to forward the logs to New Relic One, you'll need to configure a diagnostic setting to forward the logs to EventHub. For more information, see the Microsoft documentation to create diagnostic settings for sendig platform logs and metrics to different destinations. Azure activity logs Activating the Azure activity logs forwarding is optional. It provides: More visibility of your Azure resources Activity of the Azure resources Information about performed actions Events and their timestamps The user who performed an action, if applicable These logs give your organization more control over the resources. However, be aware of wrong or unintentional changes on your resources and even unexpected actions. For more information about this kind of event, see the Microsoft Azure Activity Log event schema. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.66522,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward <em>logs</em> and activity <em>logs</em> from Azure",
        "sections": "Azure Resources <em>Manager</em> <em>logs</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "If your <em>log</em> data is already being monitored by Microsoft Azure (ARM), you can use our template to forward and enrich your <em>log</em> data in <em>New</em> <em>Relic</em>. Forwarding your Azure EventHub <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert on your"
      },
      "id": "6150545de7b9d2ae518de37f"
    }
  ],
  "/docs/logs/forward-logs/fluent-bit-plugin-log-forwarding": [
    {
      "sections": [
        "Kubernetes plugin for log forwarding",
        "Enable Kubernetes for log management",
        "View log data",
        "What's next?"
      ],
      "title": "Kubernetes plugin for log forwarding",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "c55c6a084a1de057aac4f85ed3807c41317f63e1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/kubernetes-plugin-log-forwarding/",
      "published_at": "2021-10-18T05:22:43Z",
      "updated_at": "2021-10-13T02:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your log data in New Relic. This plugin is also provided in a standalone Docker image that can be installed in a Kubernetes cluster in the form of a DaemonSet. We refer to this as our Kubernetes plugin. Forwarding your Kubernetes logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Enable Kubernetes for log management To forward your logs to New Relic with our Kubernetes plugin for Fluent Bit: Make sure you have: A New Relic license key Kubernetes cluster deployed Follow the procedures in GitHub to install the Kubernetes plugin. Optional: If you are installing the plugin as a Helm chart, you can set numerous configurations. However, we recommend the standard setup, as it is valid for most users. Generate some traffic and wait a few minutes, then check your account for data. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 297.08014,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kubernetes plugin for <em>log</em> forwarding",
        "sections": "<em>Enable</em> Kubernetes for <em>log</em> <em>management</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " to this as our Kubernetes plugin. Forwarding your Kubernetes <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert on your <em>log</em> data. <em>Enable</em> Kubernetes for <em>log</em> <em>management</em> To forward your <em>logs</em> to <em>New</em> <em>Relic</em> with our Kubernetes plugin for Fluent Bit"
      },
      "id": "603ec19128ccbcad24eba748"
    },
    {
      "sections": [
        "Ruby: Configure logs in context",
        "Set up your Ruby app",
        "Standard Ruby on Rails configuration",
        "Incompatible gems",
        "Rails advanced configuration",
        "Ruby configuration without Rails",
        "Lograge configuration",
        "Other logging extensions",
        "Troubleshooting",
        "What's next?"
      ],
      "title": "Ruby: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Logs in context for Ruby"
      ],
      "external_id": "d2100c0d6ddffe6b52a1f968f61f777a1e56a13c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/configure-logs-context-ruby/",
      "published_at": "2021-10-19T03:54:54Z",
      "updated_at": "2021-10-19T03:54:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Ruby agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Ruby app To enable logs in context for APM apps monitored by Ruby: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Ruby agent version, and enable distributed tracing. Use Ruby agent version 6.7.0 or higher for logs in context. For Rails applications, use a supported Rails version. Configure logs in context for Ruby. Standard Ruby on Rails configuration Rails logging is controlled by two components: A logger you can customize by setting config.logger A log formatter you can customize by setting config.log_formatter) In most cases, you should configure logs in context by setting config.log_formatter to the DecoratingFormatter in your Rails application. For more information about Rails configuration, see the rubyonrails.org documentation. In your application's config, require newrelic_rpm, then add the following line: module ________ class Application < Rails::Application ... config.log_formatter = ::NewRelic::Agent::Logging::DecoratingFormatter.new end end Copy This configuration uses the New Relic formatter for log messages, but the remaining configuration is provided by the other Rails settings. Incompatible gems New Relic's decorating logger is known to be incompatible with the following gems: logging semantic logger rails_stdout_logger rails_12factor Rails advanced configuration If setting the log_formatter option doesn't meet your needs, replace the entire Rails logger with an instance of the New Relic logger. Provide the parameters to the logger's constructor, like this: module ________ class Application < Rails::Application ... config.logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( \"log/application.log\", #etc... ) end end Copy Ruby configuration without Rails For non-Rails applications, use the DecoratingLogger in place of the Ruby standard ::Logger, like this: logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( 'log/application.log', #etc... ) ... logger.info(...) Copy The DecoratingLogger is a drop-in replacement for the Ruby standard ::Logger. Their constructors accept the same parameters. Lograge configuration To configure this extension with the lograge gem, follow standard procedures in this doc for Ruby on Rails configuration. No additional configuration is required for the lograge gem. Other logging extensions To use our logging extension with a different logging implementation or with your own custom logger, use the DecoratingFormatter. For example: module ________ class Application < Rails::Application ... config.logger = ::YourCustomLoggerImplementation.new( $stdout, formatter: ::NewRelic::Agent::Logging::DecoratingFormatter.new ) end end Copy To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you have configured your logging in /config/application.rb or in /config/environments/development.rb, run your application locally and check its logging output. You should see some output like this: {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"00fc7d46\",\"timestamp\":1567701375543,\"message\":\"example log message one\",\"log.level\":\"DEBUG\"} {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"6754870b\",\"timestamp\":1567702843604,\"message\":\"example log message two\",\"log.level\":\"DEBUG\"} Copy Troubleshooting If you don't see log data in the UI, follow the troubleshooting procedures. Also, if you see JSON logs in your application's output, but your query does not return logs, check your log forwarder. If the logs from your application are not formatted in JSON with fields like trace.id and span.id, there may be a problem with your log forwarding extension. In this situation: Check that the application is using a supported logging framework. Check that your logging configuration has been applied to all the environments where you want to forward and enrich the log data being sent to New Relic. Check that another logger is not configured later in your application's configuration. Logs in context for Ruby does not support tagged logging. If you are initializing your logger with a log_tags argument, your custom tags may not appear on the final version of your logs. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 282.51422,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Ruby: Configure <em>logs</em> <em>in</em> context",
        "sections": "Ruby: Configure <em>logs</em> <em>in</em> context",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in context for the Ruby agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Ruby app To <em>enable</em> <em>logs</em>"
      },
      "id": "612d460128ccbc17bf56a863"
    },
    {
      "sections": [
        "Forward logs and activity logs from Azure",
        "Azure Resources Manager logs",
        "Use the Azure Resource Manager (ARM) template",
        "View log data",
        "Send logs from Azure resources",
        "Azure activity logs",
        "What's next?"
      ],
      "title": "Forward logs and activity logs from Azure",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic",
        "Azure",
        "Cloud logs"
      ],
      "external_id": "793bb2faa9c693f007a632d4faf24b1dbaa0c906",
      "image": "https://docs.newrelic.com/static/da32e66650336026701ab338f5afbb60/5f1d2/azure-tile.png",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/azure-log-forwarding/",
      "published_at": "2021-10-18T20:17:19Z",
      "updated_at": "2021-10-07T16:10:48Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Microsoft Azure (ARM), you can use our template to forward and enrich your log data in New Relic. Forwarding your Azure EventHub logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Azure Resources Manager logs The Microsoft Azure Resources Manager (ARM) template provided by New Relic One helps you set up Azure to: Forward logs from EventHub to New Relic One. Forward activity logs to New Relic One through EventHub. The setup process is almost the same for both use cases. As part of the setup process, you can select which Azure activity logs you want to forward to New Relic, including: Administrative Azure Activity Logs Alert Azure Activity Logs Autoscale Azure Activity Logs Policy Azure Activity Logs Recommendation Azure Activity Logs Resource Health Azure Activity Logs Security Azure Activity Logs Service Health Azure Activity Logs Use the Azure Resource Manager (ARM) template The template is idempotent. You can start forwarding logs from EventHub and then rerun the same template to configure Azure activity logs forwarding by completing step 10. Make sure you have a New Relic license key. Log in to New Relic Logs and click Add more data sources on the top right of the page. Under Log ingestion, click the tile. Select the account you want to send the logs, and click Continue. Click Generate API Key and copy the generated API Key. Click Deploy to Azure and a new tab will be open with the ARM template loaded in Azure. Select the Resource Group where you want to create the necessary resources, and a Region. In the New Relic License Key field, paste the previously copied API Key. Ensure the New Relic One endpoint set is the one corresponding to your account. Optional: Set to true the Azure activity logs you want to forward. Click Review + create, review the data you've inserted, and click Create. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If you want to only query for logs coming from Azure, run the following query: SELECT * FROM Log where plugin.type='azure' Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. Send logs from Azure resources By default, this template only configures the needed function and resources to forward logs to New Relic One. We can also configure the activity logs to be forwarded, but there isn't a default log forwarding from your Azure resources. If you want to forward logs from any resource that produces them, you need to configure it by creating a diagnostic setting for the given resource. For example, if you have a function running on Azure and you want to forward the logs to New Relic One, you'll need to configure a diagnostic setting to forward the logs to EventHub. For more information, see the Microsoft documentation to create diagnostic settings for sendig platform logs and metrics to different destinations. Azure activity logs Activating the Azure activity logs forwarding is optional. It provides: More visibility of your Azure resources Activity of the Azure resources Information about performed actions Events and their timestamps The user who performed an action, if applicable These logs give your organization more control over the resources. However, be aware of wrong or unintentional changes on your resources and even unexpected actions. For more information about this kind of event, see the Microsoft Azure Activity Log event schema. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.66516,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward <em>logs</em> and activity <em>logs</em> from Azure",
        "sections": "Azure Resources <em>Manager</em> <em>logs</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "If your <em>log</em> data is already being monitored by Microsoft Azure (ARM), you can use our template to forward and enrich your <em>log</em> data in <em>New</em> <em>Relic</em>. Forwarding your Azure EventHub <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert on your"
      },
      "id": "6150545de7b9d2ae518de37f"
    }
  ],
  "/docs/logs/forward-logs/fluentd-plugin-log-forwarding": [
    {
      "sections": [
        "Kubernetes plugin for log forwarding",
        "Enable Kubernetes for log management",
        "View log data",
        "What's next?"
      ],
      "title": "Kubernetes plugin for log forwarding",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "c55c6a084a1de057aac4f85ed3807c41317f63e1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/kubernetes-plugin-log-forwarding/",
      "published_at": "2021-10-18T05:22:43Z",
      "updated_at": "2021-10-13T02:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your log data in New Relic. This plugin is also provided in a standalone Docker image that can be installed in a Kubernetes cluster in the form of a DaemonSet. We refer to this as our Kubernetes plugin. Forwarding your Kubernetes logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Enable Kubernetes for log management To forward your logs to New Relic with our Kubernetes plugin for Fluent Bit: Make sure you have: A New Relic license key Kubernetes cluster deployed Follow the procedures in GitHub to install the Kubernetes plugin. Optional: If you are installing the plugin as a Helm chart, you can set numerous configurations. However, we recommend the standard setup, as it is valid for most users. Generate some traffic and wait a few minutes, then check your account for data. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 297.08014,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kubernetes plugin for <em>log</em> forwarding",
        "sections": "<em>Enable</em> Kubernetes for <em>log</em> <em>management</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " to this as our Kubernetes plugin. Forwarding your Kubernetes <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert on your <em>log</em> data. <em>Enable</em> Kubernetes for <em>log</em> <em>management</em> To forward your <em>logs</em> to <em>New</em> <em>Relic</em> with our Kubernetes plugin for Fluent Bit"
      },
      "id": "603ec19128ccbcad24eba748"
    },
    {
      "sections": [
        "Ruby: Configure logs in context",
        "Set up your Ruby app",
        "Standard Ruby on Rails configuration",
        "Incompatible gems",
        "Rails advanced configuration",
        "Ruby configuration without Rails",
        "Lograge configuration",
        "Other logging extensions",
        "Troubleshooting",
        "What's next?"
      ],
      "title": "Ruby: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Logs in context for Ruby"
      ],
      "external_id": "d2100c0d6ddffe6b52a1f968f61f777a1e56a13c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/configure-logs-context-ruby/",
      "published_at": "2021-10-19T03:54:54Z",
      "updated_at": "2021-10-19T03:54:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Ruby agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Ruby app To enable logs in context for APM apps monitored by Ruby: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Ruby agent version, and enable distributed tracing. Use Ruby agent version 6.7.0 or higher for logs in context. For Rails applications, use a supported Rails version. Configure logs in context for Ruby. Standard Ruby on Rails configuration Rails logging is controlled by two components: A logger you can customize by setting config.logger A log formatter you can customize by setting config.log_formatter) In most cases, you should configure logs in context by setting config.log_formatter to the DecoratingFormatter in your Rails application. For more information about Rails configuration, see the rubyonrails.org documentation. In your application's config, require newrelic_rpm, then add the following line: module ________ class Application < Rails::Application ... config.log_formatter = ::NewRelic::Agent::Logging::DecoratingFormatter.new end end Copy This configuration uses the New Relic formatter for log messages, but the remaining configuration is provided by the other Rails settings. Incompatible gems New Relic's decorating logger is known to be incompatible with the following gems: logging semantic logger rails_stdout_logger rails_12factor Rails advanced configuration If setting the log_formatter option doesn't meet your needs, replace the entire Rails logger with an instance of the New Relic logger. Provide the parameters to the logger's constructor, like this: module ________ class Application < Rails::Application ... config.logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( \"log/application.log\", #etc... ) end end Copy Ruby configuration without Rails For non-Rails applications, use the DecoratingLogger in place of the Ruby standard ::Logger, like this: logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( 'log/application.log', #etc... ) ... logger.info(...) Copy The DecoratingLogger is a drop-in replacement for the Ruby standard ::Logger. Their constructors accept the same parameters. Lograge configuration To configure this extension with the lograge gem, follow standard procedures in this doc for Ruby on Rails configuration. No additional configuration is required for the lograge gem. Other logging extensions To use our logging extension with a different logging implementation or with your own custom logger, use the DecoratingFormatter. For example: module ________ class Application < Rails::Application ... config.logger = ::YourCustomLoggerImplementation.new( $stdout, formatter: ::NewRelic::Agent::Logging::DecoratingFormatter.new ) end end Copy To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you have configured your logging in /config/application.rb or in /config/environments/development.rb, run your application locally and check its logging output. You should see some output like this: {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"00fc7d46\",\"timestamp\":1567701375543,\"message\":\"example log message one\",\"log.level\":\"DEBUG\"} {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"6754870b\",\"timestamp\":1567702843604,\"message\":\"example log message two\",\"log.level\":\"DEBUG\"} Copy Troubleshooting If you don't see log data in the UI, follow the troubleshooting procedures. Also, if you see JSON logs in your application's output, but your query does not return logs, check your log forwarder. If the logs from your application are not formatted in JSON with fields like trace.id and span.id, there may be a problem with your log forwarding extension. In this situation: Check that the application is using a supported logging framework. Check that your logging configuration has been applied to all the environments where you want to forward and enrich the log data being sent to New Relic. Check that another logger is not configured later in your application's configuration. Logs in context for Ruby does not support tagged logging. If you are initializing your logger with a log_tags argument, your custom tags may not appear on the final version of your logs. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 282.51422,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Ruby: Configure <em>logs</em> <em>in</em> context",
        "sections": "Ruby: Configure <em>logs</em> <em>in</em> context",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in context for the Ruby agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Ruby app To <em>enable</em> <em>logs</em>"
      },
      "id": "612d460128ccbc17bf56a863"
    },
    {
      "sections": [
        "Forward logs and activity logs from Azure",
        "Azure Resources Manager logs",
        "Use the Azure Resource Manager (ARM) template",
        "View log data",
        "Send logs from Azure resources",
        "Azure activity logs",
        "What's next?"
      ],
      "title": "Forward logs and activity logs from Azure",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic",
        "Azure",
        "Cloud logs"
      ],
      "external_id": "793bb2faa9c693f007a632d4faf24b1dbaa0c906",
      "image": "https://docs.newrelic.com/static/da32e66650336026701ab338f5afbb60/5f1d2/azure-tile.png",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/azure-log-forwarding/",
      "published_at": "2021-10-18T20:17:19Z",
      "updated_at": "2021-10-07T16:10:48Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Microsoft Azure (ARM), you can use our template to forward and enrich your log data in New Relic. Forwarding your Azure EventHub logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Azure Resources Manager logs The Microsoft Azure Resources Manager (ARM) template provided by New Relic One helps you set up Azure to: Forward logs from EventHub to New Relic One. Forward activity logs to New Relic One through EventHub. The setup process is almost the same for both use cases. As part of the setup process, you can select which Azure activity logs you want to forward to New Relic, including: Administrative Azure Activity Logs Alert Azure Activity Logs Autoscale Azure Activity Logs Policy Azure Activity Logs Recommendation Azure Activity Logs Resource Health Azure Activity Logs Security Azure Activity Logs Service Health Azure Activity Logs Use the Azure Resource Manager (ARM) template The template is idempotent. You can start forwarding logs from EventHub and then rerun the same template to configure Azure activity logs forwarding by completing step 10. Make sure you have a New Relic license key. Log in to New Relic Logs and click Add more data sources on the top right of the page. Under Log ingestion, click the tile. Select the account you want to send the logs, and click Continue. Click Generate API Key and copy the generated API Key. Click Deploy to Azure and a new tab will be open with the ARM template loaded in Azure. Select the Resource Group where you want to create the necessary resources, and a Region. In the New Relic License Key field, paste the previously copied API Key. Ensure the New Relic One endpoint set is the one corresponding to your account. Optional: Set to true the Azure activity logs you want to forward. Click Review + create, review the data you've inserted, and click Create. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If you want to only query for logs coming from Azure, run the following query: SELECT * FROM Log where plugin.type='azure' Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. Send logs from Azure resources By default, this template only configures the needed function and resources to forward logs to New Relic One. We can also configure the activity logs to be forwarded, but there isn't a default log forwarding from your Azure resources. If you want to forward logs from any resource that produces them, you need to configure it by creating a diagnostic setting for the given resource. For example, if you have a function running on Azure and you want to forward the logs to New Relic One, you'll need to configure a diagnostic setting to forward the logs to EventHub. For more information, see the Microsoft documentation to create diagnostic settings for sendig platform logs and metrics to different destinations. Azure activity logs Activating the Azure activity logs forwarding is optional. It provides: More visibility of your Azure resources Activity of the Azure resources Information about performed actions Events and their timestamps The user who performed an action, if applicable These logs give your organization more control over the resources. However, be aware of wrong or unintentional changes on your resources and even unexpected actions. For more information about this kind of event, see the Microsoft Azure Activity Log event schema. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.66516,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward <em>logs</em> and activity <em>logs</em> from Azure",
        "sections": "Azure Resources <em>Manager</em> <em>logs</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "If your <em>log</em> data is already being monitored by Microsoft Azure (ARM), you can use our template to forward and enrich your <em>log</em> data in <em>New</em> <em>Relic</em>. Forwarding your Azure EventHub <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert on your"
      },
      "id": "6150545de7b9d2ae518de37f"
    }
  ],
  "/docs/logs/forward-logs/forward-your-logs-using-infrastructure-agent": [
    {
      "sections": [
        "Kubernetes plugin for log forwarding",
        "Enable Kubernetes for log management",
        "View log data",
        "What's next?"
      ],
      "title": "Kubernetes plugin for log forwarding",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "c55c6a084a1de057aac4f85ed3807c41317f63e1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/kubernetes-plugin-log-forwarding/",
      "published_at": "2021-10-18T05:22:43Z",
      "updated_at": "2021-10-13T02:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your log data in New Relic. This plugin is also provided in a standalone Docker image that can be installed in a Kubernetes cluster in the form of a DaemonSet. We refer to this as our Kubernetes plugin. Forwarding your Kubernetes logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Enable Kubernetes for log management To forward your logs to New Relic with our Kubernetes plugin for Fluent Bit: Make sure you have: A New Relic license key Kubernetes cluster deployed Follow the procedures in GitHub to install the Kubernetes plugin. Optional: If you are installing the plugin as a Helm chart, you can set numerous configurations. However, we recommend the standard setup, as it is valid for most users. Generate some traffic and wait a few minutes, then check your account for data. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 297.08002,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kubernetes plugin for <em>log</em> forwarding",
        "sections": "<em>Enable</em> Kubernetes for <em>log</em> <em>management</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " to this as our Kubernetes plugin. Forwarding your Kubernetes <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert on your <em>log</em> data. <em>Enable</em> Kubernetes for <em>log</em> <em>management</em> To forward your <em>logs</em> to <em>New</em> <em>Relic</em> with our Kubernetes plugin for Fluent Bit"
      },
      "id": "603ec19128ccbcad24eba748"
    },
    {
      "sections": [
        "Ruby: Configure logs in context",
        "Set up your Ruby app",
        "Standard Ruby on Rails configuration",
        "Incompatible gems",
        "Rails advanced configuration",
        "Ruby configuration without Rails",
        "Lograge configuration",
        "Other logging extensions",
        "Troubleshooting",
        "What's next?"
      ],
      "title": "Ruby: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Logs in context for Ruby"
      ],
      "external_id": "d2100c0d6ddffe6b52a1f968f61f777a1e56a13c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/configure-logs-context-ruby/",
      "published_at": "2021-10-19T03:54:54Z",
      "updated_at": "2021-10-19T03:54:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Ruby agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Ruby app To enable logs in context for APM apps monitored by Ruby: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Ruby agent version, and enable distributed tracing. Use Ruby agent version 6.7.0 or higher for logs in context. For Rails applications, use a supported Rails version. Configure logs in context for Ruby. Standard Ruby on Rails configuration Rails logging is controlled by two components: A logger you can customize by setting config.logger A log formatter you can customize by setting config.log_formatter) In most cases, you should configure logs in context by setting config.log_formatter to the DecoratingFormatter in your Rails application. For more information about Rails configuration, see the rubyonrails.org documentation. In your application's config, require newrelic_rpm, then add the following line: module ________ class Application < Rails::Application ... config.log_formatter = ::NewRelic::Agent::Logging::DecoratingFormatter.new end end Copy This configuration uses the New Relic formatter for log messages, but the remaining configuration is provided by the other Rails settings. Incompatible gems New Relic's decorating logger is known to be incompatible with the following gems: logging semantic logger rails_stdout_logger rails_12factor Rails advanced configuration If setting the log_formatter option doesn't meet your needs, replace the entire Rails logger with an instance of the New Relic logger. Provide the parameters to the logger's constructor, like this: module ________ class Application < Rails::Application ... config.logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( \"log/application.log\", #etc... ) end end Copy Ruby configuration without Rails For non-Rails applications, use the DecoratingLogger in place of the Ruby standard ::Logger, like this: logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( 'log/application.log', #etc... ) ... logger.info(...) Copy The DecoratingLogger is a drop-in replacement for the Ruby standard ::Logger. Their constructors accept the same parameters. Lograge configuration To configure this extension with the lograge gem, follow standard procedures in this doc for Ruby on Rails configuration. No additional configuration is required for the lograge gem. Other logging extensions To use our logging extension with a different logging implementation or with your own custom logger, use the DecoratingFormatter. For example: module ________ class Application < Rails::Application ... config.logger = ::YourCustomLoggerImplementation.new( $stdout, formatter: ::NewRelic::Agent::Logging::DecoratingFormatter.new ) end end Copy To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you have configured your logging in /config/application.rb or in /config/environments/development.rb, run your application locally and check its logging output. You should see some output like this: {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"00fc7d46\",\"timestamp\":1567701375543,\"message\":\"example log message one\",\"log.level\":\"DEBUG\"} {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"6754870b\",\"timestamp\":1567702843604,\"message\":\"example log message two\",\"log.level\":\"DEBUG\"} Copy Troubleshooting If you don't see log data in the UI, follow the troubleshooting procedures. Also, if you see JSON logs in your application's output, but your query does not return logs, check your log forwarder. If the logs from your application are not formatted in JSON with fields like trace.id and span.id, there may be a problem with your log forwarding extension. In this situation: Check that the application is using a supported logging framework. Check that your logging configuration has been applied to all the environments where you want to forward and enrich the log data being sent to New Relic. Check that another logger is not configured later in your application's configuration. Logs in context for Ruby does not support tagged logging. If you are initializing your logger with a log_tags argument, your custom tags may not appear on the final version of your logs. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 282.51404,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Ruby: Configure <em>logs</em> <em>in</em> context",
        "sections": "Ruby: Configure <em>logs</em> <em>in</em> context",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in context for the Ruby agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Ruby app To <em>enable</em> <em>logs</em>"
      },
      "id": "612d460128ccbc17bf56a863"
    },
    {
      "sections": [
        "Forward logs and activity logs from Azure",
        "Azure Resources Manager logs",
        "Use the Azure Resource Manager (ARM) template",
        "View log data",
        "Send logs from Azure resources",
        "Azure activity logs",
        "What's next?"
      ],
      "title": "Forward logs and activity logs from Azure",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic",
        "Azure",
        "Cloud logs"
      ],
      "external_id": "793bb2faa9c693f007a632d4faf24b1dbaa0c906",
      "image": "https://docs.newrelic.com/static/da32e66650336026701ab338f5afbb60/5f1d2/azure-tile.png",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/azure-log-forwarding/",
      "published_at": "2021-10-18T20:17:19Z",
      "updated_at": "2021-10-07T16:10:48Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Microsoft Azure (ARM), you can use our template to forward and enrich your log data in New Relic. Forwarding your Azure EventHub logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Azure Resources Manager logs The Microsoft Azure Resources Manager (ARM) template provided by New Relic One helps you set up Azure to: Forward logs from EventHub to New Relic One. Forward activity logs to New Relic One through EventHub. The setup process is almost the same for both use cases. As part of the setup process, you can select which Azure activity logs you want to forward to New Relic, including: Administrative Azure Activity Logs Alert Azure Activity Logs Autoscale Azure Activity Logs Policy Azure Activity Logs Recommendation Azure Activity Logs Resource Health Azure Activity Logs Security Azure Activity Logs Service Health Azure Activity Logs Use the Azure Resource Manager (ARM) template The template is idempotent. You can start forwarding logs from EventHub and then rerun the same template to configure Azure activity logs forwarding by completing step 10. Make sure you have a New Relic license key. Log in to New Relic Logs and click Add more data sources on the top right of the page. Under Log ingestion, click the tile. Select the account you want to send the logs, and click Continue. Click Generate API Key and copy the generated API Key. Click Deploy to Azure and a new tab will be open with the ARM template loaded in Azure. Select the Resource Group where you want to create the necessary resources, and a Region. In the New Relic License Key field, paste the previously copied API Key. Ensure the New Relic One endpoint set is the one corresponding to your account. Optional: Set to true the Azure activity logs you want to forward. Click Review + create, review the data you've inserted, and click Create. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If you want to only query for logs coming from Azure, run the following query: SELECT * FROM Log where plugin.type='azure' Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. Send logs from Azure resources By default, this template only configures the needed function and resources to forward logs to New Relic One. We can also configure the activity logs to be forwarded, but there isn't a default log forwarding from your Azure resources. If you want to forward logs from any resource that produces them, you need to configure it by creating a diagnostic setting for the given resource. For example, if you have a function running on Azure and you want to forward the logs to New Relic One, you'll need to configure a diagnostic setting to forward the logs to EventHub. For more information, see the Microsoft documentation to create diagnostic settings for sendig platform logs and metrics to different destinations. Azure activity logs Activating the Azure activity logs forwarding is optional. It provides: More visibility of your Azure resources Activity of the Azure resources Information about performed actions Events and their timestamps The user who performed an action, if applicable These logs give your organization more control over the resources. However, be aware of wrong or unintentional changes on your resources and even unexpected actions. For more information about this kind of event, see the Microsoft Azure Activity Log event schema. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.66513,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward <em>logs</em> and activity <em>logs</em> from Azure",
        "sections": "Azure Resources <em>Manager</em> <em>logs</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "If your <em>log</em> data is already being monitored by Microsoft Azure (ARM), you can use our template to forward and enrich your <em>log</em> data in <em>New</em> <em>Relic</em>. Forwarding your Azure EventHub <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert on your"
      },
      "id": "6150545de7b9d2ae518de37f"
    }
  ],
  "/docs/logs/forward-logs/google-cloud-platform-log-forwarding": [
    {
      "sections": [
        "Kubernetes plugin for log forwarding",
        "Enable Kubernetes for log management",
        "View log data",
        "What's next?"
      ],
      "title": "Kubernetes plugin for log forwarding",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "c55c6a084a1de057aac4f85ed3807c41317f63e1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/kubernetes-plugin-log-forwarding/",
      "published_at": "2021-10-18T05:22:43Z",
      "updated_at": "2021-10-13T02:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your log data in New Relic. This plugin is also provided in a standalone Docker image that can be installed in a Kubernetes cluster in the form of a DaemonSet. We refer to this as our Kubernetes plugin. Forwarding your Kubernetes logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Enable Kubernetes for log management To forward your logs to New Relic with our Kubernetes plugin for Fluent Bit: Make sure you have: A New Relic license key Kubernetes cluster deployed Follow the procedures in GitHub to install the Kubernetes plugin. Optional: If you are installing the plugin as a Helm chart, you can set numerous configurations. However, we recommend the standard setup, as it is valid for most users. Generate some traffic and wait a few minutes, then check your account for data. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 300.8492,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kubernetes plugin for <em>log</em> forwarding",
        "sections": "<em>Enable</em> Kubernetes for <em>log</em> <em>management</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " to this as our Kubernetes plugin. Forwarding your Kubernetes <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert on your <em>log</em> data. <em>Enable</em> Kubernetes for <em>log</em> <em>management</em> To forward your <em>logs</em> to <em>New</em> <em>Relic</em> with our Kubernetes plugin for Fluent Bit"
      },
      "id": "603ec19128ccbcad24eba748"
    },
    {
      "sections": [
        "Ruby: Configure logs in context",
        "Set up your Ruby app",
        "Standard Ruby on Rails configuration",
        "Incompatible gems",
        "Rails advanced configuration",
        "Ruby configuration without Rails",
        "Lograge configuration",
        "Other logging extensions",
        "Troubleshooting",
        "What's next?"
      ],
      "title": "Ruby: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Logs in context for Ruby"
      ],
      "external_id": "d2100c0d6ddffe6b52a1f968f61f777a1e56a13c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/configure-logs-context-ruby/",
      "published_at": "2021-10-19T03:54:54Z",
      "updated_at": "2021-10-19T03:54:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Ruby agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Ruby app To enable logs in context for APM apps monitored by Ruby: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Ruby agent version, and enable distributed tracing. Use Ruby agent version 6.7.0 or higher for logs in context. For Rails applications, use a supported Rails version. Configure logs in context for Ruby. Standard Ruby on Rails configuration Rails logging is controlled by two components: A logger you can customize by setting config.logger A log formatter you can customize by setting config.log_formatter) In most cases, you should configure logs in context by setting config.log_formatter to the DecoratingFormatter in your Rails application. For more information about Rails configuration, see the rubyonrails.org documentation. In your application's config, require newrelic_rpm, then add the following line: module ________ class Application < Rails::Application ... config.log_formatter = ::NewRelic::Agent::Logging::DecoratingFormatter.new end end Copy This configuration uses the New Relic formatter for log messages, but the remaining configuration is provided by the other Rails settings. Incompatible gems New Relic's decorating logger is known to be incompatible with the following gems: logging semantic logger rails_stdout_logger rails_12factor Rails advanced configuration If setting the log_formatter option doesn't meet your needs, replace the entire Rails logger with an instance of the New Relic logger. Provide the parameters to the logger's constructor, like this: module ________ class Application < Rails::Application ... config.logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( \"log/application.log\", #etc... ) end end Copy Ruby configuration without Rails For non-Rails applications, use the DecoratingLogger in place of the Ruby standard ::Logger, like this: logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( 'log/application.log', #etc... ) ... logger.info(...) Copy The DecoratingLogger is a drop-in replacement for the Ruby standard ::Logger. Their constructors accept the same parameters. Lograge configuration To configure this extension with the lograge gem, follow standard procedures in this doc for Ruby on Rails configuration. No additional configuration is required for the lograge gem. Other logging extensions To use our logging extension with a different logging implementation or with your own custom logger, use the DecoratingFormatter. For example: module ________ class Application < Rails::Application ... config.logger = ::YourCustomLoggerImplementation.new( $stdout, formatter: ::NewRelic::Agent::Logging::DecoratingFormatter.new ) end end Copy To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you have configured your logging in /config/application.rb or in /config/environments/development.rb, run your application locally and check its logging output. You should see some output like this: {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"00fc7d46\",\"timestamp\":1567701375543,\"message\":\"example log message one\",\"log.level\":\"DEBUG\"} {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"6754870b\",\"timestamp\":1567702843604,\"message\":\"example log message two\",\"log.level\":\"DEBUG\"} Copy Troubleshooting If you don't see log data in the UI, follow the troubleshooting procedures. Also, if you see JSON logs in your application's output, but your query does not return logs, check your log forwarder. If the logs from your application are not formatted in JSON with fields like trace.id and span.id, there may be a problem with your log forwarding extension. In this situation: Check that the application is using a supported logging framework. Check that your logging configuration has been applied to all the environments where you want to forward and enrich the log data being sent to New Relic. Check that another logger is not configured later in your application's configuration. Logs in context for Ruby does not support tagged logging. If you are initializing your logger with a log_tags argument, your custom tags may not appear on the final version of your logs. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 285.71475,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Ruby: Configure <em>logs</em> <em>in</em> context",
        "sections": "Ruby: Configure <em>logs</em> <em>in</em> context",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in context for the Ruby agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Ruby app To <em>enable</em> <em>logs</em>"
      },
      "id": "612d460128ccbc17bf56a863"
    },
    {
      "sections": [
        "Forward logs and activity logs from Azure",
        "Azure Resources Manager logs",
        "Use the Azure Resource Manager (ARM) template",
        "View log data",
        "Send logs from Azure resources",
        "Azure activity logs",
        "What's next?"
      ],
      "title": "Forward logs and activity logs from Azure",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic",
        "Azure",
        "Cloud logs"
      ],
      "external_id": "793bb2faa9c693f007a632d4faf24b1dbaa0c906",
      "image": "https://docs.newrelic.com/static/da32e66650336026701ab338f5afbb60/5f1d2/azure-tile.png",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/azure-log-forwarding/",
      "published_at": "2021-10-18T20:17:19Z",
      "updated_at": "2021-10-07T16:10:48Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Microsoft Azure (ARM), you can use our template to forward and enrich your log data in New Relic. Forwarding your Azure EventHub logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Azure Resources Manager logs The Microsoft Azure Resources Manager (ARM) template provided by New Relic One helps you set up Azure to: Forward logs from EventHub to New Relic One. Forward activity logs to New Relic One through EventHub. The setup process is almost the same for both use cases. As part of the setup process, you can select which Azure activity logs you want to forward to New Relic, including: Administrative Azure Activity Logs Alert Azure Activity Logs Autoscale Azure Activity Logs Policy Azure Activity Logs Recommendation Azure Activity Logs Resource Health Azure Activity Logs Security Azure Activity Logs Service Health Azure Activity Logs Use the Azure Resource Manager (ARM) template The template is idempotent. You can start forwarding logs from EventHub and then rerun the same template to configure Azure activity logs forwarding by completing step 10. Make sure you have a New Relic license key. Log in to New Relic Logs and click Add more data sources on the top right of the page. Under Log ingestion, click the tile. Select the account you want to send the logs, and click Continue. Click Generate API Key and copy the generated API Key. Click Deploy to Azure and a new tab will be open with the ARM template loaded in Azure. Select the Resource Group where you want to create the necessary resources, and a Region. In the New Relic License Key field, paste the previously copied API Key. Ensure the New Relic One endpoint set is the one corresponding to your account. Optional: Set to true the Azure activity logs you want to forward. Click Review + create, review the data you've inserted, and click Create. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If you want to only query for logs coming from Azure, run the following query: SELECT * FROM Log where plugin.type='azure' Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. Send logs from Azure resources By default, this template only configures the needed function and resources to forward logs to New Relic One. We can also configure the activity logs to be forwarded, but there isn't a default log forwarding from your Azure resources. If you want to forward logs from any resource that produces them, you need to configure it by creating a diagnostic setting for the given resource. For example, if you have a function running on Azure and you want to forward the logs to New Relic One, you'll need to configure a diagnostic setting to forward the logs to EventHub. For more information, see the Microsoft documentation to create diagnostic settings for sendig platform logs and metrics to different destinations. Azure activity logs Activating the Azure activity logs forwarding is optional. It provides: More visibility of your Azure resources Activity of the Azure resources Information about performed actions Events and their timestamps The user who performed an action, if applicable These logs give your organization more control over the resources. However, be aware of wrong or unintentional changes on your resources and even unexpected actions. For more information about this kind of event, see the Microsoft Azure Activity Log event schema. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.1495,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward <em>logs</em> and activity <em>logs</em> from Azure",
        "sections": "Azure Resources <em>Manager</em> <em>logs</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "If your <em>log</em> data is already being monitored by Microsoft Azure (ARM), you can use our template to forward and enrich your <em>log</em> data in <em>New</em> <em>Relic</em>. Forwarding your Azure EventHub <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert on your"
      },
      "id": "6150545de7b9d2ae518de37f"
    }
  ],
  "/docs/logs/forward-logs/heroku-log-forwarding": [
    {
      "sections": [
        "Kubernetes plugin for log forwarding",
        "Enable Kubernetes for log management",
        "View log data",
        "What's next?"
      ],
      "title": "Kubernetes plugin for log forwarding",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "c55c6a084a1de057aac4f85ed3807c41317f63e1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/kubernetes-plugin-log-forwarding/",
      "published_at": "2021-10-18T05:22:43Z",
      "updated_at": "2021-10-13T02:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your log data in New Relic. This plugin is also provided in a standalone Docker image that can be installed in a Kubernetes cluster in the form of a DaemonSet. We refer to this as our Kubernetes plugin. Forwarding your Kubernetes logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Enable Kubernetes for log management To forward your logs to New Relic with our Kubernetes plugin for Fluent Bit: Make sure you have: A New Relic license key Kubernetes cluster deployed Follow the procedures in GitHub to install the Kubernetes plugin. Optional: If you are installing the plugin as a Helm chart, you can set numerous configurations. However, we recommend the standard setup, as it is valid for most users. Generate some traffic and wait a few minutes, then check your account for data. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 297.07993,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kubernetes plugin for <em>log</em> forwarding",
        "sections": "<em>Enable</em> Kubernetes for <em>log</em> <em>management</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " to this as our Kubernetes plugin. Forwarding your Kubernetes <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert on your <em>log</em> data. <em>Enable</em> Kubernetes for <em>log</em> <em>management</em> To forward your <em>logs</em> to <em>New</em> <em>Relic</em> with our Kubernetes plugin for Fluent Bit"
      },
      "id": "603ec19128ccbcad24eba748"
    },
    {
      "sections": [
        "Ruby: Configure logs in context",
        "Set up your Ruby app",
        "Standard Ruby on Rails configuration",
        "Incompatible gems",
        "Rails advanced configuration",
        "Ruby configuration without Rails",
        "Lograge configuration",
        "Other logging extensions",
        "Troubleshooting",
        "What's next?"
      ],
      "title": "Ruby: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Logs in context for Ruby"
      ],
      "external_id": "d2100c0d6ddffe6b52a1f968f61f777a1e56a13c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/configure-logs-context-ruby/",
      "published_at": "2021-10-19T03:54:54Z",
      "updated_at": "2021-10-19T03:54:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Ruby agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Ruby app To enable logs in context for APM apps monitored by Ruby: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Ruby agent version, and enable distributed tracing. Use Ruby agent version 6.7.0 or higher for logs in context. For Rails applications, use a supported Rails version. Configure logs in context for Ruby. Standard Ruby on Rails configuration Rails logging is controlled by two components: A logger you can customize by setting config.logger A log formatter you can customize by setting config.log_formatter) In most cases, you should configure logs in context by setting config.log_formatter to the DecoratingFormatter in your Rails application. For more information about Rails configuration, see the rubyonrails.org documentation. In your application's config, require newrelic_rpm, then add the following line: module ________ class Application < Rails::Application ... config.log_formatter = ::NewRelic::Agent::Logging::DecoratingFormatter.new end end Copy This configuration uses the New Relic formatter for log messages, but the remaining configuration is provided by the other Rails settings. Incompatible gems New Relic's decorating logger is known to be incompatible with the following gems: logging semantic logger rails_stdout_logger rails_12factor Rails advanced configuration If setting the log_formatter option doesn't meet your needs, replace the entire Rails logger with an instance of the New Relic logger. Provide the parameters to the logger's constructor, like this: module ________ class Application < Rails::Application ... config.logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( \"log/application.log\", #etc... ) end end Copy Ruby configuration without Rails For non-Rails applications, use the DecoratingLogger in place of the Ruby standard ::Logger, like this: logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( 'log/application.log', #etc... ) ... logger.info(...) Copy The DecoratingLogger is a drop-in replacement for the Ruby standard ::Logger. Their constructors accept the same parameters. Lograge configuration To configure this extension with the lograge gem, follow standard procedures in this doc for Ruby on Rails configuration. No additional configuration is required for the lograge gem. Other logging extensions To use our logging extension with a different logging implementation or with your own custom logger, use the DecoratingFormatter. For example: module ________ class Application < Rails::Application ... config.logger = ::YourCustomLoggerImplementation.new( $stdout, formatter: ::NewRelic::Agent::Logging::DecoratingFormatter.new ) end end Copy To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you have configured your logging in /config/application.rb or in /config/environments/development.rb, run your application locally and check its logging output. You should see some output like this: {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"00fc7d46\",\"timestamp\":1567701375543,\"message\":\"example log message one\",\"log.level\":\"DEBUG\"} {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"6754870b\",\"timestamp\":1567702843604,\"message\":\"example log message two\",\"log.level\":\"DEBUG\"} Copy Troubleshooting If you don't see log data in the UI, follow the troubleshooting procedures. Also, if you see JSON logs in your application's output, but your query does not return logs, check your log forwarder. If the logs from your application are not formatted in JSON with fields like trace.id and span.id, there may be a problem with your log forwarding extension. In this situation: Check that the application is using a supported logging framework. Check that your logging configuration has been applied to all the environments where you want to forward and enrich the log data being sent to New Relic. Check that another logger is not configured later in your application's configuration. Logs in context for Ruby does not support tagged logging. If you are initializing your logger with a log_tags argument, your custom tags may not appear on the final version of your logs. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 282.51382,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Ruby: Configure <em>logs</em> <em>in</em> context",
        "sections": "Ruby: Configure <em>logs</em> <em>in</em> context",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in context for the Ruby agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Ruby app To <em>enable</em> <em>logs</em>"
      },
      "id": "612d460128ccbc17bf56a863"
    },
    {
      "sections": [
        "Forward logs and activity logs from Azure",
        "Azure Resources Manager logs",
        "Use the Azure Resource Manager (ARM) template",
        "View log data",
        "Send logs from Azure resources",
        "Azure activity logs",
        "What's next?"
      ],
      "title": "Forward logs and activity logs from Azure",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic",
        "Azure",
        "Cloud logs"
      ],
      "external_id": "793bb2faa9c693f007a632d4faf24b1dbaa0c906",
      "image": "https://docs.newrelic.com/static/da32e66650336026701ab338f5afbb60/5f1d2/azure-tile.png",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/azure-log-forwarding/",
      "published_at": "2021-10-18T20:17:19Z",
      "updated_at": "2021-10-07T16:10:48Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Microsoft Azure (ARM), you can use our template to forward and enrich your log data in New Relic. Forwarding your Azure EventHub logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Azure Resources Manager logs The Microsoft Azure Resources Manager (ARM) template provided by New Relic One helps you set up Azure to: Forward logs from EventHub to New Relic One. Forward activity logs to New Relic One through EventHub. The setup process is almost the same for both use cases. As part of the setup process, you can select which Azure activity logs you want to forward to New Relic, including: Administrative Azure Activity Logs Alert Azure Activity Logs Autoscale Azure Activity Logs Policy Azure Activity Logs Recommendation Azure Activity Logs Resource Health Azure Activity Logs Security Azure Activity Logs Service Health Azure Activity Logs Use the Azure Resource Manager (ARM) template The template is idempotent. You can start forwarding logs from EventHub and then rerun the same template to configure Azure activity logs forwarding by completing step 10. Make sure you have a New Relic license key. Log in to New Relic Logs and click Add more data sources on the top right of the page. Under Log ingestion, click the tile. Select the account you want to send the logs, and click Continue. Click Generate API Key and copy the generated API Key. Click Deploy to Azure and a new tab will be open with the ARM template loaded in Azure. Select the Resource Group where you want to create the necessary resources, and a Region. In the New Relic License Key field, paste the previously copied API Key. Ensure the New Relic One endpoint set is the one corresponding to your account. Optional: Set to true the Azure activity logs you want to forward. Click Review + create, review the data you've inserted, and click Create. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If you want to only query for logs coming from Azure, run the following query: SELECT * FROM Log where plugin.type='azure' Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. Send logs from Azure resources By default, this template only configures the needed function and resources to forward logs to New Relic One. We can also configure the activity logs to be forwarded, but there isn't a default log forwarding from your Azure resources. If you want to forward logs from any resource that produces them, you need to configure it by creating a diagnostic setting for the given resource. For example, if you have a function running on Azure and you want to forward the logs to New Relic One, you'll need to configure a diagnostic setting to forward the logs to EventHub. For more information, see the Microsoft documentation to create diagnostic settings for sendig platform logs and metrics to different destinations. Azure activity logs Activating the Azure activity logs forwarding is optional. It provides: More visibility of your Azure resources Activity of the Azure resources Information about performed actions Events and their timestamps The user who performed an action, if applicable These logs give your organization more control over the resources. However, be aware of wrong or unintentional changes on your resources and even unexpected actions. For more information about this kind of event, see the Microsoft Azure Activity Log event schema. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.66507,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward <em>logs</em> and activity <em>logs</em> from Azure",
        "sections": "Azure Resources <em>Manager</em> <em>logs</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "If your <em>log</em> data is already being monitored by Microsoft Azure (ARM), you can use our template to forward and enrich your <em>log</em> data in <em>New</em> <em>Relic</em>. Forwarding your Azure EventHub <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert on your"
      },
      "id": "6150545de7b9d2ae518de37f"
    }
  ],
  "/docs/logs/forward-logs/kubernetes-plugin-log-forwarding": [
    {
      "sections": [
        "Ruby: Configure logs in context",
        "Set up your Ruby app",
        "Standard Ruby on Rails configuration",
        "Incompatible gems",
        "Rails advanced configuration",
        "Ruby configuration without Rails",
        "Lograge configuration",
        "Other logging extensions",
        "Troubleshooting",
        "What's next?"
      ],
      "title": "Ruby: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Logs in context for Ruby"
      ],
      "external_id": "d2100c0d6ddffe6b52a1f968f61f777a1e56a13c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/configure-logs-context-ruby/",
      "published_at": "2021-10-19T03:54:54Z",
      "updated_at": "2021-10-19T03:54:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Ruby agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Ruby app To enable logs in context for APM apps monitored by Ruby: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Ruby agent version, and enable distributed tracing. Use Ruby agent version 6.7.0 or higher for logs in context. For Rails applications, use a supported Rails version. Configure logs in context for Ruby. Standard Ruby on Rails configuration Rails logging is controlled by two components: A logger you can customize by setting config.logger A log formatter you can customize by setting config.log_formatter) In most cases, you should configure logs in context by setting config.log_formatter to the DecoratingFormatter in your Rails application. For more information about Rails configuration, see the rubyonrails.org documentation. In your application's config, require newrelic_rpm, then add the following line: module ________ class Application < Rails::Application ... config.log_formatter = ::NewRelic::Agent::Logging::DecoratingFormatter.new end end Copy This configuration uses the New Relic formatter for log messages, but the remaining configuration is provided by the other Rails settings. Incompatible gems New Relic's decorating logger is known to be incompatible with the following gems: logging semantic logger rails_stdout_logger rails_12factor Rails advanced configuration If setting the log_formatter option doesn't meet your needs, replace the entire Rails logger with an instance of the New Relic logger. Provide the parameters to the logger's constructor, like this: module ________ class Application < Rails::Application ... config.logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( \"log/application.log\", #etc... ) end end Copy Ruby configuration without Rails For non-Rails applications, use the DecoratingLogger in place of the Ruby standard ::Logger, like this: logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( 'log/application.log', #etc... ) ... logger.info(...) Copy The DecoratingLogger is a drop-in replacement for the Ruby standard ::Logger. Their constructors accept the same parameters. Lograge configuration To configure this extension with the lograge gem, follow standard procedures in this doc for Ruby on Rails configuration. No additional configuration is required for the lograge gem. Other logging extensions To use our logging extension with a different logging implementation or with your own custom logger, use the DecoratingFormatter. For example: module ________ class Application < Rails::Application ... config.logger = ::YourCustomLoggerImplementation.new( $stdout, formatter: ::NewRelic::Agent::Logging::DecoratingFormatter.new ) end end Copy To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you have configured your logging in /config/application.rb or in /config/environments/development.rb, run your application locally and check its logging output. You should see some output like this: {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"00fc7d46\",\"timestamp\":1567701375543,\"message\":\"example log message one\",\"log.level\":\"DEBUG\"} {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"6754870b\",\"timestamp\":1567702843604,\"message\":\"example log message two\",\"log.level\":\"DEBUG\"} Copy Troubleshooting If you don't see log data in the UI, follow the troubleshooting procedures. Also, if you see JSON logs in your application's output, but your query does not return logs, check your log forwarder. If the logs from your application are not formatted in JSON with fields like trace.id and span.id, there may be a problem with your log forwarding extension. In this situation: Check that the application is using a supported logging framework. Check that your logging configuration has been applied to all the environments where you want to forward and enrich the log data being sent to New Relic. Check that another logger is not configured later in your application's configuration. Logs in context for Ruby does not support tagged logging. If you are initializing your logger with a log_tags argument, your custom tags may not appear on the final version of your logs. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 282.51382,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Ruby: Configure <em>logs</em> <em>in</em> context",
        "sections": "Ruby: Configure <em>logs</em> <em>in</em> context",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in context for the Ruby agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Ruby app To <em>enable</em> <em>logs</em>"
      },
      "id": "612d460128ccbc17bf56a863"
    },
    {
      "sections": [
        "Forward logs and activity logs from Azure",
        "Azure Resources Manager logs",
        "Use the Azure Resource Manager (ARM) template",
        "View log data",
        "Send logs from Azure resources",
        "Azure activity logs",
        "What's next?"
      ],
      "title": "Forward logs and activity logs from Azure",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic",
        "Azure",
        "Cloud logs"
      ],
      "external_id": "793bb2faa9c693f007a632d4faf24b1dbaa0c906",
      "image": "https://docs.newrelic.com/static/da32e66650336026701ab338f5afbb60/5f1d2/azure-tile.png",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/azure-log-forwarding/",
      "published_at": "2021-10-18T20:17:19Z",
      "updated_at": "2021-10-07T16:10:48Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Microsoft Azure (ARM), you can use our template to forward and enrich your log data in New Relic. Forwarding your Azure EventHub logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Azure Resources Manager logs The Microsoft Azure Resources Manager (ARM) template provided by New Relic One helps you set up Azure to: Forward logs from EventHub to New Relic One. Forward activity logs to New Relic One through EventHub. The setup process is almost the same for both use cases. As part of the setup process, you can select which Azure activity logs you want to forward to New Relic, including: Administrative Azure Activity Logs Alert Azure Activity Logs Autoscale Azure Activity Logs Policy Azure Activity Logs Recommendation Azure Activity Logs Resource Health Azure Activity Logs Security Azure Activity Logs Service Health Azure Activity Logs Use the Azure Resource Manager (ARM) template The template is idempotent. You can start forwarding logs from EventHub and then rerun the same template to configure Azure activity logs forwarding by completing step 10. Make sure you have a New Relic license key. Log in to New Relic Logs and click Add more data sources on the top right of the page. Under Log ingestion, click the tile. Select the account you want to send the logs, and click Continue. Click Generate API Key and copy the generated API Key. Click Deploy to Azure and a new tab will be open with the ARM template loaded in Azure. Select the Resource Group where you want to create the necessary resources, and a Region. In the New Relic License Key field, paste the previously copied API Key. Ensure the New Relic One endpoint set is the one corresponding to your account. Optional: Set to true the Azure activity logs you want to forward. Click Review + create, review the data you've inserted, and click Create. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If you want to only query for logs coming from Azure, run the following query: SELECT * FROM Log where plugin.type='azure' Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. Send logs from Azure resources By default, this template only configures the needed function and resources to forward logs to New Relic One. We can also configure the activity logs to be forwarded, but there isn't a default log forwarding from your Azure resources. If you want to forward logs from any resource that produces them, you need to configure it by creating a diagnostic setting for the given resource. For example, if you have a function running on Azure and you want to forward the logs to New Relic One, you'll need to configure a diagnostic setting to forward the logs to EventHub. For more information, see the Microsoft documentation to create diagnostic settings for sendig platform logs and metrics to different destinations. Azure activity logs Activating the Azure activity logs forwarding is optional. It provides: More visibility of your Azure resources Activity of the Azure resources Information about performed actions Events and their timestamps The user who performed an action, if applicable These logs give your organization more control over the resources. However, be aware of wrong or unintentional changes on your resources and even unexpected actions. For more information about this kind of event, see the Microsoft Azure Activity Log event schema. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.66507,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward <em>logs</em> and activity <em>logs</em> from Azure",
        "sections": "Azure Resources <em>Manager</em> <em>logs</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "If your <em>log</em> data is already being monitored by Microsoft Azure (ARM), you can use our template to forward and enrich your <em>log</em> data in <em>New</em> <em>Relic</em>. Forwarding your Azure EventHub <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert on your"
      },
      "id": "6150545de7b9d2ae518de37f"
    },
    {
      "sections": [
        "Vector output sink for log forwarding",
        "Configure the Vector New Relic logs sink",
        "Test the Vector New Relic logs sink",
        "View log data",
        "What's next?"
      ],
      "title": "Vector output sink for log forwarding",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "de9a28d924377f4b85681d6e876afbf5f9f64f54",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/vector-output-sink-log-forwarding/",
      "published_at": "2021-10-18T05:22:42Z",
      "updated_at": "2021-10-08T01:56:52Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Vector, you can use our Vector output sink to forward and enrich your log data in New Relic. Forwarding your Vector logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Configure the Vector New Relic logs sink To forward your logs from Vector to New Relic: Make sure you have: A New Relic license key Vector version 0.7.0 or higher Follow the procedures to configure the Vector logs sink for New Relic. Add a snippet to your vector.toml file (located in /etc/vector by default), replacing YOUR_LICENSE_KEY with your New Relic license key: # Ingest data by tailing one or more files [sources.mylog] type = \"file\" include = [\"/path/to/file\"] # Specify file or files to be tailed ignore_older = 86400 # Ignore events older than 1 day file_key = \"file\" # Add filename to log events host_key = \"host\" # Add hostname to log events # Configure sink to forward events to New Relic Logs [sinks.new_relic_logs] # REQUIRED type = \"new_relic_logs\" # must be: \"new_relic_logs\" inputs = [\"mylog\"] # example - value must be one or more source IDs license_key = \"YOUR_LICENSE_KEY\" region = \"us\" # Enum, must be one of: \"us\" \"eu\" depending on your New Relic Logs account region encoding.codec = \"json\" # OPTIONAL healthcheck = true # default Copy Restart the Vector service to ensure your changes are applied. Test the Vector New Relic logs sink To test if the New Relic logs sink is forwarding events: Run the following command to append a test log message to your log file: echo \"test message\" >> /PATH/TO/YOUR/LOG/FILE Copy Search New Relic Logs UI for your test message. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 266.1046,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Vector output sink for <em>log</em> forwarding",
        "sections": "Configure the Vector <em>New</em> <em>Relic</em> <em>logs</em> sink",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " in both of these places: <em>New</em> <em>Relic</em> <em>Logs</em> UI <em>New</em> <em>Relic</em> tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM <em>Log</em> Copy If no data appears after you <em>enable</em> our <em>log</em> <em>management</em> capabilities, follow our standard <em>log</em> troubleshooting procedures. What&#x27;s next? Explore"
      },
      "id": "6045057664441f9a4a378f17"
    }
  ],
  "/docs/logs/forward-logs/logstash-plugin-log-forwarding": [
    {
      "sections": [
        "Kubernetes plugin for log forwarding",
        "Enable Kubernetes for log management",
        "View log data",
        "What's next?"
      ],
      "title": "Kubernetes plugin for log forwarding",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "c55c6a084a1de057aac4f85ed3807c41317f63e1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/kubernetes-plugin-log-forwarding/",
      "published_at": "2021-10-18T05:22:43Z",
      "updated_at": "2021-10-13T02:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your log data in New Relic. This plugin is also provided in a standalone Docker image that can be installed in a Kubernetes cluster in the form of a DaemonSet. We refer to this as our Kubernetes plugin. Forwarding your Kubernetes logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Enable Kubernetes for log management To forward your logs to New Relic with our Kubernetes plugin for Fluent Bit: Make sure you have: A New Relic license key Kubernetes cluster deployed Follow the procedures in GitHub to install the Kubernetes plugin. Optional: If you are installing the plugin as a Helm chart, you can set numerous configurations. However, we recommend the standard setup, as it is valid for most users. Generate some traffic and wait a few minutes, then check your account for data. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 297.07983,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kubernetes plugin for <em>log</em> forwarding",
        "sections": "<em>Enable</em> Kubernetes for <em>log</em> <em>management</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " to this as our Kubernetes plugin. Forwarding your Kubernetes <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert on your <em>log</em> data. <em>Enable</em> Kubernetes for <em>log</em> <em>management</em> To forward your <em>logs</em> to <em>New</em> <em>Relic</em> with our Kubernetes plugin for Fluent Bit"
      },
      "id": "603ec19128ccbcad24eba748"
    },
    {
      "sections": [
        "Ruby: Configure logs in context",
        "Set up your Ruby app",
        "Standard Ruby on Rails configuration",
        "Incompatible gems",
        "Rails advanced configuration",
        "Ruby configuration without Rails",
        "Lograge configuration",
        "Other logging extensions",
        "Troubleshooting",
        "What's next?"
      ],
      "title": "Ruby: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Logs in context for Ruby"
      ],
      "external_id": "d2100c0d6ddffe6b52a1f968f61f777a1e56a13c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/configure-logs-context-ruby/",
      "published_at": "2021-10-19T03:54:54Z",
      "updated_at": "2021-10-19T03:54:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Ruby agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Ruby app To enable logs in context for APM apps monitored by Ruby: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Ruby agent version, and enable distributed tracing. Use Ruby agent version 6.7.0 or higher for logs in context. For Rails applications, use a supported Rails version. Configure logs in context for Ruby. Standard Ruby on Rails configuration Rails logging is controlled by two components: A logger you can customize by setting config.logger A log formatter you can customize by setting config.log_formatter) In most cases, you should configure logs in context by setting config.log_formatter to the DecoratingFormatter in your Rails application. For more information about Rails configuration, see the rubyonrails.org documentation. In your application's config, require newrelic_rpm, then add the following line: module ________ class Application < Rails::Application ... config.log_formatter = ::NewRelic::Agent::Logging::DecoratingFormatter.new end end Copy This configuration uses the New Relic formatter for log messages, but the remaining configuration is provided by the other Rails settings. Incompatible gems New Relic's decorating logger is known to be incompatible with the following gems: logging semantic logger rails_stdout_logger rails_12factor Rails advanced configuration If setting the log_formatter option doesn't meet your needs, replace the entire Rails logger with an instance of the New Relic logger. Provide the parameters to the logger's constructor, like this: module ________ class Application < Rails::Application ... config.logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( \"log/application.log\", #etc... ) end end Copy Ruby configuration without Rails For non-Rails applications, use the DecoratingLogger in place of the Ruby standard ::Logger, like this: logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( 'log/application.log', #etc... ) ... logger.info(...) Copy The DecoratingLogger is a drop-in replacement for the Ruby standard ::Logger. Their constructors accept the same parameters. Lograge configuration To configure this extension with the lograge gem, follow standard procedures in this doc for Ruby on Rails configuration. No additional configuration is required for the lograge gem. Other logging extensions To use our logging extension with a different logging implementation or with your own custom logger, use the DecoratingFormatter. For example: module ________ class Application < Rails::Application ... config.logger = ::YourCustomLoggerImplementation.new( $stdout, formatter: ::NewRelic::Agent::Logging::DecoratingFormatter.new ) end end Copy To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you have configured your logging in /config/application.rb or in /config/environments/development.rb, run your application locally and check its logging output. You should see some output like this: {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"00fc7d46\",\"timestamp\":1567701375543,\"message\":\"example log message one\",\"log.level\":\"DEBUG\"} {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"6754870b\",\"timestamp\":1567702843604,\"message\":\"example log message two\",\"log.level\":\"DEBUG\"} Copy Troubleshooting If you don't see log data in the UI, follow the troubleshooting procedures. Also, if you see JSON logs in your application's output, but your query does not return logs, check your log forwarder. If the logs from your application are not formatted in JSON with fields like trace.id and span.id, there may be a problem with your log forwarding extension. In this situation: Check that the application is using a supported logging framework. Check that your logging configuration has been applied to all the environments where you want to forward and enrich the log data being sent to New Relic. Check that another logger is not configured later in your application's configuration. Logs in context for Ruby does not support tagged logging. If you are initializing your logger with a log_tags argument, your custom tags may not appear on the final version of your logs. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 282.51364,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Ruby: Configure <em>logs</em> <em>in</em> context",
        "sections": "Ruby: Configure <em>logs</em> <em>in</em> context",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in context for the Ruby agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Ruby app To <em>enable</em> <em>logs</em>"
      },
      "id": "612d460128ccbc17bf56a863"
    },
    {
      "sections": [
        "Forward logs and activity logs from Azure",
        "Azure Resources Manager logs",
        "Use the Azure Resource Manager (ARM) template",
        "View log data",
        "Send logs from Azure resources",
        "Azure activity logs",
        "What's next?"
      ],
      "title": "Forward logs and activity logs from Azure",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic",
        "Azure",
        "Cloud logs"
      ],
      "external_id": "793bb2faa9c693f007a632d4faf24b1dbaa0c906",
      "image": "https://docs.newrelic.com/static/da32e66650336026701ab338f5afbb60/5f1d2/azure-tile.png",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/azure-log-forwarding/",
      "published_at": "2021-10-18T20:17:19Z",
      "updated_at": "2021-10-07T16:10:48Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Microsoft Azure (ARM), you can use our template to forward and enrich your log data in New Relic. Forwarding your Azure EventHub logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Azure Resources Manager logs The Microsoft Azure Resources Manager (ARM) template provided by New Relic One helps you set up Azure to: Forward logs from EventHub to New Relic One. Forward activity logs to New Relic One through EventHub. The setup process is almost the same for both use cases. As part of the setup process, you can select which Azure activity logs you want to forward to New Relic, including: Administrative Azure Activity Logs Alert Azure Activity Logs Autoscale Azure Activity Logs Policy Azure Activity Logs Recommendation Azure Activity Logs Resource Health Azure Activity Logs Security Azure Activity Logs Service Health Azure Activity Logs Use the Azure Resource Manager (ARM) template The template is idempotent. You can start forwarding logs from EventHub and then rerun the same template to configure Azure activity logs forwarding by completing step 10. Make sure you have a New Relic license key. Log in to New Relic Logs and click Add more data sources on the top right of the page. Under Log ingestion, click the tile. Select the account you want to send the logs, and click Continue. Click Generate API Key and copy the generated API Key. Click Deploy to Azure and a new tab will be open with the ARM template loaded in Azure. Select the Resource Group where you want to create the necessary resources, and a Region. In the New Relic License Key field, paste the previously copied API Key. Ensure the New Relic One endpoint set is the one corresponding to your account. Optional: Set to true the Azure activity logs you want to forward. Click Review + create, review the data you've inserted, and click Create. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If you want to only query for logs coming from Azure, run the following query: SELECT * FROM Log where plugin.type='azure' Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. Send logs from Azure resources By default, this template only configures the needed function and resources to forward logs to New Relic One. We can also configure the activity logs to be forwarded, but there isn't a default log forwarding from your Azure resources. If you want to forward logs from any resource that produces them, you need to configure it by creating a diagnostic setting for the given resource. For example, if you have a function running on Azure and you want to forward the logs to New Relic One, you'll need to configure a diagnostic setting to forward the logs to EventHub. For more information, see the Microsoft documentation to create diagnostic settings for sendig platform logs and metrics to different destinations. Azure activity logs Activating the Azure activity logs forwarding is optional. It provides: More visibility of your Azure resources Activity of the Azure resources Information about performed actions Events and their timestamps The user who performed an action, if applicable These logs give your organization more control over the resources. However, be aware of wrong or unintentional changes on your resources and even unexpected actions. For more information about this kind of event, see the Microsoft Azure Activity Log event schema. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.665,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward <em>logs</em> and activity <em>logs</em> from Azure",
        "sections": "Azure Resources <em>Manager</em> <em>logs</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "If your <em>log</em> data is already being monitored by Microsoft Azure (ARM), you can use our template to forward and enrich your <em>log</em> data in <em>New</em> <em>Relic</em>. Forwarding your Azure EventHub <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert on your"
      },
      "id": "6150545de7b9d2ae518de37f"
    }
  ],
  "/docs/logs/forward-logs/stream-logs-using-kinesis-data-firehose": [
    {
      "sections": [
        "Kubernetes plugin for log forwarding",
        "Enable Kubernetes for log management",
        "View log data",
        "What's next?"
      ],
      "title": "Kubernetes plugin for log forwarding",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "c55c6a084a1de057aac4f85ed3807c41317f63e1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/kubernetes-plugin-log-forwarding/",
      "published_at": "2021-10-18T05:22:43Z",
      "updated_at": "2021-10-13T02:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your log data in New Relic. This plugin is also provided in a standalone Docker image that can be installed in a Kubernetes cluster in the form of a DaemonSet. We refer to this as our Kubernetes plugin. Forwarding your Kubernetes logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Enable Kubernetes for log management To forward your logs to New Relic with our Kubernetes plugin for Fluent Bit: Make sure you have: A New Relic license key Kubernetes cluster deployed Follow the procedures in GitHub to install the Kubernetes plugin. Optional: If you are installing the plugin as a Helm chart, you can set numerous configurations. However, we recommend the standard setup, as it is valid for most users. Generate some traffic and wait a few minutes, then check your account for data. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 297.07983,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kubernetes plugin for <em>log</em> forwarding",
        "sections": "<em>Enable</em> Kubernetes for <em>log</em> <em>management</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " to this as our Kubernetes plugin. Forwarding your Kubernetes <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert on your <em>log</em> data. <em>Enable</em> Kubernetes for <em>log</em> <em>management</em> To forward your <em>logs</em> to <em>New</em> <em>Relic</em> with our Kubernetes plugin for Fluent Bit"
      },
      "id": "603ec19128ccbcad24eba748"
    },
    {
      "sections": [
        "Ruby: Configure logs in context",
        "Set up your Ruby app",
        "Standard Ruby on Rails configuration",
        "Incompatible gems",
        "Rails advanced configuration",
        "Ruby configuration without Rails",
        "Lograge configuration",
        "Other logging extensions",
        "Troubleshooting",
        "What's next?"
      ],
      "title": "Ruby: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Logs in context for Ruby"
      ],
      "external_id": "d2100c0d6ddffe6b52a1f968f61f777a1e56a13c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/configure-logs-context-ruby/",
      "published_at": "2021-10-19T03:54:54Z",
      "updated_at": "2021-10-19T03:54:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Ruby agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Ruby app To enable logs in context for APM apps monitored by Ruby: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Ruby agent version, and enable distributed tracing. Use Ruby agent version 6.7.0 or higher for logs in context. For Rails applications, use a supported Rails version. Configure logs in context for Ruby. Standard Ruby on Rails configuration Rails logging is controlled by two components: A logger you can customize by setting config.logger A log formatter you can customize by setting config.log_formatter) In most cases, you should configure logs in context by setting config.log_formatter to the DecoratingFormatter in your Rails application. For more information about Rails configuration, see the rubyonrails.org documentation. In your application's config, require newrelic_rpm, then add the following line: module ________ class Application < Rails::Application ... config.log_formatter = ::NewRelic::Agent::Logging::DecoratingFormatter.new end end Copy This configuration uses the New Relic formatter for log messages, but the remaining configuration is provided by the other Rails settings. Incompatible gems New Relic's decorating logger is known to be incompatible with the following gems: logging semantic logger rails_stdout_logger rails_12factor Rails advanced configuration If setting the log_formatter option doesn't meet your needs, replace the entire Rails logger with an instance of the New Relic logger. Provide the parameters to the logger's constructor, like this: module ________ class Application < Rails::Application ... config.logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( \"log/application.log\", #etc... ) end end Copy Ruby configuration without Rails For non-Rails applications, use the DecoratingLogger in place of the Ruby standard ::Logger, like this: logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( 'log/application.log', #etc... ) ... logger.info(...) Copy The DecoratingLogger is a drop-in replacement for the Ruby standard ::Logger. Their constructors accept the same parameters. Lograge configuration To configure this extension with the lograge gem, follow standard procedures in this doc for Ruby on Rails configuration. No additional configuration is required for the lograge gem. Other logging extensions To use our logging extension with a different logging implementation or with your own custom logger, use the DecoratingFormatter. For example: module ________ class Application < Rails::Application ... config.logger = ::YourCustomLoggerImplementation.new( $stdout, formatter: ::NewRelic::Agent::Logging::DecoratingFormatter.new ) end end Copy To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you have configured your logging in /config/application.rb or in /config/environments/development.rb, run your application locally and check its logging output. You should see some output like this: {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"00fc7d46\",\"timestamp\":1567701375543,\"message\":\"example log message one\",\"log.level\":\"DEBUG\"} {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"6754870b\",\"timestamp\":1567702843604,\"message\":\"example log message two\",\"log.level\":\"DEBUG\"} Copy Troubleshooting If you don't see log data in the UI, follow the troubleshooting procedures. Also, if you see JSON logs in your application's output, but your query does not return logs, check your log forwarder. If the logs from your application are not formatted in JSON with fields like trace.id and span.id, there may be a problem with your log forwarding extension. In this situation: Check that the application is using a supported logging framework. Check that your logging configuration has been applied to all the environments where you want to forward and enrich the log data being sent to New Relic. Check that another logger is not configured later in your application's configuration. Logs in context for Ruby does not support tagged logging. If you are initializing your logger with a log_tags argument, your custom tags may not appear on the final version of your logs. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 282.51364,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Ruby: Configure <em>logs</em> <em>in</em> context",
        "sections": "Ruby: Configure <em>logs</em> <em>in</em> context",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in context for the Ruby agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Ruby app To <em>enable</em> <em>logs</em>"
      },
      "id": "612d460128ccbc17bf56a863"
    },
    {
      "sections": [
        "Forward logs and activity logs from Azure",
        "Azure Resources Manager logs",
        "Use the Azure Resource Manager (ARM) template",
        "View log data",
        "Send logs from Azure resources",
        "Azure activity logs",
        "What's next?"
      ],
      "title": "Forward logs and activity logs from Azure",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic",
        "Azure",
        "Cloud logs"
      ],
      "external_id": "793bb2faa9c693f007a632d4faf24b1dbaa0c906",
      "image": "https://docs.newrelic.com/static/da32e66650336026701ab338f5afbb60/5f1d2/azure-tile.png",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/azure-log-forwarding/",
      "published_at": "2021-10-18T20:17:19Z",
      "updated_at": "2021-10-07T16:10:48Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Microsoft Azure (ARM), you can use our template to forward and enrich your log data in New Relic. Forwarding your Azure EventHub logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Azure Resources Manager logs The Microsoft Azure Resources Manager (ARM) template provided by New Relic One helps you set up Azure to: Forward logs from EventHub to New Relic One. Forward activity logs to New Relic One through EventHub. The setup process is almost the same for both use cases. As part of the setup process, you can select which Azure activity logs you want to forward to New Relic, including: Administrative Azure Activity Logs Alert Azure Activity Logs Autoscale Azure Activity Logs Policy Azure Activity Logs Recommendation Azure Activity Logs Resource Health Azure Activity Logs Security Azure Activity Logs Service Health Azure Activity Logs Use the Azure Resource Manager (ARM) template The template is idempotent. You can start forwarding logs from EventHub and then rerun the same template to configure Azure activity logs forwarding by completing step 10. Make sure you have a New Relic license key. Log in to New Relic Logs and click Add more data sources on the top right of the page. Under Log ingestion, click the tile. Select the account you want to send the logs, and click Continue. Click Generate API Key and copy the generated API Key. Click Deploy to Azure and a new tab will be open with the ARM template loaded in Azure. Select the Resource Group where you want to create the necessary resources, and a Region. In the New Relic License Key field, paste the previously copied API Key. Ensure the New Relic One endpoint set is the one corresponding to your account. Optional: Set to true the Azure activity logs you want to forward. Click Review + create, review the data you've inserted, and click Create. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If you want to only query for logs coming from Azure, run the following query: SELECT * FROM Log where plugin.type='azure' Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. Send logs from Azure resources By default, this template only configures the needed function and resources to forward logs to New Relic One. We can also configure the activity logs to be forwarded, but there isn't a default log forwarding from your Azure resources. If you want to forward logs from any resource that produces them, you need to configure it by creating a diagnostic setting for the given resource. For example, if you have a function running on Azure and you want to forward the logs to New Relic One, you'll need to configure a diagnostic setting to forward the logs to EventHub. For more information, see the Microsoft documentation to create diagnostic settings for sendig platform logs and metrics to different destinations. Azure activity logs Activating the Azure activity logs forwarding is optional. It provides: More visibility of your Azure resources Activity of the Azure resources Information about performed actions Events and their timestamps The user who performed an action, if applicable These logs give your organization more control over the resources. However, be aware of wrong or unintentional changes on your resources and even unexpected actions. For more information about this kind of event, see the Microsoft Azure Activity Log event schema. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.665,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward <em>logs</em> and activity <em>logs</em> from Azure",
        "sections": "Azure Resources <em>Manager</em> <em>logs</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "If your <em>log</em> data is already being monitored by Microsoft Azure (ARM), you can use our template to forward and enrich your <em>log</em> data in <em>New</em> <em>Relic</em>. Forwarding your Azure EventHub <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert on your"
      },
      "id": "6150545de7b9d2ae518de37f"
    }
  ],
  "/docs/logs/forward-logs/vector-output-sink-log-forwarding": [
    {
      "sections": [
        "Kubernetes plugin for log forwarding",
        "Enable Kubernetes for log management",
        "View log data",
        "What's next?"
      ],
      "title": "Kubernetes plugin for log forwarding",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic"
      ],
      "external_id": "c55c6a084a1de057aac4f85ed3807c41317f63e1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/kubernetes-plugin-log-forwarding/",
      "published_at": "2021-10-18T05:22:43Z",
      "updated_at": "2021-10-13T02:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Fluent Bit, you can use our Fluent Bit output plugin to forward and enrich your log data in New Relic. This plugin is also provided in a standalone Docker image that can be installed in a Kubernetes cluster in the form of a DaemonSet. We refer to this as our Kubernetes plugin. Forwarding your Kubernetes logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Enable Kubernetes for log management To forward your logs to New Relic with our Kubernetes plugin for Fluent Bit: Make sure you have: A New Relic license key Kubernetes cluster deployed Follow the procedures in GitHub to install the Kubernetes plugin. Optional: If you are installing the plugin as a Helm chart, you can set numerous configurations. However, we recommend the standard setup, as it is valid for most users. Generate some traffic and wait a few minutes, then check your account for data. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 297.07974,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kubernetes plugin for <em>log</em> forwarding",
        "sections": "<em>Enable</em> Kubernetes for <em>log</em> <em>management</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": " to this as our Kubernetes plugin. Forwarding your Kubernetes <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert on your <em>log</em> data. <em>Enable</em> Kubernetes for <em>log</em> <em>management</em> To forward your <em>logs</em> to <em>New</em> <em>Relic</em> with our Kubernetes plugin for Fluent Bit"
      },
      "id": "603ec19128ccbcad24eba748"
    },
    {
      "sections": [
        "Ruby: Configure logs in context",
        "Set up your Ruby app",
        "Standard Ruby on Rails configuration",
        "Incompatible gems",
        "Rails advanced configuration",
        "Ruby configuration without Rails",
        "Lograge configuration",
        "Other logging extensions",
        "Troubleshooting",
        "What's next?"
      ],
      "title": "Ruby: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Logs in context for Ruby"
      ],
      "external_id": "d2100c0d6ddffe6b52a1f968f61f777a1e56a13c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/configure-logs-context-ruby/",
      "published_at": "2021-10-19T03:54:54Z",
      "updated_at": "2021-10-19T03:54:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Ruby agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Ruby app To enable logs in context for APM apps monitored by Ruby: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Ruby agent version, and enable distributed tracing. Use Ruby agent version 6.7.0 or higher for logs in context. For Rails applications, use a supported Rails version. Configure logs in context for Ruby. Standard Ruby on Rails configuration Rails logging is controlled by two components: A logger you can customize by setting config.logger A log formatter you can customize by setting config.log_formatter) In most cases, you should configure logs in context by setting config.log_formatter to the DecoratingFormatter in your Rails application. For more information about Rails configuration, see the rubyonrails.org documentation. In your application's config, require newrelic_rpm, then add the following line: module ________ class Application < Rails::Application ... config.log_formatter = ::NewRelic::Agent::Logging::DecoratingFormatter.new end end Copy This configuration uses the New Relic formatter for log messages, but the remaining configuration is provided by the other Rails settings. Incompatible gems New Relic's decorating logger is known to be incompatible with the following gems: logging semantic logger rails_stdout_logger rails_12factor Rails advanced configuration If setting the log_formatter option doesn't meet your needs, replace the entire Rails logger with an instance of the New Relic logger. Provide the parameters to the logger's constructor, like this: module ________ class Application < Rails::Application ... config.logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( \"log/application.log\", #etc... ) end end Copy Ruby configuration without Rails For non-Rails applications, use the DecoratingLogger in place of the Ruby standard ::Logger, like this: logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( 'log/application.log', #etc... ) ... logger.info(...) Copy The DecoratingLogger is a drop-in replacement for the Ruby standard ::Logger. Their constructors accept the same parameters. Lograge configuration To configure this extension with the lograge gem, follow standard procedures in this doc for Ruby on Rails configuration. No additional configuration is required for the lograge gem. Other logging extensions To use our logging extension with a different logging implementation or with your own custom logger, use the DecoratingFormatter. For example: module ________ class Application < Rails::Application ... config.logger = ::YourCustomLoggerImplementation.new( $stdout, formatter: ::NewRelic::Agent::Logging::DecoratingFormatter.new ) end end Copy To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you have configured your logging in /config/application.rb or in /config/environments/development.rb, run your application locally and check its logging output. You should see some output like this: {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"00fc7d46\",\"timestamp\":1567701375543,\"message\":\"example log message one\",\"log.level\":\"DEBUG\"} {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"6754870b\",\"timestamp\":1567702843604,\"message\":\"example log message two\",\"log.level\":\"DEBUG\"} Copy Troubleshooting If you don't see log data in the UI, follow the troubleshooting procedures. Also, if you see JSON logs in your application's output, but your query does not return logs, check your log forwarder. If the logs from your application are not formatted in JSON with fields like trace.id and span.id, there may be a problem with your log forwarding extension. In this situation: Check that the application is using a supported logging framework. Check that your logging configuration has been applied to all the environments where you want to forward and enrich the log data being sent to New Relic. Check that another logger is not configured later in your application's configuration. Logs in context for Ruby does not support tagged logging. If you are initializing your logger with a log_tags argument, your custom tags may not appear on the final version of your logs. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 282.51343,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Ruby: Configure <em>logs</em> <em>in</em> context",
        "sections": "Ruby: Configure <em>logs</em> <em>in</em> context",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in context for the Ruby agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Ruby app To <em>enable</em> <em>logs</em>"
      },
      "id": "612d460128ccbc17bf56a863"
    },
    {
      "sections": [
        "Forward logs and activity logs from Azure",
        "Azure Resources Manager logs",
        "Use the Azure Resource Manager (ARM) template",
        "View log data",
        "Send logs from Azure resources",
        "Azure activity logs",
        "What's next?"
      ],
      "title": "Forward logs and activity logs from Azure",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Enable log monitoring in New Relic",
        "Azure",
        "Cloud logs"
      ],
      "external_id": "793bb2faa9c693f007a632d4faf24b1dbaa0c906",
      "image": "https://docs.newrelic.com/static/da32e66650336026701ab338f5afbb60/5f1d2/azure-tile.png",
      "url": "https://docs.newrelic.com/docs/logs/forward-logs/azure-log-forwarding/",
      "published_at": "2021-10-18T20:17:19Z",
      "updated_at": "2021-10-07T16:10:48Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If your log data is already being monitored by Microsoft Azure (ARM), you can use our template to forward and enrich your log data in New Relic. Forwarding your Azure EventHub logs to New Relic will give you enhanced log management capabilities to collect, process, explore, query, and alert on your log data. Azure Resources Manager logs The Microsoft Azure Resources Manager (ARM) template provided by New Relic One helps you set up Azure to: Forward logs from EventHub to New Relic One. Forward activity logs to New Relic One through EventHub. The setup process is almost the same for both use cases. As part of the setup process, you can select which Azure activity logs you want to forward to New Relic, including: Administrative Azure Activity Logs Alert Azure Activity Logs Autoscale Azure Activity Logs Policy Azure Activity Logs Recommendation Azure Activity Logs Resource Health Azure Activity Logs Security Azure Activity Logs Service Health Azure Activity Logs Use the Azure Resource Manager (ARM) template The template is idempotent. You can start forwarding logs from EventHub and then rerun the same template to configure Azure activity logs forwarding by completing step 10. Make sure you have a New Relic license key. Log in to New Relic Logs and click Add more data sources on the top right of the page. Under Log ingestion, click the tile. Select the account you want to send the logs, and click Continue. Click Generate API Key and copy the generated API Key. Click Deploy to Azure and a new tab will be open with the ARM template loaded in Azure. Select the Resource Group where you want to create the necessary resources, and a Region. In the New Relic License Key field, paste the previously copied API Key. Ensure the New Relic One endpoint set is the one corresponding to your account. Optional: Set to true the Azure activity logs you want to forward. Click Review + create, review the data you've inserted, and click Create. View log data If everything is configured correctly and your data is being collected, you should see data logs in both of these places: New Relic Logs UI New Relic tools for running NRQL queries. For example, you can execute a query like this: SELECT * FROM Log Copy If you want to only query for logs coming from Azure, run the following query: SELECT * FROM Log where plugin.type='azure' Copy If no data appears after you enable our log management capabilities, follow our standard log troubleshooting procedures. Send logs from Azure resources By default, this template only configures the needed function and resources to forward logs to New Relic One. We can also configure the activity logs to be forwarded, but there isn't a default log forwarding from your Azure resources. If you want to forward logs from any resource that produces them, you need to configure it by creating a diagnostic setting for the given resource. For example, if you have a function running on Azure and you want to forward the logs to New Relic One, you'll need to configure a diagnostic setting to forward the logs to EventHub. For more information, see the Microsoft documentation to create diagnostic settings for sendig platform logs and metrics to different destinations. Azure activity logs Activating the Azure activity logs forwarding is optional. It provides: More visibility of your Azure resources Activity of the Azure resources Information about performed actions Events and their timestamps The user who performed an action, if applicable These logs give your organization more control over the resources. However, be aware of wrong or unintentional changes on your resources and even unexpected actions. For more information about this kind of event, see the Microsoft Azure Activity Log event schema. What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.66498,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Forward <em>logs</em> and activity <em>logs</em> from Azure",
        "sections": "Azure Resources <em>Manager</em> <em>logs</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "If your <em>log</em> data is already being monitored by Microsoft Azure (ARM), you can use our template to forward and enrich your <em>log</em> data in <em>New</em> <em>Relic</em>. Forwarding your Azure EventHub <em>logs</em> to <em>New</em> <em>Relic</em> will give you enhanced <em>log</em> <em>management</em> capabilities to collect, process, explore, query, and alert on your"
      },
      "id": "6150545de7b9d2ae518de37f"
    }
  ],
  "/docs/logs/index": [
    {
      "sections": [
        "Introduction to the Log API",
        "HTTP endpoint",
        "HTTP setup",
        "HTTP headers",
        "HTTP query parameters",
        "JSON body",
        "Simplified JSON body message",
        "Detailed JSON body message",
        "Limits and restricted characters",
        "Caution",
        "Important",
        "Rate limit violations",
        "HTTP requests per minute",
        "JSON bytes per minute",
        "Log payload format",
        "JSON message attributes",
        "Common block attributes",
        "Logs block attributes",
        "JSON message attribute parsing",
        "Log JSON examples",
        "Log POST message example",
        "JSON POST request example",
        "What's next?"
      ],
      "title": "Introduction to the Log API",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "Log API"
      ],
      "external_id": "198ebbf54f4a13fdf2f5b0f19d8cc8677afd09a2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/log-management/log-api/introduction-log-api/",
      "published_at": "2021-10-19T03:43:55Z",
      "updated_at": "2021-10-19T03:43:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If our log forwarding solutions don't meet your needs, you can use our Log API to send log data directly to New Relic via an HTTP endpoint. Want to try out our Log API? Create a New Relic account for free! No credit card required. HTTP endpoint Use the endpoint that's applicable for your New Relic account: United States (US) endpoint: https://log-api.newrelic.com/log/v1 Copy European Union (EU) endpoint: https://log-api.eu.newrelic.com/log/v1 Copy HTTP setup To send log data to your New Relic account via the Log API: Get your New Relic license key. Review the limits and restricted characters for your JSON payload. Generate the JSON message using the required headers and body fields. Ensure that your Api-Key or License-Key is included in your headers or query parameters. Refer to the log JSON examples. Send your JSON message to the appropriate HTTP endpoint for your New Relic account in a POST request. US: https://log-api.newrelic.com/log/v1 EU: https://log-api.eu.newrelic.com/log/v1 Generate some traffic and wait a few minutes, then check your account for data. If no data appears after you enable our log management capabilities, follow our troubleshooting procedures. HTTP headers When creating your HTTP headers, use these guidelines: Header Supported values Content-Type Required application/json json application/gzip gzip Api-Key Required A New Relic license key. You can also send this via query parameter. You can also use an Insights insert key but the license key is preferred. Gzipped JSON formatting is accepted. If sending compressed JSON, please include the Content-Type: application/json and Content-Encoding: gzip headers. HTTP query parameters The license key can also be passed as a query string parameter. This can be useful when sending logs from cloud-based sources that don't allow custom HTTP request headers. Query parameter Value Api-Key Your license key. Use this key whenever you send a header. You can also use an Insights insert key but the license key is preferred. JSON body You can send your JSON message using either a simplified or detailed set of attributes: Simplified JSON body message When using the simplified format to create your JSON message, send a single JSON object with the following: Field Value type Format Required Notes \"timestamp\" Integer Either milliseconds or seconds since epoch No If the field is not specific as millisecond or seconds since epoch, the message will be timestamped using the ingest time \"message\" String any string No This is the main log message field that is searched by default \"logtype\" String any string No Primary field for identifying logs and matching parsing rules other_fields (must not contain white space) String any string No These will become attributes of the log message Note: Log management does not support white space in attribute names Detailed JSON body message When using the detailed format to create your body, it must be a JSON array containing one or more JSON objects, each of which with the following format: Field Value type Format Required Notes \"common\" Object See common. No Any attributes that are common to all log messages \"logs\" Array See logs. Yes Array with the log entries Limits and restricted characters Caution Avoid calling our API from within the code of a customer-facing application. This can cause performance issues or block your application if response time is slow. If you need to do it this way, call our API asynchronously to avoid these performance issues. Restrictions on logs sent to the Log API: Payload total size: 1MB(10^6 bytes) maximum per POST. We highly recommend using compression. The payload must be encoded as UTF-8. Number of attributes per event: 255 maximum Length of attribute name: 255 characters Length of attribute value: 4096 maximum character length Some specific attributes have additional restrictions: accountId: This is a reserved attribute name. If it is included, it will be dropped during ingest. entity.guid, entity.name, and entity.type: These attributes are used internally to identify entities. Any values submitted with these keys in the attributes section of a metric data point may cause undefined behavior such as missing entities in the UI or telemetry not associating with the expected entities. For more information please refer to Entity synthesis. eventType: This is a reserved attribute name. If it is included, it will be dropped during ingest. timestamp: Must be a Unix epoch timestamp. You can define timestamps either in seconds or in milliseconds. Important Payloads with timestamps older than 48 hours may be dropped. Rate limits on logs sent to the Log API: Maximum rate for HTTP requests sent to the Log API: 300,000 requests per minute Maximum rate of uncompressed Log JSON bytes sent to the Log API: 10 GB per minute Rate limit violations Exceeding rate limits affects how the Log API behaves. Follow these instructions if this happens. HTTP requests per minute When the maximum request rate limit is exceeded for an account, the New Relic Log API returns a 429 response for the remainder of the minute. This response includes a Retry-After header indicating how long to wait in seconds before resubmitting or sending new data. To resolve this issue, either reduce the number of data points you are sending, or request a rate limit change. Subsequent subscription changes do not impact modified rate limits. If an account change impacts your rate limit, you must notify us to adjust your rate limit. To request rate limit changes, contact your New Relic account representative, or visit our Support portal. JSON bytes per minute When the maximum Log JSON byte limit is exceeded for an account, the New Relic Log API returns a 429 response for the remainder of the minute. This response includes a Retry-After header indicating how long to wait in seconds before resubmitting or sending new data. To resolve this issue, try to reduce the amount of log data you are sending, or spread it out over a larger period of time. To request rate limit changes, contact your New Relic account representative, or visit our Support portal. Log payload format We accept any valid JSON payload. The payload must encoded as UTF-8. Important Log management does not support white space in attribute names. For example, {\"Sample Attribute\": \"Value\"} would cause errors. JSON message attributes Common block attributes This is a block containing attributes that will be common to all log entries in logs: Field Value type Format Required Notes \"timestamp\" Integer Milliseconds or seconds since epoch No Message timestamp default to ingest time \"attributes\" Object JSON No This sub-object contains all other attributes of the message Logs block attributes This is an array containing log entries with the following format: Field Value type Format Required Notes \"timestamp\" Integer Milliseconds or seconds since epoch No Message timestamp default to ingest time \"attributes\" Object JSON No This sub-object contains all other attributes of the message \"message\" String (any string) Yes This is the main log message field that is searched by default \"log\" String (any string) No We will rewrite this string as the field message on ingest \"LOG\" String (any string) No We will rewrite this string as the field message on ingest \"MESSAGE\" String (any string) No We will rewrite this string as the field message on ingest JSON message attribute parsing Our log management capabilities will parse any message attribute as JSON. The resulting JSON attributes in the parsed message will be added to the event. If the message attribute is not JSON, it is left as is. Important New Relic's log management capabilities do not support white space in attribute names. For example, {\"Sample Attribute\": \"Value\"} would cause errors. Here is an example message attribute: { \"timestamp\": 1562767499238, \"message\": \"{\\\"service-name\\\": \\\"login-service\\\", \\\"user\\\": {\\\"id\\\": 123, \\\"name\\\": \\\"alice\\\"}}\" } Copy This will be treated as: { \"timestamp\": 1562767499238, \"message\": \"{\\\"service-name\\\": \\\"my-service\\\", \\\"user\\\": {\\\"id\\\": 123, \\\"name\\\": \\\"alice\\\"}}\", \"service-name\": \"my-service\", \"user\": { \"id\": 123, \"name\": \"alice\" } } Copy Log JSON examples Attributes can be scalar JSON types like string and number. They can also be compound (or nested) objects. Compound attributes will have their associated attributes stored with flattened names. For example, here is a compound user attribute in a log entry's attributes: \"attributes\": { \"action\": \"login\", \"user\": { \"id\": 123, \"name\": \"alice\" } } Copy This will result in the following attributes being stored with the log event: Attribute Value \"action\" \"login\" \"user.id\" 123 \"user.name\" \"alice\" Log POST message example Log POST message example: POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: <YOUR_LICENSE_KEY> Accept: */* Content-Length: 319 [{ \"common\": { \"attributes\": { \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } }, \"logs\": [{ \"timestamp\": <TIMESTAMP_IN_UNIX_EPOCH>, \"message\": \"User 'xyz' logged in\" },{ \"timestamp\": <TIMESTAMP_IN_UNIX_EPOCH>, \"message\": \"User 'xyz' logged out\", \"attributes\": { \"auditId\": 123 } }] }] Copy This POST message would result in the following log messages being stored in New Relic: Attribute Value \"logtype\" \"accesslogs\" \"service\" \"login-service\" \"hostname\" \"login.example.com\" Here's an example of stored logs block attributes: Attribute Value \"timestamp\" 1550086450124 \"message\" \"User 'xyz' logged out\" \"auditId\" 123 JSON POST request example Here's an example of a JSON POST request: POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: <YOUR_LICENSE_KEY> Accept: */* Content-Length: 133 { \"timestamp\": <TIMESTAMP_IN_UNIX_EPOCH>, \"message\": \"User 'xyz' logged in\", \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } Copy What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 95.18326,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to the <em>Log</em> API",
        "sections": "<em>Logs</em> block attributes",
        "tags": "<em>Logs</em>",
        "body": " or seconds since epoch, the message will be timestamped using the ingest time &quot;message&quot; String any string No This is the main <em>log</em> message field that is searched by default &quot;logtype&quot; String any string No Primary field for identifying <em>logs</em> and matching parsing rules other_fields (must not contain white"
      },
      "id": "603ea832196a6726e7a83da1"
    },
    {
      "sections": [
        "Discover value in log data with patterns",
        "Technical overview",
        "Availability",
        "Get started",
        "Explore log patterns",
        "Explore logs with no pattern",
        "Masked attributes and wildcards",
        "Troubleshooting",
        "Put the platform to work with patterns"
      ],
      "title": "Discover value in log data with patterns",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "8f1e27c94327ca4888a945f8e12f9c2310ccd7a6",
      "image": "https://docs.newrelic.com/static/578d7186bb34352855696e5307cc82f2/c1b63/log-patterns-logs-without-a-pattern.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/find-unusual-logs-log-patterns/",
      "published_at": "2021-10-19T03:56:24Z",
      "updated_at": "2021-10-19T03:56:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Log patterns are the fastest way to discover value in log data without searching. Log data is high volume telemetry with a low value per individual record. Searching can quickly lead to logs that provide a root cause explanation, but most data is repetitive and hard to contextualize when browsing. Patterns can make log data discoverable without spending a lot of time reading through low value data. Logs > Patterns: Use patterns as the basis for alerts when the frequency of important data changes, or for configuring drop rules to get rid of unnecessary repetitive data. Technical overview Log patterns functionality applies machine learning to normalize and group log messages that are consistent in format but variable in content. These grouped messages can be sorted, making it easy to find the most frequent or rarest sets of logs in your environment. Use patterns as the basis for alerts when the frequency of important data changes, or to configure drop rules to get rid of unnecessary repetitive data. Log patterns use advanced clustering algorithms to group together similar log messages automatically. With patterns, you can: Orient more quickly through millions of logs. Reduce the time it takes to identify unusual behavior in your log estate. Monitor the frequency of known patterns over time to focus your energy on what matters, and exclude what's irrelevant. Availability The ability to configure this feature is dependent on role-based permissions. For example, accounts still using our original pricing model require an Owner or Admin role. If you see Patterns are turned off in your Log management Patterns UI, click the Configure Patterns button and enable it. If you don't see patterns within 30 minutes of enabling the feature, there may be a lack of data with a message attribute for the system to create a pattern from it. Log patterns Limitations and considerations Pricing There is no separate pricing for log patterns. The only cost is for additional data generated and added to your log records. A pattern attribute will be added to all logs that match a pattern. Attributes also may be added when common values are discovered, such as GUIDs, IP addresses, URL, or email addresses. These attributes are automatically extracted from the log message as part of the pattern process. HITRUST accounts The log patterns feature is not FedRAMP compliant. FedRAMP or other HITRUST accounts are not eligible to use patterns. Parsing limits We have a system of safety limits on memory and CPU resources when processing logs and their patterns. These parsing limits can have an impact on the data you get. For more information, see our documentation about parsing limits. Get started To start examining patterns: Go to one.newrelic.com > Logs, and use the account picker dropdown to select the target account where you want to explore patterns. In the left navigation of the Logs UI, click Patterns. The main log UI changes to show patterns that match the query in the query bar. Logs > Log patterns: The line chart shows the top 5 patterns over time. Use the time picker and query bar to adjust the results. Explore log patterns By default the log patterns UI first shows the most frequent occurrence of patterns. To sort to show the rarest patterns first, click the Count column. You can also use the query bar or attributes bar to filter your log patterns. If you want to... Do this... Understand the rate of change in patterns Look at the line chart. The color-coded patterns correspond to the plot column in the table. You can toggle individual plot patterns to narrow your focus. See the individual log messages that match each pattern Click pattern to expand the row and see a table of individual log records. To see additional records, scroll up or down. To explore an individual log in more detail, click it to open the details panel. Group and filter patterns by their attributes Use the query bar and time picker. As you apply different filters and time windows, the log patterns adjust to your new target data. Create an alert from a pattern Add the pattern to the query bar and run the query. Then click Create alert condition in the left nav. Troubleshoot log messages that haven't been clustered into a pattern Use the Logs with no pattern tab in the Log patterns UI. Clicking a specific log message will open the log message details panel you're familiar with from the Logs management page. Explore logs with no pattern The Logs with no pattern tab groups all recent log messages in your account that were not clustered into a known pattern yet. These log messages don't represent any problem or flaw in the system; they have no pattern because they are too new to have been processed by the machine learning system. This makes them valuable to explore when you want to understand what has recently changed in your environment. Logs > Log patterns: New Relic's log patterns feature automatically groups logs without a matching pattern. For example: Are any of these logs tied to a recent problem? This is a quick way to discover unique log data that is appearing for the first time in your environment. Does your log data have a new format? Sometimes the logs don't represent a problem, but a new format of log data that deviates from the data model you expect your applications to follow. Catching these logs early gives you the opportunity to ask developers to correct any deviations in their log output. The more consistent people are in the way log data is generated, the easier it becomes to use logs across a diverse set of teams. Masked attributes and wildcards Parts of the log messages in patterns are classified as variables and are substituted by masked attributes. The masking process supports and improves the clustering phase by allowing the algorithm to ignore changing details and focus on the repetitive structure. Masked attributes include: date_time ip url uuid Masked attributes are highlighted and are easy to identify, as shown in the following example. Here is an example of a log pattern that has masked attributes. Log patterns extract other less trivial variables that don't belong to any masked attribute. These variables are indicated as wildcards *. Here is an example of how wildcards * group variables in the Log patterns UI. Troubleshooting Here are a few reasons why you might have patterns enabled but not see any pattern data. If you're sure none of the items below are true, get help from support.newrelic.com. No data has arrived in the timeframe you're observing. Try expanding the time range you're viewing with the time picker. It's been less than 24 hours since patterns were enabled in the account. This means the ML model may not be generated for the account yet. None of the data coming in has a message field. Patterns will only be generated for values in the message field of a log record. If your logs don't contain message, there will be no data. Put the platform to work with patterns Patterns are a value that is enriched onto the existing log message as a new attribute named newrelic.logPattern. Anything you can do with logs generally can be done with log patterns, such as: Build your own dashboards with patterns, to monitor a specific pattern or group of patterns you care about. Create alerts for patterns by adding NRQL alerts. Use baseline alert conditions to detect anomalies in known log patterns.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 83.65546,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Discover value in <em>log</em> data with patterns",
        "sections": "Explore <em>logs</em> with no pattern",
        "tags": "<em>Logs</em>",
        "body": "<em>Log</em> patterns are the fastest way to discover value in <em>log</em> data without searching. <em>Log</em> data is high volume telemetry with a low value per individual record. Searching can quickly lead to <em>logs</em> that provide a root cause explanation, but most data is repetitive and hard to contextualize when browsing"
      },
      "id": "6072d46128ccbc244451c18b"
    },
    {
      "sections": [
        "Parsing log data",
        "Example",
        "How log parsing works",
        "Important",
        "Limits",
        "Tip",
        "Built-in parsing rulesets",
        "List of built-in rulesets",
        "Add the logtype attribute",
        "New Relic infrastructure agent example",
        "Fluentd example",
        "Fluent Bit example",
        "Logstash example",
        "Logs API example",
        "Create custom parsing rules",
        "Troubleshooting"
      ],
      "title": "Parsing log data",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "52955adb68242c4ca582ba9cb8e22963955a8275",
      "image": "https://docs.newrelic.com/static/dc392bb7142d2fdb253a649daf4ebe6d/c1b63/log-parsing-rule-ui.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/parsing/",
      "published_at": "2021-10-19T03:43:56Z",
      "updated_at": "2021-10-19T03:43:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Parsing is the process of splitting unstructured log data into attributes (key/value pairs). You can use these attributes to facet or filter logs in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial on YouTube (approx. 4-1/2 minutes). New Relic parses log data according to rules. This document describes how logs parsing works, how to use built-in rules, and how to create custom rules. You can also create, query, and manage your log parsing rules by using NerdGraph, our GraphQL API, at api.newrelic.com/graphiql. For more information, see our NerdGraph tutorial for parsing. Example A good example is a default NGINX access log containing unstructured text. It is useful for searching but not much else. Here's an example of a typical line: 127.180.71.3 - - [10/May/1997:08:05:32 +0000] \"GET /downloads/product_1 HTTP/1.1\" 304 0 \"-\" \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" Copy In an unparsed format, you would need to do a full text search to answer most questions. After parsing, the log is organized into attributes, like response code and request URL: { \"remote_addr\":\"93.180.71.3\", \"time\":\"1586514731\", \"method\":\"GET\", \"path\":\"/downloads/product_1\", \"version\":\"HTTP/1.1\", \"response\":\"304\", \"bytesSent\": 0, \"user_agent\": \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" } Copy Parsing makes it easier to create custom queries that facet on those values. This helps you understand the distribution of response codes per request URL and quickly find problematic pages. How log parsing works Here's an overview of how New Relic implements parsing of logs: Log parsing How it works What All parsing takes place against the message field; no other fields can be parsed. Each parsing rule is created with matching criteria that determines which logs the rule will attempt to parse. To simplify the matching process, we recommend adding a logtype attribute to your logs. However, you are not limited to using logtype; any attribute can be used as matching criteria. When Parsing will only be applied once to each log message. If multiple parsing rules match the log, only the first that succeeds will be applied. Parsing takes place during log ingestion, before data is written to NRDB. Once data has been written to storage, it can no longer be parsed. Parsing occurs in the pipeline before data enrichments take place. Be careful when defining the matching criteria for a parsing rule. If the criteria is based on an attribute that doesn't exist untail after parsing or enrichment take place, that data won't be present in the logs when matching occurs. As a result, no parsing will happen. How Rules can be written in Grok, regex, or a mixture of the two. Grok is a collection of patterns that abstract away complicated regular expressions. If the content of the message field is JSON, it will be parsed automatically. New Relic's log ingestion pipeline can parse data by matching a log event to a rule that describes how the log should be parsed. There are two ways log events can be parsed: Use a built-in rule. Define a custom rule. Rules are a combination of matching logic and parsing logic. Matching is done by defining a query match on an attribute of the logs. Rules are not applied retroactively. Logs collected before a rule is created are not parsed by that rule. The simplest way to organize your logs and how they are parsed is to include the logtype field in your log event. This tells New Relic what built-in ruleset to apply to the logs. Important Once a parsing rule is active, data parsed by the rule is permanently changed. This cannot be reverted. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. Limits Parsing is computationally expensive, which introduces risk. Parsing is done for custom rules defined in an account and for matching patterns to a log. A large number of patterns or poorly defined custom rules will consume a huge amount of memory and CPU resources while also taking a very long time to complete. In order to prevent problems, we apply two parsing limits: per-message-per-rule and per-account. Limit Description Per-message-per-rule The per-message-per-rule limit prevents the time spent parsing any single message from being greater than 100 ms. If that limit is reached, the system will cease attempting to parse the log message with that rule. The ingestion pipeline will attempt to run any other applicable on that message, and the message will still be passed through the ingestion pipeline and stored in NRDB. The log message will be in its original, unparsed format. Per-account The per-account limit exists to prevent accounts from using more than their fair share of resources. The limit considers the total time spent processing all log messages for an account per-minute. The limit is not a fixed value; it scales up or down proportionally to the volume of data stored daily by the account and the environment size that is subsequently allocated to support that customer. Tip To easily check if your rate limits have been reached, go to your system Limits page in the New Relic UI. Built-in parsing rulesets Common log formats have well-established parsing rules already created for them. To get the benefit of built-in parsing rules, add the logtype attribute when forwarding logs. Set the value to something listed in the following table, and the rules for that type of log will be applied automatically. List of built-in rulesets The following logtype attribute values map to a predefined parsing rule. For example, to query the Application Load Balancer: From the New Relic UI, use the format logtype: alb. From NerdGraph, use the format logtype = 'alb'. To learn what fields are parsed for each rule, see our documentation about built-in parsing rules. logtype Example matching query alb AWS Application Load Balancer logtype:alb apache Apache Access logtype:apache cloudfront-web CloudFront Web logtype:cloudfront-web elb Amazon Elastic Load Balancer logtype:elb iis_w3c Microsoft IIS server logs - W3C format logtype:iis_w3c monit Monit logs logtype:monit mysql-error MySQL Error logtype:mysql-error nginx NGINX access logs logtype:nginx nginx-error NGINX error logs logtype:nginx-error route-53 Amazon Route 53 logs logtype:route-53 syslog-rfc5424 Syslog logtype:syslog-rfc5424 Add the logtype attribute When aggregating logs, it's important to provide metadata that makes it easy to organize, search, and parse those logs. One simple way of doing this is to add the attribute logtype to the log messages when they are shipped. Built-in parsing rules are applied by default to certain logtype values. Here are some examples of how to add logtype to logs sent by some of our supported shipping methods. New Relic infrastructure agent example Add logtype as an attribute. You must set the logtype for each named source. Learn more about using the infrastructure agent to add attributes. logs: - name: file-simple file: /path/to/file attributes: logtype: fileRaw - name: nginx-example file: /var/log/nginx.log attributes: logtype: nginx Copy Fluentd example Add a filter block to the .conf file, which uses a record_transformer to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluentd examples. <filter containers> @type record_transformer enable_ruby true <record> #Add logtype to trigger a built-in parsing rule for nginx access logs logtype nginx #Set timestamp from the value contained in the field \"time\" timestamp record[\"time\"] #Add hostname and tag fields to all records hostname \"#{Socket.gethostname}\" tag ${tag} </record> </filter> Copy Fluent Bit example Add a filter block to the .conf file that uses a record_modifier to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluent Bit examples. [FILTER] Name record_modifier Match * Record logtype nginx Record hostname ${HOSTNAME} Record service_name Sample-App-Name Copy Logstash example Add a filter block to the Logstash configuration which uses an add_field mutate filter to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Logstash examples. filter { mutate { add_field => { \"logtype\" => \"nginx\" \"service_name\" => \"myservicename\" \"hostname\" => \"%{host}\" } } } Copy Logs API example You can add attributes to the JSON request sent to New Relic. In this example we add a logtype attribute of value nginx to trigger the built-in NGINX parsing rule. Learn more about using the Logs API. POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: YOUR_LICENSE_KEY Accept: */* Content-Length: 133 { \"timestamp\": TIMESTAMP_IN_UNIX_EPOCH, \"message\": \"User 'xyz' logged in\", \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } Copy Create custom parsing rules Many logs are formatted or structured in a unique way. In order to parse them, custom logic must be built and applied. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. To create and manage your own, custom parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing, then click Create parsing rule. Enter the parsing rule's name. Choose an attribute and value to match on. Write your Grok pattern and test the rule. To learn about Grok and custom parsing rules, read our blog post about how to parse logs with Grok patterns. Enable and save the custom parsing rule. To view the list of custom parsing rules: From Manage Data on the left nav of the Logs UI, click Parsing. To view existing parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing. Troubleshooting If parsing is not working the way you intended, it may be due to: Logic: The parsing rule matching logic does not match the logs you want. Timing: If your parsing matching rule targets a value that doesn't exist yet, it will fail. This can occur if the value is added later in the pipeline as part of the enrichment process. Limits: There is a fixed amount of time available every minute to process logs via parsing, patterns, drop filters, etc. If the maximum amount of time has been spent, parsing will be skipped for additional log event records. To resolve these problems, create or adjust your custom parsing rules.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 83.61244,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Parsing <em>log</em> data",
        "sections": "<em>Logstash</em> example",
        "tags": "<em>Logs</em>",
        "body": "Parsing is the process of splitting unstructured <em>log</em> data into attributes (key&#x2F;value pairs). You can use these attributes to facet or filter <em>logs</em> in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial"
      },
      "id": "603e7eb4196a67b0c4a83dd1"
    }
  ],
  "/docs/logs/log-management/get-started/get-started-log-management": [
    {
      "sections": [
        "New Relic's log management security and privacy",
        "Automatic obfuscation",
        "Customize your security settings"
      ],
      "title": "New Relic's log management security and privacy",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "Get started"
      ],
      "external_id": "ea5a441833677e4e7e60dfca315a3a410a7c9309",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/log-management/get-started/new-relics-log-management-security-privacy/",
      "published_at": "2021-10-18T14:47:32Z",
      "updated_at": "2021-10-13T02:49:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With our log management solution, you have direct control over what data is reported to New Relic. To ensure data privacy, and to limit the types of information New Relic receives, no customer data is captured except what you supply in API calls or log forwarder configuration. All data for the logs service is then reported to New Relic over HTTPS. This document describes additional security considerations for your logging data. For more information about New Relic's security measures: See our security and privacy documentation. Visit the New Relic security website. Read this blog post that explains how you can use our log management tools to gain visibility into some of the most severe threats to modern digital businesses. Automatic obfuscation The log management service automatically masks number patterns that appear to be for items such as credit cards or Social Security numbers. All integers, including spaces and hyphens that may be used as delimiters, are replaced with a string of Xes. Numbers that appear to be a credit card (thirteen to sixteen digits) are obfuscated as XXXXXXXXXXXXXXXX. For example: Numbers with hyphens, such as 4111-1111-1111-1111 Numbers with spaces, such as 4111 1111 1111 1111 Numbers with thirteen (Visa), fourteen (Diner's Club), fifteen (American Express, JCB), or sixteen digits (Visa, Mastercard, Discover, JCB), such as 4111111111111111 Nine-digit numbers with hyphens that appear to be Social Security numbers, such as 123-45-6789, are obfuscated as XXXXXXXXX. Nine-digit numbers with spaces, such as 123 45 6789, or hyphens in a different pattern, such as 12-345-67-89, are not automatically obfuscated. If you need to opt out of automatic obfuscation, get support at support.newrelic.com. Customize your security settings The data you send to New Relic, including any additional filtering, is controlled by the configuration of the log forwarder you use. You control what customer data is logged, so be sure to follow your organization's security guidelines to mask, obfuscate, or prevent sending any sensitive data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 220.7396,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic&#x27;s <em>log</em> <em>management</em> security and privacy",
        "sections": "New Relic&#x27;s <em>log</em> <em>management</em> security and privacy",
        "tags": "<em>Log</em> <em>management</em>",
        "body": "With our <em>log</em> <em>management</em> solution, you have direct control over what data is reported to New Relic. To ensure data privacy, and to limit the types of information New Relic receives, no customer data is captured except what you supply in API calls or <em>log</em> forwarder configuration. All data for the <em>logs</em>"
      },
      "id": "603ea3dc64441f0bc14e884f"
    },
    {
      "sections": [
        "Discover value in log data with patterns",
        "Technical overview",
        "Availability",
        "Get started",
        "Explore log patterns",
        "Explore logs with no pattern",
        "Masked attributes and wildcards",
        "Troubleshooting",
        "Put the platform to work with patterns"
      ],
      "title": "Discover value in log data with patterns",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "8f1e27c94327ca4888a945f8e12f9c2310ccd7a6",
      "image": "https://docs.newrelic.com/static/578d7186bb34352855696e5307cc82f2/c1b63/log-patterns-logs-without-a-pattern.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/find-unusual-logs-log-patterns/",
      "published_at": "2021-10-19T03:56:24Z",
      "updated_at": "2021-10-19T03:56:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Log patterns are the fastest way to discover value in log data without searching. Log data is high volume telemetry with a low value per individual record. Searching can quickly lead to logs that provide a root cause explanation, but most data is repetitive and hard to contextualize when browsing. Patterns can make log data discoverable without spending a lot of time reading through low value data. Logs > Patterns: Use patterns as the basis for alerts when the frequency of important data changes, or for configuring drop rules to get rid of unnecessary repetitive data. Technical overview Log patterns functionality applies machine learning to normalize and group log messages that are consistent in format but variable in content. These grouped messages can be sorted, making it easy to find the most frequent or rarest sets of logs in your environment. Use patterns as the basis for alerts when the frequency of important data changes, or to configure drop rules to get rid of unnecessary repetitive data. Log patterns use advanced clustering algorithms to group together similar log messages automatically. With patterns, you can: Orient more quickly through millions of logs. Reduce the time it takes to identify unusual behavior in your log estate. Monitor the frequency of known patterns over time to focus your energy on what matters, and exclude what's irrelevant. Availability The ability to configure this feature is dependent on role-based permissions. For example, accounts still using our original pricing model require an Owner or Admin role. If you see Patterns are turned off in your Log management Patterns UI, click the Configure Patterns button and enable it. If you don't see patterns within 30 minutes of enabling the feature, there may be a lack of data with a message attribute for the system to create a pattern from it. Log patterns Limitations and considerations Pricing There is no separate pricing for log patterns. The only cost is for additional data generated and added to your log records. A pattern attribute will be added to all logs that match a pattern. Attributes also may be added when common values are discovered, such as GUIDs, IP addresses, URL, or email addresses. These attributes are automatically extracted from the log message as part of the pattern process. HITRUST accounts The log patterns feature is not FedRAMP compliant. FedRAMP or other HITRUST accounts are not eligible to use patterns. Parsing limits We have a system of safety limits on memory and CPU resources when processing logs and their patterns. These parsing limits can have an impact on the data you get. For more information, see our documentation about parsing limits. Get started To start examining patterns: Go to one.newrelic.com > Logs, and use the account picker dropdown to select the target account where you want to explore patterns. In the left navigation of the Logs UI, click Patterns. The main log UI changes to show patterns that match the query in the query bar. Logs > Log patterns: The line chart shows the top 5 patterns over time. Use the time picker and query bar to adjust the results. Explore log patterns By default the log patterns UI first shows the most frequent occurrence of patterns. To sort to show the rarest patterns first, click the Count column. You can also use the query bar or attributes bar to filter your log patterns. If you want to... Do this... Understand the rate of change in patterns Look at the line chart. The color-coded patterns correspond to the plot column in the table. You can toggle individual plot patterns to narrow your focus. See the individual log messages that match each pattern Click pattern to expand the row and see a table of individual log records. To see additional records, scroll up or down. To explore an individual log in more detail, click it to open the details panel. Group and filter patterns by their attributes Use the query bar and time picker. As you apply different filters and time windows, the log patterns adjust to your new target data. Create an alert from a pattern Add the pattern to the query bar and run the query. Then click Create alert condition in the left nav. Troubleshoot log messages that haven't been clustered into a pattern Use the Logs with no pattern tab in the Log patterns UI. Clicking a specific log message will open the log message details panel you're familiar with from the Logs management page. Explore logs with no pattern The Logs with no pattern tab groups all recent log messages in your account that were not clustered into a known pattern yet. These log messages don't represent any problem or flaw in the system; they have no pattern because they are too new to have been processed by the machine learning system. This makes them valuable to explore when you want to understand what has recently changed in your environment. Logs > Log patterns: New Relic's log patterns feature automatically groups logs without a matching pattern. For example: Are any of these logs tied to a recent problem? This is a quick way to discover unique log data that is appearing for the first time in your environment. Does your log data have a new format? Sometimes the logs don't represent a problem, but a new format of log data that deviates from the data model you expect your applications to follow. Catching these logs early gives you the opportunity to ask developers to correct any deviations in their log output. The more consistent people are in the way log data is generated, the easier it becomes to use logs across a diverse set of teams. Masked attributes and wildcards Parts of the log messages in patterns are classified as variables and are substituted by masked attributes. The masking process supports and improves the clustering phase by allowing the algorithm to ignore changing details and focus on the repetitive structure. Masked attributes include: date_time ip url uuid Masked attributes are highlighted and are easy to identify, as shown in the following example. Here is an example of a log pattern that has masked attributes. Log patterns extract other less trivial variables that don't belong to any masked attribute. These variables are indicated as wildcards *. Here is an example of how wildcards * group variables in the Log patterns UI. Troubleshooting Here are a few reasons why you might have patterns enabled but not see any pattern data. If you're sure none of the items below are true, get help from support.newrelic.com. No data has arrived in the timeframe you're observing. Try expanding the time range you're viewing with the time picker. It's been less than 24 hours since patterns were enabled in the account. This means the ML model may not be generated for the account yet. None of the data coming in has a message field. Patterns will only be generated for values in the message field of a log record. If your logs don't contain message, there will be no data. Put the platform to work with patterns Patterns are a value that is enriched onto the existing log message as a new attribute named newrelic.logPattern. Anything you can do with logs generally can be done with log patterns, such as: Build your own dashboards with patterns, to monitor a specific pattern or group of patterns you care about. Create alerts for patterns by adding NRQL alerts. Use baseline alert conditions to detect anomalies in known log patterns.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 214.28033,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Discover value in <em>log</em> data with patterns",
        "sections": "<em>Get</em> <em>started</em>",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " <em>started</em> To <em>start</em> examining patterns: Go to one.newrelic.com &gt; <em>Logs</em>, and use the account picker dropdown to select the target account where you want to explore patterns. In the left navigation of the <em>Logs</em> UI, click Patterns. The main <em>log</em> UI changes to show patterns that match the query in the query bar"
      },
      "id": "6072d46128ccbc244451c18b"
    },
    {
      "sections": [
        "Parsing log data",
        "Example",
        "How log parsing works",
        "Important",
        "Limits",
        "Tip",
        "Built-in parsing rulesets",
        "List of built-in rulesets",
        "Add the logtype attribute",
        "New Relic infrastructure agent example",
        "Fluentd example",
        "Fluent Bit example",
        "Logstash example",
        "Logs API example",
        "Create custom parsing rules",
        "Troubleshooting"
      ],
      "title": "Parsing log data",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "52955adb68242c4ca582ba9cb8e22963955a8275",
      "image": "https://docs.newrelic.com/static/dc392bb7142d2fdb253a649daf4ebe6d/c1b63/log-parsing-rule-ui.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/parsing/",
      "published_at": "2021-10-19T03:43:56Z",
      "updated_at": "2021-10-19T03:43:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Parsing is the process of splitting unstructured log data into attributes (key/value pairs). You can use these attributes to facet or filter logs in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial on YouTube (approx. 4-1/2 minutes). New Relic parses log data according to rules. This document describes how logs parsing works, how to use built-in rules, and how to create custom rules. You can also create, query, and manage your log parsing rules by using NerdGraph, our GraphQL API, at api.newrelic.com/graphiql. For more information, see our NerdGraph tutorial for parsing. Example A good example is a default NGINX access log containing unstructured text. It is useful for searching but not much else. Here's an example of a typical line: 127.180.71.3 - - [10/May/1997:08:05:32 +0000] \"GET /downloads/product_1 HTTP/1.1\" 304 0 \"-\" \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" Copy In an unparsed format, you would need to do a full text search to answer most questions. After parsing, the log is organized into attributes, like response code and request URL: { \"remote_addr\":\"93.180.71.3\", \"time\":\"1586514731\", \"method\":\"GET\", \"path\":\"/downloads/product_1\", \"version\":\"HTTP/1.1\", \"response\":\"304\", \"bytesSent\": 0, \"user_agent\": \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" } Copy Parsing makes it easier to create custom queries that facet on those values. This helps you understand the distribution of response codes per request URL and quickly find problematic pages. How log parsing works Here's an overview of how New Relic implements parsing of logs: Log parsing How it works What All parsing takes place against the message field; no other fields can be parsed. Each parsing rule is created with matching criteria that determines which logs the rule will attempt to parse. To simplify the matching process, we recommend adding a logtype attribute to your logs. However, you are not limited to using logtype; any attribute can be used as matching criteria. When Parsing will only be applied once to each log message. If multiple parsing rules match the log, only the first that succeeds will be applied. Parsing takes place during log ingestion, before data is written to NRDB. Once data has been written to storage, it can no longer be parsed. Parsing occurs in the pipeline before data enrichments take place. Be careful when defining the matching criteria for a parsing rule. If the criteria is based on an attribute that doesn't exist untail after parsing or enrichment take place, that data won't be present in the logs when matching occurs. As a result, no parsing will happen. How Rules can be written in Grok, regex, or a mixture of the two. Grok is a collection of patterns that abstract away complicated regular expressions. If the content of the message field is JSON, it will be parsed automatically. New Relic's log ingestion pipeline can parse data by matching a log event to a rule that describes how the log should be parsed. There are two ways log events can be parsed: Use a built-in rule. Define a custom rule. Rules are a combination of matching logic and parsing logic. Matching is done by defining a query match on an attribute of the logs. Rules are not applied retroactively. Logs collected before a rule is created are not parsed by that rule. The simplest way to organize your logs and how they are parsed is to include the logtype field in your log event. This tells New Relic what built-in ruleset to apply to the logs. Important Once a parsing rule is active, data parsed by the rule is permanently changed. This cannot be reverted. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. Limits Parsing is computationally expensive, which introduces risk. Parsing is done for custom rules defined in an account and for matching patterns to a log. A large number of patterns or poorly defined custom rules will consume a huge amount of memory and CPU resources while also taking a very long time to complete. In order to prevent problems, we apply two parsing limits: per-message-per-rule and per-account. Limit Description Per-message-per-rule The per-message-per-rule limit prevents the time spent parsing any single message from being greater than 100 ms. If that limit is reached, the system will cease attempting to parse the log message with that rule. The ingestion pipeline will attempt to run any other applicable on that message, and the message will still be passed through the ingestion pipeline and stored in NRDB. The log message will be in its original, unparsed format. Per-account The per-account limit exists to prevent accounts from using more than their fair share of resources. The limit considers the total time spent processing all log messages for an account per-minute. The limit is not a fixed value; it scales up or down proportionally to the volume of data stored daily by the account and the environment size that is subsequently allocated to support that customer. Tip To easily check if your rate limits have been reached, go to your system Limits page in the New Relic UI. Built-in parsing rulesets Common log formats have well-established parsing rules already created for them. To get the benefit of built-in parsing rules, add the logtype attribute when forwarding logs. Set the value to something listed in the following table, and the rules for that type of log will be applied automatically. List of built-in rulesets The following logtype attribute values map to a predefined parsing rule. For example, to query the Application Load Balancer: From the New Relic UI, use the format logtype: alb. From NerdGraph, use the format logtype = 'alb'. To learn what fields are parsed for each rule, see our documentation about built-in parsing rules. logtype Example matching query alb AWS Application Load Balancer logtype:alb apache Apache Access logtype:apache cloudfront-web CloudFront Web logtype:cloudfront-web elb Amazon Elastic Load Balancer logtype:elb iis_w3c Microsoft IIS server logs - W3C format logtype:iis_w3c monit Monit logs logtype:monit mysql-error MySQL Error logtype:mysql-error nginx NGINX access logs logtype:nginx nginx-error NGINX error logs logtype:nginx-error route-53 Amazon Route 53 logs logtype:route-53 syslog-rfc5424 Syslog logtype:syslog-rfc5424 Add the logtype attribute When aggregating logs, it's important to provide metadata that makes it easy to organize, search, and parse those logs. One simple way of doing this is to add the attribute logtype to the log messages when they are shipped. Built-in parsing rules are applied by default to certain logtype values. Here are some examples of how to add logtype to logs sent by some of our supported shipping methods. New Relic infrastructure agent example Add logtype as an attribute. You must set the logtype for each named source. Learn more about using the infrastructure agent to add attributes. logs: - name: file-simple file: /path/to/file attributes: logtype: fileRaw - name: nginx-example file: /var/log/nginx.log attributes: logtype: nginx Copy Fluentd example Add a filter block to the .conf file, which uses a record_transformer to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluentd examples. <filter containers> @type record_transformer enable_ruby true <record> #Add logtype to trigger a built-in parsing rule for nginx access logs logtype nginx #Set timestamp from the value contained in the field \"time\" timestamp record[\"time\"] #Add hostname and tag fields to all records hostname \"#{Socket.gethostname}\" tag ${tag} </record> </filter> Copy Fluent Bit example Add a filter block to the .conf file that uses a record_modifier to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluent Bit examples. [FILTER] Name record_modifier Match * Record logtype nginx Record hostname ${HOSTNAME} Record service_name Sample-App-Name Copy Logstash example Add a filter block to the Logstash configuration which uses an add_field mutate filter to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Logstash examples. filter { mutate { add_field => { \"logtype\" => \"nginx\" \"service_name\" => \"myservicename\" \"hostname\" => \"%{host}\" } } } Copy Logs API example You can add attributes to the JSON request sent to New Relic. In this example we add a logtype attribute of value nginx to trigger the built-in NGINX parsing rule. Learn more about using the Logs API. POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: YOUR_LICENSE_KEY Accept: */* Content-Length: 133 { \"timestamp\": TIMESTAMP_IN_UNIX_EPOCH, \"message\": \"User 'xyz' logged in\", \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } Copy Create custom parsing rules Many logs are formatted or structured in a unique way. In order to parse them, custom logic must be built and applied. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. To create and manage your own, custom parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing, then click Create parsing rule. Enter the parsing rule's name. Choose an attribute and value to match on. Write your Grok pattern and test the rule. To learn about Grok and custom parsing rules, read our blog post about how to parse logs with Grok patterns. Enable and save the custom parsing rule. To view the list of custom parsing rules: From Manage Data on the left nav of the Logs UI, click Parsing. To view existing parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing. Troubleshooting If parsing is not working the way you intended, it may be due to: Logic: The parsing rule matching logic does not match the logs you want. Timing: If your parsing matching rule targets a value that doesn't exist yet, it will fail. This can occur if the value is added later in the pipeline as part of the enrichment process. Limits: There is a fixed amount of time available every minute to process logs via parsing, patterns, drop filters, etc. If the maximum amount of time has been spent, parsing will be skipped for additional log event records. To resolve these problems, create or adjust your custom parsing rules.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 208.5441,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Parsing <em>log</em> data",
        "sections": "<em>Logstash</em> example",
        "tags": "<em>Log</em> <em>management</em>",
        "body": "Parsing is the process of splitting unstructured <em>log</em> data into attributes (key&#x2F;value pairs). You can use these attributes to facet or filter <em>logs</em> in useful ways. This in turn helps you build better charts and alerts. To <em>get</em> <em>started</em> with parsing, you may want to watch the following video tutorial"
      },
      "id": "603e7eb4196a67b0c4a83dd1"
    }
  ],
  "/docs/logs/log-management/get-started/new-relics-log-management-security-privacy": [
    {
      "sections": [
        "Get started with log management",
        "Find problems faster, reduce context switching",
        "Bring in your logging data",
        "View your logging data in New Relic"
      ],
      "title": "Get started with log management",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "Get started"
      ],
      "external_id": "77761091d3c83970c78e92210970ade2a7441df9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/log-management/get-started/get-started-log-management/",
      "published_at": "2021-10-18T14:47:32Z",
      "updated_at": "2021-10-13T02:48:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "As applications move towards the cloud, microservices architecture is becoming more dispersed, making the ability to monitor logs essential. New Relic offers a fast, scalable log management platform so you can connect your logs with the rest of your telemetry and infrastructure data in a single place. Our log management solution provides deeper visibility into application and infrastructure performance data (events and errors) to reduce mean-time-to-resolve (MTTR) and quickly troubleshoot production incidents. It does this by providing super-fast searching capabilities, alerts, and co-location of application, infrastructure, and log data, while visualizing everything from a single place. Find problems faster, reduce context switching Log management provides a way to connect your log data with the rest of your application and infrastructure data, allowing you to get to the root cause of problems quickly, without losing context switching between tools. Log management features include: Instantly search through your logs. Visualize your log data directly from the Logs UI. Use logging data to create custom charts, dashboards, and alerts. Troubleshoot performance issues without switching between tools. Bring in your logging data To bring your log data into New Relic, you can: Use our infrastructure monitoring agent as a lightweight data collector, without having to install additional software. Select from a wide range of log forwarding plugins, including Amazon, Microsoft, Fluentd, Fluent Bit, Kubernetes, Logstash, and more. Use our OpenTelemetry solutions. Send your log data by using the Log API or TCP endpoint. Once log management is enabled, you can also connect your logs with your APM agent, Kubernetes clusters, or distributed tracing to get additional contextual logging data with our logs in context extensions. View your logging data in New Relic You can explore your logging data in the UI or by API: Logs UI at one.newrelic.com Logs UI for EU region data center if applicable: one.eu.newrelic.com You can also query the Log data type. For example, use NRQL to run: SELECT * FROM Log Copy You can also use NerdGraph, our GraphQL-format API, to request the exact data you need. For more information, see our documentation about query options in New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 220.73508,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Get</em> <em>started</em> with <em>log</em> <em>management</em>",
        "sections": "<em>Get</em> <em>started</em> with <em>log</em> <em>management</em>",
        "tags": "<em>Log</em> <em>management</em>",
        "body": ", Fluent Bit, Kubernetes, Logstash, and more. Use our OpenTelemetry solutions. Send your <em>log</em> data by using the <em>Log</em> API or TCP endpoint. Once <em>log</em> <em>management</em> is enabled, you can also connect your <em>logs</em> with your APM agent, Kubernetes clusters, or distributed tracing to <em>get</em> additional contextual logging"
      },
      "id": "603ea62ee7b9d249432a07e2"
    },
    {
      "sections": [
        "Discover value in log data with patterns",
        "Technical overview",
        "Availability",
        "Get started",
        "Explore log patterns",
        "Explore logs with no pattern",
        "Masked attributes and wildcards",
        "Troubleshooting",
        "Put the platform to work with patterns"
      ],
      "title": "Discover value in log data with patterns",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "8f1e27c94327ca4888a945f8e12f9c2310ccd7a6",
      "image": "https://docs.newrelic.com/static/578d7186bb34352855696e5307cc82f2/c1b63/log-patterns-logs-without-a-pattern.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/find-unusual-logs-log-patterns/",
      "published_at": "2021-10-19T03:56:24Z",
      "updated_at": "2021-10-19T03:56:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Log patterns are the fastest way to discover value in log data without searching. Log data is high volume telemetry with a low value per individual record. Searching can quickly lead to logs that provide a root cause explanation, but most data is repetitive and hard to contextualize when browsing. Patterns can make log data discoverable without spending a lot of time reading through low value data. Logs > Patterns: Use patterns as the basis for alerts when the frequency of important data changes, or for configuring drop rules to get rid of unnecessary repetitive data. Technical overview Log patterns functionality applies machine learning to normalize and group log messages that are consistent in format but variable in content. These grouped messages can be sorted, making it easy to find the most frequent or rarest sets of logs in your environment. Use patterns as the basis for alerts when the frequency of important data changes, or to configure drop rules to get rid of unnecessary repetitive data. Log patterns use advanced clustering algorithms to group together similar log messages automatically. With patterns, you can: Orient more quickly through millions of logs. Reduce the time it takes to identify unusual behavior in your log estate. Monitor the frequency of known patterns over time to focus your energy on what matters, and exclude what's irrelevant. Availability The ability to configure this feature is dependent on role-based permissions. For example, accounts still using our original pricing model require an Owner or Admin role. If you see Patterns are turned off in your Log management Patterns UI, click the Configure Patterns button and enable it. If you don't see patterns within 30 minutes of enabling the feature, there may be a lack of data with a message attribute for the system to create a pattern from it. Log patterns Limitations and considerations Pricing There is no separate pricing for log patterns. The only cost is for additional data generated and added to your log records. A pattern attribute will be added to all logs that match a pattern. Attributes also may be added when common values are discovered, such as GUIDs, IP addresses, URL, or email addresses. These attributes are automatically extracted from the log message as part of the pattern process. HITRUST accounts The log patterns feature is not FedRAMP compliant. FedRAMP or other HITRUST accounts are not eligible to use patterns. Parsing limits We have a system of safety limits on memory and CPU resources when processing logs and their patterns. These parsing limits can have an impact on the data you get. For more information, see our documentation about parsing limits. Get started To start examining patterns: Go to one.newrelic.com > Logs, and use the account picker dropdown to select the target account where you want to explore patterns. In the left navigation of the Logs UI, click Patterns. The main log UI changes to show patterns that match the query in the query bar. Logs > Log patterns: The line chart shows the top 5 patterns over time. Use the time picker and query bar to adjust the results. Explore log patterns By default the log patterns UI first shows the most frequent occurrence of patterns. To sort to show the rarest patterns first, click the Count column. You can also use the query bar or attributes bar to filter your log patterns. If you want to... Do this... Understand the rate of change in patterns Look at the line chart. The color-coded patterns correspond to the plot column in the table. You can toggle individual plot patterns to narrow your focus. See the individual log messages that match each pattern Click pattern to expand the row and see a table of individual log records. To see additional records, scroll up or down. To explore an individual log in more detail, click it to open the details panel. Group and filter patterns by their attributes Use the query bar and time picker. As you apply different filters and time windows, the log patterns adjust to your new target data. Create an alert from a pattern Add the pattern to the query bar and run the query. Then click Create alert condition in the left nav. Troubleshoot log messages that haven't been clustered into a pattern Use the Logs with no pattern tab in the Log patterns UI. Clicking a specific log message will open the log message details panel you're familiar with from the Logs management page. Explore logs with no pattern The Logs with no pattern tab groups all recent log messages in your account that were not clustered into a known pattern yet. These log messages don't represent any problem or flaw in the system; they have no pattern because they are too new to have been processed by the machine learning system. This makes them valuable to explore when you want to understand what has recently changed in your environment. Logs > Log patterns: New Relic's log patterns feature automatically groups logs without a matching pattern. For example: Are any of these logs tied to a recent problem? This is a quick way to discover unique log data that is appearing for the first time in your environment. Does your log data have a new format? Sometimes the logs don't represent a problem, but a new format of log data that deviates from the data model you expect your applications to follow. Catching these logs early gives you the opportunity to ask developers to correct any deviations in their log output. The more consistent people are in the way log data is generated, the easier it becomes to use logs across a diverse set of teams. Masked attributes and wildcards Parts of the log messages in patterns are classified as variables and are substituted by masked attributes. The masking process supports and improves the clustering phase by allowing the algorithm to ignore changing details and focus on the repetitive structure. Masked attributes include: date_time ip url uuid Masked attributes are highlighted and are easy to identify, as shown in the following example. Here is an example of a log pattern that has masked attributes. Log patterns extract other less trivial variables that don't belong to any masked attribute. These variables are indicated as wildcards *. Here is an example of how wildcards * group variables in the Log patterns UI. Troubleshooting Here are a few reasons why you might have patterns enabled but not see any pattern data. If you're sure none of the items below are true, get help from support.newrelic.com. No data has arrived in the timeframe you're observing. Try expanding the time range you're viewing with the time picker. It's been less than 24 hours since patterns were enabled in the account. This means the ML model may not be generated for the account yet. None of the data coming in has a message field. Patterns will only be generated for values in the message field of a log record. If your logs don't contain message, there will be no data. Put the platform to work with patterns Patterns are a value that is enriched onto the existing log message as a new attribute named newrelic.logPattern. Anything you can do with logs generally can be done with log patterns, such as: Build your own dashboards with patterns, to monitor a specific pattern or group of patterns you care about. Create alerts for patterns by adding NRQL alerts. Use baseline alert conditions to detect anomalies in known log patterns.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 214.28033,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Discover value in <em>log</em> data with patterns",
        "sections": "<em>Get</em> <em>started</em>",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " <em>started</em> To <em>start</em> examining patterns: Go to one.newrelic.com &gt; <em>Logs</em>, and use the account picker dropdown to select the target account where you want to explore patterns. In the left navigation of the <em>Logs</em> UI, click Patterns. The main <em>log</em> UI changes to show patterns that match the query in the query bar"
      },
      "id": "6072d46128ccbc244451c18b"
    },
    {
      "sections": [
        "Parsing log data",
        "Example",
        "How log parsing works",
        "Important",
        "Limits",
        "Tip",
        "Built-in parsing rulesets",
        "List of built-in rulesets",
        "Add the logtype attribute",
        "New Relic infrastructure agent example",
        "Fluentd example",
        "Fluent Bit example",
        "Logstash example",
        "Logs API example",
        "Create custom parsing rules",
        "Troubleshooting"
      ],
      "title": "Parsing log data",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "52955adb68242c4ca582ba9cb8e22963955a8275",
      "image": "https://docs.newrelic.com/static/dc392bb7142d2fdb253a649daf4ebe6d/c1b63/log-parsing-rule-ui.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/parsing/",
      "published_at": "2021-10-19T03:43:56Z",
      "updated_at": "2021-10-19T03:43:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Parsing is the process of splitting unstructured log data into attributes (key/value pairs). You can use these attributes to facet or filter logs in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial on YouTube (approx. 4-1/2 minutes). New Relic parses log data according to rules. This document describes how logs parsing works, how to use built-in rules, and how to create custom rules. You can also create, query, and manage your log parsing rules by using NerdGraph, our GraphQL API, at api.newrelic.com/graphiql. For more information, see our NerdGraph tutorial for parsing. Example A good example is a default NGINX access log containing unstructured text. It is useful for searching but not much else. Here's an example of a typical line: 127.180.71.3 - - [10/May/1997:08:05:32 +0000] \"GET /downloads/product_1 HTTP/1.1\" 304 0 \"-\" \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" Copy In an unparsed format, you would need to do a full text search to answer most questions. After parsing, the log is organized into attributes, like response code and request URL: { \"remote_addr\":\"93.180.71.3\", \"time\":\"1586514731\", \"method\":\"GET\", \"path\":\"/downloads/product_1\", \"version\":\"HTTP/1.1\", \"response\":\"304\", \"bytesSent\": 0, \"user_agent\": \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" } Copy Parsing makes it easier to create custom queries that facet on those values. This helps you understand the distribution of response codes per request URL and quickly find problematic pages. How log parsing works Here's an overview of how New Relic implements parsing of logs: Log parsing How it works What All parsing takes place against the message field; no other fields can be parsed. Each parsing rule is created with matching criteria that determines which logs the rule will attempt to parse. To simplify the matching process, we recommend adding a logtype attribute to your logs. However, you are not limited to using logtype; any attribute can be used as matching criteria. When Parsing will only be applied once to each log message. If multiple parsing rules match the log, only the first that succeeds will be applied. Parsing takes place during log ingestion, before data is written to NRDB. Once data has been written to storage, it can no longer be parsed. Parsing occurs in the pipeline before data enrichments take place. Be careful when defining the matching criteria for a parsing rule. If the criteria is based on an attribute that doesn't exist untail after parsing or enrichment take place, that data won't be present in the logs when matching occurs. As a result, no parsing will happen. How Rules can be written in Grok, regex, or a mixture of the two. Grok is a collection of patterns that abstract away complicated regular expressions. If the content of the message field is JSON, it will be parsed automatically. New Relic's log ingestion pipeline can parse data by matching a log event to a rule that describes how the log should be parsed. There are two ways log events can be parsed: Use a built-in rule. Define a custom rule. Rules are a combination of matching logic and parsing logic. Matching is done by defining a query match on an attribute of the logs. Rules are not applied retroactively. Logs collected before a rule is created are not parsed by that rule. The simplest way to organize your logs and how they are parsed is to include the logtype field in your log event. This tells New Relic what built-in ruleset to apply to the logs. Important Once a parsing rule is active, data parsed by the rule is permanently changed. This cannot be reverted. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. Limits Parsing is computationally expensive, which introduces risk. Parsing is done for custom rules defined in an account and for matching patterns to a log. A large number of patterns or poorly defined custom rules will consume a huge amount of memory and CPU resources while also taking a very long time to complete. In order to prevent problems, we apply two parsing limits: per-message-per-rule and per-account. Limit Description Per-message-per-rule The per-message-per-rule limit prevents the time spent parsing any single message from being greater than 100 ms. If that limit is reached, the system will cease attempting to parse the log message with that rule. The ingestion pipeline will attempt to run any other applicable on that message, and the message will still be passed through the ingestion pipeline and stored in NRDB. The log message will be in its original, unparsed format. Per-account The per-account limit exists to prevent accounts from using more than their fair share of resources. The limit considers the total time spent processing all log messages for an account per-minute. The limit is not a fixed value; it scales up or down proportionally to the volume of data stored daily by the account and the environment size that is subsequently allocated to support that customer. Tip To easily check if your rate limits have been reached, go to your system Limits page in the New Relic UI. Built-in parsing rulesets Common log formats have well-established parsing rules already created for them. To get the benefit of built-in parsing rules, add the logtype attribute when forwarding logs. Set the value to something listed in the following table, and the rules for that type of log will be applied automatically. List of built-in rulesets The following logtype attribute values map to a predefined parsing rule. For example, to query the Application Load Balancer: From the New Relic UI, use the format logtype: alb. From NerdGraph, use the format logtype = 'alb'. To learn what fields are parsed for each rule, see our documentation about built-in parsing rules. logtype Example matching query alb AWS Application Load Balancer logtype:alb apache Apache Access logtype:apache cloudfront-web CloudFront Web logtype:cloudfront-web elb Amazon Elastic Load Balancer logtype:elb iis_w3c Microsoft IIS server logs - W3C format logtype:iis_w3c monit Monit logs logtype:monit mysql-error MySQL Error logtype:mysql-error nginx NGINX access logs logtype:nginx nginx-error NGINX error logs logtype:nginx-error route-53 Amazon Route 53 logs logtype:route-53 syslog-rfc5424 Syslog logtype:syslog-rfc5424 Add the logtype attribute When aggregating logs, it's important to provide metadata that makes it easy to organize, search, and parse those logs. One simple way of doing this is to add the attribute logtype to the log messages when they are shipped. Built-in parsing rules are applied by default to certain logtype values. Here are some examples of how to add logtype to logs sent by some of our supported shipping methods. New Relic infrastructure agent example Add logtype as an attribute. You must set the logtype for each named source. Learn more about using the infrastructure agent to add attributes. logs: - name: file-simple file: /path/to/file attributes: logtype: fileRaw - name: nginx-example file: /var/log/nginx.log attributes: logtype: nginx Copy Fluentd example Add a filter block to the .conf file, which uses a record_transformer to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluentd examples. <filter containers> @type record_transformer enable_ruby true <record> #Add logtype to trigger a built-in parsing rule for nginx access logs logtype nginx #Set timestamp from the value contained in the field \"time\" timestamp record[\"time\"] #Add hostname and tag fields to all records hostname \"#{Socket.gethostname}\" tag ${tag} </record> </filter> Copy Fluent Bit example Add a filter block to the .conf file that uses a record_modifier to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluent Bit examples. [FILTER] Name record_modifier Match * Record logtype nginx Record hostname ${HOSTNAME} Record service_name Sample-App-Name Copy Logstash example Add a filter block to the Logstash configuration which uses an add_field mutate filter to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Logstash examples. filter { mutate { add_field => { \"logtype\" => \"nginx\" \"service_name\" => \"myservicename\" \"hostname\" => \"%{host}\" } } } Copy Logs API example You can add attributes to the JSON request sent to New Relic. In this example we add a logtype attribute of value nginx to trigger the built-in NGINX parsing rule. Learn more about using the Logs API. POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: YOUR_LICENSE_KEY Accept: */* Content-Length: 133 { \"timestamp\": TIMESTAMP_IN_UNIX_EPOCH, \"message\": \"User 'xyz' logged in\", \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } Copy Create custom parsing rules Many logs are formatted or structured in a unique way. In order to parse them, custom logic must be built and applied. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. To create and manage your own, custom parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing, then click Create parsing rule. Enter the parsing rule's name. Choose an attribute and value to match on. Write your Grok pattern and test the rule. To learn about Grok and custom parsing rules, read our blog post about how to parse logs with Grok patterns. Enable and save the custom parsing rule. To view the list of custom parsing rules: From Manage Data on the left nav of the Logs UI, click Parsing. To view existing parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing. Troubleshooting If parsing is not working the way you intended, it may be due to: Logic: The parsing rule matching logic does not match the logs you want. Timing: If your parsing matching rule targets a value that doesn't exist yet, it will fail. This can occur if the value is added later in the pipeline as part of the enrichment process. Limits: There is a fixed amount of time available every minute to process logs via parsing, patterns, drop filters, etc. If the maximum amount of time has been spent, parsing will be skipped for additional log event records. To resolve these problems, create or adjust your custom parsing rules.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 208.5441,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Parsing <em>log</em> data",
        "sections": "<em>Logstash</em> example",
        "tags": "<em>Log</em> <em>management</em>",
        "body": "Parsing is the process of splitting unstructured <em>log</em> data into attributes (key&#x2F;value pairs). You can use these attributes to facet or filter <em>logs</em> in useful ways. This in turn helps you build better charts and alerts. To <em>get</em> <em>started</em> with parsing, you may want to watch the following video tutorial"
      },
      "id": "603e7eb4196a67b0c4a83dd1"
    }
  ],
  "/docs/logs/log-management/log-api/introduction-log-api": [
    {
      "sections": [
        "Parsing log data",
        "Example",
        "How log parsing works",
        "Important",
        "Limits",
        "Tip",
        "Built-in parsing rulesets",
        "List of built-in rulesets",
        "Add the logtype attribute",
        "New Relic infrastructure agent example",
        "Fluentd example",
        "Fluent Bit example",
        "Logstash example",
        "Logs API example",
        "Create custom parsing rules",
        "Troubleshooting"
      ],
      "title": "Parsing log data",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "52955adb68242c4ca582ba9cb8e22963955a8275",
      "image": "https://docs.newrelic.com/static/dc392bb7142d2fdb253a649daf4ebe6d/c1b63/log-parsing-rule-ui.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/parsing/",
      "published_at": "2021-10-19T03:43:56Z",
      "updated_at": "2021-10-19T03:43:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Parsing is the process of splitting unstructured log data into attributes (key/value pairs). You can use these attributes to facet or filter logs in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial on YouTube (approx. 4-1/2 minutes). New Relic parses log data according to rules. This document describes how logs parsing works, how to use built-in rules, and how to create custom rules. You can also create, query, and manage your log parsing rules by using NerdGraph, our GraphQL API, at api.newrelic.com/graphiql. For more information, see our NerdGraph tutorial for parsing. Example A good example is a default NGINX access log containing unstructured text. It is useful for searching but not much else. Here's an example of a typical line: 127.180.71.3 - - [10/May/1997:08:05:32 +0000] \"GET /downloads/product_1 HTTP/1.1\" 304 0 \"-\" \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" Copy In an unparsed format, you would need to do a full text search to answer most questions. After parsing, the log is organized into attributes, like response code and request URL: { \"remote_addr\":\"93.180.71.3\", \"time\":\"1586514731\", \"method\":\"GET\", \"path\":\"/downloads/product_1\", \"version\":\"HTTP/1.1\", \"response\":\"304\", \"bytesSent\": 0, \"user_agent\": \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" } Copy Parsing makes it easier to create custom queries that facet on those values. This helps you understand the distribution of response codes per request URL and quickly find problematic pages. How log parsing works Here's an overview of how New Relic implements parsing of logs: Log parsing How it works What All parsing takes place against the message field; no other fields can be parsed. Each parsing rule is created with matching criteria that determines which logs the rule will attempt to parse. To simplify the matching process, we recommend adding a logtype attribute to your logs. However, you are not limited to using logtype; any attribute can be used as matching criteria. When Parsing will only be applied once to each log message. If multiple parsing rules match the log, only the first that succeeds will be applied. Parsing takes place during log ingestion, before data is written to NRDB. Once data has been written to storage, it can no longer be parsed. Parsing occurs in the pipeline before data enrichments take place. Be careful when defining the matching criteria for a parsing rule. If the criteria is based on an attribute that doesn't exist untail after parsing or enrichment take place, that data won't be present in the logs when matching occurs. As a result, no parsing will happen. How Rules can be written in Grok, regex, or a mixture of the two. Grok is a collection of patterns that abstract away complicated regular expressions. If the content of the message field is JSON, it will be parsed automatically. New Relic's log ingestion pipeline can parse data by matching a log event to a rule that describes how the log should be parsed. There are two ways log events can be parsed: Use a built-in rule. Define a custom rule. Rules are a combination of matching logic and parsing logic. Matching is done by defining a query match on an attribute of the logs. Rules are not applied retroactively. Logs collected before a rule is created are not parsed by that rule. The simplest way to organize your logs and how they are parsed is to include the logtype field in your log event. This tells New Relic what built-in ruleset to apply to the logs. Important Once a parsing rule is active, data parsed by the rule is permanently changed. This cannot be reverted. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. Limits Parsing is computationally expensive, which introduces risk. Parsing is done for custom rules defined in an account and for matching patterns to a log. A large number of patterns or poorly defined custom rules will consume a huge amount of memory and CPU resources while also taking a very long time to complete. In order to prevent problems, we apply two parsing limits: per-message-per-rule and per-account. Limit Description Per-message-per-rule The per-message-per-rule limit prevents the time spent parsing any single message from being greater than 100 ms. If that limit is reached, the system will cease attempting to parse the log message with that rule. The ingestion pipeline will attempt to run any other applicable on that message, and the message will still be passed through the ingestion pipeline and stored in NRDB. The log message will be in its original, unparsed format. Per-account The per-account limit exists to prevent accounts from using more than their fair share of resources. The limit considers the total time spent processing all log messages for an account per-minute. The limit is not a fixed value; it scales up or down proportionally to the volume of data stored daily by the account and the environment size that is subsequently allocated to support that customer. Tip To easily check if your rate limits have been reached, go to your system Limits page in the New Relic UI. Built-in parsing rulesets Common log formats have well-established parsing rules already created for them. To get the benefit of built-in parsing rules, add the logtype attribute when forwarding logs. Set the value to something listed in the following table, and the rules for that type of log will be applied automatically. List of built-in rulesets The following logtype attribute values map to a predefined parsing rule. For example, to query the Application Load Balancer: From the New Relic UI, use the format logtype: alb. From NerdGraph, use the format logtype = 'alb'. To learn what fields are parsed for each rule, see our documentation about built-in parsing rules. logtype Example matching query alb AWS Application Load Balancer logtype:alb apache Apache Access logtype:apache cloudfront-web CloudFront Web logtype:cloudfront-web elb Amazon Elastic Load Balancer logtype:elb iis_w3c Microsoft IIS server logs - W3C format logtype:iis_w3c monit Monit logs logtype:monit mysql-error MySQL Error logtype:mysql-error nginx NGINX access logs logtype:nginx nginx-error NGINX error logs logtype:nginx-error route-53 Amazon Route 53 logs logtype:route-53 syslog-rfc5424 Syslog logtype:syslog-rfc5424 Add the logtype attribute When aggregating logs, it's important to provide metadata that makes it easy to organize, search, and parse those logs. One simple way of doing this is to add the attribute logtype to the log messages when they are shipped. Built-in parsing rules are applied by default to certain logtype values. Here are some examples of how to add logtype to logs sent by some of our supported shipping methods. New Relic infrastructure agent example Add logtype as an attribute. You must set the logtype for each named source. Learn more about using the infrastructure agent to add attributes. logs: - name: file-simple file: /path/to/file attributes: logtype: fileRaw - name: nginx-example file: /var/log/nginx.log attributes: logtype: nginx Copy Fluentd example Add a filter block to the .conf file, which uses a record_transformer to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluentd examples. <filter containers> @type record_transformer enable_ruby true <record> #Add logtype to trigger a built-in parsing rule for nginx access logs logtype nginx #Set timestamp from the value contained in the field \"time\" timestamp record[\"time\"] #Add hostname and tag fields to all records hostname \"#{Socket.gethostname}\" tag ${tag} </record> </filter> Copy Fluent Bit example Add a filter block to the .conf file that uses a record_modifier to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluent Bit examples. [FILTER] Name record_modifier Match * Record logtype nginx Record hostname ${HOSTNAME} Record service_name Sample-App-Name Copy Logstash example Add a filter block to the Logstash configuration which uses an add_field mutate filter to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Logstash examples. filter { mutate { add_field => { \"logtype\" => \"nginx\" \"service_name\" => \"myservicename\" \"hostname\" => \"%{host}\" } } } Copy Logs API example You can add attributes to the JSON request sent to New Relic. In this example we add a logtype attribute of value nginx to trigger the built-in NGINX parsing rule. Learn more about using the Logs API. POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: YOUR_LICENSE_KEY Accept: */* Content-Length: 133 { \"timestamp\": TIMESTAMP_IN_UNIX_EPOCH, \"message\": \"User 'xyz' logged in\", \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } Copy Create custom parsing rules Many logs are formatted or structured in a unique way. In order to parse them, custom logic must be built and applied. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. To create and manage your own, custom parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing, then click Create parsing rule. Enter the parsing rule's name. Choose an attribute and value to match on. Write your Grok pattern and test the rule. To learn about Grok and custom parsing rules, read our blog post about how to parse logs with Grok patterns. Enable and save the custom parsing rule. To view the list of custom parsing rules: From Manage Data on the left nav of the Logs UI, click Parsing. To view existing parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing. Troubleshooting If parsing is not working the way you intended, it may be due to: Logic: The parsing rule matching logic does not match the logs you want. Timing: If your parsing matching rule targets a value that doesn't exist yet, it will fail. This can occur if the value is added later in the pipeline as part of the enrichment process. Limits: There is a fixed amount of time available every minute to process logs via parsing, patterns, drop filters, etc. If the maximum amount of time has been spent, parsing will be skipped for additional log event records. To resolve these problems, create or adjust your custom parsing rules.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 210.86356,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Parsing <em>log</em> data",
        "sections": "<em>Logs</em> <em>API</em> example",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " on YouTube (approx. 4-1&#x2F;2 minutes). New Relic parses <em>log</em> data according to rules. This document describes how <em>logs</em> parsing works, how to use built-in rules, and how to create custom rules. You can also create, query, and manage your <em>log</em> parsing rules by using NerdGraph, our GraphQL <em>API</em>"
      },
      "id": "603e7eb4196a67b0c4a83dd1"
    },
    {
      "sections": [
        "Discover value in log data with patterns",
        "Technical overview",
        "Availability",
        "Get started",
        "Explore log patterns",
        "Explore logs with no pattern",
        "Masked attributes and wildcards",
        "Troubleshooting",
        "Put the platform to work with patterns"
      ],
      "title": "Discover value in log data with patterns",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "8f1e27c94327ca4888a945f8e12f9c2310ccd7a6",
      "image": "https://docs.newrelic.com/static/578d7186bb34352855696e5307cc82f2/c1b63/log-patterns-logs-without-a-pattern.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/find-unusual-logs-log-patterns/",
      "published_at": "2021-10-19T03:56:24Z",
      "updated_at": "2021-10-19T03:56:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Log patterns are the fastest way to discover value in log data without searching. Log data is high volume telemetry with a low value per individual record. Searching can quickly lead to logs that provide a root cause explanation, but most data is repetitive and hard to contextualize when browsing. Patterns can make log data discoverable without spending a lot of time reading through low value data. Logs > Patterns: Use patterns as the basis for alerts when the frequency of important data changes, or for configuring drop rules to get rid of unnecessary repetitive data. Technical overview Log patterns functionality applies machine learning to normalize and group log messages that are consistent in format but variable in content. These grouped messages can be sorted, making it easy to find the most frequent or rarest sets of logs in your environment. Use patterns as the basis for alerts when the frequency of important data changes, or to configure drop rules to get rid of unnecessary repetitive data. Log patterns use advanced clustering algorithms to group together similar log messages automatically. With patterns, you can: Orient more quickly through millions of logs. Reduce the time it takes to identify unusual behavior in your log estate. Monitor the frequency of known patterns over time to focus your energy on what matters, and exclude what's irrelevant. Availability The ability to configure this feature is dependent on role-based permissions. For example, accounts still using our original pricing model require an Owner or Admin role. If you see Patterns are turned off in your Log management Patterns UI, click the Configure Patterns button and enable it. If you don't see patterns within 30 minutes of enabling the feature, there may be a lack of data with a message attribute for the system to create a pattern from it. Log patterns Limitations and considerations Pricing There is no separate pricing for log patterns. The only cost is for additional data generated and added to your log records. A pattern attribute will be added to all logs that match a pattern. Attributes also may be added when common values are discovered, such as GUIDs, IP addresses, URL, or email addresses. These attributes are automatically extracted from the log message as part of the pattern process. HITRUST accounts The log patterns feature is not FedRAMP compliant. FedRAMP or other HITRUST accounts are not eligible to use patterns. Parsing limits We have a system of safety limits on memory and CPU resources when processing logs and their patterns. These parsing limits can have an impact on the data you get. For more information, see our documentation about parsing limits. Get started To start examining patterns: Go to one.newrelic.com > Logs, and use the account picker dropdown to select the target account where you want to explore patterns. In the left navigation of the Logs UI, click Patterns. The main log UI changes to show patterns that match the query in the query bar. Logs > Log patterns: The line chart shows the top 5 patterns over time. Use the time picker and query bar to adjust the results. Explore log patterns By default the log patterns UI first shows the most frequent occurrence of patterns. To sort to show the rarest patterns first, click the Count column. You can also use the query bar or attributes bar to filter your log patterns. If you want to... Do this... Understand the rate of change in patterns Look at the line chart. The color-coded patterns correspond to the plot column in the table. You can toggle individual plot patterns to narrow your focus. See the individual log messages that match each pattern Click pattern to expand the row and see a table of individual log records. To see additional records, scroll up or down. To explore an individual log in more detail, click it to open the details panel. Group and filter patterns by their attributes Use the query bar and time picker. As you apply different filters and time windows, the log patterns adjust to your new target data. Create an alert from a pattern Add the pattern to the query bar and run the query. Then click Create alert condition in the left nav. Troubleshoot log messages that haven't been clustered into a pattern Use the Logs with no pattern tab in the Log patterns UI. Clicking a specific log message will open the log message details panel you're familiar with from the Logs management page. Explore logs with no pattern The Logs with no pattern tab groups all recent log messages in your account that were not clustered into a known pattern yet. These log messages don't represent any problem or flaw in the system; they have no pattern because they are too new to have been processed by the machine learning system. This makes them valuable to explore when you want to understand what has recently changed in your environment. Logs > Log patterns: New Relic's log patterns feature automatically groups logs without a matching pattern. For example: Are any of these logs tied to a recent problem? This is a quick way to discover unique log data that is appearing for the first time in your environment. Does your log data have a new format? Sometimes the logs don't represent a problem, but a new format of log data that deviates from the data model you expect your applications to follow. Catching these logs early gives you the opportunity to ask developers to correct any deviations in their log output. The more consistent people are in the way log data is generated, the easier it becomes to use logs across a diverse set of teams. Masked attributes and wildcards Parts of the log messages in patterns are classified as variables and are substituted by masked attributes. The masking process supports and improves the clustering phase by allowing the algorithm to ignore changing details and focus on the repetitive structure. Masked attributes include: date_time ip url uuid Masked attributes are highlighted and are easy to identify, as shown in the following example. Here is an example of a log pattern that has masked attributes. Log patterns extract other less trivial variables that don't belong to any masked attribute. These variables are indicated as wildcards *. Here is an example of how wildcards * group variables in the Log patterns UI. Troubleshooting Here are a few reasons why you might have patterns enabled but not see any pattern data. If you're sure none of the items below are true, get help from support.newrelic.com. No data has arrived in the timeframe you're observing. Try expanding the time range you're viewing with the time picker. It's been less than 24 hours since patterns were enabled in the account. This means the ML model may not be generated for the account yet. None of the data coming in has a message field. Patterns will only be generated for values in the message field of a log record. If your logs don't contain message, there will be no data. Put the platform to work with patterns Patterns are a value that is enriched onto the existing log message as a new attribute named newrelic.logPattern. Anything you can do with logs generally can be done with log patterns, such as: Build your own dashboards with patterns, to monitor a specific pattern or group of patterns you care about. Create alerts for patterns by adding NRQL alerts. Use baseline alert conditions to detect anomalies in known log patterns.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 192.03305,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Discover value in <em>log</em> data with patterns",
        "sections": "Explore <em>logs</em> with no pattern",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " messages that haven&#x27;t been clustered into a pattern Use the <em>Logs</em> with no pattern tab in the <em>Log</em> patterns UI. Clicking a specific <em>log</em> message will open the <em>log</em> message details panel you&#x27;re familiar with from the <em>Logs</em> <em>management</em> page. Explore <em>logs</em> with no pattern The <em>Logs</em> with no pattern tab groups all"
      },
      "id": "6072d46128ccbc244451c18b"
    },
    {
      "sections": [
        "Built-in log parsing rulesets",
        "Apache",
        "Application Load Balancer",
        "Cloudfront",
        "Elastic Load Balancer",
        "Microsoft IIS",
        "Monit",
        "MySQL Error",
        "NGINX",
        "NGINX Error",
        "Route 53",
        "Syslog RFC-5424"
      ],
      "title": "Built-in log parsing rulesets",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "cb5909f2453d475a85d408d75cd3b2a321a8518e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/built-log-parsing-rulesets/",
      "published_at": "2021-10-19T03:43:56Z",
      "updated_at": "2021-10-19T03:43:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic can parse common log formats according to built-in rulesets, so that you don't have to create your own parsing rules. Here are the log parsing rulesets, their Grok patterns, and what fields are parsed. To enable built-in log parsing, see our documentation for adding the logtype attribute. To manage your parsing rulesets programmatically, you can use NerdGraph, our graphQL-format API, at api.newrelic.com/graphiql. For more information, see the NerdGraph tutorial to create, query, and delete your parsing rulesets. Apache Source: logtype = 'apache' Grok: %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{NUMBER:bytes}|-) %{QS:referrer} %{QS:agent} Copy Results: clientip: The IP address of the client. verb: The HTTP verb ident: The user identity of the client making the request response: The HTTP status code of the response request: The URI and request being made httpversion: The HTTP version of the request rawrequest: The raw HTTP request if data is posted bytes: The number of bytes sent referrer: The HTTP referrer agent: The client's user agent Application Load Balancer Source: logtype = 'alb' Grok: ^%{NOTSPACE:type} %{TIMESTAMP_ISO8601:time} %{NOTSPACE:elb} %{NOTSPACE:client_ip}:%{NOTSPACE:client_port} ((%{NOTSPACE:target_ip}:%{NOTSPACE:target_port})|-) %{NOTSPACE:request_processing_time} %{NOTSPACE:target_processing_time} %{NOTSPACE:response_processing_time} %{NOTSPACE:elb_status_code} %{NOTSPACE:target_status_code} %{NOTSPACE:received_bytes} %{NOTSPACE:sent_bytes} \"%{DATA:request}\" \"%{DATA:user_agent}\" %{NOTSPACE:ssl_cipher} %{NOTSPACE:ssl_protocol} %{NOTSPACE:target_group_arn} \"%{DATA:trace_id}\" \"%{NOTSPACE:domain_name}\" \"%{NOTSPACE:chosen_cert_arn}\" %{NOTSPACE:matched_rule_priority} %{TIMESTAMP_ISO8601:request_creation_time} \"%{NOTSPACE:actions_executed}\" \"%{NOTSPACE:redirect_url}\" \"%{NOTSPACE:error_reason}\" (?:\"|)%{DATA:target_port_list}(?:\"|) (?:\"|)%{DATA:target_status_code_list}(?:\"|) \"%{NOTSPACE:classification}\" \"%{NOTSPACE:classification_reason}\" Copy Results: Field Definition type The type of request or connection. Possible values are: http: HTTP https: HTTP over SSL/TLS h2: HTTP/2 over SSL/TLS ws: WebSockets wss: WebSockets over SSL/TLS elb The resource ID of the load balancer. If you are parsing access log entries, note that resources IDs can contain forward slashes (/). client The IP address and port of the requesting client target The IP address and port of the target that processed this request. If the client didn't send a full request, the load balancer can't dispatch the request to a target, and this value is set to -. If the target is a Lambda function, this value is set to -. If the request is blocked by AWS WAF, this value is set to -, and the value of elb_status_code is set to 403. request_processing_time The total time elapsed (in seconds, with millisecond precision) from the time the load balancer received the request until the time it sent it to a target. This value is set to -1 if the load balancer can't dispatch the request to a target. This can happen if the target closes the connection before the idle timeout or if the client sends a malformed request. This value can also be set to -1 if the registered target does not respond before the idle timeout. target_processing_time The total time elapsed (in seconds, with millisecond precision) from the time the load balancer sent the request to a target until the target started to send the response headers. This value is set to -1 if the load balancer can't dispatch the request to a target. This can happen if the target closes the connection before the idle timeout or if the client sends a malformed request. This value can also be set to -1 if the registered target does not respond before the idle timeout. response_processing_time The total time elapsed (in seconds, with millisecond precision) from the time the load balancer received the response header from the target until it started to send the response to the client. This includes both the queuing time at the load balancer and the connection acquisition time from the load balancer to the client. This value is set to -1 if the load balancer can't send the request to a target. This can happen if the target closes the connection before the idle timeout or if the client sends a malformed request. elb_status_code The status code of the response from the load balancer target_status_code The status code of the response from the target. This value is recorded only if a connection was established to the target and the target sent a response. Otherwise, it is set to -. received_bytes The size of the request, in bytes, received from the client (requester). For HTTP requests, this includes the headers. For WebSockets, this is the total number of bytes received from the client on the connection. sent_bytes The size of the response, in bytes, sent to the client (requester). For HTTP requests, this includes the headers. For WebSockets, this is the total number of bytes sent to the client on the connection. method The HTTP verb of the request uri The URI the request was targeting http_version The HTTP version number of the request user_agent User-Agent string that identifies the client that originated the request, enclosed in double quotes. The string consists of one or more product identifiers, product/version. If the string is longer than 8 KB, it is truncated. ssl_cipher The SSL cipher. This value is set to - if the listener is not an HTTPS listener. ssl_protocol The SSL protocol. This value is set to - if the listener is not an HTTPS listener. target_group_arn The Amazon Resource Name (ARN) of the target group trace_id The contents of the X-Amzn-Trace-Id header, enclosed in double quotes domain_name The SNI domain provided by the client during the TLS handshake, enclosed in double quotes. This value is set to - if the client doesn't support SNI or the domain doesn't match a certificate and the default certificate is presented to the client. chosen_cert_arn The ARN of the certificate presented to the client, enclosed in double quotes. Set to session-reused if the session is reused. Set to - if the listener is not an HTTPS listener. matched_rule_priority The priority value of the rule that matched the request. If a rule matched, this is a value from 1 to 50000. If no rule matched and the default action was taken, this value is set to 0. If an error occurs during rules evaluation, it is set to -1. For any other error, it is set to -. request_creation_time The time when the load balancer received the request from the client, in ISO 8601 format. actions_executed The actions taken when processing the request, enclosed in double quotes. This value is a comma-separated list that can include the values described in actions_taken. If no action was taken, such as for a malformed request, this value is set to -. redirect_url The URL of the redirect target for the location header of the HTTP response, enclosed in double quotes. If no redirect actions were taken, this value is set to -. error_reason The error reason code, enclosed in double quotes. If the request failed, this is one of the error codes described in Error Reason Codes. If the actions taken do not include an authenticate action or the target is not a Lambda function, this value is set to -. Cloudfront Source: logtype = 'cloudfront-web' Grok: ^%{NOTSPACE:date}%{SPACE}%{NOTSPACE:time}%{SPACE}%{NOTSPACE:x_edge_location}%{SPACE}%{NOTSPACE:sc_bytes}%{SPACE}%{NOTSPACE:c_ip}%{SPACE}%{NOTSPACE:cs_method}%{SPACE}%{NOTSPACE:cs_host}%{SPACE}%{NOTSPACE:cs_uri_stem}%{SPACE}%{NOTSPACE:sc_status}%{SPACE}%{NOTSPACE:cs_referer}%{SPACE}%{NOTSPACE:cs_user_agent}%{SPACE}%{NOTSPACE:cs_uri_query}%{SPACE}%{NOTSPACE:cs_Cookie}%{SPACE}%{NOTSPACE:x_edge_result_type}%{SPACE}%{NOTSPACE:x_edge_request_id}%{SPACE}%{NOTSPACE:x_host_header}%{SPACE}%{NOTSPACE:cs_protocol}%{SPACE}%{NOTSPACE:cs_bytes}%{SPACE}%{NOTSPACE:time_taken}%{SPACE}%{NOTSPACE:x_forwarded_for}%{SPACE}%{NOTSPACE:ssl_protocol}%{SPACE}%{NOTSPACE:ssl_cipher}%{SPACE}%{NOTSPACE:x_edge_response_result_type}%{SPACE}%{NOTSPACE:cs_protocol_version}%{SPACE}%{NOTSPACE:fle_status}%{SPACE}%{NOTSPACE:fle_encrypted_fields}%{SPACE}%{NOTSPACE:c_port}%{SPACE}%{NOTSPACE:time_to_first_byte}%{SPACE}%{NOTSPACE:x_edge_detailed_result_type}%{SPACE}%{NOTSPACE:sc_content_type}%{SPACE}%{NOTSPACE:sc_content_len}%{SPACE}%{NOTSPACE:sc_range_start}%{SPACE}%{NOTSPACE:sc_range_end} Copy Results: Field Definition x_edge_location The edge location that served the request. Each edge location is identified by a three-letter code and an arbitrarily assigned number; for example, DFW3. The three-letter code typically corresponds with the International Air Transport Association airport code for an airport near the edge location. (These abbreviations might change in the future.) sc_bytes The total number of bytes that CloudFront served to the viewer in response to the request, including headers; for example, 1045619. For WebSocket connections, this is the total number of bytes sent from the server to the client through the connection. c_ip The IP address of the viewer that made the request, either in IPv4 or IPv6 format. If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip is the IP address of the proxy or load balancer. See also X-Forwarded-For. cs_method The HTTP request method: DELETE, GET, HEAD, OPTIONS, PATCH, POST, or PUT. cs_host The domain name of the CloudFront distribution; for example, d111111abcdef8.cloudfront.net. cs_uri_stem The portion of the URI that identifies the path and object; for example, /images/cat.jpg. Question marks in URLs and query strings are not included. sc_status An HTTP status code; for example, 200. Status code 000 indicates the viewer closed the connection (for example, closed the browser tab) before CloudFront could respond to a request. If the viewer closes the connection after CloudFront starts to send the response, the log contains the applicable HTTP status code. cs_referer The name of the domain that originated the request. Common referrers include search engines, other websites that link directly to your objects, and your own website. cs_user_agent The value of the User-Agent header in the request. The User-Agent header identifies the source of the request, such as the type of device and browser that submitted the request, and which search engine if applicable. cs_uri_query The query string portion of the URI, if any. When a URI doesn't contain a query string, this field's value is a hyphen (-). cs_cookie The cookie header in the request, including name-value pairs and the associated attributes. If you enable cookie logging, CloudFront logs the cookies in all requests regardless of which cookies you choose to forward to the origin. If a request doesn't include a cookie header, this field's value is a hyphen (-). x_edge_result_type How CloudFront classifies the response after the last byte left the edge location. In some cases, the result type can change between the time that CloudFront is ready to send the response and the time that CloudFront has finished sending the response. x_edge_request_id An encrypted string that uniquely identifies a request. In the response header, this is x-amz-cf-id. x_host_header The value that the viewer included in the Host header for this request. This is the domain name in the request. If you're using the CloudFront domain name in your object URLs, this field contains that domain name. If you're using alternate domain names in your object URLs, such as http://example.com/logo.png, this field contains the alternate domain name, such as example.com. To use alternate domain names, you must add them to your distribution. cs_protocol The protocol that the viewer specified in the request: http, https, ws, or wss. cs_bytes The number of bytes of data that the viewer included in the request, including headers. For WebSocket connections, this is the total number of bytes sent from the client to the server on the connection. time_taken The number of seconds (to the thousandth of a second; for example, 0.002) between the time that a CloudFront edge server receives a viewer's request and the time that CloudFront writes the last byte of the response to the edge server's output queue as measured on the server. From the perspective of the viewer, the total time to get the full object will be longer than this value due to network latency and TCP buffering. x_forwarded_for If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip in field 5 is the IP address of the proxy or load balancer. In that case, this field is the IP address of the viewer that originated the request. This field contains IPv4 and IPv6 addresses as applicable. If the viewer did not use an HTTP proxy or a load balancer, the value of x_forwarded_for is a hyphen (-). ssl_protocol When cs_protocol in field 17 is https, this field contains the SSL/TLS protocol that the client and CloudFront negotiated for transmitting the request and response. Possible values include: SSLv3 TLSv1 TLSv1.1 TLSv1.2 When cs_protocol in field 17 is http, the value for this field is a hyphen (-). ssl_cipher When cs_protocol in field 17 is https, this field contains the SSL/TLS cipher that the client and CloudFront negotiated for encrypting the request and response. Possible values include: ECDHE-RSA-AES128-GCM-SHA256 ECDHE-RSA-AES128-SHA256 ECDHE-RSA-AES128-SHA ECDHE-RSA-AES256-GCM-SHA384 ECDHE-RSA-AES256-SHA384 ECDHE-RSA-AES256-SHA AES128-GCM-SHA256 AES256-GCM-SHA384 AES128-SHA256 AES256-SHA AES128-SHA DES-CBC3-SHA RC4-MD5 When cs_protocol in field 17 is http, the value for this field is a hyphen (-). x_edge_response_result_type How CloudFront classified the response just before returning the response to the viewer. Possible values include: Hit: CloudFront served the object to the viewer from the edge cache. RefreshHit: CloudFront found the object in the edge cache but it had expired, so CloudFront contacted the origin to verify that the cache has the latest version of the object. Miss: The request could not be satisfied by an object in the edge cache, so CloudFront forwarded the request to the origin server and returned the result to the viewer. LimitExceeded: The request was denied because a CloudFront limit was exceeded. CapacityExceeded: CloudFront returned a 503 error because the edge location didn't have enough capacity at the time of the request to serve the object. Error: Typically this means the request resulted in a client error (sc_status is 4xx) or a server error (sc_status is 5xx). If the value of x_edge_result_type is Error and the value of this field is not Error, the client disconnected before finishing the download. Redirect: CloudFront redirects from HTTP to HTTPS. If sc_status is 403 and you configured CloudFront to restrict the geographic distribution of your content, the request might have come from a restricted location. cs_protocol_version The HTTP version that the viewer specified in the request. Possible values include: HTTP/0.9 HTTP/1.0 HTTP/1.1 HTTP/2.0 fle_status When field-level encryption is configured for a distribution, this field contains a code that indicates whether the request body was successfully processed. If field-level encryption is not configured for the distribution, the value is a hyphen (-). fle-encrypted-fields The number of fields that CloudFront encrypted and forwarded to the origin. CloudFront streams the processed request to the origin as it encrypts data, so fle-encrypted-fields can have a value even if the value of fle-status is an error. If field-level encryption is not configured for the distribution, the value of fle-encrypted-fields is a hyphen (-). c_port The port number of the request from the viewer. time_to_first_byte The number of seconds between receiving the request and writing the first byte of the response, as measured on the server. x_edge_detailed_result_type When x-edge-result-type is not Error, this field contains the same value as x-edge-result-type. When x-edge-result-type is Error, this field contains the specific type of error. sc_content_type The value of the HTTP Content-Type header of the response. sc_content_len The value of the HTTP Content-Length header of the response. sc_range_start When the response contains the HTTP Content-Range header, this field contains the range start value. sc-range-end When the response contains the HTTP Content-Range header, this field contains the range end value. Elastic Load Balancer Source: logtype = 'elb' Grok: ^%{TIMESTAMP_ISO8601:time} %{NOTSPACE:elb} %{NOTSPACE:client_ip}:%{NOTSPACE:client_port} ((%{NOTSPACE:backend_ip}:%{NOTSPACE:backend_port})|-) %{NOTSPACE:request_processing_time} %{NOTSPACE:backend_processing_time} %{NOTSPACE:response_processing_time} %{NOTSPACE:elb_status_code} %{NOTSPACE:backend_status_code} %{NOTSPACE:received_bytes} %{NOTSPACE:sent_bytes} \"%{DATA:request}\" \"%{DATA:user_agent}\" %{NOTSPACE:ssl_cipher} %{NOTSPACE:ssl_protocol} Copy Results: Field Definition x_edge_location The edge location that served the request. Each edge location is identified by a three-letter code and an arbitrarily assigned number; for example, DFW3. The three-letter code typically corresponds with the International Air Transport Association airport code for an airport near the edge location. (These abbreviations might change in the future.) sc_bytes The total number of bytes that CloudFront served to the viewer in response to the request, including headers; for example, 1045619. For WebSocket connections, this is the total number of bytes sent from the server to the client through the connection. c_ip The IP address of the viewer that made the request. If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip is the IP address of the proxy or load balancer. cs_method The HTTP request method: DELETE, GET, HEAD, OPTIONS, PATCH, POST, or PUT. cs_host The domain name of the CloudFront distribution; for example, d111111abcdef8.cloudfront.net. cs_uri_stem The portion of the URI that identifies the path and object; for example, /images/cat.jpg. Question marks (?) in URLs and query strings are not included in the log. sc_status An HTTP status code (for example, 200). Status code 000 indicates that the viewer closed the connection (for example, closed the browser tab) before CloudFront could respond to a request. If the viewer closes the connection after CloudFront starts to send the response, the log contains the applicable HTTP status code. cs_referer he name of the domain that originated the request. Common referrers include search engines, other websites that link directly to your objects, and your own website. cs_user_agent The value of the User-Agent header in the request. The User-Agent header identifies the source of the request, such as the type of device and browser that submitted the request and which search engine if applicable. cs_uri_query The query string portion of the URI, if any. When a URI doesn't contain a query string, this field's value is a hyphen (-). cs_cookie The cookie header in the request, including name-value pairs and the associated attributes. If you enable cookie logging, CloudFront logs the cookies in all requests, regardless of which cookies you choose to forward to the origin. If a request doesn't include a cookie header, this field's value is a hyphen (-). x_edge_result_type How CloudFront classifies the response after the last byte left the edge location. In some cases, the result type can change between the time that CloudFront is ready to send the response and the time that CloudFront has finished sending the response. x_edge_request_id An encrypted string that uniquely identifies a request. In the response header, this is x-amz-cf-id. x_host_header The value that the viewer included in the Host header for this request. This is the domain name in the request. If you're using the CloudFront domain name in your object URLs, this field contains that domain name. If you're using alternate domain names in your object URLs, such as [http://example.com/logo.png, this field contains the alternate domain name, such as example.com. To use alternate domain names, you must add them to your distribution. cs_protocol The protocol that the viewer specified in the request: http, https, ws, or wss. cs_bytes The number of bytes of data that the viewer included in the request, including headers. For WebSocket connections, this is the total number of bytes sent from the client to the server on the connection. time_taken The number of seconds (to the thousandth of a second; for example, 0.002) between the time that a CloudFront edge server receives a viewer's request and the time that CloudFront writes the last byte of the response to the edge server's output queue as measured on the server. From the perspective of the viewer, the total time to get the full object will be longer than this value due to network latency and TCP buffering. x_forwarded_for If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip in field 5 is the IP address of the proxy or load balancer. In that case, this field is the IP address of the viewer that originated the request. This field contains IPv4 and IPv6 addresses as applicable. If the viewer did not use an HTTP proxy or a load balancer, the value of x_forwarded_for is a hyphen (-). ssl_protocol When cs_protocol in field 17 is https, this field contains the SSL/TLS protocol that the client and CloudFront negotiated for transmitting the request and response. Possible values include: SSLv3 TLSv1 TLSv1.1 TLSv1.2 When cs_protocol in field 17 is http, the value for this field is a hyphen (-). ssl_cipher When cs_protocol in field 17 is https, this field contains the SSL/TLS cipher that the client and CloudFront negotiated for encrypting the request and response. Possible values include: ECDHE-RSA-AES128-GCM-SHA256 ECDHE-RSA-AES128-SHA256 ECDHE-RSA-AES128-SHA ECDHE-RSA-AES256-GCM-SHA384 ECDHE-RSA-AES256-SHA384 ECDHE-RSA-AES256-SHA AES128-GCM-SHA256 AES256-GCM-SHA384 AES128-SHA256 AES256-SHA AES128-SHA DES-CBC3-SHA RC4-MD5 When cs_protocol is http, the value for this field is a hyphen (-). x_edge_response_result_type How CloudFront classified the response just before returning the response to the viewer. Possible values include: Hit: CloudFront served the object to the viewer from the edge cache. RefreshHit: CloudFront found the object in the edge cache but it had expired, so CloudFront contacted the origin to verify that the cache has the latest version of the object. Miss: The request could not be satisfied by an object in the edge cache, so CloudFront forwarded the request to the origin server and returned the result to the viewer. LimitExceeded: The request was denied because a CloudFront limit was exceeded. CapacityExceeded: CloudFront returned a 503 error because the edge location didn't have enough capacity at the time of the request to serve the object. Error: Typically this means the request resulted in a client error (sc_status is 4xx) or a server error (sc_status is 5xx). If the value of x_edge_result_type is Error and the value of this field is not Error, the client disconnected before finishing the download. Redirect: CloudFront redirects from HTTP to HTTPS. If sc_status is 403 and you configured CloudFront to restrict the geographic distribution of your content, the request might have come from a restricted location. cs_protocol_version The HTTP version that the viewer specified in the request. Possible values include: HTTP/0.9 HTTP/1.0 HTTP/1.1 HTTP/2.0 fle_status When field-level encryption is configured for a distribution, this field contains a code that indicates whether the request body was successfully processed. If field-level encryption is not configured for the distribution, the value of this field is a hyphen (-). When CloudFront successfully processes the request body, encrypts values in the specified fields, and forwards the request to the origin, the value of this field is Processed. The value of x_edge_result_type can still indicate a client-side or server-side error in this case. If the request exceeds a field-level encryption limit, fle-status contains one of the following error codes, and CloudFront returns HTTP status code 400 to the viewer. fle-encrypted-fields The number of fields that CloudFront encrypted and forwarded to the origin. CloudFront streams the processed request to the origin as it encrypts data, so fle_encrypted_fields can have a value even if the value of fle_status is an error. If field-level encryption is not configured for the distribution, the value of fle_encrypted_fields is a hyphen (-). c_port The port number of the request from the viewer. time_to_first_byte The number of seconds between receiving the request and writing the first byte of the response, as measured on the server. x_edge_detailed_result_type When x_edge_result_type is not Error, this field contains the same value as x_edge_result_type. When x_edge_result_type is Error, this field contains the specific type of error. sc_content_type The value of the HTTP Content-Type header of the response. sc_content_len The value of the HTTP Content-Length header of the response. sc_range_start When the response contains the HTTP Content-Range header, this field contains the range start value. sc-range-end When the response contains the HTTP Content-Range header, this field contains the range end value. Microsoft IIS Source: logtype = 'iis_w3c' Grok: %{TIMESTAMP_ISO8601:log_timestamp} %{NOTSPACE:server_ip} %{WORD:method} %{NOTSPACE:uri} %{NOTSPACE:uri_query} %{NOTSPACE:server_port} %{NOTSPACE:username} %{NOTSPACE:client_ip} %{NOTSPACE:user_agent} %{NOTSPACE:referer} %{NOTSPACE:status} %{NOTSPACE:substatus} %{NOTSPACE:win32_status} %{NOTSPACE:time_taken} Copy Monit Source: logtype = 'monit' Grok: \\\\[%{NOTSPACE:tz} %{SYSLOGTIMESTAMP:nr_timestamp}\\\\] %{WORD:state}%{SPACE}: %{GREEDYDATA:message} Copy Results: state: The severity of the log line message: The message MySQL Error Source: logtype = 'mysql-error' Grok: \\\\[%{WORD:log_level}\\\\] Copy Results: log_level: The severity of the log line NGINX Source: logtype = 'nginx' Grok: %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{NUMBER:bytes}|-) %{QS:referrer} %{QS:agent} Copy Results: clientip: The IP address of the client verb: The HTTP verb ident: The user identity of the client making the request response: The HTTP status code of the response request: The URI and request being made httpversion: The HTTP version of the request rawrequest: The raw HTTP request if data is posted bytes: The number of bytes sent referrer: The HTTP referrer agent: The client's user agent NGINX Error Source: logtype = 'nginx-error' Grok: ^(?<timestamp>%{YEAR:year}[./-]%{MONTHNUM:month}[./-]%{MONTHDAY:day}[- ]%{TIME:time}) \\\\[%{LOGLEVEL:severity}\\\\] %{POSINT:pid}#%{NUMBER}: %{GREEDYDATA:errormessage}(?:, client: (?<client>%{IP:clientip}|%{HOSTNAME:hostname}))(?:, server: %{IPORHOSTORUNDERSCORE:server})(?:, request: %{QS:request})?(?:, upstream: \\\"%{URI:upstream}\\\")?(?:, host: %{QS:host})?(?:, referrer: \\\"%{URI:referrer}\\\")?$ Copy Results: severity: The severity of the log line pid: The server process ID errormessage: The error message clientip: The IP address of the calling client server: The server IP address request: The full request upstream: The upstream URI host: The server's hostname referrer: The HTTP referrer Route 53 Source: logtype = 'route-53' Grok: %{NUMBER:log_format_version} %{TIMESTAMP_ISO8601} %{WORD:zone_id} %{IPORHOST:query} %{WORD:query_type} %{WORD:response_code} %{WORD:protocol} %{WORD:edge_location} %{IP:resolver_ip} %{GREEDYDATA:edns_client_subnet} Copy Results: log_format_version: A versioned format for the log. zone_id: The ID of the hosted zone that is associated with all the DNS queries in this log. query: The domain or subdomain that was specified in the request. query_type: Either the DNS record type that was specified in the request, or ANY. response_code: The DNS response code that Route 53 returned in response to the DNS query. protocol: The protocol that was used to submit the query, either TCP or UDP. edge_location: The Route 53 edge location that responded to the query. Each edge location is identified by a three-letter code and an arbitrary number; for example, DFW3. The three-letter code typically corresponds with the International Air Transport Association airport code for an airport near the edge location. (These abbreviations might change in the future.) resolver_ip: The IP address of the DNS resolver that submitted the request to Route 53. edns_client_subnet: A partial IP address for the client that the request originated from, if available from the DNS resolver. Syslog RFC-5424 Source: logtype = 'syslog-rfc5424' Grok: <%{NONNEGINT:pri}>%{NONNEGINT:version} +(?:%{TIMESTAMP_ISO8601:log.timestamp}|-) +(?:%{HOSTNAME:hostname}|-) +(?:\\\\-|%{NOTSPACE:app.name}) +(?:\\\\-|%{NOTSPACE:procid}) (?:\\\\-|%{NOTSPACE:msgid}) +(?:\\[%{DATA:structured.data}\\]|-|) +%{GREEDYDATA:message} Copy Results: pri: The priority represents both the message facility and severity. version: Syslog protocol version. log.timestamp: Original timestamp. hostname: The machine that originally sent the Syslog message. app.name: The device or application that originated the message. procid: The process name or process ID associated with a Syslog system. msgid: Identifies the type of message. structured.data: Structured data string value. sd.<var>sd-id</var>.<var>sd-param-name</var>: The structured.data content is also parsed into separate attributes following a predefined naming convention: sd.<var>sd-id</var>.<var>sd-param-name</var>. See the structured data parsing examples, which follow. message: Free-form message that provides information about the event. Structured data parsing examples: The structured data [example one=\"1\" two=\"2\"] would be parsed into two different attributes: sd.example.one: \"1\" sd.example.two: \"2\" Copy If the same structured data block contains duplicate param names, it also appends an index-based suffix on the attribute name. For example, the structured data [example number=\"1\" number=\"2\"] would be parsed as: sd.example.number.0: \"1\" sd.example.number.1: \"2\" Copy For structured data with enterprise numbers assigned, an extra attribute is also parsed. For example, the structured data [example@123 number=\"1\"] would be parsed as: sd.example.enterprise.number: 123 sd.example.number: \"1\" Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 191.93431,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Built-in <em>log</em> parsing rulesets",
        "sections": "Built-in <em>log</em> parsing rulesets",
        "tags": "<em>Log</em> <em>management</em>",
        "body": "New Relic can parse common <em>log</em> formats according to built-in rulesets, so that you don&#x27;t have to create your own parsing rules. Here are the <em>log</em> parsing rulesets, their Grok patterns, and what fields are parsed. To enable built-in <em>log</em> parsing, see our documentation for adding the logtype attribute"
      },
      "id": "603e7b9164441f1b2d4e8872"
    }
  ],
  "/docs/logs/log-management/log-api/log-event-data": [
    {
      "sections": [
        "Discover value in log data with patterns",
        "Technical overview",
        "Availability",
        "Get started",
        "Explore log patterns",
        "Explore logs with no pattern",
        "Masked attributes and wildcards",
        "Troubleshooting",
        "Put the platform to work with patterns"
      ],
      "title": "Discover value in log data with patterns",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "8f1e27c94327ca4888a945f8e12f9c2310ccd7a6",
      "image": "https://docs.newrelic.com/static/578d7186bb34352855696e5307cc82f2/c1b63/log-patterns-logs-without-a-pattern.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/find-unusual-logs-log-patterns/",
      "published_at": "2021-10-19T03:56:24Z",
      "updated_at": "2021-10-19T03:56:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Log patterns are the fastest way to discover value in log data without searching. Log data is high volume telemetry with a low value per individual record. Searching can quickly lead to logs that provide a root cause explanation, but most data is repetitive and hard to contextualize when browsing. Patterns can make log data discoverable without spending a lot of time reading through low value data. Logs > Patterns: Use patterns as the basis for alerts when the frequency of important data changes, or for configuring drop rules to get rid of unnecessary repetitive data. Technical overview Log patterns functionality applies machine learning to normalize and group log messages that are consistent in format but variable in content. These grouped messages can be sorted, making it easy to find the most frequent or rarest sets of logs in your environment. Use patterns as the basis for alerts when the frequency of important data changes, or to configure drop rules to get rid of unnecessary repetitive data. Log patterns use advanced clustering algorithms to group together similar log messages automatically. With patterns, you can: Orient more quickly through millions of logs. Reduce the time it takes to identify unusual behavior in your log estate. Monitor the frequency of known patterns over time to focus your energy on what matters, and exclude what's irrelevant. Availability The ability to configure this feature is dependent on role-based permissions. For example, accounts still using our original pricing model require an Owner or Admin role. If you see Patterns are turned off in your Log management Patterns UI, click the Configure Patterns button and enable it. If you don't see patterns within 30 minutes of enabling the feature, there may be a lack of data with a message attribute for the system to create a pattern from it. Log patterns Limitations and considerations Pricing There is no separate pricing for log patterns. The only cost is for additional data generated and added to your log records. A pattern attribute will be added to all logs that match a pattern. Attributes also may be added when common values are discovered, such as GUIDs, IP addresses, URL, or email addresses. These attributes are automatically extracted from the log message as part of the pattern process. HITRUST accounts The log patterns feature is not FedRAMP compliant. FedRAMP or other HITRUST accounts are not eligible to use patterns. Parsing limits We have a system of safety limits on memory and CPU resources when processing logs and their patterns. These parsing limits can have an impact on the data you get. For more information, see our documentation about parsing limits. Get started To start examining patterns: Go to one.newrelic.com > Logs, and use the account picker dropdown to select the target account where you want to explore patterns. In the left navigation of the Logs UI, click Patterns. The main log UI changes to show patterns that match the query in the query bar. Logs > Log patterns: The line chart shows the top 5 patterns over time. Use the time picker and query bar to adjust the results. Explore log patterns By default the log patterns UI first shows the most frequent occurrence of patterns. To sort to show the rarest patterns first, click the Count column. You can also use the query bar or attributes bar to filter your log patterns. If you want to... Do this... Understand the rate of change in patterns Look at the line chart. The color-coded patterns correspond to the plot column in the table. You can toggle individual plot patterns to narrow your focus. See the individual log messages that match each pattern Click pattern to expand the row and see a table of individual log records. To see additional records, scroll up or down. To explore an individual log in more detail, click it to open the details panel. Group and filter patterns by their attributes Use the query bar and time picker. As you apply different filters and time windows, the log patterns adjust to your new target data. Create an alert from a pattern Add the pattern to the query bar and run the query. Then click Create alert condition in the left nav. Troubleshoot log messages that haven't been clustered into a pattern Use the Logs with no pattern tab in the Log patterns UI. Clicking a specific log message will open the log message details panel you're familiar with from the Logs management page. Explore logs with no pattern The Logs with no pattern tab groups all recent log messages in your account that were not clustered into a known pattern yet. These log messages don't represent any problem or flaw in the system; they have no pattern because they are too new to have been processed by the machine learning system. This makes them valuable to explore when you want to understand what has recently changed in your environment. Logs > Log patterns: New Relic's log patterns feature automatically groups logs without a matching pattern. For example: Are any of these logs tied to a recent problem? This is a quick way to discover unique log data that is appearing for the first time in your environment. Does your log data have a new format? Sometimes the logs don't represent a problem, but a new format of log data that deviates from the data model you expect your applications to follow. Catching these logs early gives you the opportunity to ask developers to correct any deviations in their log output. The more consistent people are in the way log data is generated, the easier it becomes to use logs across a diverse set of teams. Masked attributes and wildcards Parts of the log messages in patterns are classified as variables and are substituted by masked attributes. The masking process supports and improves the clustering phase by allowing the algorithm to ignore changing details and focus on the repetitive structure. Masked attributes include: date_time ip url uuid Masked attributes are highlighted and are easy to identify, as shown in the following example. Here is an example of a log pattern that has masked attributes. Log patterns extract other less trivial variables that don't belong to any masked attribute. These variables are indicated as wildcards *. Here is an example of how wildcards * group variables in the Log patterns UI. Troubleshooting Here are a few reasons why you might have patterns enabled but not see any pattern data. If you're sure none of the items below are true, get help from support.newrelic.com. No data has arrived in the timeframe you're observing. Try expanding the time range you're viewing with the time picker. It's been less than 24 hours since patterns were enabled in the account. This means the ML model may not be generated for the account yet. None of the data coming in has a message field. Patterns will only be generated for values in the message field of a log record. If your logs don't contain message, there will be no data. Put the platform to work with patterns Patterns are a value that is enriched onto the existing log message as a new attribute named newrelic.logPattern. Anything you can do with logs generally can be done with log patterns, such as: Build your own dashboards with patterns, to monitor a specific pattern or group of patterns you care about. Create alerts for patterns by adding NRQL alerts. Use baseline alert conditions to detect anomalies in known log patterns.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 345.17786,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Discover value in <em>log</em> <em>data</em> with patterns",
        "sections": "Discover value in <em>log</em> <em>data</em> with patterns",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " using our original pricing model require an Owner or Admin role. If you see Patterns are turned off in your <em>Log</em> <em>management</em> Patterns <em>UI</em>, click the Configure Patterns button and enable it. If you don&#x27;t see patterns within 30 minutes of enabling the feature, there may be a lack of <em>data</em> with a message"
      },
      "id": "6072d46128ccbc244451c18b"
    },
    {
      "sections": [
        "Parsing log data",
        "Example",
        "How log parsing works",
        "Important",
        "Limits",
        "Tip",
        "Built-in parsing rulesets",
        "List of built-in rulesets",
        "Add the logtype attribute",
        "New Relic infrastructure agent example",
        "Fluentd example",
        "Fluent Bit example",
        "Logstash example",
        "Logs API example",
        "Create custom parsing rules",
        "Troubleshooting"
      ],
      "title": "Parsing log data",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "52955adb68242c4ca582ba9cb8e22963955a8275",
      "image": "https://docs.newrelic.com/static/dc392bb7142d2fdb253a649daf4ebe6d/c1b63/log-parsing-rule-ui.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/parsing/",
      "published_at": "2021-10-19T03:43:56Z",
      "updated_at": "2021-10-19T03:43:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Parsing is the process of splitting unstructured log data into attributes (key/value pairs). You can use these attributes to facet or filter logs in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial on YouTube (approx. 4-1/2 minutes). New Relic parses log data according to rules. This document describes how logs parsing works, how to use built-in rules, and how to create custom rules. You can also create, query, and manage your log parsing rules by using NerdGraph, our GraphQL API, at api.newrelic.com/graphiql. For more information, see our NerdGraph tutorial for parsing. Example A good example is a default NGINX access log containing unstructured text. It is useful for searching but not much else. Here's an example of a typical line: 127.180.71.3 - - [10/May/1997:08:05:32 +0000] \"GET /downloads/product_1 HTTP/1.1\" 304 0 \"-\" \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" Copy In an unparsed format, you would need to do a full text search to answer most questions. After parsing, the log is organized into attributes, like response code and request URL: { \"remote_addr\":\"93.180.71.3\", \"time\":\"1586514731\", \"method\":\"GET\", \"path\":\"/downloads/product_1\", \"version\":\"HTTP/1.1\", \"response\":\"304\", \"bytesSent\": 0, \"user_agent\": \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" } Copy Parsing makes it easier to create custom queries that facet on those values. This helps you understand the distribution of response codes per request URL and quickly find problematic pages. How log parsing works Here's an overview of how New Relic implements parsing of logs: Log parsing How it works What All parsing takes place against the message field; no other fields can be parsed. Each parsing rule is created with matching criteria that determines which logs the rule will attempt to parse. To simplify the matching process, we recommend adding a logtype attribute to your logs. However, you are not limited to using logtype; any attribute can be used as matching criteria. When Parsing will only be applied once to each log message. If multiple parsing rules match the log, only the first that succeeds will be applied. Parsing takes place during log ingestion, before data is written to NRDB. Once data has been written to storage, it can no longer be parsed. Parsing occurs in the pipeline before data enrichments take place. Be careful when defining the matching criteria for a parsing rule. If the criteria is based on an attribute that doesn't exist untail after parsing or enrichment take place, that data won't be present in the logs when matching occurs. As a result, no parsing will happen. How Rules can be written in Grok, regex, or a mixture of the two. Grok is a collection of patterns that abstract away complicated regular expressions. If the content of the message field is JSON, it will be parsed automatically. New Relic's log ingestion pipeline can parse data by matching a log event to a rule that describes how the log should be parsed. There are two ways log events can be parsed: Use a built-in rule. Define a custom rule. Rules are a combination of matching logic and parsing logic. Matching is done by defining a query match on an attribute of the logs. Rules are not applied retroactively. Logs collected before a rule is created are not parsed by that rule. The simplest way to organize your logs and how they are parsed is to include the logtype field in your log event. This tells New Relic what built-in ruleset to apply to the logs. Important Once a parsing rule is active, data parsed by the rule is permanently changed. This cannot be reverted. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. Limits Parsing is computationally expensive, which introduces risk. Parsing is done for custom rules defined in an account and for matching patterns to a log. A large number of patterns or poorly defined custom rules will consume a huge amount of memory and CPU resources while also taking a very long time to complete. In order to prevent problems, we apply two parsing limits: per-message-per-rule and per-account. Limit Description Per-message-per-rule The per-message-per-rule limit prevents the time spent parsing any single message from being greater than 100 ms. If that limit is reached, the system will cease attempting to parse the log message with that rule. The ingestion pipeline will attempt to run any other applicable on that message, and the message will still be passed through the ingestion pipeline and stored in NRDB. The log message will be in its original, unparsed format. Per-account The per-account limit exists to prevent accounts from using more than their fair share of resources. The limit considers the total time spent processing all log messages for an account per-minute. The limit is not a fixed value; it scales up or down proportionally to the volume of data stored daily by the account and the environment size that is subsequently allocated to support that customer. Tip To easily check if your rate limits have been reached, go to your system Limits page in the New Relic UI. Built-in parsing rulesets Common log formats have well-established parsing rules already created for them. To get the benefit of built-in parsing rules, add the logtype attribute when forwarding logs. Set the value to something listed in the following table, and the rules for that type of log will be applied automatically. List of built-in rulesets The following logtype attribute values map to a predefined parsing rule. For example, to query the Application Load Balancer: From the New Relic UI, use the format logtype: alb. From NerdGraph, use the format logtype = 'alb'. To learn what fields are parsed for each rule, see our documentation about built-in parsing rules. logtype Example matching query alb AWS Application Load Balancer logtype:alb apache Apache Access logtype:apache cloudfront-web CloudFront Web logtype:cloudfront-web elb Amazon Elastic Load Balancer logtype:elb iis_w3c Microsoft IIS server logs - W3C format logtype:iis_w3c monit Monit logs logtype:monit mysql-error MySQL Error logtype:mysql-error nginx NGINX access logs logtype:nginx nginx-error NGINX error logs logtype:nginx-error route-53 Amazon Route 53 logs logtype:route-53 syslog-rfc5424 Syslog logtype:syslog-rfc5424 Add the logtype attribute When aggregating logs, it's important to provide metadata that makes it easy to organize, search, and parse those logs. One simple way of doing this is to add the attribute logtype to the log messages when they are shipped. Built-in parsing rules are applied by default to certain logtype values. Here are some examples of how to add logtype to logs sent by some of our supported shipping methods. New Relic infrastructure agent example Add logtype as an attribute. You must set the logtype for each named source. Learn more about using the infrastructure agent to add attributes. logs: - name: file-simple file: /path/to/file attributes: logtype: fileRaw - name: nginx-example file: /var/log/nginx.log attributes: logtype: nginx Copy Fluentd example Add a filter block to the .conf file, which uses a record_transformer to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluentd examples. <filter containers> @type record_transformer enable_ruby true <record> #Add logtype to trigger a built-in parsing rule for nginx access logs logtype nginx #Set timestamp from the value contained in the field \"time\" timestamp record[\"time\"] #Add hostname and tag fields to all records hostname \"#{Socket.gethostname}\" tag ${tag} </record> </filter> Copy Fluent Bit example Add a filter block to the .conf file that uses a record_modifier to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluent Bit examples. [FILTER] Name record_modifier Match * Record logtype nginx Record hostname ${HOSTNAME} Record service_name Sample-App-Name Copy Logstash example Add a filter block to the Logstash configuration which uses an add_field mutate filter to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Logstash examples. filter { mutate { add_field => { \"logtype\" => \"nginx\" \"service_name\" => \"myservicename\" \"hostname\" => \"%{host}\" } } } Copy Logs API example You can add attributes to the JSON request sent to New Relic. In this example we add a logtype attribute of value nginx to trigger the built-in NGINX parsing rule. Learn more about using the Logs API. POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: YOUR_LICENSE_KEY Accept: */* Content-Length: 133 { \"timestamp\": TIMESTAMP_IN_UNIX_EPOCH, \"message\": \"User 'xyz' logged in\", \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } Copy Create custom parsing rules Many logs are formatted or structured in a unique way. In order to parse them, custom logic must be built and applied. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. To create and manage your own, custom parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing, then click Create parsing rule. Enter the parsing rule's name. Choose an attribute and value to match on. Write your Grok pattern and test the rule. To learn about Grok and custom parsing rules, read our blog post about how to parse logs with Grok patterns. Enable and save the custom parsing rule. To view the list of custom parsing rules: From Manage Data on the left nav of the Logs UI, click Parsing. To view existing parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing. Troubleshooting If parsing is not working the way you intended, it may be due to: Logic: The parsing rule matching logic does not match the logs you want. Timing: If your parsing matching rule targets a value that doesn't exist yet, it will fail. This can occur if the value is added later in the pipeline as part of the enrichment process. Limits: There is a fixed amount of time available every minute to process logs via parsing, patterns, drop filters, etc. If the maximum amount of time has been spent, parsing will be skipped for additional log event records. To resolve these problems, create or adjust your custom parsing rules.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 345.00037,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Parsing <em>log</em> <em>data</em>",
        "sections": "Parsing <em>log</em> <em>data</em>",
        "tags": "<em>Log</em> <em>management</em>",
        "body": "Parsing is the process of splitting unstructured <em>log</em> <em>data</em> into attributes (key&#x2F;value pairs). You can use these attributes to facet or filter <em>logs</em> in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial"
      },
      "id": "603e7eb4196a67b0c4a83dd1"
    },
    {
      "sections": [
        "Built-in log parsing rulesets",
        "Apache",
        "Application Load Balancer",
        "Cloudfront",
        "Elastic Load Balancer",
        "Microsoft IIS",
        "Monit",
        "MySQL Error",
        "NGINX",
        "NGINX Error",
        "Route 53",
        "Syslog RFC-5424"
      ],
      "title": "Built-in log parsing rulesets",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "cb5909f2453d475a85d408d75cd3b2a321a8518e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/built-log-parsing-rulesets/",
      "published_at": "2021-10-19T03:43:56Z",
      "updated_at": "2021-10-19T03:43:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic can parse common log formats according to built-in rulesets, so that you don't have to create your own parsing rules. Here are the log parsing rulesets, their Grok patterns, and what fields are parsed. To enable built-in log parsing, see our documentation for adding the logtype attribute. To manage your parsing rulesets programmatically, you can use NerdGraph, our graphQL-format API, at api.newrelic.com/graphiql. For more information, see the NerdGraph tutorial to create, query, and delete your parsing rulesets. Apache Source: logtype = 'apache' Grok: %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{NUMBER:bytes}|-) %{QS:referrer} %{QS:agent} Copy Results: clientip: The IP address of the client. verb: The HTTP verb ident: The user identity of the client making the request response: The HTTP status code of the response request: The URI and request being made httpversion: The HTTP version of the request rawrequest: The raw HTTP request if data is posted bytes: The number of bytes sent referrer: The HTTP referrer agent: The client's user agent Application Load Balancer Source: logtype = 'alb' Grok: ^%{NOTSPACE:type} %{TIMESTAMP_ISO8601:time} %{NOTSPACE:elb} %{NOTSPACE:client_ip}:%{NOTSPACE:client_port} ((%{NOTSPACE:target_ip}:%{NOTSPACE:target_port})|-) %{NOTSPACE:request_processing_time} %{NOTSPACE:target_processing_time} %{NOTSPACE:response_processing_time} %{NOTSPACE:elb_status_code} %{NOTSPACE:target_status_code} %{NOTSPACE:received_bytes} %{NOTSPACE:sent_bytes} \"%{DATA:request}\" \"%{DATA:user_agent}\" %{NOTSPACE:ssl_cipher} %{NOTSPACE:ssl_protocol} %{NOTSPACE:target_group_arn} \"%{DATA:trace_id}\" \"%{NOTSPACE:domain_name}\" \"%{NOTSPACE:chosen_cert_arn}\" %{NOTSPACE:matched_rule_priority} %{TIMESTAMP_ISO8601:request_creation_time} \"%{NOTSPACE:actions_executed}\" \"%{NOTSPACE:redirect_url}\" \"%{NOTSPACE:error_reason}\" (?:\"|)%{DATA:target_port_list}(?:\"|) (?:\"|)%{DATA:target_status_code_list}(?:\"|) \"%{NOTSPACE:classification}\" \"%{NOTSPACE:classification_reason}\" Copy Results: Field Definition type The type of request or connection. Possible values are: http: HTTP https: HTTP over SSL/TLS h2: HTTP/2 over SSL/TLS ws: WebSockets wss: WebSockets over SSL/TLS elb The resource ID of the load balancer. If you are parsing access log entries, note that resources IDs can contain forward slashes (/). client The IP address and port of the requesting client target The IP address and port of the target that processed this request. If the client didn't send a full request, the load balancer can't dispatch the request to a target, and this value is set to -. If the target is a Lambda function, this value is set to -. If the request is blocked by AWS WAF, this value is set to -, and the value of elb_status_code is set to 403. request_processing_time The total time elapsed (in seconds, with millisecond precision) from the time the load balancer received the request until the time it sent it to a target. This value is set to -1 if the load balancer can't dispatch the request to a target. This can happen if the target closes the connection before the idle timeout or if the client sends a malformed request. This value can also be set to -1 if the registered target does not respond before the idle timeout. target_processing_time The total time elapsed (in seconds, with millisecond precision) from the time the load balancer sent the request to a target until the target started to send the response headers. This value is set to -1 if the load balancer can't dispatch the request to a target. This can happen if the target closes the connection before the idle timeout or if the client sends a malformed request. This value can also be set to -1 if the registered target does not respond before the idle timeout. response_processing_time The total time elapsed (in seconds, with millisecond precision) from the time the load balancer received the response header from the target until it started to send the response to the client. This includes both the queuing time at the load balancer and the connection acquisition time from the load balancer to the client. This value is set to -1 if the load balancer can't send the request to a target. This can happen if the target closes the connection before the idle timeout or if the client sends a malformed request. elb_status_code The status code of the response from the load balancer target_status_code The status code of the response from the target. This value is recorded only if a connection was established to the target and the target sent a response. Otherwise, it is set to -. received_bytes The size of the request, in bytes, received from the client (requester). For HTTP requests, this includes the headers. For WebSockets, this is the total number of bytes received from the client on the connection. sent_bytes The size of the response, in bytes, sent to the client (requester). For HTTP requests, this includes the headers. For WebSockets, this is the total number of bytes sent to the client on the connection. method The HTTP verb of the request uri The URI the request was targeting http_version The HTTP version number of the request user_agent User-Agent string that identifies the client that originated the request, enclosed in double quotes. The string consists of one or more product identifiers, product/version. If the string is longer than 8 KB, it is truncated. ssl_cipher The SSL cipher. This value is set to - if the listener is not an HTTPS listener. ssl_protocol The SSL protocol. This value is set to - if the listener is not an HTTPS listener. target_group_arn The Amazon Resource Name (ARN) of the target group trace_id The contents of the X-Amzn-Trace-Id header, enclosed in double quotes domain_name The SNI domain provided by the client during the TLS handshake, enclosed in double quotes. This value is set to - if the client doesn't support SNI or the domain doesn't match a certificate and the default certificate is presented to the client. chosen_cert_arn The ARN of the certificate presented to the client, enclosed in double quotes. Set to session-reused if the session is reused. Set to - if the listener is not an HTTPS listener. matched_rule_priority The priority value of the rule that matched the request. If a rule matched, this is a value from 1 to 50000. If no rule matched and the default action was taken, this value is set to 0. If an error occurs during rules evaluation, it is set to -1. For any other error, it is set to -. request_creation_time The time when the load balancer received the request from the client, in ISO 8601 format. actions_executed The actions taken when processing the request, enclosed in double quotes. This value is a comma-separated list that can include the values described in actions_taken. If no action was taken, such as for a malformed request, this value is set to -. redirect_url The URL of the redirect target for the location header of the HTTP response, enclosed in double quotes. If no redirect actions were taken, this value is set to -. error_reason The error reason code, enclosed in double quotes. If the request failed, this is one of the error codes described in Error Reason Codes. If the actions taken do not include an authenticate action or the target is not a Lambda function, this value is set to -. Cloudfront Source: logtype = 'cloudfront-web' Grok: ^%{NOTSPACE:date}%{SPACE}%{NOTSPACE:time}%{SPACE}%{NOTSPACE:x_edge_location}%{SPACE}%{NOTSPACE:sc_bytes}%{SPACE}%{NOTSPACE:c_ip}%{SPACE}%{NOTSPACE:cs_method}%{SPACE}%{NOTSPACE:cs_host}%{SPACE}%{NOTSPACE:cs_uri_stem}%{SPACE}%{NOTSPACE:sc_status}%{SPACE}%{NOTSPACE:cs_referer}%{SPACE}%{NOTSPACE:cs_user_agent}%{SPACE}%{NOTSPACE:cs_uri_query}%{SPACE}%{NOTSPACE:cs_Cookie}%{SPACE}%{NOTSPACE:x_edge_result_type}%{SPACE}%{NOTSPACE:x_edge_request_id}%{SPACE}%{NOTSPACE:x_host_header}%{SPACE}%{NOTSPACE:cs_protocol}%{SPACE}%{NOTSPACE:cs_bytes}%{SPACE}%{NOTSPACE:time_taken}%{SPACE}%{NOTSPACE:x_forwarded_for}%{SPACE}%{NOTSPACE:ssl_protocol}%{SPACE}%{NOTSPACE:ssl_cipher}%{SPACE}%{NOTSPACE:x_edge_response_result_type}%{SPACE}%{NOTSPACE:cs_protocol_version}%{SPACE}%{NOTSPACE:fle_status}%{SPACE}%{NOTSPACE:fle_encrypted_fields}%{SPACE}%{NOTSPACE:c_port}%{SPACE}%{NOTSPACE:time_to_first_byte}%{SPACE}%{NOTSPACE:x_edge_detailed_result_type}%{SPACE}%{NOTSPACE:sc_content_type}%{SPACE}%{NOTSPACE:sc_content_len}%{SPACE}%{NOTSPACE:sc_range_start}%{SPACE}%{NOTSPACE:sc_range_end} Copy Results: Field Definition x_edge_location The edge location that served the request. Each edge location is identified by a three-letter code and an arbitrarily assigned number; for example, DFW3. The three-letter code typically corresponds with the International Air Transport Association airport code for an airport near the edge location. (These abbreviations might change in the future.) sc_bytes The total number of bytes that CloudFront served to the viewer in response to the request, including headers; for example, 1045619. For WebSocket connections, this is the total number of bytes sent from the server to the client through the connection. c_ip The IP address of the viewer that made the request, either in IPv4 or IPv6 format. If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip is the IP address of the proxy or load balancer. See also X-Forwarded-For. cs_method The HTTP request method: DELETE, GET, HEAD, OPTIONS, PATCH, POST, or PUT. cs_host The domain name of the CloudFront distribution; for example, d111111abcdef8.cloudfront.net. cs_uri_stem The portion of the URI that identifies the path and object; for example, /images/cat.jpg. Question marks in URLs and query strings are not included. sc_status An HTTP status code; for example, 200. Status code 000 indicates the viewer closed the connection (for example, closed the browser tab) before CloudFront could respond to a request. If the viewer closes the connection after CloudFront starts to send the response, the log contains the applicable HTTP status code. cs_referer The name of the domain that originated the request. Common referrers include search engines, other websites that link directly to your objects, and your own website. cs_user_agent The value of the User-Agent header in the request. The User-Agent header identifies the source of the request, such as the type of device and browser that submitted the request, and which search engine if applicable. cs_uri_query The query string portion of the URI, if any. When a URI doesn't contain a query string, this field's value is a hyphen (-). cs_cookie The cookie header in the request, including name-value pairs and the associated attributes. If you enable cookie logging, CloudFront logs the cookies in all requests regardless of which cookies you choose to forward to the origin. If a request doesn't include a cookie header, this field's value is a hyphen (-). x_edge_result_type How CloudFront classifies the response after the last byte left the edge location. In some cases, the result type can change between the time that CloudFront is ready to send the response and the time that CloudFront has finished sending the response. x_edge_request_id An encrypted string that uniquely identifies a request. In the response header, this is x-amz-cf-id. x_host_header The value that the viewer included in the Host header for this request. This is the domain name in the request. If you're using the CloudFront domain name in your object URLs, this field contains that domain name. If you're using alternate domain names in your object URLs, such as http://example.com/logo.png, this field contains the alternate domain name, such as example.com. To use alternate domain names, you must add them to your distribution. cs_protocol The protocol that the viewer specified in the request: http, https, ws, or wss. cs_bytes The number of bytes of data that the viewer included in the request, including headers. For WebSocket connections, this is the total number of bytes sent from the client to the server on the connection. time_taken The number of seconds (to the thousandth of a second; for example, 0.002) between the time that a CloudFront edge server receives a viewer's request and the time that CloudFront writes the last byte of the response to the edge server's output queue as measured on the server. From the perspective of the viewer, the total time to get the full object will be longer than this value due to network latency and TCP buffering. x_forwarded_for If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip in field 5 is the IP address of the proxy or load balancer. In that case, this field is the IP address of the viewer that originated the request. This field contains IPv4 and IPv6 addresses as applicable. If the viewer did not use an HTTP proxy or a load balancer, the value of x_forwarded_for is a hyphen (-). ssl_protocol When cs_protocol in field 17 is https, this field contains the SSL/TLS protocol that the client and CloudFront negotiated for transmitting the request and response. Possible values include: SSLv3 TLSv1 TLSv1.1 TLSv1.2 When cs_protocol in field 17 is http, the value for this field is a hyphen (-). ssl_cipher When cs_protocol in field 17 is https, this field contains the SSL/TLS cipher that the client and CloudFront negotiated for encrypting the request and response. Possible values include: ECDHE-RSA-AES128-GCM-SHA256 ECDHE-RSA-AES128-SHA256 ECDHE-RSA-AES128-SHA ECDHE-RSA-AES256-GCM-SHA384 ECDHE-RSA-AES256-SHA384 ECDHE-RSA-AES256-SHA AES128-GCM-SHA256 AES256-GCM-SHA384 AES128-SHA256 AES256-SHA AES128-SHA DES-CBC3-SHA RC4-MD5 When cs_protocol in field 17 is http, the value for this field is a hyphen (-). x_edge_response_result_type How CloudFront classified the response just before returning the response to the viewer. Possible values include: Hit: CloudFront served the object to the viewer from the edge cache. RefreshHit: CloudFront found the object in the edge cache but it had expired, so CloudFront contacted the origin to verify that the cache has the latest version of the object. Miss: The request could not be satisfied by an object in the edge cache, so CloudFront forwarded the request to the origin server and returned the result to the viewer. LimitExceeded: The request was denied because a CloudFront limit was exceeded. CapacityExceeded: CloudFront returned a 503 error because the edge location didn't have enough capacity at the time of the request to serve the object. Error: Typically this means the request resulted in a client error (sc_status is 4xx) or a server error (sc_status is 5xx). If the value of x_edge_result_type is Error and the value of this field is not Error, the client disconnected before finishing the download. Redirect: CloudFront redirects from HTTP to HTTPS. If sc_status is 403 and you configured CloudFront to restrict the geographic distribution of your content, the request might have come from a restricted location. cs_protocol_version The HTTP version that the viewer specified in the request. Possible values include: HTTP/0.9 HTTP/1.0 HTTP/1.1 HTTP/2.0 fle_status When field-level encryption is configured for a distribution, this field contains a code that indicates whether the request body was successfully processed. If field-level encryption is not configured for the distribution, the value is a hyphen (-). fle-encrypted-fields The number of fields that CloudFront encrypted and forwarded to the origin. CloudFront streams the processed request to the origin as it encrypts data, so fle-encrypted-fields can have a value even if the value of fle-status is an error. If field-level encryption is not configured for the distribution, the value of fle-encrypted-fields is a hyphen (-). c_port The port number of the request from the viewer. time_to_first_byte The number of seconds between receiving the request and writing the first byte of the response, as measured on the server. x_edge_detailed_result_type When x-edge-result-type is not Error, this field contains the same value as x-edge-result-type. When x-edge-result-type is Error, this field contains the specific type of error. sc_content_type The value of the HTTP Content-Type header of the response. sc_content_len The value of the HTTP Content-Length header of the response. sc_range_start When the response contains the HTTP Content-Range header, this field contains the range start value. sc-range-end When the response contains the HTTP Content-Range header, this field contains the range end value. Elastic Load Balancer Source: logtype = 'elb' Grok: ^%{TIMESTAMP_ISO8601:time} %{NOTSPACE:elb} %{NOTSPACE:client_ip}:%{NOTSPACE:client_port} ((%{NOTSPACE:backend_ip}:%{NOTSPACE:backend_port})|-) %{NOTSPACE:request_processing_time} %{NOTSPACE:backend_processing_time} %{NOTSPACE:response_processing_time} %{NOTSPACE:elb_status_code} %{NOTSPACE:backend_status_code} %{NOTSPACE:received_bytes} %{NOTSPACE:sent_bytes} \"%{DATA:request}\" \"%{DATA:user_agent}\" %{NOTSPACE:ssl_cipher} %{NOTSPACE:ssl_protocol} Copy Results: Field Definition x_edge_location The edge location that served the request. Each edge location is identified by a three-letter code and an arbitrarily assigned number; for example, DFW3. The three-letter code typically corresponds with the International Air Transport Association airport code for an airport near the edge location. (These abbreviations might change in the future.) sc_bytes The total number of bytes that CloudFront served to the viewer in response to the request, including headers; for example, 1045619. For WebSocket connections, this is the total number of bytes sent from the server to the client through the connection. c_ip The IP address of the viewer that made the request. If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip is the IP address of the proxy or load balancer. cs_method The HTTP request method: DELETE, GET, HEAD, OPTIONS, PATCH, POST, or PUT. cs_host The domain name of the CloudFront distribution; for example, d111111abcdef8.cloudfront.net. cs_uri_stem The portion of the URI that identifies the path and object; for example, /images/cat.jpg. Question marks (?) in URLs and query strings are not included in the log. sc_status An HTTP status code (for example, 200). Status code 000 indicates that the viewer closed the connection (for example, closed the browser tab) before CloudFront could respond to a request. If the viewer closes the connection after CloudFront starts to send the response, the log contains the applicable HTTP status code. cs_referer he name of the domain that originated the request. Common referrers include search engines, other websites that link directly to your objects, and your own website. cs_user_agent The value of the User-Agent header in the request. The User-Agent header identifies the source of the request, such as the type of device and browser that submitted the request and which search engine if applicable. cs_uri_query The query string portion of the URI, if any. When a URI doesn't contain a query string, this field's value is a hyphen (-). cs_cookie The cookie header in the request, including name-value pairs and the associated attributes. If you enable cookie logging, CloudFront logs the cookies in all requests, regardless of which cookies you choose to forward to the origin. If a request doesn't include a cookie header, this field's value is a hyphen (-). x_edge_result_type How CloudFront classifies the response after the last byte left the edge location. In some cases, the result type can change between the time that CloudFront is ready to send the response and the time that CloudFront has finished sending the response. x_edge_request_id An encrypted string that uniquely identifies a request. In the response header, this is x-amz-cf-id. x_host_header The value that the viewer included in the Host header for this request. This is the domain name in the request. If you're using the CloudFront domain name in your object URLs, this field contains that domain name. If you're using alternate domain names in your object URLs, such as [http://example.com/logo.png, this field contains the alternate domain name, such as example.com. To use alternate domain names, you must add them to your distribution. cs_protocol The protocol that the viewer specified in the request: http, https, ws, or wss. cs_bytes The number of bytes of data that the viewer included in the request, including headers. For WebSocket connections, this is the total number of bytes sent from the client to the server on the connection. time_taken The number of seconds (to the thousandth of a second; for example, 0.002) between the time that a CloudFront edge server receives a viewer's request and the time that CloudFront writes the last byte of the response to the edge server's output queue as measured on the server. From the perspective of the viewer, the total time to get the full object will be longer than this value due to network latency and TCP buffering. x_forwarded_for If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip in field 5 is the IP address of the proxy or load balancer. In that case, this field is the IP address of the viewer that originated the request. This field contains IPv4 and IPv6 addresses as applicable. If the viewer did not use an HTTP proxy or a load balancer, the value of x_forwarded_for is a hyphen (-). ssl_protocol When cs_protocol in field 17 is https, this field contains the SSL/TLS protocol that the client and CloudFront negotiated for transmitting the request and response. Possible values include: SSLv3 TLSv1 TLSv1.1 TLSv1.2 When cs_protocol in field 17 is http, the value for this field is a hyphen (-). ssl_cipher When cs_protocol in field 17 is https, this field contains the SSL/TLS cipher that the client and CloudFront negotiated for encrypting the request and response. Possible values include: ECDHE-RSA-AES128-GCM-SHA256 ECDHE-RSA-AES128-SHA256 ECDHE-RSA-AES128-SHA ECDHE-RSA-AES256-GCM-SHA384 ECDHE-RSA-AES256-SHA384 ECDHE-RSA-AES256-SHA AES128-GCM-SHA256 AES256-GCM-SHA384 AES128-SHA256 AES256-SHA AES128-SHA DES-CBC3-SHA RC4-MD5 When cs_protocol is http, the value for this field is a hyphen (-). x_edge_response_result_type How CloudFront classified the response just before returning the response to the viewer. Possible values include: Hit: CloudFront served the object to the viewer from the edge cache. RefreshHit: CloudFront found the object in the edge cache but it had expired, so CloudFront contacted the origin to verify that the cache has the latest version of the object. Miss: The request could not be satisfied by an object in the edge cache, so CloudFront forwarded the request to the origin server and returned the result to the viewer. LimitExceeded: The request was denied because a CloudFront limit was exceeded. CapacityExceeded: CloudFront returned a 503 error because the edge location didn't have enough capacity at the time of the request to serve the object. Error: Typically this means the request resulted in a client error (sc_status is 4xx) or a server error (sc_status is 5xx). If the value of x_edge_result_type is Error and the value of this field is not Error, the client disconnected before finishing the download. Redirect: CloudFront redirects from HTTP to HTTPS. If sc_status is 403 and you configured CloudFront to restrict the geographic distribution of your content, the request might have come from a restricted location. cs_protocol_version The HTTP version that the viewer specified in the request. Possible values include: HTTP/0.9 HTTP/1.0 HTTP/1.1 HTTP/2.0 fle_status When field-level encryption is configured for a distribution, this field contains a code that indicates whether the request body was successfully processed. If field-level encryption is not configured for the distribution, the value of this field is a hyphen (-). When CloudFront successfully processes the request body, encrypts values in the specified fields, and forwards the request to the origin, the value of this field is Processed. The value of x_edge_result_type can still indicate a client-side or server-side error in this case. If the request exceeds a field-level encryption limit, fle-status contains one of the following error codes, and CloudFront returns HTTP status code 400 to the viewer. fle-encrypted-fields The number of fields that CloudFront encrypted and forwarded to the origin. CloudFront streams the processed request to the origin as it encrypts data, so fle_encrypted_fields can have a value even if the value of fle_status is an error. If field-level encryption is not configured for the distribution, the value of fle_encrypted_fields is a hyphen (-). c_port The port number of the request from the viewer. time_to_first_byte The number of seconds between receiving the request and writing the first byte of the response, as measured on the server. x_edge_detailed_result_type When x_edge_result_type is not Error, this field contains the same value as x_edge_result_type. When x_edge_result_type is Error, this field contains the specific type of error. sc_content_type The value of the HTTP Content-Type header of the response. sc_content_len The value of the HTTP Content-Length header of the response. sc_range_start When the response contains the HTTP Content-Range header, this field contains the range start value. sc-range-end When the response contains the HTTP Content-Range header, this field contains the range end value. Microsoft IIS Source: logtype = 'iis_w3c' Grok: %{TIMESTAMP_ISO8601:log_timestamp} %{NOTSPACE:server_ip} %{WORD:method} %{NOTSPACE:uri} %{NOTSPACE:uri_query} %{NOTSPACE:server_port} %{NOTSPACE:username} %{NOTSPACE:client_ip} %{NOTSPACE:user_agent} %{NOTSPACE:referer} %{NOTSPACE:status} %{NOTSPACE:substatus} %{NOTSPACE:win32_status} %{NOTSPACE:time_taken} Copy Monit Source: logtype = 'monit' Grok: \\\\[%{NOTSPACE:tz} %{SYSLOGTIMESTAMP:nr_timestamp}\\\\] %{WORD:state}%{SPACE}: %{GREEDYDATA:message} Copy Results: state: The severity of the log line message: The message MySQL Error Source: logtype = 'mysql-error' Grok: \\\\[%{WORD:log_level}\\\\] Copy Results: log_level: The severity of the log line NGINX Source: logtype = 'nginx' Grok: %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{NUMBER:bytes}|-) %{QS:referrer} %{QS:agent} Copy Results: clientip: The IP address of the client verb: The HTTP verb ident: The user identity of the client making the request response: The HTTP status code of the response request: The URI and request being made httpversion: The HTTP version of the request rawrequest: The raw HTTP request if data is posted bytes: The number of bytes sent referrer: The HTTP referrer agent: The client's user agent NGINX Error Source: logtype = 'nginx-error' Grok: ^(?<timestamp>%{YEAR:year}[./-]%{MONTHNUM:month}[./-]%{MONTHDAY:day}[- ]%{TIME:time}) \\\\[%{LOGLEVEL:severity}\\\\] %{POSINT:pid}#%{NUMBER}: %{GREEDYDATA:errormessage}(?:, client: (?<client>%{IP:clientip}|%{HOSTNAME:hostname}))(?:, server: %{IPORHOSTORUNDERSCORE:server})(?:, request: %{QS:request})?(?:, upstream: \\\"%{URI:upstream}\\\")?(?:, host: %{QS:host})?(?:, referrer: \\\"%{URI:referrer}\\\")?$ Copy Results: severity: The severity of the log line pid: The server process ID errormessage: The error message clientip: The IP address of the calling client server: The server IP address request: The full request upstream: The upstream URI host: The server's hostname referrer: The HTTP referrer Route 53 Source: logtype = 'route-53' Grok: %{NUMBER:log_format_version} %{TIMESTAMP_ISO8601} %{WORD:zone_id} %{IPORHOST:query} %{WORD:query_type} %{WORD:response_code} %{WORD:protocol} %{WORD:edge_location} %{IP:resolver_ip} %{GREEDYDATA:edns_client_subnet} Copy Results: log_format_version: A versioned format for the log. zone_id: The ID of the hosted zone that is associated with all the DNS queries in this log. query: The domain or subdomain that was specified in the request. query_type: Either the DNS record type that was specified in the request, or ANY. response_code: The DNS response code that Route 53 returned in response to the DNS query. protocol: The protocol that was used to submit the query, either TCP or UDP. edge_location: The Route 53 edge location that responded to the query. Each edge location is identified by a three-letter code and an arbitrary number; for example, DFW3. The three-letter code typically corresponds with the International Air Transport Association airport code for an airport near the edge location. (These abbreviations might change in the future.) resolver_ip: The IP address of the DNS resolver that submitted the request to Route 53. edns_client_subnet: A partial IP address for the client that the request originated from, if available from the DNS resolver. Syslog RFC-5424 Source: logtype = 'syslog-rfc5424' Grok: <%{NONNEGINT:pri}>%{NONNEGINT:version} +(?:%{TIMESTAMP_ISO8601:log.timestamp}|-) +(?:%{HOSTNAME:hostname}|-) +(?:\\\\-|%{NOTSPACE:app.name}) +(?:\\\\-|%{NOTSPACE:procid}) (?:\\\\-|%{NOTSPACE:msgid}) +(?:\\[%{DATA:structured.data}\\]|-|) +%{GREEDYDATA:message} Copy Results: pri: The priority represents both the message facility and severity. version: Syslog protocol version. log.timestamp: Original timestamp. hostname: The machine that originally sent the Syslog message. app.name: The device or application that originated the message. procid: The process name or process ID associated with a Syslog system. msgid: Identifies the type of message. structured.data: Structured data string value. sd.<var>sd-id</var>.<var>sd-param-name</var>: The structured.data content is also parsed into separate attributes following a predefined naming convention: sd.<var>sd-id</var>.<var>sd-param-name</var>. See the structured data parsing examples, which follow. message: Free-form message that provides information about the event. Structured data parsing examples: The structured data [example one=\"1\" two=\"2\"] would be parsed into two different attributes: sd.example.one: \"1\" sd.example.two: \"2\" Copy If the same structured data block contains duplicate param names, it also appends an index-based suffix on the attribute name. For example, the structured data [example number=\"1\" number=\"2\"] would be parsed as: sd.example.number.0: \"1\" sd.example.number.1: \"2\" Copy For structured data with enterprise numbers assigned, an extra attribute is also parsed. For example, the structured data [example@123 number=\"1\"] would be parsed as: sd.example.enterprise.number: 123 sd.example.number: \"1\" Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 345.00037,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Built-in <em>log</em> parsing rulesets",
        "sections": "Built-in <em>log</em> parsing rulesets",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " the DNS resolver. Syslog RFC-5424 Source: logtype = &#x27;syslog-rfc5424&#x27; Grok: &lt;%{NONNEGINT:pri}&gt;%{NONNEGINT:version} +(?:%{TIMESTAMP_ISO8601:<em>log</em>.timestamp}|-) +(?:%{HOSTNAME:hostname}|-) +(?:\\\\-|%{NOTSPACE:app.name}) +(?:\\\\-|%{NOTSPACE:procid}) (?:\\\\-|%{NOTSPACE:msgid}) +(?:\\[%{<em>DATA:structured.data</em>"
      },
      "id": "603e7b9164441f1b2d4e8872"
    }
  ],
  "/docs/logs/log-management/log-api/use-tcp-endpoint-forward-logs-new-relic": [
    {
      "sections": [
        "Introduction to the Log API",
        "HTTP endpoint",
        "HTTP setup",
        "HTTP headers",
        "HTTP query parameters",
        "JSON body",
        "Simplified JSON body message",
        "Detailed JSON body message",
        "Limits and restricted characters",
        "Caution",
        "Important",
        "Rate limit violations",
        "HTTP requests per minute",
        "JSON bytes per minute",
        "Log payload format",
        "JSON message attributes",
        "Common block attributes",
        "Logs block attributes",
        "JSON message attribute parsing",
        "Log JSON examples",
        "Log POST message example",
        "JSON POST request example",
        "What's next?"
      ],
      "title": "Introduction to the Log API",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "Log API"
      ],
      "external_id": "198ebbf54f4a13fdf2f5b0f19d8cc8677afd09a2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/log-management/log-api/introduction-log-api/",
      "published_at": "2021-10-19T03:43:55Z",
      "updated_at": "2021-10-19T03:43:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If our log forwarding solutions don't meet your needs, you can use our Log API to send log data directly to New Relic via an HTTP endpoint. Want to try out our Log API? Create a New Relic account for free! No credit card required. HTTP endpoint Use the endpoint that's applicable for your New Relic account: United States (US) endpoint: https://log-api.newrelic.com/log/v1 Copy European Union (EU) endpoint: https://log-api.eu.newrelic.com/log/v1 Copy HTTP setup To send log data to your New Relic account via the Log API: Get your New Relic license key. Review the limits and restricted characters for your JSON payload. Generate the JSON message using the required headers and body fields. Ensure that your Api-Key or License-Key is included in your headers or query parameters. Refer to the log JSON examples. Send your JSON message to the appropriate HTTP endpoint for your New Relic account in a POST request. US: https://log-api.newrelic.com/log/v1 EU: https://log-api.eu.newrelic.com/log/v1 Generate some traffic and wait a few minutes, then check your account for data. If no data appears after you enable our log management capabilities, follow our troubleshooting procedures. HTTP headers When creating your HTTP headers, use these guidelines: Header Supported values Content-Type Required application/json json application/gzip gzip Api-Key Required A New Relic license key. You can also send this via query parameter. You can also use an Insights insert key but the license key is preferred. Gzipped JSON formatting is accepted. If sending compressed JSON, please include the Content-Type: application/json and Content-Encoding: gzip headers. HTTP query parameters The license key can also be passed as a query string parameter. This can be useful when sending logs from cloud-based sources that don't allow custom HTTP request headers. Query parameter Value Api-Key Your license key. Use this key whenever you send a header. You can also use an Insights insert key but the license key is preferred. JSON body You can send your JSON message using either a simplified or detailed set of attributes: Simplified JSON body message When using the simplified format to create your JSON message, send a single JSON object with the following: Field Value type Format Required Notes \"timestamp\" Integer Either milliseconds or seconds since epoch No If the field is not specific as millisecond or seconds since epoch, the message will be timestamped using the ingest time \"message\" String any string No This is the main log message field that is searched by default \"logtype\" String any string No Primary field for identifying logs and matching parsing rules other_fields (must not contain white space) String any string No These will become attributes of the log message Note: Log management does not support white space in attribute names Detailed JSON body message When using the detailed format to create your body, it must be a JSON array containing one or more JSON objects, each of which with the following format: Field Value type Format Required Notes \"common\" Object See common. No Any attributes that are common to all log messages \"logs\" Array See logs. Yes Array with the log entries Limits and restricted characters Caution Avoid calling our API from within the code of a customer-facing application. This can cause performance issues or block your application if response time is slow. If you need to do it this way, call our API asynchronously to avoid these performance issues. Restrictions on logs sent to the Log API: Payload total size: 1MB(10^6 bytes) maximum per POST. We highly recommend using compression. The payload must be encoded as UTF-8. Number of attributes per event: 255 maximum Length of attribute name: 255 characters Length of attribute value: 4096 maximum character length Some specific attributes have additional restrictions: accountId: This is a reserved attribute name. If it is included, it will be dropped during ingest. entity.guid, entity.name, and entity.type: These attributes are used internally to identify entities. Any values submitted with these keys in the attributes section of a metric data point may cause undefined behavior such as missing entities in the UI or telemetry not associating with the expected entities. For more information please refer to Entity synthesis. eventType: This is a reserved attribute name. If it is included, it will be dropped during ingest. timestamp: Must be a Unix epoch timestamp. You can define timestamps either in seconds or in milliseconds. Important Payloads with timestamps older than 48 hours may be dropped. Rate limits on logs sent to the Log API: Maximum rate for HTTP requests sent to the Log API: 300,000 requests per minute Maximum rate of uncompressed Log JSON bytes sent to the Log API: 10 GB per minute Rate limit violations Exceeding rate limits affects how the Log API behaves. Follow these instructions if this happens. HTTP requests per minute When the maximum request rate limit is exceeded for an account, the New Relic Log API returns a 429 response for the remainder of the minute. This response includes a Retry-After header indicating how long to wait in seconds before resubmitting or sending new data. To resolve this issue, either reduce the number of data points you are sending, or request a rate limit change. Subsequent subscription changes do not impact modified rate limits. If an account change impacts your rate limit, you must notify us to adjust your rate limit. To request rate limit changes, contact your New Relic account representative, or visit our Support portal. JSON bytes per minute When the maximum Log JSON byte limit is exceeded for an account, the New Relic Log API returns a 429 response for the remainder of the minute. This response includes a Retry-After header indicating how long to wait in seconds before resubmitting or sending new data. To resolve this issue, try to reduce the amount of log data you are sending, or spread it out over a larger period of time. To request rate limit changes, contact your New Relic account representative, or visit our Support portal. Log payload format We accept any valid JSON payload. The payload must encoded as UTF-8. Important Log management does not support white space in attribute names. For example, {\"Sample Attribute\": \"Value\"} would cause errors. JSON message attributes Common block attributes This is a block containing attributes that will be common to all log entries in logs: Field Value type Format Required Notes \"timestamp\" Integer Milliseconds or seconds since epoch No Message timestamp default to ingest time \"attributes\" Object JSON No This sub-object contains all other attributes of the message Logs block attributes This is an array containing log entries with the following format: Field Value type Format Required Notes \"timestamp\" Integer Milliseconds or seconds since epoch No Message timestamp default to ingest time \"attributes\" Object JSON No This sub-object contains all other attributes of the message \"message\" String (any string) Yes This is the main log message field that is searched by default \"log\" String (any string) No We will rewrite this string as the field message on ingest \"LOG\" String (any string) No We will rewrite this string as the field message on ingest \"MESSAGE\" String (any string) No We will rewrite this string as the field message on ingest JSON message attribute parsing Our log management capabilities will parse any message attribute as JSON. The resulting JSON attributes in the parsed message will be added to the event. If the message attribute is not JSON, it is left as is. Important New Relic's log management capabilities do not support white space in attribute names. For example, {\"Sample Attribute\": \"Value\"} would cause errors. Here is an example message attribute: { \"timestamp\": 1562767499238, \"message\": \"{\\\"service-name\\\": \\\"login-service\\\", \\\"user\\\": {\\\"id\\\": 123, \\\"name\\\": \\\"alice\\\"}}\" } Copy This will be treated as: { \"timestamp\": 1562767499238, \"message\": \"{\\\"service-name\\\": \\\"my-service\\\", \\\"user\\\": {\\\"id\\\": 123, \\\"name\\\": \\\"alice\\\"}}\", \"service-name\": \"my-service\", \"user\": { \"id\": 123, \"name\": \"alice\" } } Copy Log JSON examples Attributes can be scalar JSON types like string and number. They can also be compound (or nested) objects. Compound attributes will have their associated attributes stored with flattened names. For example, here is a compound user attribute in a log entry's attributes: \"attributes\": { \"action\": \"login\", \"user\": { \"id\": 123, \"name\": \"alice\" } } Copy This will result in the following attributes being stored with the log event: Attribute Value \"action\" \"login\" \"user.id\" 123 \"user.name\" \"alice\" Log POST message example Log POST message example: POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: <YOUR_LICENSE_KEY> Accept: */* Content-Length: 319 [{ \"common\": { \"attributes\": { \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } }, \"logs\": [{ \"timestamp\": <TIMESTAMP_IN_UNIX_EPOCH>, \"message\": \"User 'xyz' logged in\" },{ \"timestamp\": <TIMESTAMP_IN_UNIX_EPOCH>, \"message\": \"User 'xyz' logged out\", \"attributes\": { \"auditId\": 123 } }] }] Copy This POST message would result in the following log messages being stored in New Relic: Attribute Value \"logtype\" \"accesslogs\" \"service\" \"login-service\" \"hostname\" \"login.example.com\" Here's an example of stored logs block attributes: Attribute Value \"timestamp\" 1550086450124 \"message\" \"User 'xyz' logged out\" \"auditId\" 123 JSON POST request example Here's an example of a JSON POST request: POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: <YOUR_LICENSE_KEY> Accept: */* Content-Length: 133 { \"timestamp\": <TIMESTAMP_IN_UNIX_EPOCH>, \"message\": \"User 'xyz' logged in\", \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } Copy What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 313.77008,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to the <em>Log</em> <em>API</em>",
        "sections": "Introduction to the <em>Log</em> <em>API</em>",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " in a POST request. US: https:&#x2F;&#x2F;<em>log</em>-<em>api</em>.newrelic.com&#x2F;<em>log</em>&#x2F;v1 EU: https:&#x2F;&#x2F;<em>log</em>-<em>api</em>.eu.newrelic.com&#x2F;<em>log</em>&#x2F;v1 Generate some traffic and wait a few minutes, then check your account for data. If no data appears after you enable our <em>log</em> <em>management</em> capabilities, follow our troubleshooting procedures. HTTP headers"
      },
      "id": "603ea832196a6726e7a83da1"
    },
    {
      "sections": [
        "Parsing log data",
        "Example",
        "How log parsing works",
        "Important",
        "Limits",
        "Tip",
        "Built-in parsing rulesets",
        "List of built-in rulesets",
        "Add the logtype attribute",
        "New Relic infrastructure agent example",
        "Fluentd example",
        "Fluent Bit example",
        "Logstash example",
        "Logs API example",
        "Create custom parsing rules",
        "Troubleshooting"
      ],
      "title": "Parsing log data",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "52955adb68242c4ca582ba9cb8e22963955a8275",
      "image": "https://docs.newrelic.com/static/dc392bb7142d2fdb253a649daf4ebe6d/c1b63/log-parsing-rule-ui.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/parsing/",
      "published_at": "2021-10-19T03:43:56Z",
      "updated_at": "2021-10-19T03:43:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Parsing is the process of splitting unstructured log data into attributes (key/value pairs). You can use these attributes to facet or filter logs in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial on YouTube (approx. 4-1/2 minutes). New Relic parses log data according to rules. This document describes how logs parsing works, how to use built-in rules, and how to create custom rules. You can also create, query, and manage your log parsing rules by using NerdGraph, our GraphQL API, at api.newrelic.com/graphiql. For more information, see our NerdGraph tutorial for parsing. Example A good example is a default NGINX access log containing unstructured text. It is useful for searching but not much else. Here's an example of a typical line: 127.180.71.3 - - [10/May/1997:08:05:32 +0000] \"GET /downloads/product_1 HTTP/1.1\" 304 0 \"-\" \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" Copy In an unparsed format, you would need to do a full text search to answer most questions. After parsing, the log is organized into attributes, like response code and request URL: { \"remote_addr\":\"93.180.71.3\", \"time\":\"1586514731\", \"method\":\"GET\", \"path\":\"/downloads/product_1\", \"version\":\"HTTP/1.1\", \"response\":\"304\", \"bytesSent\": 0, \"user_agent\": \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" } Copy Parsing makes it easier to create custom queries that facet on those values. This helps you understand the distribution of response codes per request URL and quickly find problematic pages. How log parsing works Here's an overview of how New Relic implements parsing of logs: Log parsing How it works What All parsing takes place against the message field; no other fields can be parsed. Each parsing rule is created with matching criteria that determines which logs the rule will attempt to parse. To simplify the matching process, we recommend adding a logtype attribute to your logs. However, you are not limited to using logtype; any attribute can be used as matching criteria. When Parsing will only be applied once to each log message. If multiple parsing rules match the log, only the first that succeeds will be applied. Parsing takes place during log ingestion, before data is written to NRDB. Once data has been written to storage, it can no longer be parsed. Parsing occurs in the pipeline before data enrichments take place. Be careful when defining the matching criteria for a parsing rule. If the criteria is based on an attribute that doesn't exist untail after parsing or enrichment take place, that data won't be present in the logs when matching occurs. As a result, no parsing will happen. How Rules can be written in Grok, regex, or a mixture of the two. Grok is a collection of patterns that abstract away complicated regular expressions. If the content of the message field is JSON, it will be parsed automatically. New Relic's log ingestion pipeline can parse data by matching a log event to a rule that describes how the log should be parsed. There are two ways log events can be parsed: Use a built-in rule. Define a custom rule. Rules are a combination of matching logic and parsing logic. Matching is done by defining a query match on an attribute of the logs. Rules are not applied retroactively. Logs collected before a rule is created are not parsed by that rule. The simplest way to organize your logs and how they are parsed is to include the logtype field in your log event. This tells New Relic what built-in ruleset to apply to the logs. Important Once a parsing rule is active, data parsed by the rule is permanently changed. This cannot be reverted. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. Limits Parsing is computationally expensive, which introduces risk. Parsing is done for custom rules defined in an account and for matching patterns to a log. A large number of patterns or poorly defined custom rules will consume a huge amount of memory and CPU resources while also taking a very long time to complete. In order to prevent problems, we apply two parsing limits: per-message-per-rule and per-account. Limit Description Per-message-per-rule The per-message-per-rule limit prevents the time spent parsing any single message from being greater than 100 ms. If that limit is reached, the system will cease attempting to parse the log message with that rule. The ingestion pipeline will attempt to run any other applicable on that message, and the message will still be passed through the ingestion pipeline and stored in NRDB. The log message will be in its original, unparsed format. Per-account The per-account limit exists to prevent accounts from using more than their fair share of resources. The limit considers the total time spent processing all log messages for an account per-minute. The limit is not a fixed value; it scales up or down proportionally to the volume of data stored daily by the account and the environment size that is subsequently allocated to support that customer. Tip To easily check if your rate limits have been reached, go to your system Limits page in the New Relic UI. Built-in parsing rulesets Common log formats have well-established parsing rules already created for them. To get the benefit of built-in parsing rules, add the logtype attribute when forwarding logs. Set the value to something listed in the following table, and the rules for that type of log will be applied automatically. List of built-in rulesets The following logtype attribute values map to a predefined parsing rule. For example, to query the Application Load Balancer: From the New Relic UI, use the format logtype: alb. From NerdGraph, use the format logtype = 'alb'. To learn what fields are parsed for each rule, see our documentation about built-in parsing rules. logtype Example matching query alb AWS Application Load Balancer logtype:alb apache Apache Access logtype:apache cloudfront-web CloudFront Web logtype:cloudfront-web elb Amazon Elastic Load Balancer logtype:elb iis_w3c Microsoft IIS server logs - W3C format logtype:iis_w3c monit Monit logs logtype:monit mysql-error MySQL Error logtype:mysql-error nginx NGINX access logs logtype:nginx nginx-error NGINX error logs logtype:nginx-error route-53 Amazon Route 53 logs logtype:route-53 syslog-rfc5424 Syslog logtype:syslog-rfc5424 Add the logtype attribute When aggregating logs, it's important to provide metadata that makes it easy to organize, search, and parse those logs. One simple way of doing this is to add the attribute logtype to the log messages when they are shipped. Built-in parsing rules are applied by default to certain logtype values. Here are some examples of how to add logtype to logs sent by some of our supported shipping methods. New Relic infrastructure agent example Add logtype as an attribute. You must set the logtype for each named source. Learn more about using the infrastructure agent to add attributes. logs: - name: file-simple file: /path/to/file attributes: logtype: fileRaw - name: nginx-example file: /var/log/nginx.log attributes: logtype: nginx Copy Fluentd example Add a filter block to the .conf file, which uses a record_transformer to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluentd examples. <filter containers> @type record_transformer enable_ruby true <record> #Add logtype to trigger a built-in parsing rule for nginx access logs logtype nginx #Set timestamp from the value contained in the field \"time\" timestamp record[\"time\"] #Add hostname and tag fields to all records hostname \"#{Socket.gethostname}\" tag ${tag} </record> </filter> Copy Fluent Bit example Add a filter block to the .conf file that uses a record_modifier to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluent Bit examples. [FILTER] Name record_modifier Match * Record logtype nginx Record hostname ${HOSTNAME} Record service_name Sample-App-Name Copy Logstash example Add a filter block to the Logstash configuration which uses an add_field mutate filter to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Logstash examples. filter { mutate { add_field => { \"logtype\" => \"nginx\" \"service_name\" => \"myservicename\" \"hostname\" => \"%{host}\" } } } Copy Logs API example You can add attributes to the JSON request sent to New Relic. In this example we add a logtype attribute of value nginx to trigger the built-in NGINX parsing rule. Learn more about using the Logs API. POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: YOUR_LICENSE_KEY Accept: */* Content-Length: 133 { \"timestamp\": TIMESTAMP_IN_UNIX_EPOCH, \"message\": \"User 'xyz' logged in\", \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } Copy Create custom parsing rules Many logs are formatted or structured in a unique way. In order to parse them, custom logic must be built and applied. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. To create and manage your own, custom parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing, then click Create parsing rule. Enter the parsing rule's name. Choose an attribute and value to match on. Write your Grok pattern and test the rule. To learn about Grok and custom parsing rules, read our blog post about how to parse logs with Grok patterns. Enable and save the custom parsing rule. To view the list of custom parsing rules: From Manage Data on the left nav of the Logs UI, click Parsing. To view existing parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing. Troubleshooting If parsing is not working the way you intended, it may be due to: Logic: The parsing rule matching logic does not match the logs you want. Timing: If your parsing matching rule targets a value that doesn't exist yet, it will fail. This can occur if the value is added later in the pipeline as part of the enrichment process. Limits: There is a fixed amount of time available every minute to process logs via parsing, patterns, drop filters, etc. If the maximum amount of time has been spent, parsing will be skipped for additional log event records. To resolve these problems, create or adjust your custom parsing rules.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 210.8634,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Parsing <em>log</em> data",
        "sections": "<em>Logs</em> <em>API</em> example",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " on YouTube (approx. 4-1&#x2F;2 minutes). New Relic parses <em>log</em> data according to rules. This document describes how <em>logs</em> parsing works, how to use built-in rules, and how to create custom rules. You can also create, query, and manage your <em>log</em> parsing rules by using NerdGraph, our GraphQL <em>API</em>"
      },
      "id": "603e7eb4196a67b0c4a83dd1"
    },
    {
      "sections": [
        "Discover value in log data with patterns",
        "Technical overview",
        "Availability",
        "Get started",
        "Explore log patterns",
        "Explore logs with no pattern",
        "Masked attributes and wildcards",
        "Troubleshooting",
        "Put the platform to work with patterns"
      ],
      "title": "Discover value in log data with patterns",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "8f1e27c94327ca4888a945f8e12f9c2310ccd7a6",
      "image": "https://docs.newrelic.com/static/578d7186bb34352855696e5307cc82f2/c1b63/log-patterns-logs-without-a-pattern.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/find-unusual-logs-log-patterns/",
      "published_at": "2021-10-19T03:56:24Z",
      "updated_at": "2021-10-19T03:56:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Log patterns are the fastest way to discover value in log data without searching. Log data is high volume telemetry with a low value per individual record. Searching can quickly lead to logs that provide a root cause explanation, but most data is repetitive and hard to contextualize when browsing. Patterns can make log data discoverable without spending a lot of time reading through low value data. Logs > Patterns: Use patterns as the basis for alerts when the frequency of important data changes, or for configuring drop rules to get rid of unnecessary repetitive data. Technical overview Log patterns functionality applies machine learning to normalize and group log messages that are consistent in format but variable in content. These grouped messages can be sorted, making it easy to find the most frequent or rarest sets of logs in your environment. Use patterns as the basis for alerts when the frequency of important data changes, or to configure drop rules to get rid of unnecessary repetitive data. Log patterns use advanced clustering algorithms to group together similar log messages automatically. With patterns, you can: Orient more quickly through millions of logs. Reduce the time it takes to identify unusual behavior in your log estate. Monitor the frequency of known patterns over time to focus your energy on what matters, and exclude what's irrelevant. Availability The ability to configure this feature is dependent on role-based permissions. For example, accounts still using our original pricing model require an Owner or Admin role. If you see Patterns are turned off in your Log management Patterns UI, click the Configure Patterns button and enable it. If you don't see patterns within 30 minutes of enabling the feature, there may be a lack of data with a message attribute for the system to create a pattern from it. Log patterns Limitations and considerations Pricing There is no separate pricing for log patterns. The only cost is for additional data generated and added to your log records. A pattern attribute will be added to all logs that match a pattern. Attributes also may be added when common values are discovered, such as GUIDs, IP addresses, URL, or email addresses. These attributes are automatically extracted from the log message as part of the pattern process. HITRUST accounts The log patterns feature is not FedRAMP compliant. FedRAMP or other HITRUST accounts are not eligible to use patterns. Parsing limits We have a system of safety limits on memory and CPU resources when processing logs and their patterns. These parsing limits can have an impact on the data you get. For more information, see our documentation about parsing limits. Get started To start examining patterns: Go to one.newrelic.com > Logs, and use the account picker dropdown to select the target account where you want to explore patterns. In the left navigation of the Logs UI, click Patterns. The main log UI changes to show patterns that match the query in the query bar. Logs > Log patterns: The line chart shows the top 5 patterns over time. Use the time picker and query bar to adjust the results. Explore log patterns By default the log patterns UI first shows the most frequent occurrence of patterns. To sort to show the rarest patterns first, click the Count column. You can also use the query bar or attributes bar to filter your log patterns. If you want to... Do this... Understand the rate of change in patterns Look at the line chart. The color-coded patterns correspond to the plot column in the table. You can toggle individual plot patterns to narrow your focus. See the individual log messages that match each pattern Click pattern to expand the row and see a table of individual log records. To see additional records, scroll up or down. To explore an individual log in more detail, click it to open the details panel. Group and filter patterns by their attributes Use the query bar and time picker. As you apply different filters and time windows, the log patterns adjust to your new target data. Create an alert from a pattern Add the pattern to the query bar and run the query. Then click Create alert condition in the left nav. Troubleshoot log messages that haven't been clustered into a pattern Use the Logs with no pattern tab in the Log patterns UI. Clicking a specific log message will open the log message details panel you're familiar with from the Logs management page. Explore logs with no pattern The Logs with no pattern tab groups all recent log messages in your account that were not clustered into a known pattern yet. These log messages don't represent any problem or flaw in the system; they have no pattern because they are too new to have been processed by the machine learning system. This makes them valuable to explore when you want to understand what has recently changed in your environment. Logs > Log patterns: New Relic's log patterns feature automatically groups logs without a matching pattern. For example: Are any of these logs tied to a recent problem? This is a quick way to discover unique log data that is appearing for the first time in your environment. Does your log data have a new format? Sometimes the logs don't represent a problem, but a new format of log data that deviates from the data model you expect your applications to follow. Catching these logs early gives you the opportunity to ask developers to correct any deviations in their log output. The more consistent people are in the way log data is generated, the easier it becomes to use logs across a diverse set of teams. Masked attributes and wildcards Parts of the log messages in patterns are classified as variables and are substituted by masked attributes. The masking process supports and improves the clustering phase by allowing the algorithm to ignore changing details and focus on the repetitive structure. Masked attributes include: date_time ip url uuid Masked attributes are highlighted and are easy to identify, as shown in the following example. Here is an example of a log pattern that has masked attributes. Log patterns extract other less trivial variables that don't belong to any masked attribute. These variables are indicated as wildcards *. Here is an example of how wildcards * group variables in the Log patterns UI. Troubleshooting Here are a few reasons why you might have patterns enabled but not see any pattern data. If you're sure none of the items below are true, get help from support.newrelic.com. No data has arrived in the timeframe you're observing. Try expanding the time range you're viewing with the time picker. It's been less than 24 hours since patterns were enabled in the account. This means the ML model may not be generated for the account yet. None of the data coming in has a message field. Patterns will only be generated for values in the message field of a log record. If your logs don't contain message, there will be no data. Put the platform to work with patterns Patterns are a value that is enriched onto the existing log message as a new attribute named newrelic.logPattern. Anything you can do with logs generally can be done with log patterns, such as: Build your own dashboards with patterns, to monitor a specific pattern or group of patterns you care about. Create alerts for patterns by adding NRQL alerts. Use baseline alert conditions to detect anomalies in known log patterns.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 192.03291,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Discover value in <em>log</em> data with patterns",
        "sections": "Explore <em>logs</em> with no pattern",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " messages that haven&#x27;t been clustered into a pattern Use the <em>Logs</em> with no pattern tab in the <em>Log</em> patterns UI. Clicking a specific <em>log</em> message will open the <em>log</em> message details panel you&#x27;re familiar with from the <em>Logs</em> <em>management</em> page. Explore <em>logs</em> with no pattern The <em>Logs</em> with no pattern tab groups all"
      },
      "id": "6072d46128ccbc244451c18b"
    }
  ],
  "/docs/logs/log-management/troubleshooting/find-issues-cause-or-impact-surrounding-logs": [
    {
      "sections": [
        "Introduction to the Log API",
        "HTTP endpoint",
        "HTTP setup",
        "HTTP headers",
        "HTTP query parameters",
        "JSON body",
        "Simplified JSON body message",
        "Detailed JSON body message",
        "Limits and restricted characters",
        "Caution",
        "Important",
        "Rate limit violations",
        "HTTP requests per minute",
        "JSON bytes per minute",
        "Log payload format",
        "JSON message attributes",
        "Common block attributes",
        "Logs block attributes",
        "JSON message attribute parsing",
        "Log JSON examples",
        "Log POST message example",
        "JSON POST request example",
        "What's next?"
      ],
      "title": "Introduction to the Log API",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "Log API"
      ],
      "external_id": "198ebbf54f4a13fdf2f5b0f19d8cc8677afd09a2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/log-management/log-api/introduction-log-api/",
      "published_at": "2021-10-19T03:43:55Z",
      "updated_at": "2021-10-19T03:43:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If our log forwarding solutions don't meet your needs, you can use our Log API to send log data directly to New Relic via an HTTP endpoint. Want to try out our Log API? Create a New Relic account for free! No credit card required. HTTP endpoint Use the endpoint that's applicable for your New Relic account: United States (US) endpoint: https://log-api.newrelic.com/log/v1 Copy European Union (EU) endpoint: https://log-api.eu.newrelic.com/log/v1 Copy HTTP setup To send log data to your New Relic account via the Log API: Get your New Relic license key. Review the limits and restricted characters for your JSON payload. Generate the JSON message using the required headers and body fields. Ensure that your Api-Key or License-Key is included in your headers or query parameters. Refer to the log JSON examples. Send your JSON message to the appropriate HTTP endpoint for your New Relic account in a POST request. US: https://log-api.newrelic.com/log/v1 EU: https://log-api.eu.newrelic.com/log/v1 Generate some traffic and wait a few minutes, then check your account for data. If no data appears after you enable our log management capabilities, follow our troubleshooting procedures. HTTP headers When creating your HTTP headers, use these guidelines: Header Supported values Content-Type Required application/json json application/gzip gzip Api-Key Required A New Relic license key. You can also send this via query parameter. You can also use an Insights insert key but the license key is preferred. Gzipped JSON formatting is accepted. If sending compressed JSON, please include the Content-Type: application/json and Content-Encoding: gzip headers. HTTP query parameters The license key can also be passed as a query string parameter. This can be useful when sending logs from cloud-based sources that don't allow custom HTTP request headers. Query parameter Value Api-Key Your license key. Use this key whenever you send a header. You can also use an Insights insert key but the license key is preferred. JSON body You can send your JSON message using either a simplified or detailed set of attributes: Simplified JSON body message When using the simplified format to create your JSON message, send a single JSON object with the following: Field Value type Format Required Notes \"timestamp\" Integer Either milliseconds or seconds since epoch No If the field is not specific as millisecond or seconds since epoch, the message will be timestamped using the ingest time \"message\" String any string No This is the main log message field that is searched by default \"logtype\" String any string No Primary field for identifying logs and matching parsing rules other_fields (must not contain white space) String any string No These will become attributes of the log message Note: Log management does not support white space in attribute names Detailed JSON body message When using the detailed format to create your body, it must be a JSON array containing one or more JSON objects, each of which with the following format: Field Value type Format Required Notes \"common\" Object See common. No Any attributes that are common to all log messages \"logs\" Array See logs. Yes Array with the log entries Limits and restricted characters Caution Avoid calling our API from within the code of a customer-facing application. This can cause performance issues or block your application if response time is slow. If you need to do it this way, call our API asynchronously to avoid these performance issues. Restrictions on logs sent to the Log API: Payload total size: 1MB(10^6 bytes) maximum per POST. We highly recommend using compression. The payload must be encoded as UTF-8. Number of attributes per event: 255 maximum Length of attribute name: 255 characters Length of attribute value: 4096 maximum character length Some specific attributes have additional restrictions: accountId: This is a reserved attribute name. If it is included, it will be dropped during ingest. entity.guid, entity.name, and entity.type: These attributes are used internally to identify entities. Any values submitted with these keys in the attributes section of a metric data point may cause undefined behavior such as missing entities in the UI or telemetry not associating with the expected entities. For more information please refer to Entity synthesis. eventType: This is a reserved attribute name. If it is included, it will be dropped during ingest. timestamp: Must be a Unix epoch timestamp. You can define timestamps either in seconds or in milliseconds. Important Payloads with timestamps older than 48 hours may be dropped. Rate limits on logs sent to the Log API: Maximum rate for HTTP requests sent to the Log API: 300,000 requests per minute Maximum rate of uncompressed Log JSON bytes sent to the Log API: 10 GB per minute Rate limit violations Exceeding rate limits affects how the Log API behaves. Follow these instructions if this happens. HTTP requests per minute When the maximum request rate limit is exceeded for an account, the New Relic Log API returns a 429 response for the remainder of the minute. This response includes a Retry-After header indicating how long to wait in seconds before resubmitting or sending new data. To resolve this issue, either reduce the number of data points you are sending, or request a rate limit change. Subsequent subscription changes do not impact modified rate limits. If an account change impacts your rate limit, you must notify us to adjust your rate limit. To request rate limit changes, contact your New Relic account representative, or visit our Support portal. JSON bytes per minute When the maximum Log JSON byte limit is exceeded for an account, the New Relic Log API returns a 429 response for the remainder of the minute. This response includes a Retry-After header indicating how long to wait in seconds before resubmitting or sending new data. To resolve this issue, try to reduce the amount of log data you are sending, or spread it out over a larger period of time. To request rate limit changes, contact your New Relic account representative, or visit our Support portal. Log payload format We accept any valid JSON payload. The payload must encoded as UTF-8. Important Log management does not support white space in attribute names. For example, {\"Sample Attribute\": \"Value\"} would cause errors. JSON message attributes Common block attributes This is a block containing attributes that will be common to all log entries in logs: Field Value type Format Required Notes \"timestamp\" Integer Milliseconds or seconds since epoch No Message timestamp default to ingest time \"attributes\" Object JSON No This sub-object contains all other attributes of the message Logs block attributes This is an array containing log entries with the following format: Field Value type Format Required Notes \"timestamp\" Integer Milliseconds or seconds since epoch No Message timestamp default to ingest time \"attributes\" Object JSON No This sub-object contains all other attributes of the message \"message\" String (any string) Yes This is the main log message field that is searched by default \"log\" String (any string) No We will rewrite this string as the field message on ingest \"LOG\" String (any string) No We will rewrite this string as the field message on ingest \"MESSAGE\" String (any string) No We will rewrite this string as the field message on ingest JSON message attribute parsing Our log management capabilities will parse any message attribute as JSON. The resulting JSON attributes in the parsed message will be added to the event. If the message attribute is not JSON, it is left as is. Important New Relic's log management capabilities do not support white space in attribute names. For example, {\"Sample Attribute\": \"Value\"} would cause errors. Here is an example message attribute: { \"timestamp\": 1562767499238, \"message\": \"{\\\"service-name\\\": \\\"login-service\\\", \\\"user\\\": {\\\"id\\\": 123, \\\"name\\\": \\\"alice\\\"}}\" } Copy This will be treated as: { \"timestamp\": 1562767499238, \"message\": \"{\\\"service-name\\\": \\\"my-service\\\", \\\"user\\\": {\\\"id\\\": 123, \\\"name\\\": \\\"alice\\\"}}\", \"service-name\": \"my-service\", \"user\": { \"id\": 123, \"name\": \"alice\" } } Copy Log JSON examples Attributes can be scalar JSON types like string and number. They can also be compound (or nested) objects. Compound attributes will have their associated attributes stored with flattened names. For example, here is a compound user attribute in a log entry's attributes: \"attributes\": { \"action\": \"login\", \"user\": { \"id\": 123, \"name\": \"alice\" } } Copy This will result in the following attributes being stored with the log event: Attribute Value \"action\" \"login\" \"user.id\" 123 \"user.name\" \"alice\" Log POST message example Log POST message example: POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: <YOUR_LICENSE_KEY> Accept: */* Content-Length: 319 [{ \"common\": { \"attributes\": { \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } }, \"logs\": [{ \"timestamp\": <TIMESTAMP_IN_UNIX_EPOCH>, \"message\": \"User 'xyz' logged in\" },{ \"timestamp\": <TIMESTAMP_IN_UNIX_EPOCH>, \"message\": \"User 'xyz' logged out\", \"attributes\": { \"auditId\": 123 } }] }] Copy This POST message would result in the following log messages being stored in New Relic: Attribute Value \"logtype\" \"accesslogs\" \"service\" \"login-service\" \"hostname\" \"login.example.com\" Here's an example of stored logs block attributes: Attribute Value \"timestamp\" 1550086450124 \"message\" \"User 'xyz' logged out\" \"auditId\" 123 JSON POST request example Here's an example of a JSON POST request: POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: <YOUR_LICENSE_KEY> Accept: */* Content-Length: 133 { \"timestamp\": <TIMESTAMP_IN_UNIX_EPOCH>, \"message\": \"User 'xyz' logged in\", \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } Copy What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.5347,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to the <em>Log</em> API",
        "sections": "<em>Logs</em> block attributes",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " in a POST request. US: https:&#x2F;&#x2F;<em>log</em>-api.newrelic.com&#x2F;<em>log</em>&#x2F;v1 EU: https:&#x2F;&#x2F;<em>log</em>-api.eu.newrelic.com&#x2F;<em>log</em>&#x2F;v1 Generate some traffic and wait a few minutes, then check your account for data. If no data appears after you enable our <em>log</em> <em>management</em> capabilities, follow our <em>troubleshooting</em> procedures. HTTP headers"
      },
      "id": "603ea832196a6726e7a83da1"
    },
    {
      "sections": [
        "Discover value in log data with patterns",
        "Technical overview",
        "Availability",
        "Get started",
        "Explore log patterns",
        "Explore logs with no pattern",
        "Masked attributes and wildcards",
        "Troubleshooting",
        "Put the platform to work with patterns"
      ],
      "title": "Discover value in log data with patterns",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "8f1e27c94327ca4888a945f8e12f9c2310ccd7a6",
      "image": "https://docs.newrelic.com/static/578d7186bb34352855696e5307cc82f2/c1b63/log-patterns-logs-without-a-pattern.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/find-unusual-logs-log-patterns/",
      "published_at": "2021-10-19T03:56:24Z",
      "updated_at": "2021-10-19T03:56:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Log patterns are the fastest way to discover value in log data without searching. Log data is high volume telemetry with a low value per individual record. Searching can quickly lead to logs that provide a root cause explanation, but most data is repetitive and hard to contextualize when browsing. Patterns can make log data discoverable without spending a lot of time reading through low value data. Logs > Patterns: Use patterns as the basis for alerts when the frequency of important data changes, or for configuring drop rules to get rid of unnecessary repetitive data. Technical overview Log patterns functionality applies machine learning to normalize and group log messages that are consistent in format but variable in content. These grouped messages can be sorted, making it easy to find the most frequent or rarest sets of logs in your environment. Use patterns as the basis for alerts when the frequency of important data changes, or to configure drop rules to get rid of unnecessary repetitive data. Log patterns use advanced clustering algorithms to group together similar log messages automatically. With patterns, you can: Orient more quickly through millions of logs. Reduce the time it takes to identify unusual behavior in your log estate. Monitor the frequency of known patterns over time to focus your energy on what matters, and exclude what's irrelevant. Availability The ability to configure this feature is dependent on role-based permissions. For example, accounts still using our original pricing model require an Owner or Admin role. If you see Patterns are turned off in your Log management Patterns UI, click the Configure Patterns button and enable it. If you don't see patterns within 30 minutes of enabling the feature, there may be a lack of data with a message attribute for the system to create a pattern from it. Log patterns Limitations and considerations Pricing There is no separate pricing for log patterns. The only cost is for additional data generated and added to your log records. A pattern attribute will be added to all logs that match a pattern. Attributes also may be added when common values are discovered, such as GUIDs, IP addresses, URL, or email addresses. These attributes are automatically extracted from the log message as part of the pattern process. HITRUST accounts The log patterns feature is not FedRAMP compliant. FedRAMP or other HITRUST accounts are not eligible to use patterns. Parsing limits We have a system of safety limits on memory and CPU resources when processing logs and their patterns. These parsing limits can have an impact on the data you get. For more information, see our documentation about parsing limits. Get started To start examining patterns: Go to one.newrelic.com > Logs, and use the account picker dropdown to select the target account where you want to explore patterns. In the left navigation of the Logs UI, click Patterns. The main log UI changes to show patterns that match the query in the query bar. Logs > Log patterns: The line chart shows the top 5 patterns over time. Use the time picker and query bar to adjust the results. Explore log patterns By default the log patterns UI first shows the most frequent occurrence of patterns. To sort to show the rarest patterns first, click the Count column. You can also use the query bar or attributes bar to filter your log patterns. If you want to... Do this... Understand the rate of change in patterns Look at the line chart. The color-coded patterns correspond to the plot column in the table. You can toggle individual plot patterns to narrow your focus. See the individual log messages that match each pattern Click pattern to expand the row and see a table of individual log records. To see additional records, scroll up or down. To explore an individual log in more detail, click it to open the details panel. Group and filter patterns by their attributes Use the query bar and time picker. As you apply different filters and time windows, the log patterns adjust to your new target data. Create an alert from a pattern Add the pattern to the query bar and run the query. Then click Create alert condition in the left nav. Troubleshoot log messages that haven't been clustered into a pattern Use the Logs with no pattern tab in the Log patterns UI. Clicking a specific log message will open the log message details panel you're familiar with from the Logs management page. Explore logs with no pattern The Logs with no pattern tab groups all recent log messages in your account that were not clustered into a known pattern yet. These log messages don't represent any problem or flaw in the system; they have no pattern because they are too new to have been processed by the machine learning system. This makes them valuable to explore when you want to understand what has recently changed in your environment. Logs > Log patterns: New Relic's log patterns feature automatically groups logs without a matching pattern. For example: Are any of these logs tied to a recent problem? This is a quick way to discover unique log data that is appearing for the first time in your environment. Does your log data have a new format? Sometimes the logs don't represent a problem, but a new format of log data that deviates from the data model you expect your applications to follow. Catching these logs early gives you the opportunity to ask developers to correct any deviations in their log output. The more consistent people are in the way log data is generated, the easier it becomes to use logs across a diverse set of teams. Masked attributes and wildcards Parts of the log messages in patterns are classified as variables and are substituted by masked attributes. The masking process supports and improves the clustering phase by allowing the algorithm to ignore changing details and focus on the repetitive structure. Masked attributes include: date_time ip url uuid Masked attributes are highlighted and are easy to identify, as shown in the following example. Here is an example of a log pattern that has masked attributes. Log patterns extract other less trivial variables that don't belong to any masked attribute. These variables are indicated as wildcards *. Here is an example of how wildcards * group variables in the Log patterns UI. Troubleshooting Here are a few reasons why you might have patterns enabled but not see any pattern data. If you're sure none of the items below are true, get help from support.newrelic.com. No data has arrived in the timeframe you're observing. Try expanding the time range you're viewing with the time picker. It's been less than 24 hours since patterns were enabled in the account. This means the ML model may not be generated for the account yet. None of the data coming in has a message field. Patterns will only be generated for values in the message field of a log record. If your logs don't contain message, there will be no data. Put the platform to work with patterns Patterns are a value that is enriched onto the existing log message as a new attribute named newrelic.logPattern. Anything you can do with logs generally can be done with log patterns, such as: Build your own dashboards with patterns, to monitor a specific pattern or group of patterns you care about. Create alerts for patterns by adding NRQL alerts. Use baseline alert conditions to detect anomalies in known log patterns.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 219.63745,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Discover value in <em>log</em> data with patterns",
        "sections": "Explore <em>logs</em> with no pattern",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " messages that haven&#x27;t been clustered into a pattern Use the <em>Logs</em> with no pattern tab in the <em>Log</em> patterns UI. Clicking a specific <em>log</em> message will open the <em>log</em> message details panel you&#x27;re familiar with from the <em>Logs</em> <em>management</em> page. Explore <em>logs</em> with no pattern The <em>Logs</em> with no pattern tab groups all"
      },
      "id": "6072d46128ccbc244451c18b"
    },
    {
      "sections": [
        "Parsing log data",
        "Example",
        "How log parsing works",
        "Important",
        "Limits",
        "Tip",
        "Built-in parsing rulesets",
        "List of built-in rulesets",
        "Add the logtype attribute",
        "New Relic infrastructure agent example",
        "Fluentd example",
        "Fluent Bit example",
        "Logstash example",
        "Logs API example",
        "Create custom parsing rules",
        "Troubleshooting"
      ],
      "title": "Parsing log data",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "52955adb68242c4ca582ba9cb8e22963955a8275",
      "image": "https://docs.newrelic.com/static/dc392bb7142d2fdb253a649daf4ebe6d/c1b63/log-parsing-rule-ui.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/parsing/",
      "published_at": "2021-10-19T03:43:56Z",
      "updated_at": "2021-10-19T03:43:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Parsing is the process of splitting unstructured log data into attributes (key/value pairs). You can use these attributes to facet or filter logs in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial on YouTube (approx. 4-1/2 minutes). New Relic parses log data according to rules. This document describes how logs parsing works, how to use built-in rules, and how to create custom rules. You can also create, query, and manage your log parsing rules by using NerdGraph, our GraphQL API, at api.newrelic.com/graphiql. For more information, see our NerdGraph tutorial for parsing. Example A good example is a default NGINX access log containing unstructured text. It is useful for searching but not much else. Here's an example of a typical line: 127.180.71.3 - - [10/May/1997:08:05:32 +0000] \"GET /downloads/product_1 HTTP/1.1\" 304 0 \"-\" \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" Copy In an unparsed format, you would need to do a full text search to answer most questions. After parsing, the log is organized into attributes, like response code and request URL: { \"remote_addr\":\"93.180.71.3\", \"time\":\"1586514731\", \"method\":\"GET\", \"path\":\"/downloads/product_1\", \"version\":\"HTTP/1.1\", \"response\":\"304\", \"bytesSent\": 0, \"user_agent\": \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" } Copy Parsing makes it easier to create custom queries that facet on those values. This helps you understand the distribution of response codes per request URL and quickly find problematic pages. How log parsing works Here's an overview of how New Relic implements parsing of logs: Log parsing How it works What All parsing takes place against the message field; no other fields can be parsed. Each parsing rule is created with matching criteria that determines which logs the rule will attempt to parse. To simplify the matching process, we recommend adding a logtype attribute to your logs. However, you are not limited to using logtype; any attribute can be used as matching criteria. When Parsing will only be applied once to each log message. If multiple parsing rules match the log, only the first that succeeds will be applied. Parsing takes place during log ingestion, before data is written to NRDB. Once data has been written to storage, it can no longer be parsed. Parsing occurs in the pipeline before data enrichments take place. Be careful when defining the matching criteria for a parsing rule. If the criteria is based on an attribute that doesn't exist untail after parsing or enrichment take place, that data won't be present in the logs when matching occurs. As a result, no parsing will happen. How Rules can be written in Grok, regex, or a mixture of the two. Grok is a collection of patterns that abstract away complicated regular expressions. If the content of the message field is JSON, it will be parsed automatically. New Relic's log ingestion pipeline can parse data by matching a log event to a rule that describes how the log should be parsed. There are two ways log events can be parsed: Use a built-in rule. Define a custom rule. Rules are a combination of matching logic and parsing logic. Matching is done by defining a query match on an attribute of the logs. Rules are not applied retroactively. Logs collected before a rule is created are not parsed by that rule. The simplest way to organize your logs and how they are parsed is to include the logtype field in your log event. This tells New Relic what built-in ruleset to apply to the logs. Important Once a parsing rule is active, data parsed by the rule is permanently changed. This cannot be reverted. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. Limits Parsing is computationally expensive, which introduces risk. Parsing is done for custom rules defined in an account and for matching patterns to a log. A large number of patterns or poorly defined custom rules will consume a huge amount of memory and CPU resources while also taking a very long time to complete. In order to prevent problems, we apply two parsing limits: per-message-per-rule and per-account. Limit Description Per-message-per-rule The per-message-per-rule limit prevents the time spent parsing any single message from being greater than 100 ms. If that limit is reached, the system will cease attempting to parse the log message with that rule. The ingestion pipeline will attempt to run any other applicable on that message, and the message will still be passed through the ingestion pipeline and stored in NRDB. The log message will be in its original, unparsed format. Per-account The per-account limit exists to prevent accounts from using more than their fair share of resources. The limit considers the total time spent processing all log messages for an account per-minute. The limit is not a fixed value; it scales up or down proportionally to the volume of data stored daily by the account and the environment size that is subsequently allocated to support that customer. Tip To easily check if your rate limits have been reached, go to your system Limits page in the New Relic UI. Built-in parsing rulesets Common log formats have well-established parsing rules already created for them. To get the benefit of built-in parsing rules, add the logtype attribute when forwarding logs. Set the value to something listed in the following table, and the rules for that type of log will be applied automatically. List of built-in rulesets The following logtype attribute values map to a predefined parsing rule. For example, to query the Application Load Balancer: From the New Relic UI, use the format logtype: alb. From NerdGraph, use the format logtype = 'alb'. To learn what fields are parsed for each rule, see our documentation about built-in parsing rules. logtype Example matching query alb AWS Application Load Balancer logtype:alb apache Apache Access logtype:apache cloudfront-web CloudFront Web logtype:cloudfront-web elb Amazon Elastic Load Balancer logtype:elb iis_w3c Microsoft IIS server logs - W3C format logtype:iis_w3c monit Monit logs logtype:monit mysql-error MySQL Error logtype:mysql-error nginx NGINX access logs logtype:nginx nginx-error NGINX error logs logtype:nginx-error route-53 Amazon Route 53 logs logtype:route-53 syslog-rfc5424 Syslog logtype:syslog-rfc5424 Add the logtype attribute When aggregating logs, it's important to provide metadata that makes it easy to organize, search, and parse those logs. One simple way of doing this is to add the attribute logtype to the log messages when they are shipped. Built-in parsing rules are applied by default to certain logtype values. Here are some examples of how to add logtype to logs sent by some of our supported shipping methods. New Relic infrastructure agent example Add logtype as an attribute. You must set the logtype for each named source. Learn more about using the infrastructure agent to add attributes. logs: - name: file-simple file: /path/to/file attributes: logtype: fileRaw - name: nginx-example file: /var/log/nginx.log attributes: logtype: nginx Copy Fluentd example Add a filter block to the .conf file, which uses a record_transformer to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluentd examples. <filter containers> @type record_transformer enable_ruby true <record> #Add logtype to trigger a built-in parsing rule for nginx access logs logtype nginx #Set timestamp from the value contained in the field \"time\" timestamp record[\"time\"] #Add hostname and tag fields to all records hostname \"#{Socket.gethostname}\" tag ${tag} </record> </filter> Copy Fluent Bit example Add a filter block to the .conf file that uses a record_modifier to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluent Bit examples. [FILTER] Name record_modifier Match * Record logtype nginx Record hostname ${HOSTNAME} Record service_name Sample-App-Name Copy Logstash example Add a filter block to the Logstash configuration which uses an add_field mutate filter to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Logstash examples. filter { mutate { add_field => { \"logtype\" => \"nginx\" \"service_name\" => \"myservicename\" \"hostname\" => \"%{host}\" } } } Copy Logs API example You can add attributes to the JSON request sent to New Relic. In this example we add a logtype attribute of value nginx to trigger the built-in NGINX parsing rule. Learn more about using the Logs API. POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: YOUR_LICENSE_KEY Accept: */* Content-Length: 133 { \"timestamp\": TIMESTAMP_IN_UNIX_EPOCH, \"message\": \"User 'xyz' logged in\", \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } Copy Create custom parsing rules Many logs are formatted or structured in a unique way. In order to parse them, custom logic must be built and applied. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. To create and manage your own, custom parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing, then click Create parsing rule. Enter the parsing rule's name. Choose an attribute and value to match on. Write your Grok pattern and test the rule. To learn about Grok and custom parsing rules, read our blog post about how to parse logs with Grok patterns. Enable and save the custom parsing rule. To view the list of custom parsing rules: From Manage Data on the left nav of the Logs UI, click Parsing. To view existing parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing. Troubleshooting If parsing is not working the way you intended, it may be due to: Logic: The parsing rule matching logic does not match the logs you want. Timing: If your parsing matching rule targets a value that doesn't exist yet, it will fail. This can occur if the value is added later in the pipeline as part of the enrichment process. Limits: There is a fixed amount of time available every minute to process logs via parsing, patterns, drop filters, etc. If the maximum amount of time has been spent, parsing will be skipped for additional log event records. To resolve these problems, create or adjust your custom parsing rules.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 212.20901,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Parsing <em>log</em> data",
        "sections": "<em>Logstash</em> example",
        "tags": "<em>Log</em> <em>management</em>",
        "body": "Parsing is the process of splitting unstructured <em>log</em> data into attributes (key&#x2F;value pairs). You can use these attributes to facet or filter <em>logs</em> in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial"
      },
      "id": "603e7eb4196a67b0c4a83dd1"
    }
  ],
  "/docs/logs/log-management/troubleshooting/json-message-not-parsed": [
    {
      "sections": [
        "Introduction to the Log API",
        "HTTP endpoint",
        "HTTP setup",
        "HTTP headers",
        "HTTP query parameters",
        "JSON body",
        "Simplified JSON body message",
        "Detailed JSON body message",
        "Limits and restricted characters",
        "Caution",
        "Important",
        "Rate limit violations",
        "HTTP requests per minute",
        "JSON bytes per minute",
        "Log payload format",
        "JSON message attributes",
        "Common block attributes",
        "Logs block attributes",
        "JSON message attribute parsing",
        "Log JSON examples",
        "Log POST message example",
        "JSON POST request example",
        "What's next?"
      ],
      "title": "Introduction to the Log API",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "Log API"
      ],
      "external_id": "198ebbf54f4a13fdf2f5b0f19d8cc8677afd09a2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/log-management/log-api/introduction-log-api/",
      "published_at": "2021-10-19T03:43:55Z",
      "updated_at": "2021-10-19T03:43:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If our log forwarding solutions don't meet your needs, you can use our Log API to send log data directly to New Relic via an HTTP endpoint. Want to try out our Log API? Create a New Relic account for free! No credit card required. HTTP endpoint Use the endpoint that's applicable for your New Relic account: United States (US) endpoint: https://log-api.newrelic.com/log/v1 Copy European Union (EU) endpoint: https://log-api.eu.newrelic.com/log/v1 Copy HTTP setup To send log data to your New Relic account via the Log API: Get your New Relic license key. Review the limits and restricted characters for your JSON payload. Generate the JSON message using the required headers and body fields. Ensure that your Api-Key or License-Key is included in your headers or query parameters. Refer to the log JSON examples. Send your JSON message to the appropriate HTTP endpoint for your New Relic account in a POST request. US: https://log-api.newrelic.com/log/v1 EU: https://log-api.eu.newrelic.com/log/v1 Generate some traffic and wait a few minutes, then check your account for data. If no data appears after you enable our log management capabilities, follow our troubleshooting procedures. HTTP headers When creating your HTTP headers, use these guidelines: Header Supported values Content-Type Required application/json json application/gzip gzip Api-Key Required A New Relic license key. You can also send this via query parameter. You can also use an Insights insert key but the license key is preferred. Gzipped JSON formatting is accepted. If sending compressed JSON, please include the Content-Type: application/json and Content-Encoding: gzip headers. HTTP query parameters The license key can also be passed as a query string parameter. This can be useful when sending logs from cloud-based sources that don't allow custom HTTP request headers. Query parameter Value Api-Key Your license key. Use this key whenever you send a header. You can also use an Insights insert key but the license key is preferred. JSON body You can send your JSON message using either a simplified or detailed set of attributes: Simplified JSON body message When using the simplified format to create your JSON message, send a single JSON object with the following: Field Value type Format Required Notes \"timestamp\" Integer Either milliseconds or seconds since epoch No If the field is not specific as millisecond or seconds since epoch, the message will be timestamped using the ingest time \"message\" String any string No This is the main log message field that is searched by default \"logtype\" String any string No Primary field for identifying logs and matching parsing rules other_fields (must not contain white space) String any string No These will become attributes of the log message Note: Log management does not support white space in attribute names Detailed JSON body message When using the detailed format to create your body, it must be a JSON array containing one or more JSON objects, each of which with the following format: Field Value type Format Required Notes \"common\" Object See common. No Any attributes that are common to all log messages \"logs\" Array See logs. Yes Array with the log entries Limits and restricted characters Caution Avoid calling our API from within the code of a customer-facing application. This can cause performance issues or block your application if response time is slow. If you need to do it this way, call our API asynchronously to avoid these performance issues. Restrictions on logs sent to the Log API: Payload total size: 1MB(10^6 bytes) maximum per POST. We highly recommend using compression. The payload must be encoded as UTF-8. Number of attributes per event: 255 maximum Length of attribute name: 255 characters Length of attribute value: 4096 maximum character length Some specific attributes have additional restrictions: accountId: This is a reserved attribute name. If it is included, it will be dropped during ingest. entity.guid, entity.name, and entity.type: These attributes are used internally to identify entities. Any values submitted with these keys in the attributes section of a metric data point may cause undefined behavior such as missing entities in the UI or telemetry not associating with the expected entities. For more information please refer to Entity synthesis. eventType: This is a reserved attribute name. If it is included, it will be dropped during ingest. timestamp: Must be a Unix epoch timestamp. You can define timestamps either in seconds or in milliseconds. Important Payloads with timestamps older than 48 hours may be dropped. Rate limits on logs sent to the Log API: Maximum rate for HTTP requests sent to the Log API: 300,000 requests per minute Maximum rate of uncompressed Log JSON bytes sent to the Log API: 10 GB per minute Rate limit violations Exceeding rate limits affects how the Log API behaves. Follow these instructions if this happens. HTTP requests per minute When the maximum request rate limit is exceeded for an account, the New Relic Log API returns a 429 response for the remainder of the minute. This response includes a Retry-After header indicating how long to wait in seconds before resubmitting or sending new data. To resolve this issue, either reduce the number of data points you are sending, or request a rate limit change. Subsequent subscription changes do not impact modified rate limits. If an account change impacts your rate limit, you must notify us to adjust your rate limit. To request rate limit changes, contact your New Relic account representative, or visit our Support portal. JSON bytes per minute When the maximum Log JSON byte limit is exceeded for an account, the New Relic Log API returns a 429 response for the remainder of the minute. This response includes a Retry-After header indicating how long to wait in seconds before resubmitting or sending new data. To resolve this issue, try to reduce the amount of log data you are sending, or spread it out over a larger period of time. To request rate limit changes, contact your New Relic account representative, or visit our Support portal. Log payload format We accept any valid JSON payload. The payload must encoded as UTF-8. Important Log management does not support white space in attribute names. For example, {\"Sample Attribute\": \"Value\"} would cause errors. JSON message attributes Common block attributes This is a block containing attributes that will be common to all log entries in logs: Field Value type Format Required Notes \"timestamp\" Integer Milliseconds or seconds since epoch No Message timestamp default to ingest time \"attributes\" Object JSON No This sub-object contains all other attributes of the message Logs block attributes This is an array containing log entries with the following format: Field Value type Format Required Notes \"timestamp\" Integer Milliseconds or seconds since epoch No Message timestamp default to ingest time \"attributes\" Object JSON No This sub-object contains all other attributes of the message \"message\" String (any string) Yes This is the main log message field that is searched by default \"log\" String (any string) No We will rewrite this string as the field message on ingest \"LOG\" String (any string) No We will rewrite this string as the field message on ingest \"MESSAGE\" String (any string) No We will rewrite this string as the field message on ingest JSON message attribute parsing Our log management capabilities will parse any message attribute as JSON. The resulting JSON attributes in the parsed message will be added to the event. If the message attribute is not JSON, it is left as is. Important New Relic's log management capabilities do not support white space in attribute names. For example, {\"Sample Attribute\": \"Value\"} would cause errors. Here is an example message attribute: { \"timestamp\": 1562767499238, \"message\": \"{\\\"service-name\\\": \\\"login-service\\\", \\\"user\\\": {\\\"id\\\": 123, \\\"name\\\": \\\"alice\\\"}}\" } Copy This will be treated as: { \"timestamp\": 1562767499238, \"message\": \"{\\\"service-name\\\": \\\"my-service\\\", \\\"user\\\": {\\\"id\\\": 123, \\\"name\\\": \\\"alice\\\"}}\", \"service-name\": \"my-service\", \"user\": { \"id\": 123, \"name\": \"alice\" } } Copy Log JSON examples Attributes can be scalar JSON types like string and number. They can also be compound (or nested) objects. Compound attributes will have their associated attributes stored with flattened names. For example, here is a compound user attribute in a log entry's attributes: \"attributes\": { \"action\": \"login\", \"user\": { \"id\": 123, \"name\": \"alice\" } } Copy This will result in the following attributes being stored with the log event: Attribute Value \"action\" \"login\" \"user.id\" 123 \"user.name\" \"alice\" Log POST message example Log POST message example: POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: <YOUR_LICENSE_KEY> Accept: */* Content-Length: 319 [{ \"common\": { \"attributes\": { \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } }, \"logs\": [{ \"timestamp\": <TIMESTAMP_IN_UNIX_EPOCH>, \"message\": \"User 'xyz' logged in\" },{ \"timestamp\": <TIMESTAMP_IN_UNIX_EPOCH>, \"message\": \"User 'xyz' logged out\", \"attributes\": { \"auditId\": 123 } }] }] Copy This POST message would result in the following log messages being stored in New Relic: Attribute Value \"logtype\" \"accesslogs\" \"service\" \"login-service\" \"hostname\" \"login.example.com\" Here's an example of stored logs block attributes: Attribute Value \"timestamp\" 1550086450124 \"message\" \"User 'xyz' logged out\" \"auditId\" 123 JSON POST request example Here's an example of a JSON POST request: POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: <YOUR_LICENSE_KEY> Accept: */* Content-Length: 133 { \"timestamp\": <TIMESTAMP_IN_UNIX_EPOCH>, \"message\": \"User 'xyz' logged in\", \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } Copy What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.53455,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to the <em>Log</em> API",
        "sections": "<em>Logs</em> block attributes",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " in a POST request. US: https:&#x2F;&#x2F;<em>log</em>-api.newrelic.com&#x2F;<em>log</em>&#x2F;v1 EU: https:&#x2F;&#x2F;<em>log</em>-api.eu.newrelic.com&#x2F;<em>log</em>&#x2F;v1 Generate some traffic and wait a few minutes, then check your account for data. If no data appears after you enable our <em>log</em> <em>management</em> capabilities, follow our <em>troubleshooting</em> procedures. HTTP headers"
      },
      "id": "603ea832196a6726e7a83da1"
    },
    {
      "sections": [
        "Discover value in log data with patterns",
        "Technical overview",
        "Availability",
        "Get started",
        "Explore log patterns",
        "Explore logs with no pattern",
        "Masked attributes and wildcards",
        "Troubleshooting",
        "Put the platform to work with patterns"
      ],
      "title": "Discover value in log data with patterns",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "8f1e27c94327ca4888a945f8e12f9c2310ccd7a6",
      "image": "https://docs.newrelic.com/static/578d7186bb34352855696e5307cc82f2/c1b63/log-patterns-logs-without-a-pattern.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/find-unusual-logs-log-patterns/",
      "published_at": "2021-10-19T03:56:24Z",
      "updated_at": "2021-10-19T03:56:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Log patterns are the fastest way to discover value in log data without searching. Log data is high volume telemetry with a low value per individual record. Searching can quickly lead to logs that provide a root cause explanation, but most data is repetitive and hard to contextualize when browsing. Patterns can make log data discoverable without spending a lot of time reading through low value data. Logs > Patterns: Use patterns as the basis for alerts when the frequency of important data changes, or for configuring drop rules to get rid of unnecessary repetitive data. Technical overview Log patterns functionality applies machine learning to normalize and group log messages that are consistent in format but variable in content. These grouped messages can be sorted, making it easy to find the most frequent or rarest sets of logs in your environment. Use patterns as the basis for alerts when the frequency of important data changes, or to configure drop rules to get rid of unnecessary repetitive data. Log patterns use advanced clustering algorithms to group together similar log messages automatically. With patterns, you can: Orient more quickly through millions of logs. Reduce the time it takes to identify unusual behavior in your log estate. Monitor the frequency of known patterns over time to focus your energy on what matters, and exclude what's irrelevant. Availability The ability to configure this feature is dependent on role-based permissions. For example, accounts still using our original pricing model require an Owner or Admin role. If you see Patterns are turned off in your Log management Patterns UI, click the Configure Patterns button and enable it. If you don't see patterns within 30 minutes of enabling the feature, there may be a lack of data with a message attribute for the system to create a pattern from it. Log patterns Limitations and considerations Pricing There is no separate pricing for log patterns. The only cost is for additional data generated and added to your log records. A pattern attribute will be added to all logs that match a pattern. Attributes also may be added when common values are discovered, such as GUIDs, IP addresses, URL, or email addresses. These attributes are automatically extracted from the log message as part of the pattern process. HITRUST accounts The log patterns feature is not FedRAMP compliant. FedRAMP or other HITRUST accounts are not eligible to use patterns. Parsing limits We have a system of safety limits on memory and CPU resources when processing logs and their patterns. These parsing limits can have an impact on the data you get. For more information, see our documentation about parsing limits. Get started To start examining patterns: Go to one.newrelic.com > Logs, and use the account picker dropdown to select the target account where you want to explore patterns. In the left navigation of the Logs UI, click Patterns. The main log UI changes to show patterns that match the query in the query bar. Logs > Log patterns: The line chart shows the top 5 patterns over time. Use the time picker and query bar to adjust the results. Explore log patterns By default the log patterns UI first shows the most frequent occurrence of patterns. To sort to show the rarest patterns first, click the Count column. You can also use the query bar or attributes bar to filter your log patterns. If you want to... Do this... Understand the rate of change in patterns Look at the line chart. The color-coded patterns correspond to the plot column in the table. You can toggle individual plot patterns to narrow your focus. See the individual log messages that match each pattern Click pattern to expand the row and see a table of individual log records. To see additional records, scroll up or down. To explore an individual log in more detail, click it to open the details panel. Group and filter patterns by their attributes Use the query bar and time picker. As you apply different filters and time windows, the log patterns adjust to your new target data. Create an alert from a pattern Add the pattern to the query bar and run the query. Then click Create alert condition in the left nav. Troubleshoot log messages that haven't been clustered into a pattern Use the Logs with no pattern tab in the Log patterns UI. Clicking a specific log message will open the log message details panel you're familiar with from the Logs management page. Explore logs with no pattern The Logs with no pattern tab groups all recent log messages in your account that were not clustered into a known pattern yet. These log messages don't represent any problem or flaw in the system; they have no pattern because they are too new to have been processed by the machine learning system. This makes them valuable to explore when you want to understand what has recently changed in your environment. Logs > Log patterns: New Relic's log patterns feature automatically groups logs without a matching pattern. For example: Are any of these logs tied to a recent problem? This is a quick way to discover unique log data that is appearing for the first time in your environment. Does your log data have a new format? Sometimes the logs don't represent a problem, but a new format of log data that deviates from the data model you expect your applications to follow. Catching these logs early gives you the opportunity to ask developers to correct any deviations in their log output. The more consistent people are in the way log data is generated, the easier it becomes to use logs across a diverse set of teams. Masked attributes and wildcards Parts of the log messages in patterns are classified as variables and are substituted by masked attributes. The masking process supports and improves the clustering phase by allowing the algorithm to ignore changing details and focus on the repetitive structure. Masked attributes include: date_time ip url uuid Masked attributes are highlighted and are easy to identify, as shown in the following example. Here is an example of a log pattern that has masked attributes. Log patterns extract other less trivial variables that don't belong to any masked attribute. These variables are indicated as wildcards *. Here is an example of how wildcards * group variables in the Log patterns UI. Troubleshooting Here are a few reasons why you might have patterns enabled but not see any pattern data. If you're sure none of the items below are true, get help from support.newrelic.com. No data has arrived in the timeframe you're observing. Try expanding the time range you're viewing with the time picker. It's been less than 24 hours since patterns were enabled in the account. This means the ML model may not be generated for the account yet. None of the data coming in has a message field. Patterns will only be generated for values in the message field of a log record. If your logs don't contain message, there will be no data. Put the platform to work with patterns Patterns are a value that is enriched onto the existing log message as a new attribute named newrelic.logPattern. Anything you can do with logs generally can be done with log patterns, such as: Build your own dashboards with patterns, to monitor a specific pattern or group of patterns you care about. Create alerts for patterns by adding NRQL alerts. Use baseline alert conditions to detect anomalies in known log patterns.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 219.6373,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Discover value in <em>log</em> data with patterns",
        "sections": "Explore <em>logs</em> with no pattern",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " messages that haven&#x27;t been clustered into a pattern Use the <em>Logs</em> with no pattern tab in the <em>Log</em> patterns UI. Clicking a specific <em>log</em> message will open the <em>log</em> message details panel you&#x27;re familiar with from the <em>Logs</em> <em>management</em> page. Explore <em>logs</em> with no pattern The <em>Logs</em> with no pattern tab groups all"
      },
      "id": "6072d46128ccbc244451c18b"
    },
    {
      "sections": [
        "Parsing log data",
        "Example",
        "How log parsing works",
        "Important",
        "Limits",
        "Tip",
        "Built-in parsing rulesets",
        "List of built-in rulesets",
        "Add the logtype attribute",
        "New Relic infrastructure agent example",
        "Fluentd example",
        "Fluent Bit example",
        "Logstash example",
        "Logs API example",
        "Create custom parsing rules",
        "Troubleshooting"
      ],
      "title": "Parsing log data",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "52955adb68242c4ca582ba9cb8e22963955a8275",
      "image": "https://docs.newrelic.com/static/dc392bb7142d2fdb253a649daf4ebe6d/c1b63/log-parsing-rule-ui.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/parsing/",
      "published_at": "2021-10-19T03:43:56Z",
      "updated_at": "2021-10-19T03:43:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Parsing is the process of splitting unstructured log data into attributes (key/value pairs). You can use these attributes to facet or filter logs in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial on YouTube (approx. 4-1/2 minutes). New Relic parses log data according to rules. This document describes how logs parsing works, how to use built-in rules, and how to create custom rules. You can also create, query, and manage your log parsing rules by using NerdGraph, our GraphQL API, at api.newrelic.com/graphiql. For more information, see our NerdGraph tutorial for parsing. Example A good example is a default NGINX access log containing unstructured text. It is useful for searching but not much else. Here's an example of a typical line: 127.180.71.3 - - [10/May/1997:08:05:32 +0000] \"GET /downloads/product_1 HTTP/1.1\" 304 0 \"-\" \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" Copy In an unparsed format, you would need to do a full text search to answer most questions. After parsing, the log is organized into attributes, like response code and request URL: { \"remote_addr\":\"93.180.71.3\", \"time\":\"1586514731\", \"method\":\"GET\", \"path\":\"/downloads/product_1\", \"version\":\"HTTP/1.1\", \"response\":\"304\", \"bytesSent\": 0, \"user_agent\": \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" } Copy Parsing makes it easier to create custom queries that facet on those values. This helps you understand the distribution of response codes per request URL and quickly find problematic pages. How log parsing works Here's an overview of how New Relic implements parsing of logs: Log parsing How it works What All parsing takes place against the message field; no other fields can be parsed. Each parsing rule is created with matching criteria that determines which logs the rule will attempt to parse. To simplify the matching process, we recommend adding a logtype attribute to your logs. However, you are not limited to using logtype; any attribute can be used as matching criteria. When Parsing will only be applied once to each log message. If multiple parsing rules match the log, only the first that succeeds will be applied. Parsing takes place during log ingestion, before data is written to NRDB. Once data has been written to storage, it can no longer be parsed. Parsing occurs in the pipeline before data enrichments take place. Be careful when defining the matching criteria for a parsing rule. If the criteria is based on an attribute that doesn't exist untail after parsing or enrichment take place, that data won't be present in the logs when matching occurs. As a result, no parsing will happen. How Rules can be written in Grok, regex, or a mixture of the two. Grok is a collection of patterns that abstract away complicated regular expressions. If the content of the message field is JSON, it will be parsed automatically. New Relic's log ingestion pipeline can parse data by matching a log event to a rule that describes how the log should be parsed. There are two ways log events can be parsed: Use a built-in rule. Define a custom rule. Rules are a combination of matching logic and parsing logic. Matching is done by defining a query match on an attribute of the logs. Rules are not applied retroactively. Logs collected before a rule is created are not parsed by that rule. The simplest way to organize your logs and how they are parsed is to include the logtype field in your log event. This tells New Relic what built-in ruleset to apply to the logs. Important Once a parsing rule is active, data parsed by the rule is permanently changed. This cannot be reverted. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. Limits Parsing is computationally expensive, which introduces risk. Parsing is done for custom rules defined in an account and for matching patterns to a log. A large number of patterns or poorly defined custom rules will consume a huge amount of memory and CPU resources while also taking a very long time to complete. In order to prevent problems, we apply two parsing limits: per-message-per-rule and per-account. Limit Description Per-message-per-rule The per-message-per-rule limit prevents the time spent parsing any single message from being greater than 100 ms. If that limit is reached, the system will cease attempting to parse the log message with that rule. The ingestion pipeline will attempt to run any other applicable on that message, and the message will still be passed through the ingestion pipeline and stored in NRDB. The log message will be in its original, unparsed format. Per-account The per-account limit exists to prevent accounts from using more than their fair share of resources. The limit considers the total time spent processing all log messages for an account per-minute. The limit is not a fixed value; it scales up or down proportionally to the volume of data stored daily by the account and the environment size that is subsequently allocated to support that customer. Tip To easily check if your rate limits have been reached, go to your system Limits page in the New Relic UI. Built-in parsing rulesets Common log formats have well-established parsing rules already created for them. To get the benefit of built-in parsing rules, add the logtype attribute when forwarding logs. Set the value to something listed in the following table, and the rules for that type of log will be applied automatically. List of built-in rulesets The following logtype attribute values map to a predefined parsing rule. For example, to query the Application Load Balancer: From the New Relic UI, use the format logtype: alb. From NerdGraph, use the format logtype = 'alb'. To learn what fields are parsed for each rule, see our documentation about built-in parsing rules. logtype Example matching query alb AWS Application Load Balancer logtype:alb apache Apache Access logtype:apache cloudfront-web CloudFront Web logtype:cloudfront-web elb Amazon Elastic Load Balancer logtype:elb iis_w3c Microsoft IIS server logs - W3C format logtype:iis_w3c monit Monit logs logtype:monit mysql-error MySQL Error logtype:mysql-error nginx NGINX access logs logtype:nginx nginx-error NGINX error logs logtype:nginx-error route-53 Amazon Route 53 logs logtype:route-53 syslog-rfc5424 Syslog logtype:syslog-rfc5424 Add the logtype attribute When aggregating logs, it's important to provide metadata that makes it easy to organize, search, and parse those logs. One simple way of doing this is to add the attribute logtype to the log messages when they are shipped. Built-in parsing rules are applied by default to certain logtype values. Here are some examples of how to add logtype to logs sent by some of our supported shipping methods. New Relic infrastructure agent example Add logtype as an attribute. You must set the logtype for each named source. Learn more about using the infrastructure agent to add attributes. logs: - name: file-simple file: /path/to/file attributes: logtype: fileRaw - name: nginx-example file: /var/log/nginx.log attributes: logtype: nginx Copy Fluentd example Add a filter block to the .conf file, which uses a record_transformer to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluentd examples. <filter containers> @type record_transformer enable_ruby true <record> #Add logtype to trigger a built-in parsing rule for nginx access logs logtype nginx #Set timestamp from the value contained in the field \"time\" timestamp record[\"time\"] #Add hostname and tag fields to all records hostname \"#{Socket.gethostname}\" tag ${tag} </record> </filter> Copy Fluent Bit example Add a filter block to the .conf file that uses a record_modifier to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluent Bit examples. [FILTER] Name record_modifier Match * Record logtype nginx Record hostname ${HOSTNAME} Record service_name Sample-App-Name Copy Logstash example Add a filter block to the Logstash configuration which uses an add_field mutate filter to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Logstash examples. filter { mutate { add_field => { \"logtype\" => \"nginx\" \"service_name\" => \"myservicename\" \"hostname\" => \"%{host}\" } } } Copy Logs API example You can add attributes to the JSON request sent to New Relic. In this example we add a logtype attribute of value nginx to trigger the built-in NGINX parsing rule. Learn more about using the Logs API. POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: YOUR_LICENSE_KEY Accept: */* Content-Length: 133 { \"timestamp\": TIMESTAMP_IN_UNIX_EPOCH, \"message\": \"User 'xyz' logged in\", \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } Copy Create custom parsing rules Many logs are formatted or structured in a unique way. In order to parse them, custom logic must be built and applied. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. To create and manage your own, custom parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing, then click Create parsing rule. Enter the parsing rule's name. Choose an attribute and value to match on. Write your Grok pattern and test the rule. To learn about Grok and custom parsing rules, read our blog post about how to parse logs with Grok patterns. Enable and save the custom parsing rule. To view the list of custom parsing rules: From Manage Data on the left nav of the Logs UI, click Parsing. To view existing parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing. Troubleshooting If parsing is not working the way you intended, it may be due to: Logic: The parsing rule matching logic does not match the logs you want. Timing: If your parsing matching rule targets a value that doesn't exist yet, it will fail. This can occur if the value is added later in the pipeline as part of the enrichment process. Limits: There is a fixed amount of time available every minute to process logs via parsing, patterns, drop filters, etc. If the maximum amount of time has been spent, parsing will be skipped for additional log event records. To resolve these problems, create or adjust your custom parsing rules.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 212.20886,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Parsing <em>log</em> data",
        "sections": "<em>Logstash</em> example",
        "tags": "<em>Log</em> <em>management</em>",
        "body": "Parsing is the process of splitting unstructured <em>log</em> data into attributes (key&#x2F;value pairs). You can use these attributes to facet or filter <em>logs</em> in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial"
      },
      "id": "603e7eb4196a67b0c4a83dd1"
    }
  ],
  "/docs/logs/log-management/troubleshooting/log-message-truncated": [
    {
      "sections": [
        "Introduction to the Log API",
        "HTTP endpoint",
        "HTTP setup",
        "HTTP headers",
        "HTTP query parameters",
        "JSON body",
        "Simplified JSON body message",
        "Detailed JSON body message",
        "Limits and restricted characters",
        "Caution",
        "Important",
        "Rate limit violations",
        "HTTP requests per minute",
        "JSON bytes per minute",
        "Log payload format",
        "JSON message attributes",
        "Common block attributes",
        "Logs block attributes",
        "JSON message attribute parsing",
        "Log JSON examples",
        "Log POST message example",
        "JSON POST request example",
        "What's next?"
      ],
      "title": "Introduction to the Log API",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "Log API"
      ],
      "external_id": "198ebbf54f4a13fdf2f5b0f19d8cc8677afd09a2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/log-management/log-api/introduction-log-api/",
      "published_at": "2021-10-19T03:43:55Z",
      "updated_at": "2021-10-19T03:43:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If our log forwarding solutions don't meet your needs, you can use our Log API to send log data directly to New Relic via an HTTP endpoint. Want to try out our Log API? Create a New Relic account for free! No credit card required. HTTP endpoint Use the endpoint that's applicable for your New Relic account: United States (US) endpoint: https://log-api.newrelic.com/log/v1 Copy European Union (EU) endpoint: https://log-api.eu.newrelic.com/log/v1 Copy HTTP setup To send log data to your New Relic account via the Log API: Get your New Relic license key. Review the limits and restricted characters for your JSON payload. Generate the JSON message using the required headers and body fields. Ensure that your Api-Key or License-Key is included in your headers or query parameters. Refer to the log JSON examples. Send your JSON message to the appropriate HTTP endpoint for your New Relic account in a POST request. US: https://log-api.newrelic.com/log/v1 EU: https://log-api.eu.newrelic.com/log/v1 Generate some traffic and wait a few minutes, then check your account for data. If no data appears after you enable our log management capabilities, follow our troubleshooting procedures. HTTP headers When creating your HTTP headers, use these guidelines: Header Supported values Content-Type Required application/json json application/gzip gzip Api-Key Required A New Relic license key. You can also send this via query parameter. You can also use an Insights insert key but the license key is preferred. Gzipped JSON formatting is accepted. If sending compressed JSON, please include the Content-Type: application/json and Content-Encoding: gzip headers. HTTP query parameters The license key can also be passed as a query string parameter. This can be useful when sending logs from cloud-based sources that don't allow custom HTTP request headers. Query parameter Value Api-Key Your license key. Use this key whenever you send a header. You can also use an Insights insert key but the license key is preferred. JSON body You can send your JSON message using either a simplified or detailed set of attributes: Simplified JSON body message When using the simplified format to create your JSON message, send a single JSON object with the following: Field Value type Format Required Notes \"timestamp\" Integer Either milliseconds or seconds since epoch No If the field is not specific as millisecond or seconds since epoch, the message will be timestamped using the ingest time \"message\" String any string No This is the main log message field that is searched by default \"logtype\" String any string No Primary field for identifying logs and matching parsing rules other_fields (must not contain white space) String any string No These will become attributes of the log message Note: Log management does not support white space in attribute names Detailed JSON body message When using the detailed format to create your body, it must be a JSON array containing one or more JSON objects, each of which with the following format: Field Value type Format Required Notes \"common\" Object See common. No Any attributes that are common to all log messages \"logs\" Array See logs. Yes Array with the log entries Limits and restricted characters Caution Avoid calling our API from within the code of a customer-facing application. This can cause performance issues or block your application if response time is slow. If you need to do it this way, call our API asynchronously to avoid these performance issues. Restrictions on logs sent to the Log API: Payload total size: 1MB(10^6 bytes) maximum per POST. We highly recommend using compression. The payload must be encoded as UTF-8. Number of attributes per event: 255 maximum Length of attribute name: 255 characters Length of attribute value: 4096 maximum character length Some specific attributes have additional restrictions: accountId: This is a reserved attribute name. If it is included, it will be dropped during ingest. entity.guid, entity.name, and entity.type: These attributes are used internally to identify entities. Any values submitted with these keys in the attributes section of a metric data point may cause undefined behavior such as missing entities in the UI or telemetry not associating with the expected entities. For more information please refer to Entity synthesis. eventType: This is a reserved attribute name. If it is included, it will be dropped during ingest. timestamp: Must be a Unix epoch timestamp. You can define timestamps either in seconds or in milliseconds. Important Payloads with timestamps older than 48 hours may be dropped. Rate limits on logs sent to the Log API: Maximum rate for HTTP requests sent to the Log API: 300,000 requests per minute Maximum rate of uncompressed Log JSON bytes sent to the Log API: 10 GB per minute Rate limit violations Exceeding rate limits affects how the Log API behaves. Follow these instructions if this happens. HTTP requests per minute When the maximum request rate limit is exceeded for an account, the New Relic Log API returns a 429 response for the remainder of the minute. This response includes a Retry-After header indicating how long to wait in seconds before resubmitting or sending new data. To resolve this issue, either reduce the number of data points you are sending, or request a rate limit change. Subsequent subscription changes do not impact modified rate limits. If an account change impacts your rate limit, you must notify us to adjust your rate limit. To request rate limit changes, contact your New Relic account representative, or visit our Support portal. JSON bytes per minute When the maximum Log JSON byte limit is exceeded for an account, the New Relic Log API returns a 429 response for the remainder of the minute. This response includes a Retry-After header indicating how long to wait in seconds before resubmitting or sending new data. To resolve this issue, try to reduce the amount of log data you are sending, or spread it out over a larger period of time. To request rate limit changes, contact your New Relic account representative, or visit our Support portal. Log payload format We accept any valid JSON payload. The payload must encoded as UTF-8. Important Log management does not support white space in attribute names. For example, {\"Sample Attribute\": \"Value\"} would cause errors. JSON message attributes Common block attributes This is a block containing attributes that will be common to all log entries in logs: Field Value type Format Required Notes \"timestamp\" Integer Milliseconds or seconds since epoch No Message timestamp default to ingest time \"attributes\" Object JSON No This sub-object contains all other attributes of the message Logs block attributes This is an array containing log entries with the following format: Field Value type Format Required Notes \"timestamp\" Integer Milliseconds or seconds since epoch No Message timestamp default to ingest time \"attributes\" Object JSON No This sub-object contains all other attributes of the message \"message\" String (any string) Yes This is the main log message field that is searched by default \"log\" String (any string) No We will rewrite this string as the field message on ingest \"LOG\" String (any string) No We will rewrite this string as the field message on ingest \"MESSAGE\" String (any string) No We will rewrite this string as the field message on ingest JSON message attribute parsing Our log management capabilities will parse any message attribute as JSON. The resulting JSON attributes in the parsed message will be added to the event. If the message attribute is not JSON, it is left as is. Important New Relic's log management capabilities do not support white space in attribute names. For example, {\"Sample Attribute\": \"Value\"} would cause errors. Here is an example message attribute: { \"timestamp\": 1562767499238, \"message\": \"{\\\"service-name\\\": \\\"login-service\\\", \\\"user\\\": {\\\"id\\\": 123, \\\"name\\\": \\\"alice\\\"}}\" } Copy This will be treated as: { \"timestamp\": 1562767499238, \"message\": \"{\\\"service-name\\\": \\\"my-service\\\", \\\"user\\\": {\\\"id\\\": 123, \\\"name\\\": \\\"alice\\\"}}\", \"service-name\": \"my-service\", \"user\": { \"id\": 123, \"name\": \"alice\" } } Copy Log JSON examples Attributes can be scalar JSON types like string and number. They can also be compound (or nested) objects. Compound attributes will have their associated attributes stored with flattened names. For example, here is a compound user attribute in a log entry's attributes: \"attributes\": { \"action\": \"login\", \"user\": { \"id\": 123, \"name\": \"alice\" } } Copy This will result in the following attributes being stored with the log event: Attribute Value \"action\" \"login\" \"user.id\" 123 \"user.name\" \"alice\" Log POST message example Log POST message example: POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: <YOUR_LICENSE_KEY> Accept: */* Content-Length: 319 [{ \"common\": { \"attributes\": { \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } }, \"logs\": [{ \"timestamp\": <TIMESTAMP_IN_UNIX_EPOCH>, \"message\": \"User 'xyz' logged in\" },{ \"timestamp\": <TIMESTAMP_IN_UNIX_EPOCH>, \"message\": \"User 'xyz' logged out\", \"attributes\": { \"auditId\": 123 } }] }] Copy This POST message would result in the following log messages being stored in New Relic: Attribute Value \"logtype\" \"accesslogs\" \"service\" \"login-service\" \"hostname\" \"login.example.com\" Here's an example of stored logs block attributes: Attribute Value \"timestamp\" 1550086450124 \"message\" \"User 'xyz' logged out\" \"auditId\" 123 JSON POST request example Here's an example of a JSON POST request: POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: <YOUR_LICENSE_KEY> Accept: */* Content-Length: 133 { \"timestamp\": <TIMESTAMP_IN_UNIX_EPOCH>, \"message\": \"User 'xyz' logged in\", \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } Copy What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.53455,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to the <em>Log</em> API",
        "sections": "<em>Logs</em> block attributes",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " in a POST request. US: https:&#x2F;&#x2F;<em>log</em>-api.newrelic.com&#x2F;<em>log</em>&#x2F;v1 EU: https:&#x2F;&#x2F;<em>log</em>-api.eu.newrelic.com&#x2F;<em>log</em>&#x2F;v1 Generate some traffic and wait a few minutes, then check your account for data. If no data appears after you enable our <em>log</em> <em>management</em> capabilities, follow our <em>troubleshooting</em> procedures. HTTP headers"
      },
      "id": "603ea832196a6726e7a83da1"
    },
    {
      "sections": [
        "Discover value in log data with patterns",
        "Technical overview",
        "Availability",
        "Get started",
        "Explore log patterns",
        "Explore logs with no pattern",
        "Masked attributes and wildcards",
        "Troubleshooting",
        "Put the platform to work with patterns"
      ],
      "title": "Discover value in log data with patterns",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "8f1e27c94327ca4888a945f8e12f9c2310ccd7a6",
      "image": "https://docs.newrelic.com/static/578d7186bb34352855696e5307cc82f2/c1b63/log-patterns-logs-without-a-pattern.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/find-unusual-logs-log-patterns/",
      "published_at": "2021-10-19T03:56:24Z",
      "updated_at": "2021-10-19T03:56:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Log patterns are the fastest way to discover value in log data without searching. Log data is high volume telemetry with a low value per individual record. Searching can quickly lead to logs that provide a root cause explanation, but most data is repetitive and hard to contextualize when browsing. Patterns can make log data discoverable without spending a lot of time reading through low value data. Logs > Patterns: Use patterns as the basis for alerts when the frequency of important data changes, or for configuring drop rules to get rid of unnecessary repetitive data. Technical overview Log patterns functionality applies machine learning to normalize and group log messages that are consistent in format but variable in content. These grouped messages can be sorted, making it easy to find the most frequent or rarest sets of logs in your environment. Use patterns as the basis for alerts when the frequency of important data changes, or to configure drop rules to get rid of unnecessary repetitive data. Log patterns use advanced clustering algorithms to group together similar log messages automatically. With patterns, you can: Orient more quickly through millions of logs. Reduce the time it takes to identify unusual behavior in your log estate. Monitor the frequency of known patterns over time to focus your energy on what matters, and exclude what's irrelevant. Availability The ability to configure this feature is dependent on role-based permissions. For example, accounts still using our original pricing model require an Owner or Admin role. If you see Patterns are turned off in your Log management Patterns UI, click the Configure Patterns button and enable it. If you don't see patterns within 30 minutes of enabling the feature, there may be a lack of data with a message attribute for the system to create a pattern from it. Log patterns Limitations and considerations Pricing There is no separate pricing for log patterns. The only cost is for additional data generated and added to your log records. A pattern attribute will be added to all logs that match a pattern. Attributes also may be added when common values are discovered, such as GUIDs, IP addresses, URL, or email addresses. These attributes are automatically extracted from the log message as part of the pattern process. HITRUST accounts The log patterns feature is not FedRAMP compliant. FedRAMP or other HITRUST accounts are not eligible to use patterns. Parsing limits We have a system of safety limits on memory and CPU resources when processing logs and their patterns. These parsing limits can have an impact on the data you get. For more information, see our documentation about parsing limits. Get started To start examining patterns: Go to one.newrelic.com > Logs, and use the account picker dropdown to select the target account where you want to explore patterns. In the left navigation of the Logs UI, click Patterns. The main log UI changes to show patterns that match the query in the query bar. Logs > Log patterns: The line chart shows the top 5 patterns over time. Use the time picker and query bar to adjust the results. Explore log patterns By default the log patterns UI first shows the most frequent occurrence of patterns. To sort to show the rarest patterns first, click the Count column. You can also use the query bar or attributes bar to filter your log patterns. If you want to... Do this... Understand the rate of change in patterns Look at the line chart. The color-coded patterns correspond to the plot column in the table. You can toggle individual plot patterns to narrow your focus. See the individual log messages that match each pattern Click pattern to expand the row and see a table of individual log records. To see additional records, scroll up or down. To explore an individual log in more detail, click it to open the details panel. Group and filter patterns by their attributes Use the query bar and time picker. As you apply different filters and time windows, the log patterns adjust to your new target data. Create an alert from a pattern Add the pattern to the query bar and run the query. Then click Create alert condition in the left nav. Troubleshoot log messages that haven't been clustered into a pattern Use the Logs with no pattern tab in the Log patterns UI. Clicking a specific log message will open the log message details panel you're familiar with from the Logs management page. Explore logs with no pattern The Logs with no pattern tab groups all recent log messages in your account that were not clustered into a known pattern yet. These log messages don't represent any problem or flaw in the system; they have no pattern because they are too new to have been processed by the machine learning system. This makes them valuable to explore when you want to understand what has recently changed in your environment. Logs > Log patterns: New Relic's log patterns feature automatically groups logs without a matching pattern. For example: Are any of these logs tied to a recent problem? This is a quick way to discover unique log data that is appearing for the first time in your environment. Does your log data have a new format? Sometimes the logs don't represent a problem, but a new format of log data that deviates from the data model you expect your applications to follow. Catching these logs early gives you the opportunity to ask developers to correct any deviations in their log output. The more consistent people are in the way log data is generated, the easier it becomes to use logs across a diverse set of teams. Masked attributes and wildcards Parts of the log messages in patterns are classified as variables and are substituted by masked attributes. The masking process supports and improves the clustering phase by allowing the algorithm to ignore changing details and focus on the repetitive structure. Masked attributes include: date_time ip url uuid Masked attributes are highlighted and are easy to identify, as shown in the following example. Here is an example of a log pattern that has masked attributes. Log patterns extract other less trivial variables that don't belong to any masked attribute. These variables are indicated as wildcards *. Here is an example of how wildcards * group variables in the Log patterns UI. Troubleshooting Here are a few reasons why you might have patterns enabled but not see any pattern data. If you're sure none of the items below are true, get help from support.newrelic.com. No data has arrived in the timeframe you're observing. Try expanding the time range you're viewing with the time picker. It's been less than 24 hours since patterns were enabled in the account. This means the ML model may not be generated for the account yet. None of the data coming in has a message field. Patterns will only be generated for values in the message field of a log record. If your logs don't contain message, there will be no data. Put the platform to work with patterns Patterns are a value that is enriched onto the existing log message as a new attribute named newrelic.logPattern. Anything you can do with logs generally can be done with log patterns, such as: Build your own dashboards with patterns, to monitor a specific pattern or group of patterns you care about. Create alerts for patterns by adding NRQL alerts. Use baseline alert conditions to detect anomalies in known log patterns.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 219.6373,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Discover value in <em>log</em> data with patterns",
        "sections": "Explore <em>logs</em> with no pattern",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " messages that haven&#x27;t been clustered into a pattern Use the <em>Logs</em> with no pattern tab in the <em>Log</em> patterns UI. Clicking a specific <em>log</em> message will open the <em>log</em> message details panel you&#x27;re familiar with from the <em>Logs</em> <em>management</em> page. Explore <em>logs</em> with no pattern The <em>Logs</em> with no pattern tab groups all"
      },
      "id": "6072d46128ccbc244451c18b"
    },
    {
      "sections": [
        "Parsing log data",
        "Example",
        "How log parsing works",
        "Important",
        "Limits",
        "Tip",
        "Built-in parsing rulesets",
        "List of built-in rulesets",
        "Add the logtype attribute",
        "New Relic infrastructure agent example",
        "Fluentd example",
        "Fluent Bit example",
        "Logstash example",
        "Logs API example",
        "Create custom parsing rules",
        "Troubleshooting"
      ],
      "title": "Parsing log data",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "52955adb68242c4ca582ba9cb8e22963955a8275",
      "image": "https://docs.newrelic.com/static/dc392bb7142d2fdb253a649daf4ebe6d/c1b63/log-parsing-rule-ui.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/parsing/",
      "published_at": "2021-10-19T03:43:56Z",
      "updated_at": "2021-10-19T03:43:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Parsing is the process of splitting unstructured log data into attributes (key/value pairs). You can use these attributes to facet or filter logs in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial on YouTube (approx. 4-1/2 minutes). New Relic parses log data according to rules. This document describes how logs parsing works, how to use built-in rules, and how to create custom rules. You can also create, query, and manage your log parsing rules by using NerdGraph, our GraphQL API, at api.newrelic.com/graphiql. For more information, see our NerdGraph tutorial for parsing. Example A good example is a default NGINX access log containing unstructured text. It is useful for searching but not much else. Here's an example of a typical line: 127.180.71.3 - - [10/May/1997:08:05:32 +0000] \"GET /downloads/product_1 HTTP/1.1\" 304 0 \"-\" \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" Copy In an unparsed format, you would need to do a full text search to answer most questions. After parsing, the log is organized into attributes, like response code and request URL: { \"remote_addr\":\"93.180.71.3\", \"time\":\"1586514731\", \"method\":\"GET\", \"path\":\"/downloads/product_1\", \"version\":\"HTTP/1.1\", \"response\":\"304\", \"bytesSent\": 0, \"user_agent\": \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" } Copy Parsing makes it easier to create custom queries that facet on those values. This helps you understand the distribution of response codes per request URL and quickly find problematic pages. How log parsing works Here's an overview of how New Relic implements parsing of logs: Log parsing How it works What All parsing takes place against the message field; no other fields can be parsed. Each parsing rule is created with matching criteria that determines which logs the rule will attempt to parse. To simplify the matching process, we recommend adding a logtype attribute to your logs. However, you are not limited to using logtype; any attribute can be used as matching criteria. When Parsing will only be applied once to each log message. If multiple parsing rules match the log, only the first that succeeds will be applied. Parsing takes place during log ingestion, before data is written to NRDB. Once data has been written to storage, it can no longer be parsed. Parsing occurs in the pipeline before data enrichments take place. Be careful when defining the matching criteria for a parsing rule. If the criteria is based on an attribute that doesn't exist untail after parsing or enrichment take place, that data won't be present in the logs when matching occurs. As a result, no parsing will happen. How Rules can be written in Grok, regex, or a mixture of the two. Grok is a collection of patterns that abstract away complicated regular expressions. If the content of the message field is JSON, it will be parsed automatically. New Relic's log ingestion pipeline can parse data by matching a log event to a rule that describes how the log should be parsed. There are two ways log events can be parsed: Use a built-in rule. Define a custom rule. Rules are a combination of matching logic and parsing logic. Matching is done by defining a query match on an attribute of the logs. Rules are not applied retroactively. Logs collected before a rule is created are not parsed by that rule. The simplest way to organize your logs and how they are parsed is to include the logtype field in your log event. This tells New Relic what built-in ruleset to apply to the logs. Important Once a parsing rule is active, data parsed by the rule is permanently changed. This cannot be reverted. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. Limits Parsing is computationally expensive, which introduces risk. Parsing is done for custom rules defined in an account and for matching patterns to a log. A large number of patterns or poorly defined custom rules will consume a huge amount of memory and CPU resources while also taking a very long time to complete. In order to prevent problems, we apply two parsing limits: per-message-per-rule and per-account. Limit Description Per-message-per-rule The per-message-per-rule limit prevents the time spent parsing any single message from being greater than 100 ms. If that limit is reached, the system will cease attempting to parse the log message with that rule. The ingestion pipeline will attempt to run any other applicable on that message, and the message will still be passed through the ingestion pipeline and stored in NRDB. The log message will be in its original, unparsed format. Per-account The per-account limit exists to prevent accounts from using more than their fair share of resources. The limit considers the total time spent processing all log messages for an account per-minute. The limit is not a fixed value; it scales up or down proportionally to the volume of data stored daily by the account and the environment size that is subsequently allocated to support that customer. Tip To easily check if your rate limits have been reached, go to your system Limits page in the New Relic UI. Built-in parsing rulesets Common log formats have well-established parsing rules already created for them. To get the benefit of built-in parsing rules, add the logtype attribute when forwarding logs. Set the value to something listed in the following table, and the rules for that type of log will be applied automatically. List of built-in rulesets The following logtype attribute values map to a predefined parsing rule. For example, to query the Application Load Balancer: From the New Relic UI, use the format logtype: alb. From NerdGraph, use the format logtype = 'alb'. To learn what fields are parsed for each rule, see our documentation about built-in parsing rules. logtype Example matching query alb AWS Application Load Balancer logtype:alb apache Apache Access logtype:apache cloudfront-web CloudFront Web logtype:cloudfront-web elb Amazon Elastic Load Balancer logtype:elb iis_w3c Microsoft IIS server logs - W3C format logtype:iis_w3c monit Monit logs logtype:monit mysql-error MySQL Error logtype:mysql-error nginx NGINX access logs logtype:nginx nginx-error NGINX error logs logtype:nginx-error route-53 Amazon Route 53 logs logtype:route-53 syslog-rfc5424 Syslog logtype:syslog-rfc5424 Add the logtype attribute When aggregating logs, it's important to provide metadata that makes it easy to organize, search, and parse those logs. One simple way of doing this is to add the attribute logtype to the log messages when they are shipped. Built-in parsing rules are applied by default to certain logtype values. Here are some examples of how to add logtype to logs sent by some of our supported shipping methods. New Relic infrastructure agent example Add logtype as an attribute. You must set the logtype for each named source. Learn more about using the infrastructure agent to add attributes. logs: - name: file-simple file: /path/to/file attributes: logtype: fileRaw - name: nginx-example file: /var/log/nginx.log attributes: logtype: nginx Copy Fluentd example Add a filter block to the .conf file, which uses a record_transformer to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluentd examples. <filter containers> @type record_transformer enable_ruby true <record> #Add logtype to trigger a built-in parsing rule for nginx access logs logtype nginx #Set timestamp from the value contained in the field \"time\" timestamp record[\"time\"] #Add hostname and tag fields to all records hostname \"#{Socket.gethostname}\" tag ${tag} </record> </filter> Copy Fluent Bit example Add a filter block to the .conf file that uses a record_modifier to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluent Bit examples. [FILTER] Name record_modifier Match * Record logtype nginx Record hostname ${HOSTNAME} Record service_name Sample-App-Name Copy Logstash example Add a filter block to the Logstash configuration which uses an add_field mutate filter to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Logstash examples. filter { mutate { add_field => { \"logtype\" => \"nginx\" \"service_name\" => \"myservicename\" \"hostname\" => \"%{host}\" } } } Copy Logs API example You can add attributes to the JSON request sent to New Relic. In this example we add a logtype attribute of value nginx to trigger the built-in NGINX parsing rule. Learn more about using the Logs API. POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: YOUR_LICENSE_KEY Accept: */* Content-Length: 133 { \"timestamp\": TIMESTAMP_IN_UNIX_EPOCH, \"message\": \"User 'xyz' logged in\", \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } Copy Create custom parsing rules Many logs are formatted or structured in a unique way. In order to parse them, custom logic must be built and applied. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. To create and manage your own, custom parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing, then click Create parsing rule. Enter the parsing rule's name. Choose an attribute and value to match on. Write your Grok pattern and test the rule. To learn about Grok and custom parsing rules, read our blog post about how to parse logs with Grok patterns. Enable and save the custom parsing rule. To view the list of custom parsing rules: From Manage Data on the left nav of the Logs UI, click Parsing. To view existing parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing. Troubleshooting If parsing is not working the way you intended, it may be due to: Logic: The parsing rule matching logic does not match the logs you want. Timing: If your parsing matching rule targets a value that doesn't exist yet, it will fail. This can occur if the value is added later in the pipeline as part of the enrichment process. Limits: There is a fixed amount of time available every minute to process logs via parsing, patterns, drop filters, etc. If the maximum amount of time has been spent, parsing will be skipped for additional log event records. To resolve these problems, create or adjust your custom parsing rules.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 212.20886,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Parsing <em>log</em> data",
        "sections": "<em>Logstash</em> example",
        "tags": "<em>Log</em> <em>management</em>",
        "body": "Parsing is the process of splitting unstructured <em>log</em> data into attributes (key&#x2F;value pairs). You can use these attributes to facet or filter <em>logs</em> in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial"
      },
      "id": "603e7eb4196a67b0c4a83dd1"
    }
  ],
  "/docs/logs/log-management/troubleshooting/no-log-data-appears-ui": [
    {
      "sections": [
        "Introduction to the Log API",
        "HTTP endpoint",
        "HTTP setup",
        "HTTP headers",
        "HTTP query parameters",
        "JSON body",
        "Simplified JSON body message",
        "Detailed JSON body message",
        "Limits and restricted characters",
        "Caution",
        "Important",
        "Rate limit violations",
        "HTTP requests per minute",
        "JSON bytes per minute",
        "Log payload format",
        "JSON message attributes",
        "Common block attributes",
        "Logs block attributes",
        "JSON message attribute parsing",
        "Log JSON examples",
        "Log POST message example",
        "JSON POST request example",
        "What's next?"
      ],
      "title": "Introduction to the Log API",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "Log API"
      ],
      "external_id": "198ebbf54f4a13fdf2f5b0f19d8cc8677afd09a2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/log-management/log-api/introduction-log-api/",
      "published_at": "2021-10-19T03:43:55Z",
      "updated_at": "2021-10-19T03:43:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If our log forwarding solutions don't meet your needs, you can use our Log API to send log data directly to New Relic via an HTTP endpoint. Want to try out our Log API? Create a New Relic account for free! No credit card required. HTTP endpoint Use the endpoint that's applicable for your New Relic account: United States (US) endpoint: https://log-api.newrelic.com/log/v1 Copy European Union (EU) endpoint: https://log-api.eu.newrelic.com/log/v1 Copy HTTP setup To send log data to your New Relic account via the Log API: Get your New Relic license key. Review the limits and restricted characters for your JSON payload. Generate the JSON message using the required headers and body fields. Ensure that your Api-Key or License-Key is included in your headers or query parameters. Refer to the log JSON examples. Send your JSON message to the appropriate HTTP endpoint for your New Relic account in a POST request. US: https://log-api.newrelic.com/log/v1 EU: https://log-api.eu.newrelic.com/log/v1 Generate some traffic and wait a few minutes, then check your account for data. If no data appears after you enable our log management capabilities, follow our troubleshooting procedures. HTTP headers When creating your HTTP headers, use these guidelines: Header Supported values Content-Type Required application/json json application/gzip gzip Api-Key Required A New Relic license key. You can also send this via query parameter. You can also use an Insights insert key but the license key is preferred. Gzipped JSON formatting is accepted. If sending compressed JSON, please include the Content-Type: application/json and Content-Encoding: gzip headers. HTTP query parameters The license key can also be passed as a query string parameter. This can be useful when sending logs from cloud-based sources that don't allow custom HTTP request headers. Query parameter Value Api-Key Your license key. Use this key whenever you send a header. You can also use an Insights insert key but the license key is preferred. JSON body You can send your JSON message using either a simplified or detailed set of attributes: Simplified JSON body message When using the simplified format to create your JSON message, send a single JSON object with the following: Field Value type Format Required Notes \"timestamp\" Integer Either milliseconds or seconds since epoch No If the field is not specific as millisecond or seconds since epoch, the message will be timestamped using the ingest time \"message\" String any string No This is the main log message field that is searched by default \"logtype\" String any string No Primary field for identifying logs and matching parsing rules other_fields (must not contain white space) String any string No These will become attributes of the log message Note: Log management does not support white space in attribute names Detailed JSON body message When using the detailed format to create your body, it must be a JSON array containing one or more JSON objects, each of which with the following format: Field Value type Format Required Notes \"common\" Object See common. No Any attributes that are common to all log messages \"logs\" Array See logs. Yes Array with the log entries Limits and restricted characters Caution Avoid calling our API from within the code of a customer-facing application. This can cause performance issues or block your application if response time is slow. If you need to do it this way, call our API asynchronously to avoid these performance issues. Restrictions on logs sent to the Log API: Payload total size: 1MB(10^6 bytes) maximum per POST. We highly recommend using compression. The payload must be encoded as UTF-8. Number of attributes per event: 255 maximum Length of attribute name: 255 characters Length of attribute value: 4096 maximum character length Some specific attributes have additional restrictions: accountId: This is a reserved attribute name. If it is included, it will be dropped during ingest. entity.guid, entity.name, and entity.type: These attributes are used internally to identify entities. Any values submitted with these keys in the attributes section of a metric data point may cause undefined behavior such as missing entities in the UI or telemetry not associating with the expected entities. For more information please refer to Entity synthesis. eventType: This is a reserved attribute name. If it is included, it will be dropped during ingest. timestamp: Must be a Unix epoch timestamp. You can define timestamps either in seconds or in milliseconds. Important Payloads with timestamps older than 48 hours may be dropped. Rate limits on logs sent to the Log API: Maximum rate for HTTP requests sent to the Log API: 300,000 requests per minute Maximum rate of uncompressed Log JSON bytes sent to the Log API: 10 GB per minute Rate limit violations Exceeding rate limits affects how the Log API behaves. Follow these instructions if this happens. HTTP requests per minute When the maximum request rate limit is exceeded for an account, the New Relic Log API returns a 429 response for the remainder of the minute. This response includes a Retry-After header indicating how long to wait in seconds before resubmitting or sending new data. To resolve this issue, either reduce the number of data points you are sending, or request a rate limit change. Subsequent subscription changes do not impact modified rate limits. If an account change impacts your rate limit, you must notify us to adjust your rate limit. To request rate limit changes, contact your New Relic account representative, or visit our Support portal. JSON bytes per minute When the maximum Log JSON byte limit is exceeded for an account, the New Relic Log API returns a 429 response for the remainder of the minute. This response includes a Retry-After header indicating how long to wait in seconds before resubmitting or sending new data. To resolve this issue, try to reduce the amount of log data you are sending, or spread it out over a larger period of time. To request rate limit changes, contact your New Relic account representative, or visit our Support portal. Log payload format We accept any valid JSON payload. The payload must encoded as UTF-8. Important Log management does not support white space in attribute names. For example, {\"Sample Attribute\": \"Value\"} would cause errors. JSON message attributes Common block attributes This is a block containing attributes that will be common to all log entries in logs: Field Value type Format Required Notes \"timestamp\" Integer Milliseconds or seconds since epoch No Message timestamp default to ingest time \"attributes\" Object JSON No This sub-object contains all other attributes of the message Logs block attributes This is an array containing log entries with the following format: Field Value type Format Required Notes \"timestamp\" Integer Milliseconds or seconds since epoch No Message timestamp default to ingest time \"attributes\" Object JSON No This sub-object contains all other attributes of the message \"message\" String (any string) Yes This is the main log message field that is searched by default \"log\" String (any string) No We will rewrite this string as the field message on ingest \"LOG\" String (any string) No We will rewrite this string as the field message on ingest \"MESSAGE\" String (any string) No We will rewrite this string as the field message on ingest JSON message attribute parsing Our log management capabilities will parse any message attribute as JSON. The resulting JSON attributes in the parsed message will be added to the event. If the message attribute is not JSON, it is left as is. Important New Relic's log management capabilities do not support white space in attribute names. For example, {\"Sample Attribute\": \"Value\"} would cause errors. Here is an example message attribute: { \"timestamp\": 1562767499238, \"message\": \"{\\\"service-name\\\": \\\"login-service\\\", \\\"user\\\": {\\\"id\\\": 123, \\\"name\\\": \\\"alice\\\"}}\" } Copy This will be treated as: { \"timestamp\": 1562767499238, \"message\": \"{\\\"service-name\\\": \\\"my-service\\\", \\\"user\\\": {\\\"id\\\": 123, \\\"name\\\": \\\"alice\\\"}}\", \"service-name\": \"my-service\", \"user\": { \"id\": 123, \"name\": \"alice\" } } Copy Log JSON examples Attributes can be scalar JSON types like string and number. They can also be compound (or nested) objects. Compound attributes will have their associated attributes stored with flattened names. For example, here is a compound user attribute in a log entry's attributes: \"attributes\": { \"action\": \"login\", \"user\": { \"id\": 123, \"name\": \"alice\" } } Copy This will result in the following attributes being stored with the log event: Attribute Value \"action\" \"login\" \"user.id\" 123 \"user.name\" \"alice\" Log POST message example Log POST message example: POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: <YOUR_LICENSE_KEY> Accept: */* Content-Length: 319 [{ \"common\": { \"attributes\": { \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } }, \"logs\": [{ \"timestamp\": <TIMESTAMP_IN_UNIX_EPOCH>, \"message\": \"User 'xyz' logged in\" },{ \"timestamp\": <TIMESTAMP_IN_UNIX_EPOCH>, \"message\": \"User 'xyz' logged out\", \"attributes\": { \"auditId\": 123 } }] }] Copy This POST message would result in the following log messages being stored in New Relic: Attribute Value \"logtype\" \"accesslogs\" \"service\" \"login-service\" \"hostname\" \"login.example.com\" Here's an example of stored logs block attributes: Attribute Value \"timestamp\" 1550086450124 \"message\" \"User 'xyz' logged out\" \"auditId\" 123 JSON POST request example Here's an example of a JSON POST request: POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: <YOUR_LICENSE_KEY> Accept: */* Content-Length: 133 { \"timestamp\": <TIMESTAMP_IN_UNIX_EPOCH>, \"message\": \"User 'xyz' logged in\", \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } Copy What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.5344,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to the <em>Log</em> API",
        "sections": "<em>Logs</em> block attributes",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " in a POST request. US: https:&#x2F;&#x2F;<em>log</em>-api.newrelic.com&#x2F;<em>log</em>&#x2F;v1 EU: https:&#x2F;&#x2F;<em>log</em>-api.eu.newrelic.com&#x2F;<em>log</em>&#x2F;v1 Generate some traffic and wait a few minutes, then check your account for data. If no data appears after you enable our <em>log</em> <em>management</em> capabilities, follow our <em>troubleshooting</em> procedures. HTTP headers"
      },
      "id": "603ea832196a6726e7a83da1"
    },
    {
      "sections": [
        "Discover value in log data with patterns",
        "Technical overview",
        "Availability",
        "Get started",
        "Explore log patterns",
        "Explore logs with no pattern",
        "Masked attributes and wildcards",
        "Troubleshooting",
        "Put the platform to work with patterns"
      ],
      "title": "Discover value in log data with patterns",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "8f1e27c94327ca4888a945f8e12f9c2310ccd7a6",
      "image": "https://docs.newrelic.com/static/578d7186bb34352855696e5307cc82f2/c1b63/log-patterns-logs-without-a-pattern.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/find-unusual-logs-log-patterns/",
      "published_at": "2021-10-19T03:56:24Z",
      "updated_at": "2021-10-19T03:56:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Log patterns are the fastest way to discover value in log data without searching. Log data is high volume telemetry with a low value per individual record. Searching can quickly lead to logs that provide a root cause explanation, but most data is repetitive and hard to contextualize when browsing. Patterns can make log data discoverable without spending a lot of time reading through low value data. Logs > Patterns: Use patterns as the basis for alerts when the frequency of important data changes, or for configuring drop rules to get rid of unnecessary repetitive data. Technical overview Log patterns functionality applies machine learning to normalize and group log messages that are consistent in format but variable in content. These grouped messages can be sorted, making it easy to find the most frequent or rarest sets of logs in your environment. Use patterns as the basis for alerts when the frequency of important data changes, or to configure drop rules to get rid of unnecessary repetitive data. Log patterns use advanced clustering algorithms to group together similar log messages automatically. With patterns, you can: Orient more quickly through millions of logs. Reduce the time it takes to identify unusual behavior in your log estate. Monitor the frequency of known patterns over time to focus your energy on what matters, and exclude what's irrelevant. Availability The ability to configure this feature is dependent on role-based permissions. For example, accounts still using our original pricing model require an Owner or Admin role. If you see Patterns are turned off in your Log management Patterns UI, click the Configure Patterns button and enable it. If you don't see patterns within 30 minutes of enabling the feature, there may be a lack of data with a message attribute for the system to create a pattern from it. Log patterns Limitations and considerations Pricing There is no separate pricing for log patterns. The only cost is for additional data generated and added to your log records. A pattern attribute will be added to all logs that match a pattern. Attributes also may be added when common values are discovered, such as GUIDs, IP addresses, URL, or email addresses. These attributes are automatically extracted from the log message as part of the pattern process. HITRUST accounts The log patterns feature is not FedRAMP compliant. FedRAMP or other HITRUST accounts are not eligible to use patterns. Parsing limits We have a system of safety limits on memory and CPU resources when processing logs and their patterns. These parsing limits can have an impact on the data you get. For more information, see our documentation about parsing limits. Get started To start examining patterns: Go to one.newrelic.com > Logs, and use the account picker dropdown to select the target account where you want to explore patterns. In the left navigation of the Logs UI, click Patterns. The main log UI changes to show patterns that match the query in the query bar. Logs > Log patterns: The line chart shows the top 5 patterns over time. Use the time picker and query bar to adjust the results. Explore log patterns By default the log patterns UI first shows the most frequent occurrence of patterns. To sort to show the rarest patterns first, click the Count column. You can also use the query bar or attributes bar to filter your log patterns. If you want to... Do this... Understand the rate of change in patterns Look at the line chart. The color-coded patterns correspond to the plot column in the table. You can toggle individual plot patterns to narrow your focus. See the individual log messages that match each pattern Click pattern to expand the row and see a table of individual log records. To see additional records, scroll up or down. To explore an individual log in more detail, click it to open the details panel. Group and filter patterns by their attributes Use the query bar and time picker. As you apply different filters and time windows, the log patterns adjust to your new target data. Create an alert from a pattern Add the pattern to the query bar and run the query. Then click Create alert condition in the left nav. Troubleshoot log messages that haven't been clustered into a pattern Use the Logs with no pattern tab in the Log patterns UI. Clicking a specific log message will open the log message details panel you're familiar with from the Logs management page. Explore logs with no pattern The Logs with no pattern tab groups all recent log messages in your account that were not clustered into a known pattern yet. These log messages don't represent any problem or flaw in the system; they have no pattern because they are too new to have been processed by the machine learning system. This makes them valuable to explore when you want to understand what has recently changed in your environment. Logs > Log patterns: New Relic's log patterns feature automatically groups logs without a matching pattern. For example: Are any of these logs tied to a recent problem? This is a quick way to discover unique log data that is appearing for the first time in your environment. Does your log data have a new format? Sometimes the logs don't represent a problem, but a new format of log data that deviates from the data model you expect your applications to follow. Catching these logs early gives you the opportunity to ask developers to correct any deviations in their log output. The more consistent people are in the way log data is generated, the easier it becomes to use logs across a diverse set of teams. Masked attributes and wildcards Parts of the log messages in patterns are classified as variables and are substituted by masked attributes. The masking process supports and improves the clustering phase by allowing the algorithm to ignore changing details and focus on the repetitive structure. Masked attributes include: date_time ip url uuid Masked attributes are highlighted and are easy to identify, as shown in the following example. Here is an example of a log pattern that has masked attributes. Log patterns extract other less trivial variables that don't belong to any masked attribute. These variables are indicated as wildcards *. Here is an example of how wildcards * group variables in the Log patterns UI. Troubleshooting Here are a few reasons why you might have patterns enabled but not see any pattern data. If you're sure none of the items below are true, get help from support.newrelic.com. No data has arrived in the timeframe you're observing. Try expanding the time range you're viewing with the time picker. It's been less than 24 hours since patterns were enabled in the account. This means the ML model may not be generated for the account yet. None of the data coming in has a message field. Patterns will only be generated for values in the message field of a log record. If your logs don't contain message, there will be no data. Put the platform to work with patterns Patterns are a value that is enriched onto the existing log message as a new attribute named newrelic.logPattern. Anything you can do with logs generally can be done with log patterns, such as: Build your own dashboards with patterns, to monitor a specific pattern or group of patterns you care about. Create alerts for patterns by adding NRQL alerts. Use baseline alert conditions to detect anomalies in known log patterns.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 219.63716,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Discover value in <em>log</em> data with patterns",
        "sections": "Explore <em>logs</em> with no pattern",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " messages that haven&#x27;t been clustered into a pattern Use the <em>Logs</em> with no pattern tab in the <em>Log</em> patterns UI. Clicking a specific <em>log</em> message will open the <em>log</em> message details panel you&#x27;re familiar with from the <em>Logs</em> <em>management</em> page. Explore <em>logs</em> with no pattern The <em>Logs</em> with no pattern tab groups all"
      },
      "id": "6072d46128ccbc244451c18b"
    },
    {
      "sections": [
        "Parsing log data",
        "Example",
        "How log parsing works",
        "Important",
        "Limits",
        "Tip",
        "Built-in parsing rulesets",
        "List of built-in rulesets",
        "Add the logtype attribute",
        "New Relic infrastructure agent example",
        "Fluentd example",
        "Fluent Bit example",
        "Logstash example",
        "Logs API example",
        "Create custom parsing rules",
        "Troubleshooting"
      ],
      "title": "Parsing log data",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "52955adb68242c4ca582ba9cb8e22963955a8275",
      "image": "https://docs.newrelic.com/static/dc392bb7142d2fdb253a649daf4ebe6d/c1b63/log-parsing-rule-ui.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/parsing/",
      "published_at": "2021-10-19T03:43:56Z",
      "updated_at": "2021-10-19T03:43:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Parsing is the process of splitting unstructured log data into attributes (key/value pairs). You can use these attributes to facet or filter logs in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial on YouTube (approx. 4-1/2 minutes). New Relic parses log data according to rules. This document describes how logs parsing works, how to use built-in rules, and how to create custom rules. You can also create, query, and manage your log parsing rules by using NerdGraph, our GraphQL API, at api.newrelic.com/graphiql. For more information, see our NerdGraph tutorial for parsing. Example A good example is a default NGINX access log containing unstructured text. It is useful for searching but not much else. Here's an example of a typical line: 127.180.71.3 - - [10/May/1997:08:05:32 +0000] \"GET /downloads/product_1 HTTP/1.1\" 304 0 \"-\" \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" Copy In an unparsed format, you would need to do a full text search to answer most questions. After parsing, the log is organized into attributes, like response code and request URL: { \"remote_addr\":\"93.180.71.3\", \"time\":\"1586514731\", \"method\":\"GET\", \"path\":\"/downloads/product_1\", \"version\":\"HTTP/1.1\", \"response\":\"304\", \"bytesSent\": 0, \"user_agent\": \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" } Copy Parsing makes it easier to create custom queries that facet on those values. This helps you understand the distribution of response codes per request URL and quickly find problematic pages. How log parsing works Here's an overview of how New Relic implements parsing of logs: Log parsing How it works What All parsing takes place against the message field; no other fields can be parsed. Each parsing rule is created with matching criteria that determines which logs the rule will attempt to parse. To simplify the matching process, we recommend adding a logtype attribute to your logs. However, you are not limited to using logtype; any attribute can be used as matching criteria. When Parsing will only be applied once to each log message. If multiple parsing rules match the log, only the first that succeeds will be applied. Parsing takes place during log ingestion, before data is written to NRDB. Once data has been written to storage, it can no longer be parsed. Parsing occurs in the pipeline before data enrichments take place. Be careful when defining the matching criteria for a parsing rule. If the criteria is based on an attribute that doesn't exist untail after parsing or enrichment take place, that data won't be present in the logs when matching occurs. As a result, no parsing will happen. How Rules can be written in Grok, regex, or a mixture of the two. Grok is a collection of patterns that abstract away complicated regular expressions. If the content of the message field is JSON, it will be parsed automatically. New Relic's log ingestion pipeline can parse data by matching a log event to a rule that describes how the log should be parsed. There are two ways log events can be parsed: Use a built-in rule. Define a custom rule. Rules are a combination of matching logic and parsing logic. Matching is done by defining a query match on an attribute of the logs. Rules are not applied retroactively. Logs collected before a rule is created are not parsed by that rule. The simplest way to organize your logs and how they are parsed is to include the logtype field in your log event. This tells New Relic what built-in ruleset to apply to the logs. Important Once a parsing rule is active, data parsed by the rule is permanently changed. This cannot be reverted. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. Limits Parsing is computationally expensive, which introduces risk. Parsing is done for custom rules defined in an account and for matching patterns to a log. A large number of patterns or poorly defined custom rules will consume a huge amount of memory and CPU resources while also taking a very long time to complete. In order to prevent problems, we apply two parsing limits: per-message-per-rule and per-account. Limit Description Per-message-per-rule The per-message-per-rule limit prevents the time spent parsing any single message from being greater than 100 ms. If that limit is reached, the system will cease attempting to parse the log message with that rule. The ingestion pipeline will attempt to run any other applicable on that message, and the message will still be passed through the ingestion pipeline and stored in NRDB. The log message will be in its original, unparsed format. Per-account The per-account limit exists to prevent accounts from using more than their fair share of resources. The limit considers the total time spent processing all log messages for an account per-minute. The limit is not a fixed value; it scales up or down proportionally to the volume of data stored daily by the account and the environment size that is subsequently allocated to support that customer. Tip To easily check if your rate limits have been reached, go to your system Limits page in the New Relic UI. Built-in parsing rulesets Common log formats have well-established parsing rules already created for them. To get the benefit of built-in parsing rules, add the logtype attribute when forwarding logs. Set the value to something listed in the following table, and the rules for that type of log will be applied automatically. List of built-in rulesets The following logtype attribute values map to a predefined parsing rule. For example, to query the Application Load Balancer: From the New Relic UI, use the format logtype: alb. From NerdGraph, use the format logtype = 'alb'. To learn what fields are parsed for each rule, see our documentation about built-in parsing rules. logtype Example matching query alb AWS Application Load Balancer logtype:alb apache Apache Access logtype:apache cloudfront-web CloudFront Web logtype:cloudfront-web elb Amazon Elastic Load Balancer logtype:elb iis_w3c Microsoft IIS server logs - W3C format logtype:iis_w3c monit Monit logs logtype:monit mysql-error MySQL Error logtype:mysql-error nginx NGINX access logs logtype:nginx nginx-error NGINX error logs logtype:nginx-error route-53 Amazon Route 53 logs logtype:route-53 syslog-rfc5424 Syslog logtype:syslog-rfc5424 Add the logtype attribute When aggregating logs, it's important to provide metadata that makes it easy to organize, search, and parse those logs. One simple way of doing this is to add the attribute logtype to the log messages when they are shipped. Built-in parsing rules are applied by default to certain logtype values. Here are some examples of how to add logtype to logs sent by some of our supported shipping methods. New Relic infrastructure agent example Add logtype as an attribute. You must set the logtype for each named source. Learn more about using the infrastructure agent to add attributes. logs: - name: file-simple file: /path/to/file attributes: logtype: fileRaw - name: nginx-example file: /var/log/nginx.log attributes: logtype: nginx Copy Fluentd example Add a filter block to the .conf file, which uses a record_transformer to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluentd examples. <filter containers> @type record_transformer enable_ruby true <record> #Add logtype to trigger a built-in parsing rule for nginx access logs logtype nginx #Set timestamp from the value contained in the field \"time\" timestamp record[\"time\"] #Add hostname and tag fields to all records hostname \"#{Socket.gethostname}\" tag ${tag} </record> </filter> Copy Fluent Bit example Add a filter block to the .conf file that uses a record_modifier to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluent Bit examples. [FILTER] Name record_modifier Match * Record logtype nginx Record hostname ${HOSTNAME} Record service_name Sample-App-Name Copy Logstash example Add a filter block to the Logstash configuration which uses an add_field mutate filter to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Logstash examples. filter { mutate { add_field => { \"logtype\" => \"nginx\" \"service_name\" => \"myservicename\" \"hostname\" => \"%{host}\" } } } Copy Logs API example You can add attributes to the JSON request sent to New Relic. In this example we add a logtype attribute of value nginx to trigger the built-in NGINX parsing rule. Learn more about using the Logs API. POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: YOUR_LICENSE_KEY Accept: */* Content-Length: 133 { \"timestamp\": TIMESTAMP_IN_UNIX_EPOCH, \"message\": \"User 'xyz' logged in\", \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } Copy Create custom parsing rules Many logs are formatted or structured in a unique way. In order to parse them, custom logic must be built and applied. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. To create and manage your own, custom parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing, then click Create parsing rule. Enter the parsing rule's name. Choose an attribute and value to match on. Write your Grok pattern and test the rule. To learn about Grok and custom parsing rules, read our blog post about how to parse logs with Grok patterns. Enable and save the custom parsing rule. To view the list of custom parsing rules: From Manage Data on the left nav of the Logs UI, click Parsing. To view existing parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing. Troubleshooting If parsing is not working the way you intended, it may be due to: Logic: The parsing rule matching logic does not match the logs you want. Timing: If your parsing matching rule targets a value that doesn't exist yet, it will fail. This can occur if the value is added later in the pipeline as part of the enrichment process. Limits: There is a fixed amount of time available every minute to process logs via parsing, patterns, drop filters, etc. If the maximum amount of time has been spent, parsing will be skipped for additional log event records. To resolve these problems, create or adjust your custom parsing rules.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 212.20872,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Parsing <em>log</em> data",
        "sections": "<em>Logstash</em> example",
        "tags": "<em>Log</em> <em>management</em>",
        "body": "Parsing is the process of splitting unstructured <em>log</em> data into attributes (key&#x2F;value pairs). You can use these attributes to facet or filter <em>logs</em> in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial"
      },
      "id": "603e7eb4196a67b0c4a83dd1"
    }
  ],
  "/docs/logs/log-management/troubleshooting/view-log-messages-real-time-live-tail": [
    {
      "sections": [
        "Introduction to the Log API",
        "HTTP endpoint",
        "HTTP setup",
        "HTTP headers",
        "HTTP query parameters",
        "JSON body",
        "Simplified JSON body message",
        "Detailed JSON body message",
        "Limits and restricted characters",
        "Caution",
        "Important",
        "Rate limit violations",
        "HTTP requests per minute",
        "JSON bytes per minute",
        "Log payload format",
        "JSON message attributes",
        "Common block attributes",
        "Logs block attributes",
        "JSON message attribute parsing",
        "Log JSON examples",
        "Log POST message example",
        "JSON POST request example",
        "What's next?"
      ],
      "title": "Introduction to the Log API",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "Log API"
      ],
      "external_id": "198ebbf54f4a13fdf2f5b0f19d8cc8677afd09a2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/log-management/log-api/introduction-log-api/",
      "published_at": "2021-10-19T03:43:55Z",
      "updated_at": "2021-10-19T03:43:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If our log forwarding solutions don't meet your needs, you can use our Log API to send log data directly to New Relic via an HTTP endpoint. Want to try out our Log API? Create a New Relic account for free! No credit card required. HTTP endpoint Use the endpoint that's applicable for your New Relic account: United States (US) endpoint: https://log-api.newrelic.com/log/v1 Copy European Union (EU) endpoint: https://log-api.eu.newrelic.com/log/v1 Copy HTTP setup To send log data to your New Relic account via the Log API: Get your New Relic license key. Review the limits and restricted characters for your JSON payload. Generate the JSON message using the required headers and body fields. Ensure that your Api-Key or License-Key is included in your headers or query parameters. Refer to the log JSON examples. Send your JSON message to the appropriate HTTP endpoint for your New Relic account in a POST request. US: https://log-api.newrelic.com/log/v1 EU: https://log-api.eu.newrelic.com/log/v1 Generate some traffic and wait a few minutes, then check your account for data. If no data appears after you enable our log management capabilities, follow our troubleshooting procedures. HTTP headers When creating your HTTP headers, use these guidelines: Header Supported values Content-Type Required application/json json application/gzip gzip Api-Key Required A New Relic license key. You can also send this via query parameter. You can also use an Insights insert key but the license key is preferred. Gzipped JSON formatting is accepted. If sending compressed JSON, please include the Content-Type: application/json and Content-Encoding: gzip headers. HTTP query parameters The license key can also be passed as a query string parameter. This can be useful when sending logs from cloud-based sources that don't allow custom HTTP request headers. Query parameter Value Api-Key Your license key. Use this key whenever you send a header. You can also use an Insights insert key but the license key is preferred. JSON body You can send your JSON message using either a simplified or detailed set of attributes: Simplified JSON body message When using the simplified format to create your JSON message, send a single JSON object with the following: Field Value type Format Required Notes \"timestamp\" Integer Either milliseconds or seconds since epoch No If the field is not specific as millisecond or seconds since epoch, the message will be timestamped using the ingest time \"message\" String any string No This is the main log message field that is searched by default \"logtype\" String any string No Primary field for identifying logs and matching parsing rules other_fields (must not contain white space) String any string No These will become attributes of the log message Note: Log management does not support white space in attribute names Detailed JSON body message When using the detailed format to create your body, it must be a JSON array containing one or more JSON objects, each of which with the following format: Field Value type Format Required Notes \"common\" Object See common. No Any attributes that are common to all log messages \"logs\" Array See logs. Yes Array with the log entries Limits and restricted characters Caution Avoid calling our API from within the code of a customer-facing application. This can cause performance issues or block your application if response time is slow. If you need to do it this way, call our API asynchronously to avoid these performance issues. Restrictions on logs sent to the Log API: Payload total size: 1MB(10^6 bytes) maximum per POST. We highly recommend using compression. The payload must be encoded as UTF-8. Number of attributes per event: 255 maximum Length of attribute name: 255 characters Length of attribute value: 4096 maximum character length Some specific attributes have additional restrictions: accountId: This is a reserved attribute name. If it is included, it will be dropped during ingest. entity.guid, entity.name, and entity.type: These attributes are used internally to identify entities. Any values submitted with these keys in the attributes section of a metric data point may cause undefined behavior such as missing entities in the UI or telemetry not associating with the expected entities. For more information please refer to Entity synthesis. eventType: This is a reserved attribute name. If it is included, it will be dropped during ingest. timestamp: Must be a Unix epoch timestamp. You can define timestamps either in seconds or in milliseconds. Important Payloads with timestamps older than 48 hours may be dropped. Rate limits on logs sent to the Log API: Maximum rate for HTTP requests sent to the Log API: 300,000 requests per minute Maximum rate of uncompressed Log JSON bytes sent to the Log API: 10 GB per minute Rate limit violations Exceeding rate limits affects how the Log API behaves. Follow these instructions if this happens. HTTP requests per minute When the maximum request rate limit is exceeded for an account, the New Relic Log API returns a 429 response for the remainder of the minute. This response includes a Retry-After header indicating how long to wait in seconds before resubmitting or sending new data. To resolve this issue, either reduce the number of data points you are sending, or request a rate limit change. Subsequent subscription changes do not impact modified rate limits. If an account change impacts your rate limit, you must notify us to adjust your rate limit. To request rate limit changes, contact your New Relic account representative, or visit our Support portal. JSON bytes per minute When the maximum Log JSON byte limit is exceeded for an account, the New Relic Log API returns a 429 response for the remainder of the minute. This response includes a Retry-After header indicating how long to wait in seconds before resubmitting or sending new data. To resolve this issue, try to reduce the amount of log data you are sending, or spread it out over a larger period of time. To request rate limit changes, contact your New Relic account representative, or visit our Support portal. Log payload format We accept any valid JSON payload. The payload must encoded as UTF-8. Important Log management does not support white space in attribute names. For example, {\"Sample Attribute\": \"Value\"} would cause errors. JSON message attributes Common block attributes This is a block containing attributes that will be common to all log entries in logs: Field Value type Format Required Notes \"timestamp\" Integer Milliseconds or seconds since epoch No Message timestamp default to ingest time \"attributes\" Object JSON No This sub-object contains all other attributes of the message Logs block attributes This is an array containing log entries with the following format: Field Value type Format Required Notes \"timestamp\" Integer Milliseconds or seconds since epoch No Message timestamp default to ingest time \"attributes\" Object JSON No This sub-object contains all other attributes of the message \"message\" String (any string) Yes This is the main log message field that is searched by default \"log\" String (any string) No We will rewrite this string as the field message on ingest \"LOG\" String (any string) No We will rewrite this string as the field message on ingest \"MESSAGE\" String (any string) No We will rewrite this string as the field message on ingest JSON message attribute parsing Our log management capabilities will parse any message attribute as JSON. The resulting JSON attributes in the parsed message will be added to the event. If the message attribute is not JSON, it is left as is. Important New Relic's log management capabilities do not support white space in attribute names. For example, {\"Sample Attribute\": \"Value\"} would cause errors. Here is an example message attribute: { \"timestamp\": 1562767499238, \"message\": \"{\\\"service-name\\\": \\\"login-service\\\", \\\"user\\\": {\\\"id\\\": 123, \\\"name\\\": \\\"alice\\\"}}\" } Copy This will be treated as: { \"timestamp\": 1562767499238, \"message\": \"{\\\"service-name\\\": \\\"my-service\\\", \\\"user\\\": {\\\"id\\\": 123, \\\"name\\\": \\\"alice\\\"}}\", \"service-name\": \"my-service\", \"user\": { \"id\": 123, \"name\": \"alice\" } } Copy Log JSON examples Attributes can be scalar JSON types like string and number. They can also be compound (or nested) objects. Compound attributes will have their associated attributes stored with flattened names. For example, here is a compound user attribute in a log entry's attributes: \"attributes\": { \"action\": \"login\", \"user\": { \"id\": 123, \"name\": \"alice\" } } Copy This will result in the following attributes being stored with the log event: Attribute Value \"action\" \"login\" \"user.id\" 123 \"user.name\" \"alice\" Log POST message example Log POST message example: POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: <YOUR_LICENSE_KEY> Accept: */* Content-Length: 319 [{ \"common\": { \"attributes\": { \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } }, \"logs\": [{ \"timestamp\": <TIMESTAMP_IN_UNIX_EPOCH>, \"message\": \"User 'xyz' logged in\" },{ \"timestamp\": <TIMESTAMP_IN_UNIX_EPOCH>, \"message\": \"User 'xyz' logged out\", \"attributes\": { \"auditId\": 123 } }] }] Copy This POST message would result in the following log messages being stored in New Relic: Attribute Value \"logtype\" \"accesslogs\" \"service\" \"login-service\" \"hostname\" \"login.example.com\" Here's an example of stored logs block attributes: Attribute Value \"timestamp\" 1550086450124 \"message\" \"User 'xyz' logged out\" \"auditId\" 123 JSON POST request example Here's an example of a JSON POST request: POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: <YOUR_LICENSE_KEY> Accept: */* Content-Length: 133 { \"timestamp\": <TIMESTAMP_IN_UNIX_EPOCH>, \"message\": \"User 'xyz' logged in\", \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } Copy What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.5344,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to the <em>Log</em> API",
        "sections": "<em>Logs</em> block attributes",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " in a POST request. US: https:&#x2F;&#x2F;<em>log</em>-api.newrelic.com&#x2F;<em>log</em>&#x2F;v1 EU: https:&#x2F;&#x2F;<em>log</em>-api.eu.newrelic.com&#x2F;<em>log</em>&#x2F;v1 Generate some traffic and wait a few minutes, then check your account for data. If no data appears after you enable our <em>log</em> <em>management</em> capabilities, follow our <em>troubleshooting</em> procedures. HTTP headers"
      },
      "id": "603ea832196a6726e7a83da1"
    },
    {
      "sections": [
        "Discover value in log data with patterns",
        "Technical overview",
        "Availability",
        "Get started",
        "Explore log patterns",
        "Explore logs with no pattern",
        "Masked attributes and wildcards",
        "Troubleshooting",
        "Put the platform to work with patterns"
      ],
      "title": "Discover value in log data with patterns",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "8f1e27c94327ca4888a945f8e12f9c2310ccd7a6",
      "image": "https://docs.newrelic.com/static/578d7186bb34352855696e5307cc82f2/c1b63/log-patterns-logs-without-a-pattern.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/find-unusual-logs-log-patterns/",
      "published_at": "2021-10-19T03:56:24Z",
      "updated_at": "2021-10-19T03:56:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Log patterns are the fastest way to discover value in log data without searching. Log data is high volume telemetry with a low value per individual record. Searching can quickly lead to logs that provide a root cause explanation, but most data is repetitive and hard to contextualize when browsing. Patterns can make log data discoverable without spending a lot of time reading through low value data. Logs > Patterns: Use patterns as the basis for alerts when the frequency of important data changes, or for configuring drop rules to get rid of unnecessary repetitive data. Technical overview Log patterns functionality applies machine learning to normalize and group log messages that are consistent in format but variable in content. These grouped messages can be sorted, making it easy to find the most frequent or rarest sets of logs in your environment. Use patterns as the basis for alerts when the frequency of important data changes, or to configure drop rules to get rid of unnecessary repetitive data. Log patterns use advanced clustering algorithms to group together similar log messages automatically. With patterns, you can: Orient more quickly through millions of logs. Reduce the time it takes to identify unusual behavior in your log estate. Monitor the frequency of known patterns over time to focus your energy on what matters, and exclude what's irrelevant. Availability The ability to configure this feature is dependent on role-based permissions. For example, accounts still using our original pricing model require an Owner or Admin role. If you see Patterns are turned off in your Log management Patterns UI, click the Configure Patterns button and enable it. If you don't see patterns within 30 minutes of enabling the feature, there may be a lack of data with a message attribute for the system to create a pattern from it. Log patterns Limitations and considerations Pricing There is no separate pricing for log patterns. The only cost is for additional data generated and added to your log records. A pattern attribute will be added to all logs that match a pattern. Attributes also may be added when common values are discovered, such as GUIDs, IP addresses, URL, or email addresses. These attributes are automatically extracted from the log message as part of the pattern process. HITRUST accounts The log patterns feature is not FedRAMP compliant. FedRAMP or other HITRUST accounts are not eligible to use patterns. Parsing limits We have a system of safety limits on memory and CPU resources when processing logs and their patterns. These parsing limits can have an impact on the data you get. For more information, see our documentation about parsing limits. Get started To start examining patterns: Go to one.newrelic.com > Logs, and use the account picker dropdown to select the target account where you want to explore patterns. In the left navigation of the Logs UI, click Patterns. The main log UI changes to show patterns that match the query in the query bar. Logs > Log patterns: The line chart shows the top 5 patterns over time. Use the time picker and query bar to adjust the results. Explore log patterns By default the log patterns UI first shows the most frequent occurrence of patterns. To sort to show the rarest patterns first, click the Count column. You can also use the query bar or attributes bar to filter your log patterns. If you want to... Do this... Understand the rate of change in patterns Look at the line chart. The color-coded patterns correspond to the plot column in the table. You can toggle individual plot patterns to narrow your focus. See the individual log messages that match each pattern Click pattern to expand the row and see a table of individual log records. To see additional records, scroll up or down. To explore an individual log in more detail, click it to open the details panel. Group and filter patterns by their attributes Use the query bar and time picker. As you apply different filters and time windows, the log patterns adjust to your new target data. Create an alert from a pattern Add the pattern to the query bar and run the query. Then click Create alert condition in the left nav. Troubleshoot log messages that haven't been clustered into a pattern Use the Logs with no pattern tab in the Log patterns UI. Clicking a specific log message will open the log message details panel you're familiar with from the Logs management page. Explore logs with no pattern The Logs with no pattern tab groups all recent log messages in your account that were not clustered into a known pattern yet. These log messages don't represent any problem or flaw in the system; they have no pattern because they are too new to have been processed by the machine learning system. This makes them valuable to explore when you want to understand what has recently changed in your environment. Logs > Log patterns: New Relic's log patterns feature automatically groups logs without a matching pattern. For example: Are any of these logs tied to a recent problem? This is a quick way to discover unique log data that is appearing for the first time in your environment. Does your log data have a new format? Sometimes the logs don't represent a problem, but a new format of log data that deviates from the data model you expect your applications to follow. Catching these logs early gives you the opportunity to ask developers to correct any deviations in their log output. The more consistent people are in the way log data is generated, the easier it becomes to use logs across a diverse set of teams. Masked attributes and wildcards Parts of the log messages in patterns are classified as variables and are substituted by masked attributes. The masking process supports and improves the clustering phase by allowing the algorithm to ignore changing details and focus on the repetitive structure. Masked attributes include: date_time ip url uuid Masked attributes are highlighted and are easy to identify, as shown in the following example. Here is an example of a log pattern that has masked attributes. Log patterns extract other less trivial variables that don't belong to any masked attribute. These variables are indicated as wildcards *. Here is an example of how wildcards * group variables in the Log patterns UI. Troubleshooting Here are a few reasons why you might have patterns enabled but not see any pattern data. If you're sure none of the items below are true, get help from support.newrelic.com. No data has arrived in the timeframe you're observing. Try expanding the time range you're viewing with the time picker. It's been less than 24 hours since patterns were enabled in the account. This means the ML model may not be generated for the account yet. None of the data coming in has a message field. Patterns will only be generated for values in the message field of a log record. If your logs don't contain message, there will be no data. Put the platform to work with patterns Patterns are a value that is enriched onto the existing log message as a new attribute named newrelic.logPattern. Anything you can do with logs generally can be done with log patterns, such as: Build your own dashboards with patterns, to monitor a specific pattern or group of patterns you care about. Create alerts for patterns by adding NRQL alerts. Use baseline alert conditions to detect anomalies in known log patterns.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 219.63716,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Discover value in <em>log</em> data with patterns",
        "sections": "Explore <em>logs</em> with no pattern",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " messages that haven&#x27;t been clustered into a pattern Use the <em>Logs</em> with no pattern tab in the <em>Log</em> patterns UI. Clicking a specific <em>log</em> message will open the <em>log</em> message details panel you&#x27;re familiar with from the <em>Logs</em> <em>management</em> page. Explore <em>logs</em> with no pattern The <em>Logs</em> with no pattern tab groups all"
      },
      "id": "6072d46128ccbc244451c18b"
    },
    {
      "sections": [
        "Parsing log data",
        "Example",
        "How log parsing works",
        "Important",
        "Limits",
        "Tip",
        "Built-in parsing rulesets",
        "List of built-in rulesets",
        "Add the logtype attribute",
        "New Relic infrastructure agent example",
        "Fluentd example",
        "Fluent Bit example",
        "Logstash example",
        "Logs API example",
        "Create custom parsing rules",
        "Troubleshooting"
      ],
      "title": "Parsing log data",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "52955adb68242c4ca582ba9cb8e22963955a8275",
      "image": "https://docs.newrelic.com/static/dc392bb7142d2fdb253a649daf4ebe6d/c1b63/log-parsing-rule-ui.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/parsing/",
      "published_at": "2021-10-19T03:43:56Z",
      "updated_at": "2021-10-19T03:43:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Parsing is the process of splitting unstructured log data into attributes (key/value pairs). You can use these attributes to facet or filter logs in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial on YouTube (approx. 4-1/2 minutes). New Relic parses log data according to rules. This document describes how logs parsing works, how to use built-in rules, and how to create custom rules. You can also create, query, and manage your log parsing rules by using NerdGraph, our GraphQL API, at api.newrelic.com/graphiql. For more information, see our NerdGraph tutorial for parsing. Example A good example is a default NGINX access log containing unstructured text. It is useful for searching but not much else. Here's an example of a typical line: 127.180.71.3 - - [10/May/1997:08:05:32 +0000] \"GET /downloads/product_1 HTTP/1.1\" 304 0 \"-\" \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" Copy In an unparsed format, you would need to do a full text search to answer most questions. After parsing, the log is organized into attributes, like response code and request URL: { \"remote_addr\":\"93.180.71.3\", \"time\":\"1586514731\", \"method\":\"GET\", \"path\":\"/downloads/product_1\", \"version\":\"HTTP/1.1\", \"response\":\"304\", \"bytesSent\": 0, \"user_agent\": \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" } Copy Parsing makes it easier to create custom queries that facet on those values. This helps you understand the distribution of response codes per request URL and quickly find problematic pages. How log parsing works Here's an overview of how New Relic implements parsing of logs: Log parsing How it works What All parsing takes place against the message field; no other fields can be parsed. Each parsing rule is created with matching criteria that determines which logs the rule will attempt to parse. To simplify the matching process, we recommend adding a logtype attribute to your logs. However, you are not limited to using logtype; any attribute can be used as matching criteria. When Parsing will only be applied once to each log message. If multiple parsing rules match the log, only the first that succeeds will be applied. Parsing takes place during log ingestion, before data is written to NRDB. Once data has been written to storage, it can no longer be parsed. Parsing occurs in the pipeline before data enrichments take place. Be careful when defining the matching criteria for a parsing rule. If the criteria is based on an attribute that doesn't exist untail after parsing or enrichment take place, that data won't be present in the logs when matching occurs. As a result, no parsing will happen. How Rules can be written in Grok, regex, or a mixture of the two. Grok is a collection of patterns that abstract away complicated regular expressions. If the content of the message field is JSON, it will be parsed automatically. New Relic's log ingestion pipeline can parse data by matching a log event to a rule that describes how the log should be parsed. There are two ways log events can be parsed: Use a built-in rule. Define a custom rule. Rules are a combination of matching logic and parsing logic. Matching is done by defining a query match on an attribute of the logs. Rules are not applied retroactively. Logs collected before a rule is created are not parsed by that rule. The simplest way to organize your logs and how they are parsed is to include the logtype field in your log event. This tells New Relic what built-in ruleset to apply to the logs. Important Once a parsing rule is active, data parsed by the rule is permanently changed. This cannot be reverted. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. Limits Parsing is computationally expensive, which introduces risk. Parsing is done for custom rules defined in an account and for matching patterns to a log. A large number of patterns or poorly defined custom rules will consume a huge amount of memory and CPU resources while also taking a very long time to complete. In order to prevent problems, we apply two parsing limits: per-message-per-rule and per-account. Limit Description Per-message-per-rule The per-message-per-rule limit prevents the time spent parsing any single message from being greater than 100 ms. If that limit is reached, the system will cease attempting to parse the log message with that rule. The ingestion pipeline will attempt to run any other applicable on that message, and the message will still be passed through the ingestion pipeline and stored in NRDB. The log message will be in its original, unparsed format. Per-account The per-account limit exists to prevent accounts from using more than their fair share of resources. The limit considers the total time spent processing all log messages for an account per-minute. The limit is not a fixed value; it scales up or down proportionally to the volume of data stored daily by the account and the environment size that is subsequently allocated to support that customer. Tip To easily check if your rate limits have been reached, go to your system Limits page in the New Relic UI. Built-in parsing rulesets Common log formats have well-established parsing rules already created for them. To get the benefit of built-in parsing rules, add the logtype attribute when forwarding logs. Set the value to something listed in the following table, and the rules for that type of log will be applied automatically. List of built-in rulesets The following logtype attribute values map to a predefined parsing rule. For example, to query the Application Load Balancer: From the New Relic UI, use the format logtype: alb. From NerdGraph, use the format logtype = 'alb'. To learn what fields are parsed for each rule, see our documentation about built-in parsing rules. logtype Example matching query alb AWS Application Load Balancer logtype:alb apache Apache Access logtype:apache cloudfront-web CloudFront Web logtype:cloudfront-web elb Amazon Elastic Load Balancer logtype:elb iis_w3c Microsoft IIS server logs - W3C format logtype:iis_w3c monit Monit logs logtype:monit mysql-error MySQL Error logtype:mysql-error nginx NGINX access logs logtype:nginx nginx-error NGINX error logs logtype:nginx-error route-53 Amazon Route 53 logs logtype:route-53 syslog-rfc5424 Syslog logtype:syslog-rfc5424 Add the logtype attribute When aggregating logs, it's important to provide metadata that makes it easy to organize, search, and parse those logs. One simple way of doing this is to add the attribute logtype to the log messages when they are shipped. Built-in parsing rules are applied by default to certain logtype values. Here are some examples of how to add logtype to logs sent by some of our supported shipping methods. New Relic infrastructure agent example Add logtype as an attribute. You must set the logtype for each named source. Learn more about using the infrastructure agent to add attributes. logs: - name: file-simple file: /path/to/file attributes: logtype: fileRaw - name: nginx-example file: /var/log/nginx.log attributes: logtype: nginx Copy Fluentd example Add a filter block to the .conf file, which uses a record_transformer to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluentd examples. <filter containers> @type record_transformer enable_ruby true <record> #Add logtype to trigger a built-in parsing rule for nginx access logs logtype nginx #Set timestamp from the value contained in the field \"time\" timestamp record[\"time\"] #Add hostname and tag fields to all records hostname \"#{Socket.gethostname}\" tag ${tag} </record> </filter> Copy Fluent Bit example Add a filter block to the .conf file that uses a record_modifier to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluent Bit examples. [FILTER] Name record_modifier Match * Record logtype nginx Record hostname ${HOSTNAME} Record service_name Sample-App-Name Copy Logstash example Add a filter block to the Logstash configuration which uses an add_field mutate filter to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Logstash examples. filter { mutate { add_field => { \"logtype\" => \"nginx\" \"service_name\" => \"myservicename\" \"hostname\" => \"%{host}\" } } } Copy Logs API example You can add attributes to the JSON request sent to New Relic. In this example we add a logtype attribute of value nginx to trigger the built-in NGINX parsing rule. Learn more about using the Logs API. POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: YOUR_LICENSE_KEY Accept: */* Content-Length: 133 { \"timestamp\": TIMESTAMP_IN_UNIX_EPOCH, \"message\": \"User 'xyz' logged in\", \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } Copy Create custom parsing rules Many logs are formatted or structured in a unique way. In order to parse them, custom logic must be built and applied. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. To create and manage your own, custom parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing, then click Create parsing rule. Enter the parsing rule's name. Choose an attribute and value to match on. Write your Grok pattern and test the rule. To learn about Grok and custom parsing rules, read our blog post about how to parse logs with Grok patterns. Enable and save the custom parsing rule. To view the list of custom parsing rules: From Manage Data on the left nav of the Logs UI, click Parsing. To view existing parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing. Troubleshooting If parsing is not working the way you intended, it may be due to: Logic: The parsing rule matching logic does not match the logs you want. Timing: If your parsing matching rule targets a value that doesn't exist yet, it will fail. This can occur if the value is added later in the pipeline as part of the enrichment process. Limits: There is a fixed amount of time available every minute to process logs via parsing, patterns, drop filters, etc. If the maximum amount of time has been spent, parsing will be skipped for additional log event records. To resolve these problems, create or adjust your custom parsing rules.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 212.20872,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Parsing <em>log</em> data",
        "sections": "<em>Logstash</em> example",
        "tags": "<em>Log</em> <em>management</em>",
        "body": "Parsing is the process of splitting unstructured <em>log</em> data into attributes (key&#x2F;value pairs). You can use these attributes to facet or filter <em>logs</em> in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial"
      },
      "id": "603e7eb4196a67b0c4a83dd1"
    }
  ],
  "/docs/logs/log-management/ui-data/built-log-parsing-rulesets": [
    {
      "sections": [
        "Discover value in log data with patterns",
        "Technical overview",
        "Availability",
        "Get started",
        "Explore log patterns",
        "Explore logs with no pattern",
        "Masked attributes and wildcards",
        "Troubleshooting",
        "Put the platform to work with patterns"
      ],
      "title": "Discover value in log data with patterns",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "8f1e27c94327ca4888a945f8e12f9c2310ccd7a6",
      "image": "https://docs.newrelic.com/static/578d7186bb34352855696e5307cc82f2/c1b63/log-patterns-logs-without-a-pattern.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/find-unusual-logs-log-patterns/",
      "published_at": "2021-10-19T03:56:24Z",
      "updated_at": "2021-10-19T03:56:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Log patterns are the fastest way to discover value in log data without searching. Log data is high volume telemetry with a low value per individual record. Searching can quickly lead to logs that provide a root cause explanation, but most data is repetitive and hard to contextualize when browsing. Patterns can make log data discoverable without spending a lot of time reading through low value data. Logs > Patterns: Use patterns as the basis for alerts when the frequency of important data changes, or for configuring drop rules to get rid of unnecessary repetitive data. Technical overview Log patterns functionality applies machine learning to normalize and group log messages that are consistent in format but variable in content. These grouped messages can be sorted, making it easy to find the most frequent or rarest sets of logs in your environment. Use patterns as the basis for alerts when the frequency of important data changes, or to configure drop rules to get rid of unnecessary repetitive data. Log patterns use advanced clustering algorithms to group together similar log messages automatically. With patterns, you can: Orient more quickly through millions of logs. Reduce the time it takes to identify unusual behavior in your log estate. Monitor the frequency of known patterns over time to focus your energy on what matters, and exclude what's irrelevant. Availability The ability to configure this feature is dependent on role-based permissions. For example, accounts still using our original pricing model require an Owner or Admin role. If you see Patterns are turned off in your Log management Patterns UI, click the Configure Patterns button and enable it. If you don't see patterns within 30 minutes of enabling the feature, there may be a lack of data with a message attribute for the system to create a pattern from it. Log patterns Limitations and considerations Pricing There is no separate pricing for log patterns. The only cost is for additional data generated and added to your log records. A pattern attribute will be added to all logs that match a pattern. Attributes also may be added when common values are discovered, such as GUIDs, IP addresses, URL, or email addresses. These attributes are automatically extracted from the log message as part of the pattern process. HITRUST accounts The log patterns feature is not FedRAMP compliant. FedRAMP or other HITRUST accounts are not eligible to use patterns. Parsing limits We have a system of safety limits on memory and CPU resources when processing logs and their patterns. These parsing limits can have an impact on the data you get. For more information, see our documentation about parsing limits. Get started To start examining patterns: Go to one.newrelic.com > Logs, and use the account picker dropdown to select the target account where you want to explore patterns. In the left navigation of the Logs UI, click Patterns. The main log UI changes to show patterns that match the query in the query bar. Logs > Log patterns: The line chart shows the top 5 patterns over time. Use the time picker and query bar to adjust the results. Explore log patterns By default the log patterns UI first shows the most frequent occurrence of patterns. To sort to show the rarest patterns first, click the Count column. You can also use the query bar or attributes bar to filter your log patterns. If you want to... Do this... Understand the rate of change in patterns Look at the line chart. The color-coded patterns correspond to the plot column in the table. You can toggle individual plot patterns to narrow your focus. See the individual log messages that match each pattern Click pattern to expand the row and see a table of individual log records. To see additional records, scroll up or down. To explore an individual log in more detail, click it to open the details panel. Group and filter patterns by their attributes Use the query bar and time picker. As you apply different filters and time windows, the log patterns adjust to your new target data. Create an alert from a pattern Add the pattern to the query bar and run the query. Then click Create alert condition in the left nav. Troubleshoot log messages that haven't been clustered into a pattern Use the Logs with no pattern tab in the Log patterns UI. Clicking a specific log message will open the log message details panel you're familiar with from the Logs management page. Explore logs with no pattern The Logs with no pattern tab groups all recent log messages in your account that were not clustered into a known pattern yet. These log messages don't represent any problem or flaw in the system; they have no pattern because they are too new to have been processed by the machine learning system. This makes them valuable to explore when you want to understand what has recently changed in your environment. Logs > Log patterns: New Relic's log patterns feature automatically groups logs without a matching pattern. For example: Are any of these logs tied to a recent problem? This is a quick way to discover unique log data that is appearing for the first time in your environment. Does your log data have a new format? Sometimes the logs don't represent a problem, but a new format of log data that deviates from the data model you expect your applications to follow. Catching these logs early gives you the opportunity to ask developers to correct any deviations in their log output. The more consistent people are in the way log data is generated, the easier it becomes to use logs across a diverse set of teams. Masked attributes and wildcards Parts of the log messages in patterns are classified as variables and are substituted by masked attributes. The masking process supports and improves the clustering phase by allowing the algorithm to ignore changing details and focus on the repetitive structure. Masked attributes include: date_time ip url uuid Masked attributes are highlighted and are easy to identify, as shown in the following example. Here is an example of a log pattern that has masked attributes. Log patterns extract other less trivial variables that don't belong to any masked attribute. These variables are indicated as wildcards *. Here is an example of how wildcards * group variables in the Log patterns UI. Troubleshooting Here are a few reasons why you might have patterns enabled but not see any pattern data. If you're sure none of the items below are true, get help from support.newrelic.com. No data has arrived in the timeframe you're observing. Try expanding the time range you're viewing with the time picker. It's been less than 24 hours since patterns were enabled in the account. This means the ML model may not be generated for the account yet. None of the data coming in has a message field. Patterns will only be generated for values in the message field of a log record. If your logs don't contain message, there will be no data. Put the platform to work with patterns Patterns are a value that is enriched onto the existing log message as a new attribute named newrelic.logPattern. Anything you can do with logs generally can be done with log patterns, such as: Build your own dashboards with patterns, to monitor a specific pattern or group of patterns you care about. Create alerts for patterns by adding NRQL alerts. Use baseline alert conditions to detect anomalies in known log patterns.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 345.17688,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Discover value in <em>log</em> <em>data</em> with patterns",
        "sections": "Discover value in <em>log</em> <em>data</em> with patterns",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " using our original pricing model require an Owner or Admin role. If you see Patterns are turned off in your <em>Log</em> <em>management</em> Patterns <em>UI</em>, click the Configure Patterns button and enable it. If you don&#x27;t see patterns within 30 minutes of enabling the feature, there may be a lack of <em>data</em> with a message"
      },
      "id": "6072d46128ccbc244451c18b"
    },
    {
      "sections": [
        "Parsing log data",
        "Example",
        "How log parsing works",
        "Important",
        "Limits",
        "Tip",
        "Built-in parsing rulesets",
        "List of built-in rulesets",
        "Add the logtype attribute",
        "New Relic infrastructure agent example",
        "Fluentd example",
        "Fluent Bit example",
        "Logstash example",
        "Logs API example",
        "Create custom parsing rules",
        "Troubleshooting"
      ],
      "title": "Parsing log data",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "52955adb68242c4ca582ba9cb8e22963955a8275",
      "image": "https://docs.newrelic.com/static/dc392bb7142d2fdb253a649daf4ebe6d/c1b63/log-parsing-rule-ui.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/parsing/",
      "published_at": "2021-10-19T03:43:56Z",
      "updated_at": "2021-10-19T03:43:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Parsing is the process of splitting unstructured log data into attributes (key/value pairs). You can use these attributes to facet or filter logs in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial on YouTube (approx. 4-1/2 minutes). New Relic parses log data according to rules. This document describes how logs parsing works, how to use built-in rules, and how to create custom rules. You can also create, query, and manage your log parsing rules by using NerdGraph, our GraphQL API, at api.newrelic.com/graphiql. For more information, see our NerdGraph tutorial for parsing. Example A good example is a default NGINX access log containing unstructured text. It is useful for searching but not much else. Here's an example of a typical line: 127.180.71.3 - - [10/May/1997:08:05:32 +0000] \"GET /downloads/product_1 HTTP/1.1\" 304 0 \"-\" \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" Copy In an unparsed format, you would need to do a full text search to answer most questions. After parsing, the log is organized into attributes, like response code and request URL: { \"remote_addr\":\"93.180.71.3\", \"time\":\"1586514731\", \"method\":\"GET\", \"path\":\"/downloads/product_1\", \"version\":\"HTTP/1.1\", \"response\":\"304\", \"bytesSent\": 0, \"user_agent\": \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" } Copy Parsing makes it easier to create custom queries that facet on those values. This helps you understand the distribution of response codes per request URL and quickly find problematic pages. How log parsing works Here's an overview of how New Relic implements parsing of logs: Log parsing How it works What All parsing takes place against the message field; no other fields can be parsed. Each parsing rule is created with matching criteria that determines which logs the rule will attempt to parse. To simplify the matching process, we recommend adding a logtype attribute to your logs. However, you are not limited to using logtype; any attribute can be used as matching criteria. When Parsing will only be applied once to each log message. If multiple parsing rules match the log, only the first that succeeds will be applied. Parsing takes place during log ingestion, before data is written to NRDB. Once data has been written to storage, it can no longer be parsed. Parsing occurs in the pipeline before data enrichments take place. Be careful when defining the matching criteria for a parsing rule. If the criteria is based on an attribute that doesn't exist untail after parsing or enrichment take place, that data won't be present in the logs when matching occurs. As a result, no parsing will happen. How Rules can be written in Grok, regex, or a mixture of the two. Grok is a collection of patterns that abstract away complicated regular expressions. If the content of the message field is JSON, it will be parsed automatically. New Relic's log ingestion pipeline can parse data by matching a log event to a rule that describes how the log should be parsed. There are two ways log events can be parsed: Use a built-in rule. Define a custom rule. Rules are a combination of matching logic and parsing logic. Matching is done by defining a query match on an attribute of the logs. Rules are not applied retroactively. Logs collected before a rule is created are not parsed by that rule. The simplest way to organize your logs and how they are parsed is to include the logtype field in your log event. This tells New Relic what built-in ruleset to apply to the logs. Important Once a parsing rule is active, data parsed by the rule is permanently changed. This cannot be reverted. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. Limits Parsing is computationally expensive, which introduces risk. Parsing is done for custom rules defined in an account and for matching patterns to a log. A large number of patterns or poorly defined custom rules will consume a huge amount of memory and CPU resources while also taking a very long time to complete. In order to prevent problems, we apply two parsing limits: per-message-per-rule and per-account. Limit Description Per-message-per-rule The per-message-per-rule limit prevents the time spent parsing any single message from being greater than 100 ms. If that limit is reached, the system will cease attempting to parse the log message with that rule. The ingestion pipeline will attempt to run any other applicable on that message, and the message will still be passed through the ingestion pipeline and stored in NRDB. The log message will be in its original, unparsed format. Per-account The per-account limit exists to prevent accounts from using more than their fair share of resources. The limit considers the total time spent processing all log messages for an account per-minute. The limit is not a fixed value; it scales up or down proportionally to the volume of data stored daily by the account and the environment size that is subsequently allocated to support that customer. Tip To easily check if your rate limits have been reached, go to your system Limits page in the New Relic UI. Built-in parsing rulesets Common log formats have well-established parsing rules already created for them. To get the benefit of built-in parsing rules, add the logtype attribute when forwarding logs. Set the value to something listed in the following table, and the rules for that type of log will be applied automatically. List of built-in rulesets The following logtype attribute values map to a predefined parsing rule. For example, to query the Application Load Balancer: From the New Relic UI, use the format logtype: alb. From NerdGraph, use the format logtype = 'alb'. To learn what fields are parsed for each rule, see our documentation about built-in parsing rules. logtype Example matching query alb AWS Application Load Balancer logtype:alb apache Apache Access logtype:apache cloudfront-web CloudFront Web logtype:cloudfront-web elb Amazon Elastic Load Balancer logtype:elb iis_w3c Microsoft IIS server logs - W3C format logtype:iis_w3c monit Monit logs logtype:monit mysql-error MySQL Error logtype:mysql-error nginx NGINX access logs logtype:nginx nginx-error NGINX error logs logtype:nginx-error route-53 Amazon Route 53 logs logtype:route-53 syslog-rfc5424 Syslog logtype:syslog-rfc5424 Add the logtype attribute When aggregating logs, it's important to provide metadata that makes it easy to organize, search, and parse those logs. One simple way of doing this is to add the attribute logtype to the log messages when they are shipped. Built-in parsing rules are applied by default to certain logtype values. Here are some examples of how to add logtype to logs sent by some of our supported shipping methods. New Relic infrastructure agent example Add logtype as an attribute. You must set the logtype for each named source. Learn more about using the infrastructure agent to add attributes. logs: - name: file-simple file: /path/to/file attributes: logtype: fileRaw - name: nginx-example file: /var/log/nginx.log attributes: logtype: nginx Copy Fluentd example Add a filter block to the .conf file, which uses a record_transformer to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluentd examples. <filter containers> @type record_transformer enable_ruby true <record> #Add logtype to trigger a built-in parsing rule for nginx access logs logtype nginx #Set timestamp from the value contained in the field \"time\" timestamp record[\"time\"] #Add hostname and tag fields to all records hostname \"#{Socket.gethostname}\" tag ${tag} </record> </filter> Copy Fluent Bit example Add a filter block to the .conf file that uses a record_modifier to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluent Bit examples. [FILTER] Name record_modifier Match * Record logtype nginx Record hostname ${HOSTNAME} Record service_name Sample-App-Name Copy Logstash example Add a filter block to the Logstash configuration which uses an add_field mutate filter to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Logstash examples. filter { mutate { add_field => { \"logtype\" => \"nginx\" \"service_name\" => \"myservicename\" \"hostname\" => \"%{host}\" } } } Copy Logs API example You can add attributes to the JSON request sent to New Relic. In this example we add a logtype attribute of value nginx to trigger the built-in NGINX parsing rule. Learn more about using the Logs API. POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: YOUR_LICENSE_KEY Accept: */* Content-Length: 133 { \"timestamp\": TIMESTAMP_IN_UNIX_EPOCH, \"message\": \"User 'xyz' logged in\", \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } Copy Create custom parsing rules Many logs are formatted or structured in a unique way. In order to parse them, custom logic must be built and applied. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. To create and manage your own, custom parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing, then click Create parsing rule. Enter the parsing rule's name. Choose an attribute and value to match on. Write your Grok pattern and test the rule. To learn about Grok and custom parsing rules, read our blog post about how to parse logs with Grok patterns. Enable and save the custom parsing rule. To view the list of custom parsing rules: From Manage Data on the left nav of the Logs UI, click Parsing. To view existing parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing. Troubleshooting If parsing is not working the way you intended, it may be due to: Logic: The parsing rule matching logic does not match the logs you want. Timing: If your parsing matching rule targets a value that doesn't exist yet, it will fail. This can occur if the value is added later in the pipeline as part of the enrichment process. Limits: There is a fixed amount of time available every minute to process logs via parsing, patterns, drop filters, etc. If the maximum amount of time has been spent, parsing will be skipped for additional log event records. To resolve these problems, create or adjust your custom parsing rules.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 344.9994,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Parsing <em>log</em> <em>data</em>",
        "sections": "Parsing <em>log</em> <em>data</em>",
        "tags": "<em>Log</em> <em>management</em>",
        "body": "Parsing is the process of splitting unstructured <em>log</em> <em>data</em> into attributes (key&#x2F;value pairs). You can use these attributes to facet or filter <em>logs</em> in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial"
      },
      "id": "603e7eb4196a67b0c4a83dd1"
    },
    {
      "sections": [
        "Drop data with drop filter rules",
        "Savings, security, speed",
        "Caution",
        "How drop filter rules work",
        "Cautions when dropping data",
        "Create drop filter rules",
        "Manage drop filter rules via NerdGraph API",
        "Types of drop filter rules",
        "Drop log events",
        "Drop attributes",
        "Tip",
        "View or delete drop filter rules"
      ],
      "title": "Drop data with drop filter rules",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "9590bd4593abd451633a4beacd94d56eb1a481bd",
      "image": "https://docs.newrelic.com/static/db4b077fafd911b9f5019b022b3048ab/b04e4/ingest-pipeline.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/drop-data-drop-filter-rules/",
      "published_at": "2021-10-19T03:43:55Z",
      "updated_at": "2021-10-19T03:43:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After log event data has been shipped to New Relic, it can either be stored in our NRDB database or dropped (discarded). We can drop both log events and event attributes via drop filter rules. You can manage drop filter rules using our Logs UI, as explained in this document. You can also use NerdGraph, our GraphQL-format API explorer. Savings, security, speed Drop filter rules help you accomplish several important goals: Lower costs by storing only logs relevant to your account. Protect privacy and security by removing personal identifiable information (PII). Reduce noise by removing irrelevant events and attributes. Caution Use caution when deciding to drop data. The data you drop is not recoverable. Before using this feature, review the responsibilities and considerations for dropping data. How drop filter rules work A drop filter rule matches data based on a query. When triggered, the drop filter rule removes the matching data from the ingestion pipeline before it is written to NRDB. This creates an explicit demarcation between the logs being forwarded from your domain and the data that New Relic collects. Since the data removed by the drop filter rule doesn't reach our backend, it cannot be queried: the data is gone and cannot be restored. During the ingestion process, customer log data can be parsed, transformed, or dropped before being stored in New Relic's database. Cautions when dropping data When creating drop rules, you are responsible for ensuring that the rules accurately identify and discard the data that meets the conditions that you have established. You are also responsible for monitoring the rule, as well as the data you disclose to New Relic. New Relic cannot guarantee that this functionality will completely resolve data disclosure concerns you may have. New Relic doesn't review or monitor how effective the rules you develop are. Creating rules about sensitive data can leak information about what kinds of data you maintain, including the format of your data or systems (for example, through referencing email addresses or specific credit card numbers). Any user with the relevant role-based access control permissions can view and edit all information in the rules you create. Create drop filter rules To create and edit drop filters, you must have admin permissions in New Relic, or you must be a member of a role with create and edit permissions for Insights > NRQL Drop Rules. Once a drop filter rule is active, it's applied to all log events ingested from that point onwards. Rules are not applied retroactively. Logs collected before creating a rule are not filtered by that rule. Filter or query the set of logs that contain the data you want to drop. Then, from Manage Data on the left nav of the Logs UI, click Create drop filter. To create a new drop filter rule, you can use new or existing log queries. Go to one.newrelic.com > Logs. Filter or query to the specific set of logs that contain the data to be dropped. Once the query is active, from Manage Data on the left nav of the Logs UI, click Create drop filter. Recommendation: Change the drop rule's default name to a meaningful name. Choose to either drop the entire log event that matches the query or just a specific subset of attributes in the matching events. Review the log partitions where this drop rule applies. Save the drop filter rule. Manage drop filter rules via NerdGraph API If you want to manage your drop filter rules programmatically, you can use NerdGraph, our graphQL-format API, at api.newrelic.com/graphiql. For more information, see the NerdGraph tutorial to create, query, and delete your drop filter rules. Types of drop filter rules The drop filters UI prompts you to select whether to drop logs based on the query or on specific attributes. Drop log events The default type of drop filter rule is to drop logs. This option drops the entire log events that match the filter or query. When creating a rule, try to provide a specific query that only matches log data that should be dropped. Our drop filters process won't let you create drop filter rules without values in the matching query. This prevents badly formed rules from dropping all log data. Drop attributes You can specify attributes to be dropped in a log event that matches your query. At least one or more attributes must be selected. Any attribute which is selected will be dropped; all remaining attributes will be kept and stored in NRDB. Tip We recommend this method for removing fields that could contain personal identifiable information (PII) or other sensitive attributes without losing valuable monitoring data. View or delete drop filter rules To view or delete a drop filter rule: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Drop filters. Click the delete icon next to the drop filter rule you want to remove. Once deleted, rules no longer filter ingested log events.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 344.99918,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Drop <em>data</em> with drop filter rules",
        "sections": "Drop <em>data</em> with drop filter rules",
        "tags": "<em>Log</em> <em>management</em>",
        "body": "After <em>log</em> event <em>data</em> has been shipped to New Relic, it can either be stored in our NRDB database or dropped (discarded). We can drop both <em>log</em> events and event attributes via drop filter rules. You can manage drop filter rules using our <em>Logs</em> <em>UI</em>, as explained in this document. You can also use"
      },
      "id": "603e813f28ccbc08c1eba787"
    }
  ],
  "/docs/logs/log-management/ui-data/data-partitions": [
    {
      "sections": [
        "Discover value in log data with patterns",
        "Technical overview",
        "Availability",
        "Get started",
        "Explore log patterns",
        "Explore logs with no pattern",
        "Masked attributes and wildcards",
        "Troubleshooting",
        "Put the platform to work with patterns"
      ],
      "title": "Discover value in log data with patterns",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "8f1e27c94327ca4888a945f8e12f9c2310ccd7a6",
      "image": "https://docs.newrelic.com/static/578d7186bb34352855696e5307cc82f2/c1b63/log-patterns-logs-without-a-pattern.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/find-unusual-logs-log-patterns/",
      "published_at": "2021-10-19T03:56:24Z",
      "updated_at": "2021-10-19T03:56:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Log patterns are the fastest way to discover value in log data without searching. Log data is high volume telemetry with a low value per individual record. Searching can quickly lead to logs that provide a root cause explanation, but most data is repetitive and hard to contextualize when browsing. Patterns can make log data discoverable without spending a lot of time reading through low value data. Logs > Patterns: Use patterns as the basis for alerts when the frequency of important data changes, or for configuring drop rules to get rid of unnecessary repetitive data. Technical overview Log patterns functionality applies machine learning to normalize and group log messages that are consistent in format but variable in content. These grouped messages can be sorted, making it easy to find the most frequent or rarest sets of logs in your environment. Use patterns as the basis for alerts when the frequency of important data changes, or to configure drop rules to get rid of unnecessary repetitive data. Log patterns use advanced clustering algorithms to group together similar log messages automatically. With patterns, you can: Orient more quickly through millions of logs. Reduce the time it takes to identify unusual behavior in your log estate. Monitor the frequency of known patterns over time to focus your energy on what matters, and exclude what's irrelevant. Availability The ability to configure this feature is dependent on role-based permissions. For example, accounts still using our original pricing model require an Owner or Admin role. If you see Patterns are turned off in your Log management Patterns UI, click the Configure Patterns button and enable it. If you don't see patterns within 30 minutes of enabling the feature, there may be a lack of data with a message attribute for the system to create a pattern from it. Log patterns Limitations and considerations Pricing There is no separate pricing for log patterns. The only cost is for additional data generated and added to your log records. A pattern attribute will be added to all logs that match a pattern. Attributes also may be added when common values are discovered, such as GUIDs, IP addresses, URL, or email addresses. These attributes are automatically extracted from the log message as part of the pattern process. HITRUST accounts The log patterns feature is not FedRAMP compliant. FedRAMP or other HITRUST accounts are not eligible to use patterns. Parsing limits We have a system of safety limits on memory and CPU resources when processing logs and their patterns. These parsing limits can have an impact on the data you get. For more information, see our documentation about parsing limits. Get started To start examining patterns: Go to one.newrelic.com > Logs, and use the account picker dropdown to select the target account where you want to explore patterns. In the left navigation of the Logs UI, click Patterns. The main log UI changes to show patterns that match the query in the query bar. Logs > Log patterns: The line chart shows the top 5 patterns over time. Use the time picker and query bar to adjust the results. Explore log patterns By default the log patterns UI first shows the most frequent occurrence of patterns. To sort to show the rarest patterns first, click the Count column. You can also use the query bar or attributes bar to filter your log patterns. If you want to... Do this... Understand the rate of change in patterns Look at the line chart. The color-coded patterns correspond to the plot column in the table. You can toggle individual plot patterns to narrow your focus. See the individual log messages that match each pattern Click pattern to expand the row and see a table of individual log records. To see additional records, scroll up or down. To explore an individual log in more detail, click it to open the details panel. Group and filter patterns by their attributes Use the query bar and time picker. As you apply different filters and time windows, the log patterns adjust to your new target data. Create an alert from a pattern Add the pattern to the query bar and run the query. Then click Create alert condition in the left nav. Troubleshoot log messages that haven't been clustered into a pattern Use the Logs with no pattern tab in the Log patterns UI. Clicking a specific log message will open the log message details panel you're familiar with from the Logs management page. Explore logs with no pattern The Logs with no pattern tab groups all recent log messages in your account that were not clustered into a known pattern yet. These log messages don't represent any problem or flaw in the system; they have no pattern because they are too new to have been processed by the machine learning system. This makes them valuable to explore when you want to understand what has recently changed in your environment. Logs > Log patterns: New Relic's log patterns feature automatically groups logs without a matching pattern. For example: Are any of these logs tied to a recent problem? This is a quick way to discover unique log data that is appearing for the first time in your environment. Does your log data have a new format? Sometimes the logs don't represent a problem, but a new format of log data that deviates from the data model you expect your applications to follow. Catching these logs early gives you the opportunity to ask developers to correct any deviations in their log output. The more consistent people are in the way log data is generated, the easier it becomes to use logs across a diverse set of teams. Masked attributes and wildcards Parts of the log messages in patterns are classified as variables and are substituted by masked attributes. The masking process supports and improves the clustering phase by allowing the algorithm to ignore changing details and focus on the repetitive structure. Masked attributes include: date_time ip url uuid Masked attributes are highlighted and are easy to identify, as shown in the following example. Here is an example of a log pattern that has masked attributes. Log patterns extract other less trivial variables that don't belong to any masked attribute. These variables are indicated as wildcards *. Here is an example of how wildcards * group variables in the Log patterns UI. Troubleshooting Here are a few reasons why you might have patterns enabled but not see any pattern data. If you're sure none of the items below are true, get help from support.newrelic.com. No data has arrived in the timeframe you're observing. Try expanding the time range you're viewing with the time picker. It's been less than 24 hours since patterns were enabled in the account. This means the ML model may not be generated for the account yet. None of the data coming in has a message field. Patterns will only be generated for values in the message field of a log record. If your logs don't contain message, there will be no data. Put the platform to work with patterns Patterns are a value that is enriched onto the existing log message as a new attribute named newrelic.logPattern. Anything you can do with logs generally can be done with log patterns, such as: Build your own dashboards with patterns, to monitor a specific pattern or group of patterns you care about. Create alerts for patterns by adding NRQL alerts. Use baseline alert conditions to detect anomalies in known log patterns.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 345.17688,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Discover value in <em>log</em> <em>data</em> with patterns",
        "sections": "Discover value in <em>log</em> <em>data</em> with patterns",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " using our original pricing model require an Owner or Admin role. If you see Patterns are turned off in your <em>Log</em> <em>management</em> Patterns <em>UI</em>, click the Configure Patterns button and enable it. If you don&#x27;t see patterns within 30 minutes of enabling the feature, there may be a lack of <em>data</em> with a message"
      },
      "id": "6072d46128ccbc244451c18b"
    },
    {
      "sections": [
        "Parsing log data",
        "Example",
        "How log parsing works",
        "Important",
        "Limits",
        "Tip",
        "Built-in parsing rulesets",
        "List of built-in rulesets",
        "Add the logtype attribute",
        "New Relic infrastructure agent example",
        "Fluentd example",
        "Fluent Bit example",
        "Logstash example",
        "Logs API example",
        "Create custom parsing rules",
        "Troubleshooting"
      ],
      "title": "Parsing log data",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "52955adb68242c4ca582ba9cb8e22963955a8275",
      "image": "https://docs.newrelic.com/static/dc392bb7142d2fdb253a649daf4ebe6d/c1b63/log-parsing-rule-ui.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/parsing/",
      "published_at": "2021-10-19T03:43:56Z",
      "updated_at": "2021-10-19T03:43:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Parsing is the process of splitting unstructured log data into attributes (key/value pairs). You can use these attributes to facet or filter logs in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial on YouTube (approx. 4-1/2 minutes). New Relic parses log data according to rules. This document describes how logs parsing works, how to use built-in rules, and how to create custom rules. You can also create, query, and manage your log parsing rules by using NerdGraph, our GraphQL API, at api.newrelic.com/graphiql. For more information, see our NerdGraph tutorial for parsing. Example A good example is a default NGINX access log containing unstructured text. It is useful for searching but not much else. Here's an example of a typical line: 127.180.71.3 - - [10/May/1997:08:05:32 +0000] \"GET /downloads/product_1 HTTP/1.1\" 304 0 \"-\" \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" Copy In an unparsed format, you would need to do a full text search to answer most questions. After parsing, the log is organized into attributes, like response code and request URL: { \"remote_addr\":\"93.180.71.3\", \"time\":\"1586514731\", \"method\":\"GET\", \"path\":\"/downloads/product_1\", \"version\":\"HTTP/1.1\", \"response\":\"304\", \"bytesSent\": 0, \"user_agent\": \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" } Copy Parsing makes it easier to create custom queries that facet on those values. This helps you understand the distribution of response codes per request URL and quickly find problematic pages. How log parsing works Here's an overview of how New Relic implements parsing of logs: Log parsing How it works What All parsing takes place against the message field; no other fields can be parsed. Each parsing rule is created with matching criteria that determines which logs the rule will attempt to parse. To simplify the matching process, we recommend adding a logtype attribute to your logs. However, you are not limited to using logtype; any attribute can be used as matching criteria. When Parsing will only be applied once to each log message. If multiple parsing rules match the log, only the first that succeeds will be applied. Parsing takes place during log ingestion, before data is written to NRDB. Once data has been written to storage, it can no longer be parsed. Parsing occurs in the pipeline before data enrichments take place. Be careful when defining the matching criteria for a parsing rule. If the criteria is based on an attribute that doesn't exist untail after parsing or enrichment take place, that data won't be present in the logs when matching occurs. As a result, no parsing will happen. How Rules can be written in Grok, regex, or a mixture of the two. Grok is a collection of patterns that abstract away complicated regular expressions. If the content of the message field is JSON, it will be parsed automatically. New Relic's log ingestion pipeline can parse data by matching a log event to a rule that describes how the log should be parsed. There are two ways log events can be parsed: Use a built-in rule. Define a custom rule. Rules are a combination of matching logic and parsing logic. Matching is done by defining a query match on an attribute of the logs. Rules are not applied retroactively. Logs collected before a rule is created are not parsed by that rule. The simplest way to organize your logs and how they are parsed is to include the logtype field in your log event. This tells New Relic what built-in ruleset to apply to the logs. Important Once a parsing rule is active, data parsed by the rule is permanently changed. This cannot be reverted. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. Limits Parsing is computationally expensive, which introduces risk. Parsing is done for custom rules defined in an account and for matching patterns to a log. A large number of patterns or poorly defined custom rules will consume a huge amount of memory and CPU resources while also taking a very long time to complete. In order to prevent problems, we apply two parsing limits: per-message-per-rule and per-account. Limit Description Per-message-per-rule The per-message-per-rule limit prevents the time spent parsing any single message from being greater than 100 ms. If that limit is reached, the system will cease attempting to parse the log message with that rule. The ingestion pipeline will attempt to run any other applicable on that message, and the message will still be passed through the ingestion pipeline and stored in NRDB. The log message will be in its original, unparsed format. Per-account The per-account limit exists to prevent accounts from using more than their fair share of resources. The limit considers the total time spent processing all log messages for an account per-minute. The limit is not a fixed value; it scales up or down proportionally to the volume of data stored daily by the account and the environment size that is subsequently allocated to support that customer. Tip To easily check if your rate limits have been reached, go to your system Limits page in the New Relic UI. Built-in parsing rulesets Common log formats have well-established parsing rules already created for them. To get the benefit of built-in parsing rules, add the logtype attribute when forwarding logs. Set the value to something listed in the following table, and the rules for that type of log will be applied automatically. List of built-in rulesets The following logtype attribute values map to a predefined parsing rule. For example, to query the Application Load Balancer: From the New Relic UI, use the format logtype: alb. From NerdGraph, use the format logtype = 'alb'. To learn what fields are parsed for each rule, see our documentation about built-in parsing rules. logtype Example matching query alb AWS Application Load Balancer logtype:alb apache Apache Access logtype:apache cloudfront-web CloudFront Web logtype:cloudfront-web elb Amazon Elastic Load Balancer logtype:elb iis_w3c Microsoft IIS server logs - W3C format logtype:iis_w3c monit Monit logs logtype:monit mysql-error MySQL Error logtype:mysql-error nginx NGINX access logs logtype:nginx nginx-error NGINX error logs logtype:nginx-error route-53 Amazon Route 53 logs logtype:route-53 syslog-rfc5424 Syslog logtype:syslog-rfc5424 Add the logtype attribute When aggregating logs, it's important to provide metadata that makes it easy to organize, search, and parse those logs. One simple way of doing this is to add the attribute logtype to the log messages when they are shipped. Built-in parsing rules are applied by default to certain logtype values. Here are some examples of how to add logtype to logs sent by some of our supported shipping methods. New Relic infrastructure agent example Add logtype as an attribute. You must set the logtype for each named source. Learn more about using the infrastructure agent to add attributes. logs: - name: file-simple file: /path/to/file attributes: logtype: fileRaw - name: nginx-example file: /var/log/nginx.log attributes: logtype: nginx Copy Fluentd example Add a filter block to the .conf file, which uses a record_transformer to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluentd examples. <filter containers> @type record_transformer enable_ruby true <record> #Add logtype to trigger a built-in parsing rule for nginx access logs logtype nginx #Set timestamp from the value contained in the field \"time\" timestamp record[\"time\"] #Add hostname and tag fields to all records hostname \"#{Socket.gethostname}\" tag ${tag} </record> </filter> Copy Fluent Bit example Add a filter block to the .conf file that uses a record_modifier to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluent Bit examples. [FILTER] Name record_modifier Match * Record logtype nginx Record hostname ${HOSTNAME} Record service_name Sample-App-Name Copy Logstash example Add a filter block to the Logstash configuration which uses an add_field mutate filter to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Logstash examples. filter { mutate { add_field => { \"logtype\" => \"nginx\" \"service_name\" => \"myservicename\" \"hostname\" => \"%{host}\" } } } Copy Logs API example You can add attributes to the JSON request sent to New Relic. In this example we add a logtype attribute of value nginx to trigger the built-in NGINX parsing rule. Learn more about using the Logs API. POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: YOUR_LICENSE_KEY Accept: */* Content-Length: 133 { \"timestamp\": TIMESTAMP_IN_UNIX_EPOCH, \"message\": \"User 'xyz' logged in\", \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } Copy Create custom parsing rules Many logs are formatted or structured in a unique way. In order to parse them, custom logic must be built and applied. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. To create and manage your own, custom parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing, then click Create parsing rule. Enter the parsing rule's name. Choose an attribute and value to match on. Write your Grok pattern and test the rule. To learn about Grok and custom parsing rules, read our blog post about how to parse logs with Grok patterns. Enable and save the custom parsing rule. To view the list of custom parsing rules: From Manage Data on the left nav of the Logs UI, click Parsing. To view existing parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing. Troubleshooting If parsing is not working the way you intended, it may be due to: Logic: The parsing rule matching logic does not match the logs you want. Timing: If your parsing matching rule targets a value that doesn't exist yet, it will fail. This can occur if the value is added later in the pipeline as part of the enrichment process. Limits: There is a fixed amount of time available every minute to process logs via parsing, patterns, drop filters, etc. If the maximum amount of time has been spent, parsing will be skipped for additional log event records. To resolve these problems, create or adjust your custom parsing rules.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 344.9994,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Parsing <em>log</em> <em>data</em>",
        "sections": "Parsing <em>log</em> <em>data</em>",
        "tags": "<em>Log</em> <em>management</em>",
        "body": "Parsing is the process of splitting unstructured <em>log</em> <em>data</em> into attributes (key&#x2F;value pairs). You can use these attributes to facet or filter <em>logs</em> in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial"
      },
      "id": "603e7eb4196a67b0c4a83dd1"
    },
    {
      "sections": [
        "Built-in log parsing rulesets",
        "Apache",
        "Application Load Balancer",
        "Cloudfront",
        "Elastic Load Balancer",
        "Microsoft IIS",
        "Monit",
        "MySQL Error",
        "NGINX",
        "NGINX Error",
        "Route 53",
        "Syslog RFC-5424"
      ],
      "title": "Built-in log parsing rulesets",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "cb5909f2453d475a85d408d75cd3b2a321a8518e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/built-log-parsing-rulesets/",
      "published_at": "2021-10-19T03:43:56Z",
      "updated_at": "2021-10-19T03:43:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic can parse common log formats according to built-in rulesets, so that you don't have to create your own parsing rules. Here are the log parsing rulesets, their Grok patterns, and what fields are parsed. To enable built-in log parsing, see our documentation for adding the logtype attribute. To manage your parsing rulesets programmatically, you can use NerdGraph, our graphQL-format API, at api.newrelic.com/graphiql. For more information, see the NerdGraph tutorial to create, query, and delete your parsing rulesets. Apache Source: logtype = 'apache' Grok: %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{NUMBER:bytes}|-) %{QS:referrer} %{QS:agent} Copy Results: clientip: The IP address of the client. verb: The HTTP verb ident: The user identity of the client making the request response: The HTTP status code of the response request: The URI and request being made httpversion: The HTTP version of the request rawrequest: The raw HTTP request if data is posted bytes: The number of bytes sent referrer: The HTTP referrer agent: The client's user agent Application Load Balancer Source: logtype = 'alb' Grok: ^%{NOTSPACE:type} %{TIMESTAMP_ISO8601:time} %{NOTSPACE:elb} %{NOTSPACE:client_ip}:%{NOTSPACE:client_port} ((%{NOTSPACE:target_ip}:%{NOTSPACE:target_port})|-) %{NOTSPACE:request_processing_time} %{NOTSPACE:target_processing_time} %{NOTSPACE:response_processing_time} %{NOTSPACE:elb_status_code} %{NOTSPACE:target_status_code} %{NOTSPACE:received_bytes} %{NOTSPACE:sent_bytes} \"%{DATA:request}\" \"%{DATA:user_agent}\" %{NOTSPACE:ssl_cipher} %{NOTSPACE:ssl_protocol} %{NOTSPACE:target_group_arn} \"%{DATA:trace_id}\" \"%{NOTSPACE:domain_name}\" \"%{NOTSPACE:chosen_cert_arn}\" %{NOTSPACE:matched_rule_priority} %{TIMESTAMP_ISO8601:request_creation_time} \"%{NOTSPACE:actions_executed}\" \"%{NOTSPACE:redirect_url}\" \"%{NOTSPACE:error_reason}\" (?:\"|)%{DATA:target_port_list}(?:\"|) (?:\"|)%{DATA:target_status_code_list}(?:\"|) \"%{NOTSPACE:classification}\" \"%{NOTSPACE:classification_reason}\" Copy Results: Field Definition type The type of request or connection. Possible values are: http: HTTP https: HTTP over SSL/TLS h2: HTTP/2 over SSL/TLS ws: WebSockets wss: WebSockets over SSL/TLS elb The resource ID of the load balancer. If you are parsing access log entries, note that resources IDs can contain forward slashes (/). client The IP address and port of the requesting client target The IP address and port of the target that processed this request. If the client didn't send a full request, the load balancer can't dispatch the request to a target, and this value is set to -. If the target is a Lambda function, this value is set to -. If the request is blocked by AWS WAF, this value is set to -, and the value of elb_status_code is set to 403. request_processing_time The total time elapsed (in seconds, with millisecond precision) from the time the load balancer received the request until the time it sent it to a target. This value is set to -1 if the load balancer can't dispatch the request to a target. This can happen if the target closes the connection before the idle timeout or if the client sends a malformed request. This value can also be set to -1 if the registered target does not respond before the idle timeout. target_processing_time The total time elapsed (in seconds, with millisecond precision) from the time the load balancer sent the request to a target until the target started to send the response headers. This value is set to -1 if the load balancer can't dispatch the request to a target. This can happen if the target closes the connection before the idle timeout or if the client sends a malformed request. This value can also be set to -1 if the registered target does not respond before the idle timeout. response_processing_time The total time elapsed (in seconds, with millisecond precision) from the time the load balancer received the response header from the target until it started to send the response to the client. This includes both the queuing time at the load balancer and the connection acquisition time from the load balancer to the client. This value is set to -1 if the load balancer can't send the request to a target. This can happen if the target closes the connection before the idle timeout or if the client sends a malformed request. elb_status_code The status code of the response from the load balancer target_status_code The status code of the response from the target. This value is recorded only if a connection was established to the target and the target sent a response. Otherwise, it is set to -. received_bytes The size of the request, in bytes, received from the client (requester). For HTTP requests, this includes the headers. For WebSockets, this is the total number of bytes received from the client on the connection. sent_bytes The size of the response, in bytes, sent to the client (requester). For HTTP requests, this includes the headers. For WebSockets, this is the total number of bytes sent to the client on the connection. method The HTTP verb of the request uri The URI the request was targeting http_version The HTTP version number of the request user_agent User-Agent string that identifies the client that originated the request, enclosed in double quotes. The string consists of one or more product identifiers, product/version. If the string is longer than 8 KB, it is truncated. ssl_cipher The SSL cipher. This value is set to - if the listener is not an HTTPS listener. ssl_protocol The SSL protocol. This value is set to - if the listener is not an HTTPS listener. target_group_arn The Amazon Resource Name (ARN) of the target group trace_id The contents of the X-Amzn-Trace-Id header, enclosed in double quotes domain_name The SNI domain provided by the client during the TLS handshake, enclosed in double quotes. This value is set to - if the client doesn't support SNI or the domain doesn't match a certificate and the default certificate is presented to the client. chosen_cert_arn The ARN of the certificate presented to the client, enclosed in double quotes. Set to session-reused if the session is reused. Set to - if the listener is not an HTTPS listener. matched_rule_priority The priority value of the rule that matched the request. If a rule matched, this is a value from 1 to 50000. If no rule matched and the default action was taken, this value is set to 0. If an error occurs during rules evaluation, it is set to -1. For any other error, it is set to -. request_creation_time The time when the load balancer received the request from the client, in ISO 8601 format. actions_executed The actions taken when processing the request, enclosed in double quotes. This value is a comma-separated list that can include the values described in actions_taken. If no action was taken, such as for a malformed request, this value is set to -. redirect_url The URL of the redirect target for the location header of the HTTP response, enclosed in double quotes. If no redirect actions were taken, this value is set to -. error_reason The error reason code, enclosed in double quotes. If the request failed, this is one of the error codes described in Error Reason Codes. If the actions taken do not include an authenticate action or the target is not a Lambda function, this value is set to -. Cloudfront Source: logtype = 'cloudfront-web' Grok: ^%{NOTSPACE:date}%{SPACE}%{NOTSPACE:time}%{SPACE}%{NOTSPACE:x_edge_location}%{SPACE}%{NOTSPACE:sc_bytes}%{SPACE}%{NOTSPACE:c_ip}%{SPACE}%{NOTSPACE:cs_method}%{SPACE}%{NOTSPACE:cs_host}%{SPACE}%{NOTSPACE:cs_uri_stem}%{SPACE}%{NOTSPACE:sc_status}%{SPACE}%{NOTSPACE:cs_referer}%{SPACE}%{NOTSPACE:cs_user_agent}%{SPACE}%{NOTSPACE:cs_uri_query}%{SPACE}%{NOTSPACE:cs_Cookie}%{SPACE}%{NOTSPACE:x_edge_result_type}%{SPACE}%{NOTSPACE:x_edge_request_id}%{SPACE}%{NOTSPACE:x_host_header}%{SPACE}%{NOTSPACE:cs_protocol}%{SPACE}%{NOTSPACE:cs_bytes}%{SPACE}%{NOTSPACE:time_taken}%{SPACE}%{NOTSPACE:x_forwarded_for}%{SPACE}%{NOTSPACE:ssl_protocol}%{SPACE}%{NOTSPACE:ssl_cipher}%{SPACE}%{NOTSPACE:x_edge_response_result_type}%{SPACE}%{NOTSPACE:cs_protocol_version}%{SPACE}%{NOTSPACE:fle_status}%{SPACE}%{NOTSPACE:fle_encrypted_fields}%{SPACE}%{NOTSPACE:c_port}%{SPACE}%{NOTSPACE:time_to_first_byte}%{SPACE}%{NOTSPACE:x_edge_detailed_result_type}%{SPACE}%{NOTSPACE:sc_content_type}%{SPACE}%{NOTSPACE:sc_content_len}%{SPACE}%{NOTSPACE:sc_range_start}%{SPACE}%{NOTSPACE:sc_range_end} Copy Results: Field Definition x_edge_location The edge location that served the request. Each edge location is identified by a three-letter code and an arbitrarily assigned number; for example, DFW3. The three-letter code typically corresponds with the International Air Transport Association airport code for an airport near the edge location. (These abbreviations might change in the future.) sc_bytes The total number of bytes that CloudFront served to the viewer in response to the request, including headers; for example, 1045619. For WebSocket connections, this is the total number of bytes sent from the server to the client through the connection. c_ip The IP address of the viewer that made the request, either in IPv4 or IPv6 format. If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip is the IP address of the proxy or load balancer. See also X-Forwarded-For. cs_method The HTTP request method: DELETE, GET, HEAD, OPTIONS, PATCH, POST, or PUT. cs_host The domain name of the CloudFront distribution; for example, d111111abcdef8.cloudfront.net. cs_uri_stem The portion of the URI that identifies the path and object; for example, /images/cat.jpg. Question marks in URLs and query strings are not included. sc_status An HTTP status code; for example, 200. Status code 000 indicates the viewer closed the connection (for example, closed the browser tab) before CloudFront could respond to a request. If the viewer closes the connection after CloudFront starts to send the response, the log contains the applicable HTTP status code. cs_referer The name of the domain that originated the request. Common referrers include search engines, other websites that link directly to your objects, and your own website. cs_user_agent The value of the User-Agent header in the request. The User-Agent header identifies the source of the request, such as the type of device and browser that submitted the request, and which search engine if applicable. cs_uri_query The query string portion of the URI, if any. When a URI doesn't contain a query string, this field's value is a hyphen (-). cs_cookie The cookie header in the request, including name-value pairs and the associated attributes. If you enable cookie logging, CloudFront logs the cookies in all requests regardless of which cookies you choose to forward to the origin. If a request doesn't include a cookie header, this field's value is a hyphen (-). x_edge_result_type How CloudFront classifies the response after the last byte left the edge location. In some cases, the result type can change between the time that CloudFront is ready to send the response and the time that CloudFront has finished sending the response. x_edge_request_id An encrypted string that uniquely identifies a request. In the response header, this is x-amz-cf-id. x_host_header The value that the viewer included in the Host header for this request. This is the domain name in the request. If you're using the CloudFront domain name in your object URLs, this field contains that domain name. If you're using alternate domain names in your object URLs, such as http://example.com/logo.png, this field contains the alternate domain name, such as example.com. To use alternate domain names, you must add them to your distribution. cs_protocol The protocol that the viewer specified in the request: http, https, ws, or wss. cs_bytes The number of bytes of data that the viewer included in the request, including headers. For WebSocket connections, this is the total number of bytes sent from the client to the server on the connection. time_taken The number of seconds (to the thousandth of a second; for example, 0.002) between the time that a CloudFront edge server receives a viewer's request and the time that CloudFront writes the last byte of the response to the edge server's output queue as measured on the server. From the perspective of the viewer, the total time to get the full object will be longer than this value due to network latency and TCP buffering. x_forwarded_for If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip in field 5 is the IP address of the proxy or load balancer. In that case, this field is the IP address of the viewer that originated the request. This field contains IPv4 and IPv6 addresses as applicable. If the viewer did not use an HTTP proxy or a load balancer, the value of x_forwarded_for is a hyphen (-). ssl_protocol When cs_protocol in field 17 is https, this field contains the SSL/TLS protocol that the client and CloudFront negotiated for transmitting the request and response. Possible values include: SSLv3 TLSv1 TLSv1.1 TLSv1.2 When cs_protocol in field 17 is http, the value for this field is a hyphen (-). ssl_cipher When cs_protocol in field 17 is https, this field contains the SSL/TLS cipher that the client and CloudFront negotiated for encrypting the request and response. Possible values include: ECDHE-RSA-AES128-GCM-SHA256 ECDHE-RSA-AES128-SHA256 ECDHE-RSA-AES128-SHA ECDHE-RSA-AES256-GCM-SHA384 ECDHE-RSA-AES256-SHA384 ECDHE-RSA-AES256-SHA AES128-GCM-SHA256 AES256-GCM-SHA384 AES128-SHA256 AES256-SHA AES128-SHA DES-CBC3-SHA RC4-MD5 When cs_protocol in field 17 is http, the value for this field is a hyphen (-). x_edge_response_result_type How CloudFront classified the response just before returning the response to the viewer. Possible values include: Hit: CloudFront served the object to the viewer from the edge cache. RefreshHit: CloudFront found the object in the edge cache but it had expired, so CloudFront contacted the origin to verify that the cache has the latest version of the object. Miss: The request could not be satisfied by an object in the edge cache, so CloudFront forwarded the request to the origin server and returned the result to the viewer. LimitExceeded: The request was denied because a CloudFront limit was exceeded. CapacityExceeded: CloudFront returned a 503 error because the edge location didn't have enough capacity at the time of the request to serve the object. Error: Typically this means the request resulted in a client error (sc_status is 4xx) or a server error (sc_status is 5xx). If the value of x_edge_result_type is Error and the value of this field is not Error, the client disconnected before finishing the download. Redirect: CloudFront redirects from HTTP to HTTPS. If sc_status is 403 and you configured CloudFront to restrict the geographic distribution of your content, the request might have come from a restricted location. cs_protocol_version The HTTP version that the viewer specified in the request. Possible values include: HTTP/0.9 HTTP/1.0 HTTP/1.1 HTTP/2.0 fle_status When field-level encryption is configured for a distribution, this field contains a code that indicates whether the request body was successfully processed. If field-level encryption is not configured for the distribution, the value is a hyphen (-). fle-encrypted-fields The number of fields that CloudFront encrypted and forwarded to the origin. CloudFront streams the processed request to the origin as it encrypts data, so fle-encrypted-fields can have a value even if the value of fle-status is an error. If field-level encryption is not configured for the distribution, the value of fle-encrypted-fields is a hyphen (-). c_port The port number of the request from the viewer. time_to_first_byte The number of seconds between receiving the request and writing the first byte of the response, as measured on the server. x_edge_detailed_result_type When x-edge-result-type is not Error, this field contains the same value as x-edge-result-type. When x-edge-result-type is Error, this field contains the specific type of error. sc_content_type The value of the HTTP Content-Type header of the response. sc_content_len The value of the HTTP Content-Length header of the response. sc_range_start When the response contains the HTTP Content-Range header, this field contains the range start value. sc-range-end When the response contains the HTTP Content-Range header, this field contains the range end value. Elastic Load Balancer Source: logtype = 'elb' Grok: ^%{TIMESTAMP_ISO8601:time} %{NOTSPACE:elb} %{NOTSPACE:client_ip}:%{NOTSPACE:client_port} ((%{NOTSPACE:backend_ip}:%{NOTSPACE:backend_port})|-) %{NOTSPACE:request_processing_time} %{NOTSPACE:backend_processing_time} %{NOTSPACE:response_processing_time} %{NOTSPACE:elb_status_code} %{NOTSPACE:backend_status_code} %{NOTSPACE:received_bytes} %{NOTSPACE:sent_bytes} \"%{DATA:request}\" \"%{DATA:user_agent}\" %{NOTSPACE:ssl_cipher} %{NOTSPACE:ssl_protocol} Copy Results: Field Definition x_edge_location The edge location that served the request. Each edge location is identified by a three-letter code and an arbitrarily assigned number; for example, DFW3. The three-letter code typically corresponds with the International Air Transport Association airport code for an airport near the edge location. (These abbreviations might change in the future.) sc_bytes The total number of bytes that CloudFront served to the viewer in response to the request, including headers; for example, 1045619. For WebSocket connections, this is the total number of bytes sent from the server to the client through the connection. c_ip The IP address of the viewer that made the request. If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip is the IP address of the proxy or load balancer. cs_method The HTTP request method: DELETE, GET, HEAD, OPTIONS, PATCH, POST, or PUT. cs_host The domain name of the CloudFront distribution; for example, d111111abcdef8.cloudfront.net. cs_uri_stem The portion of the URI that identifies the path and object; for example, /images/cat.jpg. Question marks (?) in URLs and query strings are not included in the log. sc_status An HTTP status code (for example, 200). Status code 000 indicates that the viewer closed the connection (for example, closed the browser tab) before CloudFront could respond to a request. If the viewer closes the connection after CloudFront starts to send the response, the log contains the applicable HTTP status code. cs_referer he name of the domain that originated the request. Common referrers include search engines, other websites that link directly to your objects, and your own website. cs_user_agent The value of the User-Agent header in the request. The User-Agent header identifies the source of the request, such as the type of device and browser that submitted the request and which search engine if applicable. cs_uri_query The query string portion of the URI, if any. When a URI doesn't contain a query string, this field's value is a hyphen (-). cs_cookie The cookie header in the request, including name-value pairs and the associated attributes. If you enable cookie logging, CloudFront logs the cookies in all requests, regardless of which cookies you choose to forward to the origin. If a request doesn't include a cookie header, this field's value is a hyphen (-). x_edge_result_type How CloudFront classifies the response after the last byte left the edge location. In some cases, the result type can change between the time that CloudFront is ready to send the response and the time that CloudFront has finished sending the response. x_edge_request_id An encrypted string that uniquely identifies a request. In the response header, this is x-amz-cf-id. x_host_header The value that the viewer included in the Host header for this request. This is the domain name in the request. If you're using the CloudFront domain name in your object URLs, this field contains that domain name. If you're using alternate domain names in your object URLs, such as [http://example.com/logo.png, this field contains the alternate domain name, such as example.com. To use alternate domain names, you must add them to your distribution. cs_protocol The protocol that the viewer specified in the request: http, https, ws, or wss. cs_bytes The number of bytes of data that the viewer included in the request, including headers. For WebSocket connections, this is the total number of bytes sent from the client to the server on the connection. time_taken The number of seconds (to the thousandth of a second; for example, 0.002) between the time that a CloudFront edge server receives a viewer's request and the time that CloudFront writes the last byte of the response to the edge server's output queue as measured on the server. From the perspective of the viewer, the total time to get the full object will be longer than this value due to network latency and TCP buffering. x_forwarded_for If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip in field 5 is the IP address of the proxy or load balancer. In that case, this field is the IP address of the viewer that originated the request. This field contains IPv4 and IPv6 addresses as applicable. If the viewer did not use an HTTP proxy or a load balancer, the value of x_forwarded_for is a hyphen (-). ssl_protocol When cs_protocol in field 17 is https, this field contains the SSL/TLS protocol that the client and CloudFront negotiated for transmitting the request and response. Possible values include: SSLv3 TLSv1 TLSv1.1 TLSv1.2 When cs_protocol in field 17 is http, the value for this field is a hyphen (-). ssl_cipher When cs_protocol in field 17 is https, this field contains the SSL/TLS cipher that the client and CloudFront negotiated for encrypting the request and response. Possible values include: ECDHE-RSA-AES128-GCM-SHA256 ECDHE-RSA-AES128-SHA256 ECDHE-RSA-AES128-SHA ECDHE-RSA-AES256-GCM-SHA384 ECDHE-RSA-AES256-SHA384 ECDHE-RSA-AES256-SHA AES128-GCM-SHA256 AES256-GCM-SHA384 AES128-SHA256 AES256-SHA AES128-SHA DES-CBC3-SHA RC4-MD5 When cs_protocol is http, the value for this field is a hyphen (-). x_edge_response_result_type How CloudFront classified the response just before returning the response to the viewer. Possible values include: Hit: CloudFront served the object to the viewer from the edge cache. RefreshHit: CloudFront found the object in the edge cache but it had expired, so CloudFront contacted the origin to verify that the cache has the latest version of the object. Miss: The request could not be satisfied by an object in the edge cache, so CloudFront forwarded the request to the origin server and returned the result to the viewer. LimitExceeded: The request was denied because a CloudFront limit was exceeded. CapacityExceeded: CloudFront returned a 503 error because the edge location didn't have enough capacity at the time of the request to serve the object. Error: Typically this means the request resulted in a client error (sc_status is 4xx) or a server error (sc_status is 5xx). If the value of x_edge_result_type is Error and the value of this field is not Error, the client disconnected before finishing the download. Redirect: CloudFront redirects from HTTP to HTTPS. If sc_status is 403 and you configured CloudFront to restrict the geographic distribution of your content, the request might have come from a restricted location. cs_protocol_version The HTTP version that the viewer specified in the request. Possible values include: HTTP/0.9 HTTP/1.0 HTTP/1.1 HTTP/2.0 fle_status When field-level encryption is configured for a distribution, this field contains a code that indicates whether the request body was successfully processed. If field-level encryption is not configured for the distribution, the value of this field is a hyphen (-). When CloudFront successfully processes the request body, encrypts values in the specified fields, and forwards the request to the origin, the value of this field is Processed. The value of x_edge_result_type can still indicate a client-side or server-side error in this case. If the request exceeds a field-level encryption limit, fle-status contains one of the following error codes, and CloudFront returns HTTP status code 400 to the viewer. fle-encrypted-fields The number of fields that CloudFront encrypted and forwarded to the origin. CloudFront streams the processed request to the origin as it encrypts data, so fle_encrypted_fields can have a value even if the value of fle_status is an error. If field-level encryption is not configured for the distribution, the value of fle_encrypted_fields is a hyphen (-). c_port The port number of the request from the viewer. time_to_first_byte The number of seconds between receiving the request and writing the first byte of the response, as measured on the server. x_edge_detailed_result_type When x_edge_result_type is not Error, this field contains the same value as x_edge_result_type. When x_edge_result_type is Error, this field contains the specific type of error. sc_content_type The value of the HTTP Content-Type header of the response. sc_content_len The value of the HTTP Content-Length header of the response. sc_range_start When the response contains the HTTP Content-Range header, this field contains the range start value. sc-range-end When the response contains the HTTP Content-Range header, this field contains the range end value. Microsoft IIS Source: logtype = 'iis_w3c' Grok: %{TIMESTAMP_ISO8601:log_timestamp} %{NOTSPACE:server_ip} %{WORD:method} %{NOTSPACE:uri} %{NOTSPACE:uri_query} %{NOTSPACE:server_port} %{NOTSPACE:username} %{NOTSPACE:client_ip} %{NOTSPACE:user_agent} %{NOTSPACE:referer} %{NOTSPACE:status} %{NOTSPACE:substatus} %{NOTSPACE:win32_status} %{NOTSPACE:time_taken} Copy Monit Source: logtype = 'monit' Grok: \\\\[%{NOTSPACE:tz} %{SYSLOGTIMESTAMP:nr_timestamp}\\\\] %{WORD:state}%{SPACE}: %{GREEDYDATA:message} Copy Results: state: The severity of the log line message: The message MySQL Error Source: logtype = 'mysql-error' Grok: \\\\[%{WORD:log_level}\\\\] Copy Results: log_level: The severity of the log line NGINX Source: logtype = 'nginx' Grok: %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{NUMBER:bytes}|-) %{QS:referrer} %{QS:agent} Copy Results: clientip: The IP address of the client verb: The HTTP verb ident: The user identity of the client making the request response: The HTTP status code of the response request: The URI and request being made httpversion: The HTTP version of the request rawrequest: The raw HTTP request if data is posted bytes: The number of bytes sent referrer: The HTTP referrer agent: The client's user agent NGINX Error Source: logtype = 'nginx-error' Grok: ^(?<timestamp>%{YEAR:year}[./-]%{MONTHNUM:month}[./-]%{MONTHDAY:day}[- ]%{TIME:time}) \\\\[%{LOGLEVEL:severity}\\\\] %{POSINT:pid}#%{NUMBER}: %{GREEDYDATA:errormessage}(?:, client: (?<client>%{IP:clientip}|%{HOSTNAME:hostname}))(?:, server: %{IPORHOSTORUNDERSCORE:server})(?:, request: %{QS:request})?(?:, upstream: \\\"%{URI:upstream}\\\")?(?:, host: %{QS:host})?(?:, referrer: \\\"%{URI:referrer}\\\")?$ Copy Results: severity: The severity of the log line pid: The server process ID errormessage: The error message clientip: The IP address of the calling client server: The server IP address request: The full request upstream: The upstream URI host: The server's hostname referrer: The HTTP referrer Route 53 Source: logtype = 'route-53' Grok: %{NUMBER:log_format_version} %{TIMESTAMP_ISO8601} %{WORD:zone_id} %{IPORHOST:query} %{WORD:query_type} %{WORD:response_code} %{WORD:protocol} %{WORD:edge_location} %{IP:resolver_ip} %{GREEDYDATA:edns_client_subnet} Copy Results: log_format_version: A versioned format for the log. zone_id: The ID of the hosted zone that is associated with all the DNS queries in this log. query: The domain or subdomain that was specified in the request. query_type: Either the DNS record type that was specified in the request, or ANY. response_code: The DNS response code that Route 53 returned in response to the DNS query. protocol: The protocol that was used to submit the query, either TCP or UDP. edge_location: The Route 53 edge location that responded to the query. Each edge location is identified by a three-letter code and an arbitrary number; for example, DFW3. The three-letter code typically corresponds with the International Air Transport Association airport code for an airport near the edge location. (These abbreviations might change in the future.) resolver_ip: The IP address of the DNS resolver that submitted the request to Route 53. edns_client_subnet: A partial IP address for the client that the request originated from, if available from the DNS resolver. Syslog RFC-5424 Source: logtype = 'syslog-rfc5424' Grok: <%{NONNEGINT:pri}>%{NONNEGINT:version} +(?:%{TIMESTAMP_ISO8601:log.timestamp}|-) +(?:%{HOSTNAME:hostname}|-) +(?:\\\\-|%{NOTSPACE:app.name}) +(?:\\\\-|%{NOTSPACE:procid}) (?:\\\\-|%{NOTSPACE:msgid}) +(?:\\[%{DATA:structured.data}\\]|-|) +%{GREEDYDATA:message} Copy Results: pri: The priority represents both the message facility and severity. version: Syslog protocol version. log.timestamp: Original timestamp. hostname: The machine that originally sent the Syslog message. app.name: The device or application that originated the message. procid: The process name or process ID associated with a Syslog system. msgid: Identifies the type of message. structured.data: Structured data string value. sd.<var>sd-id</var>.<var>sd-param-name</var>: The structured.data content is also parsed into separate attributes following a predefined naming convention: sd.<var>sd-id</var>.<var>sd-param-name</var>. See the structured data parsing examples, which follow. message: Free-form message that provides information about the event. Structured data parsing examples: The structured data [example one=\"1\" two=\"2\"] would be parsed into two different attributes: sd.example.one: \"1\" sd.example.two: \"2\" Copy If the same structured data block contains duplicate param names, it also appends an index-based suffix on the attribute name. For example, the structured data [example number=\"1\" number=\"2\"] would be parsed as: sd.example.number.0: \"1\" sd.example.number.1: \"2\" Copy For structured data with enterprise numbers assigned, an extra attribute is also parsed. For example, the structured data [example@123 number=\"1\"] would be parsed as: sd.example.enterprise.number: 123 sd.example.number: \"1\" Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 344.9994,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Built-in <em>log</em> parsing rulesets",
        "sections": "Built-in <em>log</em> parsing rulesets",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " the DNS resolver. Syslog RFC-5424 Source: logtype = &#x27;syslog-rfc5424&#x27; Grok: &lt;%{NONNEGINT:pri}&gt;%{NONNEGINT:version} +(?:%{TIMESTAMP_ISO8601:<em>log</em>.timestamp}|-) +(?:%{HOSTNAME:hostname}|-) +(?:\\\\-|%{NOTSPACE:app.name}) +(?:\\\\-|%{NOTSPACE:procid}) (?:\\\\-|%{NOTSPACE:msgid}) +(?:\\[%{<em>DATA:structured.data</em>"
      },
      "id": "603e7b9164441f1b2d4e8872"
    }
  ],
  "/docs/logs/log-management/ui-data/drop-data-drop-filter-rules": [
    {
      "sections": [
        "Discover value in log data with patterns",
        "Technical overview",
        "Availability",
        "Get started",
        "Explore log patterns",
        "Explore logs with no pattern",
        "Masked attributes and wildcards",
        "Troubleshooting",
        "Put the platform to work with patterns"
      ],
      "title": "Discover value in log data with patterns",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "8f1e27c94327ca4888a945f8e12f9c2310ccd7a6",
      "image": "https://docs.newrelic.com/static/578d7186bb34352855696e5307cc82f2/c1b63/log-patterns-logs-without-a-pattern.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/find-unusual-logs-log-patterns/",
      "published_at": "2021-10-19T03:56:24Z",
      "updated_at": "2021-10-19T03:56:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Log patterns are the fastest way to discover value in log data without searching. Log data is high volume telemetry with a low value per individual record. Searching can quickly lead to logs that provide a root cause explanation, but most data is repetitive and hard to contextualize when browsing. Patterns can make log data discoverable without spending a lot of time reading through low value data. Logs > Patterns: Use patterns as the basis for alerts when the frequency of important data changes, or for configuring drop rules to get rid of unnecessary repetitive data. Technical overview Log patterns functionality applies machine learning to normalize and group log messages that are consistent in format but variable in content. These grouped messages can be sorted, making it easy to find the most frequent or rarest sets of logs in your environment. Use patterns as the basis for alerts when the frequency of important data changes, or to configure drop rules to get rid of unnecessary repetitive data. Log patterns use advanced clustering algorithms to group together similar log messages automatically. With patterns, you can: Orient more quickly through millions of logs. Reduce the time it takes to identify unusual behavior in your log estate. Monitor the frequency of known patterns over time to focus your energy on what matters, and exclude what's irrelevant. Availability The ability to configure this feature is dependent on role-based permissions. For example, accounts still using our original pricing model require an Owner or Admin role. If you see Patterns are turned off in your Log management Patterns UI, click the Configure Patterns button and enable it. If you don't see patterns within 30 minutes of enabling the feature, there may be a lack of data with a message attribute for the system to create a pattern from it. Log patterns Limitations and considerations Pricing There is no separate pricing for log patterns. The only cost is for additional data generated and added to your log records. A pattern attribute will be added to all logs that match a pattern. Attributes also may be added when common values are discovered, such as GUIDs, IP addresses, URL, or email addresses. These attributes are automatically extracted from the log message as part of the pattern process. HITRUST accounts The log patterns feature is not FedRAMP compliant. FedRAMP or other HITRUST accounts are not eligible to use patterns. Parsing limits We have a system of safety limits on memory and CPU resources when processing logs and their patterns. These parsing limits can have an impact on the data you get. For more information, see our documentation about parsing limits. Get started To start examining patterns: Go to one.newrelic.com > Logs, and use the account picker dropdown to select the target account where you want to explore patterns. In the left navigation of the Logs UI, click Patterns. The main log UI changes to show patterns that match the query in the query bar. Logs > Log patterns: The line chart shows the top 5 patterns over time. Use the time picker and query bar to adjust the results. Explore log patterns By default the log patterns UI first shows the most frequent occurrence of patterns. To sort to show the rarest patterns first, click the Count column. You can also use the query bar or attributes bar to filter your log patterns. If you want to... Do this... Understand the rate of change in patterns Look at the line chart. The color-coded patterns correspond to the plot column in the table. You can toggle individual plot patterns to narrow your focus. See the individual log messages that match each pattern Click pattern to expand the row and see a table of individual log records. To see additional records, scroll up or down. To explore an individual log in more detail, click it to open the details panel. Group and filter patterns by their attributes Use the query bar and time picker. As you apply different filters and time windows, the log patterns adjust to your new target data. Create an alert from a pattern Add the pattern to the query bar and run the query. Then click Create alert condition in the left nav. Troubleshoot log messages that haven't been clustered into a pattern Use the Logs with no pattern tab in the Log patterns UI. Clicking a specific log message will open the log message details panel you're familiar with from the Logs management page. Explore logs with no pattern The Logs with no pattern tab groups all recent log messages in your account that were not clustered into a known pattern yet. These log messages don't represent any problem or flaw in the system; they have no pattern because they are too new to have been processed by the machine learning system. This makes them valuable to explore when you want to understand what has recently changed in your environment. Logs > Log patterns: New Relic's log patterns feature automatically groups logs without a matching pattern. For example: Are any of these logs tied to a recent problem? This is a quick way to discover unique log data that is appearing for the first time in your environment. Does your log data have a new format? Sometimes the logs don't represent a problem, but a new format of log data that deviates from the data model you expect your applications to follow. Catching these logs early gives you the opportunity to ask developers to correct any deviations in their log output. The more consistent people are in the way log data is generated, the easier it becomes to use logs across a diverse set of teams. Masked attributes and wildcards Parts of the log messages in patterns are classified as variables and are substituted by masked attributes. The masking process supports and improves the clustering phase by allowing the algorithm to ignore changing details and focus on the repetitive structure. Masked attributes include: date_time ip url uuid Masked attributes are highlighted and are easy to identify, as shown in the following example. Here is an example of a log pattern that has masked attributes. Log patterns extract other less trivial variables that don't belong to any masked attribute. These variables are indicated as wildcards *. Here is an example of how wildcards * group variables in the Log patterns UI. Troubleshooting Here are a few reasons why you might have patterns enabled but not see any pattern data. If you're sure none of the items below are true, get help from support.newrelic.com. No data has arrived in the timeframe you're observing. Try expanding the time range you're viewing with the time picker. It's been less than 24 hours since patterns were enabled in the account. This means the ML model may not be generated for the account yet. None of the data coming in has a message field. Patterns will only be generated for values in the message field of a log record. If your logs don't contain message, there will be no data. Put the platform to work with patterns Patterns are a value that is enriched onto the existing log message as a new attribute named newrelic.logPattern. Anything you can do with logs generally can be done with log patterns, such as: Build your own dashboards with patterns, to monitor a specific pattern or group of patterns you care about. Create alerts for patterns by adding NRQL alerts. Use baseline alert conditions to detect anomalies in known log patterns.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 345.17664,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Discover value in <em>log</em> <em>data</em> with patterns",
        "sections": "Discover value in <em>log</em> <em>data</em> with patterns",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " using our original pricing model require an Owner or Admin role. If you see Patterns are turned off in your <em>Log</em> <em>management</em> Patterns <em>UI</em>, click the Configure Patterns button and enable it. If you don&#x27;t see patterns within 30 minutes of enabling the feature, there may be a lack of <em>data</em> with a message"
      },
      "id": "6072d46128ccbc244451c18b"
    },
    {
      "sections": [
        "Parsing log data",
        "Example",
        "How log parsing works",
        "Important",
        "Limits",
        "Tip",
        "Built-in parsing rulesets",
        "List of built-in rulesets",
        "Add the logtype attribute",
        "New Relic infrastructure agent example",
        "Fluentd example",
        "Fluent Bit example",
        "Logstash example",
        "Logs API example",
        "Create custom parsing rules",
        "Troubleshooting"
      ],
      "title": "Parsing log data",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "52955adb68242c4ca582ba9cb8e22963955a8275",
      "image": "https://docs.newrelic.com/static/dc392bb7142d2fdb253a649daf4ebe6d/c1b63/log-parsing-rule-ui.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/parsing/",
      "published_at": "2021-10-19T03:43:56Z",
      "updated_at": "2021-10-19T03:43:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Parsing is the process of splitting unstructured log data into attributes (key/value pairs). You can use these attributes to facet or filter logs in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial on YouTube (approx. 4-1/2 minutes). New Relic parses log data according to rules. This document describes how logs parsing works, how to use built-in rules, and how to create custom rules. You can also create, query, and manage your log parsing rules by using NerdGraph, our GraphQL API, at api.newrelic.com/graphiql. For more information, see our NerdGraph tutorial for parsing. Example A good example is a default NGINX access log containing unstructured text. It is useful for searching but not much else. Here's an example of a typical line: 127.180.71.3 - - [10/May/1997:08:05:32 +0000] \"GET /downloads/product_1 HTTP/1.1\" 304 0 \"-\" \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" Copy In an unparsed format, you would need to do a full text search to answer most questions. After parsing, the log is organized into attributes, like response code and request URL: { \"remote_addr\":\"93.180.71.3\", \"time\":\"1586514731\", \"method\":\"GET\", \"path\":\"/downloads/product_1\", \"version\":\"HTTP/1.1\", \"response\":\"304\", \"bytesSent\": 0, \"user_agent\": \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" } Copy Parsing makes it easier to create custom queries that facet on those values. This helps you understand the distribution of response codes per request URL and quickly find problematic pages. How log parsing works Here's an overview of how New Relic implements parsing of logs: Log parsing How it works What All parsing takes place against the message field; no other fields can be parsed. Each parsing rule is created with matching criteria that determines which logs the rule will attempt to parse. To simplify the matching process, we recommend adding a logtype attribute to your logs. However, you are not limited to using logtype; any attribute can be used as matching criteria. When Parsing will only be applied once to each log message. If multiple parsing rules match the log, only the first that succeeds will be applied. Parsing takes place during log ingestion, before data is written to NRDB. Once data has been written to storage, it can no longer be parsed. Parsing occurs in the pipeline before data enrichments take place. Be careful when defining the matching criteria for a parsing rule. If the criteria is based on an attribute that doesn't exist untail after parsing or enrichment take place, that data won't be present in the logs when matching occurs. As a result, no parsing will happen. How Rules can be written in Grok, regex, or a mixture of the two. Grok is a collection of patterns that abstract away complicated regular expressions. If the content of the message field is JSON, it will be parsed automatically. New Relic's log ingestion pipeline can parse data by matching a log event to a rule that describes how the log should be parsed. There are two ways log events can be parsed: Use a built-in rule. Define a custom rule. Rules are a combination of matching logic and parsing logic. Matching is done by defining a query match on an attribute of the logs. Rules are not applied retroactively. Logs collected before a rule is created are not parsed by that rule. The simplest way to organize your logs and how they are parsed is to include the logtype field in your log event. This tells New Relic what built-in ruleset to apply to the logs. Important Once a parsing rule is active, data parsed by the rule is permanently changed. This cannot be reverted. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. Limits Parsing is computationally expensive, which introduces risk. Parsing is done for custom rules defined in an account and for matching patterns to a log. A large number of patterns or poorly defined custom rules will consume a huge amount of memory and CPU resources while also taking a very long time to complete. In order to prevent problems, we apply two parsing limits: per-message-per-rule and per-account. Limit Description Per-message-per-rule The per-message-per-rule limit prevents the time spent parsing any single message from being greater than 100 ms. If that limit is reached, the system will cease attempting to parse the log message with that rule. The ingestion pipeline will attempt to run any other applicable on that message, and the message will still be passed through the ingestion pipeline and stored in NRDB. The log message will be in its original, unparsed format. Per-account The per-account limit exists to prevent accounts from using more than their fair share of resources. The limit considers the total time spent processing all log messages for an account per-minute. The limit is not a fixed value; it scales up or down proportionally to the volume of data stored daily by the account and the environment size that is subsequently allocated to support that customer. Tip To easily check if your rate limits have been reached, go to your system Limits page in the New Relic UI. Built-in parsing rulesets Common log formats have well-established parsing rules already created for them. To get the benefit of built-in parsing rules, add the logtype attribute when forwarding logs. Set the value to something listed in the following table, and the rules for that type of log will be applied automatically. List of built-in rulesets The following logtype attribute values map to a predefined parsing rule. For example, to query the Application Load Balancer: From the New Relic UI, use the format logtype: alb. From NerdGraph, use the format logtype = 'alb'. To learn what fields are parsed for each rule, see our documentation about built-in parsing rules. logtype Example matching query alb AWS Application Load Balancer logtype:alb apache Apache Access logtype:apache cloudfront-web CloudFront Web logtype:cloudfront-web elb Amazon Elastic Load Balancer logtype:elb iis_w3c Microsoft IIS server logs - W3C format logtype:iis_w3c monit Monit logs logtype:monit mysql-error MySQL Error logtype:mysql-error nginx NGINX access logs logtype:nginx nginx-error NGINX error logs logtype:nginx-error route-53 Amazon Route 53 logs logtype:route-53 syslog-rfc5424 Syslog logtype:syslog-rfc5424 Add the logtype attribute When aggregating logs, it's important to provide metadata that makes it easy to organize, search, and parse those logs. One simple way of doing this is to add the attribute logtype to the log messages when they are shipped. Built-in parsing rules are applied by default to certain logtype values. Here are some examples of how to add logtype to logs sent by some of our supported shipping methods. New Relic infrastructure agent example Add logtype as an attribute. You must set the logtype for each named source. Learn more about using the infrastructure agent to add attributes. logs: - name: file-simple file: /path/to/file attributes: logtype: fileRaw - name: nginx-example file: /var/log/nginx.log attributes: logtype: nginx Copy Fluentd example Add a filter block to the .conf file, which uses a record_transformer to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluentd examples. <filter containers> @type record_transformer enable_ruby true <record> #Add logtype to trigger a built-in parsing rule for nginx access logs logtype nginx #Set timestamp from the value contained in the field \"time\" timestamp record[\"time\"] #Add hostname and tag fields to all records hostname \"#{Socket.gethostname}\" tag ${tag} </record> </filter> Copy Fluent Bit example Add a filter block to the .conf file that uses a record_modifier to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluent Bit examples. [FILTER] Name record_modifier Match * Record logtype nginx Record hostname ${HOSTNAME} Record service_name Sample-App-Name Copy Logstash example Add a filter block to the Logstash configuration which uses an add_field mutate filter to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Logstash examples. filter { mutate { add_field => { \"logtype\" => \"nginx\" \"service_name\" => \"myservicename\" \"hostname\" => \"%{host}\" } } } Copy Logs API example You can add attributes to the JSON request sent to New Relic. In this example we add a logtype attribute of value nginx to trigger the built-in NGINX parsing rule. Learn more about using the Logs API. POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: YOUR_LICENSE_KEY Accept: */* Content-Length: 133 { \"timestamp\": TIMESTAMP_IN_UNIX_EPOCH, \"message\": \"User 'xyz' logged in\", \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } Copy Create custom parsing rules Many logs are formatted or structured in a unique way. In order to parse them, custom logic must be built and applied. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. To create and manage your own, custom parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing, then click Create parsing rule. Enter the parsing rule's name. Choose an attribute and value to match on. Write your Grok pattern and test the rule. To learn about Grok and custom parsing rules, read our blog post about how to parse logs with Grok patterns. Enable and save the custom parsing rule. To view the list of custom parsing rules: From Manage Data on the left nav of the Logs UI, click Parsing. To view existing parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing. Troubleshooting If parsing is not working the way you intended, it may be due to: Logic: The parsing rule matching logic does not match the logs you want. Timing: If your parsing matching rule targets a value that doesn't exist yet, it will fail. This can occur if the value is added later in the pipeline as part of the enrichment process. Limits: There is a fixed amount of time available every minute to process logs via parsing, patterns, drop filters, etc. If the maximum amount of time has been spent, parsing will be skipped for additional log event records. To resolve these problems, create or adjust your custom parsing rules.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 344.99918,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Parsing <em>log</em> <em>data</em>",
        "sections": "Parsing <em>log</em> <em>data</em>",
        "tags": "<em>Log</em> <em>management</em>",
        "body": "Parsing is the process of splitting unstructured <em>log</em> <em>data</em> into attributes (key&#x2F;value pairs). You can use these attributes to facet or filter <em>logs</em> in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial"
      },
      "id": "603e7eb4196a67b0c4a83dd1"
    },
    {
      "sections": [
        "Built-in log parsing rulesets",
        "Apache",
        "Application Load Balancer",
        "Cloudfront",
        "Elastic Load Balancer",
        "Microsoft IIS",
        "Monit",
        "MySQL Error",
        "NGINX",
        "NGINX Error",
        "Route 53",
        "Syslog RFC-5424"
      ],
      "title": "Built-in log parsing rulesets",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "cb5909f2453d475a85d408d75cd3b2a321a8518e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/built-log-parsing-rulesets/",
      "published_at": "2021-10-19T03:43:56Z",
      "updated_at": "2021-10-19T03:43:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic can parse common log formats according to built-in rulesets, so that you don't have to create your own parsing rules. Here are the log parsing rulesets, their Grok patterns, and what fields are parsed. To enable built-in log parsing, see our documentation for adding the logtype attribute. To manage your parsing rulesets programmatically, you can use NerdGraph, our graphQL-format API, at api.newrelic.com/graphiql. For more information, see the NerdGraph tutorial to create, query, and delete your parsing rulesets. Apache Source: logtype = 'apache' Grok: %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{NUMBER:bytes}|-) %{QS:referrer} %{QS:agent} Copy Results: clientip: The IP address of the client. verb: The HTTP verb ident: The user identity of the client making the request response: The HTTP status code of the response request: The URI and request being made httpversion: The HTTP version of the request rawrequest: The raw HTTP request if data is posted bytes: The number of bytes sent referrer: The HTTP referrer agent: The client's user agent Application Load Balancer Source: logtype = 'alb' Grok: ^%{NOTSPACE:type} %{TIMESTAMP_ISO8601:time} %{NOTSPACE:elb} %{NOTSPACE:client_ip}:%{NOTSPACE:client_port} ((%{NOTSPACE:target_ip}:%{NOTSPACE:target_port})|-) %{NOTSPACE:request_processing_time} %{NOTSPACE:target_processing_time} %{NOTSPACE:response_processing_time} %{NOTSPACE:elb_status_code} %{NOTSPACE:target_status_code} %{NOTSPACE:received_bytes} %{NOTSPACE:sent_bytes} \"%{DATA:request}\" \"%{DATA:user_agent}\" %{NOTSPACE:ssl_cipher} %{NOTSPACE:ssl_protocol} %{NOTSPACE:target_group_arn} \"%{DATA:trace_id}\" \"%{NOTSPACE:domain_name}\" \"%{NOTSPACE:chosen_cert_arn}\" %{NOTSPACE:matched_rule_priority} %{TIMESTAMP_ISO8601:request_creation_time} \"%{NOTSPACE:actions_executed}\" \"%{NOTSPACE:redirect_url}\" \"%{NOTSPACE:error_reason}\" (?:\"|)%{DATA:target_port_list}(?:\"|) (?:\"|)%{DATA:target_status_code_list}(?:\"|) \"%{NOTSPACE:classification}\" \"%{NOTSPACE:classification_reason}\" Copy Results: Field Definition type The type of request or connection. Possible values are: http: HTTP https: HTTP over SSL/TLS h2: HTTP/2 over SSL/TLS ws: WebSockets wss: WebSockets over SSL/TLS elb The resource ID of the load balancer. If you are parsing access log entries, note that resources IDs can contain forward slashes (/). client The IP address and port of the requesting client target The IP address and port of the target that processed this request. If the client didn't send a full request, the load balancer can't dispatch the request to a target, and this value is set to -. If the target is a Lambda function, this value is set to -. If the request is blocked by AWS WAF, this value is set to -, and the value of elb_status_code is set to 403. request_processing_time The total time elapsed (in seconds, with millisecond precision) from the time the load balancer received the request until the time it sent it to a target. This value is set to -1 if the load balancer can't dispatch the request to a target. This can happen if the target closes the connection before the idle timeout or if the client sends a malformed request. This value can also be set to -1 if the registered target does not respond before the idle timeout. target_processing_time The total time elapsed (in seconds, with millisecond precision) from the time the load balancer sent the request to a target until the target started to send the response headers. This value is set to -1 if the load balancer can't dispatch the request to a target. This can happen if the target closes the connection before the idle timeout or if the client sends a malformed request. This value can also be set to -1 if the registered target does not respond before the idle timeout. response_processing_time The total time elapsed (in seconds, with millisecond precision) from the time the load balancer received the response header from the target until it started to send the response to the client. This includes both the queuing time at the load balancer and the connection acquisition time from the load balancer to the client. This value is set to -1 if the load balancer can't send the request to a target. This can happen if the target closes the connection before the idle timeout or if the client sends a malformed request. elb_status_code The status code of the response from the load balancer target_status_code The status code of the response from the target. This value is recorded only if a connection was established to the target and the target sent a response. Otherwise, it is set to -. received_bytes The size of the request, in bytes, received from the client (requester). For HTTP requests, this includes the headers. For WebSockets, this is the total number of bytes received from the client on the connection. sent_bytes The size of the response, in bytes, sent to the client (requester). For HTTP requests, this includes the headers. For WebSockets, this is the total number of bytes sent to the client on the connection. method The HTTP verb of the request uri The URI the request was targeting http_version The HTTP version number of the request user_agent User-Agent string that identifies the client that originated the request, enclosed in double quotes. The string consists of one or more product identifiers, product/version. If the string is longer than 8 KB, it is truncated. ssl_cipher The SSL cipher. This value is set to - if the listener is not an HTTPS listener. ssl_protocol The SSL protocol. This value is set to - if the listener is not an HTTPS listener. target_group_arn The Amazon Resource Name (ARN) of the target group trace_id The contents of the X-Amzn-Trace-Id header, enclosed in double quotes domain_name The SNI domain provided by the client during the TLS handshake, enclosed in double quotes. This value is set to - if the client doesn't support SNI or the domain doesn't match a certificate and the default certificate is presented to the client. chosen_cert_arn The ARN of the certificate presented to the client, enclosed in double quotes. Set to session-reused if the session is reused. Set to - if the listener is not an HTTPS listener. matched_rule_priority The priority value of the rule that matched the request. If a rule matched, this is a value from 1 to 50000. If no rule matched and the default action was taken, this value is set to 0. If an error occurs during rules evaluation, it is set to -1. For any other error, it is set to -. request_creation_time The time when the load balancer received the request from the client, in ISO 8601 format. actions_executed The actions taken when processing the request, enclosed in double quotes. This value is a comma-separated list that can include the values described in actions_taken. If no action was taken, such as for a malformed request, this value is set to -. redirect_url The URL of the redirect target for the location header of the HTTP response, enclosed in double quotes. If no redirect actions were taken, this value is set to -. error_reason The error reason code, enclosed in double quotes. If the request failed, this is one of the error codes described in Error Reason Codes. If the actions taken do not include an authenticate action or the target is not a Lambda function, this value is set to -. Cloudfront Source: logtype = 'cloudfront-web' Grok: ^%{NOTSPACE:date}%{SPACE}%{NOTSPACE:time}%{SPACE}%{NOTSPACE:x_edge_location}%{SPACE}%{NOTSPACE:sc_bytes}%{SPACE}%{NOTSPACE:c_ip}%{SPACE}%{NOTSPACE:cs_method}%{SPACE}%{NOTSPACE:cs_host}%{SPACE}%{NOTSPACE:cs_uri_stem}%{SPACE}%{NOTSPACE:sc_status}%{SPACE}%{NOTSPACE:cs_referer}%{SPACE}%{NOTSPACE:cs_user_agent}%{SPACE}%{NOTSPACE:cs_uri_query}%{SPACE}%{NOTSPACE:cs_Cookie}%{SPACE}%{NOTSPACE:x_edge_result_type}%{SPACE}%{NOTSPACE:x_edge_request_id}%{SPACE}%{NOTSPACE:x_host_header}%{SPACE}%{NOTSPACE:cs_protocol}%{SPACE}%{NOTSPACE:cs_bytes}%{SPACE}%{NOTSPACE:time_taken}%{SPACE}%{NOTSPACE:x_forwarded_for}%{SPACE}%{NOTSPACE:ssl_protocol}%{SPACE}%{NOTSPACE:ssl_cipher}%{SPACE}%{NOTSPACE:x_edge_response_result_type}%{SPACE}%{NOTSPACE:cs_protocol_version}%{SPACE}%{NOTSPACE:fle_status}%{SPACE}%{NOTSPACE:fle_encrypted_fields}%{SPACE}%{NOTSPACE:c_port}%{SPACE}%{NOTSPACE:time_to_first_byte}%{SPACE}%{NOTSPACE:x_edge_detailed_result_type}%{SPACE}%{NOTSPACE:sc_content_type}%{SPACE}%{NOTSPACE:sc_content_len}%{SPACE}%{NOTSPACE:sc_range_start}%{SPACE}%{NOTSPACE:sc_range_end} Copy Results: Field Definition x_edge_location The edge location that served the request. Each edge location is identified by a three-letter code and an arbitrarily assigned number; for example, DFW3. The three-letter code typically corresponds with the International Air Transport Association airport code for an airport near the edge location. (These abbreviations might change in the future.) sc_bytes The total number of bytes that CloudFront served to the viewer in response to the request, including headers; for example, 1045619. For WebSocket connections, this is the total number of bytes sent from the server to the client through the connection. c_ip The IP address of the viewer that made the request, either in IPv4 or IPv6 format. If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip is the IP address of the proxy or load balancer. See also X-Forwarded-For. cs_method The HTTP request method: DELETE, GET, HEAD, OPTIONS, PATCH, POST, or PUT. cs_host The domain name of the CloudFront distribution; for example, d111111abcdef8.cloudfront.net. cs_uri_stem The portion of the URI that identifies the path and object; for example, /images/cat.jpg. Question marks in URLs and query strings are not included. sc_status An HTTP status code; for example, 200. Status code 000 indicates the viewer closed the connection (for example, closed the browser tab) before CloudFront could respond to a request. If the viewer closes the connection after CloudFront starts to send the response, the log contains the applicable HTTP status code. cs_referer The name of the domain that originated the request. Common referrers include search engines, other websites that link directly to your objects, and your own website. cs_user_agent The value of the User-Agent header in the request. The User-Agent header identifies the source of the request, such as the type of device and browser that submitted the request, and which search engine if applicable. cs_uri_query The query string portion of the URI, if any. When a URI doesn't contain a query string, this field's value is a hyphen (-). cs_cookie The cookie header in the request, including name-value pairs and the associated attributes. If you enable cookie logging, CloudFront logs the cookies in all requests regardless of which cookies you choose to forward to the origin. If a request doesn't include a cookie header, this field's value is a hyphen (-). x_edge_result_type How CloudFront classifies the response after the last byte left the edge location. In some cases, the result type can change between the time that CloudFront is ready to send the response and the time that CloudFront has finished sending the response. x_edge_request_id An encrypted string that uniquely identifies a request. In the response header, this is x-amz-cf-id. x_host_header The value that the viewer included in the Host header for this request. This is the domain name in the request. If you're using the CloudFront domain name in your object URLs, this field contains that domain name. If you're using alternate domain names in your object URLs, such as http://example.com/logo.png, this field contains the alternate domain name, such as example.com. To use alternate domain names, you must add them to your distribution. cs_protocol The protocol that the viewer specified in the request: http, https, ws, or wss. cs_bytes The number of bytes of data that the viewer included in the request, including headers. For WebSocket connections, this is the total number of bytes sent from the client to the server on the connection. time_taken The number of seconds (to the thousandth of a second; for example, 0.002) between the time that a CloudFront edge server receives a viewer's request and the time that CloudFront writes the last byte of the response to the edge server's output queue as measured on the server. From the perspective of the viewer, the total time to get the full object will be longer than this value due to network latency and TCP buffering. x_forwarded_for If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip in field 5 is the IP address of the proxy or load balancer. In that case, this field is the IP address of the viewer that originated the request. This field contains IPv4 and IPv6 addresses as applicable. If the viewer did not use an HTTP proxy or a load balancer, the value of x_forwarded_for is a hyphen (-). ssl_protocol When cs_protocol in field 17 is https, this field contains the SSL/TLS protocol that the client and CloudFront negotiated for transmitting the request and response. Possible values include: SSLv3 TLSv1 TLSv1.1 TLSv1.2 When cs_protocol in field 17 is http, the value for this field is a hyphen (-). ssl_cipher When cs_protocol in field 17 is https, this field contains the SSL/TLS cipher that the client and CloudFront negotiated for encrypting the request and response. Possible values include: ECDHE-RSA-AES128-GCM-SHA256 ECDHE-RSA-AES128-SHA256 ECDHE-RSA-AES128-SHA ECDHE-RSA-AES256-GCM-SHA384 ECDHE-RSA-AES256-SHA384 ECDHE-RSA-AES256-SHA AES128-GCM-SHA256 AES256-GCM-SHA384 AES128-SHA256 AES256-SHA AES128-SHA DES-CBC3-SHA RC4-MD5 When cs_protocol in field 17 is http, the value for this field is a hyphen (-). x_edge_response_result_type How CloudFront classified the response just before returning the response to the viewer. Possible values include: Hit: CloudFront served the object to the viewer from the edge cache. RefreshHit: CloudFront found the object in the edge cache but it had expired, so CloudFront contacted the origin to verify that the cache has the latest version of the object. Miss: The request could not be satisfied by an object in the edge cache, so CloudFront forwarded the request to the origin server and returned the result to the viewer. LimitExceeded: The request was denied because a CloudFront limit was exceeded. CapacityExceeded: CloudFront returned a 503 error because the edge location didn't have enough capacity at the time of the request to serve the object. Error: Typically this means the request resulted in a client error (sc_status is 4xx) or a server error (sc_status is 5xx). If the value of x_edge_result_type is Error and the value of this field is not Error, the client disconnected before finishing the download. Redirect: CloudFront redirects from HTTP to HTTPS. If sc_status is 403 and you configured CloudFront to restrict the geographic distribution of your content, the request might have come from a restricted location. cs_protocol_version The HTTP version that the viewer specified in the request. Possible values include: HTTP/0.9 HTTP/1.0 HTTP/1.1 HTTP/2.0 fle_status When field-level encryption is configured for a distribution, this field contains a code that indicates whether the request body was successfully processed. If field-level encryption is not configured for the distribution, the value is a hyphen (-). fle-encrypted-fields The number of fields that CloudFront encrypted and forwarded to the origin. CloudFront streams the processed request to the origin as it encrypts data, so fle-encrypted-fields can have a value even if the value of fle-status is an error. If field-level encryption is not configured for the distribution, the value of fle-encrypted-fields is a hyphen (-). c_port The port number of the request from the viewer. time_to_first_byte The number of seconds between receiving the request and writing the first byte of the response, as measured on the server. x_edge_detailed_result_type When x-edge-result-type is not Error, this field contains the same value as x-edge-result-type. When x-edge-result-type is Error, this field contains the specific type of error. sc_content_type The value of the HTTP Content-Type header of the response. sc_content_len The value of the HTTP Content-Length header of the response. sc_range_start When the response contains the HTTP Content-Range header, this field contains the range start value. sc-range-end When the response contains the HTTP Content-Range header, this field contains the range end value. Elastic Load Balancer Source: logtype = 'elb' Grok: ^%{TIMESTAMP_ISO8601:time} %{NOTSPACE:elb} %{NOTSPACE:client_ip}:%{NOTSPACE:client_port} ((%{NOTSPACE:backend_ip}:%{NOTSPACE:backend_port})|-) %{NOTSPACE:request_processing_time} %{NOTSPACE:backend_processing_time} %{NOTSPACE:response_processing_time} %{NOTSPACE:elb_status_code} %{NOTSPACE:backend_status_code} %{NOTSPACE:received_bytes} %{NOTSPACE:sent_bytes} \"%{DATA:request}\" \"%{DATA:user_agent}\" %{NOTSPACE:ssl_cipher} %{NOTSPACE:ssl_protocol} Copy Results: Field Definition x_edge_location The edge location that served the request. Each edge location is identified by a three-letter code and an arbitrarily assigned number; for example, DFW3. The three-letter code typically corresponds with the International Air Transport Association airport code for an airport near the edge location. (These abbreviations might change in the future.) sc_bytes The total number of bytes that CloudFront served to the viewer in response to the request, including headers; for example, 1045619. For WebSocket connections, this is the total number of bytes sent from the server to the client through the connection. c_ip The IP address of the viewer that made the request. If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip is the IP address of the proxy or load balancer. cs_method The HTTP request method: DELETE, GET, HEAD, OPTIONS, PATCH, POST, or PUT. cs_host The domain name of the CloudFront distribution; for example, d111111abcdef8.cloudfront.net. cs_uri_stem The portion of the URI that identifies the path and object; for example, /images/cat.jpg. Question marks (?) in URLs and query strings are not included in the log. sc_status An HTTP status code (for example, 200). Status code 000 indicates that the viewer closed the connection (for example, closed the browser tab) before CloudFront could respond to a request. If the viewer closes the connection after CloudFront starts to send the response, the log contains the applicable HTTP status code. cs_referer he name of the domain that originated the request. Common referrers include search engines, other websites that link directly to your objects, and your own website. cs_user_agent The value of the User-Agent header in the request. The User-Agent header identifies the source of the request, such as the type of device and browser that submitted the request and which search engine if applicable. cs_uri_query The query string portion of the URI, if any. When a URI doesn't contain a query string, this field's value is a hyphen (-). cs_cookie The cookie header in the request, including name-value pairs and the associated attributes. If you enable cookie logging, CloudFront logs the cookies in all requests, regardless of which cookies you choose to forward to the origin. If a request doesn't include a cookie header, this field's value is a hyphen (-). x_edge_result_type How CloudFront classifies the response after the last byte left the edge location. In some cases, the result type can change between the time that CloudFront is ready to send the response and the time that CloudFront has finished sending the response. x_edge_request_id An encrypted string that uniquely identifies a request. In the response header, this is x-amz-cf-id. x_host_header The value that the viewer included in the Host header for this request. This is the domain name in the request. If you're using the CloudFront domain name in your object URLs, this field contains that domain name. If you're using alternate domain names in your object URLs, such as [http://example.com/logo.png, this field contains the alternate domain name, such as example.com. To use alternate domain names, you must add them to your distribution. cs_protocol The protocol that the viewer specified in the request: http, https, ws, or wss. cs_bytes The number of bytes of data that the viewer included in the request, including headers. For WebSocket connections, this is the total number of bytes sent from the client to the server on the connection. time_taken The number of seconds (to the thousandth of a second; for example, 0.002) between the time that a CloudFront edge server receives a viewer's request and the time that CloudFront writes the last byte of the response to the edge server's output queue as measured on the server. From the perspective of the viewer, the total time to get the full object will be longer than this value due to network latency and TCP buffering. x_forwarded_for If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip in field 5 is the IP address of the proxy or load balancer. In that case, this field is the IP address of the viewer that originated the request. This field contains IPv4 and IPv6 addresses as applicable. If the viewer did not use an HTTP proxy or a load balancer, the value of x_forwarded_for is a hyphen (-). ssl_protocol When cs_protocol in field 17 is https, this field contains the SSL/TLS protocol that the client and CloudFront negotiated for transmitting the request and response. Possible values include: SSLv3 TLSv1 TLSv1.1 TLSv1.2 When cs_protocol in field 17 is http, the value for this field is a hyphen (-). ssl_cipher When cs_protocol in field 17 is https, this field contains the SSL/TLS cipher that the client and CloudFront negotiated for encrypting the request and response. Possible values include: ECDHE-RSA-AES128-GCM-SHA256 ECDHE-RSA-AES128-SHA256 ECDHE-RSA-AES128-SHA ECDHE-RSA-AES256-GCM-SHA384 ECDHE-RSA-AES256-SHA384 ECDHE-RSA-AES256-SHA AES128-GCM-SHA256 AES256-GCM-SHA384 AES128-SHA256 AES256-SHA AES128-SHA DES-CBC3-SHA RC4-MD5 When cs_protocol is http, the value for this field is a hyphen (-). x_edge_response_result_type How CloudFront classified the response just before returning the response to the viewer. Possible values include: Hit: CloudFront served the object to the viewer from the edge cache. RefreshHit: CloudFront found the object in the edge cache but it had expired, so CloudFront contacted the origin to verify that the cache has the latest version of the object. Miss: The request could not be satisfied by an object in the edge cache, so CloudFront forwarded the request to the origin server and returned the result to the viewer. LimitExceeded: The request was denied because a CloudFront limit was exceeded. CapacityExceeded: CloudFront returned a 503 error because the edge location didn't have enough capacity at the time of the request to serve the object. Error: Typically this means the request resulted in a client error (sc_status is 4xx) or a server error (sc_status is 5xx). If the value of x_edge_result_type is Error and the value of this field is not Error, the client disconnected before finishing the download. Redirect: CloudFront redirects from HTTP to HTTPS. If sc_status is 403 and you configured CloudFront to restrict the geographic distribution of your content, the request might have come from a restricted location. cs_protocol_version The HTTP version that the viewer specified in the request. Possible values include: HTTP/0.9 HTTP/1.0 HTTP/1.1 HTTP/2.0 fle_status When field-level encryption is configured for a distribution, this field contains a code that indicates whether the request body was successfully processed. If field-level encryption is not configured for the distribution, the value of this field is a hyphen (-). When CloudFront successfully processes the request body, encrypts values in the specified fields, and forwards the request to the origin, the value of this field is Processed. The value of x_edge_result_type can still indicate a client-side or server-side error in this case. If the request exceeds a field-level encryption limit, fle-status contains one of the following error codes, and CloudFront returns HTTP status code 400 to the viewer. fle-encrypted-fields The number of fields that CloudFront encrypted and forwarded to the origin. CloudFront streams the processed request to the origin as it encrypts data, so fle_encrypted_fields can have a value even if the value of fle_status is an error. If field-level encryption is not configured for the distribution, the value of fle_encrypted_fields is a hyphen (-). c_port The port number of the request from the viewer. time_to_first_byte The number of seconds between receiving the request and writing the first byte of the response, as measured on the server. x_edge_detailed_result_type When x_edge_result_type is not Error, this field contains the same value as x_edge_result_type. When x_edge_result_type is Error, this field contains the specific type of error. sc_content_type The value of the HTTP Content-Type header of the response. sc_content_len The value of the HTTP Content-Length header of the response. sc_range_start When the response contains the HTTP Content-Range header, this field contains the range start value. sc-range-end When the response contains the HTTP Content-Range header, this field contains the range end value. Microsoft IIS Source: logtype = 'iis_w3c' Grok: %{TIMESTAMP_ISO8601:log_timestamp} %{NOTSPACE:server_ip} %{WORD:method} %{NOTSPACE:uri} %{NOTSPACE:uri_query} %{NOTSPACE:server_port} %{NOTSPACE:username} %{NOTSPACE:client_ip} %{NOTSPACE:user_agent} %{NOTSPACE:referer} %{NOTSPACE:status} %{NOTSPACE:substatus} %{NOTSPACE:win32_status} %{NOTSPACE:time_taken} Copy Monit Source: logtype = 'monit' Grok: \\\\[%{NOTSPACE:tz} %{SYSLOGTIMESTAMP:nr_timestamp}\\\\] %{WORD:state}%{SPACE}: %{GREEDYDATA:message} Copy Results: state: The severity of the log line message: The message MySQL Error Source: logtype = 'mysql-error' Grok: \\\\[%{WORD:log_level}\\\\] Copy Results: log_level: The severity of the log line NGINX Source: logtype = 'nginx' Grok: %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{NUMBER:bytes}|-) %{QS:referrer} %{QS:agent} Copy Results: clientip: The IP address of the client verb: The HTTP verb ident: The user identity of the client making the request response: The HTTP status code of the response request: The URI and request being made httpversion: The HTTP version of the request rawrequest: The raw HTTP request if data is posted bytes: The number of bytes sent referrer: The HTTP referrer agent: The client's user agent NGINX Error Source: logtype = 'nginx-error' Grok: ^(?<timestamp>%{YEAR:year}[./-]%{MONTHNUM:month}[./-]%{MONTHDAY:day}[- ]%{TIME:time}) \\\\[%{LOGLEVEL:severity}\\\\] %{POSINT:pid}#%{NUMBER}: %{GREEDYDATA:errormessage}(?:, client: (?<client>%{IP:clientip}|%{HOSTNAME:hostname}))(?:, server: %{IPORHOSTORUNDERSCORE:server})(?:, request: %{QS:request})?(?:, upstream: \\\"%{URI:upstream}\\\")?(?:, host: %{QS:host})?(?:, referrer: \\\"%{URI:referrer}\\\")?$ Copy Results: severity: The severity of the log line pid: The server process ID errormessage: The error message clientip: The IP address of the calling client server: The server IP address request: The full request upstream: The upstream URI host: The server's hostname referrer: The HTTP referrer Route 53 Source: logtype = 'route-53' Grok: %{NUMBER:log_format_version} %{TIMESTAMP_ISO8601} %{WORD:zone_id} %{IPORHOST:query} %{WORD:query_type} %{WORD:response_code} %{WORD:protocol} %{WORD:edge_location} %{IP:resolver_ip} %{GREEDYDATA:edns_client_subnet} Copy Results: log_format_version: A versioned format for the log. zone_id: The ID of the hosted zone that is associated with all the DNS queries in this log. query: The domain or subdomain that was specified in the request. query_type: Either the DNS record type that was specified in the request, or ANY. response_code: The DNS response code that Route 53 returned in response to the DNS query. protocol: The protocol that was used to submit the query, either TCP or UDP. edge_location: The Route 53 edge location that responded to the query. Each edge location is identified by a three-letter code and an arbitrary number; for example, DFW3. The three-letter code typically corresponds with the International Air Transport Association airport code for an airport near the edge location. (These abbreviations might change in the future.) resolver_ip: The IP address of the DNS resolver that submitted the request to Route 53. edns_client_subnet: A partial IP address for the client that the request originated from, if available from the DNS resolver. Syslog RFC-5424 Source: logtype = 'syslog-rfc5424' Grok: <%{NONNEGINT:pri}>%{NONNEGINT:version} +(?:%{TIMESTAMP_ISO8601:log.timestamp}|-) +(?:%{HOSTNAME:hostname}|-) +(?:\\\\-|%{NOTSPACE:app.name}) +(?:\\\\-|%{NOTSPACE:procid}) (?:\\\\-|%{NOTSPACE:msgid}) +(?:\\[%{DATA:structured.data}\\]|-|) +%{GREEDYDATA:message} Copy Results: pri: The priority represents both the message facility and severity. version: Syslog protocol version. log.timestamp: Original timestamp. hostname: The machine that originally sent the Syslog message. app.name: The device or application that originated the message. procid: The process name or process ID associated with a Syslog system. msgid: Identifies the type of message. structured.data: Structured data string value. sd.<var>sd-id</var>.<var>sd-param-name</var>: The structured.data content is also parsed into separate attributes following a predefined naming convention: sd.<var>sd-id</var>.<var>sd-param-name</var>. See the structured data parsing examples, which follow. message: Free-form message that provides information about the event. Structured data parsing examples: The structured data [example one=\"1\" two=\"2\"] would be parsed into two different attributes: sd.example.one: \"1\" sd.example.two: \"2\" Copy If the same structured data block contains duplicate param names, it also appends an index-based suffix on the attribute name. For example, the structured data [example number=\"1\" number=\"2\"] would be parsed as: sd.example.number.0: \"1\" sd.example.number.1: \"2\" Copy For structured data with enterprise numbers assigned, an extra attribute is also parsed. For example, the structured data [example@123 number=\"1\"] would be parsed as: sd.example.enterprise.number: 123 sd.example.number: \"1\" Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 344.99918,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Built-in <em>log</em> parsing rulesets",
        "sections": "Built-in <em>log</em> parsing rulesets",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " the DNS resolver. Syslog RFC-5424 Source: logtype = &#x27;syslog-rfc5424&#x27; Grok: &lt;%{NONNEGINT:pri}&gt;%{NONNEGINT:version} +(?:%{TIMESTAMP_ISO8601:<em>log</em>.timestamp}|-) +(?:%{HOSTNAME:hostname}|-) +(?:\\\\-|%{NOTSPACE:app.name}) +(?:\\\\-|%{NOTSPACE:procid}) (?:\\\\-|%{NOTSPACE:msgid}) +(?:\\[%{<em>DATA:structured.data</em>"
      },
      "id": "603e7b9164441f1b2d4e8872"
    }
  ],
  "/docs/logs/log-management/ui-data/find-unusual-logs-log-patterns": [
    {
      "sections": [
        "Parsing log data",
        "Example",
        "How log parsing works",
        "Important",
        "Limits",
        "Tip",
        "Built-in parsing rulesets",
        "List of built-in rulesets",
        "Add the logtype attribute",
        "New Relic infrastructure agent example",
        "Fluentd example",
        "Fluent Bit example",
        "Logstash example",
        "Logs API example",
        "Create custom parsing rules",
        "Troubleshooting"
      ],
      "title": "Parsing log data",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "52955adb68242c4ca582ba9cb8e22963955a8275",
      "image": "https://docs.newrelic.com/static/dc392bb7142d2fdb253a649daf4ebe6d/c1b63/log-parsing-rule-ui.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/parsing/",
      "published_at": "2021-10-19T03:43:56Z",
      "updated_at": "2021-10-19T03:43:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Parsing is the process of splitting unstructured log data into attributes (key/value pairs). You can use these attributes to facet or filter logs in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial on YouTube (approx. 4-1/2 minutes). New Relic parses log data according to rules. This document describes how logs parsing works, how to use built-in rules, and how to create custom rules. You can also create, query, and manage your log parsing rules by using NerdGraph, our GraphQL API, at api.newrelic.com/graphiql. For more information, see our NerdGraph tutorial for parsing. Example A good example is a default NGINX access log containing unstructured text. It is useful for searching but not much else. Here's an example of a typical line: 127.180.71.3 - - [10/May/1997:08:05:32 +0000] \"GET /downloads/product_1 HTTP/1.1\" 304 0 \"-\" \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" Copy In an unparsed format, you would need to do a full text search to answer most questions. After parsing, the log is organized into attributes, like response code and request URL: { \"remote_addr\":\"93.180.71.3\", \"time\":\"1586514731\", \"method\":\"GET\", \"path\":\"/downloads/product_1\", \"version\":\"HTTP/1.1\", \"response\":\"304\", \"bytesSent\": 0, \"user_agent\": \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" } Copy Parsing makes it easier to create custom queries that facet on those values. This helps you understand the distribution of response codes per request URL and quickly find problematic pages. How log parsing works Here's an overview of how New Relic implements parsing of logs: Log parsing How it works What All parsing takes place against the message field; no other fields can be parsed. Each parsing rule is created with matching criteria that determines which logs the rule will attempt to parse. To simplify the matching process, we recommend adding a logtype attribute to your logs. However, you are not limited to using logtype; any attribute can be used as matching criteria. When Parsing will only be applied once to each log message. If multiple parsing rules match the log, only the first that succeeds will be applied. Parsing takes place during log ingestion, before data is written to NRDB. Once data has been written to storage, it can no longer be parsed. Parsing occurs in the pipeline before data enrichments take place. Be careful when defining the matching criteria for a parsing rule. If the criteria is based on an attribute that doesn't exist untail after parsing or enrichment take place, that data won't be present in the logs when matching occurs. As a result, no parsing will happen. How Rules can be written in Grok, regex, or a mixture of the two. Grok is a collection of patterns that abstract away complicated regular expressions. If the content of the message field is JSON, it will be parsed automatically. New Relic's log ingestion pipeline can parse data by matching a log event to a rule that describes how the log should be parsed. There are two ways log events can be parsed: Use a built-in rule. Define a custom rule. Rules are a combination of matching logic and parsing logic. Matching is done by defining a query match on an attribute of the logs. Rules are not applied retroactively. Logs collected before a rule is created are not parsed by that rule. The simplest way to organize your logs and how they are parsed is to include the logtype field in your log event. This tells New Relic what built-in ruleset to apply to the logs. Important Once a parsing rule is active, data parsed by the rule is permanently changed. This cannot be reverted. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. Limits Parsing is computationally expensive, which introduces risk. Parsing is done for custom rules defined in an account and for matching patterns to a log. A large number of patterns or poorly defined custom rules will consume a huge amount of memory and CPU resources while also taking a very long time to complete. In order to prevent problems, we apply two parsing limits: per-message-per-rule and per-account. Limit Description Per-message-per-rule The per-message-per-rule limit prevents the time spent parsing any single message from being greater than 100 ms. If that limit is reached, the system will cease attempting to parse the log message with that rule. The ingestion pipeline will attempt to run any other applicable on that message, and the message will still be passed through the ingestion pipeline and stored in NRDB. The log message will be in its original, unparsed format. Per-account The per-account limit exists to prevent accounts from using more than their fair share of resources. The limit considers the total time spent processing all log messages for an account per-minute. The limit is not a fixed value; it scales up or down proportionally to the volume of data stored daily by the account and the environment size that is subsequently allocated to support that customer. Tip To easily check if your rate limits have been reached, go to your system Limits page in the New Relic UI. Built-in parsing rulesets Common log formats have well-established parsing rules already created for them. To get the benefit of built-in parsing rules, add the logtype attribute when forwarding logs. Set the value to something listed in the following table, and the rules for that type of log will be applied automatically. List of built-in rulesets The following logtype attribute values map to a predefined parsing rule. For example, to query the Application Load Balancer: From the New Relic UI, use the format logtype: alb. From NerdGraph, use the format logtype = 'alb'. To learn what fields are parsed for each rule, see our documentation about built-in parsing rules. logtype Example matching query alb AWS Application Load Balancer logtype:alb apache Apache Access logtype:apache cloudfront-web CloudFront Web logtype:cloudfront-web elb Amazon Elastic Load Balancer logtype:elb iis_w3c Microsoft IIS server logs - W3C format logtype:iis_w3c monit Monit logs logtype:monit mysql-error MySQL Error logtype:mysql-error nginx NGINX access logs logtype:nginx nginx-error NGINX error logs logtype:nginx-error route-53 Amazon Route 53 logs logtype:route-53 syslog-rfc5424 Syslog logtype:syslog-rfc5424 Add the logtype attribute When aggregating logs, it's important to provide metadata that makes it easy to organize, search, and parse those logs. One simple way of doing this is to add the attribute logtype to the log messages when they are shipped. Built-in parsing rules are applied by default to certain logtype values. Here are some examples of how to add logtype to logs sent by some of our supported shipping methods. New Relic infrastructure agent example Add logtype as an attribute. You must set the logtype for each named source. Learn more about using the infrastructure agent to add attributes. logs: - name: file-simple file: /path/to/file attributes: logtype: fileRaw - name: nginx-example file: /var/log/nginx.log attributes: logtype: nginx Copy Fluentd example Add a filter block to the .conf file, which uses a record_transformer to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluentd examples. <filter containers> @type record_transformer enable_ruby true <record> #Add logtype to trigger a built-in parsing rule for nginx access logs logtype nginx #Set timestamp from the value contained in the field \"time\" timestamp record[\"time\"] #Add hostname and tag fields to all records hostname \"#{Socket.gethostname}\" tag ${tag} </record> </filter> Copy Fluent Bit example Add a filter block to the .conf file that uses a record_modifier to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluent Bit examples. [FILTER] Name record_modifier Match * Record logtype nginx Record hostname ${HOSTNAME} Record service_name Sample-App-Name Copy Logstash example Add a filter block to the Logstash configuration which uses an add_field mutate filter to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Logstash examples. filter { mutate { add_field => { \"logtype\" => \"nginx\" \"service_name\" => \"myservicename\" \"hostname\" => \"%{host}\" } } } Copy Logs API example You can add attributes to the JSON request sent to New Relic. In this example we add a logtype attribute of value nginx to trigger the built-in NGINX parsing rule. Learn more about using the Logs API. POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: YOUR_LICENSE_KEY Accept: */* Content-Length: 133 { \"timestamp\": TIMESTAMP_IN_UNIX_EPOCH, \"message\": \"User 'xyz' logged in\", \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } Copy Create custom parsing rules Many logs are formatted or structured in a unique way. In order to parse them, custom logic must be built and applied. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. To create and manage your own, custom parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing, then click Create parsing rule. Enter the parsing rule's name. Choose an attribute and value to match on. Write your Grok pattern and test the rule. To learn about Grok and custom parsing rules, read our blog post about how to parse logs with Grok patterns. Enable and save the custom parsing rule. To view the list of custom parsing rules: From Manage Data on the left nav of the Logs UI, click Parsing. To view existing parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing. Troubleshooting If parsing is not working the way you intended, it may be due to: Logic: The parsing rule matching logic does not match the logs you want. Timing: If your parsing matching rule targets a value that doesn't exist yet, it will fail. This can occur if the value is added later in the pipeline as part of the enrichment process. Limits: There is a fixed amount of time available every minute to process logs via parsing, patterns, drop filters, etc. If the maximum amount of time has been spent, parsing will be skipped for additional log event records. To resolve these problems, create or adjust your custom parsing rules.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 344.99918,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Parsing <em>log</em> <em>data</em>",
        "sections": "Parsing <em>log</em> <em>data</em>",
        "tags": "<em>Log</em> <em>management</em>",
        "body": "Parsing is the process of splitting unstructured <em>log</em> <em>data</em> into attributes (key&#x2F;value pairs). You can use these attributes to facet or filter <em>logs</em> in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial"
      },
      "id": "603e7eb4196a67b0c4a83dd1"
    },
    {
      "sections": [
        "Built-in log parsing rulesets",
        "Apache",
        "Application Load Balancer",
        "Cloudfront",
        "Elastic Load Balancer",
        "Microsoft IIS",
        "Monit",
        "MySQL Error",
        "NGINX",
        "NGINX Error",
        "Route 53",
        "Syslog RFC-5424"
      ],
      "title": "Built-in log parsing rulesets",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "cb5909f2453d475a85d408d75cd3b2a321a8518e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/built-log-parsing-rulesets/",
      "published_at": "2021-10-19T03:43:56Z",
      "updated_at": "2021-10-19T03:43:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic can parse common log formats according to built-in rulesets, so that you don't have to create your own parsing rules. Here are the log parsing rulesets, their Grok patterns, and what fields are parsed. To enable built-in log parsing, see our documentation for adding the logtype attribute. To manage your parsing rulesets programmatically, you can use NerdGraph, our graphQL-format API, at api.newrelic.com/graphiql. For more information, see the NerdGraph tutorial to create, query, and delete your parsing rulesets. Apache Source: logtype = 'apache' Grok: %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{NUMBER:bytes}|-) %{QS:referrer} %{QS:agent} Copy Results: clientip: The IP address of the client. verb: The HTTP verb ident: The user identity of the client making the request response: The HTTP status code of the response request: The URI and request being made httpversion: The HTTP version of the request rawrequest: The raw HTTP request if data is posted bytes: The number of bytes sent referrer: The HTTP referrer agent: The client's user agent Application Load Balancer Source: logtype = 'alb' Grok: ^%{NOTSPACE:type} %{TIMESTAMP_ISO8601:time} %{NOTSPACE:elb} %{NOTSPACE:client_ip}:%{NOTSPACE:client_port} ((%{NOTSPACE:target_ip}:%{NOTSPACE:target_port})|-) %{NOTSPACE:request_processing_time} %{NOTSPACE:target_processing_time} %{NOTSPACE:response_processing_time} %{NOTSPACE:elb_status_code} %{NOTSPACE:target_status_code} %{NOTSPACE:received_bytes} %{NOTSPACE:sent_bytes} \"%{DATA:request}\" \"%{DATA:user_agent}\" %{NOTSPACE:ssl_cipher} %{NOTSPACE:ssl_protocol} %{NOTSPACE:target_group_arn} \"%{DATA:trace_id}\" \"%{NOTSPACE:domain_name}\" \"%{NOTSPACE:chosen_cert_arn}\" %{NOTSPACE:matched_rule_priority} %{TIMESTAMP_ISO8601:request_creation_time} \"%{NOTSPACE:actions_executed}\" \"%{NOTSPACE:redirect_url}\" \"%{NOTSPACE:error_reason}\" (?:\"|)%{DATA:target_port_list}(?:\"|) (?:\"|)%{DATA:target_status_code_list}(?:\"|) \"%{NOTSPACE:classification}\" \"%{NOTSPACE:classification_reason}\" Copy Results: Field Definition type The type of request or connection. Possible values are: http: HTTP https: HTTP over SSL/TLS h2: HTTP/2 over SSL/TLS ws: WebSockets wss: WebSockets over SSL/TLS elb The resource ID of the load balancer. If you are parsing access log entries, note that resources IDs can contain forward slashes (/). client The IP address and port of the requesting client target The IP address and port of the target that processed this request. If the client didn't send a full request, the load balancer can't dispatch the request to a target, and this value is set to -. If the target is a Lambda function, this value is set to -. If the request is blocked by AWS WAF, this value is set to -, and the value of elb_status_code is set to 403. request_processing_time The total time elapsed (in seconds, with millisecond precision) from the time the load balancer received the request until the time it sent it to a target. This value is set to -1 if the load balancer can't dispatch the request to a target. This can happen if the target closes the connection before the idle timeout or if the client sends a malformed request. This value can also be set to -1 if the registered target does not respond before the idle timeout. target_processing_time The total time elapsed (in seconds, with millisecond precision) from the time the load balancer sent the request to a target until the target started to send the response headers. This value is set to -1 if the load balancer can't dispatch the request to a target. This can happen if the target closes the connection before the idle timeout or if the client sends a malformed request. This value can also be set to -1 if the registered target does not respond before the idle timeout. response_processing_time The total time elapsed (in seconds, with millisecond precision) from the time the load balancer received the response header from the target until it started to send the response to the client. This includes both the queuing time at the load balancer and the connection acquisition time from the load balancer to the client. This value is set to -1 if the load balancer can't send the request to a target. This can happen if the target closes the connection before the idle timeout or if the client sends a malformed request. elb_status_code The status code of the response from the load balancer target_status_code The status code of the response from the target. This value is recorded only if a connection was established to the target and the target sent a response. Otherwise, it is set to -. received_bytes The size of the request, in bytes, received from the client (requester). For HTTP requests, this includes the headers. For WebSockets, this is the total number of bytes received from the client on the connection. sent_bytes The size of the response, in bytes, sent to the client (requester). For HTTP requests, this includes the headers. For WebSockets, this is the total number of bytes sent to the client on the connection. method The HTTP verb of the request uri The URI the request was targeting http_version The HTTP version number of the request user_agent User-Agent string that identifies the client that originated the request, enclosed in double quotes. The string consists of one or more product identifiers, product/version. If the string is longer than 8 KB, it is truncated. ssl_cipher The SSL cipher. This value is set to - if the listener is not an HTTPS listener. ssl_protocol The SSL protocol. This value is set to - if the listener is not an HTTPS listener. target_group_arn The Amazon Resource Name (ARN) of the target group trace_id The contents of the X-Amzn-Trace-Id header, enclosed in double quotes domain_name The SNI domain provided by the client during the TLS handshake, enclosed in double quotes. This value is set to - if the client doesn't support SNI or the domain doesn't match a certificate and the default certificate is presented to the client. chosen_cert_arn The ARN of the certificate presented to the client, enclosed in double quotes. Set to session-reused if the session is reused. Set to - if the listener is not an HTTPS listener. matched_rule_priority The priority value of the rule that matched the request. If a rule matched, this is a value from 1 to 50000. If no rule matched and the default action was taken, this value is set to 0. If an error occurs during rules evaluation, it is set to -1. For any other error, it is set to -. request_creation_time The time when the load balancer received the request from the client, in ISO 8601 format. actions_executed The actions taken when processing the request, enclosed in double quotes. This value is a comma-separated list that can include the values described in actions_taken. If no action was taken, such as for a malformed request, this value is set to -. redirect_url The URL of the redirect target for the location header of the HTTP response, enclosed in double quotes. If no redirect actions were taken, this value is set to -. error_reason The error reason code, enclosed in double quotes. If the request failed, this is one of the error codes described in Error Reason Codes. If the actions taken do not include an authenticate action or the target is not a Lambda function, this value is set to -. Cloudfront Source: logtype = 'cloudfront-web' Grok: ^%{NOTSPACE:date}%{SPACE}%{NOTSPACE:time}%{SPACE}%{NOTSPACE:x_edge_location}%{SPACE}%{NOTSPACE:sc_bytes}%{SPACE}%{NOTSPACE:c_ip}%{SPACE}%{NOTSPACE:cs_method}%{SPACE}%{NOTSPACE:cs_host}%{SPACE}%{NOTSPACE:cs_uri_stem}%{SPACE}%{NOTSPACE:sc_status}%{SPACE}%{NOTSPACE:cs_referer}%{SPACE}%{NOTSPACE:cs_user_agent}%{SPACE}%{NOTSPACE:cs_uri_query}%{SPACE}%{NOTSPACE:cs_Cookie}%{SPACE}%{NOTSPACE:x_edge_result_type}%{SPACE}%{NOTSPACE:x_edge_request_id}%{SPACE}%{NOTSPACE:x_host_header}%{SPACE}%{NOTSPACE:cs_protocol}%{SPACE}%{NOTSPACE:cs_bytes}%{SPACE}%{NOTSPACE:time_taken}%{SPACE}%{NOTSPACE:x_forwarded_for}%{SPACE}%{NOTSPACE:ssl_protocol}%{SPACE}%{NOTSPACE:ssl_cipher}%{SPACE}%{NOTSPACE:x_edge_response_result_type}%{SPACE}%{NOTSPACE:cs_protocol_version}%{SPACE}%{NOTSPACE:fle_status}%{SPACE}%{NOTSPACE:fle_encrypted_fields}%{SPACE}%{NOTSPACE:c_port}%{SPACE}%{NOTSPACE:time_to_first_byte}%{SPACE}%{NOTSPACE:x_edge_detailed_result_type}%{SPACE}%{NOTSPACE:sc_content_type}%{SPACE}%{NOTSPACE:sc_content_len}%{SPACE}%{NOTSPACE:sc_range_start}%{SPACE}%{NOTSPACE:sc_range_end} Copy Results: Field Definition x_edge_location The edge location that served the request. Each edge location is identified by a three-letter code and an arbitrarily assigned number; for example, DFW3. The three-letter code typically corresponds with the International Air Transport Association airport code for an airport near the edge location. (These abbreviations might change in the future.) sc_bytes The total number of bytes that CloudFront served to the viewer in response to the request, including headers; for example, 1045619. For WebSocket connections, this is the total number of bytes sent from the server to the client through the connection. c_ip The IP address of the viewer that made the request, either in IPv4 or IPv6 format. If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip is the IP address of the proxy or load balancer. See also X-Forwarded-For. cs_method The HTTP request method: DELETE, GET, HEAD, OPTIONS, PATCH, POST, or PUT. cs_host The domain name of the CloudFront distribution; for example, d111111abcdef8.cloudfront.net. cs_uri_stem The portion of the URI that identifies the path and object; for example, /images/cat.jpg. Question marks in URLs and query strings are not included. sc_status An HTTP status code; for example, 200. Status code 000 indicates the viewer closed the connection (for example, closed the browser tab) before CloudFront could respond to a request. If the viewer closes the connection after CloudFront starts to send the response, the log contains the applicable HTTP status code. cs_referer The name of the domain that originated the request. Common referrers include search engines, other websites that link directly to your objects, and your own website. cs_user_agent The value of the User-Agent header in the request. The User-Agent header identifies the source of the request, such as the type of device and browser that submitted the request, and which search engine if applicable. cs_uri_query The query string portion of the URI, if any. When a URI doesn't contain a query string, this field's value is a hyphen (-). cs_cookie The cookie header in the request, including name-value pairs and the associated attributes. If you enable cookie logging, CloudFront logs the cookies in all requests regardless of which cookies you choose to forward to the origin. If a request doesn't include a cookie header, this field's value is a hyphen (-). x_edge_result_type How CloudFront classifies the response after the last byte left the edge location. In some cases, the result type can change between the time that CloudFront is ready to send the response and the time that CloudFront has finished sending the response. x_edge_request_id An encrypted string that uniquely identifies a request. In the response header, this is x-amz-cf-id. x_host_header The value that the viewer included in the Host header for this request. This is the domain name in the request. If you're using the CloudFront domain name in your object URLs, this field contains that domain name. If you're using alternate domain names in your object URLs, such as http://example.com/logo.png, this field contains the alternate domain name, such as example.com. To use alternate domain names, you must add them to your distribution. cs_protocol The protocol that the viewer specified in the request: http, https, ws, or wss. cs_bytes The number of bytes of data that the viewer included in the request, including headers. For WebSocket connections, this is the total number of bytes sent from the client to the server on the connection. time_taken The number of seconds (to the thousandth of a second; for example, 0.002) between the time that a CloudFront edge server receives a viewer's request and the time that CloudFront writes the last byte of the response to the edge server's output queue as measured on the server. From the perspective of the viewer, the total time to get the full object will be longer than this value due to network latency and TCP buffering. x_forwarded_for If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip in field 5 is the IP address of the proxy or load balancer. In that case, this field is the IP address of the viewer that originated the request. This field contains IPv4 and IPv6 addresses as applicable. If the viewer did not use an HTTP proxy or a load balancer, the value of x_forwarded_for is a hyphen (-). ssl_protocol When cs_protocol in field 17 is https, this field contains the SSL/TLS protocol that the client and CloudFront negotiated for transmitting the request and response. Possible values include: SSLv3 TLSv1 TLSv1.1 TLSv1.2 When cs_protocol in field 17 is http, the value for this field is a hyphen (-). ssl_cipher When cs_protocol in field 17 is https, this field contains the SSL/TLS cipher that the client and CloudFront negotiated for encrypting the request and response. Possible values include: ECDHE-RSA-AES128-GCM-SHA256 ECDHE-RSA-AES128-SHA256 ECDHE-RSA-AES128-SHA ECDHE-RSA-AES256-GCM-SHA384 ECDHE-RSA-AES256-SHA384 ECDHE-RSA-AES256-SHA AES128-GCM-SHA256 AES256-GCM-SHA384 AES128-SHA256 AES256-SHA AES128-SHA DES-CBC3-SHA RC4-MD5 When cs_protocol in field 17 is http, the value for this field is a hyphen (-). x_edge_response_result_type How CloudFront classified the response just before returning the response to the viewer. Possible values include: Hit: CloudFront served the object to the viewer from the edge cache. RefreshHit: CloudFront found the object in the edge cache but it had expired, so CloudFront contacted the origin to verify that the cache has the latest version of the object. Miss: The request could not be satisfied by an object in the edge cache, so CloudFront forwarded the request to the origin server and returned the result to the viewer. LimitExceeded: The request was denied because a CloudFront limit was exceeded. CapacityExceeded: CloudFront returned a 503 error because the edge location didn't have enough capacity at the time of the request to serve the object. Error: Typically this means the request resulted in a client error (sc_status is 4xx) or a server error (sc_status is 5xx). If the value of x_edge_result_type is Error and the value of this field is not Error, the client disconnected before finishing the download. Redirect: CloudFront redirects from HTTP to HTTPS. If sc_status is 403 and you configured CloudFront to restrict the geographic distribution of your content, the request might have come from a restricted location. cs_protocol_version The HTTP version that the viewer specified in the request. Possible values include: HTTP/0.9 HTTP/1.0 HTTP/1.1 HTTP/2.0 fle_status When field-level encryption is configured for a distribution, this field contains a code that indicates whether the request body was successfully processed. If field-level encryption is not configured for the distribution, the value is a hyphen (-). fle-encrypted-fields The number of fields that CloudFront encrypted and forwarded to the origin. CloudFront streams the processed request to the origin as it encrypts data, so fle-encrypted-fields can have a value even if the value of fle-status is an error. If field-level encryption is not configured for the distribution, the value of fle-encrypted-fields is a hyphen (-). c_port The port number of the request from the viewer. time_to_first_byte The number of seconds between receiving the request and writing the first byte of the response, as measured on the server. x_edge_detailed_result_type When x-edge-result-type is not Error, this field contains the same value as x-edge-result-type. When x-edge-result-type is Error, this field contains the specific type of error. sc_content_type The value of the HTTP Content-Type header of the response. sc_content_len The value of the HTTP Content-Length header of the response. sc_range_start When the response contains the HTTP Content-Range header, this field contains the range start value. sc-range-end When the response contains the HTTP Content-Range header, this field contains the range end value. Elastic Load Balancer Source: logtype = 'elb' Grok: ^%{TIMESTAMP_ISO8601:time} %{NOTSPACE:elb} %{NOTSPACE:client_ip}:%{NOTSPACE:client_port} ((%{NOTSPACE:backend_ip}:%{NOTSPACE:backend_port})|-) %{NOTSPACE:request_processing_time} %{NOTSPACE:backend_processing_time} %{NOTSPACE:response_processing_time} %{NOTSPACE:elb_status_code} %{NOTSPACE:backend_status_code} %{NOTSPACE:received_bytes} %{NOTSPACE:sent_bytes} \"%{DATA:request}\" \"%{DATA:user_agent}\" %{NOTSPACE:ssl_cipher} %{NOTSPACE:ssl_protocol} Copy Results: Field Definition x_edge_location The edge location that served the request. Each edge location is identified by a three-letter code and an arbitrarily assigned number; for example, DFW3. The three-letter code typically corresponds with the International Air Transport Association airport code for an airport near the edge location. (These abbreviations might change in the future.) sc_bytes The total number of bytes that CloudFront served to the viewer in response to the request, including headers; for example, 1045619. For WebSocket connections, this is the total number of bytes sent from the server to the client through the connection. c_ip The IP address of the viewer that made the request. If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip is the IP address of the proxy or load balancer. cs_method The HTTP request method: DELETE, GET, HEAD, OPTIONS, PATCH, POST, or PUT. cs_host The domain name of the CloudFront distribution; for example, d111111abcdef8.cloudfront.net. cs_uri_stem The portion of the URI that identifies the path and object; for example, /images/cat.jpg. Question marks (?) in URLs and query strings are not included in the log. sc_status An HTTP status code (for example, 200). Status code 000 indicates that the viewer closed the connection (for example, closed the browser tab) before CloudFront could respond to a request. If the viewer closes the connection after CloudFront starts to send the response, the log contains the applicable HTTP status code. cs_referer he name of the domain that originated the request. Common referrers include search engines, other websites that link directly to your objects, and your own website. cs_user_agent The value of the User-Agent header in the request. The User-Agent header identifies the source of the request, such as the type of device and browser that submitted the request and which search engine if applicable. cs_uri_query The query string portion of the URI, if any. When a URI doesn't contain a query string, this field's value is a hyphen (-). cs_cookie The cookie header in the request, including name-value pairs and the associated attributes. If you enable cookie logging, CloudFront logs the cookies in all requests, regardless of which cookies you choose to forward to the origin. If a request doesn't include a cookie header, this field's value is a hyphen (-). x_edge_result_type How CloudFront classifies the response after the last byte left the edge location. In some cases, the result type can change between the time that CloudFront is ready to send the response and the time that CloudFront has finished sending the response. x_edge_request_id An encrypted string that uniquely identifies a request. In the response header, this is x-amz-cf-id. x_host_header The value that the viewer included in the Host header for this request. This is the domain name in the request. If you're using the CloudFront domain name in your object URLs, this field contains that domain name. If you're using alternate domain names in your object URLs, such as [http://example.com/logo.png, this field contains the alternate domain name, such as example.com. To use alternate domain names, you must add them to your distribution. cs_protocol The protocol that the viewer specified in the request: http, https, ws, or wss. cs_bytes The number of bytes of data that the viewer included in the request, including headers. For WebSocket connections, this is the total number of bytes sent from the client to the server on the connection. time_taken The number of seconds (to the thousandth of a second; for example, 0.002) between the time that a CloudFront edge server receives a viewer's request and the time that CloudFront writes the last byte of the response to the edge server's output queue as measured on the server. From the perspective of the viewer, the total time to get the full object will be longer than this value due to network latency and TCP buffering. x_forwarded_for If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip in field 5 is the IP address of the proxy or load balancer. In that case, this field is the IP address of the viewer that originated the request. This field contains IPv4 and IPv6 addresses as applicable. If the viewer did not use an HTTP proxy or a load balancer, the value of x_forwarded_for is a hyphen (-). ssl_protocol When cs_protocol in field 17 is https, this field contains the SSL/TLS protocol that the client and CloudFront negotiated for transmitting the request and response. Possible values include: SSLv3 TLSv1 TLSv1.1 TLSv1.2 When cs_protocol in field 17 is http, the value for this field is a hyphen (-). ssl_cipher When cs_protocol in field 17 is https, this field contains the SSL/TLS cipher that the client and CloudFront negotiated for encrypting the request and response. Possible values include: ECDHE-RSA-AES128-GCM-SHA256 ECDHE-RSA-AES128-SHA256 ECDHE-RSA-AES128-SHA ECDHE-RSA-AES256-GCM-SHA384 ECDHE-RSA-AES256-SHA384 ECDHE-RSA-AES256-SHA AES128-GCM-SHA256 AES256-GCM-SHA384 AES128-SHA256 AES256-SHA AES128-SHA DES-CBC3-SHA RC4-MD5 When cs_protocol is http, the value for this field is a hyphen (-). x_edge_response_result_type How CloudFront classified the response just before returning the response to the viewer. Possible values include: Hit: CloudFront served the object to the viewer from the edge cache. RefreshHit: CloudFront found the object in the edge cache but it had expired, so CloudFront contacted the origin to verify that the cache has the latest version of the object. Miss: The request could not be satisfied by an object in the edge cache, so CloudFront forwarded the request to the origin server and returned the result to the viewer. LimitExceeded: The request was denied because a CloudFront limit was exceeded. CapacityExceeded: CloudFront returned a 503 error because the edge location didn't have enough capacity at the time of the request to serve the object. Error: Typically this means the request resulted in a client error (sc_status is 4xx) or a server error (sc_status is 5xx). If the value of x_edge_result_type is Error and the value of this field is not Error, the client disconnected before finishing the download. Redirect: CloudFront redirects from HTTP to HTTPS. If sc_status is 403 and you configured CloudFront to restrict the geographic distribution of your content, the request might have come from a restricted location. cs_protocol_version The HTTP version that the viewer specified in the request. Possible values include: HTTP/0.9 HTTP/1.0 HTTP/1.1 HTTP/2.0 fle_status When field-level encryption is configured for a distribution, this field contains a code that indicates whether the request body was successfully processed. If field-level encryption is not configured for the distribution, the value of this field is a hyphen (-). When CloudFront successfully processes the request body, encrypts values in the specified fields, and forwards the request to the origin, the value of this field is Processed. The value of x_edge_result_type can still indicate a client-side or server-side error in this case. If the request exceeds a field-level encryption limit, fle-status contains one of the following error codes, and CloudFront returns HTTP status code 400 to the viewer. fle-encrypted-fields The number of fields that CloudFront encrypted and forwarded to the origin. CloudFront streams the processed request to the origin as it encrypts data, so fle_encrypted_fields can have a value even if the value of fle_status is an error. If field-level encryption is not configured for the distribution, the value of fle_encrypted_fields is a hyphen (-). c_port The port number of the request from the viewer. time_to_first_byte The number of seconds between receiving the request and writing the first byte of the response, as measured on the server. x_edge_detailed_result_type When x_edge_result_type is not Error, this field contains the same value as x_edge_result_type. When x_edge_result_type is Error, this field contains the specific type of error. sc_content_type The value of the HTTP Content-Type header of the response. sc_content_len The value of the HTTP Content-Length header of the response. sc_range_start When the response contains the HTTP Content-Range header, this field contains the range start value. sc-range-end When the response contains the HTTP Content-Range header, this field contains the range end value. Microsoft IIS Source: logtype = 'iis_w3c' Grok: %{TIMESTAMP_ISO8601:log_timestamp} %{NOTSPACE:server_ip} %{WORD:method} %{NOTSPACE:uri} %{NOTSPACE:uri_query} %{NOTSPACE:server_port} %{NOTSPACE:username} %{NOTSPACE:client_ip} %{NOTSPACE:user_agent} %{NOTSPACE:referer} %{NOTSPACE:status} %{NOTSPACE:substatus} %{NOTSPACE:win32_status} %{NOTSPACE:time_taken} Copy Monit Source: logtype = 'monit' Grok: \\\\[%{NOTSPACE:tz} %{SYSLOGTIMESTAMP:nr_timestamp}\\\\] %{WORD:state}%{SPACE}: %{GREEDYDATA:message} Copy Results: state: The severity of the log line message: The message MySQL Error Source: logtype = 'mysql-error' Grok: \\\\[%{WORD:log_level}\\\\] Copy Results: log_level: The severity of the log line NGINX Source: logtype = 'nginx' Grok: %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{NUMBER:bytes}|-) %{QS:referrer} %{QS:agent} Copy Results: clientip: The IP address of the client verb: The HTTP verb ident: The user identity of the client making the request response: The HTTP status code of the response request: The URI and request being made httpversion: The HTTP version of the request rawrequest: The raw HTTP request if data is posted bytes: The number of bytes sent referrer: The HTTP referrer agent: The client's user agent NGINX Error Source: logtype = 'nginx-error' Grok: ^(?<timestamp>%{YEAR:year}[./-]%{MONTHNUM:month}[./-]%{MONTHDAY:day}[- ]%{TIME:time}) \\\\[%{LOGLEVEL:severity}\\\\] %{POSINT:pid}#%{NUMBER}: %{GREEDYDATA:errormessage}(?:, client: (?<client>%{IP:clientip}|%{HOSTNAME:hostname}))(?:, server: %{IPORHOSTORUNDERSCORE:server})(?:, request: %{QS:request})?(?:, upstream: \\\"%{URI:upstream}\\\")?(?:, host: %{QS:host})?(?:, referrer: \\\"%{URI:referrer}\\\")?$ Copy Results: severity: The severity of the log line pid: The server process ID errormessage: The error message clientip: The IP address of the calling client server: The server IP address request: The full request upstream: The upstream URI host: The server's hostname referrer: The HTTP referrer Route 53 Source: logtype = 'route-53' Grok: %{NUMBER:log_format_version} %{TIMESTAMP_ISO8601} %{WORD:zone_id} %{IPORHOST:query} %{WORD:query_type} %{WORD:response_code} %{WORD:protocol} %{WORD:edge_location} %{IP:resolver_ip} %{GREEDYDATA:edns_client_subnet} Copy Results: log_format_version: A versioned format for the log. zone_id: The ID of the hosted zone that is associated with all the DNS queries in this log. query: The domain or subdomain that was specified in the request. query_type: Either the DNS record type that was specified in the request, or ANY. response_code: The DNS response code that Route 53 returned in response to the DNS query. protocol: The protocol that was used to submit the query, either TCP or UDP. edge_location: The Route 53 edge location that responded to the query. Each edge location is identified by a three-letter code and an arbitrary number; for example, DFW3. The three-letter code typically corresponds with the International Air Transport Association airport code for an airport near the edge location. (These abbreviations might change in the future.) resolver_ip: The IP address of the DNS resolver that submitted the request to Route 53. edns_client_subnet: A partial IP address for the client that the request originated from, if available from the DNS resolver. Syslog RFC-5424 Source: logtype = 'syslog-rfc5424' Grok: <%{NONNEGINT:pri}>%{NONNEGINT:version} +(?:%{TIMESTAMP_ISO8601:log.timestamp}|-) +(?:%{HOSTNAME:hostname}|-) +(?:\\\\-|%{NOTSPACE:app.name}) +(?:\\\\-|%{NOTSPACE:procid}) (?:\\\\-|%{NOTSPACE:msgid}) +(?:\\[%{DATA:structured.data}\\]|-|) +%{GREEDYDATA:message} Copy Results: pri: The priority represents both the message facility and severity. version: Syslog protocol version. log.timestamp: Original timestamp. hostname: The machine that originally sent the Syslog message. app.name: The device or application that originated the message. procid: The process name or process ID associated with a Syslog system. msgid: Identifies the type of message. structured.data: Structured data string value. sd.<var>sd-id</var>.<var>sd-param-name</var>: The structured.data content is also parsed into separate attributes following a predefined naming convention: sd.<var>sd-id</var>.<var>sd-param-name</var>. See the structured data parsing examples, which follow. message: Free-form message that provides information about the event. Structured data parsing examples: The structured data [example one=\"1\" two=\"2\"] would be parsed into two different attributes: sd.example.one: \"1\" sd.example.two: \"2\" Copy If the same structured data block contains duplicate param names, it also appends an index-based suffix on the attribute name. For example, the structured data [example number=\"1\" number=\"2\"] would be parsed as: sd.example.number.0: \"1\" sd.example.number.1: \"2\" Copy For structured data with enterprise numbers assigned, an extra attribute is also parsed. For example, the structured data [example@123 number=\"1\"] would be parsed as: sd.example.enterprise.number: 123 sd.example.number: \"1\" Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 344.99918,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Built-in <em>log</em> parsing rulesets",
        "sections": "Built-in <em>log</em> parsing rulesets",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " the DNS resolver. Syslog RFC-5424 Source: logtype = &#x27;syslog-rfc5424&#x27; Grok: &lt;%{NONNEGINT:pri}&gt;%{NONNEGINT:version} +(?:%{TIMESTAMP_ISO8601:<em>log</em>.timestamp}|-) +(?:%{HOSTNAME:hostname}|-) +(?:\\\\-|%{NOTSPACE:app.name}) +(?:\\\\-|%{NOTSPACE:procid}) (?:\\\\-|%{NOTSPACE:msgid}) +(?:\\[%{<em>DATA:structured.data</em>"
      },
      "id": "603e7b9164441f1b2d4e8872"
    },
    {
      "sections": [
        "Drop data with drop filter rules",
        "Savings, security, speed",
        "Caution",
        "How drop filter rules work",
        "Cautions when dropping data",
        "Create drop filter rules",
        "Manage drop filter rules via NerdGraph API",
        "Types of drop filter rules",
        "Drop log events",
        "Drop attributes",
        "Tip",
        "View or delete drop filter rules"
      ],
      "title": "Drop data with drop filter rules",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "9590bd4593abd451633a4beacd94d56eb1a481bd",
      "image": "https://docs.newrelic.com/static/db4b077fafd911b9f5019b022b3048ab/b04e4/ingest-pipeline.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/drop-data-drop-filter-rules/",
      "published_at": "2021-10-19T03:43:55Z",
      "updated_at": "2021-10-19T03:43:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After log event data has been shipped to New Relic, it can either be stored in our NRDB database or dropped (discarded). We can drop both log events and event attributes via drop filter rules. You can manage drop filter rules using our Logs UI, as explained in this document. You can also use NerdGraph, our GraphQL-format API explorer. Savings, security, speed Drop filter rules help you accomplish several important goals: Lower costs by storing only logs relevant to your account. Protect privacy and security by removing personal identifiable information (PII). Reduce noise by removing irrelevant events and attributes. Caution Use caution when deciding to drop data. The data you drop is not recoverable. Before using this feature, review the responsibilities and considerations for dropping data. How drop filter rules work A drop filter rule matches data based on a query. When triggered, the drop filter rule removes the matching data from the ingestion pipeline before it is written to NRDB. This creates an explicit demarcation between the logs being forwarded from your domain and the data that New Relic collects. Since the data removed by the drop filter rule doesn't reach our backend, it cannot be queried: the data is gone and cannot be restored. During the ingestion process, customer log data can be parsed, transformed, or dropped before being stored in New Relic's database. Cautions when dropping data When creating drop rules, you are responsible for ensuring that the rules accurately identify and discard the data that meets the conditions that you have established. You are also responsible for monitoring the rule, as well as the data you disclose to New Relic. New Relic cannot guarantee that this functionality will completely resolve data disclosure concerns you may have. New Relic doesn't review or monitor how effective the rules you develop are. Creating rules about sensitive data can leak information about what kinds of data you maintain, including the format of your data or systems (for example, through referencing email addresses or specific credit card numbers). Any user with the relevant role-based access control permissions can view and edit all information in the rules you create. Create drop filter rules To create and edit drop filters, you must have admin permissions in New Relic, or you must be a member of a role with create and edit permissions for Insights > NRQL Drop Rules. Once a drop filter rule is active, it's applied to all log events ingested from that point onwards. Rules are not applied retroactively. Logs collected before creating a rule are not filtered by that rule. Filter or query the set of logs that contain the data you want to drop. Then, from Manage Data on the left nav of the Logs UI, click Create drop filter. To create a new drop filter rule, you can use new or existing log queries. Go to one.newrelic.com > Logs. Filter or query to the specific set of logs that contain the data to be dropped. Once the query is active, from Manage Data on the left nav of the Logs UI, click Create drop filter. Recommendation: Change the drop rule's default name to a meaningful name. Choose to either drop the entire log event that matches the query or just a specific subset of attributes in the matching events. Review the log partitions where this drop rule applies. Save the drop filter rule. Manage drop filter rules via NerdGraph API If you want to manage your drop filter rules programmatically, you can use NerdGraph, our graphQL-format API, at api.newrelic.com/graphiql. For more information, see the NerdGraph tutorial to create, query, and delete your drop filter rules. Types of drop filter rules The drop filters UI prompts you to select whether to drop logs based on the query or on specific attributes. Drop log events The default type of drop filter rule is to drop logs. This option drops the entire log events that match the filter or query. When creating a rule, try to provide a specific query that only matches log data that should be dropped. Our drop filters process won't let you create drop filter rules without values in the matching query. This prevents badly formed rules from dropping all log data. Drop attributes You can specify attributes to be dropped in a log event that matches your query. At least one or more attributes must be selected. Any attribute which is selected will be dropped; all remaining attributes will be kept and stored in NRDB. Tip We recommend this method for removing fields that could contain personal identifiable information (PII) or other sensitive attributes without losing valuable monitoring data. View or delete drop filter rules To view or delete a drop filter rule: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Drop filters. Click the delete icon next to the drop filter rule you want to remove. Once deleted, rules no longer filter ingested log events.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 344.99893,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Drop <em>data</em> with drop filter rules",
        "sections": "Drop <em>data</em> with drop filter rules",
        "tags": "<em>Log</em> <em>management</em>",
        "body": "After <em>log</em> event <em>data</em> has been shipped to New Relic, it can either be stored in our NRDB database or dropped (discarded). We can drop both <em>log</em> events and event attributes via drop filter rules. You can manage drop filter rules using our <em>Logs</em> <em>UI</em>, as explained in this document. You can also use"
      },
      "id": "603e813f28ccbc08c1eba787"
    }
  ],
  "/docs/logs/log-management/ui-data/long-logs-blobs": [
    {
      "sections": [
        "Discover value in log data with patterns",
        "Technical overview",
        "Availability",
        "Get started",
        "Explore log patterns",
        "Explore logs with no pattern",
        "Masked attributes and wildcards",
        "Troubleshooting",
        "Put the platform to work with patterns"
      ],
      "title": "Discover value in log data with patterns",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "8f1e27c94327ca4888a945f8e12f9c2310ccd7a6",
      "image": "https://docs.newrelic.com/static/578d7186bb34352855696e5307cc82f2/c1b63/log-patterns-logs-without-a-pattern.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/find-unusual-logs-log-patterns/",
      "published_at": "2021-10-19T03:56:24Z",
      "updated_at": "2021-10-19T03:56:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Log patterns are the fastest way to discover value in log data without searching. Log data is high volume telemetry with a low value per individual record. Searching can quickly lead to logs that provide a root cause explanation, but most data is repetitive and hard to contextualize when browsing. Patterns can make log data discoverable without spending a lot of time reading through low value data. Logs > Patterns: Use patterns as the basis for alerts when the frequency of important data changes, or for configuring drop rules to get rid of unnecessary repetitive data. Technical overview Log patterns functionality applies machine learning to normalize and group log messages that are consistent in format but variable in content. These grouped messages can be sorted, making it easy to find the most frequent or rarest sets of logs in your environment. Use patterns as the basis for alerts when the frequency of important data changes, or to configure drop rules to get rid of unnecessary repetitive data. Log patterns use advanced clustering algorithms to group together similar log messages automatically. With patterns, you can: Orient more quickly through millions of logs. Reduce the time it takes to identify unusual behavior in your log estate. Monitor the frequency of known patterns over time to focus your energy on what matters, and exclude what's irrelevant. Availability The ability to configure this feature is dependent on role-based permissions. For example, accounts still using our original pricing model require an Owner or Admin role. If you see Patterns are turned off in your Log management Patterns UI, click the Configure Patterns button and enable it. If you don't see patterns within 30 minutes of enabling the feature, there may be a lack of data with a message attribute for the system to create a pattern from it. Log patterns Limitations and considerations Pricing There is no separate pricing for log patterns. The only cost is for additional data generated and added to your log records. A pattern attribute will be added to all logs that match a pattern. Attributes also may be added when common values are discovered, such as GUIDs, IP addresses, URL, or email addresses. These attributes are automatically extracted from the log message as part of the pattern process. HITRUST accounts The log patterns feature is not FedRAMP compliant. FedRAMP or other HITRUST accounts are not eligible to use patterns. Parsing limits We have a system of safety limits on memory and CPU resources when processing logs and their patterns. These parsing limits can have an impact on the data you get. For more information, see our documentation about parsing limits. Get started To start examining patterns: Go to one.newrelic.com > Logs, and use the account picker dropdown to select the target account where you want to explore patterns. In the left navigation of the Logs UI, click Patterns. The main log UI changes to show patterns that match the query in the query bar. Logs > Log patterns: The line chart shows the top 5 patterns over time. Use the time picker and query bar to adjust the results. Explore log patterns By default the log patterns UI first shows the most frequent occurrence of patterns. To sort to show the rarest patterns first, click the Count column. You can also use the query bar or attributes bar to filter your log patterns. If you want to... Do this... Understand the rate of change in patterns Look at the line chart. The color-coded patterns correspond to the plot column in the table. You can toggle individual plot patterns to narrow your focus. See the individual log messages that match each pattern Click pattern to expand the row and see a table of individual log records. To see additional records, scroll up or down. To explore an individual log in more detail, click it to open the details panel. Group and filter patterns by their attributes Use the query bar and time picker. As you apply different filters and time windows, the log patterns adjust to your new target data. Create an alert from a pattern Add the pattern to the query bar and run the query. Then click Create alert condition in the left nav. Troubleshoot log messages that haven't been clustered into a pattern Use the Logs with no pattern tab in the Log patterns UI. Clicking a specific log message will open the log message details panel you're familiar with from the Logs management page. Explore logs with no pattern The Logs with no pattern tab groups all recent log messages in your account that were not clustered into a known pattern yet. These log messages don't represent any problem or flaw in the system; they have no pattern because they are too new to have been processed by the machine learning system. This makes them valuable to explore when you want to understand what has recently changed in your environment. Logs > Log patterns: New Relic's log patterns feature automatically groups logs without a matching pattern. For example: Are any of these logs tied to a recent problem? This is a quick way to discover unique log data that is appearing for the first time in your environment. Does your log data have a new format? Sometimes the logs don't represent a problem, but a new format of log data that deviates from the data model you expect your applications to follow. Catching these logs early gives you the opportunity to ask developers to correct any deviations in their log output. The more consistent people are in the way log data is generated, the easier it becomes to use logs across a diverse set of teams. Masked attributes and wildcards Parts of the log messages in patterns are classified as variables and are substituted by masked attributes. The masking process supports and improves the clustering phase by allowing the algorithm to ignore changing details and focus on the repetitive structure. Masked attributes include: date_time ip url uuid Masked attributes are highlighted and are easy to identify, as shown in the following example. Here is an example of a log pattern that has masked attributes. Log patterns extract other less trivial variables that don't belong to any masked attribute. These variables are indicated as wildcards *. Here is an example of how wildcards * group variables in the Log patterns UI. Troubleshooting Here are a few reasons why you might have patterns enabled but not see any pattern data. If you're sure none of the items below are true, get help from support.newrelic.com. No data has arrived in the timeframe you're observing. Try expanding the time range you're viewing with the time picker. It's been less than 24 hours since patterns were enabled in the account. This means the ML model may not be generated for the account yet. None of the data coming in has a message field. Patterns will only be generated for values in the message field of a log record. If your logs don't contain message, there will be no data. Put the platform to work with patterns Patterns are a value that is enriched onto the existing log message as a new attribute named newrelic.logPattern. Anything you can do with logs generally can be done with log patterns, such as: Build your own dashboards with patterns, to monitor a specific pattern or group of patterns you care about. Create alerts for patterns by adding NRQL alerts. Use baseline alert conditions to detect anomalies in known log patterns.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 345.17642,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Discover value in <em>log</em> <em>data</em> with patterns",
        "sections": "Discover value in <em>log</em> <em>data</em> with patterns",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " using our original pricing model require an Owner or Admin role. If you see Patterns are turned off in your <em>Log</em> <em>management</em> Patterns <em>UI</em>, click the Configure Patterns button and enable it. If you don&#x27;t see patterns within 30 minutes of enabling the feature, there may be a lack of <em>data</em> with a message"
      },
      "id": "6072d46128ccbc244451c18b"
    },
    {
      "sections": [
        "Parsing log data",
        "Example",
        "How log parsing works",
        "Important",
        "Limits",
        "Tip",
        "Built-in parsing rulesets",
        "List of built-in rulesets",
        "Add the logtype attribute",
        "New Relic infrastructure agent example",
        "Fluentd example",
        "Fluent Bit example",
        "Logstash example",
        "Logs API example",
        "Create custom parsing rules",
        "Troubleshooting"
      ],
      "title": "Parsing log data",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "52955adb68242c4ca582ba9cb8e22963955a8275",
      "image": "https://docs.newrelic.com/static/dc392bb7142d2fdb253a649daf4ebe6d/c1b63/log-parsing-rule-ui.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/parsing/",
      "published_at": "2021-10-19T03:43:56Z",
      "updated_at": "2021-10-19T03:43:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Parsing is the process of splitting unstructured log data into attributes (key/value pairs). You can use these attributes to facet or filter logs in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial on YouTube (approx. 4-1/2 minutes). New Relic parses log data according to rules. This document describes how logs parsing works, how to use built-in rules, and how to create custom rules. You can also create, query, and manage your log parsing rules by using NerdGraph, our GraphQL API, at api.newrelic.com/graphiql. For more information, see our NerdGraph tutorial for parsing. Example A good example is a default NGINX access log containing unstructured text. It is useful for searching but not much else. Here's an example of a typical line: 127.180.71.3 - - [10/May/1997:08:05:32 +0000] \"GET /downloads/product_1 HTTP/1.1\" 304 0 \"-\" \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" Copy In an unparsed format, you would need to do a full text search to answer most questions. After parsing, the log is organized into attributes, like response code and request URL: { \"remote_addr\":\"93.180.71.3\", \"time\":\"1586514731\", \"method\":\"GET\", \"path\":\"/downloads/product_1\", \"version\":\"HTTP/1.1\", \"response\":\"304\", \"bytesSent\": 0, \"user_agent\": \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" } Copy Parsing makes it easier to create custom queries that facet on those values. This helps you understand the distribution of response codes per request URL and quickly find problematic pages. How log parsing works Here's an overview of how New Relic implements parsing of logs: Log parsing How it works What All parsing takes place against the message field; no other fields can be parsed. Each parsing rule is created with matching criteria that determines which logs the rule will attempt to parse. To simplify the matching process, we recommend adding a logtype attribute to your logs. However, you are not limited to using logtype; any attribute can be used as matching criteria. When Parsing will only be applied once to each log message. If multiple parsing rules match the log, only the first that succeeds will be applied. Parsing takes place during log ingestion, before data is written to NRDB. Once data has been written to storage, it can no longer be parsed. Parsing occurs in the pipeline before data enrichments take place. Be careful when defining the matching criteria for a parsing rule. If the criteria is based on an attribute that doesn't exist untail after parsing or enrichment take place, that data won't be present in the logs when matching occurs. As a result, no parsing will happen. How Rules can be written in Grok, regex, or a mixture of the two. Grok is a collection of patterns that abstract away complicated regular expressions. If the content of the message field is JSON, it will be parsed automatically. New Relic's log ingestion pipeline can parse data by matching a log event to a rule that describes how the log should be parsed. There are two ways log events can be parsed: Use a built-in rule. Define a custom rule. Rules are a combination of matching logic and parsing logic. Matching is done by defining a query match on an attribute of the logs. Rules are not applied retroactively. Logs collected before a rule is created are not parsed by that rule. The simplest way to organize your logs and how they are parsed is to include the logtype field in your log event. This tells New Relic what built-in ruleset to apply to the logs. Important Once a parsing rule is active, data parsed by the rule is permanently changed. This cannot be reverted. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. Limits Parsing is computationally expensive, which introduces risk. Parsing is done for custom rules defined in an account and for matching patterns to a log. A large number of patterns or poorly defined custom rules will consume a huge amount of memory and CPU resources while also taking a very long time to complete. In order to prevent problems, we apply two parsing limits: per-message-per-rule and per-account. Limit Description Per-message-per-rule The per-message-per-rule limit prevents the time spent parsing any single message from being greater than 100 ms. If that limit is reached, the system will cease attempting to parse the log message with that rule. The ingestion pipeline will attempt to run any other applicable on that message, and the message will still be passed through the ingestion pipeline and stored in NRDB. The log message will be in its original, unparsed format. Per-account The per-account limit exists to prevent accounts from using more than their fair share of resources. The limit considers the total time spent processing all log messages for an account per-minute. The limit is not a fixed value; it scales up or down proportionally to the volume of data stored daily by the account and the environment size that is subsequently allocated to support that customer. Tip To easily check if your rate limits have been reached, go to your system Limits page in the New Relic UI. Built-in parsing rulesets Common log formats have well-established parsing rules already created for them. To get the benefit of built-in parsing rules, add the logtype attribute when forwarding logs. Set the value to something listed in the following table, and the rules for that type of log will be applied automatically. List of built-in rulesets The following logtype attribute values map to a predefined parsing rule. For example, to query the Application Load Balancer: From the New Relic UI, use the format logtype: alb. From NerdGraph, use the format logtype = 'alb'. To learn what fields are parsed for each rule, see our documentation about built-in parsing rules. logtype Example matching query alb AWS Application Load Balancer logtype:alb apache Apache Access logtype:apache cloudfront-web CloudFront Web logtype:cloudfront-web elb Amazon Elastic Load Balancer logtype:elb iis_w3c Microsoft IIS server logs - W3C format logtype:iis_w3c monit Monit logs logtype:monit mysql-error MySQL Error logtype:mysql-error nginx NGINX access logs logtype:nginx nginx-error NGINX error logs logtype:nginx-error route-53 Amazon Route 53 logs logtype:route-53 syslog-rfc5424 Syslog logtype:syslog-rfc5424 Add the logtype attribute When aggregating logs, it's important to provide metadata that makes it easy to organize, search, and parse those logs. One simple way of doing this is to add the attribute logtype to the log messages when they are shipped. Built-in parsing rules are applied by default to certain logtype values. Here are some examples of how to add logtype to logs sent by some of our supported shipping methods. New Relic infrastructure agent example Add logtype as an attribute. You must set the logtype for each named source. Learn more about using the infrastructure agent to add attributes. logs: - name: file-simple file: /path/to/file attributes: logtype: fileRaw - name: nginx-example file: /var/log/nginx.log attributes: logtype: nginx Copy Fluentd example Add a filter block to the .conf file, which uses a record_transformer to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluentd examples. <filter containers> @type record_transformer enable_ruby true <record> #Add logtype to trigger a built-in parsing rule for nginx access logs logtype nginx #Set timestamp from the value contained in the field \"time\" timestamp record[\"time\"] #Add hostname and tag fields to all records hostname \"#{Socket.gethostname}\" tag ${tag} </record> </filter> Copy Fluent Bit example Add a filter block to the .conf file that uses a record_modifier to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluent Bit examples. [FILTER] Name record_modifier Match * Record logtype nginx Record hostname ${HOSTNAME} Record service_name Sample-App-Name Copy Logstash example Add a filter block to the Logstash configuration which uses an add_field mutate filter to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Logstash examples. filter { mutate { add_field => { \"logtype\" => \"nginx\" \"service_name\" => \"myservicename\" \"hostname\" => \"%{host}\" } } } Copy Logs API example You can add attributes to the JSON request sent to New Relic. In this example we add a logtype attribute of value nginx to trigger the built-in NGINX parsing rule. Learn more about using the Logs API. POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: YOUR_LICENSE_KEY Accept: */* Content-Length: 133 { \"timestamp\": TIMESTAMP_IN_UNIX_EPOCH, \"message\": \"User 'xyz' logged in\", \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } Copy Create custom parsing rules Many logs are formatted or structured in a unique way. In order to parse them, custom logic must be built and applied. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. To create and manage your own, custom parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing, then click Create parsing rule. Enter the parsing rule's name. Choose an attribute and value to match on. Write your Grok pattern and test the rule. To learn about Grok and custom parsing rules, read our blog post about how to parse logs with Grok patterns. Enable and save the custom parsing rule. To view the list of custom parsing rules: From Manage Data on the left nav of the Logs UI, click Parsing. To view existing parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing. Troubleshooting If parsing is not working the way you intended, it may be due to: Logic: The parsing rule matching logic does not match the logs you want. Timing: If your parsing matching rule targets a value that doesn't exist yet, it will fail. This can occur if the value is added later in the pipeline as part of the enrichment process. Limits: There is a fixed amount of time available every minute to process logs via parsing, patterns, drop filters, etc. If the maximum amount of time has been spent, parsing will be skipped for additional log event records. To resolve these problems, create or adjust your custom parsing rules.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 344.99893,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Parsing <em>log</em> <em>data</em>",
        "sections": "Parsing <em>log</em> <em>data</em>",
        "tags": "<em>Log</em> <em>management</em>",
        "body": "Parsing is the process of splitting unstructured <em>log</em> <em>data</em> into attributes (key&#x2F;value pairs). You can use these attributes to facet or filter <em>logs</em> in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial"
      },
      "id": "603e7eb4196a67b0c4a83dd1"
    },
    {
      "sections": [
        "Built-in log parsing rulesets",
        "Apache",
        "Application Load Balancer",
        "Cloudfront",
        "Elastic Load Balancer",
        "Microsoft IIS",
        "Monit",
        "MySQL Error",
        "NGINX",
        "NGINX Error",
        "Route 53",
        "Syslog RFC-5424"
      ],
      "title": "Built-in log parsing rulesets",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "cb5909f2453d475a85d408d75cd3b2a321a8518e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/built-log-parsing-rulesets/",
      "published_at": "2021-10-19T03:43:56Z",
      "updated_at": "2021-10-19T03:43:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic can parse common log formats according to built-in rulesets, so that you don't have to create your own parsing rules. Here are the log parsing rulesets, their Grok patterns, and what fields are parsed. To enable built-in log parsing, see our documentation for adding the logtype attribute. To manage your parsing rulesets programmatically, you can use NerdGraph, our graphQL-format API, at api.newrelic.com/graphiql. For more information, see the NerdGraph tutorial to create, query, and delete your parsing rulesets. Apache Source: logtype = 'apache' Grok: %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{NUMBER:bytes}|-) %{QS:referrer} %{QS:agent} Copy Results: clientip: The IP address of the client. verb: The HTTP verb ident: The user identity of the client making the request response: The HTTP status code of the response request: The URI and request being made httpversion: The HTTP version of the request rawrequest: The raw HTTP request if data is posted bytes: The number of bytes sent referrer: The HTTP referrer agent: The client's user agent Application Load Balancer Source: logtype = 'alb' Grok: ^%{NOTSPACE:type} %{TIMESTAMP_ISO8601:time} %{NOTSPACE:elb} %{NOTSPACE:client_ip}:%{NOTSPACE:client_port} ((%{NOTSPACE:target_ip}:%{NOTSPACE:target_port})|-) %{NOTSPACE:request_processing_time} %{NOTSPACE:target_processing_time} %{NOTSPACE:response_processing_time} %{NOTSPACE:elb_status_code} %{NOTSPACE:target_status_code} %{NOTSPACE:received_bytes} %{NOTSPACE:sent_bytes} \"%{DATA:request}\" \"%{DATA:user_agent}\" %{NOTSPACE:ssl_cipher} %{NOTSPACE:ssl_protocol} %{NOTSPACE:target_group_arn} \"%{DATA:trace_id}\" \"%{NOTSPACE:domain_name}\" \"%{NOTSPACE:chosen_cert_arn}\" %{NOTSPACE:matched_rule_priority} %{TIMESTAMP_ISO8601:request_creation_time} \"%{NOTSPACE:actions_executed}\" \"%{NOTSPACE:redirect_url}\" \"%{NOTSPACE:error_reason}\" (?:\"|)%{DATA:target_port_list}(?:\"|) (?:\"|)%{DATA:target_status_code_list}(?:\"|) \"%{NOTSPACE:classification}\" \"%{NOTSPACE:classification_reason}\" Copy Results: Field Definition type The type of request or connection. Possible values are: http: HTTP https: HTTP over SSL/TLS h2: HTTP/2 over SSL/TLS ws: WebSockets wss: WebSockets over SSL/TLS elb The resource ID of the load balancer. If you are parsing access log entries, note that resources IDs can contain forward slashes (/). client The IP address and port of the requesting client target The IP address and port of the target that processed this request. If the client didn't send a full request, the load balancer can't dispatch the request to a target, and this value is set to -. If the target is a Lambda function, this value is set to -. If the request is blocked by AWS WAF, this value is set to -, and the value of elb_status_code is set to 403. request_processing_time The total time elapsed (in seconds, with millisecond precision) from the time the load balancer received the request until the time it sent it to a target. This value is set to -1 if the load balancer can't dispatch the request to a target. This can happen if the target closes the connection before the idle timeout or if the client sends a malformed request. This value can also be set to -1 if the registered target does not respond before the idle timeout. target_processing_time The total time elapsed (in seconds, with millisecond precision) from the time the load balancer sent the request to a target until the target started to send the response headers. This value is set to -1 if the load balancer can't dispatch the request to a target. This can happen if the target closes the connection before the idle timeout or if the client sends a malformed request. This value can also be set to -1 if the registered target does not respond before the idle timeout. response_processing_time The total time elapsed (in seconds, with millisecond precision) from the time the load balancer received the response header from the target until it started to send the response to the client. This includes both the queuing time at the load balancer and the connection acquisition time from the load balancer to the client. This value is set to -1 if the load balancer can't send the request to a target. This can happen if the target closes the connection before the idle timeout or if the client sends a malformed request. elb_status_code The status code of the response from the load balancer target_status_code The status code of the response from the target. This value is recorded only if a connection was established to the target and the target sent a response. Otherwise, it is set to -. received_bytes The size of the request, in bytes, received from the client (requester). For HTTP requests, this includes the headers. For WebSockets, this is the total number of bytes received from the client on the connection. sent_bytes The size of the response, in bytes, sent to the client (requester). For HTTP requests, this includes the headers. For WebSockets, this is the total number of bytes sent to the client on the connection. method The HTTP verb of the request uri The URI the request was targeting http_version The HTTP version number of the request user_agent User-Agent string that identifies the client that originated the request, enclosed in double quotes. The string consists of one or more product identifiers, product/version. If the string is longer than 8 KB, it is truncated. ssl_cipher The SSL cipher. This value is set to - if the listener is not an HTTPS listener. ssl_protocol The SSL protocol. This value is set to - if the listener is not an HTTPS listener. target_group_arn The Amazon Resource Name (ARN) of the target group trace_id The contents of the X-Amzn-Trace-Id header, enclosed in double quotes domain_name The SNI domain provided by the client during the TLS handshake, enclosed in double quotes. This value is set to - if the client doesn't support SNI or the domain doesn't match a certificate and the default certificate is presented to the client. chosen_cert_arn The ARN of the certificate presented to the client, enclosed in double quotes. Set to session-reused if the session is reused. Set to - if the listener is not an HTTPS listener. matched_rule_priority The priority value of the rule that matched the request. If a rule matched, this is a value from 1 to 50000. If no rule matched and the default action was taken, this value is set to 0. If an error occurs during rules evaluation, it is set to -1. For any other error, it is set to -. request_creation_time The time when the load balancer received the request from the client, in ISO 8601 format. actions_executed The actions taken when processing the request, enclosed in double quotes. This value is a comma-separated list that can include the values described in actions_taken. If no action was taken, such as for a malformed request, this value is set to -. redirect_url The URL of the redirect target for the location header of the HTTP response, enclosed in double quotes. If no redirect actions were taken, this value is set to -. error_reason The error reason code, enclosed in double quotes. If the request failed, this is one of the error codes described in Error Reason Codes. If the actions taken do not include an authenticate action or the target is not a Lambda function, this value is set to -. Cloudfront Source: logtype = 'cloudfront-web' Grok: ^%{NOTSPACE:date}%{SPACE}%{NOTSPACE:time}%{SPACE}%{NOTSPACE:x_edge_location}%{SPACE}%{NOTSPACE:sc_bytes}%{SPACE}%{NOTSPACE:c_ip}%{SPACE}%{NOTSPACE:cs_method}%{SPACE}%{NOTSPACE:cs_host}%{SPACE}%{NOTSPACE:cs_uri_stem}%{SPACE}%{NOTSPACE:sc_status}%{SPACE}%{NOTSPACE:cs_referer}%{SPACE}%{NOTSPACE:cs_user_agent}%{SPACE}%{NOTSPACE:cs_uri_query}%{SPACE}%{NOTSPACE:cs_Cookie}%{SPACE}%{NOTSPACE:x_edge_result_type}%{SPACE}%{NOTSPACE:x_edge_request_id}%{SPACE}%{NOTSPACE:x_host_header}%{SPACE}%{NOTSPACE:cs_protocol}%{SPACE}%{NOTSPACE:cs_bytes}%{SPACE}%{NOTSPACE:time_taken}%{SPACE}%{NOTSPACE:x_forwarded_for}%{SPACE}%{NOTSPACE:ssl_protocol}%{SPACE}%{NOTSPACE:ssl_cipher}%{SPACE}%{NOTSPACE:x_edge_response_result_type}%{SPACE}%{NOTSPACE:cs_protocol_version}%{SPACE}%{NOTSPACE:fle_status}%{SPACE}%{NOTSPACE:fle_encrypted_fields}%{SPACE}%{NOTSPACE:c_port}%{SPACE}%{NOTSPACE:time_to_first_byte}%{SPACE}%{NOTSPACE:x_edge_detailed_result_type}%{SPACE}%{NOTSPACE:sc_content_type}%{SPACE}%{NOTSPACE:sc_content_len}%{SPACE}%{NOTSPACE:sc_range_start}%{SPACE}%{NOTSPACE:sc_range_end} Copy Results: Field Definition x_edge_location The edge location that served the request. Each edge location is identified by a three-letter code and an arbitrarily assigned number; for example, DFW3. The three-letter code typically corresponds with the International Air Transport Association airport code for an airport near the edge location. (These abbreviations might change in the future.) sc_bytes The total number of bytes that CloudFront served to the viewer in response to the request, including headers; for example, 1045619. For WebSocket connections, this is the total number of bytes sent from the server to the client through the connection. c_ip The IP address of the viewer that made the request, either in IPv4 or IPv6 format. If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip is the IP address of the proxy or load balancer. See also X-Forwarded-For. cs_method The HTTP request method: DELETE, GET, HEAD, OPTIONS, PATCH, POST, or PUT. cs_host The domain name of the CloudFront distribution; for example, d111111abcdef8.cloudfront.net. cs_uri_stem The portion of the URI that identifies the path and object; for example, /images/cat.jpg. Question marks in URLs and query strings are not included. sc_status An HTTP status code; for example, 200. Status code 000 indicates the viewer closed the connection (for example, closed the browser tab) before CloudFront could respond to a request. If the viewer closes the connection after CloudFront starts to send the response, the log contains the applicable HTTP status code. cs_referer The name of the domain that originated the request. Common referrers include search engines, other websites that link directly to your objects, and your own website. cs_user_agent The value of the User-Agent header in the request. The User-Agent header identifies the source of the request, such as the type of device and browser that submitted the request, and which search engine if applicable. cs_uri_query The query string portion of the URI, if any. When a URI doesn't contain a query string, this field's value is a hyphen (-). cs_cookie The cookie header in the request, including name-value pairs and the associated attributes. If you enable cookie logging, CloudFront logs the cookies in all requests regardless of which cookies you choose to forward to the origin. If a request doesn't include a cookie header, this field's value is a hyphen (-). x_edge_result_type How CloudFront classifies the response after the last byte left the edge location. In some cases, the result type can change between the time that CloudFront is ready to send the response and the time that CloudFront has finished sending the response. x_edge_request_id An encrypted string that uniquely identifies a request. In the response header, this is x-amz-cf-id. x_host_header The value that the viewer included in the Host header for this request. This is the domain name in the request. If you're using the CloudFront domain name in your object URLs, this field contains that domain name. If you're using alternate domain names in your object URLs, such as http://example.com/logo.png, this field contains the alternate domain name, such as example.com. To use alternate domain names, you must add them to your distribution. cs_protocol The protocol that the viewer specified in the request: http, https, ws, or wss. cs_bytes The number of bytes of data that the viewer included in the request, including headers. For WebSocket connections, this is the total number of bytes sent from the client to the server on the connection. time_taken The number of seconds (to the thousandth of a second; for example, 0.002) between the time that a CloudFront edge server receives a viewer's request and the time that CloudFront writes the last byte of the response to the edge server's output queue as measured on the server. From the perspective of the viewer, the total time to get the full object will be longer than this value due to network latency and TCP buffering. x_forwarded_for If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip in field 5 is the IP address of the proxy or load balancer. In that case, this field is the IP address of the viewer that originated the request. This field contains IPv4 and IPv6 addresses as applicable. If the viewer did not use an HTTP proxy or a load balancer, the value of x_forwarded_for is a hyphen (-). ssl_protocol When cs_protocol in field 17 is https, this field contains the SSL/TLS protocol that the client and CloudFront negotiated for transmitting the request and response. Possible values include: SSLv3 TLSv1 TLSv1.1 TLSv1.2 When cs_protocol in field 17 is http, the value for this field is a hyphen (-). ssl_cipher When cs_protocol in field 17 is https, this field contains the SSL/TLS cipher that the client and CloudFront negotiated for encrypting the request and response. Possible values include: ECDHE-RSA-AES128-GCM-SHA256 ECDHE-RSA-AES128-SHA256 ECDHE-RSA-AES128-SHA ECDHE-RSA-AES256-GCM-SHA384 ECDHE-RSA-AES256-SHA384 ECDHE-RSA-AES256-SHA AES128-GCM-SHA256 AES256-GCM-SHA384 AES128-SHA256 AES256-SHA AES128-SHA DES-CBC3-SHA RC4-MD5 When cs_protocol in field 17 is http, the value for this field is a hyphen (-). x_edge_response_result_type How CloudFront classified the response just before returning the response to the viewer. Possible values include: Hit: CloudFront served the object to the viewer from the edge cache. RefreshHit: CloudFront found the object in the edge cache but it had expired, so CloudFront contacted the origin to verify that the cache has the latest version of the object. Miss: The request could not be satisfied by an object in the edge cache, so CloudFront forwarded the request to the origin server and returned the result to the viewer. LimitExceeded: The request was denied because a CloudFront limit was exceeded. CapacityExceeded: CloudFront returned a 503 error because the edge location didn't have enough capacity at the time of the request to serve the object. Error: Typically this means the request resulted in a client error (sc_status is 4xx) or a server error (sc_status is 5xx). If the value of x_edge_result_type is Error and the value of this field is not Error, the client disconnected before finishing the download. Redirect: CloudFront redirects from HTTP to HTTPS. If sc_status is 403 and you configured CloudFront to restrict the geographic distribution of your content, the request might have come from a restricted location. cs_protocol_version The HTTP version that the viewer specified in the request. Possible values include: HTTP/0.9 HTTP/1.0 HTTP/1.1 HTTP/2.0 fle_status When field-level encryption is configured for a distribution, this field contains a code that indicates whether the request body was successfully processed. If field-level encryption is not configured for the distribution, the value is a hyphen (-). fle-encrypted-fields The number of fields that CloudFront encrypted and forwarded to the origin. CloudFront streams the processed request to the origin as it encrypts data, so fle-encrypted-fields can have a value even if the value of fle-status is an error. If field-level encryption is not configured for the distribution, the value of fle-encrypted-fields is a hyphen (-). c_port The port number of the request from the viewer. time_to_first_byte The number of seconds between receiving the request and writing the first byte of the response, as measured on the server. x_edge_detailed_result_type When x-edge-result-type is not Error, this field contains the same value as x-edge-result-type. When x-edge-result-type is Error, this field contains the specific type of error. sc_content_type The value of the HTTP Content-Type header of the response. sc_content_len The value of the HTTP Content-Length header of the response. sc_range_start When the response contains the HTTP Content-Range header, this field contains the range start value. sc-range-end When the response contains the HTTP Content-Range header, this field contains the range end value. Elastic Load Balancer Source: logtype = 'elb' Grok: ^%{TIMESTAMP_ISO8601:time} %{NOTSPACE:elb} %{NOTSPACE:client_ip}:%{NOTSPACE:client_port} ((%{NOTSPACE:backend_ip}:%{NOTSPACE:backend_port})|-) %{NOTSPACE:request_processing_time} %{NOTSPACE:backend_processing_time} %{NOTSPACE:response_processing_time} %{NOTSPACE:elb_status_code} %{NOTSPACE:backend_status_code} %{NOTSPACE:received_bytes} %{NOTSPACE:sent_bytes} \"%{DATA:request}\" \"%{DATA:user_agent}\" %{NOTSPACE:ssl_cipher} %{NOTSPACE:ssl_protocol} Copy Results: Field Definition x_edge_location The edge location that served the request. Each edge location is identified by a three-letter code and an arbitrarily assigned number; for example, DFW3. The three-letter code typically corresponds with the International Air Transport Association airport code for an airport near the edge location. (These abbreviations might change in the future.) sc_bytes The total number of bytes that CloudFront served to the viewer in response to the request, including headers; for example, 1045619. For WebSocket connections, this is the total number of bytes sent from the server to the client through the connection. c_ip The IP address of the viewer that made the request. If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip is the IP address of the proxy or load balancer. cs_method The HTTP request method: DELETE, GET, HEAD, OPTIONS, PATCH, POST, or PUT. cs_host The domain name of the CloudFront distribution; for example, d111111abcdef8.cloudfront.net. cs_uri_stem The portion of the URI that identifies the path and object; for example, /images/cat.jpg. Question marks (?) in URLs and query strings are not included in the log. sc_status An HTTP status code (for example, 200). Status code 000 indicates that the viewer closed the connection (for example, closed the browser tab) before CloudFront could respond to a request. If the viewer closes the connection after CloudFront starts to send the response, the log contains the applicable HTTP status code. cs_referer he name of the domain that originated the request. Common referrers include search engines, other websites that link directly to your objects, and your own website. cs_user_agent The value of the User-Agent header in the request. The User-Agent header identifies the source of the request, such as the type of device and browser that submitted the request and which search engine if applicable. cs_uri_query The query string portion of the URI, if any. When a URI doesn't contain a query string, this field's value is a hyphen (-). cs_cookie The cookie header in the request, including name-value pairs and the associated attributes. If you enable cookie logging, CloudFront logs the cookies in all requests, regardless of which cookies you choose to forward to the origin. If a request doesn't include a cookie header, this field's value is a hyphen (-). x_edge_result_type How CloudFront classifies the response after the last byte left the edge location. In some cases, the result type can change between the time that CloudFront is ready to send the response and the time that CloudFront has finished sending the response. x_edge_request_id An encrypted string that uniquely identifies a request. In the response header, this is x-amz-cf-id. x_host_header The value that the viewer included in the Host header for this request. This is the domain name in the request. If you're using the CloudFront domain name in your object URLs, this field contains that domain name. If you're using alternate domain names in your object URLs, such as [http://example.com/logo.png, this field contains the alternate domain name, such as example.com. To use alternate domain names, you must add them to your distribution. cs_protocol The protocol that the viewer specified in the request: http, https, ws, or wss. cs_bytes The number of bytes of data that the viewer included in the request, including headers. For WebSocket connections, this is the total number of bytes sent from the client to the server on the connection. time_taken The number of seconds (to the thousandth of a second; for example, 0.002) between the time that a CloudFront edge server receives a viewer's request and the time that CloudFront writes the last byte of the response to the edge server's output queue as measured on the server. From the perspective of the viewer, the total time to get the full object will be longer than this value due to network latency and TCP buffering. x_forwarded_for If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip in field 5 is the IP address of the proxy or load balancer. In that case, this field is the IP address of the viewer that originated the request. This field contains IPv4 and IPv6 addresses as applicable. If the viewer did not use an HTTP proxy or a load balancer, the value of x_forwarded_for is a hyphen (-). ssl_protocol When cs_protocol in field 17 is https, this field contains the SSL/TLS protocol that the client and CloudFront negotiated for transmitting the request and response. Possible values include: SSLv3 TLSv1 TLSv1.1 TLSv1.2 When cs_protocol in field 17 is http, the value for this field is a hyphen (-). ssl_cipher When cs_protocol in field 17 is https, this field contains the SSL/TLS cipher that the client and CloudFront negotiated for encrypting the request and response. Possible values include: ECDHE-RSA-AES128-GCM-SHA256 ECDHE-RSA-AES128-SHA256 ECDHE-RSA-AES128-SHA ECDHE-RSA-AES256-GCM-SHA384 ECDHE-RSA-AES256-SHA384 ECDHE-RSA-AES256-SHA AES128-GCM-SHA256 AES256-GCM-SHA384 AES128-SHA256 AES256-SHA AES128-SHA DES-CBC3-SHA RC4-MD5 When cs_protocol is http, the value for this field is a hyphen (-). x_edge_response_result_type How CloudFront classified the response just before returning the response to the viewer. Possible values include: Hit: CloudFront served the object to the viewer from the edge cache. RefreshHit: CloudFront found the object in the edge cache but it had expired, so CloudFront contacted the origin to verify that the cache has the latest version of the object. Miss: The request could not be satisfied by an object in the edge cache, so CloudFront forwarded the request to the origin server and returned the result to the viewer. LimitExceeded: The request was denied because a CloudFront limit was exceeded. CapacityExceeded: CloudFront returned a 503 error because the edge location didn't have enough capacity at the time of the request to serve the object. Error: Typically this means the request resulted in a client error (sc_status is 4xx) or a server error (sc_status is 5xx). If the value of x_edge_result_type is Error and the value of this field is not Error, the client disconnected before finishing the download. Redirect: CloudFront redirects from HTTP to HTTPS. If sc_status is 403 and you configured CloudFront to restrict the geographic distribution of your content, the request might have come from a restricted location. cs_protocol_version The HTTP version that the viewer specified in the request. Possible values include: HTTP/0.9 HTTP/1.0 HTTP/1.1 HTTP/2.0 fle_status When field-level encryption is configured for a distribution, this field contains a code that indicates whether the request body was successfully processed. If field-level encryption is not configured for the distribution, the value of this field is a hyphen (-). When CloudFront successfully processes the request body, encrypts values in the specified fields, and forwards the request to the origin, the value of this field is Processed. The value of x_edge_result_type can still indicate a client-side or server-side error in this case. If the request exceeds a field-level encryption limit, fle-status contains one of the following error codes, and CloudFront returns HTTP status code 400 to the viewer. fle-encrypted-fields The number of fields that CloudFront encrypted and forwarded to the origin. CloudFront streams the processed request to the origin as it encrypts data, so fle_encrypted_fields can have a value even if the value of fle_status is an error. If field-level encryption is not configured for the distribution, the value of fle_encrypted_fields is a hyphen (-). c_port The port number of the request from the viewer. time_to_first_byte The number of seconds between receiving the request and writing the first byte of the response, as measured on the server. x_edge_detailed_result_type When x_edge_result_type is not Error, this field contains the same value as x_edge_result_type. When x_edge_result_type is Error, this field contains the specific type of error. sc_content_type The value of the HTTP Content-Type header of the response. sc_content_len The value of the HTTP Content-Length header of the response. sc_range_start When the response contains the HTTP Content-Range header, this field contains the range start value. sc-range-end When the response contains the HTTP Content-Range header, this field contains the range end value. Microsoft IIS Source: logtype = 'iis_w3c' Grok: %{TIMESTAMP_ISO8601:log_timestamp} %{NOTSPACE:server_ip} %{WORD:method} %{NOTSPACE:uri} %{NOTSPACE:uri_query} %{NOTSPACE:server_port} %{NOTSPACE:username} %{NOTSPACE:client_ip} %{NOTSPACE:user_agent} %{NOTSPACE:referer} %{NOTSPACE:status} %{NOTSPACE:substatus} %{NOTSPACE:win32_status} %{NOTSPACE:time_taken} Copy Monit Source: logtype = 'monit' Grok: \\\\[%{NOTSPACE:tz} %{SYSLOGTIMESTAMP:nr_timestamp}\\\\] %{WORD:state}%{SPACE}: %{GREEDYDATA:message} Copy Results: state: The severity of the log line message: The message MySQL Error Source: logtype = 'mysql-error' Grok: \\\\[%{WORD:log_level}\\\\] Copy Results: log_level: The severity of the log line NGINX Source: logtype = 'nginx' Grok: %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{NUMBER:bytes}|-) %{QS:referrer} %{QS:agent} Copy Results: clientip: The IP address of the client verb: The HTTP verb ident: The user identity of the client making the request response: The HTTP status code of the response request: The URI and request being made httpversion: The HTTP version of the request rawrequest: The raw HTTP request if data is posted bytes: The number of bytes sent referrer: The HTTP referrer agent: The client's user agent NGINX Error Source: logtype = 'nginx-error' Grok: ^(?<timestamp>%{YEAR:year}[./-]%{MONTHNUM:month}[./-]%{MONTHDAY:day}[- ]%{TIME:time}) \\\\[%{LOGLEVEL:severity}\\\\] %{POSINT:pid}#%{NUMBER}: %{GREEDYDATA:errormessage}(?:, client: (?<client>%{IP:clientip}|%{HOSTNAME:hostname}))(?:, server: %{IPORHOSTORUNDERSCORE:server})(?:, request: %{QS:request})?(?:, upstream: \\\"%{URI:upstream}\\\")?(?:, host: %{QS:host})?(?:, referrer: \\\"%{URI:referrer}\\\")?$ Copy Results: severity: The severity of the log line pid: The server process ID errormessage: The error message clientip: The IP address of the calling client server: The server IP address request: The full request upstream: The upstream URI host: The server's hostname referrer: The HTTP referrer Route 53 Source: logtype = 'route-53' Grok: %{NUMBER:log_format_version} %{TIMESTAMP_ISO8601} %{WORD:zone_id} %{IPORHOST:query} %{WORD:query_type} %{WORD:response_code} %{WORD:protocol} %{WORD:edge_location} %{IP:resolver_ip} %{GREEDYDATA:edns_client_subnet} Copy Results: log_format_version: A versioned format for the log. zone_id: The ID of the hosted zone that is associated with all the DNS queries in this log. query: The domain or subdomain that was specified in the request. query_type: Either the DNS record type that was specified in the request, or ANY. response_code: The DNS response code that Route 53 returned in response to the DNS query. protocol: The protocol that was used to submit the query, either TCP or UDP. edge_location: The Route 53 edge location that responded to the query. Each edge location is identified by a three-letter code and an arbitrary number; for example, DFW3. The three-letter code typically corresponds with the International Air Transport Association airport code for an airport near the edge location. (These abbreviations might change in the future.) resolver_ip: The IP address of the DNS resolver that submitted the request to Route 53. edns_client_subnet: A partial IP address for the client that the request originated from, if available from the DNS resolver. Syslog RFC-5424 Source: logtype = 'syslog-rfc5424' Grok: <%{NONNEGINT:pri}>%{NONNEGINT:version} +(?:%{TIMESTAMP_ISO8601:log.timestamp}|-) +(?:%{HOSTNAME:hostname}|-) +(?:\\\\-|%{NOTSPACE:app.name}) +(?:\\\\-|%{NOTSPACE:procid}) (?:\\\\-|%{NOTSPACE:msgid}) +(?:\\[%{DATA:structured.data}\\]|-|) +%{GREEDYDATA:message} Copy Results: pri: The priority represents both the message facility and severity. version: Syslog protocol version. log.timestamp: Original timestamp. hostname: The machine that originally sent the Syslog message. app.name: The device or application that originated the message. procid: The process name or process ID associated with a Syslog system. msgid: Identifies the type of message. structured.data: Structured data string value. sd.<var>sd-id</var>.<var>sd-param-name</var>: The structured.data content is also parsed into separate attributes following a predefined naming convention: sd.<var>sd-id</var>.<var>sd-param-name</var>. See the structured data parsing examples, which follow. message: Free-form message that provides information about the event. Structured data parsing examples: The structured data [example one=\"1\" two=\"2\"] would be parsed into two different attributes: sd.example.one: \"1\" sd.example.two: \"2\" Copy If the same structured data block contains duplicate param names, it also appends an index-based suffix on the attribute name. For example, the structured data [example number=\"1\" number=\"2\"] would be parsed as: sd.example.number.0: \"1\" sd.example.number.1: \"2\" Copy For structured data with enterprise numbers assigned, an extra attribute is also parsed. For example, the structured data [example@123 number=\"1\"] would be parsed as: sd.example.enterprise.number: 123 sd.example.number: \"1\" Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 344.99893,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Built-in <em>log</em> parsing rulesets",
        "sections": "Built-in <em>log</em> parsing rulesets",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " the DNS resolver. Syslog RFC-5424 Source: logtype = &#x27;syslog-rfc5424&#x27; Grok: &lt;%{NONNEGINT:pri}&gt;%{NONNEGINT:version} +(?:%{TIMESTAMP_ISO8601:<em>log</em>.timestamp}|-) +(?:%{HOSTNAME:hostname}|-) +(?:\\\\-|%{NOTSPACE:app.name}) +(?:\\\\-|%{NOTSPACE:procid}) (?:\\\\-|%{NOTSPACE:msgid}) +(?:\\[%{<em>DATA:structured.data</em>"
      },
      "id": "603e7b9164441f1b2d4e8872"
    }
  ],
  "/docs/logs/log-management/ui-data/parsing": [
    {
      "sections": [
        "Discover value in log data with patterns",
        "Technical overview",
        "Availability",
        "Get started",
        "Explore log patterns",
        "Explore logs with no pattern",
        "Masked attributes and wildcards",
        "Troubleshooting",
        "Put the platform to work with patterns"
      ],
      "title": "Discover value in log data with patterns",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "8f1e27c94327ca4888a945f8e12f9c2310ccd7a6",
      "image": "https://docs.newrelic.com/static/578d7186bb34352855696e5307cc82f2/c1b63/log-patterns-logs-without-a-pattern.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/find-unusual-logs-log-patterns/",
      "published_at": "2021-10-19T03:56:24Z",
      "updated_at": "2021-10-19T03:56:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Log patterns are the fastest way to discover value in log data without searching. Log data is high volume telemetry with a low value per individual record. Searching can quickly lead to logs that provide a root cause explanation, but most data is repetitive and hard to contextualize when browsing. Patterns can make log data discoverable without spending a lot of time reading through low value data. Logs > Patterns: Use patterns as the basis for alerts when the frequency of important data changes, or for configuring drop rules to get rid of unnecessary repetitive data. Technical overview Log patterns functionality applies machine learning to normalize and group log messages that are consistent in format but variable in content. These grouped messages can be sorted, making it easy to find the most frequent or rarest sets of logs in your environment. Use patterns as the basis for alerts when the frequency of important data changes, or to configure drop rules to get rid of unnecessary repetitive data. Log patterns use advanced clustering algorithms to group together similar log messages automatically. With patterns, you can: Orient more quickly through millions of logs. Reduce the time it takes to identify unusual behavior in your log estate. Monitor the frequency of known patterns over time to focus your energy on what matters, and exclude what's irrelevant. Availability The ability to configure this feature is dependent on role-based permissions. For example, accounts still using our original pricing model require an Owner or Admin role. If you see Patterns are turned off in your Log management Patterns UI, click the Configure Patterns button and enable it. If you don't see patterns within 30 minutes of enabling the feature, there may be a lack of data with a message attribute for the system to create a pattern from it. Log patterns Limitations and considerations Pricing There is no separate pricing for log patterns. The only cost is for additional data generated and added to your log records. A pattern attribute will be added to all logs that match a pattern. Attributes also may be added when common values are discovered, such as GUIDs, IP addresses, URL, or email addresses. These attributes are automatically extracted from the log message as part of the pattern process. HITRUST accounts The log patterns feature is not FedRAMP compliant. FedRAMP or other HITRUST accounts are not eligible to use patterns. Parsing limits We have a system of safety limits on memory and CPU resources when processing logs and their patterns. These parsing limits can have an impact on the data you get. For more information, see our documentation about parsing limits. Get started To start examining patterns: Go to one.newrelic.com > Logs, and use the account picker dropdown to select the target account where you want to explore patterns. In the left navigation of the Logs UI, click Patterns. The main log UI changes to show patterns that match the query in the query bar. Logs > Log patterns: The line chart shows the top 5 patterns over time. Use the time picker and query bar to adjust the results. Explore log patterns By default the log patterns UI first shows the most frequent occurrence of patterns. To sort to show the rarest patterns first, click the Count column. You can also use the query bar or attributes bar to filter your log patterns. If you want to... Do this... Understand the rate of change in patterns Look at the line chart. The color-coded patterns correspond to the plot column in the table. You can toggle individual plot patterns to narrow your focus. See the individual log messages that match each pattern Click pattern to expand the row and see a table of individual log records. To see additional records, scroll up or down. To explore an individual log in more detail, click it to open the details panel. Group and filter patterns by their attributes Use the query bar and time picker. As you apply different filters and time windows, the log patterns adjust to your new target data. Create an alert from a pattern Add the pattern to the query bar and run the query. Then click Create alert condition in the left nav. Troubleshoot log messages that haven't been clustered into a pattern Use the Logs with no pattern tab in the Log patterns UI. Clicking a specific log message will open the log message details panel you're familiar with from the Logs management page. Explore logs with no pattern The Logs with no pattern tab groups all recent log messages in your account that were not clustered into a known pattern yet. These log messages don't represent any problem or flaw in the system; they have no pattern because they are too new to have been processed by the machine learning system. This makes them valuable to explore when you want to understand what has recently changed in your environment. Logs > Log patterns: New Relic's log patterns feature automatically groups logs without a matching pattern. For example: Are any of these logs tied to a recent problem? This is a quick way to discover unique log data that is appearing for the first time in your environment. Does your log data have a new format? Sometimes the logs don't represent a problem, but a new format of log data that deviates from the data model you expect your applications to follow. Catching these logs early gives you the opportunity to ask developers to correct any deviations in their log output. The more consistent people are in the way log data is generated, the easier it becomes to use logs across a diverse set of teams. Masked attributes and wildcards Parts of the log messages in patterns are classified as variables and are substituted by masked attributes. The masking process supports and improves the clustering phase by allowing the algorithm to ignore changing details and focus on the repetitive structure. Masked attributes include: date_time ip url uuid Masked attributes are highlighted and are easy to identify, as shown in the following example. Here is an example of a log pattern that has masked attributes. Log patterns extract other less trivial variables that don't belong to any masked attribute. These variables are indicated as wildcards *. Here is an example of how wildcards * group variables in the Log patterns UI. Troubleshooting Here are a few reasons why you might have patterns enabled but not see any pattern data. If you're sure none of the items below are true, get help from support.newrelic.com. No data has arrived in the timeframe you're observing. Try expanding the time range you're viewing with the time picker. It's been less than 24 hours since patterns were enabled in the account. This means the ML model may not be generated for the account yet. None of the data coming in has a message field. Patterns will only be generated for values in the message field of a log record. If your logs don't contain message, there will be no data. Put the platform to work with patterns Patterns are a value that is enriched onto the existing log message as a new attribute named newrelic.logPattern. Anything you can do with logs generally can be done with log patterns, such as: Build your own dashboards with patterns, to monitor a specific pattern or group of patterns you care about. Create alerts for patterns by adding NRQL alerts. Use baseline alert conditions to detect anomalies in known log patterns.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 345.17642,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Discover value in <em>log</em> <em>data</em> with patterns",
        "sections": "Discover value in <em>log</em> <em>data</em> with patterns",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " using our original pricing model require an Owner or Admin role. If you see Patterns are turned off in your <em>Log</em> <em>management</em> Patterns <em>UI</em>, click the Configure Patterns button and enable it. If you don&#x27;t see patterns within 30 minutes of enabling the feature, there may be a lack of <em>data</em> with a message"
      },
      "id": "6072d46128ccbc244451c18b"
    },
    {
      "sections": [
        "Built-in log parsing rulesets",
        "Apache",
        "Application Load Balancer",
        "Cloudfront",
        "Elastic Load Balancer",
        "Microsoft IIS",
        "Monit",
        "MySQL Error",
        "NGINX",
        "NGINX Error",
        "Route 53",
        "Syslog RFC-5424"
      ],
      "title": "Built-in log parsing rulesets",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "cb5909f2453d475a85d408d75cd3b2a321a8518e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/built-log-parsing-rulesets/",
      "published_at": "2021-10-19T03:43:56Z",
      "updated_at": "2021-10-19T03:43:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic can parse common log formats according to built-in rulesets, so that you don't have to create your own parsing rules. Here are the log parsing rulesets, their Grok patterns, and what fields are parsed. To enable built-in log parsing, see our documentation for adding the logtype attribute. To manage your parsing rulesets programmatically, you can use NerdGraph, our graphQL-format API, at api.newrelic.com/graphiql. For more information, see the NerdGraph tutorial to create, query, and delete your parsing rulesets. Apache Source: logtype = 'apache' Grok: %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{NUMBER:bytes}|-) %{QS:referrer} %{QS:agent} Copy Results: clientip: The IP address of the client. verb: The HTTP verb ident: The user identity of the client making the request response: The HTTP status code of the response request: The URI and request being made httpversion: The HTTP version of the request rawrequest: The raw HTTP request if data is posted bytes: The number of bytes sent referrer: The HTTP referrer agent: The client's user agent Application Load Balancer Source: logtype = 'alb' Grok: ^%{NOTSPACE:type} %{TIMESTAMP_ISO8601:time} %{NOTSPACE:elb} %{NOTSPACE:client_ip}:%{NOTSPACE:client_port} ((%{NOTSPACE:target_ip}:%{NOTSPACE:target_port})|-) %{NOTSPACE:request_processing_time} %{NOTSPACE:target_processing_time} %{NOTSPACE:response_processing_time} %{NOTSPACE:elb_status_code} %{NOTSPACE:target_status_code} %{NOTSPACE:received_bytes} %{NOTSPACE:sent_bytes} \"%{DATA:request}\" \"%{DATA:user_agent}\" %{NOTSPACE:ssl_cipher} %{NOTSPACE:ssl_protocol} %{NOTSPACE:target_group_arn} \"%{DATA:trace_id}\" \"%{NOTSPACE:domain_name}\" \"%{NOTSPACE:chosen_cert_arn}\" %{NOTSPACE:matched_rule_priority} %{TIMESTAMP_ISO8601:request_creation_time} \"%{NOTSPACE:actions_executed}\" \"%{NOTSPACE:redirect_url}\" \"%{NOTSPACE:error_reason}\" (?:\"|)%{DATA:target_port_list}(?:\"|) (?:\"|)%{DATA:target_status_code_list}(?:\"|) \"%{NOTSPACE:classification}\" \"%{NOTSPACE:classification_reason}\" Copy Results: Field Definition type The type of request or connection. Possible values are: http: HTTP https: HTTP over SSL/TLS h2: HTTP/2 over SSL/TLS ws: WebSockets wss: WebSockets over SSL/TLS elb The resource ID of the load balancer. If you are parsing access log entries, note that resources IDs can contain forward slashes (/). client The IP address and port of the requesting client target The IP address and port of the target that processed this request. If the client didn't send a full request, the load balancer can't dispatch the request to a target, and this value is set to -. If the target is a Lambda function, this value is set to -. If the request is blocked by AWS WAF, this value is set to -, and the value of elb_status_code is set to 403. request_processing_time The total time elapsed (in seconds, with millisecond precision) from the time the load balancer received the request until the time it sent it to a target. This value is set to -1 if the load balancer can't dispatch the request to a target. This can happen if the target closes the connection before the idle timeout or if the client sends a malformed request. This value can also be set to -1 if the registered target does not respond before the idle timeout. target_processing_time The total time elapsed (in seconds, with millisecond precision) from the time the load balancer sent the request to a target until the target started to send the response headers. This value is set to -1 if the load balancer can't dispatch the request to a target. This can happen if the target closes the connection before the idle timeout or if the client sends a malformed request. This value can also be set to -1 if the registered target does not respond before the idle timeout. response_processing_time The total time elapsed (in seconds, with millisecond precision) from the time the load balancer received the response header from the target until it started to send the response to the client. This includes both the queuing time at the load balancer and the connection acquisition time from the load balancer to the client. This value is set to -1 if the load balancer can't send the request to a target. This can happen if the target closes the connection before the idle timeout or if the client sends a malformed request. elb_status_code The status code of the response from the load balancer target_status_code The status code of the response from the target. This value is recorded only if a connection was established to the target and the target sent a response. Otherwise, it is set to -. received_bytes The size of the request, in bytes, received from the client (requester). For HTTP requests, this includes the headers. For WebSockets, this is the total number of bytes received from the client on the connection. sent_bytes The size of the response, in bytes, sent to the client (requester). For HTTP requests, this includes the headers. For WebSockets, this is the total number of bytes sent to the client on the connection. method The HTTP verb of the request uri The URI the request was targeting http_version The HTTP version number of the request user_agent User-Agent string that identifies the client that originated the request, enclosed in double quotes. The string consists of one or more product identifiers, product/version. If the string is longer than 8 KB, it is truncated. ssl_cipher The SSL cipher. This value is set to - if the listener is not an HTTPS listener. ssl_protocol The SSL protocol. This value is set to - if the listener is not an HTTPS listener. target_group_arn The Amazon Resource Name (ARN) of the target group trace_id The contents of the X-Amzn-Trace-Id header, enclosed in double quotes domain_name The SNI domain provided by the client during the TLS handshake, enclosed in double quotes. This value is set to - if the client doesn't support SNI or the domain doesn't match a certificate and the default certificate is presented to the client. chosen_cert_arn The ARN of the certificate presented to the client, enclosed in double quotes. Set to session-reused if the session is reused. Set to - if the listener is not an HTTPS listener. matched_rule_priority The priority value of the rule that matched the request. If a rule matched, this is a value from 1 to 50000. If no rule matched and the default action was taken, this value is set to 0. If an error occurs during rules evaluation, it is set to -1. For any other error, it is set to -. request_creation_time The time when the load balancer received the request from the client, in ISO 8601 format. actions_executed The actions taken when processing the request, enclosed in double quotes. This value is a comma-separated list that can include the values described in actions_taken. If no action was taken, such as for a malformed request, this value is set to -. redirect_url The URL of the redirect target for the location header of the HTTP response, enclosed in double quotes. If no redirect actions were taken, this value is set to -. error_reason The error reason code, enclosed in double quotes. If the request failed, this is one of the error codes described in Error Reason Codes. If the actions taken do not include an authenticate action or the target is not a Lambda function, this value is set to -. Cloudfront Source: logtype = 'cloudfront-web' Grok: ^%{NOTSPACE:date}%{SPACE}%{NOTSPACE:time}%{SPACE}%{NOTSPACE:x_edge_location}%{SPACE}%{NOTSPACE:sc_bytes}%{SPACE}%{NOTSPACE:c_ip}%{SPACE}%{NOTSPACE:cs_method}%{SPACE}%{NOTSPACE:cs_host}%{SPACE}%{NOTSPACE:cs_uri_stem}%{SPACE}%{NOTSPACE:sc_status}%{SPACE}%{NOTSPACE:cs_referer}%{SPACE}%{NOTSPACE:cs_user_agent}%{SPACE}%{NOTSPACE:cs_uri_query}%{SPACE}%{NOTSPACE:cs_Cookie}%{SPACE}%{NOTSPACE:x_edge_result_type}%{SPACE}%{NOTSPACE:x_edge_request_id}%{SPACE}%{NOTSPACE:x_host_header}%{SPACE}%{NOTSPACE:cs_protocol}%{SPACE}%{NOTSPACE:cs_bytes}%{SPACE}%{NOTSPACE:time_taken}%{SPACE}%{NOTSPACE:x_forwarded_for}%{SPACE}%{NOTSPACE:ssl_protocol}%{SPACE}%{NOTSPACE:ssl_cipher}%{SPACE}%{NOTSPACE:x_edge_response_result_type}%{SPACE}%{NOTSPACE:cs_protocol_version}%{SPACE}%{NOTSPACE:fle_status}%{SPACE}%{NOTSPACE:fle_encrypted_fields}%{SPACE}%{NOTSPACE:c_port}%{SPACE}%{NOTSPACE:time_to_first_byte}%{SPACE}%{NOTSPACE:x_edge_detailed_result_type}%{SPACE}%{NOTSPACE:sc_content_type}%{SPACE}%{NOTSPACE:sc_content_len}%{SPACE}%{NOTSPACE:sc_range_start}%{SPACE}%{NOTSPACE:sc_range_end} Copy Results: Field Definition x_edge_location The edge location that served the request. Each edge location is identified by a three-letter code and an arbitrarily assigned number; for example, DFW3. The three-letter code typically corresponds with the International Air Transport Association airport code for an airport near the edge location. (These abbreviations might change in the future.) sc_bytes The total number of bytes that CloudFront served to the viewer in response to the request, including headers; for example, 1045619. For WebSocket connections, this is the total number of bytes sent from the server to the client through the connection. c_ip The IP address of the viewer that made the request, either in IPv4 or IPv6 format. If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip is the IP address of the proxy or load balancer. See also X-Forwarded-For. cs_method The HTTP request method: DELETE, GET, HEAD, OPTIONS, PATCH, POST, or PUT. cs_host The domain name of the CloudFront distribution; for example, d111111abcdef8.cloudfront.net. cs_uri_stem The portion of the URI that identifies the path and object; for example, /images/cat.jpg. Question marks in URLs and query strings are not included. sc_status An HTTP status code; for example, 200. Status code 000 indicates the viewer closed the connection (for example, closed the browser tab) before CloudFront could respond to a request. If the viewer closes the connection after CloudFront starts to send the response, the log contains the applicable HTTP status code. cs_referer The name of the domain that originated the request. Common referrers include search engines, other websites that link directly to your objects, and your own website. cs_user_agent The value of the User-Agent header in the request. The User-Agent header identifies the source of the request, such as the type of device and browser that submitted the request, and which search engine if applicable. cs_uri_query The query string portion of the URI, if any. When a URI doesn't contain a query string, this field's value is a hyphen (-). cs_cookie The cookie header in the request, including name-value pairs and the associated attributes. If you enable cookie logging, CloudFront logs the cookies in all requests regardless of which cookies you choose to forward to the origin. If a request doesn't include a cookie header, this field's value is a hyphen (-). x_edge_result_type How CloudFront classifies the response after the last byte left the edge location. In some cases, the result type can change between the time that CloudFront is ready to send the response and the time that CloudFront has finished sending the response. x_edge_request_id An encrypted string that uniquely identifies a request. In the response header, this is x-amz-cf-id. x_host_header The value that the viewer included in the Host header for this request. This is the domain name in the request. If you're using the CloudFront domain name in your object URLs, this field contains that domain name. If you're using alternate domain names in your object URLs, such as http://example.com/logo.png, this field contains the alternate domain name, such as example.com. To use alternate domain names, you must add them to your distribution. cs_protocol The protocol that the viewer specified in the request: http, https, ws, or wss. cs_bytes The number of bytes of data that the viewer included in the request, including headers. For WebSocket connections, this is the total number of bytes sent from the client to the server on the connection. time_taken The number of seconds (to the thousandth of a second; for example, 0.002) between the time that a CloudFront edge server receives a viewer's request and the time that CloudFront writes the last byte of the response to the edge server's output queue as measured on the server. From the perspective of the viewer, the total time to get the full object will be longer than this value due to network latency and TCP buffering. x_forwarded_for If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip in field 5 is the IP address of the proxy or load balancer. In that case, this field is the IP address of the viewer that originated the request. This field contains IPv4 and IPv6 addresses as applicable. If the viewer did not use an HTTP proxy or a load balancer, the value of x_forwarded_for is a hyphen (-). ssl_protocol When cs_protocol in field 17 is https, this field contains the SSL/TLS protocol that the client and CloudFront negotiated for transmitting the request and response. Possible values include: SSLv3 TLSv1 TLSv1.1 TLSv1.2 When cs_protocol in field 17 is http, the value for this field is a hyphen (-). ssl_cipher When cs_protocol in field 17 is https, this field contains the SSL/TLS cipher that the client and CloudFront negotiated for encrypting the request and response. Possible values include: ECDHE-RSA-AES128-GCM-SHA256 ECDHE-RSA-AES128-SHA256 ECDHE-RSA-AES128-SHA ECDHE-RSA-AES256-GCM-SHA384 ECDHE-RSA-AES256-SHA384 ECDHE-RSA-AES256-SHA AES128-GCM-SHA256 AES256-GCM-SHA384 AES128-SHA256 AES256-SHA AES128-SHA DES-CBC3-SHA RC4-MD5 When cs_protocol in field 17 is http, the value for this field is a hyphen (-). x_edge_response_result_type How CloudFront classified the response just before returning the response to the viewer. Possible values include: Hit: CloudFront served the object to the viewer from the edge cache. RefreshHit: CloudFront found the object in the edge cache but it had expired, so CloudFront contacted the origin to verify that the cache has the latest version of the object. Miss: The request could not be satisfied by an object in the edge cache, so CloudFront forwarded the request to the origin server and returned the result to the viewer. LimitExceeded: The request was denied because a CloudFront limit was exceeded. CapacityExceeded: CloudFront returned a 503 error because the edge location didn't have enough capacity at the time of the request to serve the object. Error: Typically this means the request resulted in a client error (sc_status is 4xx) or a server error (sc_status is 5xx). If the value of x_edge_result_type is Error and the value of this field is not Error, the client disconnected before finishing the download. Redirect: CloudFront redirects from HTTP to HTTPS. If sc_status is 403 and you configured CloudFront to restrict the geographic distribution of your content, the request might have come from a restricted location. cs_protocol_version The HTTP version that the viewer specified in the request. Possible values include: HTTP/0.9 HTTP/1.0 HTTP/1.1 HTTP/2.0 fle_status When field-level encryption is configured for a distribution, this field contains a code that indicates whether the request body was successfully processed. If field-level encryption is not configured for the distribution, the value is a hyphen (-). fle-encrypted-fields The number of fields that CloudFront encrypted and forwarded to the origin. CloudFront streams the processed request to the origin as it encrypts data, so fle-encrypted-fields can have a value even if the value of fle-status is an error. If field-level encryption is not configured for the distribution, the value of fle-encrypted-fields is a hyphen (-). c_port The port number of the request from the viewer. time_to_first_byte The number of seconds between receiving the request and writing the first byte of the response, as measured on the server. x_edge_detailed_result_type When x-edge-result-type is not Error, this field contains the same value as x-edge-result-type. When x-edge-result-type is Error, this field contains the specific type of error. sc_content_type The value of the HTTP Content-Type header of the response. sc_content_len The value of the HTTP Content-Length header of the response. sc_range_start When the response contains the HTTP Content-Range header, this field contains the range start value. sc-range-end When the response contains the HTTP Content-Range header, this field contains the range end value. Elastic Load Balancer Source: logtype = 'elb' Grok: ^%{TIMESTAMP_ISO8601:time} %{NOTSPACE:elb} %{NOTSPACE:client_ip}:%{NOTSPACE:client_port} ((%{NOTSPACE:backend_ip}:%{NOTSPACE:backend_port})|-) %{NOTSPACE:request_processing_time} %{NOTSPACE:backend_processing_time} %{NOTSPACE:response_processing_time} %{NOTSPACE:elb_status_code} %{NOTSPACE:backend_status_code} %{NOTSPACE:received_bytes} %{NOTSPACE:sent_bytes} \"%{DATA:request}\" \"%{DATA:user_agent}\" %{NOTSPACE:ssl_cipher} %{NOTSPACE:ssl_protocol} Copy Results: Field Definition x_edge_location The edge location that served the request. Each edge location is identified by a three-letter code and an arbitrarily assigned number; for example, DFW3. The three-letter code typically corresponds with the International Air Transport Association airport code for an airport near the edge location. (These abbreviations might change in the future.) sc_bytes The total number of bytes that CloudFront served to the viewer in response to the request, including headers; for example, 1045619. For WebSocket connections, this is the total number of bytes sent from the server to the client through the connection. c_ip The IP address of the viewer that made the request. If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip is the IP address of the proxy or load balancer. cs_method The HTTP request method: DELETE, GET, HEAD, OPTIONS, PATCH, POST, or PUT. cs_host The domain name of the CloudFront distribution; for example, d111111abcdef8.cloudfront.net. cs_uri_stem The portion of the URI that identifies the path and object; for example, /images/cat.jpg. Question marks (?) in URLs and query strings are not included in the log. sc_status An HTTP status code (for example, 200). Status code 000 indicates that the viewer closed the connection (for example, closed the browser tab) before CloudFront could respond to a request. If the viewer closes the connection after CloudFront starts to send the response, the log contains the applicable HTTP status code. cs_referer he name of the domain that originated the request. Common referrers include search engines, other websites that link directly to your objects, and your own website. cs_user_agent The value of the User-Agent header in the request. The User-Agent header identifies the source of the request, such as the type of device and browser that submitted the request and which search engine if applicable. cs_uri_query The query string portion of the URI, if any. When a URI doesn't contain a query string, this field's value is a hyphen (-). cs_cookie The cookie header in the request, including name-value pairs and the associated attributes. If you enable cookie logging, CloudFront logs the cookies in all requests, regardless of which cookies you choose to forward to the origin. If a request doesn't include a cookie header, this field's value is a hyphen (-). x_edge_result_type How CloudFront classifies the response after the last byte left the edge location. In some cases, the result type can change between the time that CloudFront is ready to send the response and the time that CloudFront has finished sending the response. x_edge_request_id An encrypted string that uniquely identifies a request. In the response header, this is x-amz-cf-id. x_host_header The value that the viewer included in the Host header for this request. This is the domain name in the request. If you're using the CloudFront domain name in your object URLs, this field contains that domain name. If you're using alternate domain names in your object URLs, such as [http://example.com/logo.png, this field contains the alternate domain name, such as example.com. To use alternate domain names, you must add them to your distribution. cs_protocol The protocol that the viewer specified in the request: http, https, ws, or wss. cs_bytes The number of bytes of data that the viewer included in the request, including headers. For WebSocket connections, this is the total number of bytes sent from the client to the server on the connection. time_taken The number of seconds (to the thousandth of a second; for example, 0.002) between the time that a CloudFront edge server receives a viewer's request and the time that CloudFront writes the last byte of the response to the edge server's output queue as measured on the server. From the perspective of the viewer, the total time to get the full object will be longer than this value due to network latency and TCP buffering. x_forwarded_for If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip in field 5 is the IP address of the proxy or load balancer. In that case, this field is the IP address of the viewer that originated the request. This field contains IPv4 and IPv6 addresses as applicable. If the viewer did not use an HTTP proxy or a load balancer, the value of x_forwarded_for is a hyphen (-). ssl_protocol When cs_protocol in field 17 is https, this field contains the SSL/TLS protocol that the client and CloudFront negotiated for transmitting the request and response. Possible values include: SSLv3 TLSv1 TLSv1.1 TLSv1.2 When cs_protocol in field 17 is http, the value for this field is a hyphen (-). ssl_cipher When cs_protocol in field 17 is https, this field contains the SSL/TLS cipher that the client and CloudFront negotiated for encrypting the request and response. Possible values include: ECDHE-RSA-AES128-GCM-SHA256 ECDHE-RSA-AES128-SHA256 ECDHE-RSA-AES128-SHA ECDHE-RSA-AES256-GCM-SHA384 ECDHE-RSA-AES256-SHA384 ECDHE-RSA-AES256-SHA AES128-GCM-SHA256 AES256-GCM-SHA384 AES128-SHA256 AES256-SHA AES128-SHA DES-CBC3-SHA RC4-MD5 When cs_protocol is http, the value for this field is a hyphen (-). x_edge_response_result_type How CloudFront classified the response just before returning the response to the viewer. Possible values include: Hit: CloudFront served the object to the viewer from the edge cache. RefreshHit: CloudFront found the object in the edge cache but it had expired, so CloudFront contacted the origin to verify that the cache has the latest version of the object. Miss: The request could not be satisfied by an object in the edge cache, so CloudFront forwarded the request to the origin server and returned the result to the viewer. LimitExceeded: The request was denied because a CloudFront limit was exceeded. CapacityExceeded: CloudFront returned a 503 error because the edge location didn't have enough capacity at the time of the request to serve the object. Error: Typically this means the request resulted in a client error (sc_status is 4xx) or a server error (sc_status is 5xx). If the value of x_edge_result_type is Error and the value of this field is not Error, the client disconnected before finishing the download. Redirect: CloudFront redirects from HTTP to HTTPS. If sc_status is 403 and you configured CloudFront to restrict the geographic distribution of your content, the request might have come from a restricted location. cs_protocol_version The HTTP version that the viewer specified in the request. Possible values include: HTTP/0.9 HTTP/1.0 HTTP/1.1 HTTP/2.0 fle_status When field-level encryption is configured for a distribution, this field contains a code that indicates whether the request body was successfully processed. If field-level encryption is not configured for the distribution, the value of this field is a hyphen (-). When CloudFront successfully processes the request body, encrypts values in the specified fields, and forwards the request to the origin, the value of this field is Processed. The value of x_edge_result_type can still indicate a client-side or server-side error in this case. If the request exceeds a field-level encryption limit, fle-status contains one of the following error codes, and CloudFront returns HTTP status code 400 to the viewer. fle-encrypted-fields The number of fields that CloudFront encrypted and forwarded to the origin. CloudFront streams the processed request to the origin as it encrypts data, so fle_encrypted_fields can have a value even if the value of fle_status is an error. If field-level encryption is not configured for the distribution, the value of fle_encrypted_fields is a hyphen (-). c_port The port number of the request from the viewer. time_to_first_byte The number of seconds between receiving the request and writing the first byte of the response, as measured on the server. x_edge_detailed_result_type When x_edge_result_type is not Error, this field contains the same value as x_edge_result_type. When x_edge_result_type is Error, this field contains the specific type of error. sc_content_type The value of the HTTP Content-Type header of the response. sc_content_len The value of the HTTP Content-Length header of the response. sc_range_start When the response contains the HTTP Content-Range header, this field contains the range start value. sc-range-end When the response contains the HTTP Content-Range header, this field contains the range end value. Microsoft IIS Source: logtype = 'iis_w3c' Grok: %{TIMESTAMP_ISO8601:log_timestamp} %{NOTSPACE:server_ip} %{WORD:method} %{NOTSPACE:uri} %{NOTSPACE:uri_query} %{NOTSPACE:server_port} %{NOTSPACE:username} %{NOTSPACE:client_ip} %{NOTSPACE:user_agent} %{NOTSPACE:referer} %{NOTSPACE:status} %{NOTSPACE:substatus} %{NOTSPACE:win32_status} %{NOTSPACE:time_taken} Copy Monit Source: logtype = 'monit' Grok: \\\\[%{NOTSPACE:tz} %{SYSLOGTIMESTAMP:nr_timestamp}\\\\] %{WORD:state}%{SPACE}: %{GREEDYDATA:message} Copy Results: state: The severity of the log line message: The message MySQL Error Source: logtype = 'mysql-error' Grok: \\\\[%{WORD:log_level}\\\\] Copy Results: log_level: The severity of the log line NGINX Source: logtype = 'nginx' Grok: %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{NUMBER:bytes}|-) %{QS:referrer} %{QS:agent} Copy Results: clientip: The IP address of the client verb: The HTTP verb ident: The user identity of the client making the request response: The HTTP status code of the response request: The URI and request being made httpversion: The HTTP version of the request rawrequest: The raw HTTP request if data is posted bytes: The number of bytes sent referrer: The HTTP referrer agent: The client's user agent NGINX Error Source: logtype = 'nginx-error' Grok: ^(?<timestamp>%{YEAR:year}[./-]%{MONTHNUM:month}[./-]%{MONTHDAY:day}[- ]%{TIME:time}) \\\\[%{LOGLEVEL:severity}\\\\] %{POSINT:pid}#%{NUMBER}: %{GREEDYDATA:errormessage}(?:, client: (?<client>%{IP:clientip}|%{HOSTNAME:hostname}))(?:, server: %{IPORHOSTORUNDERSCORE:server})(?:, request: %{QS:request})?(?:, upstream: \\\"%{URI:upstream}\\\")?(?:, host: %{QS:host})?(?:, referrer: \\\"%{URI:referrer}\\\")?$ Copy Results: severity: The severity of the log line pid: The server process ID errormessage: The error message clientip: The IP address of the calling client server: The server IP address request: The full request upstream: The upstream URI host: The server's hostname referrer: The HTTP referrer Route 53 Source: logtype = 'route-53' Grok: %{NUMBER:log_format_version} %{TIMESTAMP_ISO8601} %{WORD:zone_id} %{IPORHOST:query} %{WORD:query_type} %{WORD:response_code} %{WORD:protocol} %{WORD:edge_location} %{IP:resolver_ip} %{GREEDYDATA:edns_client_subnet} Copy Results: log_format_version: A versioned format for the log. zone_id: The ID of the hosted zone that is associated with all the DNS queries in this log. query: The domain or subdomain that was specified in the request. query_type: Either the DNS record type that was specified in the request, or ANY. response_code: The DNS response code that Route 53 returned in response to the DNS query. protocol: The protocol that was used to submit the query, either TCP or UDP. edge_location: The Route 53 edge location that responded to the query. Each edge location is identified by a three-letter code and an arbitrary number; for example, DFW3. The three-letter code typically corresponds with the International Air Transport Association airport code for an airport near the edge location. (These abbreviations might change in the future.) resolver_ip: The IP address of the DNS resolver that submitted the request to Route 53. edns_client_subnet: A partial IP address for the client that the request originated from, if available from the DNS resolver. Syslog RFC-5424 Source: logtype = 'syslog-rfc5424' Grok: <%{NONNEGINT:pri}>%{NONNEGINT:version} +(?:%{TIMESTAMP_ISO8601:log.timestamp}|-) +(?:%{HOSTNAME:hostname}|-) +(?:\\\\-|%{NOTSPACE:app.name}) +(?:\\\\-|%{NOTSPACE:procid}) (?:\\\\-|%{NOTSPACE:msgid}) +(?:\\[%{DATA:structured.data}\\]|-|) +%{GREEDYDATA:message} Copy Results: pri: The priority represents both the message facility and severity. version: Syslog protocol version. log.timestamp: Original timestamp. hostname: The machine that originally sent the Syslog message. app.name: The device or application that originated the message. procid: The process name or process ID associated with a Syslog system. msgid: Identifies the type of message. structured.data: Structured data string value. sd.<var>sd-id</var>.<var>sd-param-name</var>: The structured.data content is also parsed into separate attributes following a predefined naming convention: sd.<var>sd-id</var>.<var>sd-param-name</var>. See the structured data parsing examples, which follow. message: Free-form message that provides information about the event. Structured data parsing examples: The structured data [example one=\"1\" two=\"2\"] would be parsed into two different attributes: sd.example.one: \"1\" sd.example.two: \"2\" Copy If the same structured data block contains duplicate param names, it also appends an index-based suffix on the attribute name. For example, the structured data [example number=\"1\" number=\"2\"] would be parsed as: sd.example.number.0: \"1\" sd.example.number.1: \"2\" Copy For structured data with enterprise numbers assigned, an extra attribute is also parsed. For example, the structured data [example@123 number=\"1\"] would be parsed as: sd.example.enterprise.number: 123 sd.example.number: \"1\" Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 344.99893,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Built-in <em>log</em> parsing rulesets",
        "sections": "Built-in <em>log</em> parsing rulesets",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " the DNS resolver. Syslog RFC-5424 Source: logtype = &#x27;syslog-rfc5424&#x27; Grok: &lt;%{NONNEGINT:pri}&gt;%{NONNEGINT:version} +(?:%{TIMESTAMP_ISO8601:<em>log</em>.timestamp}|-) +(?:%{HOSTNAME:hostname}|-) +(?:\\\\-|%{NOTSPACE:app.name}) +(?:\\\\-|%{NOTSPACE:procid}) (?:\\\\-|%{NOTSPACE:msgid}) +(?:\\[%{<em>DATA:structured.data</em>"
      },
      "id": "603e7b9164441f1b2d4e8872"
    },
    {
      "sections": [
        "Drop data with drop filter rules",
        "Savings, security, speed",
        "Caution",
        "How drop filter rules work",
        "Cautions when dropping data",
        "Create drop filter rules",
        "Manage drop filter rules via NerdGraph API",
        "Types of drop filter rules",
        "Drop log events",
        "Drop attributes",
        "Tip",
        "View or delete drop filter rules"
      ],
      "title": "Drop data with drop filter rules",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "9590bd4593abd451633a4beacd94d56eb1a481bd",
      "image": "https://docs.newrelic.com/static/db4b077fafd911b9f5019b022b3048ab/b04e4/ingest-pipeline.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/drop-data-drop-filter-rules/",
      "published_at": "2021-10-19T03:43:55Z",
      "updated_at": "2021-10-19T03:43:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After log event data has been shipped to New Relic, it can either be stored in our NRDB database or dropped (discarded). We can drop both log events and event attributes via drop filter rules. You can manage drop filter rules using our Logs UI, as explained in this document. You can also use NerdGraph, our GraphQL-format API explorer. Savings, security, speed Drop filter rules help you accomplish several important goals: Lower costs by storing only logs relevant to your account. Protect privacy and security by removing personal identifiable information (PII). Reduce noise by removing irrelevant events and attributes. Caution Use caution when deciding to drop data. The data you drop is not recoverable. Before using this feature, review the responsibilities and considerations for dropping data. How drop filter rules work A drop filter rule matches data based on a query. When triggered, the drop filter rule removes the matching data from the ingestion pipeline before it is written to NRDB. This creates an explicit demarcation between the logs being forwarded from your domain and the data that New Relic collects. Since the data removed by the drop filter rule doesn't reach our backend, it cannot be queried: the data is gone and cannot be restored. During the ingestion process, customer log data can be parsed, transformed, or dropped before being stored in New Relic's database. Cautions when dropping data When creating drop rules, you are responsible for ensuring that the rules accurately identify and discard the data that meets the conditions that you have established. You are also responsible for monitoring the rule, as well as the data you disclose to New Relic. New Relic cannot guarantee that this functionality will completely resolve data disclosure concerns you may have. New Relic doesn't review or monitor how effective the rules you develop are. Creating rules about sensitive data can leak information about what kinds of data you maintain, including the format of your data or systems (for example, through referencing email addresses or specific credit card numbers). Any user with the relevant role-based access control permissions can view and edit all information in the rules you create. Create drop filter rules To create and edit drop filters, you must have admin permissions in New Relic, or you must be a member of a role with create and edit permissions for Insights > NRQL Drop Rules. Once a drop filter rule is active, it's applied to all log events ingested from that point onwards. Rules are not applied retroactively. Logs collected before creating a rule are not filtered by that rule. Filter or query the set of logs that contain the data you want to drop. Then, from Manage Data on the left nav of the Logs UI, click Create drop filter. To create a new drop filter rule, you can use new or existing log queries. Go to one.newrelic.com > Logs. Filter or query to the specific set of logs that contain the data to be dropped. Once the query is active, from Manage Data on the left nav of the Logs UI, click Create drop filter. Recommendation: Change the drop rule's default name to a meaningful name. Choose to either drop the entire log event that matches the query or just a specific subset of attributes in the matching events. Review the log partitions where this drop rule applies. Save the drop filter rule. Manage drop filter rules via NerdGraph API If you want to manage your drop filter rules programmatically, you can use NerdGraph, our graphQL-format API, at api.newrelic.com/graphiql. For more information, see the NerdGraph tutorial to create, query, and delete your drop filter rules. Types of drop filter rules The drop filters UI prompts you to select whether to drop logs based on the query or on specific attributes. Drop log events The default type of drop filter rule is to drop logs. This option drops the entire log events that match the filter or query. When creating a rule, try to provide a specific query that only matches log data that should be dropped. Our drop filters process won't let you create drop filter rules without values in the matching query. This prevents badly formed rules from dropping all log data. Drop attributes You can specify attributes to be dropped in a log event that matches your query. At least one or more attributes must be selected. Any attribute which is selected will be dropped; all remaining attributes will be kept and stored in NRDB. Tip We recommend this method for removing fields that could contain personal identifiable information (PII) or other sensitive attributes without losing valuable monitoring data. View or delete drop filter rules To view or delete a drop filter rule: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Drop filters. Click the delete icon next to the drop filter rule you want to remove. Once deleted, rules no longer filter ingested log events.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 344.99872,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Drop <em>data</em> with drop filter rules",
        "sections": "Drop <em>data</em> with drop filter rules",
        "tags": "<em>Log</em> <em>management</em>",
        "body": "After <em>log</em> event <em>data</em> has been shipped to New Relic, it can either be stored in our NRDB database or dropped (discarded). We can drop both <em>log</em> events and event attributes via drop filter rules. You can manage drop filter rules using our <em>Logs</em> <em>UI</em>, as explained in this document. You can also use"
      },
      "id": "603e813f28ccbc08c1eba787"
    }
  ],
  "/docs/logs/log-management/ui-data/query-syntax-logs": [
    {
      "sections": [
        "Discover value in log data with patterns",
        "Technical overview",
        "Availability",
        "Get started",
        "Explore log patterns",
        "Explore logs with no pattern",
        "Masked attributes and wildcards",
        "Troubleshooting",
        "Put the platform to work with patterns"
      ],
      "title": "Discover value in log data with patterns",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "8f1e27c94327ca4888a945f8e12f9c2310ccd7a6",
      "image": "https://docs.newrelic.com/static/578d7186bb34352855696e5307cc82f2/c1b63/log-patterns-logs-without-a-pattern.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/find-unusual-logs-log-patterns/",
      "published_at": "2021-10-19T03:56:24Z",
      "updated_at": "2021-10-19T03:56:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Log patterns are the fastest way to discover value in log data without searching. Log data is high volume telemetry with a low value per individual record. Searching can quickly lead to logs that provide a root cause explanation, but most data is repetitive and hard to contextualize when browsing. Patterns can make log data discoverable without spending a lot of time reading through low value data. Logs > Patterns: Use patterns as the basis for alerts when the frequency of important data changes, or for configuring drop rules to get rid of unnecessary repetitive data. Technical overview Log patterns functionality applies machine learning to normalize and group log messages that are consistent in format but variable in content. These grouped messages can be sorted, making it easy to find the most frequent or rarest sets of logs in your environment. Use patterns as the basis for alerts when the frequency of important data changes, or to configure drop rules to get rid of unnecessary repetitive data. Log patterns use advanced clustering algorithms to group together similar log messages automatically. With patterns, you can: Orient more quickly through millions of logs. Reduce the time it takes to identify unusual behavior in your log estate. Monitor the frequency of known patterns over time to focus your energy on what matters, and exclude what's irrelevant. Availability The ability to configure this feature is dependent on role-based permissions. For example, accounts still using our original pricing model require an Owner or Admin role. If you see Patterns are turned off in your Log management Patterns UI, click the Configure Patterns button and enable it. If you don't see patterns within 30 minutes of enabling the feature, there may be a lack of data with a message attribute for the system to create a pattern from it. Log patterns Limitations and considerations Pricing There is no separate pricing for log patterns. The only cost is for additional data generated and added to your log records. A pattern attribute will be added to all logs that match a pattern. Attributes also may be added when common values are discovered, such as GUIDs, IP addresses, URL, or email addresses. These attributes are automatically extracted from the log message as part of the pattern process. HITRUST accounts The log patterns feature is not FedRAMP compliant. FedRAMP or other HITRUST accounts are not eligible to use patterns. Parsing limits We have a system of safety limits on memory and CPU resources when processing logs and their patterns. These parsing limits can have an impact on the data you get. For more information, see our documentation about parsing limits. Get started To start examining patterns: Go to one.newrelic.com > Logs, and use the account picker dropdown to select the target account where you want to explore patterns. In the left navigation of the Logs UI, click Patterns. The main log UI changes to show patterns that match the query in the query bar. Logs > Log patterns: The line chart shows the top 5 patterns over time. Use the time picker and query bar to adjust the results. Explore log patterns By default the log patterns UI first shows the most frequent occurrence of patterns. To sort to show the rarest patterns first, click the Count column. You can also use the query bar or attributes bar to filter your log patterns. If you want to... Do this... Understand the rate of change in patterns Look at the line chart. The color-coded patterns correspond to the plot column in the table. You can toggle individual plot patterns to narrow your focus. See the individual log messages that match each pattern Click pattern to expand the row and see a table of individual log records. To see additional records, scroll up or down. To explore an individual log in more detail, click it to open the details panel. Group and filter patterns by their attributes Use the query bar and time picker. As you apply different filters and time windows, the log patterns adjust to your new target data. Create an alert from a pattern Add the pattern to the query bar and run the query. Then click Create alert condition in the left nav. Troubleshoot log messages that haven't been clustered into a pattern Use the Logs with no pattern tab in the Log patterns UI. Clicking a specific log message will open the log message details panel you're familiar with from the Logs management page. Explore logs with no pattern The Logs with no pattern tab groups all recent log messages in your account that were not clustered into a known pattern yet. These log messages don't represent any problem or flaw in the system; they have no pattern because they are too new to have been processed by the machine learning system. This makes them valuable to explore when you want to understand what has recently changed in your environment. Logs > Log patterns: New Relic's log patterns feature automatically groups logs without a matching pattern. For example: Are any of these logs tied to a recent problem? This is a quick way to discover unique log data that is appearing for the first time in your environment. Does your log data have a new format? Sometimes the logs don't represent a problem, but a new format of log data that deviates from the data model you expect your applications to follow. Catching these logs early gives you the opportunity to ask developers to correct any deviations in their log output. The more consistent people are in the way log data is generated, the easier it becomes to use logs across a diverse set of teams. Masked attributes and wildcards Parts of the log messages in patterns are classified as variables and are substituted by masked attributes. The masking process supports and improves the clustering phase by allowing the algorithm to ignore changing details and focus on the repetitive structure. Masked attributes include: date_time ip url uuid Masked attributes are highlighted and are easy to identify, as shown in the following example. Here is an example of a log pattern that has masked attributes. Log patterns extract other less trivial variables that don't belong to any masked attribute. These variables are indicated as wildcards *. Here is an example of how wildcards * group variables in the Log patterns UI. Troubleshooting Here are a few reasons why you might have patterns enabled but not see any pattern data. If you're sure none of the items below are true, get help from support.newrelic.com. No data has arrived in the timeframe you're observing. Try expanding the time range you're viewing with the time picker. It's been less than 24 hours since patterns were enabled in the account. This means the ML model may not be generated for the account yet. None of the data coming in has a message field. Patterns will only be generated for values in the message field of a log record. If your logs don't contain message, there will be no data. Put the platform to work with patterns Patterns are a value that is enriched onto the existing log message as a new attribute named newrelic.logPattern. Anything you can do with logs generally can be done with log patterns, such as: Build your own dashboards with patterns, to monitor a specific pattern or group of patterns you care about. Create alerts for patterns by adding NRQL alerts. Use baseline alert conditions to detect anomalies in known log patterns.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 345.17618,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Discover value in <em>log</em> <em>data</em> with patterns",
        "sections": "Discover value in <em>log</em> <em>data</em> with patterns",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " using our original pricing model require an Owner or Admin role. If you see Patterns are turned off in your <em>Log</em> <em>management</em> Patterns <em>UI</em>, click the Configure Patterns button and enable it. If you don&#x27;t see patterns within 30 minutes of enabling the feature, there may be a lack of <em>data</em> with a message"
      },
      "id": "6072d46128ccbc244451c18b"
    },
    {
      "sections": [
        "Parsing log data",
        "Example",
        "How log parsing works",
        "Important",
        "Limits",
        "Tip",
        "Built-in parsing rulesets",
        "List of built-in rulesets",
        "Add the logtype attribute",
        "New Relic infrastructure agent example",
        "Fluentd example",
        "Fluent Bit example",
        "Logstash example",
        "Logs API example",
        "Create custom parsing rules",
        "Troubleshooting"
      ],
      "title": "Parsing log data",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "52955adb68242c4ca582ba9cb8e22963955a8275",
      "image": "https://docs.newrelic.com/static/dc392bb7142d2fdb253a649daf4ebe6d/c1b63/log-parsing-rule-ui.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/parsing/",
      "published_at": "2021-10-19T03:43:56Z",
      "updated_at": "2021-10-19T03:43:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Parsing is the process of splitting unstructured log data into attributes (key/value pairs). You can use these attributes to facet or filter logs in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial on YouTube (approx. 4-1/2 minutes). New Relic parses log data according to rules. This document describes how logs parsing works, how to use built-in rules, and how to create custom rules. You can also create, query, and manage your log parsing rules by using NerdGraph, our GraphQL API, at api.newrelic.com/graphiql. For more information, see our NerdGraph tutorial for parsing. Example A good example is a default NGINX access log containing unstructured text. It is useful for searching but not much else. Here's an example of a typical line: 127.180.71.3 - - [10/May/1997:08:05:32 +0000] \"GET /downloads/product_1 HTTP/1.1\" 304 0 \"-\" \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" Copy In an unparsed format, you would need to do a full text search to answer most questions. After parsing, the log is organized into attributes, like response code and request URL: { \"remote_addr\":\"93.180.71.3\", \"time\":\"1586514731\", \"method\":\"GET\", \"path\":\"/downloads/product_1\", \"version\":\"HTTP/1.1\", \"response\":\"304\", \"bytesSent\": 0, \"user_agent\": \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" } Copy Parsing makes it easier to create custom queries that facet on those values. This helps you understand the distribution of response codes per request URL and quickly find problematic pages. How log parsing works Here's an overview of how New Relic implements parsing of logs: Log parsing How it works What All parsing takes place against the message field; no other fields can be parsed. Each parsing rule is created with matching criteria that determines which logs the rule will attempt to parse. To simplify the matching process, we recommend adding a logtype attribute to your logs. However, you are not limited to using logtype; any attribute can be used as matching criteria. When Parsing will only be applied once to each log message. If multiple parsing rules match the log, only the first that succeeds will be applied. Parsing takes place during log ingestion, before data is written to NRDB. Once data has been written to storage, it can no longer be parsed. Parsing occurs in the pipeline before data enrichments take place. Be careful when defining the matching criteria for a parsing rule. If the criteria is based on an attribute that doesn't exist untail after parsing or enrichment take place, that data won't be present in the logs when matching occurs. As a result, no parsing will happen. How Rules can be written in Grok, regex, or a mixture of the two. Grok is a collection of patterns that abstract away complicated regular expressions. If the content of the message field is JSON, it will be parsed automatically. New Relic's log ingestion pipeline can parse data by matching a log event to a rule that describes how the log should be parsed. There are two ways log events can be parsed: Use a built-in rule. Define a custom rule. Rules are a combination of matching logic and parsing logic. Matching is done by defining a query match on an attribute of the logs. Rules are not applied retroactively. Logs collected before a rule is created are not parsed by that rule. The simplest way to organize your logs and how they are parsed is to include the logtype field in your log event. This tells New Relic what built-in ruleset to apply to the logs. Important Once a parsing rule is active, data parsed by the rule is permanently changed. This cannot be reverted. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. Limits Parsing is computationally expensive, which introduces risk. Parsing is done for custom rules defined in an account and for matching patterns to a log. A large number of patterns or poorly defined custom rules will consume a huge amount of memory and CPU resources while also taking a very long time to complete. In order to prevent problems, we apply two parsing limits: per-message-per-rule and per-account. Limit Description Per-message-per-rule The per-message-per-rule limit prevents the time spent parsing any single message from being greater than 100 ms. If that limit is reached, the system will cease attempting to parse the log message with that rule. The ingestion pipeline will attempt to run any other applicable on that message, and the message will still be passed through the ingestion pipeline and stored in NRDB. The log message will be in its original, unparsed format. Per-account The per-account limit exists to prevent accounts from using more than their fair share of resources. The limit considers the total time spent processing all log messages for an account per-minute. The limit is not a fixed value; it scales up or down proportionally to the volume of data stored daily by the account and the environment size that is subsequently allocated to support that customer. Tip To easily check if your rate limits have been reached, go to your system Limits page in the New Relic UI. Built-in parsing rulesets Common log formats have well-established parsing rules already created for them. To get the benefit of built-in parsing rules, add the logtype attribute when forwarding logs. Set the value to something listed in the following table, and the rules for that type of log will be applied automatically. List of built-in rulesets The following logtype attribute values map to a predefined parsing rule. For example, to query the Application Load Balancer: From the New Relic UI, use the format logtype: alb. From NerdGraph, use the format logtype = 'alb'. To learn what fields are parsed for each rule, see our documentation about built-in parsing rules. logtype Example matching query alb AWS Application Load Balancer logtype:alb apache Apache Access logtype:apache cloudfront-web CloudFront Web logtype:cloudfront-web elb Amazon Elastic Load Balancer logtype:elb iis_w3c Microsoft IIS server logs - W3C format logtype:iis_w3c monit Monit logs logtype:monit mysql-error MySQL Error logtype:mysql-error nginx NGINX access logs logtype:nginx nginx-error NGINX error logs logtype:nginx-error route-53 Amazon Route 53 logs logtype:route-53 syslog-rfc5424 Syslog logtype:syslog-rfc5424 Add the logtype attribute When aggregating logs, it's important to provide metadata that makes it easy to organize, search, and parse those logs. One simple way of doing this is to add the attribute logtype to the log messages when they are shipped. Built-in parsing rules are applied by default to certain logtype values. Here are some examples of how to add logtype to logs sent by some of our supported shipping methods. New Relic infrastructure agent example Add logtype as an attribute. You must set the logtype for each named source. Learn more about using the infrastructure agent to add attributes. logs: - name: file-simple file: /path/to/file attributes: logtype: fileRaw - name: nginx-example file: /var/log/nginx.log attributes: logtype: nginx Copy Fluentd example Add a filter block to the .conf file, which uses a record_transformer to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluentd examples. <filter containers> @type record_transformer enable_ruby true <record> #Add logtype to trigger a built-in parsing rule for nginx access logs logtype nginx #Set timestamp from the value contained in the field \"time\" timestamp record[\"time\"] #Add hostname and tag fields to all records hostname \"#{Socket.gethostname}\" tag ${tag} </record> </filter> Copy Fluent Bit example Add a filter block to the .conf file that uses a record_modifier to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluent Bit examples. [FILTER] Name record_modifier Match * Record logtype nginx Record hostname ${HOSTNAME} Record service_name Sample-App-Name Copy Logstash example Add a filter block to the Logstash configuration which uses an add_field mutate filter to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Logstash examples. filter { mutate { add_field => { \"logtype\" => \"nginx\" \"service_name\" => \"myservicename\" \"hostname\" => \"%{host}\" } } } Copy Logs API example You can add attributes to the JSON request sent to New Relic. In this example we add a logtype attribute of value nginx to trigger the built-in NGINX parsing rule. Learn more about using the Logs API. POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: YOUR_LICENSE_KEY Accept: */* Content-Length: 133 { \"timestamp\": TIMESTAMP_IN_UNIX_EPOCH, \"message\": \"User 'xyz' logged in\", \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } Copy Create custom parsing rules Many logs are formatted or structured in a unique way. In order to parse them, custom logic must be built and applied. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. To create and manage your own, custom parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing, then click Create parsing rule. Enter the parsing rule's name. Choose an attribute and value to match on. Write your Grok pattern and test the rule. To learn about Grok and custom parsing rules, read our blog post about how to parse logs with Grok patterns. Enable and save the custom parsing rule. To view the list of custom parsing rules: From Manage Data on the left nav of the Logs UI, click Parsing. To view existing parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing. Troubleshooting If parsing is not working the way you intended, it may be due to: Logic: The parsing rule matching logic does not match the logs you want. Timing: If your parsing matching rule targets a value that doesn't exist yet, it will fail. This can occur if the value is added later in the pipeline as part of the enrichment process. Limits: There is a fixed amount of time available every minute to process logs via parsing, patterns, drop filters, etc. If the maximum amount of time has been spent, parsing will be skipped for additional log event records. To resolve these problems, create or adjust your custom parsing rules.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 344.99872,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Parsing <em>log</em> <em>data</em>",
        "sections": "Parsing <em>log</em> <em>data</em>",
        "tags": "<em>Log</em> <em>management</em>",
        "body": "Parsing is the process of splitting unstructured <em>log</em> <em>data</em> into attributes (key&#x2F;value pairs). You can use these attributes to facet or filter <em>logs</em> in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial"
      },
      "id": "603e7eb4196a67b0c4a83dd1"
    },
    {
      "sections": [
        "Built-in log parsing rulesets",
        "Apache",
        "Application Load Balancer",
        "Cloudfront",
        "Elastic Load Balancer",
        "Microsoft IIS",
        "Monit",
        "MySQL Error",
        "NGINX",
        "NGINX Error",
        "Route 53",
        "Syslog RFC-5424"
      ],
      "title": "Built-in log parsing rulesets",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "cb5909f2453d475a85d408d75cd3b2a321a8518e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/built-log-parsing-rulesets/",
      "published_at": "2021-10-19T03:43:56Z",
      "updated_at": "2021-10-19T03:43:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic can parse common log formats according to built-in rulesets, so that you don't have to create your own parsing rules. Here are the log parsing rulesets, their Grok patterns, and what fields are parsed. To enable built-in log parsing, see our documentation for adding the logtype attribute. To manage your parsing rulesets programmatically, you can use NerdGraph, our graphQL-format API, at api.newrelic.com/graphiql. For more information, see the NerdGraph tutorial to create, query, and delete your parsing rulesets. Apache Source: logtype = 'apache' Grok: %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{NUMBER:bytes}|-) %{QS:referrer} %{QS:agent} Copy Results: clientip: The IP address of the client. verb: The HTTP verb ident: The user identity of the client making the request response: The HTTP status code of the response request: The URI and request being made httpversion: The HTTP version of the request rawrequest: The raw HTTP request if data is posted bytes: The number of bytes sent referrer: The HTTP referrer agent: The client's user agent Application Load Balancer Source: logtype = 'alb' Grok: ^%{NOTSPACE:type} %{TIMESTAMP_ISO8601:time} %{NOTSPACE:elb} %{NOTSPACE:client_ip}:%{NOTSPACE:client_port} ((%{NOTSPACE:target_ip}:%{NOTSPACE:target_port})|-) %{NOTSPACE:request_processing_time} %{NOTSPACE:target_processing_time} %{NOTSPACE:response_processing_time} %{NOTSPACE:elb_status_code} %{NOTSPACE:target_status_code} %{NOTSPACE:received_bytes} %{NOTSPACE:sent_bytes} \"%{DATA:request}\" \"%{DATA:user_agent}\" %{NOTSPACE:ssl_cipher} %{NOTSPACE:ssl_protocol} %{NOTSPACE:target_group_arn} \"%{DATA:trace_id}\" \"%{NOTSPACE:domain_name}\" \"%{NOTSPACE:chosen_cert_arn}\" %{NOTSPACE:matched_rule_priority} %{TIMESTAMP_ISO8601:request_creation_time} \"%{NOTSPACE:actions_executed}\" \"%{NOTSPACE:redirect_url}\" \"%{NOTSPACE:error_reason}\" (?:\"|)%{DATA:target_port_list}(?:\"|) (?:\"|)%{DATA:target_status_code_list}(?:\"|) \"%{NOTSPACE:classification}\" \"%{NOTSPACE:classification_reason}\" Copy Results: Field Definition type The type of request or connection. Possible values are: http: HTTP https: HTTP over SSL/TLS h2: HTTP/2 over SSL/TLS ws: WebSockets wss: WebSockets over SSL/TLS elb The resource ID of the load balancer. If you are parsing access log entries, note that resources IDs can contain forward slashes (/). client The IP address and port of the requesting client target The IP address and port of the target that processed this request. If the client didn't send a full request, the load balancer can't dispatch the request to a target, and this value is set to -. If the target is a Lambda function, this value is set to -. If the request is blocked by AWS WAF, this value is set to -, and the value of elb_status_code is set to 403. request_processing_time The total time elapsed (in seconds, with millisecond precision) from the time the load balancer received the request until the time it sent it to a target. This value is set to -1 if the load balancer can't dispatch the request to a target. This can happen if the target closes the connection before the idle timeout or if the client sends a malformed request. This value can also be set to -1 if the registered target does not respond before the idle timeout. target_processing_time The total time elapsed (in seconds, with millisecond precision) from the time the load balancer sent the request to a target until the target started to send the response headers. This value is set to -1 if the load balancer can't dispatch the request to a target. This can happen if the target closes the connection before the idle timeout or if the client sends a malformed request. This value can also be set to -1 if the registered target does not respond before the idle timeout. response_processing_time The total time elapsed (in seconds, with millisecond precision) from the time the load balancer received the response header from the target until it started to send the response to the client. This includes both the queuing time at the load balancer and the connection acquisition time from the load balancer to the client. This value is set to -1 if the load balancer can't send the request to a target. This can happen if the target closes the connection before the idle timeout or if the client sends a malformed request. elb_status_code The status code of the response from the load balancer target_status_code The status code of the response from the target. This value is recorded only if a connection was established to the target and the target sent a response. Otherwise, it is set to -. received_bytes The size of the request, in bytes, received from the client (requester). For HTTP requests, this includes the headers. For WebSockets, this is the total number of bytes received from the client on the connection. sent_bytes The size of the response, in bytes, sent to the client (requester). For HTTP requests, this includes the headers. For WebSockets, this is the total number of bytes sent to the client on the connection. method The HTTP verb of the request uri The URI the request was targeting http_version The HTTP version number of the request user_agent User-Agent string that identifies the client that originated the request, enclosed in double quotes. The string consists of one or more product identifiers, product/version. If the string is longer than 8 KB, it is truncated. ssl_cipher The SSL cipher. This value is set to - if the listener is not an HTTPS listener. ssl_protocol The SSL protocol. This value is set to - if the listener is not an HTTPS listener. target_group_arn The Amazon Resource Name (ARN) of the target group trace_id The contents of the X-Amzn-Trace-Id header, enclosed in double quotes domain_name The SNI domain provided by the client during the TLS handshake, enclosed in double quotes. This value is set to - if the client doesn't support SNI or the domain doesn't match a certificate and the default certificate is presented to the client. chosen_cert_arn The ARN of the certificate presented to the client, enclosed in double quotes. Set to session-reused if the session is reused. Set to - if the listener is not an HTTPS listener. matched_rule_priority The priority value of the rule that matched the request. If a rule matched, this is a value from 1 to 50000. If no rule matched and the default action was taken, this value is set to 0. If an error occurs during rules evaluation, it is set to -1. For any other error, it is set to -. request_creation_time The time when the load balancer received the request from the client, in ISO 8601 format. actions_executed The actions taken when processing the request, enclosed in double quotes. This value is a comma-separated list that can include the values described in actions_taken. If no action was taken, such as for a malformed request, this value is set to -. redirect_url The URL of the redirect target for the location header of the HTTP response, enclosed in double quotes. If no redirect actions were taken, this value is set to -. error_reason The error reason code, enclosed in double quotes. If the request failed, this is one of the error codes described in Error Reason Codes. If the actions taken do not include an authenticate action or the target is not a Lambda function, this value is set to -. Cloudfront Source: logtype = 'cloudfront-web' Grok: ^%{NOTSPACE:date}%{SPACE}%{NOTSPACE:time}%{SPACE}%{NOTSPACE:x_edge_location}%{SPACE}%{NOTSPACE:sc_bytes}%{SPACE}%{NOTSPACE:c_ip}%{SPACE}%{NOTSPACE:cs_method}%{SPACE}%{NOTSPACE:cs_host}%{SPACE}%{NOTSPACE:cs_uri_stem}%{SPACE}%{NOTSPACE:sc_status}%{SPACE}%{NOTSPACE:cs_referer}%{SPACE}%{NOTSPACE:cs_user_agent}%{SPACE}%{NOTSPACE:cs_uri_query}%{SPACE}%{NOTSPACE:cs_Cookie}%{SPACE}%{NOTSPACE:x_edge_result_type}%{SPACE}%{NOTSPACE:x_edge_request_id}%{SPACE}%{NOTSPACE:x_host_header}%{SPACE}%{NOTSPACE:cs_protocol}%{SPACE}%{NOTSPACE:cs_bytes}%{SPACE}%{NOTSPACE:time_taken}%{SPACE}%{NOTSPACE:x_forwarded_for}%{SPACE}%{NOTSPACE:ssl_protocol}%{SPACE}%{NOTSPACE:ssl_cipher}%{SPACE}%{NOTSPACE:x_edge_response_result_type}%{SPACE}%{NOTSPACE:cs_protocol_version}%{SPACE}%{NOTSPACE:fle_status}%{SPACE}%{NOTSPACE:fle_encrypted_fields}%{SPACE}%{NOTSPACE:c_port}%{SPACE}%{NOTSPACE:time_to_first_byte}%{SPACE}%{NOTSPACE:x_edge_detailed_result_type}%{SPACE}%{NOTSPACE:sc_content_type}%{SPACE}%{NOTSPACE:sc_content_len}%{SPACE}%{NOTSPACE:sc_range_start}%{SPACE}%{NOTSPACE:sc_range_end} Copy Results: Field Definition x_edge_location The edge location that served the request. Each edge location is identified by a three-letter code and an arbitrarily assigned number; for example, DFW3. The three-letter code typically corresponds with the International Air Transport Association airport code for an airport near the edge location. (These abbreviations might change in the future.) sc_bytes The total number of bytes that CloudFront served to the viewer in response to the request, including headers; for example, 1045619. For WebSocket connections, this is the total number of bytes sent from the server to the client through the connection. c_ip The IP address of the viewer that made the request, either in IPv4 or IPv6 format. If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip is the IP address of the proxy or load balancer. See also X-Forwarded-For. cs_method The HTTP request method: DELETE, GET, HEAD, OPTIONS, PATCH, POST, or PUT. cs_host The domain name of the CloudFront distribution; for example, d111111abcdef8.cloudfront.net. cs_uri_stem The portion of the URI that identifies the path and object; for example, /images/cat.jpg. Question marks in URLs and query strings are not included. sc_status An HTTP status code; for example, 200. Status code 000 indicates the viewer closed the connection (for example, closed the browser tab) before CloudFront could respond to a request. If the viewer closes the connection after CloudFront starts to send the response, the log contains the applicable HTTP status code. cs_referer The name of the domain that originated the request. Common referrers include search engines, other websites that link directly to your objects, and your own website. cs_user_agent The value of the User-Agent header in the request. The User-Agent header identifies the source of the request, such as the type of device and browser that submitted the request, and which search engine if applicable. cs_uri_query The query string portion of the URI, if any. When a URI doesn't contain a query string, this field's value is a hyphen (-). cs_cookie The cookie header in the request, including name-value pairs and the associated attributes. If you enable cookie logging, CloudFront logs the cookies in all requests regardless of which cookies you choose to forward to the origin. If a request doesn't include a cookie header, this field's value is a hyphen (-). x_edge_result_type How CloudFront classifies the response after the last byte left the edge location. In some cases, the result type can change between the time that CloudFront is ready to send the response and the time that CloudFront has finished sending the response. x_edge_request_id An encrypted string that uniquely identifies a request. In the response header, this is x-amz-cf-id. x_host_header The value that the viewer included in the Host header for this request. This is the domain name in the request. If you're using the CloudFront domain name in your object URLs, this field contains that domain name. If you're using alternate domain names in your object URLs, such as http://example.com/logo.png, this field contains the alternate domain name, such as example.com. To use alternate domain names, you must add them to your distribution. cs_protocol The protocol that the viewer specified in the request: http, https, ws, or wss. cs_bytes The number of bytes of data that the viewer included in the request, including headers. For WebSocket connections, this is the total number of bytes sent from the client to the server on the connection. time_taken The number of seconds (to the thousandth of a second; for example, 0.002) between the time that a CloudFront edge server receives a viewer's request and the time that CloudFront writes the last byte of the response to the edge server's output queue as measured on the server. From the perspective of the viewer, the total time to get the full object will be longer than this value due to network latency and TCP buffering. x_forwarded_for If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip in field 5 is the IP address of the proxy or load balancer. In that case, this field is the IP address of the viewer that originated the request. This field contains IPv4 and IPv6 addresses as applicable. If the viewer did not use an HTTP proxy or a load balancer, the value of x_forwarded_for is a hyphen (-). ssl_protocol When cs_protocol in field 17 is https, this field contains the SSL/TLS protocol that the client and CloudFront negotiated for transmitting the request and response. Possible values include: SSLv3 TLSv1 TLSv1.1 TLSv1.2 When cs_protocol in field 17 is http, the value for this field is a hyphen (-). ssl_cipher When cs_protocol in field 17 is https, this field contains the SSL/TLS cipher that the client and CloudFront negotiated for encrypting the request and response. Possible values include: ECDHE-RSA-AES128-GCM-SHA256 ECDHE-RSA-AES128-SHA256 ECDHE-RSA-AES128-SHA ECDHE-RSA-AES256-GCM-SHA384 ECDHE-RSA-AES256-SHA384 ECDHE-RSA-AES256-SHA AES128-GCM-SHA256 AES256-GCM-SHA384 AES128-SHA256 AES256-SHA AES128-SHA DES-CBC3-SHA RC4-MD5 When cs_protocol in field 17 is http, the value for this field is a hyphen (-). x_edge_response_result_type How CloudFront classified the response just before returning the response to the viewer. Possible values include: Hit: CloudFront served the object to the viewer from the edge cache. RefreshHit: CloudFront found the object in the edge cache but it had expired, so CloudFront contacted the origin to verify that the cache has the latest version of the object. Miss: The request could not be satisfied by an object in the edge cache, so CloudFront forwarded the request to the origin server and returned the result to the viewer. LimitExceeded: The request was denied because a CloudFront limit was exceeded. CapacityExceeded: CloudFront returned a 503 error because the edge location didn't have enough capacity at the time of the request to serve the object. Error: Typically this means the request resulted in a client error (sc_status is 4xx) or a server error (sc_status is 5xx). If the value of x_edge_result_type is Error and the value of this field is not Error, the client disconnected before finishing the download. Redirect: CloudFront redirects from HTTP to HTTPS. If sc_status is 403 and you configured CloudFront to restrict the geographic distribution of your content, the request might have come from a restricted location. cs_protocol_version The HTTP version that the viewer specified in the request. Possible values include: HTTP/0.9 HTTP/1.0 HTTP/1.1 HTTP/2.0 fle_status When field-level encryption is configured for a distribution, this field contains a code that indicates whether the request body was successfully processed. If field-level encryption is not configured for the distribution, the value is a hyphen (-). fle-encrypted-fields The number of fields that CloudFront encrypted and forwarded to the origin. CloudFront streams the processed request to the origin as it encrypts data, so fle-encrypted-fields can have a value even if the value of fle-status is an error. If field-level encryption is not configured for the distribution, the value of fle-encrypted-fields is a hyphen (-). c_port The port number of the request from the viewer. time_to_first_byte The number of seconds between receiving the request and writing the first byte of the response, as measured on the server. x_edge_detailed_result_type When x-edge-result-type is not Error, this field contains the same value as x-edge-result-type. When x-edge-result-type is Error, this field contains the specific type of error. sc_content_type The value of the HTTP Content-Type header of the response. sc_content_len The value of the HTTP Content-Length header of the response. sc_range_start When the response contains the HTTP Content-Range header, this field contains the range start value. sc-range-end When the response contains the HTTP Content-Range header, this field contains the range end value. Elastic Load Balancer Source: logtype = 'elb' Grok: ^%{TIMESTAMP_ISO8601:time} %{NOTSPACE:elb} %{NOTSPACE:client_ip}:%{NOTSPACE:client_port} ((%{NOTSPACE:backend_ip}:%{NOTSPACE:backend_port})|-) %{NOTSPACE:request_processing_time} %{NOTSPACE:backend_processing_time} %{NOTSPACE:response_processing_time} %{NOTSPACE:elb_status_code} %{NOTSPACE:backend_status_code} %{NOTSPACE:received_bytes} %{NOTSPACE:sent_bytes} \"%{DATA:request}\" \"%{DATA:user_agent}\" %{NOTSPACE:ssl_cipher} %{NOTSPACE:ssl_protocol} Copy Results: Field Definition x_edge_location The edge location that served the request. Each edge location is identified by a three-letter code and an arbitrarily assigned number; for example, DFW3. The three-letter code typically corresponds with the International Air Transport Association airport code for an airport near the edge location. (These abbreviations might change in the future.) sc_bytes The total number of bytes that CloudFront served to the viewer in response to the request, including headers; for example, 1045619. For WebSocket connections, this is the total number of bytes sent from the server to the client through the connection. c_ip The IP address of the viewer that made the request. If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip is the IP address of the proxy or load balancer. cs_method The HTTP request method: DELETE, GET, HEAD, OPTIONS, PATCH, POST, or PUT. cs_host The domain name of the CloudFront distribution; for example, d111111abcdef8.cloudfront.net. cs_uri_stem The portion of the URI that identifies the path and object; for example, /images/cat.jpg. Question marks (?) in URLs and query strings are not included in the log. sc_status An HTTP status code (for example, 200). Status code 000 indicates that the viewer closed the connection (for example, closed the browser tab) before CloudFront could respond to a request. If the viewer closes the connection after CloudFront starts to send the response, the log contains the applicable HTTP status code. cs_referer he name of the domain that originated the request. Common referrers include search engines, other websites that link directly to your objects, and your own website. cs_user_agent The value of the User-Agent header in the request. The User-Agent header identifies the source of the request, such as the type of device and browser that submitted the request and which search engine if applicable. cs_uri_query The query string portion of the URI, if any. When a URI doesn't contain a query string, this field's value is a hyphen (-). cs_cookie The cookie header in the request, including name-value pairs and the associated attributes. If you enable cookie logging, CloudFront logs the cookies in all requests, regardless of which cookies you choose to forward to the origin. If a request doesn't include a cookie header, this field's value is a hyphen (-). x_edge_result_type How CloudFront classifies the response after the last byte left the edge location. In some cases, the result type can change between the time that CloudFront is ready to send the response and the time that CloudFront has finished sending the response. x_edge_request_id An encrypted string that uniquely identifies a request. In the response header, this is x-amz-cf-id. x_host_header The value that the viewer included in the Host header for this request. This is the domain name in the request. If you're using the CloudFront domain name in your object URLs, this field contains that domain name. If you're using alternate domain names in your object URLs, such as [http://example.com/logo.png, this field contains the alternate domain name, such as example.com. To use alternate domain names, you must add them to your distribution. cs_protocol The protocol that the viewer specified in the request: http, https, ws, or wss. cs_bytes The number of bytes of data that the viewer included in the request, including headers. For WebSocket connections, this is the total number of bytes sent from the client to the server on the connection. time_taken The number of seconds (to the thousandth of a second; for example, 0.002) between the time that a CloudFront edge server receives a viewer's request and the time that CloudFront writes the last byte of the response to the edge server's output queue as measured on the server. From the perspective of the viewer, the total time to get the full object will be longer than this value due to network latency and TCP buffering. x_forwarded_for If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip in field 5 is the IP address of the proxy or load balancer. In that case, this field is the IP address of the viewer that originated the request. This field contains IPv4 and IPv6 addresses as applicable. If the viewer did not use an HTTP proxy or a load balancer, the value of x_forwarded_for is a hyphen (-). ssl_protocol When cs_protocol in field 17 is https, this field contains the SSL/TLS protocol that the client and CloudFront negotiated for transmitting the request and response. Possible values include: SSLv3 TLSv1 TLSv1.1 TLSv1.2 When cs_protocol in field 17 is http, the value for this field is a hyphen (-). ssl_cipher When cs_protocol in field 17 is https, this field contains the SSL/TLS cipher that the client and CloudFront negotiated for encrypting the request and response. Possible values include: ECDHE-RSA-AES128-GCM-SHA256 ECDHE-RSA-AES128-SHA256 ECDHE-RSA-AES128-SHA ECDHE-RSA-AES256-GCM-SHA384 ECDHE-RSA-AES256-SHA384 ECDHE-RSA-AES256-SHA AES128-GCM-SHA256 AES256-GCM-SHA384 AES128-SHA256 AES256-SHA AES128-SHA DES-CBC3-SHA RC4-MD5 When cs_protocol is http, the value for this field is a hyphen (-). x_edge_response_result_type How CloudFront classified the response just before returning the response to the viewer. Possible values include: Hit: CloudFront served the object to the viewer from the edge cache. RefreshHit: CloudFront found the object in the edge cache but it had expired, so CloudFront contacted the origin to verify that the cache has the latest version of the object. Miss: The request could not be satisfied by an object in the edge cache, so CloudFront forwarded the request to the origin server and returned the result to the viewer. LimitExceeded: The request was denied because a CloudFront limit was exceeded. CapacityExceeded: CloudFront returned a 503 error because the edge location didn't have enough capacity at the time of the request to serve the object. Error: Typically this means the request resulted in a client error (sc_status is 4xx) or a server error (sc_status is 5xx). If the value of x_edge_result_type is Error and the value of this field is not Error, the client disconnected before finishing the download. Redirect: CloudFront redirects from HTTP to HTTPS. If sc_status is 403 and you configured CloudFront to restrict the geographic distribution of your content, the request might have come from a restricted location. cs_protocol_version The HTTP version that the viewer specified in the request. Possible values include: HTTP/0.9 HTTP/1.0 HTTP/1.1 HTTP/2.0 fle_status When field-level encryption is configured for a distribution, this field contains a code that indicates whether the request body was successfully processed. If field-level encryption is not configured for the distribution, the value of this field is a hyphen (-). When CloudFront successfully processes the request body, encrypts values in the specified fields, and forwards the request to the origin, the value of this field is Processed. The value of x_edge_result_type can still indicate a client-side or server-side error in this case. If the request exceeds a field-level encryption limit, fle-status contains one of the following error codes, and CloudFront returns HTTP status code 400 to the viewer. fle-encrypted-fields The number of fields that CloudFront encrypted and forwarded to the origin. CloudFront streams the processed request to the origin as it encrypts data, so fle_encrypted_fields can have a value even if the value of fle_status is an error. If field-level encryption is not configured for the distribution, the value of fle_encrypted_fields is a hyphen (-). c_port The port number of the request from the viewer. time_to_first_byte The number of seconds between receiving the request and writing the first byte of the response, as measured on the server. x_edge_detailed_result_type When x_edge_result_type is not Error, this field contains the same value as x_edge_result_type. When x_edge_result_type is Error, this field contains the specific type of error. sc_content_type The value of the HTTP Content-Type header of the response. sc_content_len The value of the HTTP Content-Length header of the response. sc_range_start When the response contains the HTTP Content-Range header, this field contains the range start value. sc-range-end When the response contains the HTTP Content-Range header, this field contains the range end value. Microsoft IIS Source: logtype = 'iis_w3c' Grok: %{TIMESTAMP_ISO8601:log_timestamp} %{NOTSPACE:server_ip} %{WORD:method} %{NOTSPACE:uri} %{NOTSPACE:uri_query} %{NOTSPACE:server_port} %{NOTSPACE:username} %{NOTSPACE:client_ip} %{NOTSPACE:user_agent} %{NOTSPACE:referer} %{NOTSPACE:status} %{NOTSPACE:substatus} %{NOTSPACE:win32_status} %{NOTSPACE:time_taken} Copy Monit Source: logtype = 'monit' Grok: \\\\[%{NOTSPACE:tz} %{SYSLOGTIMESTAMP:nr_timestamp}\\\\] %{WORD:state}%{SPACE}: %{GREEDYDATA:message} Copy Results: state: The severity of the log line message: The message MySQL Error Source: logtype = 'mysql-error' Grok: \\\\[%{WORD:log_level}\\\\] Copy Results: log_level: The severity of the log line NGINX Source: logtype = 'nginx' Grok: %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{NUMBER:bytes}|-) %{QS:referrer} %{QS:agent} Copy Results: clientip: The IP address of the client verb: The HTTP verb ident: The user identity of the client making the request response: The HTTP status code of the response request: The URI and request being made httpversion: The HTTP version of the request rawrequest: The raw HTTP request if data is posted bytes: The number of bytes sent referrer: The HTTP referrer agent: The client's user agent NGINX Error Source: logtype = 'nginx-error' Grok: ^(?<timestamp>%{YEAR:year}[./-]%{MONTHNUM:month}[./-]%{MONTHDAY:day}[- ]%{TIME:time}) \\\\[%{LOGLEVEL:severity}\\\\] %{POSINT:pid}#%{NUMBER}: %{GREEDYDATA:errormessage}(?:, client: (?<client>%{IP:clientip}|%{HOSTNAME:hostname}))(?:, server: %{IPORHOSTORUNDERSCORE:server})(?:, request: %{QS:request})?(?:, upstream: \\\"%{URI:upstream}\\\")?(?:, host: %{QS:host})?(?:, referrer: \\\"%{URI:referrer}\\\")?$ Copy Results: severity: The severity of the log line pid: The server process ID errormessage: The error message clientip: The IP address of the calling client server: The server IP address request: The full request upstream: The upstream URI host: The server's hostname referrer: The HTTP referrer Route 53 Source: logtype = 'route-53' Grok: %{NUMBER:log_format_version} %{TIMESTAMP_ISO8601} %{WORD:zone_id} %{IPORHOST:query} %{WORD:query_type} %{WORD:response_code} %{WORD:protocol} %{WORD:edge_location} %{IP:resolver_ip} %{GREEDYDATA:edns_client_subnet} Copy Results: log_format_version: A versioned format for the log. zone_id: The ID of the hosted zone that is associated with all the DNS queries in this log. query: The domain or subdomain that was specified in the request. query_type: Either the DNS record type that was specified in the request, or ANY. response_code: The DNS response code that Route 53 returned in response to the DNS query. protocol: The protocol that was used to submit the query, either TCP or UDP. edge_location: The Route 53 edge location that responded to the query. Each edge location is identified by a three-letter code and an arbitrary number; for example, DFW3. The three-letter code typically corresponds with the International Air Transport Association airport code for an airport near the edge location. (These abbreviations might change in the future.) resolver_ip: The IP address of the DNS resolver that submitted the request to Route 53. edns_client_subnet: A partial IP address for the client that the request originated from, if available from the DNS resolver. Syslog RFC-5424 Source: logtype = 'syslog-rfc5424' Grok: <%{NONNEGINT:pri}>%{NONNEGINT:version} +(?:%{TIMESTAMP_ISO8601:log.timestamp}|-) +(?:%{HOSTNAME:hostname}|-) +(?:\\\\-|%{NOTSPACE:app.name}) +(?:\\\\-|%{NOTSPACE:procid}) (?:\\\\-|%{NOTSPACE:msgid}) +(?:\\[%{DATA:structured.data}\\]|-|) +%{GREEDYDATA:message} Copy Results: pri: The priority represents both the message facility and severity. version: Syslog protocol version. log.timestamp: Original timestamp. hostname: The machine that originally sent the Syslog message. app.name: The device or application that originated the message. procid: The process name or process ID associated with a Syslog system. msgid: Identifies the type of message. structured.data: Structured data string value. sd.<var>sd-id</var>.<var>sd-param-name</var>: The structured.data content is also parsed into separate attributes following a predefined naming convention: sd.<var>sd-id</var>.<var>sd-param-name</var>. See the structured data parsing examples, which follow. message: Free-form message that provides information about the event. Structured data parsing examples: The structured data [example one=\"1\" two=\"2\"] would be parsed into two different attributes: sd.example.one: \"1\" sd.example.two: \"2\" Copy If the same structured data block contains duplicate param names, it also appends an index-based suffix on the attribute name. For example, the structured data [example number=\"1\" number=\"2\"] would be parsed as: sd.example.number.0: \"1\" sd.example.number.1: \"2\" Copy For structured data with enterprise numbers assigned, an extra attribute is also parsed. For example, the structured data [example@123 number=\"1\"] would be parsed as: sd.example.enterprise.number: 123 sd.example.number: \"1\" Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 344.99872,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Built-in <em>log</em> parsing rulesets",
        "sections": "Built-in <em>log</em> parsing rulesets",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " the DNS resolver. Syslog RFC-5424 Source: logtype = &#x27;syslog-rfc5424&#x27; Grok: &lt;%{NONNEGINT:pri}&gt;%{NONNEGINT:version} +(?:%{TIMESTAMP_ISO8601:<em>log</em>.timestamp}|-) +(?:%{HOSTNAME:hostname}|-) +(?:\\\\-|%{NOTSPACE:app.name}) +(?:\\\\-|%{NOTSPACE:procid}) (?:\\\\-|%{NOTSPACE:msgid}) +(?:\\[%{<em>DATA:structured.data</em>"
      },
      "id": "603e7b9164441f1b2d4e8872"
    }
  ],
  "/docs/logs/log-management/ui-data/use-logs-ui": [
    {
      "sections": [
        "Discover value in log data with patterns",
        "Technical overview",
        "Availability",
        "Get started",
        "Explore log patterns",
        "Explore logs with no pattern",
        "Masked attributes and wildcards",
        "Troubleshooting",
        "Put the platform to work with patterns"
      ],
      "title": "Discover value in log data with patterns",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "8f1e27c94327ca4888a945f8e12f9c2310ccd7a6",
      "image": "https://docs.newrelic.com/static/578d7186bb34352855696e5307cc82f2/c1b63/log-patterns-logs-without-a-pattern.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/find-unusual-logs-log-patterns/",
      "published_at": "2021-10-19T03:56:24Z",
      "updated_at": "2021-10-19T03:56:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Log patterns are the fastest way to discover value in log data without searching. Log data is high volume telemetry with a low value per individual record. Searching can quickly lead to logs that provide a root cause explanation, but most data is repetitive and hard to contextualize when browsing. Patterns can make log data discoverable without spending a lot of time reading through low value data. Logs > Patterns: Use patterns as the basis for alerts when the frequency of important data changes, or for configuring drop rules to get rid of unnecessary repetitive data. Technical overview Log patterns functionality applies machine learning to normalize and group log messages that are consistent in format but variable in content. These grouped messages can be sorted, making it easy to find the most frequent or rarest sets of logs in your environment. Use patterns as the basis for alerts when the frequency of important data changes, or to configure drop rules to get rid of unnecessary repetitive data. Log patterns use advanced clustering algorithms to group together similar log messages automatically. With patterns, you can: Orient more quickly through millions of logs. Reduce the time it takes to identify unusual behavior in your log estate. Monitor the frequency of known patterns over time to focus your energy on what matters, and exclude what's irrelevant. Availability The ability to configure this feature is dependent on role-based permissions. For example, accounts still using our original pricing model require an Owner or Admin role. If you see Patterns are turned off in your Log management Patterns UI, click the Configure Patterns button and enable it. If you don't see patterns within 30 minutes of enabling the feature, there may be a lack of data with a message attribute for the system to create a pattern from it. Log patterns Limitations and considerations Pricing There is no separate pricing for log patterns. The only cost is for additional data generated and added to your log records. A pattern attribute will be added to all logs that match a pattern. Attributes also may be added when common values are discovered, such as GUIDs, IP addresses, URL, or email addresses. These attributes are automatically extracted from the log message as part of the pattern process. HITRUST accounts The log patterns feature is not FedRAMP compliant. FedRAMP or other HITRUST accounts are not eligible to use patterns. Parsing limits We have a system of safety limits on memory and CPU resources when processing logs and their patterns. These parsing limits can have an impact on the data you get. For more information, see our documentation about parsing limits. Get started To start examining patterns: Go to one.newrelic.com > Logs, and use the account picker dropdown to select the target account where you want to explore patterns. In the left navigation of the Logs UI, click Patterns. The main log UI changes to show patterns that match the query in the query bar. Logs > Log patterns: The line chart shows the top 5 patterns over time. Use the time picker and query bar to adjust the results. Explore log patterns By default the log patterns UI first shows the most frequent occurrence of patterns. To sort to show the rarest patterns first, click the Count column. You can also use the query bar or attributes bar to filter your log patterns. If you want to... Do this... Understand the rate of change in patterns Look at the line chart. The color-coded patterns correspond to the plot column in the table. You can toggle individual plot patterns to narrow your focus. See the individual log messages that match each pattern Click pattern to expand the row and see a table of individual log records. To see additional records, scroll up or down. To explore an individual log in more detail, click it to open the details panel. Group and filter patterns by their attributes Use the query bar and time picker. As you apply different filters and time windows, the log patterns adjust to your new target data. Create an alert from a pattern Add the pattern to the query bar and run the query. Then click Create alert condition in the left nav. Troubleshoot log messages that haven't been clustered into a pattern Use the Logs with no pattern tab in the Log patterns UI. Clicking a specific log message will open the log message details panel you're familiar with from the Logs management page. Explore logs with no pattern The Logs with no pattern tab groups all recent log messages in your account that were not clustered into a known pattern yet. These log messages don't represent any problem or flaw in the system; they have no pattern because they are too new to have been processed by the machine learning system. This makes them valuable to explore when you want to understand what has recently changed in your environment. Logs > Log patterns: New Relic's log patterns feature automatically groups logs without a matching pattern. For example: Are any of these logs tied to a recent problem? This is a quick way to discover unique log data that is appearing for the first time in your environment. Does your log data have a new format? Sometimes the logs don't represent a problem, but a new format of log data that deviates from the data model you expect your applications to follow. Catching these logs early gives you the opportunity to ask developers to correct any deviations in their log output. The more consistent people are in the way log data is generated, the easier it becomes to use logs across a diverse set of teams. Masked attributes and wildcards Parts of the log messages in patterns are classified as variables and are substituted by masked attributes. The masking process supports and improves the clustering phase by allowing the algorithm to ignore changing details and focus on the repetitive structure. Masked attributes include: date_time ip url uuid Masked attributes are highlighted and are easy to identify, as shown in the following example. Here is an example of a log pattern that has masked attributes. Log patterns extract other less trivial variables that don't belong to any masked attribute. These variables are indicated as wildcards *. Here is an example of how wildcards * group variables in the Log patterns UI. Troubleshooting Here are a few reasons why you might have patterns enabled but not see any pattern data. If you're sure none of the items below are true, get help from support.newrelic.com. No data has arrived in the timeframe you're observing. Try expanding the time range you're viewing with the time picker. It's been less than 24 hours since patterns were enabled in the account. This means the ML model may not be generated for the account yet. None of the data coming in has a message field. Patterns will only be generated for values in the message field of a log record. If your logs don't contain message, there will be no data. Put the platform to work with patterns Patterns are a value that is enriched onto the existing log message as a new attribute named newrelic.logPattern. Anything you can do with logs generally can be done with log patterns, such as: Build your own dashboards with patterns, to monitor a specific pattern or group of patterns you care about. Create alerts for patterns by adding NRQL alerts. Use baseline alert conditions to detect anomalies in known log patterns.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 345.17618,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Discover value in <em>log</em> <em>data</em> with patterns",
        "sections": "Discover value in <em>log</em> <em>data</em> with patterns",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " using our original pricing model require an Owner or Admin role. If you see Patterns are turned off in your <em>Log</em> <em>management</em> Patterns <em>UI</em>, click the Configure Patterns button and enable it. If you don&#x27;t see patterns within 30 minutes of enabling the feature, there may be a lack of <em>data</em> with a message"
      },
      "id": "6072d46128ccbc244451c18b"
    },
    {
      "sections": [
        "Parsing log data",
        "Example",
        "How log parsing works",
        "Important",
        "Limits",
        "Tip",
        "Built-in parsing rulesets",
        "List of built-in rulesets",
        "Add the logtype attribute",
        "New Relic infrastructure agent example",
        "Fluentd example",
        "Fluent Bit example",
        "Logstash example",
        "Logs API example",
        "Create custom parsing rules",
        "Troubleshooting"
      ],
      "title": "Parsing log data",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "52955adb68242c4ca582ba9cb8e22963955a8275",
      "image": "https://docs.newrelic.com/static/dc392bb7142d2fdb253a649daf4ebe6d/c1b63/log-parsing-rule-ui.png",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/parsing/",
      "published_at": "2021-10-19T03:43:56Z",
      "updated_at": "2021-10-19T03:43:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Parsing is the process of splitting unstructured log data into attributes (key/value pairs). You can use these attributes to facet or filter logs in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial on YouTube (approx. 4-1/2 minutes). New Relic parses log data according to rules. This document describes how logs parsing works, how to use built-in rules, and how to create custom rules. You can also create, query, and manage your log parsing rules by using NerdGraph, our GraphQL API, at api.newrelic.com/graphiql. For more information, see our NerdGraph tutorial for parsing. Example A good example is a default NGINX access log containing unstructured text. It is useful for searching but not much else. Here's an example of a typical line: 127.180.71.3 - - [10/May/1997:08:05:32 +0000] \"GET /downloads/product_1 HTTP/1.1\" 304 0 \"-\" \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" Copy In an unparsed format, you would need to do a full text search to answer most questions. After parsing, the log is organized into attributes, like response code and request URL: { \"remote_addr\":\"93.180.71.3\", \"time\":\"1586514731\", \"method\":\"GET\", \"path\":\"/downloads/product_1\", \"version\":\"HTTP/1.1\", \"response\":\"304\", \"bytesSent\": 0, \"user_agent\": \"Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)\" } Copy Parsing makes it easier to create custom queries that facet on those values. This helps you understand the distribution of response codes per request URL and quickly find problematic pages. How log parsing works Here's an overview of how New Relic implements parsing of logs: Log parsing How it works What All parsing takes place against the message field; no other fields can be parsed. Each parsing rule is created with matching criteria that determines which logs the rule will attempt to parse. To simplify the matching process, we recommend adding a logtype attribute to your logs. However, you are not limited to using logtype; any attribute can be used as matching criteria. When Parsing will only be applied once to each log message. If multiple parsing rules match the log, only the first that succeeds will be applied. Parsing takes place during log ingestion, before data is written to NRDB. Once data has been written to storage, it can no longer be parsed. Parsing occurs in the pipeline before data enrichments take place. Be careful when defining the matching criteria for a parsing rule. If the criteria is based on an attribute that doesn't exist untail after parsing or enrichment take place, that data won't be present in the logs when matching occurs. As a result, no parsing will happen. How Rules can be written in Grok, regex, or a mixture of the two. Grok is a collection of patterns that abstract away complicated regular expressions. If the content of the message field is JSON, it will be parsed automatically. New Relic's log ingestion pipeline can parse data by matching a log event to a rule that describes how the log should be parsed. There are two ways log events can be parsed: Use a built-in rule. Define a custom rule. Rules are a combination of matching logic and parsing logic. Matching is done by defining a query match on an attribute of the logs. Rules are not applied retroactively. Logs collected before a rule is created are not parsed by that rule. The simplest way to organize your logs and how they are parsed is to include the logtype field in your log event. This tells New Relic what built-in ruleset to apply to the logs. Important Once a parsing rule is active, data parsed by the rule is permanently changed. This cannot be reverted. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. Limits Parsing is computationally expensive, which introduces risk. Parsing is done for custom rules defined in an account and for matching patterns to a log. A large number of patterns or poorly defined custom rules will consume a huge amount of memory and CPU resources while also taking a very long time to complete. In order to prevent problems, we apply two parsing limits: per-message-per-rule and per-account. Limit Description Per-message-per-rule The per-message-per-rule limit prevents the time spent parsing any single message from being greater than 100 ms. If that limit is reached, the system will cease attempting to parse the log message with that rule. The ingestion pipeline will attempt to run any other applicable on that message, and the message will still be passed through the ingestion pipeline and stored in NRDB. The log message will be in its original, unparsed format. Per-account The per-account limit exists to prevent accounts from using more than their fair share of resources. The limit considers the total time spent processing all log messages for an account per-minute. The limit is not a fixed value; it scales up or down proportionally to the volume of data stored daily by the account and the environment size that is subsequently allocated to support that customer. Tip To easily check if your rate limits have been reached, go to your system Limits page in the New Relic UI. Built-in parsing rulesets Common log formats have well-established parsing rules already created for them. To get the benefit of built-in parsing rules, add the logtype attribute when forwarding logs. Set the value to something listed in the following table, and the rules for that type of log will be applied automatically. List of built-in rulesets The following logtype attribute values map to a predefined parsing rule. For example, to query the Application Load Balancer: From the New Relic UI, use the format logtype: alb. From NerdGraph, use the format logtype = 'alb'. To learn what fields are parsed for each rule, see our documentation about built-in parsing rules. logtype Example matching query alb AWS Application Load Balancer logtype:alb apache Apache Access logtype:apache cloudfront-web CloudFront Web logtype:cloudfront-web elb Amazon Elastic Load Balancer logtype:elb iis_w3c Microsoft IIS server logs - W3C format logtype:iis_w3c monit Monit logs logtype:monit mysql-error MySQL Error logtype:mysql-error nginx NGINX access logs logtype:nginx nginx-error NGINX error logs logtype:nginx-error route-53 Amazon Route 53 logs logtype:route-53 syslog-rfc5424 Syslog logtype:syslog-rfc5424 Add the logtype attribute When aggregating logs, it's important to provide metadata that makes it easy to organize, search, and parse those logs. One simple way of doing this is to add the attribute logtype to the log messages when they are shipped. Built-in parsing rules are applied by default to certain logtype values. Here are some examples of how to add logtype to logs sent by some of our supported shipping methods. New Relic infrastructure agent example Add logtype as an attribute. You must set the logtype for each named source. Learn more about using the infrastructure agent to add attributes. logs: - name: file-simple file: /path/to/file attributes: logtype: fileRaw - name: nginx-example file: /var/log/nginx.log attributes: logtype: nginx Copy Fluentd example Add a filter block to the .conf file, which uses a record_transformer to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluentd examples. <filter containers> @type record_transformer enable_ruby true <record> #Add logtype to trigger a built-in parsing rule for nginx access logs logtype nginx #Set timestamp from the value contained in the field \"time\" timestamp record[\"time\"] #Add hostname and tag fields to all records hostname \"#{Socket.gethostname}\" tag ${tag} </record> </filter> Copy Fluent Bit example Add a filter block to the .conf file that uses a record_modifier to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Fluent Bit examples. [FILTER] Name record_modifier Match * Record logtype nginx Record hostname ${HOSTNAME} Record service_name Sample-App-Name Copy Logstash example Add a filter block to the Logstash configuration which uses an add_field mutate filter to add a new field. In this example we use a logtype of nginx to trigger the build-in NGINX parsing rule. Check out other Logstash examples. filter { mutate { add_field => { \"logtype\" => \"nginx\" \"service_name\" => \"myservicename\" \"hostname\" => \"%{host}\" } } } Copy Logs API example You can add attributes to the JSON request sent to New Relic. In this example we add a logtype attribute of value nginx to trigger the built-in NGINX parsing rule. Learn more about using the Logs API. POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: YOUR_LICENSE_KEY Accept: */* Content-Length: 133 { \"timestamp\": TIMESTAMP_IN_UNIX_EPOCH, \"message\": \"User 'xyz' logged in\", \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } Copy Create custom parsing rules Many logs are formatted or structured in a unique way. In order to parse them, custom logic must be built and applied. From the left nav in the Logs UI, select Parsing, then create your own custom parsing rule with an attribute, value, and Grok pattern. To create and manage your own, custom parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing, then click Create parsing rule. Enter the parsing rule's name. Choose an attribute and value to match on. Write your Grok pattern and test the rule. To learn about Grok and custom parsing rules, read our blog post about how to parse logs with Grok patterns. Enable and save the custom parsing rule. To view the list of custom parsing rules: From Manage Data on the left nav of the Logs UI, click Parsing. To view existing parsing rules: Go to one.newrelic.com > Logs. From Manage Data on the left nav of the Logs UI, click Parsing. Troubleshooting If parsing is not working the way you intended, it may be due to: Logic: The parsing rule matching logic does not match the logs you want. Timing: If your parsing matching rule targets a value that doesn't exist yet, it will fail. This can occur if the value is added later in the pipeline as part of the enrichment process. Limits: There is a fixed amount of time available every minute to process logs via parsing, patterns, drop filters, etc. If the maximum amount of time has been spent, parsing will be skipped for additional log event records. To resolve these problems, create or adjust your custom parsing rules.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 344.99872,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Parsing <em>log</em> <em>data</em>",
        "sections": "Parsing <em>log</em> <em>data</em>",
        "tags": "<em>Log</em> <em>management</em>",
        "body": "Parsing is the process of splitting unstructured <em>log</em> <em>data</em> into attributes (key&#x2F;value pairs). You can use these attributes to facet or filter <em>logs</em> in useful ways. This in turn helps you build better charts and alerts. To get started with parsing, you may want to watch the following video tutorial"
      },
      "id": "603e7eb4196a67b0c4a83dd1"
    },
    {
      "sections": [
        "Built-in log parsing rulesets",
        "Apache",
        "Application Load Balancer",
        "Cloudfront",
        "Elastic Load Balancer",
        "Microsoft IIS",
        "Monit",
        "MySQL Error",
        "NGINX",
        "NGINX Error",
        "Route 53",
        "Syslog RFC-5424"
      ],
      "title": "Built-in log parsing rulesets",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "UI and data"
      ],
      "external_id": "cb5909f2453d475a85d408d75cd3b2a321a8518e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/log-management/ui-data/built-log-parsing-rulesets/",
      "published_at": "2021-10-19T03:43:56Z",
      "updated_at": "2021-10-19T03:43:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic can parse common log formats according to built-in rulesets, so that you don't have to create your own parsing rules. Here are the log parsing rulesets, their Grok patterns, and what fields are parsed. To enable built-in log parsing, see our documentation for adding the logtype attribute. To manage your parsing rulesets programmatically, you can use NerdGraph, our graphQL-format API, at api.newrelic.com/graphiql. For more information, see the NerdGraph tutorial to create, query, and delete your parsing rulesets. Apache Source: logtype = 'apache' Grok: %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{NUMBER:bytes}|-) %{QS:referrer} %{QS:agent} Copy Results: clientip: The IP address of the client. verb: The HTTP verb ident: The user identity of the client making the request response: The HTTP status code of the response request: The URI and request being made httpversion: The HTTP version of the request rawrequest: The raw HTTP request if data is posted bytes: The number of bytes sent referrer: The HTTP referrer agent: The client's user agent Application Load Balancer Source: logtype = 'alb' Grok: ^%{NOTSPACE:type} %{TIMESTAMP_ISO8601:time} %{NOTSPACE:elb} %{NOTSPACE:client_ip}:%{NOTSPACE:client_port} ((%{NOTSPACE:target_ip}:%{NOTSPACE:target_port})|-) %{NOTSPACE:request_processing_time} %{NOTSPACE:target_processing_time} %{NOTSPACE:response_processing_time} %{NOTSPACE:elb_status_code} %{NOTSPACE:target_status_code} %{NOTSPACE:received_bytes} %{NOTSPACE:sent_bytes} \"%{DATA:request}\" \"%{DATA:user_agent}\" %{NOTSPACE:ssl_cipher} %{NOTSPACE:ssl_protocol} %{NOTSPACE:target_group_arn} \"%{DATA:trace_id}\" \"%{NOTSPACE:domain_name}\" \"%{NOTSPACE:chosen_cert_arn}\" %{NOTSPACE:matched_rule_priority} %{TIMESTAMP_ISO8601:request_creation_time} \"%{NOTSPACE:actions_executed}\" \"%{NOTSPACE:redirect_url}\" \"%{NOTSPACE:error_reason}\" (?:\"|)%{DATA:target_port_list}(?:\"|) (?:\"|)%{DATA:target_status_code_list}(?:\"|) \"%{NOTSPACE:classification}\" \"%{NOTSPACE:classification_reason}\" Copy Results: Field Definition type The type of request or connection. Possible values are: http: HTTP https: HTTP over SSL/TLS h2: HTTP/2 over SSL/TLS ws: WebSockets wss: WebSockets over SSL/TLS elb The resource ID of the load balancer. If you are parsing access log entries, note that resources IDs can contain forward slashes (/). client The IP address and port of the requesting client target The IP address and port of the target that processed this request. If the client didn't send a full request, the load balancer can't dispatch the request to a target, and this value is set to -. If the target is a Lambda function, this value is set to -. If the request is blocked by AWS WAF, this value is set to -, and the value of elb_status_code is set to 403. request_processing_time The total time elapsed (in seconds, with millisecond precision) from the time the load balancer received the request until the time it sent it to a target. This value is set to -1 if the load balancer can't dispatch the request to a target. This can happen if the target closes the connection before the idle timeout or if the client sends a malformed request. This value can also be set to -1 if the registered target does not respond before the idle timeout. target_processing_time The total time elapsed (in seconds, with millisecond precision) from the time the load balancer sent the request to a target until the target started to send the response headers. This value is set to -1 if the load balancer can't dispatch the request to a target. This can happen if the target closes the connection before the idle timeout or if the client sends a malformed request. This value can also be set to -1 if the registered target does not respond before the idle timeout. response_processing_time The total time elapsed (in seconds, with millisecond precision) from the time the load balancer received the response header from the target until it started to send the response to the client. This includes both the queuing time at the load balancer and the connection acquisition time from the load balancer to the client. This value is set to -1 if the load balancer can't send the request to a target. This can happen if the target closes the connection before the idle timeout or if the client sends a malformed request. elb_status_code The status code of the response from the load balancer target_status_code The status code of the response from the target. This value is recorded only if a connection was established to the target and the target sent a response. Otherwise, it is set to -. received_bytes The size of the request, in bytes, received from the client (requester). For HTTP requests, this includes the headers. For WebSockets, this is the total number of bytes received from the client on the connection. sent_bytes The size of the response, in bytes, sent to the client (requester). For HTTP requests, this includes the headers. For WebSockets, this is the total number of bytes sent to the client on the connection. method The HTTP verb of the request uri The URI the request was targeting http_version The HTTP version number of the request user_agent User-Agent string that identifies the client that originated the request, enclosed in double quotes. The string consists of one or more product identifiers, product/version. If the string is longer than 8 KB, it is truncated. ssl_cipher The SSL cipher. This value is set to - if the listener is not an HTTPS listener. ssl_protocol The SSL protocol. This value is set to - if the listener is not an HTTPS listener. target_group_arn The Amazon Resource Name (ARN) of the target group trace_id The contents of the X-Amzn-Trace-Id header, enclosed in double quotes domain_name The SNI domain provided by the client during the TLS handshake, enclosed in double quotes. This value is set to - if the client doesn't support SNI or the domain doesn't match a certificate and the default certificate is presented to the client. chosen_cert_arn The ARN of the certificate presented to the client, enclosed in double quotes. Set to session-reused if the session is reused. Set to - if the listener is not an HTTPS listener. matched_rule_priority The priority value of the rule that matched the request. If a rule matched, this is a value from 1 to 50000. If no rule matched and the default action was taken, this value is set to 0. If an error occurs during rules evaluation, it is set to -1. For any other error, it is set to -. request_creation_time The time when the load balancer received the request from the client, in ISO 8601 format. actions_executed The actions taken when processing the request, enclosed in double quotes. This value is a comma-separated list that can include the values described in actions_taken. If no action was taken, such as for a malformed request, this value is set to -. redirect_url The URL of the redirect target for the location header of the HTTP response, enclosed in double quotes. If no redirect actions were taken, this value is set to -. error_reason The error reason code, enclosed in double quotes. If the request failed, this is one of the error codes described in Error Reason Codes. If the actions taken do not include an authenticate action or the target is not a Lambda function, this value is set to -. Cloudfront Source: logtype = 'cloudfront-web' Grok: ^%{NOTSPACE:date}%{SPACE}%{NOTSPACE:time}%{SPACE}%{NOTSPACE:x_edge_location}%{SPACE}%{NOTSPACE:sc_bytes}%{SPACE}%{NOTSPACE:c_ip}%{SPACE}%{NOTSPACE:cs_method}%{SPACE}%{NOTSPACE:cs_host}%{SPACE}%{NOTSPACE:cs_uri_stem}%{SPACE}%{NOTSPACE:sc_status}%{SPACE}%{NOTSPACE:cs_referer}%{SPACE}%{NOTSPACE:cs_user_agent}%{SPACE}%{NOTSPACE:cs_uri_query}%{SPACE}%{NOTSPACE:cs_Cookie}%{SPACE}%{NOTSPACE:x_edge_result_type}%{SPACE}%{NOTSPACE:x_edge_request_id}%{SPACE}%{NOTSPACE:x_host_header}%{SPACE}%{NOTSPACE:cs_protocol}%{SPACE}%{NOTSPACE:cs_bytes}%{SPACE}%{NOTSPACE:time_taken}%{SPACE}%{NOTSPACE:x_forwarded_for}%{SPACE}%{NOTSPACE:ssl_protocol}%{SPACE}%{NOTSPACE:ssl_cipher}%{SPACE}%{NOTSPACE:x_edge_response_result_type}%{SPACE}%{NOTSPACE:cs_protocol_version}%{SPACE}%{NOTSPACE:fle_status}%{SPACE}%{NOTSPACE:fle_encrypted_fields}%{SPACE}%{NOTSPACE:c_port}%{SPACE}%{NOTSPACE:time_to_first_byte}%{SPACE}%{NOTSPACE:x_edge_detailed_result_type}%{SPACE}%{NOTSPACE:sc_content_type}%{SPACE}%{NOTSPACE:sc_content_len}%{SPACE}%{NOTSPACE:sc_range_start}%{SPACE}%{NOTSPACE:sc_range_end} Copy Results: Field Definition x_edge_location The edge location that served the request. Each edge location is identified by a three-letter code and an arbitrarily assigned number; for example, DFW3. The three-letter code typically corresponds with the International Air Transport Association airport code for an airport near the edge location. (These abbreviations might change in the future.) sc_bytes The total number of bytes that CloudFront served to the viewer in response to the request, including headers; for example, 1045619. For WebSocket connections, this is the total number of bytes sent from the server to the client through the connection. c_ip The IP address of the viewer that made the request, either in IPv4 or IPv6 format. If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip is the IP address of the proxy or load balancer. See also X-Forwarded-For. cs_method The HTTP request method: DELETE, GET, HEAD, OPTIONS, PATCH, POST, or PUT. cs_host The domain name of the CloudFront distribution; for example, d111111abcdef8.cloudfront.net. cs_uri_stem The portion of the URI that identifies the path and object; for example, /images/cat.jpg. Question marks in URLs and query strings are not included. sc_status An HTTP status code; for example, 200. Status code 000 indicates the viewer closed the connection (for example, closed the browser tab) before CloudFront could respond to a request. If the viewer closes the connection after CloudFront starts to send the response, the log contains the applicable HTTP status code. cs_referer The name of the domain that originated the request. Common referrers include search engines, other websites that link directly to your objects, and your own website. cs_user_agent The value of the User-Agent header in the request. The User-Agent header identifies the source of the request, such as the type of device and browser that submitted the request, and which search engine if applicable. cs_uri_query The query string portion of the URI, if any. When a URI doesn't contain a query string, this field's value is a hyphen (-). cs_cookie The cookie header in the request, including name-value pairs and the associated attributes. If you enable cookie logging, CloudFront logs the cookies in all requests regardless of which cookies you choose to forward to the origin. If a request doesn't include a cookie header, this field's value is a hyphen (-). x_edge_result_type How CloudFront classifies the response after the last byte left the edge location. In some cases, the result type can change between the time that CloudFront is ready to send the response and the time that CloudFront has finished sending the response. x_edge_request_id An encrypted string that uniquely identifies a request. In the response header, this is x-amz-cf-id. x_host_header The value that the viewer included in the Host header for this request. This is the domain name in the request. If you're using the CloudFront domain name in your object URLs, this field contains that domain name. If you're using alternate domain names in your object URLs, such as http://example.com/logo.png, this field contains the alternate domain name, such as example.com. To use alternate domain names, you must add them to your distribution. cs_protocol The protocol that the viewer specified in the request: http, https, ws, or wss. cs_bytes The number of bytes of data that the viewer included in the request, including headers. For WebSocket connections, this is the total number of bytes sent from the client to the server on the connection. time_taken The number of seconds (to the thousandth of a second; for example, 0.002) between the time that a CloudFront edge server receives a viewer's request and the time that CloudFront writes the last byte of the response to the edge server's output queue as measured on the server. From the perspective of the viewer, the total time to get the full object will be longer than this value due to network latency and TCP buffering. x_forwarded_for If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip in field 5 is the IP address of the proxy or load balancer. In that case, this field is the IP address of the viewer that originated the request. This field contains IPv4 and IPv6 addresses as applicable. If the viewer did not use an HTTP proxy or a load balancer, the value of x_forwarded_for is a hyphen (-). ssl_protocol When cs_protocol in field 17 is https, this field contains the SSL/TLS protocol that the client and CloudFront negotiated for transmitting the request and response. Possible values include: SSLv3 TLSv1 TLSv1.1 TLSv1.2 When cs_protocol in field 17 is http, the value for this field is a hyphen (-). ssl_cipher When cs_protocol in field 17 is https, this field contains the SSL/TLS cipher that the client and CloudFront negotiated for encrypting the request and response. Possible values include: ECDHE-RSA-AES128-GCM-SHA256 ECDHE-RSA-AES128-SHA256 ECDHE-RSA-AES128-SHA ECDHE-RSA-AES256-GCM-SHA384 ECDHE-RSA-AES256-SHA384 ECDHE-RSA-AES256-SHA AES128-GCM-SHA256 AES256-GCM-SHA384 AES128-SHA256 AES256-SHA AES128-SHA DES-CBC3-SHA RC4-MD5 When cs_protocol in field 17 is http, the value for this field is a hyphen (-). x_edge_response_result_type How CloudFront classified the response just before returning the response to the viewer. Possible values include: Hit: CloudFront served the object to the viewer from the edge cache. RefreshHit: CloudFront found the object in the edge cache but it had expired, so CloudFront contacted the origin to verify that the cache has the latest version of the object. Miss: The request could not be satisfied by an object in the edge cache, so CloudFront forwarded the request to the origin server and returned the result to the viewer. LimitExceeded: The request was denied because a CloudFront limit was exceeded. CapacityExceeded: CloudFront returned a 503 error because the edge location didn't have enough capacity at the time of the request to serve the object. Error: Typically this means the request resulted in a client error (sc_status is 4xx) or a server error (sc_status is 5xx). If the value of x_edge_result_type is Error and the value of this field is not Error, the client disconnected before finishing the download. Redirect: CloudFront redirects from HTTP to HTTPS. If sc_status is 403 and you configured CloudFront to restrict the geographic distribution of your content, the request might have come from a restricted location. cs_protocol_version The HTTP version that the viewer specified in the request. Possible values include: HTTP/0.9 HTTP/1.0 HTTP/1.1 HTTP/2.0 fle_status When field-level encryption is configured for a distribution, this field contains a code that indicates whether the request body was successfully processed. If field-level encryption is not configured for the distribution, the value is a hyphen (-). fle-encrypted-fields The number of fields that CloudFront encrypted and forwarded to the origin. CloudFront streams the processed request to the origin as it encrypts data, so fle-encrypted-fields can have a value even if the value of fle-status is an error. If field-level encryption is not configured for the distribution, the value of fle-encrypted-fields is a hyphen (-). c_port The port number of the request from the viewer. time_to_first_byte The number of seconds between receiving the request and writing the first byte of the response, as measured on the server. x_edge_detailed_result_type When x-edge-result-type is not Error, this field contains the same value as x-edge-result-type. When x-edge-result-type is Error, this field contains the specific type of error. sc_content_type The value of the HTTP Content-Type header of the response. sc_content_len The value of the HTTP Content-Length header of the response. sc_range_start When the response contains the HTTP Content-Range header, this field contains the range start value. sc-range-end When the response contains the HTTP Content-Range header, this field contains the range end value. Elastic Load Balancer Source: logtype = 'elb' Grok: ^%{TIMESTAMP_ISO8601:time} %{NOTSPACE:elb} %{NOTSPACE:client_ip}:%{NOTSPACE:client_port} ((%{NOTSPACE:backend_ip}:%{NOTSPACE:backend_port})|-) %{NOTSPACE:request_processing_time} %{NOTSPACE:backend_processing_time} %{NOTSPACE:response_processing_time} %{NOTSPACE:elb_status_code} %{NOTSPACE:backend_status_code} %{NOTSPACE:received_bytes} %{NOTSPACE:sent_bytes} \"%{DATA:request}\" \"%{DATA:user_agent}\" %{NOTSPACE:ssl_cipher} %{NOTSPACE:ssl_protocol} Copy Results: Field Definition x_edge_location The edge location that served the request. Each edge location is identified by a three-letter code and an arbitrarily assigned number; for example, DFW3. The three-letter code typically corresponds with the International Air Transport Association airport code for an airport near the edge location. (These abbreviations might change in the future.) sc_bytes The total number of bytes that CloudFront served to the viewer in response to the request, including headers; for example, 1045619. For WebSocket connections, this is the total number of bytes sent from the server to the client through the connection. c_ip The IP address of the viewer that made the request. If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip is the IP address of the proxy or load balancer. cs_method The HTTP request method: DELETE, GET, HEAD, OPTIONS, PATCH, POST, or PUT. cs_host The domain name of the CloudFront distribution; for example, d111111abcdef8.cloudfront.net. cs_uri_stem The portion of the URI that identifies the path and object; for example, /images/cat.jpg. Question marks (?) in URLs and query strings are not included in the log. sc_status An HTTP status code (for example, 200). Status code 000 indicates that the viewer closed the connection (for example, closed the browser tab) before CloudFront could respond to a request. If the viewer closes the connection after CloudFront starts to send the response, the log contains the applicable HTTP status code. cs_referer he name of the domain that originated the request. Common referrers include search engines, other websites that link directly to your objects, and your own website. cs_user_agent The value of the User-Agent header in the request. The User-Agent header identifies the source of the request, such as the type of device and browser that submitted the request and which search engine if applicable. cs_uri_query The query string portion of the URI, if any. When a URI doesn't contain a query string, this field's value is a hyphen (-). cs_cookie The cookie header in the request, including name-value pairs and the associated attributes. If you enable cookie logging, CloudFront logs the cookies in all requests, regardless of which cookies you choose to forward to the origin. If a request doesn't include a cookie header, this field's value is a hyphen (-). x_edge_result_type How CloudFront classifies the response after the last byte left the edge location. In some cases, the result type can change between the time that CloudFront is ready to send the response and the time that CloudFront has finished sending the response. x_edge_request_id An encrypted string that uniquely identifies a request. In the response header, this is x-amz-cf-id. x_host_header The value that the viewer included in the Host header for this request. This is the domain name in the request. If you're using the CloudFront domain name in your object URLs, this field contains that domain name. If you're using alternate domain names in your object URLs, such as [http://example.com/logo.png, this field contains the alternate domain name, such as example.com. To use alternate domain names, you must add them to your distribution. cs_protocol The protocol that the viewer specified in the request: http, https, ws, or wss. cs_bytes The number of bytes of data that the viewer included in the request, including headers. For WebSocket connections, this is the total number of bytes sent from the client to the server on the connection. time_taken The number of seconds (to the thousandth of a second; for example, 0.002) between the time that a CloudFront edge server receives a viewer's request and the time that CloudFront writes the last byte of the response to the edge server's output queue as measured on the server. From the perspective of the viewer, the total time to get the full object will be longer than this value due to network latency and TCP buffering. x_forwarded_for If the viewer used an HTTP proxy or a load balancer to send the request, the value of c_ip in field 5 is the IP address of the proxy or load balancer. In that case, this field is the IP address of the viewer that originated the request. This field contains IPv4 and IPv6 addresses as applicable. If the viewer did not use an HTTP proxy or a load balancer, the value of x_forwarded_for is a hyphen (-). ssl_protocol When cs_protocol in field 17 is https, this field contains the SSL/TLS protocol that the client and CloudFront negotiated for transmitting the request and response. Possible values include: SSLv3 TLSv1 TLSv1.1 TLSv1.2 When cs_protocol in field 17 is http, the value for this field is a hyphen (-). ssl_cipher When cs_protocol in field 17 is https, this field contains the SSL/TLS cipher that the client and CloudFront negotiated for encrypting the request and response. Possible values include: ECDHE-RSA-AES128-GCM-SHA256 ECDHE-RSA-AES128-SHA256 ECDHE-RSA-AES128-SHA ECDHE-RSA-AES256-GCM-SHA384 ECDHE-RSA-AES256-SHA384 ECDHE-RSA-AES256-SHA AES128-GCM-SHA256 AES256-GCM-SHA384 AES128-SHA256 AES256-SHA AES128-SHA DES-CBC3-SHA RC4-MD5 When cs_protocol is http, the value for this field is a hyphen (-). x_edge_response_result_type How CloudFront classified the response just before returning the response to the viewer. Possible values include: Hit: CloudFront served the object to the viewer from the edge cache. RefreshHit: CloudFront found the object in the edge cache but it had expired, so CloudFront contacted the origin to verify that the cache has the latest version of the object. Miss: The request could not be satisfied by an object in the edge cache, so CloudFront forwarded the request to the origin server and returned the result to the viewer. LimitExceeded: The request was denied because a CloudFront limit was exceeded. CapacityExceeded: CloudFront returned a 503 error because the edge location didn't have enough capacity at the time of the request to serve the object. Error: Typically this means the request resulted in a client error (sc_status is 4xx) or a server error (sc_status is 5xx). If the value of x_edge_result_type is Error and the value of this field is not Error, the client disconnected before finishing the download. Redirect: CloudFront redirects from HTTP to HTTPS. If sc_status is 403 and you configured CloudFront to restrict the geographic distribution of your content, the request might have come from a restricted location. cs_protocol_version The HTTP version that the viewer specified in the request. Possible values include: HTTP/0.9 HTTP/1.0 HTTP/1.1 HTTP/2.0 fle_status When field-level encryption is configured for a distribution, this field contains a code that indicates whether the request body was successfully processed. If field-level encryption is not configured for the distribution, the value of this field is a hyphen (-). When CloudFront successfully processes the request body, encrypts values in the specified fields, and forwards the request to the origin, the value of this field is Processed. The value of x_edge_result_type can still indicate a client-side or server-side error in this case. If the request exceeds a field-level encryption limit, fle-status contains one of the following error codes, and CloudFront returns HTTP status code 400 to the viewer. fle-encrypted-fields The number of fields that CloudFront encrypted and forwarded to the origin. CloudFront streams the processed request to the origin as it encrypts data, so fle_encrypted_fields can have a value even if the value of fle_status is an error. If field-level encryption is not configured for the distribution, the value of fle_encrypted_fields is a hyphen (-). c_port The port number of the request from the viewer. time_to_first_byte The number of seconds between receiving the request and writing the first byte of the response, as measured on the server. x_edge_detailed_result_type When x_edge_result_type is not Error, this field contains the same value as x_edge_result_type. When x_edge_result_type is Error, this field contains the specific type of error. sc_content_type The value of the HTTP Content-Type header of the response. sc_content_len The value of the HTTP Content-Length header of the response. sc_range_start When the response contains the HTTP Content-Range header, this field contains the range start value. sc-range-end When the response contains the HTTP Content-Range header, this field contains the range end value. Microsoft IIS Source: logtype = 'iis_w3c' Grok: %{TIMESTAMP_ISO8601:log_timestamp} %{NOTSPACE:server_ip} %{WORD:method} %{NOTSPACE:uri} %{NOTSPACE:uri_query} %{NOTSPACE:server_port} %{NOTSPACE:username} %{NOTSPACE:client_ip} %{NOTSPACE:user_agent} %{NOTSPACE:referer} %{NOTSPACE:status} %{NOTSPACE:substatus} %{NOTSPACE:win32_status} %{NOTSPACE:time_taken} Copy Monit Source: logtype = 'monit' Grok: \\\\[%{NOTSPACE:tz} %{SYSLOGTIMESTAMP:nr_timestamp}\\\\] %{WORD:state}%{SPACE}: %{GREEDYDATA:message} Copy Results: state: The severity of the log line message: The message MySQL Error Source: logtype = 'mysql-error' Grok: \\\\[%{WORD:log_level}\\\\] Copy Results: log_level: The severity of the log line NGINX Source: logtype = 'nginx' Grok: %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{NUMBER:bytes}|-) %{QS:referrer} %{QS:agent} Copy Results: clientip: The IP address of the client verb: The HTTP verb ident: The user identity of the client making the request response: The HTTP status code of the response request: The URI and request being made httpversion: The HTTP version of the request rawrequest: The raw HTTP request if data is posted bytes: The number of bytes sent referrer: The HTTP referrer agent: The client's user agent NGINX Error Source: logtype = 'nginx-error' Grok: ^(?<timestamp>%{YEAR:year}[./-]%{MONTHNUM:month}[./-]%{MONTHDAY:day}[- ]%{TIME:time}) \\\\[%{LOGLEVEL:severity}\\\\] %{POSINT:pid}#%{NUMBER}: %{GREEDYDATA:errormessage}(?:, client: (?<client>%{IP:clientip}|%{HOSTNAME:hostname}))(?:, server: %{IPORHOSTORUNDERSCORE:server})(?:, request: %{QS:request})?(?:, upstream: \\\"%{URI:upstream}\\\")?(?:, host: %{QS:host})?(?:, referrer: \\\"%{URI:referrer}\\\")?$ Copy Results: severity: The severity of the log line pid: The server process ID errormessage: The error message clientip: The IP address of the calling client server: The server IP address request: The full request upstream: The upstream URI host: The server's hostname referrer: The HTTP referrer Route 53 Source: logtype = 'route-53' Grok: %{NUMBER:log_format_version} %{TIMESTAMP_ISO8601} %{WORD:zone_id} %{IPORHOST:query} %{WORD:query_type} %{WORD:response_code} %{WORD:protocol} %{WORD:edge_location} %{IP:resolver_ip} %{GREEDYDATA:edns_client_subnet} Copy Results: log_format_version: A versioned format for the log. zone_id: The ID of the hosted zone that is associated with all the DNS queries in this log. query: The domain or subdomain that was specified in the request. query_type: Either the DNS record type that was specified in the request, or ANY. response_code: The DNS response code that Route 53 returned in response to the DNS query. protocol: The protocol that was used to submit the query, either TCP or UDP. edge_location: The Route 53 edge location that responded to the query. Each edge location is identified by a three-letter code and an arbitrary number; for example, DFW3. The three-letter code typically corresponds with the International Air Transport Association airport code for an airport near the edge location. (These abbreviations might change in the future.) resolver_ip: The IP address of the DNS resolver that submitted the request to Route 53. edns_client_subnet: A partial IP address for the client that the request originated from, if available from the DNS resolver. Syslog RFC-5424 Source: logtype = 'syslog-rfc5424' Grok: <%{NONNEGINT:pri}>%{NONNEGINT:version} +(?:%{TIMESTAMP_ISO8601:log.timestamp}|-) +(?:%{HOSTNAME:hostname}|-) +(?:\\\\-|%{NOTSPACE:app.name}) +(?:\\\\-|%{NOTSPACE:procid}) (?:\\\\-|%{NOTSPACE:msgid}) +(?:\\[%{DATA:structured.data}\\]|-|) +%{GREEDYDATA:message} Copy Results: pri: The priority represents both the message facility and severity. version: Syslog protocol version. log.timestamp: Original timestamp. hostname: The machine that originally sent the Syslog message. app.name: The device or application that originated the message. procid: The process name or process ID associated with a Syslog system. msgid: Identifies the type of message. structured.data: Structured data string value. sd.<var>sd-id</var>.<var>sd-param-name</var>: The structured.data content is also parsed into separate attributes following a predefined naming convention: sd.<var>sd-id</var>.<var>sd-param-name</var>. See the structured data parsing examples, which follow. message: Free-form message that provides information about the event. Structured data parsing examples: The structured data [example one=\"1\" two=\"2\"] would be parsed into two different attributes: sd.example.one: \"1\" sd.example.two: \"2\" Copy If the same structured data block contains duplicate param names, it also appends an index-based suffix on the attribute name. For example, the structured data [example number=\"1\" number=\"2\"] would be parsed as: sd.example.number.0: \"1\" sd.example.number.1: \"2\" Copy For structured data with enterprise numbers assigned, an extra attribute is also parsed. For example, the structured data [example@123 number=\"1\"] would be parsed as: sd.example.enterprise.number: 123 sd.example.number: \"1\" Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 344.99872,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Built-in <em>log</em> parsing rulesets",
        "sections": "Built-in <em>log</em> parsing rulesets",
        "tags": "<em>Log</em> <em>management</em>",
        "body": " the DNS resolver. Syslog RFC-5424 Source: logtype = &#x27;syslog-rfc5424&#x27; Grok: &lt;%{NONNEGINT:pri}&gt;%{NONNEGINT:version} +(?:%{TIMESTAMP_ISO8601:<em>log</em>.timestamp}|-) +(?:%{HOSTNAME:hostname}|-) +(?:\\\\-|%{NOTSPACE:app.name}) +(?:\\\\-|%{NOTSPACE:procid}) (?:\\\\-|%{NOTSPACE:msgid}) +(?:\\[%{<em>DATA:structured.data</em>"
      },
      "id": "603e7b9164441f1b2d4e8872"
    }
  ],
  "/docs/logs/logs-context/annotate-logs-logs-context-using-apm-agent-apis": [
    {
      "sections": [
        "Ruby: Configure logs in context",
        "Set up your Ruby app",
        "Standard Ruby on Rails configuration",
        "Incompatible gems",
        "Rails advanced configuration",
        "Ruby configuration without Rails",
        "Lograge configuration",
        "Other logging extensions",
        "Troubleshooting",
        "What's next?"
      ],
      "title": "Ruby: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Logs in context for Ruby"
      ],
      "external_id": "d2100c0d6ddffe6b52a1f968f61f777a1e56a13c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/configure-logs-context-ruby/",
      "published_at": "2021-10-19T03:54:54Z",
      "updated_at": "2021-10-19T03:54:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Ruby agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Ruby app To enable logs in context for APM apps monitored by Ruby: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Ruby agent version, and enable distributed tracing. Use Ruby agent version 6.7.0 or higher for logs in context. For Rails applications, use a supported Rails version. Configure logs in context for Ruby. Standard Ruby on Rails configuration Rails logging is controlled by two components: A logger you can customize by setting config.logger A log formatter you can customize by setting config.log_formatter) In most cases, you should configure logs in context by setting config.log_formatter to the DecoratingFormatter in your Rails application. For more information about Rails configuration, see the rubyonrails.org documentation. In your application's config, require newrelic_rpm, then add the following line: module ________ class Application < Rails::Application ... config.log_formatter = ::NewRelic::Agent::Logging::DecoratingFormatter.new end end Copy This configuration uses the New Relic formatter for log messages, but the remaining configuration is provided by the other Rails settings. Incompatible gems New Relic's decorating logger is known to be incompatible with the following gems: logging semantic logger rails_stdout_logger rails_12factor Rails advanced configuration If setting the log_formatter option doesn't meet your needs, replace the entire Rails logger with an instance of the New Relic logger. Provide the parameters to the logger's constructor, like this: module ________ class Application < Rails::Application ... config.logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( \"log/application.log\", #etc... ) end end Copy Ruby configuration without Rails For non-Rails applications, use the DecoratingLogger in place of the Ruby standard ::Logger, like this: logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( 'log/application.log', #etc... ) ... logger.info(...) Copy The DecoratingLogger is a drop-in replacement for the Ruby standard ::Logger. Their constructors accept the same parameters. Lograge configuration To configure this extension with the lograge gem, follow standard procedures in this doc for Ruby on Rails configuration. No additional configuration is required for the lograge gem. Other logging extensions To use our logging extension with a different logging implementation or with your own custom logger, use the DecoratingFormatter. For example: module ________ class Application < Rails::Application ... config.logger = ::YourCustomLoggerImplementation.new( $stdout, formatter: ::NewRelic::Agent::Logging::DecoratingFormatter.new ) end end Copy To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you have configured your logging in /config/application.rb or in /config/environments/development.rb, run your application locally and check its logging output. You should see some output like this: {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"00fc7d46\",\"timestamp\":1567701375543,\"message\":\"example log message one\",\"log.level\":\"DEBUG\"} {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"6754870b\",\"timestamp\":1567702843604,\"message\":\"example log message two\",\"log.level\":\"DEBUG\"} Copy Troubleshooting If you don't see log data in the UI, follow the troubleshooting procedures. Also, if you see JSON logs in your application's output, but your query does not return logs, check your log forwarder. If the logs from your application are not formatted in JSON with fields like trace.id and span.id, there may be a problem with your log forwarding extension. In this situation: Check that the application is using a supported logging framework. Check that your logging configuration has been applied to all the environments where you want to forward and enrich the log data being sent to New Relic. Check that another logger is not configured later in your application's configuration. Logs in context for Ruby does not support tagged logging. If you are initializing your logger with a log_tags argument, your custom tags may not appear on the final version of your logs. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 285.71222,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Ruby: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "sections": "Ruby: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in <em>context</em> for the Ruby <em>agent</em> connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Ruby app To <em>enable</em> <em>logs</em>"
      },
      "id": "612d460128ccbc17bf56a863"
    },
    {
      "sections": [
        "Python: Configure logs in context",
        "Set up your Python app",
        "Python StreamHandler example",
        "What's next?"
      ],
      "title": "Python: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Logs in context for Python"
      ],
      "external_id": "a51497a54dbdd8c6ee16e8cf097a090347166d1f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/configure-logs-context-python/",
      "published_at": "2021-10-18T05:57:04Z",
      "updated_at": "2021-10-06T21:52:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Python agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Python app To enable logs in context for APM apps monitored by Python: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Python agent version, and enable distributed tracing. Use Python agent version 5.4.0 or higher for logs in context. Configure logs in context for your log handler. Python StreamHandler example Enabling logs in Python is as simple as instantiating a log formatter and adding it to your log handler. This example uses a StreamHandler which by default writes logs to sys.stderr, but any handler can be used. For more information about configuring log handlers, see the Python.org documentation. # Import the logging module and the New Relic log formatter import logging from newrelic.agent import NewRelicContextFormatter # Instantiate a new log handler handler = logging.StreamHandler() # Instantiate the log formatter and add it to the log handler formatter = NewRelicContextFormatter() handler.setFormatter(formatter) # Get the root logger and add the handler to it root_logger = logging.getLogger() root_logger.addHandler(handler) Copy To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you don't see log data in the UI, follow the troubleshooting procedures. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 196.64758,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Python: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "sections": "Python: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in <em>context</em> for the Python <em>agent</em> connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Python app To <em>enable</em> <em>logs</em>"
      },
      "id": "612d463564441f2c8042434e"
    },
    {
      "sections": [
        "Java: Configure logs in context",
        "Set up your Java app",
        "Dropwizard 1.3 or higher",
        "java.util.logging",
        "java.util.logging classpath additions",
        "Log4j 1.x",
        "Log4j 2.x",
        "Logback version 1.2.0 or higher",
        "Spring and Springboot",
        "View logs in UI",
        "What's next?"
      ],
      "title": "Java: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Configure logs in context"
      ],
      "external_id": "b9fbeafa564247287e5d6466630d5bdcdb85affb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/java-configure-logs-context-all/",
      "published_at": "2021-10-18T11:15:14Z",
      "updated_at": "2021-10-06T21:51:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Java agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Java app To enable logs in context for APM apps monitored by Java: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Java agent version. Use Java agent version 5.6.0 or higher for logs in context. Enable the JVM argument -javaagent, and enable distributed tracing. Configure logs in context for Java to enrich your log data, using any of the following extensions as applicable. If you use Spring or Spring Boot and aren't sure which extension you need, see our Spring documentation. Dropwizard 1.3 or higher We offer a Dropwizard extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with the DropWizard extension: Make sure you have the Dropwizard 1.3 or higher package installed and working on your application. Use the original Dropwizard appenders and logging factory installed and working on the application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Dropwizard 1.3 extension as applicable: Gradle: Add the following to your build.gradle file: dependencies { compile(\"com.newrelic.logging:dropwizard:2.0\") } Copy Maven: Add the following to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>dropwizard</artifactId> <version>2.0</version> </dependency> </dependencies> Copy Update your Dropwizard .yaml configuration file with a newrelic-json layout, replacing the currently used type: console or type: file with either type: newrelic-console or type: newrelic-file as appropriate. For example: logging: appenders: - type: newrelic-console # Add the two lines below if you don't have a layout specified on the appender. # If you have a layout, remove all parameters to the layout and set the type. layout: type: newrelic-json Copy The New Relic Dropwizard extension also supports a log-format layout type that uses the standard Dropwizard logging. For testing purposes, you can change the type of the layout with a one-line change: logging: appenders: - type: newrelic-file # This format will be ignored by the newrelic-json layout, but used by the log-format layout. logFormat: \"%date{ISO8601} %c %-5p: %m trace.id=%mdc{trace.id} span.id=%mdc{span.id}%n\" layout: # type: newrelic-json type: log-format Copy java.util.logging We offer a java.util.logging extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with with the java.util.logging extension: Make sure you have the java.util.logging package installed and working on the application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the java.util.logging extension as applicable. If you can't edit these files, you can instead add the jars directly to the application classpath. Gradle: Add the following to your build.gradle file: dependencies { compile(\"com.newrelic.logging:jul:2.0\") } Copy Maven: Add the following to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>jul</artifactId> <version>2.0</version> </dependency> </dependencies> Copy Check if your logging file's handlers property is set to something other than NewRelicMemoryHandler. Look for a line listing the root logger's handlers, like this: handlers = java.util.logging.FileHandler Copy Update your logging properties file to set the root logger's handler to NewRelicMemoryHandler so it intercepts messages destined for another handler: handlers = com.newrelic.logging.jul.NewRelicMemoryHandler Copy Configure the NewRelicMemoryHandler by setting the target to the handler that was previously assigned to the root logger, so it captures data New Relic needs on the thread the log message is coming from: com.newrelic.logging.jul.NewRelicMemoryHandler.target = java.util.logging.FileHandler Copy Use a NewRelicFormatter for the final handler. Update your logging properties file to set the formatter property like the following example. Make sure the handler where you set the formatter is the target handler from the previous step (java.util.logging.FileHandler in this example). java.util.logging.FileHandler.formatter = com.newrelic.logging.jul. NewRelicFormatter Copy The New Relic log format is JSON with telemetry metadata we use to correlate transactions and logs together. Currently we do not support any customization of that format. Once complete, JSON is logged instead of text. The JSON should be formatted as single objects, one per line, and should contain fields like log.level and thread.name. The trace.id, which is required for logs in context, should only have a value for log messages that occur within a transaction. java.util.logging classpath additions The most direct way to get the logs-in-context extensions is to add these dependencies to Maven's pom.xml or Gradle's build.gradle. This allows the packaging tools to pick up the correct dependencies. If you can't edit these files, you can instead add the jars directly to the application classpath for your logging framework's configuration. Before you modify the classpath: Enable the JVM argument -javaagent on your app's Java agent. Verify which logging framework the application is using. Make sure you are able to change your logging framework's configuration. Add the following three jars to the classpath if they aren't already present. Generally, we recommend taking the latest versions published on Maven Central. Group ID com.newrelic.logging and Artifact ID: Select the artifact named after your application's logging framework in Maven. Group ID com.fasterxml.jackson.core and Artifact ID: Use jackson-core. Group ID com.newrelic.agent.java and Artifact ID: Use newrelic-api. Log4j 1.x We offer a Log4j 1.x extension extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with with the Log4j 1.x extension, you must configure the Log4j extension in code or via XML. Properties files are not supported because AsyncAppender instances can only be automatically configured via XML. Make sure you have the Log4j 1.x package installed and working on the application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Log4j 1.x extension as applicable. Gradle: Add the following to your build.gradle file: dependencies { compile(\"com.newrelic.logging:log4j1:2.0\") } Copy Maven: Add the following to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>log4j1</artifactId> <version>2.0</version> </dependency> </dependencies> Copy In your logging configuration XML file, update your <appender> element with a NewRelicLayout, adding <layout class=\"com.newrelic.logging.log4j1.NewRelicLayout\"/>: <appender name=\" TypicalFile \" class=\"org.apache.log4j.FileAppender\"> <param name=\"file\" value=\"logs/log4j1-app.log\"/> <param name=\"append\" value=\"false\"/> <layout class=\" com.newrelic.logging.log4j1.NewRelicLayout \"/> <!-- only this line needs to be added --> </appender> Copy Use NewRelicAsyncAppender to wrap any appenders that will target New Relic's log forwarder. For example: <appender name=\" NewRelicFile \" class=\"com.newrelic.logging.log4j1.NewRelicAsyncAppender\"> <appender-ref ref=\" TypicalFile \" /> </appender> Copy Use the async appender on the root logger. For example: <root> <appender-ref ref=\" NewRelicFile \" /> </root> Copy Example configuration file for the Log4j 1.x extension: <?xml version=\"1.0\" encoding=\"UTF-8\" ?> <!DOCTYPE log4j:configuration SYSTEM \"log4j.dtd\"> <log4j:configuration debug=\"false\"> <appender name=\"TypicalFile\" class=\"org.apache.log4j.FileAppender\"> <param name=\"file\" value=\"logs/log4j1-app.log\"/> <param name=\"append\" value=\"false\"/> <!-- layout has been replaced --> <layout class=\"com.newrelic.logging.log4j1.NewRelicLayout\"/> </appender> <!-- this appender was added --> <appender name=\"NewRelicFile\" class=\"com.newrelic.logging.log4j1.NewRelicAsyncAppender\"> <appender-ref ref=\"TypicalFile\" /> </appender> <appender name=\"TypicalConsole\" class=\"org.apache.log4j.ConsoleAppender\"> <layout class=\"org.apache.log4j.PatternLayout\"> <param name=\"ConversionPattern\" value=\"%-5p %c{1} - %m%n\"/> </layout> </appender> <root> ​ <!-- the new appender was used here -->​​ <appender-ref ref=\"NewRelicFile\" /> <appender-ref ref=\"TypicalConsole\" /> </root> </log4j:configuration> Copy Log4j 2.x We offer a Log4j 2.x extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with with the Log4j 2.x extension: Make sure you have the Log4j 2.x or Logs4j 2 binding package installed and working on your application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Log4j 2.x extension as applicable: Gradle: Add the highlighted section to your build.gradle file: dependencies { compile(\"com.newrelic.logging:log4j2:2.0\") } Copy Maven: Add the highlighted section to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>log4j2</artifactId> <version>2.0</version> </dependency> </dependencies> Copy In your logging configuration XML file, update your <configuration> element by adding the highlighted section: <Configuration xmlns=\"http://logging.apache.org/log4j/2.0/config\" packages=\"com.newrelic.logging.log4j2\" > Copy If you're using a properties file, add packages=com.newrelic.logging.log4j2. Add <NewRelicLayout/> to use a NewRelicLayout element within one of the appenders. For example: <File name=\"MyFile\" fileName=\"logs/app-log-file.log\"> <NewRelicLayout/> </File> Copy If you're using a properties file, only change the layout.type: appender.console.type = Console appender.console.name = STDOUT appender.console.layout.type = NewRelicLayout Copy If you only modified an existing appender, skip this step. If you added a new appender, add <AppenderRef/> within <Root> to use this appender. Use the ref attribute to refer to appender name you created in the previous step. For example: <Root level=\"info\"> <AppenderRef ref=\"MyFile\"/> </Root> Copy If you're using a properties file and added a new appender, add: rootLogger.level = info rootLogger.appenderRef.stdout.ref = STDOUT ​​​​​ Copy Logback version 1.2.0 or higher We offer a Logback extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with Logback: Make sure you have Logback version 1.2.0 or higher and the New Relic Java agent version 5.6.0 or higher installed and working on your application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Logback extension as applicable: Gradle: Add the highlighted section to your build.gradle file: dependencies { compile(\"com.newrelic.logging:logback:2.0\") } Copy Maven: Add the highlighted section to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>logback</artifactId> <version>2.0</version> </dependency> </dependencies> Copy Update your logging configuration xml to replace any existing <encoder> element. If you're logging to the console (stdout/stderr), look for ConsoleAppender and replace: <appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> Copy If you're logging to a file, look for FileAppender and replace <encoder>: <appender name=\"LOG_FILE\" class=\"ch.qos.logback.core.FileAppender\"> <file>logs/app-log-file.log</file> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> Copy Update your logging configuration xml with the NewRelicAsyncAppender. To ensure that NewRelicAsyncAppender wraps any appenders that will target New Relic's log forwarder, add the following section. Change \"LOG_FILE\" to the name of the appender you updated in the previous step. <appender name=\"ASYNC\" class=\"com.newrelic.logging.logback.NewRelicAsyncAppender\"> <appender-ref ref=\"LOG_FILE\" /> </appender> Copy Make sure NewRelicAsyncAppender is the first appender used in your logger. Replace your root logger’s appenders with the ASYNC appender created in the previous step. Then list any other appenders after the NewRelicAsyncAppender in the <root> list. <root> <appender-ref ref=\"ASYNC\" /> </root> Copy Here are examples of an updated logging .xml file for the Logback extension. You can also see a working example in GitHub. Single console appender example Example configuration file after adding in the logging extension information: <configuration> <appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"> <!-- changed the encoder --> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> <!-- added the ASYNC appender --> <appender name=\"ASYNC\" class=\"com.newrelic.logging.logback.NewRelicAsyncAppender\"> <appender-ref ref=\"STDOUT\" /> </appender> <root level=\"debug\"> <!-- changed the root logger --> <appender-ref ref=\"ASYNC\" /> </root> </configuration> Copy Two console appenders example This example sends New Relic logging to a file, but still sends standard logging to the console: <configuration> <appender name=\"FILE\" class=\"ch.qos.logback.core.FileAppender\"> <file>myApp.log</file> <!-- encoder changed --> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> <!-- this appender does normal console logging --> <appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder> <pattern>%msg%n</pattern> </encoder> </appender> <!-- The required New Relic ASYNC appender wraps the FILE appender --> <appender name=\"ASYNC\" class=\"com.newrelic.logging.logback.NewRelicAsyncAppender\"> <appender-ref ref=\"FILE\" /> </appender> <root level=\"debug\"> <!-- ASYNC is one of the main appenders --> <appender-ref ref=\"ASYNC\" /> <!-- Send every message to normal console logging, as well. --> <appender-ref ref=\"STDOUT\" /> </root> </configuration> Copy Spring and Springboot We offer extensions for current versions of Spring and Spring Boot. If you already know the logging library, you can skip directly to that documentation: java.util.logging log4j 1 log4j 2 logback The extensions support default configurations only on Spring Boot 2.0 and higher. With Spring Boot: Here are tips to determine which logging library you have: If you have spring-boot-starter-log4j2 in your dependencies, you're using log4j 2.x. Refer to the Spring Boot log4j 2.x documentation for basic configuration, and the New Relic log4j 2 extension for customizing your configuration. If you're using Spring Boot but not the starter-log4j2, you're using logback by default. Refer to Spring Boot logback documentation for basic configuration, and the New Relic logback extension for customizing your configuration. With Spring (but not Spring Boot): Spring 5 or higher: Spring implements a bridge to other logging libraries that will automatically find them. However, those individual libraries must be configured and explicitly included in your project dependencies. To identify your logging dependency, consult your Gradle, Maven, or other build tool's dependency tree. Then follow the procedures to configure logs in context for your Java app with that extension. Spring 4 or lower: Spring version 4 and lower uses Apache Commons Logging for its bridge. Refer to the Spring documentation for information on configuring its bridge. View logs in UI To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you don't see log data in the UI, follow the troubleshooting procedures. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 196.64502,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "sections": "Java: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in <em>context</em> for the Java <em>agent</em> connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Java app To <em>enable</em> <em>logs</em>"
      },
      "id": "612efca3e7b9d2f718b6f223"
    }
  ],
  "/docs/logs/logs-context/c-sdk-configure-logs-context": [
    {
      "sections": [
        "Ruby: Configure logs in context",
        "Set up your Ruby app",
        "Standard Ruby on Rails configuration",
        "Incompatible gems",
        "Rails advanced configuration",
        "Ruby configuration without Rails",
        "Lograge configuration",
        "Other logging extensions",
        "Troubleshooting",
        "What's next?"
      ],
      "title": "Ruby: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Logs in context for Ruby"
      ],
      "external_id": "d2100c0d6ddffe6b52a1f968f61f777a1e56a13c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/configure-logs-context-ruby/",
      "published_at": "2021-10-19T03:54:54Z",
      "updated_at": "2021-10-19T03:54:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Ruby agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Ruby app To enable logs in context for APM apps monitored by Ruby: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Ruby agent version, and enable distributed tracing. Use Ruby agent version 6.7.0 or higher for logs in context. For Rails applications, use a supported Rails version. Configure logs in context for Ruby. Standard Ruby on Rails configuration Rails logging is controlled by two components: A logger you can customize by setting config.logger A log formatter you can customize by setting config.log_formatter) In most cases, you should configure logs in context by setting config.log_formatter to the DecoratingFormatter in your Rails application. For more information about Rails configuration, see the rubyonrails.org documentation. In your application's config, require newrelic_rpm, then add the following line: module ________ class Application < Rails::Application ... config.log_formatter = ::NewRelic::Agent::Logging::DecoratingFormatter.new end end Copy This configuration uses the New Relic formatter for log messages, but the remaining configuration is provided by the other Rails settings. Incompatible gems New Relic's decorating logger is known to be incompatible with the following gems: logging semantic logger rails_stdout_logger rails_12factor Rails advanced configuration If setting the log_formatter option doesn't meet your needs, replace the entire Rails logger with an instance of the New Relic logger. Provide the parameters to the logger's constructor, like this: module ________ class Application < Rails::Application ... config.logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( \"log/application.log\", #etc... ) end end Copy Ruby configuration without Rails For non-Rails applications, use the DecoratingLogger in place of the Ruby standard ::Logger, like this: logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( 'log/application.log', #etc... ) ... logger.info(...) Copy The DecoratingLogger is a drop-in replacement for the Ruby standard ::Logger. Their constructors accept the same parameters. Lograge configuration To configure this extension with the lograge gem, follow standard procedures in this doc for Ruby on Rails configuration. No additional configuration is required for the lograge gem. Other logging extensions To use our logging extension with a different logging implementation or with your own custom logger, use the DecoratingFormatter. For example: module ________ class Application < Rails::Application ... config.logger = ::YourCustomLoggerImplementation.new( $stdout, formatter: ::NewRelic::Agent::Logging::DecoratingFormatter.new ) end end Copy To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you have configured your logging in /config/application.rb or in /config/environments/development.rb, run your application locally and check its logging output. You should see some output like this: {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"00fc7d46\",\"timestamp\":1567701375543,\"message\":\"example log message one\",\"log.level\":\"DEBUG\"} {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"6754870b\",\"timestamp\":1567702843604,\"message\":\"example log message two\",\"log.level\":\"DEBUG\"} Copy Troubleshooting If you don't see log data in the UI, follow the troubleshooting procedures. Also, if you see JSON logs in your application's output, but your query does not return logs, check your log forwarder. If the logs from your application are not formatted in JSON with fields like trace.id and span.id, there may be a problem with your log forwarding extension. In this situation: Check that the application is using a supported logging framework. Check that your logging configuration has been applied to all the environments where you want to forward and enrich the log data being sent to New Relic. Check that another logger is not configured later in your application's configuration. Logs in context for Ruby does not support tagged logging. If you are initializing your logger with a log_tags argument, your custom tags may not appear on the final version of your logs. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 383.26025,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Ruby: <em>Configure</em> <em>logs</em> <em>in</em> <em>context</em>",
        "sections": "Ruby: <em>Configure</em> <em>logs</em> <em>in</em> <em>context</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in <em>context</em> for the Ruby agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Ruby app To <em>enable</em> <em>logs</em>"
      },
      "id": "612d460128ccbc17bf56a863"
    },
    {
      "sections": [
        "Java: Configure logs in context",
        "Set up your Java app",
        "Dropwizard 1.3 or higher",
        "java.util.logging",
        "java.util.logging classpath additions",
        "Log4j 1.x",
        "Log4j 2.x",
        "Logback version 1.2.0 or higher",
        "Spring and Springboot",
        "View logs in UI",
        "What's next?"
      ],
      "title": "Java: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Configure logs in context"
      ],
      "external_id": "b9fbeafa564247287e5d6466630d5bdcdb85affb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/java-configure-logs-context-all/",
      "published_at": "2021-10-18T11:15:14Z",
      "updated_at": "2021-10-06T21:51:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Java agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Java app To enable logs in context for APM apps monitored by Java: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Java agent version. Use Java agent version 5.6.0 or higher for logs in context. Enable the JVM argument -javaagent, and enable distributed tracing. Configure logs in context for Java to enrich your log data, using any of the following extensions as applicable. If you use Spring or Spring Boot and aren't sure which extension you need, see our Spring documentation. Dropwizard 1.3 or higher We offer a Dropwizard extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with the DropWizard extension: Make sure you have the Dropwizard 1.3 or higher package installed and working on your application. Use the original Dropwizard appenders and logging factory installed and working on the application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Dropwizard 1.3 extension as applicable: Gradle: Add the following to your build.gradle file: dependencies { compile(\"com.newrelic.logging:dropwizard:2.0\") } Copy Maven: Add the following to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>dropwizard</artifactId> <version>2.0</version> </dependency> </dependencies> Copy Update your Dropwizard .yaml configuration file with a newrelic-json layout, replacing the currently used type: console or type: file with either type: newrelic-console or type: newrelic-file as appropriate. For example: logging: appenders: - type: newrelic-console # Add the two lines below if you don't have a layout specified on the appender. # If you have a layout, remove all parameters to the layout and set the type. layout: type: newrelic-json Copy The New Relic Dropwizard extension also supports a log-format layout type that uses the standard Dropwizard logging. For testing purposes, you can change the type of the layout with a one-line change: logging: appenders: - type: newrelic-file # This format will be ignored by the newrelic-json layout, but used by the log-format layout. logFormat: \"%date{ISO8601} %c %-5p: %m trace.id=%mdc{trace.id} span.id=%mdc{span.id}%n\" layout: # type: newrelic-json type: log-format Copy java.util.logging We offer a java.util.logging extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with with the java.util.logging extension: Make sure you have the java.util.logging package installed and working on the application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the java.util.logging extension as applicable. If you can't edit these files, you can instead add the jars directly to the application classpath. Gradle: Add the following to your build.gradle file: dependencies { compile(\"com.newrelic.logging:jul:2.0\") } Copy Maven: Add the following to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>jul</artifactId> <version>2.0</version> </dependency> </dependencies> Copy Check if your logging file's handlers property is set to something other than NewRelicMemoryHandler. Look for a line listing the root logger's handlers, like this: handlers = java.util.logging.FileHandler Copy Update your logging properties file to set the root logger's handler to NewRelicMemoryHandler so it intercepts messages destined for another handler: handlers = com.newrelic.logging.jul.NewRelicMemoryHandler Copy Configure the NewRelicMemoryHandler by setting the target to the handler that was previously assigned to the root logger, so it captures data New Relic needs on the thread the log message is coming from: com.newrelic.logging.jul.NewRelicMemoryHandler.target = java.util.logging.FileHandler Copy Use a NewRelicFormatter for the final handler. Update your logging properties file to set the formatter property like the following example. Make sure the handler where you set the formatter is the target handler from the previous step (java.util.logging.FileHandler in this example). java.util.logging.FileHandler.formatter = com.newrelic.logging.jul. NewRelicFormatter Copy The New Relic log format is JSON with telemetry metadata we use to correlate transactions and logs together. Currently we do not support any customization of that format. Once complete, JSON is logged instead of text. The JSON should be formatted as single objects, one per line, and should contain fields like log.level and thread.name. The trace.id, which is required for logs in context, should only have a value for log messages that occur within a transaction. java.util.logging classpath additions The most direct way to get the logs-in-context extensions is to add these dependencies to Maven's pom.xml or Gradle's build.gradle. This allows the packaging tools to pick up the correct dependencies. If you can't edit these files, you can instead add the jars directly to the application classpath for your logging framework's configuration. Before you modify the classpath: Enable the JVM argument -javaagent on your app's Java agent. Verify which logging framework the application is using. Make sure you are able to change your logging framework's configuration. Add the following three jars to the classpath if they aren't already present. Generally, we recommend taking the latest versions published on Maven Central. Group ID com.newrelic.logging and Artifact ID: Select the artifact named after your application's logging framework in Maven. Group ID com.fasterxml.jackson.core and Artifact ID: Use jackson-core. Group ID com.newrelic.agent.java and Artifact ID: Use newrelic-api. Log4j 1.x We offer a Log4j 1.x extension extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with with the Log4j 1.x extension, you must configure the Log4j extension in code or via XML. Properties files are not supported because AsyncAppender instances can only be automatically configured via XML. Make sure you have the Log4j 1.x package installed and working on the application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Log4j 1.x extension as applicable. Gradle: Add the following to your build.gradle file: dependencies { compile(\"com.newrelic.logging:log4j1:2.0\") } Copy Maven: Add the following to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>log4j1</artifactId> <version>2.0</version> </dependency> </dependencies> Copy In your logging configuration XML file, update your <appender> element with a NewRelicLayout, adding <layout class=\"com.newrelic.logging.log4j1.NewRelicLayout\"/>: <appender name=\" TypicalFile \" class=\"org.apache.log4j.FileAppender\"> <param name=\"file\" value=\"logs/log4j1-app.log\"/> <param name=\"append\" value=\"false\"/> <layout class=\" com.newrelic.logging.log4j1.NewRelicLayout \"/> <!-- only this line needs to be added --> </appender> Copy Use NewRelicAsyncAppender to wrap any appenders that will target New Relic's log forwarder. For example: <appender name=\" NewRelicFile \" class=\"com.newrelic.logging.log4j1.NewRelicAsyncAppender\"> <appender-ref ref=\" TypicalFile \" /> </appender> Copy Use the async appender on the root logger. For example: <root> <appender-ref ref=\" NewRelicFile \" /> </root> Copy Example configuration file for the Log4j 1.x extension: <?xml version=\"1.0\" encoding=\"UTF-8\" ?> <!DOCTYPE log4j:configuration SYSTEM \"log4j.dtd\"> <log4j:configuration debug=\"false\"> <appender name=\"TypicalFile\" class=\"org.apache.log4j.FileAppender\"> <param name=\"file\" value=\"logs/log4j1-app.log\"/> <param name=\"append\" value=\"false\"/> <!-- layout has been replaced --> <layout class=\"com.newrelic.logging.log4j1.NewRelicLayout\"/> </appender> <!-- this appender was added --> <appender name=\"NewRelicFile\" class=\"com.newrelic.logging.log4j1.NewRelicAsyncAppender\"> <appender-ref ref=\"TypicalFile\" /> </appender> <appender name=\"TypicalConsole\" class=\"org.apache.log4j.ConsoleAppender\"> <layout class=\"org.apache.log4j.PatternLayout\"> <param name=\"ConversionPattern\" value=\"%-5p %c{1} - %m%n\"/> </layout> </appender> <root> ​ <!-- the new appender was used here -->​​ <appender-ref ref=\"NewRelicFile\" /> <appender-ref ref=\"TypicalConsole\" /> </root> </log4j:configuration> Copy Log4j 2.x We offer a Log4j 2.x extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with with the Log4j 2.x extension: Make sure you have the Log4j 2.x or Logs4j 2 binding package installed and working on your application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Log4j 2.x extension as applicable: Gradle: Add the highlighted section to your build.gradle file: dependencies { compile(\"com.newrelic.logging:log4j2:2.0\") } Copy Maven: Add the highlighted section to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>log4j2</artifactId> <version>2.0</version> </dependency> </dependencies> Copy In your logging configuration XML file, update your <configuration> element by adding the highlighted section: <Configuration xmlns=\"http://logging.apache.org/log4j/2.0/config\" packages=\"com.newrelic.logging.log4j2\" > Copy If you're using a properties file, add packages=com.newrelic.logging.log4j2. Add <NewRelicLayout/> to use a NewRelicLayout element within one of the appenders. For example: <File name=\"MyFile\" fileName=\"logs/app-log-file.log\"> <NewRelicLayout/> </File> Copy If you're using a properties file, only change the layout.type: appender.console.type = Console appender.console.name = STDOUT appender.console.layout.type = NewRelicLayout Copy If you only modified an existing appender, skip this step. If you added a new appender, add <AppenderRef/> within <Root> to use this appender. Use the ref attribute to refer to appender name you created in the previous step. For example: <Root level=\"info\"> <AppenderRef ref=\"MyFile\"/> </Root> Copy If you're using a properties file and added a new appender, add: rootLogger.level = info rootLogger.appenderRef.stdout.ref = STDOUT ​​​​​ Copy Logback version 1.2.0 or higher We offer a Logback extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with Logback: Make sure you have Logback version 1.2.0 or higher and the New Relic Java agent version 5.6.0 or higher installed and working on your application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Logback extension as applicable: Gradle: Add the highlighted section to your build.gradle file: dependencies { compile(\"com.newrelic.logging:logback:2.0\") } Copy Maven: Add the highlighted section to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>logback</artifactId> <version>2.0</version> </dependency> </dependencies> Copy Update your logging configuration xml to replace any existing <encoder> element. If you're logging to the console (stdout/stderr), look for ConsoleAppender and replace: <appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> Copy If you're logging to a file, look for FileAppender and replace <encoder>: <appender name=\"LOG_FILE\" class=\"ch.qos.logback.core.FileAppender\"> <file>logs/app-log-file.log</file> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> Copy Update your logging configuration xml with the NewRelicAsyncAppender. To ensure that NewRelicAsyncAppender wraps any appenders that will target New Relic's log forwarder, add the following section. Change \"LOG_FILE\" to the name of the appender you updated in the previous step. <appender name=\"ASYNC\" class=\"com.newrelic.logging.logback.NewRelicAsyncAppender\"> <appender-ref ref=\"LOG_FILE\" /> </appender> Copy Make sure NewRelicAsyncAppender is the first appender used in your logger. Replace your root logger’s appenders with the ASYNC appender created in the previous step. Then list any other appenders after the NewRelicAsyncAppender in the <root> list. <root> <appender-ref ref=\"ASYNC\" /> </root> Copy Here are examples of an updated logging .xml file for the Logback extension. You can also see a working example in GitHub. Single console appender example Example configuration file after adding in the logging extension information: <configuration> <appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"> <!-- changed the encoder --> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> <!-- added the ASYNC appender --> <appender name=\"ASYNC\" class=\"com.newrelic.logging.logback.NewRelicAsyncAppender\"> <appender-ref ref=\"STDOUT\" /> </appender> <root level=\"debug\"> <!-- changed the root logger --> <appender-ref ref=\"ASYNC\" /> </root> </configuration> Copy Two console appenders example This example sends New Relic logging to a file, but still sends standard logging to the console: <configuration> <appender name=\"FILE\" class=\"ch.qos.logback.core.FileAppender\"> <file>myApp.log</file> <!-- encoder changed --> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> <!-- this appender does normal console logging --> <appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder> <pattern>%msg%n</pattern> </encoder> </appender> <!-- The required New Relic ASYNC appender wraps the FILE appender --> <appender name=\"ASYNC\" class=\"com.newrelic.logging.logback.NewRelicAsyncAppender\"> <appender-ref ref=\"FILE\" /> </appender> <root level=\"debug\"> <!-- ASYNC is one of the main appenders --> <appender-ref ref=\"ASYNC\" /> <!-- Send every message to normal console logging, as well. --> <appender-ref ref=\"STDOUT\" /> </root> </configuration> Copy Spring and Springboot We offer extensions for current versions of Spring and Spring Boot. If you already know the logging library, you can skip directly to that documentation: java.util.logging log4j 1 log4j 2 logback The extensions support default configurations only on Spring Boot 2.0 and higher. With Spring Boot: Here are tips to determine which logging library you have: If you have spring-boot-starter-log4j2 in your dependencies, you're using log4j 2.x. Refer to the Spring Boot log4j 2.x documentation for basic configuration, and the New Relic log4j 2 extension for customizing your configuration. If you're using Spring Boot but not the starter-log4j2, you're using logback by default. Refer to Spring Boot logback documentation for basic configuration, and the New Relic logback extension for customizing your configuration. With Spring (but not Spring Boot): Spring 5 or higher: Spring implements a bridge to other logging libraries that will automatically find them. However, those individual libraries must be configured and explicitly included in your project dependencies. To identify your logging dependency, consult your Gradle, Maven, or other build tool's dependency tree. Then follow the procedures to configure logs in context for your Java app with that extension. Spring 4 or lower: Spring version 4 and lower uses Apache Commons Logging for its bridge. Refer to the Spring documentation for information on configuring its bridge. View logs in UI To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you don't see log data in the UI, follow the troubleshooting procedures. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.78998,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java: <em>Configure</em> <em>logs</em> <em>in</em> <em>context</em>",
        "sections": "Java: <em>Configure</em> <em>logs</em> <em>in</em> <em>context</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in <em>context</em> for the Java agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Java app To <em>enable</em> <em>logs</em>"
      },
      "id": "612efca3e7b9d2f718b6f223"
    },
    {
      "sections": [
        ".NET: Configure logs in context",
        "Set up your .NET app",
        "Configure log4net extension",
        "log4net workflow diagram",
        "log4net 2.0.8 or higher configuration",
        "Configure NLog extension",
        "Nlog workflow diagram",
        "Nlog 4.5 or higher configuration",
        "Nlog file-based configuration",
        "Configure Serilog 2.5 or higher extension",
        "Serilog workflow diagram",
        "Serilog 2.5 or higher configuration",
        "Serilog file-based configuration",
        "View logs in the UI",
        "What's next?"
      ],
      "title": ".NET: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Configure logs in context"
      ],
      "external_id": "69a3fc9dab232e3ebfdeccceace39b1014b70beb",
      "image": "https://docs.newrelic.com/static/90e8518fffeb7d9cc28f58f29fe749a5/e5166/LogsInContext-Log4Net.jpg",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/net-configure-logs-context-all/",
      "published_at": "2021-10-18T10:42:13Z",
      "updated_at": "2021-10-06T21:51:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the .NET agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your .NET app To enable logs in context for APM apps monitored by .NET: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest .NET agent version, and enable distributed tracing. Use .NET agent version 8.21 or higher and the New Relic .NET agent API version 8.21 or higher for logs in context. Install or update to Microsoft .NET Framework 4.5 or higher or .NET Core 2.0 or higher. Install and configure any of the following logging extensions to enrich your log data, including: log4net NLog Serilog Check your log data in the New Relic UI. Configure log4net extension You can use the Apache log4net version 2.0.8 or higher extension to link your log data with related data across the rest of the New Relic platform. log4net workflow diagram The following diagram illustrates the flow of log messages through Apache log4net, highlighting specific components of the New Relic log4net extension. Many log forwarders are available. This example uses Fluentd. Appender: The NewRelicAppender adds contextual information from the .NET agent (using the API) to the log events generated by the application. This contextual information, known as linking metadata, is used by New Relic to link log messages to the transactions and spans from which they were created. This appender will pass the enriched log events to downstream appenders for further processing. Since the NewRelicAppender is ForwardingAppender type, it needs to be the first appender in the chain. It also requires another appender that can write to an actual output destination as its child in order to work. Layout: The NewRelicLayout formats the enriched log events into the JSON format expected by New Relic. The appender, which this layout is assigned to, instructs log4net to output the JSON to a file in the location that the log forwarder expects. Log Forwarder: The log forwarder monitors an output folder and incrementally sends the properly formatted and enriched log information to the New Relic logging endpoint. log4net 2.0.8 or higher configuration Log4net uses appender and layout to store and format log messages. NewRelicAppender enriches log messages with contextual information from the New Relic .NET agent if it is attached to your application. The appender passes enriched log messages to downstream appenders to handle specific use cases for log messages. For more information about logging with log4net, see the Apache log4net Getting started documentation. To configure logs in context with the log4net extension: Using the Visual Studio NuGet Package Manager, locate and install the NewRelic.LogEnrichers.Log4Net package. In your log4net configuration file, update your logging configuration to use the NewRelicAppender as the first level appender, and reference your existing appenders as its children. Also replace the layout of the appender that writes log messages to an output destination with the NewRelicLayout. The following log4net configuration example enriches log events with New Relic linking metadata. In addition to the existing log files, it outputs new log files in a specific JSON format at C:\\logs\\log4netExample.log.json for consumption by the log forwarder: <log4net> <root> <level value=\"ALL\" /> <appender-ref ref=\"NewRelicAppender\" /> </root> <appender name=\"NewRelicAppender\" type=\"NewRelic.LogEnrichers.Log4Net.NewRelicAppender, NewRelic.LogEnrichers.Log4Net\" > <threshold value=\"ALL\"/> <appender-ref ref=\"FileAppender\" /> </appender> <appender name=\"FileAppender\" type=\"log4net.Appender.FileAppender\"> <file value=\"C:\\logs\\log4netExample.log.json\" /> <param name=\"AppendToFile\" value=\"true\" /> <layout type=\"NewRelic.LogEnrichers.Log4Net.NewRelicLayout, NewRelic.LogEnrichers.Log4Net\"> </layout> </appender> </log4net> Copy After you configure the log4net extension and update your logging file, you can configure your extension to send data to New Relic. Here is an example configuration using the Fluentd plugin for New Relic Logs: <!--NewRelicLoggingExample.conf--> <source> @type tail path C:\\logs\\log4netExample.log.json pos_file C:\\logs\\log4netExample.log.json.pos tag logfile.* <parse> @type json </parse> </source> <match **> @type newrelic license_key <YOUR NEW_RELIC_LICENSE_KEY> base_uri https://log-api.newrelic.com/log/v1 </match> Copy Configure NLog extension You can use our NLog 4.5 or higher extension to link to your log data with related data across the rest of the New Relic platform. Nlog workflow diagram The New Relic NLog extension provides a NewRelicJsonLayout that formats a log event in the way required by the New Relic logging endpoint. Next, it adds contextual information from the .NET agent when attached to your application. Then, a target can be configured to write logging data to an output folder. The log forwarder can monitor this folder and incrementally send log information to New Relic. The following diagram illustrates the flow of log messages through NLog, highlighting specific components of the New Relic NLog extension. New Relic JSON Layout: The NewRelicJsonLayout adds contextual information from the .NET agent (using the API) to the log events generated by the application, and outputs log messages in the JSON format expected by New Relic. This contextual information, known as linking metadata, is used by New Relic to link log messages to the transactions and spans where they were created. Since the NewRelicAppender is ForwardingAppender type, it needs to be the first appender in the chain. It also requires another appender that can write to an actual output destination as its child in order to work. File Target: A FileTarget defines a file on disk where log messages are written. Adding the NewRelicJsonLayout to that target allows the output to be formatted correctly for forwarding to New Relic. Log Forwarder: The log forwarder is configured to send the properly formatted and enriched log data from the FileTarget output to the New Relic logging endpoint. For more information about logging with NLog, see the nlog-project.org documentation. Nlog 4.5 or higher configuration Use our NLog 4.5 or higher extension to link to your log data with related data across the rest of the New Relic platform. Using the Visual Studio NuGet Package Manager, locate and install the NewRelic.LogEnrichers.NLog package. In your application code, update your logging configuration to add the NewRelicJsonLayout and decide if you want to collect MappedDiagnosticsContext (MDC) or the MappedDiagnosticsLogicalContext (MDLC) data. The following configuration examples result in new JSON files that are written to disk. Some of these configuration options may be useful for managing the amount of disk space used and/or the performance of the target. archiveAboveSize maxArchiveFiles bufferSize enableArchiveFileCompression autoFlush concurrentWrites Although the NLog AsyncWrapper Target is not required, it may help improve performance by performing formatting and output of log files on a different thread. Don't collect MDC or the MDLC data (default): The following code example enriches log events with New Relic linking metadata, but not with MDC or the MDLC data. In addition to the existing log files, it outputs new log files in a specific JSON format at C:\\logs\\NLogExample.log.json for consumption by the log forwarder: var loggerConfig = new LoggingConfiguration(); var newRelicFileTarget = new FileTarget(\"NewRelicFileTarget\"); newRelicFileTarget.Layout = new NewRelicJsonLayout(); newRelicFileTarget.FileName = \"C:\\logs\\NLogExample.json\"; loggerConfig.AddTarget(newRelicFileTarget); loggerConfig.AddRuleForAllLevels(\"NewRelicFileTarget\"); LogManager.Configuration = loggerConfig; var logger = LogManager.GetLogger(\"Example\"); Copy Collect MDC or the MDLC data: If your application uses the MDC or the MDLC, you can configure the NewRelicJsonLayout to include items in those collections. The following code example adds the additional configuration to enable collecting MDC and MDLC data. As in the previous example, it outputs new log files in a specific JSON format at C:\\logs\\NLogExample.log.json for consumption by the log forwarder: var loggerConfig = new LoggingConfiguration(); var newRelicFileTarget = new FileTarget(\"NewRelicFileTarget\"); var newRelicLayout = new NewRelicJsonLayout { IncludeMdc = `true,` IncludeMdlc = `true` }; newRelicFileTarget.Layout = newRelicLayout; newRelicFileTarget.FileName = \"C:\\logs\\NLogExample.json\"; loggerConfig.AddTarget(newRelicFileTarget); loggerConfig.AddRuleForAllLevels(\"NewRelicFileTarget\"); LogManager.Configuration = loggerConfig; var logger = LogManager.GetLogger(\"Example\"); Copy Once you have configured the NLog extension and updated your logging file, you can configure your extension to send data to New Relic. Here is an example configuration using the Fluentd plugin to forward logs to New Relic: <!--NewRelicLoggingExample.conf--> <source> @type tail path C:\\logs\\NLogExample.log.json pos_file C:\\logs\\NLogExample.log.json.pos tag logfile.* <parse> @type json </parse> </source> <match **> @type newrelic license_key <YOUR NEW_RELIC_LICENSE_KEY> base_uri https://log-api.newrelic.com/log/v1 </match> Copy Nlog file-based configuration You can also configure the New Relic NLog extension with file-based configuration providers. The folowing example code creates a logger based on settings contained in an App.config file. Instantiating Logger using .config file var logger = LogManager.GetLogger(\"NewRelicLog\"); logger.Info(\"Hello, New Relic!\"); Copy Sample App.config file <?xml version=\"1.0\" encoding=\"utf-8\" ?> <configuration> <configSections> <section name=\"nlog\" type=\"NLog.Config.ConfigSectionHandler, NLog\"/> </configSections> <startup> <supportedRuntime version=\"v4.0\" sku=\".NETFramework,Version=v4.5\" /> </startup> <nlog xmlns=\"http://www.nlog-project.org/schemas/NLog.xsd\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"> <extensions> <add assembly=\"NewRelic.LogEnrichers.NLog\" /> </extensions> <targets> <target name=\"NewRelicLogFile\" xsi:type=\"File\" fileName=\"C:/path/to/NewRelicLog.json\"> <layout xsi:type=\"newrelic-jsonlayout\"> </layout> </target> </targets> <rules> <logger name=\"NewRelicLog\" minlevel=\"Info\" writeTo=\"newRelicLogFile\" /> </rules> </nlog> </configuration> Copy Configure Serilog 2.5 or higher extension You can use our Serilog extension to link to your log data with related data across the rest of the New Relic platform. This requires: Serilog 2.5 or higher Serilog File Sinks v4.0 or higher Serilog workflow diagram Serilog is a structured logging framework that records log messages from your application and creates a LogEvent to store the message data. Using Enrichers, you can add additional information to the log events. Sinks and Formatters allow you to format and output those log events for downstream consumption and viewing. The following diagram illustrates the flow of log messages through Serilog, highlighting specific components of the New Relic Serilog extension. Many log forwarders are available. This example uses Fluentd. New Relic Enricher: The NewRelicEnricher adds contextual information from the .NET agent (using the API) to the log events generated by the application. This contextual information, called linking metadata, is used by New Relic to link log messages to the transactions and spans where they were created. New Relic Formatter: The NewRelicFormatter translates enriched log events into the JSON format expected by New Relic. A sink instructs Serilog to output the JSON to a file in the location that the log forwarder expects. New Relic Log Forwarder: The log forwarder is configured to send the properly formatted and enriched log data from the FileTarget output to the New Relic logging endpoint. For more information about Serilog log events, see the Serilog documentation on GitHub. Serilog 2.5 or higher configuration To configure logs in context with the Serilog extension: Use the Visual Studio NuGet Package Manager to locate and install the NewRelic.LogEnrichers.Serilog package. In your application code, update your logging configuration to add the NewRelicEnricher and NewRelicFormatter. The following code example enriches log events with New Relic linking metadata. In addition to the existing log files, it outputs new log files in a specific JSON format at C:\\logs\\SerilogExample.log.json for consumption by the log forwarder: var loggerConfig = new LoggerConfiguration() loggerConfig .Enrich.WithThreadName() .Enrich.WithThreadId() .Enrich.WithNewRelicLogsInContext() .WriteTo.File( path: @\"C:\\logs\\ExistingLoggingOutput.txt\") .WriteTo.File( formatter: new NewRelicFormatter(), path: @\"C:\\logs\\SerilogExample.log.json\"); var log = loggerConfig.CreateLogger(); Copy This configuration results in new JSON files that are written to disk. Some of these configuration options may be useful for managing the amount of disk space used and/or the performance of the sink. restrictedToMinimumLevel buffered rollingInterval rollOnFileSizeLimit retainedFileCountLimit Although not required, using the Serilog Asynchronous Sink Wrapper may help improve the performance by performing formatting and output of log files on a different thread. Once you have configured the Serilog extension and updated your logging file, you can configure your extension to send data to New Relic. Here is an example configuration using the Fluentd plugin to forward logs to New Relic: <!--NewRelicLoggingExample.conf--> <source> @type tail path C:\\logs\\SerilogExample.log.json pos_file C:\\logs\\SerilogExample.log.json.pos tag logfile.* <parse> @type json </parse> </source> <match **> @type newrelic license_key <YOUR NEW_RELIC_LICENSE_KEY> base_uri https://log-api.newrelic.com/log/v1 </match> Copy Serilog file-based configuration You can also configure the New Relic Serilog extension with file-based configuration providers.The following additional NuGet Packages are required: Microsoft.Extensions.Configuration](https://www.nuget.org/packages/Microsoft.Extensions.Configuration/) Serilog.Settings.Configuration The following example code creates a logger based on settings contained in an appSettings.json file. Instantiating logger using appsettings.json var builder = new ConfigurationBuilder() .AddJsonFile(\"appsettings.json\"); var configuration = builder.Build(); var logger = new LoggerConfiguration() .ReadFrom.Configuration(configuration) .CreateLogger(); Copy Sample appsettings.json file { \"Serilog\": { \"Using\": [ \"Serilog.Sinks.Console\", \"Serilog.Sinks.File\", \"NewRelic.LogEnrichers.Serilog\" ], \"MinimumLevel\": \"Debug\", \"Enrich\": [ \"WithNewRelicLogsInContext\" ], \"WriteTo\": [ { \"Name\": \"File\", \"Args\": { \"path\": \"C:\\\\Logs\\\\SerilogExample.log.json\", \"formatter\": \"NewRelic.LogEnrichers.Serilog.NewRelicFormatter, NewRelic.LogEnrichers.Serilog\" } } ], \"Properties\": { \"Application\": \"NewRelic Logging Serilog Example\" } } } Copy The following example code creates a logger based on settings contained in a web.config file. The Serilog.Settings.AppSettings NuGet Package is required. Instantiating logger using .config file var logger = new LoggerConfiguration() .ReadFrom.AppSettings() .CreateLogger(); Copy Sample web.config file <?xml version=\"1.0\" encoding=\"utf-8\"?> <configuration> <appSettings> <add key=\"serilog:using:NewRelic\" value=\"NewRelic.LogEnrichers.Serilog\" /> <add key=\"serilog:using:File\" value=\"Serilog.Sinks.File\" /> <!--Add other enrichers here--> <add key=\"serilog:enrich:WithNewRelicLogsInContext\" /> <add key=\"serilog:write-to:File.path\" value=\"C:\\logs\\SerilogExample.log.json\" /> <add key=\"serilog:write-to:File.formatter\" value=\"NewRelic.LogEnrichers.Serilog.NewRelicFormatter, NewRelic.LogEnrichers.Serilog\" /> </appSettings> Copy View logs in the UI To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you don't see log data in the UI, follow the troubleshooting procedures. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.78992,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": ".NET: <em>Configure</em> <em>logs</em> <em>in</em> <em>context</em>",
        "sections": ".NET: <em>Configure</em> <em>logs</em> <em>in</em> <em>context</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in <em>context</em> for the .NET agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your .NET app To <em>enable</em> <em>logs</em>"
      },
      "id": "612efe5764441ff155424352"
    }
  ],
  "/docs/logs/logs-context/configure-logs-context-apm-agents": [
    {
      "sections": [
        "Ruby: Configure logs in context",
        "Set up your Ruby app",
        "Standard Ruby on Rails configuration",
        "Incompatible gems",
        "Rails advanced configuration",
        "Ruby configuration without Rails",
        "Lograge configuration",
        "Other logging extensions",
        "Troubleshooting",
        "What's next?"
      ],
      "title": "Ruby: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Logs in context for Ruby"
      ],
      "external_id": "d2100c0d6ddffe6b52a1f968f61f777a1e56a13c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/configure-logs-context-ruby/",
      "published_at": "2021-10-19T03:54:54Z",
      "updated_at": "2021-10-19T03:54:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Ruby agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Ruby app To enable logs in context for APM apps monitored by Ruby: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Ruby agent version, and enable distributed tracing. Use Ruby agent version 6.7.0 or higher for logs in context. For Rails applications, use a supported Rails version. Configure logs in context for Ruby. Standard Ruby on Rails configuration Rails logging is controlled by two components: A logger you can customize by setting config.logger A log formatter you can customize by setting config.log_formatter) In most cases, you should configure logs in context by setting config.log_formatter to the DecoratingFormatter in your Rails application. For more information about Rails configuration, see the rubyonrails.org documentation. In your application's config, require newrelic_rpm, then add the following line: module ________ class Application < Rails::Application ... config.log_formatter = ::NewRelic::Agent::Logging::DecoratingFormatter.new end end Copy This configuration uses the New Relic formatter for log messages, but the remaining configuration is provided by the other Rails settings. Incompatible gems New Relic's decorating logger is known to be incompatible with the following gems: logging semantic logger rails_stdout_logger rails_12factor Rails advanced configuration If setting the log_formatter option doesn't meet your needs, replace the entire Rails logger with an instance of the New Relic logger. Provide the parameters to the logger's constructor, like this: module ________ class Application < Rails::Application ... config.logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( \"log/application.log\", #etc... ) end end Copy Ruby configuration without Rails For non-Rails applications, use the DecoratingLogger in place of the Ruby standard ::Logger, like this: logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( 'log/application.log', #etc... ) ... logger.info(...) Copy The DecoratingLogger is a drop-in replacement for the Ruby standard ::Logger. Their constructors accept the same parameters. Lograge configuration To configure this extension with the lograge gem, follow standard procedures in this doc for Ruby on Rails configuration. No additional configuration is required for the lograge gem. Other logging extensions To use our logging extension with a different logging implementation or with your own custom logger, use the DecoratingFormatter. For example: module ________ class Application < Rails::Application ... config.logger = ::YourCustomLoggerImplementation.new( $stdout, formatter: ::NewRelic::Agent::Logging::DecoratingFormatter.new ) end end Copy To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you have configured your logging in /config/application.rb or in /config/environments/development.rb, run your application locally and check its logging output. You should see some output like this: {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"00fc7d46\",\"timestamp\":1567701375543,\"message\":\"example log message one\",\"log.level\":\"DEBUG\"} {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"6754870b\",\"timestamp\":1567702843604,\"message\":\"example log message two\",\"log.level\":\"DEBUG\"} Copy Troubleshooting If you don't see log data in the UI, follow the troubleshooting procedures. Also, if you see JSON logs in your application's output, but your query does not return logs, check your log forwarder. If the logs from your application are not formatted in JSON with fields like trace.id and span.id, there may be a problem with your log forwarding extension. In this situation: Check that the application is using a supported logging framework. Check that your logging configuration has been applied to all the environments where you want to forward and enrich the log data being sent to New Relic. Check that another logger is not configured later in your application's configuration. Logs in context for Ruby does not support tagged logging. If you are initializing your logger with a log_tags argument, your custom tags may not appear on the final version of your logs. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 383.26,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Ruby: <em>Configure</em> <em>logs</em> <em>in</em> <em>context</em>",
        "sections": "Ruby: <em>Configure</em> <em>logs</em> <em>in</em> <em>context</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in <em>context</em> for the Ruby agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Ruby app To <em>enable</em> <em>logs</em>"
      },
      "id": "612d460128ccbc17bf56a863"
    },
    {
      "sections": [
        "Java: Configure logs in context",
        "Set up your Java app",
        "Dropwizard 1.3 or higher",
        "java.util.logging",
        "java.util.logging classpath additions",
        "Log4j 1.x",
        "Log4j 2.x",
        "Logback version 1.2.0 or higher",
        "Spring and Springboot",
        "View logs in UI",
        "What's next?"
      ],
      "title": "Java: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Configure logs in context"
      ],
      "external_id": "b9fbeafa564247287e5d6466630d5bdcdb85affb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/java-configure-logs-context-all/",
      "published_at": "2021-10-18T11:15:14Z",
      "updated_at": "2021-10-06T21:51:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Java agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Java app To enable logs in context for APM apps monitored by Java: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Java agent version. Use Java agent version 5.6.0 or higher for logs in context. Enable the JVM argument -javaagent, and enable distributed tracing. Configure logs in context for Java to enrich your log data, using any of the following extensions as applicable. If you use Spring or Spring Boot and aren't sure which extension you need, see our Spring documentation. Dropwizard 1.3 or higher We offer a Dropwizard extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with the DropWizard extension: Make sure you have the Dropwizard 1.3 or higher package installed and working on your application. Use the original Dropwizard appenders and logging factory installed and working on the application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Dropwizard 1.3 extension as applicable: Gradle: Add the following to your build.gradle file: dependencies { compile(\"com.newrelic.logging:dropwizard:2.0\") } Copy Maven: Add the following to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>dropwizard</artifactId> <version>2.0</version> </dependency> </dependencies> Copy Update your Dropwizard .yaml configuration file with a newrelic-json layout, replacing the currently used type: console or type: file with either type: newrelic-console or type: newrelic-file as appropriate. For example: logging: appenders: - type: newrelic-console # Add the two lines below if you don't have a layout specified on the appender. # If you have a layout, remove all parameters to the layout and set the type. layout: type: newrelic-json Copy The New Relic Dropwizard extension also supports a log-format layout type that uses the standard Dropwizard logging. For testing purposes, you can change the type of the layout with a one-line change: logging: appenders: - type: newrelic-file # This format will be ignored by the newrelic-json layout, but used by the log-format layout. logFormat: \"%date{ISO8601} %c %-5p: %m trace.id=%mdc{trace.id} span.id=%mdc{span.id}%n\" layout: # type: newrelic-json type: log-format Copy java.util.logging We offer a java.util.logging extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with with the java.util.logging extension: Make sure you have the java.util.logging package installed and working on the application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the java.util.logging extension as applicable. If you can't edit these files, you can instead add the jars directly to the application classpath. Gradle: Add the following to your build.gradle file: dependencies { compile(\"com.newrelic.logging:jul:2.0\") } Copy Maven: Add the following to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>jul</artifactId> <version>2.0</version> </dependency> </dependencies> Copy Check if your logging file's handlers property is set to something other than NewRelicMemoryHandler. Look for a line listing the root logger's handlers, like this: handlers = java.util.logging.FileHandler Copy Update your logging properties file to set the root logger's handler to NewRelicMemoryHandler so it intercepts messages destined for another handler: handlers = com.newrelic.logging.jul.NewRelicMemoryHandler Copy Configure the NewRelicMemoryHandler by setting the target to the handler that was previously assigned to the root logger, so it captures data New Relic needs on the thread the log message is coming from: com.newrelic.logging.jul.NewRelicMemoryHandler.target = java.util.logging.FileHandler Copy Use a NewRelicFormatter for the final handler. Update your logging properties file to set the formatter property like the following example. Make sure the handler where you set the formatter is the target handler from the previous step (java.util.logging.FileHandler in this example). java.util.logging.FileHandler.formatter = com.newrelic.logging.jul. NewRelicFormatter Copy The New Relic log format is JSON with telemetry metadata we use to correlate transactions and logs together. Currently we do not support any customization of that format. Once complete, JSON is logged instead of text. The JSON should be formatted as single objects, one per line, and should contain fields like log.level and thread.name. The trace.id, which is required for logs in context, should only have a value for log messages that occur within a transaction. java.util.logging classpath additions The most direct way to get the logs-in-context extensions is to add these dependencies to Maven's pom.xml or Gradle's build.gradle. This allows the packaging tools to pick up the correct dependencies. If you can't edit these files, you can instead add the jars directly to the application classpath for your logging framework's configuration. Before you modify the classpath: Enable the JVM argument -javaagent on your app's Java agent. Verify which logging framework the application is using. Make sure you are able to change your logging framework's configuration. Add the following three jars to the classpath if they aren't already present. Generally, we recommend taking the latest versions published on Maven Central. Group ID com.newrelic.logging and Artifact ID: Select the artifact named after your application's logging framework in Maven. Group ID com.fasterxml.jackson.core and Artifact ID: Use jackson-core. Group ID com.newrelic.agent.java and Artifact ID: Use newrelic-api. Log4j 1.x We offer a Log4j 1.x extension extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with with the Log4j 1.x extension, you must configure the Log4j extension in code or via XML. Properties files are not supported because AsyncAppender instances can only be automatically configured via XML. Make sure you have the Log4j 1.x package installed and working on the application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Log4j 1.x extension as applicable. Gradle: Add the following to your build.gradle file: dependencies { compile(\"com.newrelic.logging:log4j1:2.0\") } Copy Maven: Add the following to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>log4j1</artifactId> <version>2.0</version> </dependency> </dependencies> Copy In your logging configuration XML file, update your <appender> element with a NewRelicLayout, adding <layout class=\"com.newrelic.logging.log4j1.NewRelicLayout\"/>: <appender name=\" TypicalFile \" class=\"org.apache.log4j.FileAppender\"> <param name=\"file\" value=\"logs/log4j1-app.log\"/> <param name=\"append\" value=\"false\"/> <layout class=\" com.newrelic.logging.log4j1.NewRelicLayout \"/> <!-- only this line needs to be added --> </appender> Copy Use NewRelicAsyncAppender to wrap any appenders that will target New Relic's log forwarder. For example: <appender name=\" NewRelicFile \" class=\"com.newrelic.logging.log4j1.NewRelicAsyncAppender\"> <appender-ref ref=\" TypicalFile \" /> </appender> Copy Use the async appender on the root logger. For example: <root> <appender-ref ref=\" NewRelicFile \" /> </root> Copy Example configuration file for the Log4j 1.x extension: <?xml version=\"1.0\" encoding=\"UTF-8\" ?> <!DOCTYPE log4j:configuration SYSTEM \"log4j.dtd\"> <log4j:configuration debug=\"false\"> <appender name=\"TypicalFile\" class=\"org.apache.log4j.FileAppender\"> <param name=\"file\" value=\"logs/log4j1-app.log\"/> <param name=\"append\" value=\"false\"/> <!-- layout has been replaced --> <layout class=\"com.newrelic.logging.log4j1.NewRelicLayout\"/> </appender> <!-- this appender was added --> <appender name=\"NewRelicFile\" class=\"com.newrelic.logging.log4j1.NewRelicAsyncAppender\"> <appender-ref ref=\"TypicalFile\" /> </appender> <appender name=\"TypicalConsole\" class=\"org.apache.log4j.ConsoleAppender\"> <layout class=\"org.apache.log4j.PatternLayout\"> <param name=\"ConversionPattern\" value=\"%-5p %c{1} - %m%n\"/> </layout> </appender> <root> ​ <!-- the new appender was used here -->​​ <appender-ref ref=\"NewRelicFile\" /> <appender-ref ref=\"TypicalConsole\" /> </root> </log4j:configuration> Copy Log4j 2.x We offer a Log4j 2.x extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with with the Log4j 2.x extension: Make sure you have the Log4j 2.x or Logs4j 2 binding package installed and working on your application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Log4j 2.x extension as applicable: Gradle: Add the highlighted section to your build.gradle file: dependencies { compile(\"com.newrelic.logging:log4j2:2.0\") } Copy Maven: Add the highlighted section to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>log4j2</artifactId> <version>2.0</version> </dependency> </dependencies> Copy In your logging configuration XML file, update your <configuration> element by adding the highlighted section: <Configuration xmlns=\"http://logging.apache.org/log4j/2.0/config\" packages=\"com.newrelic.logging.log4j2\" > Copy If you're using a properties file, add packages=com.newrelic.logging.log4j2. Add <NewRelicLayout/> to use a NewRelicLayout element within one of the appenders. For example: <File name=\"MyFile\" fileName=\"logs/app-log-file.log\"> <NewRelicLayout/> </File> Copy If you're using a properties file, only change the layout.type: appender.console.type = Console appender.console.name = STDOUT appender.console.layout.type = NewRelicLayout Copy If you only modified an existing appender, skip this step. If you added a new appender, add <AppenderRef/> within <Root> to use this appender. Use the ref attribute to refer to appender name you created in the previous step. For example: <Root level=\"info\"> <AppenderRef ref=\"MyFile\"/> </Root> Copy If you're using a properties file and added a new appender, add: rootLogger.level = info rootLogger.appenderRef.stdout.ref = STDOUT ​​​​​ Copy Logback version 1.2.0 or higher We offer a Logback extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with Logback: Make sure you have Logback version 1.2.0 or higher and the New Relic Java agent version 5.6.0 or higher installed and working on your application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Logback extension as applicable: Gradle: Add the highlighted section to your build.gradle file: dependencies { compile(\"com.newrelic.logging:logback:2.0\") } Copy Maven: Add the highlighted section to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>logback</artifactId> <version>2.0</version> </dependency> </dependencies> Copy Update your logging configuration xml to replace any existing <encoder> element. If you're logging to the console (stdout/stderr), look for ConsoleAppender and replace: <appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> Copy If you're logging to a file, look for FileAppender and replace <encoder>: <appender name=\"LOG_FILE\" class=\"ch.qos.logback.core.FileAppender\"> <file>logs/app-log-file.log</file> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> Copy Update your logging configuration xml with the NewRelicAsyncAppender. To ensure that NewRelicAsyncAppender wraps any appenders that will target New Relic's log forwarder, add the following section. Change \"LOG_FILE\" to the name of the appender you updated in the previous step. <appender name=\"ASYNC\" class=\"com.newrelic.logging.logback.NewRelicAsyncAppender\"> <appender-ref ref=\"LOG_FILE\" /> </appender> Copy Make sure NewRelicAsyncAppender is the first appender used in your logger. Replace your root logger’s appenders with the ASYNC appender created in the previous step. Then list any other appenders after the NewRelicAsyncAppender in the <root> list. <root> <appender-ref ref=\"ASYNC\" /> </root> Copy Here are examples of an updated logging .xml file for the Logback extension. You can also see a working example in GitHub. Single console appender example Example configuration file after adding in the logging extension information: <configuration> <appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"> <!-- changed the encoder --> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> <!-- added the ASYNC appender --> <appender name=\"ASYNC\" class=\"com.newrelic.logging.logback.NewRelicAsyncAppender\"> <appender-ref ref=\"STDOUT\" /> </appender> <root level=\"debug\"> <!-- changed the root logger --> <appender-ref ref=\"ASYNC\" /> </root> </configuration> Copy Two console appenders example This example sends New Relic logging to a file, but still sends standard logging to the console: <configuration> <appender name=\"FILE\" class=\"ch.qos.logback.core.FileAppender\"> <file>myApp.log</file> <!-- encoder changed --> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> <!-- this appender does normal console logging --> <appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder> <pattern>%msg%n</pattern> </encoder> </appender> <!-- The required New Relic ASYNC appender wraps the FILE appender --> <appender name=\"ASYNC\" class=\"com.newrelic.logging.logback.NewRelicAsyncAppender\"> <appender-ref ref=\"FILE\" /> </appender> <root level=\"debug\"> <!-- ASYNC is one of the main appenders --> <appender-ref ref=\"ASYNC\" /> <!-- Send every message to normal console logging, as well. --> <appender-ref ref=\"STDOUT\" /> </root> </configuration> Copy Spring and Springboot We offer extensions for current versions of Spring and Spring Boot. If you already know the logging library, you can skip directly to that documentation: java.util.logging log4j 1 log4j 2 logback The extensions support default configurations only on Spring Boot 2.0 and higher. With Spring Boot: Here are tips to determine which logging library you have: If you have spring-boot-starter-log4j2 in your dependencies, you're using log4j 2.x. Refer to the Spring Boot log4j 2.x documentation for basic configuration, and the New Relic log4j 2 extension for customizing your configuration. If you're using Spring Boot but not the starter-log4j2, you're using logback by default. Refer to Spring Boot logback documentation for basic configuration, and the New Relic logback extension for customizing your configuration. With Spring (but not Spring Boot): Spring 5 or higher: Spring implements a bridge to other logging libraries that will automatically find them. However, those individual libraries must be configured and explicitly included in your project dependencies. To identify your logging dependency, consult your Gradle, Maven, or other build tool's dependency tree. Then follow the procedures to configure logs in context for your Java app with that extension. Spring 4 or lower: Spring version 4 and lower uses Apache Commons Logging for its bridge. Refer to the Spring documentation for information on configuring its bridge. View logs in UI To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you don't see log data in the UI, follow the troubleshooting procedures. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.78992,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java: <em>Configure</em> <em>logs</em> <em>in</em> <em>context</em>",
        "sections": "Java: <em>Configure</em> <em>logs</em> <em>in</em> <em>context</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in <em>context</em> for the Java agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Java app To <em>enable</em> <em>logs</em>"
      },
      "id": "612efca3e7b9d2f718b6f223"
    },
    {
      "sections": [
        "C SDK: Configure logs in context"
      ],
      "title": "C SDK: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Configure logs in context"
      ],
      "external_id": "87eeada741d7f22204b37e7d6a906b9546ab8d4a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/c-sdk-configure-logs-context/",
      "published_at": "2021-10-18T05:57:03Z",
      "updated_at": "2021-10-06T21:51:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context is not available for apps monitored by the C SDK. Instead, you can build your own logging extension library by using the Log API. Make sure your app has distributed tracing enabled, so you can quickly understand what happens to requests as they travel through upstream and downstream services. To enable distributed tracing for apps monitored by the C SDK, install or update to the latest C SDK version. Distributed tracing requires C SDK version 1.1.0 or higher.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.78986,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "C SDK: <em>Configure</em> <em>logs</em> <em>in</em> <em>context</em>",
        "sections": "C SDK: <em>Configure</em> <em>logs</em> <em>in</em> <em>context</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in <em>context</em> is not available for apps monitored by the C SDK. Instead, you can build your own logging extension library by using the <em>Log</em> API. Make sure your app has distributed tracing enabled, so you can quickly understand what happens to requests as they travel through upstream and downstream"
      },
      "id": "6127279b28ccbcf0c6f2618d"
    }
  ],
  "/docs/logs/logs-context/configure-logs-context-go": [
    {
      "sections": [
        "Ruby: Configure logs in context",
        "Set up your Ruby app",
        "Standard Ruby on Rails configuration",
        "Incompatible gems",
        "Rails advanced configuration",
        "Ruby configuration without Rails",
        "Lograge configuration",
        "Other logging extensions",
        "Troubleshooting",
        "What's next?"
      ],
      "title": "Ruby: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Logs in context for Ruby"
      ],
      "external_id": "d2100c0d6ddffe6b52a1f968f61f777a1e56a13c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/configure-logs-context-ruby/",
      "published_at": "2021-10-19T03:54:54Z",
      "updated_at": "2021-10-19T03:54:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Ruby agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Ruby app To enable logs in context for APM apps monitored by Ruby: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Ruby agent version, and enable distributed tracing. Use Ruby agent version 6.7.0 or higher for logs in context. For Rails applications, use a supported Rails version. Configure logs in context for Ruby. Standard Ruby on Rails configuration Rails logging is controlled by two components: A logger you can customize by setting config.logger A log formatter you can customize by setting config.log_formatter) In most cases, you should configure logs in context by setting config.log_formatter to the DecoratingFormatter in your Rails application. For more information about Rails configuration, see the rubyonrails.org documentation. In your application's config, require newrelic_rpm, then add the following line: module ________ class Application < Rails::Application ... config.log_formatter = ::NewRelic::Agent::Logging::DecoratingFormatter.new end end Copy This configuration uses the New Relic formatter for log messages, but the remaining configuration is provided by the other Rails settings. Incompatible gems New Relic's decorating logger is known to be incompatible with the following gems: logging semantic logger rails_stdout_logger rails_12factor Rails advanced configuration If setting the log_formatter option doesn't meet your needs, replace the entire Rails logger with an instance of the New Relic logger. Provide the parameters to the logger's constructor, like this: module ________ class Application < Rails::Application ... config.logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( \"log/application.log\", #etc... ) end end Copy Ruby configuration without Rails For non-Rails applications, use the DecoratingLogger in place of the Ruby standard ::Logger, like this: logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( 'log/application.log', #etc... ) ... logger.info(...) Copy The DecoratingLogger is a drop-in replacement for the Ruby standard ::Logger. Their constructors accept the same parameters. Lograge configuration To configure this extension with the lograge gem, follow standard procedures in this doc for Ruby on Rails configuration. No additional configuration is required for the lograge gem. Other logging extensions To use our logging extension with a different logging implementation or with your own custom logger, use the DecoratingFormatter. For example: module ________ class Application < Rails::Application ... config.logger = ::YourCustomLoggerImplementation.new( $stdout, formatter: ::NewRelic::Agent::Logging::DecoratingFormatter.new ) end end Copy To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you have configured your logging in /config/application.rb or in /config/environments/development.rb, run your application locally and check its logging output. You should see some output like this: {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"00fc7d46\",\"timestamp\":1567701375543,\"message\":\"example log message one\",\"log.level\":\"DEBUG\"} {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"6754870b\",\"timestamp\":1567702843604,\"message\":\"example log message two\",\"log.level\":\"DEBUG\"} Copy Troubleshooting If you don't see log data in the UI, follow the troubleshooting procedures. Also, if you see JSON logs in your application's output, but your query does not return logs, check your log forwarder. If the logs from your application are not formatted in JSON with fields like trace.id and span.id, there may be a problem with your log forwarding extension. In this situation: Check that the application is using a supported logging framework. Check that your logging configuration has been applied to all the environments where you want to forward and enrich the log data being sent to New Relic. Check that another logger is not configured later in your application's configuration. Logs in context for Ruby does not support tagged logging. If you are initializing your logger with a log_tags argument, your custom tags may not appear on the final version of your logs. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 282.5113,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Ruby: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "sections": "Ruby: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in <em>context</em> for the Ruby agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Ruby app To <em>enable</em> <em>logs</em>"
      },
      "id": "612d460128ccbc17bf56a863"
    },
    {
      "sections": [
        "Python: Configure logs in context",
        "Set up your Python app",
        "Python StreamHandler example",
        "What's next?"
      ],
      "title": "Python: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Logs in context for Python"
      ],
      "external_id": "a51497a54dbdd8c6ee16e8cf097a090347166d1f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/configure-logs-context-python/",
      "published_at": "2021-10-18T05:57:04Z",
      "updated_at": "2021-10-06T21:52:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Python agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Python app To enable logs in context for APM apps monitored by Python: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Python agent version, and enable distributed tracing. Use Python agent version 5.4.0 or higher for logs in context. Configure logs in context for your log handler. Python StreamHandler example Enabling logs in Python is as simple as instantiating a log formatter and adding it to your log handler. This example uses a StreamHandler which by default writes logs to sys.stderr, but any handler can be used. For more information about configuring log handlers, see the Python.org documentation. # Import the logging module and the New Relic log formatter import logging from newrelic.agent import NewRelicContextFormatter # Instantiate a new log handler handler = logging.StreamHandler() # Instantiate the log formatter and add it to the log handler formatter = NewRelicContextFormatter() handler.setFormatter(formatter) # Get the root logger and add the handler to it root_logger = logging.getLogger() root_logger.addHandler(handler) Copy To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you don't see log data in the UI, follow the troubleshooting procedures. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.44458,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Python: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "sections": "Python: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in <em>context</em> for the Python agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Python app To <em>enable</em> <em>logs</em>"
      },
      "id": "612d463564441f2c8042434e"
    },
    {
      "sections": [
        "Java: Configure logs in context",
        "Set up your Java app",
        "Dropwizard 1.3 or higher",
        "java.util.logging",
        "java.util.logging classpath additions",
        "Log4j 1.x",
        "Log4j 2.x",
        "Logback version 1.2.0 or higher",
        "Spring and Springboot",
        "View logs in UI",
        "What's next?"
      ],
      "title": "Java: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Configure logs in context"
      ],
      "external_id": "b9fbeafa564247287e5d6466630d5bdcdb85affb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/java-configure-logs-context-all/",
      "published_at": "2021-10-18T11:15:14Z",
      "updated_at": "2021-10-06T21:51:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Java agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Java app To enable logs in context for APM apps monitored by Java: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Java agent version. Use Java agent version 5.6.0 or higher for logs in context. Enable the JVM argument -javaagent, and enable distributed tracing. Configure logs in context for Java to enrich your log data, using any of the following extensions as applicable. If you use Spring or Spring Boot and aren't sure which extension you need, see our Spring documentation. Dropwizard 1.3 or higher We offer a Dropwizard extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with the DropWizard extension: Make sure you have the Dropwizard 1.3 or higher package installed and working on your application. Use the original Dropwizard appenders and logging factory installed and working on the application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Dropwizard 1.3 extension as applicable: Gradle: Add the following to your build.gradle file: dependencies { compile(\"com.newrelic.logging:dropwizard:2.0\") } Copy Maven: Add the following to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>dropwizard</artifactId> <version>2.0</version> </dependency> </dependencies> Copy Update your Dropwizard .yaml configuration file with a newrelic-json layout, replacing the currently used type: console or type: file with either type: newrelic-console or type: newrelic-file as appropriate. For example: logging: appenders: - type: newrelic-console # Add the two lines below if you don't have a layout specified on the appender. # If you have a layout, remove all parameters to the layout and set the type. layout: type: newrelic-json Copy The New Relic Dropwizard extension also supports a log-format layout type that uses the standard Dropwizard logging. For testing purposes, you can change the type of the layout with a one-line change: logging: appenders: - type: newrelic-file # This format will be ignored by the newrelic-json layout, but used by the log-format layout. logFormat: \"%date{ISO8601} %c %-5p: %m trace.id=%mdc{trace.id} span.id=%mdc{span.id}%n\" layout: # type: newrelic-json type: log-format Copy java.util.logging We offer a java.util.logging extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with with the java.util.logging extension: Make sure you have the java.util.logging package installed and working on the application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the java.util.logging extension as applicable. If you can't edit these files, you can instead add the jars directly to the application classpath. Gradle: Add the following to your build.gradle file: dependencies { compile(\"com.newrelic.logging:jul:2.0\") } Copy Maven: Add the following to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>jul</artifactId> <version>2.0</version> </dependency> </dependencies> Copy Check if your logging file's handlers property is set to something other than NewRelicMemoryHandler. Look for a line listing the root logger's handlers, like this: handlers = java.util.logging.FileHandler Copy Update your logging properties file to set the root logger's handler to NewRelicMemoryHandler so it intercepts messages destined for another handler: handlers = com.newrelic.logging.jul.NewRelicMemoryHandler Copy Configure the NewRelicMemoryHandler by setting the target to the handler that was previously assigned to the root logger, so it captures data New Relic needs on the thread the log message is coming from: com.newrelic.logging.jul.NewRelicMemoryHandler.target = java.util.logging.FileHandler Copy Use a NewRelicFormatter for the final handler. Update your logging properties file to set the formatter property like the following example. Make sure the handler where you set the formatter is the target handler from the previous step (java.util.logging.FileHandler in this example). java.util.logging.FileHandler.formatter = com.newrelic.logging.jul. NewRelicFormatter Copy The New Relic log format is JSON with telemetry metadata we use to correlate transactions and logs together. Currently we do not support any customization of that format. Once complete, JSON is logged instead of text. The JSON should be formatted as single objects, one per line, and should contain fields like log.level and thread.name. The trace.id, which is required for logs in context, should only have a value for log messages that occur within a transaction. java.util.logging classpath additions The most direct way to get the logs-in-context extensions is to add these dependencies to Maven's pom.xml or Gradle's build.gradle. This allows the packaging tools to pick up the correct dependencies. If you can't edit these files, you can instead add the jars directly to the application classpath for your logging framework's configuration. Before you modify the classpath: Enable the JVM argument -javaagent on your app's Java agent. Verify which logging framework the application is using. Make sure you are able to change your logging framework's configuration. Add the following three jars to the classpath if they aren't already present. Generally, we recommend taking the latest versions published on Maven Central. Group ID com.newrelic.logging and Artifact ID: Select the artifact named after your application's logging framework in Maven. Group ID com.fasterxml.jackson.core and Artifact ID: Use jackson-core. Group ID com.newrelic.agent.java and Artifact ID: Use newrelic-api. Log4j 1.x We offer a Log4j 1.x extension extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with with the Log4j 1.x extension, you must configure the Log4j extension in code or via XML. Properties files are not supported because AsyncAppender instances can only be automatically configured via XML. Make sure you have the Log4j 1.x package installed and working on the application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Log4j 1.x extension as applicable. Gradle: Add the following to your build.gradle file: dependencies { compile(\"com.newrelic.logging:log4j1:2.0\") } Copy Maven: Add the following to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>log4j1</artifactId> <version>2.0</version> </dependency> </dependencies> Copy In your logging configuration XML file, update your <appender> element with a NewRelicLayout, adding <layout class=\"com.newrelic.logging.log4j1.NewRelicLayout\"/>: <appender name=\" TypicalFile \" class=\"org.apache.log4j.FileAppender\"> <param name=\"file\" value=\"logs/log4j1-app.log\"/> <param name=\"append\" value=\"false\"/> <layout class=\" com.newrelic.logging.log4j1.NewRelicLayout \"/> <!-- only this line needs to be added --> </appender> Copy Use NewRelicAsyncAppender to wrap any appenders that will target New Relic's log forwarder. For example: <appender name=\" NewRelicFile \" class=\"com.newrelic.logging.log4j1.NewRelicAsyncAppender\"> <appender-ref ref=\" TypicalFile \" /> </appender> Copy Use the async appender on the root logger. For example: <root> <appender-ref ref=\" NewRelicFile \" /> </root> Copy Example configuration file for the Log4j 1.x extension: <?xml version=\"1.0\" encoding=\"UTF-8\" ?> <!DOCTYPE log4j:configuration SYSTEM \"log4j.dtd\"> <log4j:configuration debug=\"false\"> <appender name=\"TypicalFile\" class=\"org.apache.log4j.FileAppender\"> <param name=\"file\" value=\"logs/log4j1-app.log\"/> <param name=\"append\" value=\"false\"/> <!-- layout has been replaced --> <layout class=\"com.newrelic.logging.log4j1.NewRelicLayout\"/> </appender> <!-- this appender was added --> <appender name=\"NewRelicFile\" class=\"com.newrelic.logging.log4j1.NewRelicAsyncAppender\"> <appender-ref ref=\"TypicalFile\" /> </appender> <appender name=\"TypicalConsole\" class=\"org.apache.log4j.ConsoleAppender\"> <layout class=\"org.apache.log4j.PatternLayout\"> <param name=\"ConversionPattern\" value=\"%-5p %c{1} - %m%n\"/> </layout> </appender> <root> ​ <!-- the new appender was used here -->​​ <appender-ref ref=\"NewRelicFile\" /> <appender-ref ref=\"TypicalConsole\" /> </root> </log4j:configuration> Copy Log4j 2.x We offer a Log4j 2.x extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with with the Log4j 2.x extension: Make sure you have the Log4j 2.x or Logs4j 2 binding package installed and working on your application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Log4j 2.x extension as applicable: Gradle: Add the highlighted section to your build.gradle file: dependencies { compile(\"com.newrelic.logging:log4j2:2.0\") } Copy Maven: Add the highlighted section to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>log4j2</artifactId> <version>2.0</version> </dependency> </dependencies> Copy In your logging configuration XML file, update your <configuration> element by adding the highlighted section: <Configuration xmlns=\"http://logging.apache.org/log4j/2.0/config\" packages=\"com.newrelic.logging.log4j2\" > Copy If you're using a properties file, add packages=com.newrelic.logging.log4j2. Add <NewRelicLayout/> to use a NewRelicLayout element within one of the appenders. For example: <File name=\"MyFile\" fileName=\"logs/app-log-file.log\"> <NewRelicLayout/> </File> Copy If you're using a properties file, only change the layout.type: appender.console.type = Console appender.console.name = STDOUT appender.console.layout.type = NewRelicLayout Copy If you only modified an existing appender, skip this step. If you added a new appender, add <AppenderRef/> within <Root> to use this appender. Use the ref attribute to refer to appender name you created in the previous step. For example: <Root level=\"info\"> <AppenderRef ref=\"MyFile\"/> </Root> Copy If you're using a properties file and added a new appender, add: rootLogger.level = info rootLogger.appenderRef.stdout.ref = STDOUT ​​​​​ Copy Logback version 1.2.0 or higher We offer a Logback extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with Logback: Make sure you have Logback version 1.2.0 or higher and the New Relic Java agent version 5.6.0 or higher installed and working on your application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Logback extension as applicable: Gradle: Add the highlighted section to your build.gradle file: dependencies { compile(\"com.newrelic.logging:logback:2.0\") } Copy Maven: Add the highlighted section to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>logback</artifactId> <version>2.0</version> </dependency> </dependencies> Copy Update your logging configuration xml to replace any existing <encoder> element. If you're logging to the console (stdout/stderr), look for ConsoleAppender and replace: <appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> Copy If you're logging to a file, look for FileAppender and replace <encoder>: <appender name=\"LOG_FILE\" class=\"ch.qos.logback.core.FileAppender\"> <file>logs/app-log-file.log</file> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> Copy Update your logging configuration xml with the NewRelicAsyncAppender. To ensure that NewRelicAsyncAppender wraps any appenders that will target New Relic's log forwarder, add the following section. Change \"LOG_FILE\" to the name of the appender you updated in the previous step. <appender name=\"ASYNC\" class=\"com.newrelic.logging.logback.NewRelicAsyncAppender\"> <appender-ref ref=\"LOG_FILE\" /> </appender> Copy Make sure NewRelicAsyncAppender is the first appender used in your logger. Replace your root logger’s appenders with the ASYNC appender created in the previous step. Then list any other appenders after the NewRelicAsyncAppender in the <root> list. <root> <appender-ref ref=\"ASYNC\" /> </root> Copy Here are examples of an updated logging .xml file for the Logback extension. You can also see a working example in GitHub. Single console appender example Example configuration file after adding in the logging extension information: <configuration> <appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"> <!-- changed the encoder --> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> <!-- added the ASYNC appender --> <appender name=\"ASYNC\" class=\"com.newrelic.logging.logback.NewRelicAsyncAppender\"> <appender-ref ref=\"STDOUT\" /> </appender> <root level=\"debug\"> <!-- changed the root logger --> <appender-ref ref=\"ASYNC\" /> </root> </configuration> Copy Two console appenders example This example sends New Relic logging to a file, but still sends standard logging to the console: <configuration> <appender name=\"FILE\" class=\"ch.qos.logback.core.FileAppender\"> <file>myApp.log</file> <!-- encoder changed --> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> <!-- this appender does normal console logging --> <appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder> <pattern>%msg%n</pattern> </encoder> </appender> <!-- The required New Relic ASYNC appender wraps the FILE appender --> <appender name=\"ASYNC\" class=\"com.newrelic.logging.logback.NewRelicAsyncAppender\"> <appender-ref ref=\"FILE\" /> </appender> <root level=\"debug\"> <!-- ASYNC is one of the main appenders --> <appender-ref ref=\"ASYNC\" /> <!-- Send every message to normal console logging, as well. --> <appender-ref ref=\"STDOUT\" /> </root> </configuration> Copy Spring and Springboot We offer extensions for current versions of Spring and Spring Boot. If you already know the logging library, you can skip directly to that documentation: java.util.logging log4j 1 log4j 2 logback The extensions support default configurations only on Spring Boot 2.0 and higher. With Spring Boot: Here are tips to determine which logging library you have: If you have spring-boot-starter-log4j2 in your dependencies, you're using log4j 2.x. Refer to the Spring Boot log4j 2.x documentation for basic configuration, and the New Relic log4j 2 extension for customizing your configuration. If you're using Spring Boot but not the starter-log4j2, you're using logback by default. Refer to Spring Boot logback documentation for basic configuration, and the New Relic logback extension for customizing your configuration. With Spring (but not Spring Boot): Spring 5 or higher: Spring implements a bridge to other logging libraries that will automatically find them. However, those individual libraries must be configured and explicitly included in your project dependencies. To identify your logging dependency, consult your Gradle, Maven, or other build tool's dependency tree. Then follow the procedures to configure logs in context for your Java app with that extension. Spring 4 or lower: Spring version 4 and lower uses Apache Commons Logging for its bridge. Refer to the Spring documentation for information on configuring its bridge. View logs in UI To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you don't see log data in the UI, follow the troubleshooting procedures. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.44206,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "sections": "Java: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in <em>context</em> for the Java agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Java app To <em>enable</em> <em>logs</em>"
      },
      "id": "612efca3e7b9d2f718b6f223"
    }
  ],
  "/docs/logs/logs-context/configure-logs-context-nodejs": [
    {
      "sections": [
        "Ruby: Configure logs in context",
        "Set up your Ruby app",
        "Standard Ruby on Rails configuration",
        "Incompatible gems",
        "Rails advanced configuration",
        "Ruby configuration without Rails",
        "Lograge configuration",
        "Other logging extensions",
        "Troubleshooting",
        "What's next?"
      ],
      "title": "Ruby: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Logs in context for Ruby"
      ],
      "external_id": "d2100c0d6ddffe6b52a1f968f61f777a1e56a13c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/configure-logs-context-ruby/",
      "published_at": "2021-10-19T03:54:54Z",
      "updated_at": "2021-10-19T03:54:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Ruby agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Ruby app To enable logs in context for APM apps monitored by Ruby: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Ruby agent version, and enable distributed tracing. Use Ruby agent version 6.7.0 or higher for logs in context. For Rails applications, use a supported Rails version. Configure logs in context for Ruby. Standard Ruby on Rails configuration Rails logging is controlled by two components: A logger you can customize by setting config.logger A log formatter you can customize by setting config.log_formatter) In most cases, you should configure logs in context by setting config.log_formatter to the DecoratingFormatter in your Rails application. For more information about Rails configuration, see the rubyonrails.org documentation. In your application's config, require newrelic_rpm, then add the following line: module ________ class Application < Rails::Application ... config.log_formatter = ::NewRelic::Agent::Logging::DecoratingFormatter.new end end Copy This configuration uses the New Relic formatter for log messages, but the remaining configuration is provided by the other Rails settings. Incompatible gems New Relic's decorating logger is known to be incompatible with the following gems: logging semantic logger rails_stdout_logger rails_12factor Rails advanced configuration If setting the log_formatter option doesn't meet your needs, replace the entire Rails logger with an instance of the New Relic logger. Provide the parameters to the logger's constructor, like this: module ________ class Application < Rails::Application ... config.logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( \"log/application.log\", #etc... ) end end Copy Ruby configuration without Rails For non-Rails applications, use the DecoratingLogger in place of the Ruby standard ::Logger, like this: logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( 'log/application.log', #etc... ) ... logger.info(...) Copy The DecoratingLogger is a drop-in replacement for the Ruby standard ::Logger. Their constructors accept the same parameters. Lograge configuration To configure this extension with the lograge gem, follow standard procedures in this doc for Ruby on Rails configuration. No additional configuration is required for the lograge gem. Other logging extensions To use our logging extension with a different logging implementation or with your own custom logger, use the DecoratingFormatter. For example: module ________ class Application < Rails::Application ... config.logger = ::YourCustomLoggerImplementation.new( $stdout, formatter: ::NewRelic::Agent::Logging::DecoratingFormatter.new ) end end Copy To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you have configured your logging in /config/application.rb or in /config/environments/development.rb, run your application locally and check its logging output. You should see some output like this: {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"00fc7d46\",\"timestamp\":1567701375543,\"message\":\"example log message one\",\"log.level\":\"DEBUG\"} {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"6754870b\",\"timestamp\":1567702843604,\"message\":\"example log message two\",\"log.level\":\"DEBUG\"} Copy Troubleshooting If you don't see log data in the UI, follow the troubleshooting procedures. Also, if you see JSON logs in your application's output, but your query does not return logs, check your log forwarder. If the logs from your application are not formatted in JSON with fields like trace.id and span.id, there may be a problem with your log forwarding extension. In this situation: Check that the application is using a supported logging framework. Check that your logging configuration has been applied to all the environments where you want to forward and enrich the log data being sent to New Relic. Check that another logger is not configured later in your application's configuration. Logs in context for Ruby does not support tagged logging. If you are initializing your logger with a log_tags argument, your custom tags may not appear on the final version of your logs. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 285.7118,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Ruby: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "sections": "Ruby: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in <em>context</em> for the Ruby agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Ruby app To <em>enable</em> <em>logs</em>"
      },
      "id": "612d460128ccbc17bf56a863"
    },
    {
      "sections": [
        "Python: Configure logs in context",
        "Set up your Python app",
        "Python StreamHandler example",
        "What's next?"
      ],
      "title": "Python: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Logs in context for Python"
      ],
      "external_id": "a51497a54dbdd8c6ee16e8cf097a090347166d1f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/configure-logs-context-python/",
      "published_at": "2021-10-18T05:57:04Z",
      "updated_at": "2021-10-06T21:52:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Python agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Python app To enable logs in context for APM apps monitored by Python: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Python agent version, and enable distributed tracing. Use Python agent version 5.4.0 or higher for logs in context. Configure logs in context for your log handler. Python StreamHandler example Enabling logs in Python is as simple as instantiating a log formatter and adding it to your log handler. This example uses a StreamHandler which by default writes logs to sys.stderr, but any handler can be used. For more information about configuring log handlers, see the Python.org documentation. # Import the logging module and the New Relic log formatter import logging from newrelic.agent import NewRelicContextFormatter # Instantiate a new log handler handler = logging.StreamHandler() # Instantiate the log formatter and add it to the log handler formatter = NewRelicContextFormatter() handler.setFormatter(formatter) # Get the root logger and add the handler to it root_logger = logging.getLogger() root_logger.addHandler(handler) Copy To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you don't see log data in the UI, follow the troubleshooting procedures. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 196.6475,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Python: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "sections": "Python: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in <em>context</em> for the Python agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Python app To <em>enable</em> <em>logs</em>"
      },
      "id": "612d463564441f2c8042434e"
    },
    {
      "sections": [
        "Java: Configure logs in context",
        "Set up your Java app",
        "Dropwizard 1.3 or higher",
        "java.util.logging",
        "java.util.logging classpath additions",
        "Log4j 1.x",
        "Log4j 2.x",
        "Logback version 1.2.0 or higher",
        "Spring and Springboot",
        "View logs in UI",
        "What's next?"
      ],
      "title": "Java: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Configure logs in context"
      ],
      "external_id": "b9fbeafa564247287e5d6466630d5bdcdb85affb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/java-configure-logs-context-all/",
      "published_at": "2021-10-18T11:15:14Z",
      "updated_at": "2021-10-06T21:51:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Java agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Java app To enable logs in context for APM apps monitored by Java: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Java agent version. Use Java agent version 5.6.0 or higher for logs in context. Enable the JVM argument -javaagent, and enable distributed tracing. Configure logs in context for Java to enrich your log data, using any of the following extensions as applicable. If you use Spring or Spring Boot and aren't sure which extension you need, see our Spring documentation. Dropwizard 1.3 or higher We offer a Dropwizard extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with the DropWizard extension: Make sure you have the Dropwizard 1.3 or higher package installed and working on your application. Use the original Dropwizard appenders and logging factory installed and working on the application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Dropwizard 1.3 extension as applicable: Gradle: Add the following to your build.gradle file: dependencies { compile(\"com.newrelic.logging:dropwizard:2.0\") } Copy Maven: Add the following to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>dropwizard</artifactId> <version>2.0</version> </dependency> </dependencies> Copy Update your Dropwizard .yaml configuration file with a newrelic-json layout, replacing the currently used type: console or type: file with either type: newrelic-console or type: newrelic-file as appropriate. For example: logging: appenders: - type: newrelic-console # Add the two lines below if you don't have a layout specified on the appender. # If you have a layout, remove all parameters to the layout and set the type. layout: type: newrelic-json Copy The New Relic Dropwizard extension also supports a log-format layout type that uses the standard Dropwizard logging. For testing purposes, you can change the type of the layout with a one-line change: logging: appenders: - type: newrelic-file # This format will be ignored by the newrelic-json layout, but used by the log-format layout. logFormat: \"%date{ISO8601} %c %-5p: %m trace.id=%mdc{trace.id} span.id=%mdc{span.id}%n\" layout: # type: newrelic-json type: log-format Copy java.util.logging We offer a java.util.logging extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with with the java.util.logging extension: Make sure you have the java.util.logging package installed and working on the application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the java.util.logging extension as applicable. If you can't edit these files, you can instead add the jars directly to the application classpath. Gradle: Add the following to your build.gradle file: dependencies { compile(\"com.newrelic.logging:jul:2.0\") } Copy Maven: Add the following to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>jul</artifactId> <version>2.0</version> </dependency> </dependencies> Copy Check if your logging file's handlers property is set to something other than NewRelicMemoryHandler. Look for a line listing the root logger's handlers, like this: handlers = java.util.logging.FileHandler Copy Update your logging properties file to set the root logger's handler to NewRelicMemoryHandler so it intercepts messages destined for another handler: handlers = com.newrelic.logging.jul.NewRelicMemoryHandler Copy Configure the NewRelicMemoryHandler by setting the target to the handler that was previously assigned to the root logger, so it captures data New Relic needs on the thread the log message is coming from: com.newrelic.logging.jul.NewRelicMemoryHandler.target = java.util.logging.FileHandler Copy Use a NewRelicFormatter for the final handler. Update your logging properties file to set the formatter property like the following example. Make sure the handler where you set the formatter is the target handler from the previous step (java.util.logging.FileHandler in this example). java.util.logging.FileHandler.formatter = com.newrelic.logging.jul. NewRelicFormatter Copy The New Relic log format is JSON with telemetry metadata we use to correlate transactions and logs together. Currently we do not support any customization of that format. Once complete, JSON is logged instead of text. The JSON should be formatted as single objects, one per line, and should contain fields like log.level and thread.name. The trace.id, which is required for logs in context, should only have a value for log messages that occur within a transaction. java.util.logging classpath additions The most direct way to get the logs-in-context extensions is to add these dependencies to Maven's pom.xml or Gradle's build.gradle. This allows the packaging tools to pick up the correct dependencies. If you can't edit these files, you can instead add the jars directly to the application classpath for your logging framework's configuration. Before you modify the classpath: Enable the JVM argument -javaagent on your app's Java agent. Verify which logging framework the application is using. Make sure you are able to change your logging framework's configuration. Add the following three jars to the classpath if they aren't already present. Generally, we recommend taking the latest versions published on Maven Central. Group ID com.newrelic.logging and Artifact ID: Select the artifact named after your application's logging framework in Maven. Group ID com.fasterxml.jackson.core and Artifact ID: Use jackson-core. Group ID com.newrelic.agent.java and Artifact ID: Use newrelic-api. Log4j 1.x We offer a Log4j 1.x extension extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with with the Log4j 1.x extension, you must configure the Log4j extension in code or via XML. Properties files are not supported because AsyncAppender instances can only be automatically configured via XML. Make sure you have the Log4j 1.x package installed and working on the application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Log4j 1.x extension as applicable. Gradle: Add the following to your build.gradle file: dependencies { compile(\"com.newrelic.logging:log4j1:2.0\") } Copy Maven: Add the following to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>log4j1</artifactId> <version>2.0</version> </dependency> </dependencies> Copy In your logging configuration XML file, update your <appender> element with a NewRelicLayout, adding <layout class=\"com.newrelic.logging.log4j1.NewRelicLayout\"/>: <appender name=\" TypicalFile \" class=\"org.apache.log4j.FileAppender\"> <param name=\"file\" value=\"logs/log4j1-app.log\"/> <param name=\"append\" value=\"false\"/> <layout class=\" com.newrelic.logging.log4j1.NewRelicLayout \"/> <!-- only this line needs to be added --> </appender> Copy Use NewRelicAsyncAppender to wrap any appenders that will target New Relic's log forwarder. For example: <appender name=\" NewRelicFile \" class=\"com.newrelic.logging.log4j1.NewRelicAsyncAppender\"> <appender-ref ref=\" TypicalFile \" /> </appender> Copy Use the async appender on the root logger. For example: <root> <appender-ref ref=\" NewRelicFile \" /> </root> Copy Example configuration file for the Log4j 1.x extension: <?xml version=\"1.0\" encoding=\"UTF-8\" ?> <!DOCTYPE log4j:configuration SYSTEM \"log4j.dtd\"> <log4j:configuration debug=\"false\"> <appender name=\"TypicalFile\" class=\"org.apache.log4j.FileAppender\"> <param name=\"file\" value=\"logs/log4j1-app.log\"/> <param name=\"append\" value=\"false\"/> <!-- layout has been replaced --> <layout class=\"com.newrelic.logging.log4j1.NewRelicLayout\"/> </appender> <!-- this appender was added --> <appender name=\"NewRelicFile\" class=\"com.newrelic.logging.log4j1.NewRelicAsyncAppender\"> <appender-ref ref=\"TypicalFile\" /> </appender> <appender name=\"TypicalConsole\" class=\"org.apache.log4j.ConsoleAppender\"> <layout class=\"org.apache.log4j.PatternLayout\"> <param name=\"ConversionPattern\" value=\"%-5p %c{1} - %m%n\"/> </layout> </appender> <root> ​ <!-- the new appender was used here -->​​ <appender-ref ref=\"NewRelicFile\" /> <appender-ref ref=\"TypicalConsole\" /> </root> </log4j:configuration> Copy Log4j 2.x We offer a Log4j 2.x extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with with the Log4j 2.x extension: Make sure you have the Log4j 2.x or Logs4j 2 binding package installed and working on your application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Log4j 2.x extension as applicable: Gradle: Add the highlighted section to your build.gradle file: dependencies { compile(\"com.newrelic.logging:log4j2:2.0\") } Copy Maven: Add the highlighted section to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>log4j2</artifactId> <version>2.0</version> </dependency> </dependencies> Copy In your logging configuration XML file, update your <configuration> element by adding the highlighted section: <Configuration xmlns=\"http://logging.apache.org/log4j/2.0/config\" packages=\"com.newrelic.logging.log4j2\" > Copy If you're using a properties file, add packages=com.newrelic.logging.log4j2. Add <NewRelicLayout/> to use a NewRelicLayout element within one of the appenders. For example: <File name=\"MyFile\" fileName=\"logs/app-log-file.log\"> <NewRelicLayout/> </File> Copy If you're using a properties file, only change the layout.type: appender.console.type = Console appender.console.name = STDOUT appender.console.layout.type = NewRelicLayout Copy If you only modified an existing appender, skip this step. If you added a new appender, add <AppenderRef/> within <Root> to use this appender. Use the ref attribute to refer to appender name you created in the previous step. For example: <Root level=\"info\"> <AppenderRef ref=\"MyFile\"/> </Root> Copy If you're using a properties file and added a new appender, add: rootLogger.level = info rootLogger.appenderRef.stdout.ref = STDOUT ​​​​​ Copy Logback version 1.2.0 or higher We offer a Logback extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with Logback: Make sure you have Logback version 1.2.0 or higher and the New Relic Java agent version 5.6.0 or higher installed and working on your application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Logback extension as applicable: Gradle: Add the highlighted section to your build.gradle file: dependencies { compile(\"com.newrelic.logging:logback:2.0\") } Copy Maven: Add the highlighted section to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>logback</artifactId> <version>2.0</version> </dependency> </dependencies> Copy Update your logging configuration xml to replace any existing <encoder> element. If you're logging to the console (stdout/stderr), look for ConsoleAppender and replace: <appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> Copy If you're logging to a file, look for FileAppender and replace <encoder>: <appender name=\"LOG_FILE\" class=\"ch.qos.logback.core.FileAppender\"> <file>logs/app-log-file.log</file> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> Copy Update your logging configuration xml with the NewRelicAsyncAppender. To ensure that NewRelicAsyncAppender wraps any appenders that will target New Relic's log forwarder, add the following section. Change \"LOG_FILE\" to the name of the appender you updated in the previous step. <appender name=\"ASYNC\" class=\"com.newrelic.logging.logback.NewRelicAsyncAppender\"> <appender-ref ref=\"LOG_FILE\" /> </appender> Copy Make sure NewRelicAsyncAppender is the first appender used in your logger. Replace your root logger’s appenders with the ASYNC appender created in the previous step. Then list any other appenders after the NewRelicAsyncAppender in the <root> list. <root> <appender-ref ref=\"ASYNC\" /> </root> Copy Here are examples of an updated logging .xml file for the Logback extension. You can also see a working example in GitHub. Single console appender example Example configuration file after adding in the logging extension information: <configuration> <appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"> <!-- changed the encoder --> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> <!-- added the ASYNC appender --> <appender name=\"ASYNC\" class=\"com.newrelic.logging.logback.NewRelicAsyncAppender\"> <appender-ref ref=\"STDOUT\" /> </appender> <root level=\"debug\"> <!-- changed the root logger --> <appender-ref ref=\"ASYNC\" /> </root> </configuration> Copy Two console appenders example This example sends New Relic logging to a file, but still sends standard logging to the console: <configuration> <appender name=\"FILE\" class=\"ch.qos.logback.core.FileAppender\"> <file>myApp.log</file> <!-- encoder changed --> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> <!-- this appender does normal console logging --> <appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder> <pattern>%msg%n</pattern> </encoder> </appender> <!-- The required New Relic ASYNC appender wraps the FILE appender --> <appender name=\"ASYNC\" class=\"com.newrelic.logging.logback.NewRelicAsyncAppender\"> <appender-ref ref=\"FILE\" /> </appender> <root level=\"debug\"> <!-- ASYNC is one of the main appenders --> <appender-ref ref=\"ASYNC\" /> <!-- Send every message to normal console logging, as well. --> <appender-ref ref=\"STDOUT\" /> </root> </configuration> Copy Spring and Springboot We offer extensions for current versions of Spring and Spring Boot. If you already know the logging library, you can skip directly to that documentation: java.util.logging log4j 1 log4j 2 logback The extensions support default configurations only on Spring Boot 2.0 and higher. With Spring Boot: Here are tips to determine which logging library you have: If you have spring-boot-starter-log4j2 in your dependencies, you're using log4j 2.x. Refer to the Spring Boot log4j 2.x documentation for basic configuration, and the New Relic log4j 2 extension for customizing your configuration. If you're using Spring Boot but not the starter-log4j2, you're using logback by default. Refer to Spring Boot logback documentation for basic configuration, and the New Relic logback extension for customizing your configuration. With Spring (but not Spring Boot): Spring 5 or higher: Spring implements a bridge to other logging libraries that will automatically find them. However, those individual libraries must be configured and explicitly included in your project dependencies. To identify your logging dependency, consult your Gradle, Maven, or other build tool's dependency tree. Then follow the procedures to configure logs in context for your Java app with that extension. Spring 4 or lower: Spring version 4 and lower uses Apache Commons Logging for its bridge. Refer to the Spring documentation for information on configuring its bridge. View logs in UI To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you don't see log data in the UI, follow the troubleshooting procedures. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 196.64496,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "sections": "Java: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in <em>context</em> for the Java agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Java app To <em>enable</em> <em>logs</em>"
      },
      "id": "612efca3e7b9d2f718b6f223"
    }
  ],
  "/docs/logs/logs-context/configure-logs-context-php": [
    {
      "sections": [
        "Ruby: Configure logs in context",
        "Set up your Ruby app",
        "Standard Ruby on Rails configuration",
        "Incompatible gems",
        "Rails advanced configuration",
        "Ruby configuration without Rails",
        "Lograge configuration",
        "Other logging extensions",
        "Troubleshooting",
        "What's next?"
      ],
      "title": "Ruby: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Logs in context for Ruby"
      ],
      "external_id": "d2100c0d6ddffe6b52a1f968f61f777a1e56a13c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/configure-logs-context-ruby/",
      "published_at": "2021-10-19T03:54:54Z",
      "updated_at": "2021-10-19T03:54:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Ruby agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Ruby app To enable logs in context for APM apps monitored by Ruby: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Ruby agent version, and enable distributed tracing. Use Ruby agent version 6.7.0 or higher for logs in context. For Rails applications, use a supported Rails version. Configure logs in context for Ruby. Standard Ruby on Rails configuration Rails logging is controlled by two components: A logger you can customize by setting config.logger A log formatter you can customize by setting config.log_formatter) In most cases, you should configure logs in context by setting config.log_formatter to the DecoratingFormatter in your Rails application. For more information about Rails configuration, see the rubyonrails.org documentation. In your application's config, require newrelic_rpm, then add the following line: module ________ class Application < Rails::Application ... config.log_formatter = ::NewRelic::Agent::Logging::DecoratingFormatter.new end end Copy This configuration uses the New Relic formatter for log messages, but the remaining configuration is provided by the other Rails settings. Incompatible gems New Relic's decorating logger is known to be incompatible with the following gems: logging semantic logger rails_stdout_logger rails_12factor Rails advanced configuration If setting the log_formatter option doesn't meet your needs, replace the entire Rails logger with an instance of the New Relic logger. Provide the parameters to the logger's constructor, like this: module ________ class Application < Rails::Application ... config.logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( \"log/application.log\", #etc... ) end end Copy Ruby configuration without Rails For non-Rails applications, use the DecoratingLogger in place of the Ruby standard ::Logger, like this: logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( 'log/application.log', #etc... ) ... logger.info(...) Copy The DecoratingLogger is a drop-in replacement for the Ruby standard ::Logger. Their constructors accept the same parameters. Lograge configuration To configure this extension with the lograge gem, follow standard procedures in this doc for Ruby on Rails configuration. No additional configuration is required for the lograge gem. Other logging extensions To use our logging extension with a different logging implementation or with your own custom logger, use the DecoratingFormatter. For example: module ________ class Application < Rails::Application ... config.logger = ::YourCustomLoggerImplementation.new( $stdout, formatter: ::NewRelic::Agent::Logging::DecoratingFormatter.new ) end end Copy To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you have configured your logging in /config/application.rb or in /config/environments/development.rb, run your application locally and check its logging output. You should see some output like this: {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"00fc7d46\",\"timestamp\":1567701375543,\"message\":\"example log message one\",\"log.level\":\"DEBUG\"} {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"6754870b\",\"timestamp\":1567702843604,\"message\":\"example log message two\",\"log.level\":\"DEBUG\"} Copy Troubleshooting If you don't see log data in the UI, follow the troubleshooting procedures. Also, if you see JSON logs in your application's output, but your query does not return logs, check your log forwarder. If the logs from your application are not formatted in JSON with fields like trace.id and span.id, there may be a problem with your log forwarding extension. In this situation: Check that the application is using a supported logging framework. Check that your logging configuration has been applied to all the environments where you want to forward and enrich the log data being sent to New Relic. Check that another logger is not configured later in your application's configuration. Logs in context for Ruby does not support tagged logging. If you are initializing your logger with a log_tags argument, your custom tags may not appear on the final version of your logs. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 285.7118,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Ruby: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "sections": "Ruby: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in <em>context</em> for the Ruby agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Ruby app To <em>enable</em> <em>logs</em>"
      },
      "id": "612d460128ccbc17bf56a863"
    },
    {
      "sections": [
        "Python: Configure logs in context",
        "Set up your Python app",
        "Python StreamHandler example",
        "What's next?"
      ],
      "title": "Python: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Logs in context for Python"
      ],
      "external_id": "a51497a54dbdd8c6ee16e8cf097a090347166d1f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/configure-logs-context-python/",
      "published_at": "2021-10-18T05:57:04Z",
      "updated_at": "2021-10-06T21:52:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Python agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Python app To enable logs in context for APM apps monitored by Python: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Python agent version, and enable distributed tracing. Use Python agent version 5.4.0 or higher for logs in context. Configure logs in context for your log handler. Python StreamHandler example Enabling logs in Python is as simple as instantiating a log formatter and adding it to your log handler. This example uses a StreamHandler which by default writes logs to sys.stderr, but any handler can be used. For more information about configuring log handlers, see the Python.org documentation. # Import the logging module and the New Relic log formatter import logging from newrelic.agent import NewRelicContextFormatter # Instantiate a new log handler handler = logging.StreamHandler() # Instantiate the log formatter and add it to the log handler formatter = NewRelicContextFormatter() handler.setFormatter(formatter) # Get the root logger and add the handler to it root_logger = logging.getLogger() root_logger.addHandler(handler) Copy To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you don't see log data in the UI, follow the troubleshooting procedures. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 196.6475,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Python: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "sections": "Python: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in <em>context</em> for the Python agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Python app To <em>enable</em> <em>logs</em>"
      },
      "id": "612d463564441f2c8042434e"
    },
    {
      "sections": [
        "Java: Configure logs in context",
        "Set up your Java app",
        "Dropwizard 1.3 or higher",
        "java.util.logging",
        "java.util.logging classpath additions",
        "Log4j 1.x",
        "Log4j 2.x",
        "Logback version 1.2.0 or higher",
        "Spring and Springboot",
        "View logs in UI",
        "What's next?"
      ],
      "title": "Java: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Configure logs in context"
      ],
      "external_id": "b9fbeafa564247287e5d6466630d5bdcdb85affb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/java-configure-logs-context-all/",
      "published_at": "2021-10-18T11:15:14Z",
      "updated_at": "2021-10-06T21:51:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Java agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Java app To enable logs in context for APM apps monitored by Java: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Java agent version. Use Java agent version 5.6.0 or higher for logs in context. Enable the JVM argument -javaagent, and enable distributed tracing. Configure logs in context for Java to enrich your log data, using any of the following extensions as applicable. If you use Spring or Spring Boot and aren't sure which extension you need, see our Spring documentation. Dropwizard 1.3 or higher We offer a Dropwizard extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with the DropWizard extension: Make sure you have the Dropwizard 1.3 or higher package installed and working on your application. Use the original Dropwizard appenders and logging factory installed and working on the application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Dropwizard 1.3 extension as applicable: Gradle: Add the following to your build.gradle file: dependencies { compile(\"com.newrelic.logging:dropwizard:2.0\") } Copy Maven: Add the following to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>dropwizard</artifactId> <version>2.0</version> </dependency> </dependencies> Copy Update your Dropwizard .yaml configuration file with a newrelic-json layout, replacing the currently used type: console or type: file with either type: newrelic-console or type: newrelic-file as appropriate. For example: logging: appenders: - type: newrelic-console # Add the two lines below if you don't have a layout specified on the appender. # If you have a layout, remove all parameters to the layout and set the type. layout: type: newrelic-json Copy The New Relic Dropwizard extension also supports a log-format layout type that uses the standard Dropwizard logging. For testing purposes, you can change the type of the layout with a one-line change: logging: appenders: - type: newrelic-file # This format will be ignored by the newrelic-json layout, but used by the log-format layout. logFormat: \"%date{ISO8601} %c %-5p: %m trace.id=%mdc{trace.id} span.id=%mdc{span.id}%n\" layout: # type: newrelic-json type: log-format Copy java.util.logging We offer a java.util.logging extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with with the java.util.logging extension: Make sure you have the java.util.logging package installed and working on the application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the java.util.logging extension as applicable. If you can't edit these files, you can instead add the jars directly to the application classpath. Gradle: Add the following to your build.gradle file: dependencies { compile(\"com.newrelic.logging:jul:2.0\") } Copy Maven: Add the following to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>jul</artifactId> <version>2.0</version> </dependency> </dependencies> Copy Check if your logging file's handlers property is set to something other than NewRelicMemoryHandler. Look for a line listing the root logger's handlers, like this: handlers = java.util.logging.FileHandler Copy Update your logging properties file to set the root logger's handler to NewRelicMemoryHandler so it intercepts messages destined for another handler: handlers = com.newrelic.logging.jul.NewRelicMemoryHandler Copy Configure the NewRelicMemoryHandler by setting the target to the handler that was previously assigned to the root logger, so it captures data New Relic needs on the thread the log message is coming from: com.newrelic.logging.jul.NewRelicMemoryHandler.target = java.util.logging.FileHandler Copy Use a NewRelicFormatter for the final handler. Update your logging properties file to set the formatter property like the following example. Make sure the handler where you set the formatter is the target handler from the previous step (java.util.logging.FileHandler in this example). java.util.logging.FileHandler.formatter = com.newrelic.logging.jul. NewRelicFormatter Copy The New Relic log format is JSON with telemetry metadata we use to correlate transactions and logs together. Currently we do not support any customization of that format. Once complete, JSON is logged instead of text. The JSON should be formatted as single objects, one per line, and should contain fields like log.level and thread.name. The trace.id, which is required for logs in context, should only have a value for log messages that occur within a transaction. java.util.logging classpath additions The most direct way to get the logs-in-context extensions is to add these dependencies to Maven's pom.xml or Gradle's build.gradle. This allows the packaging tools to pick up the correct dependencies. If you can't edit these files, you can instead add the jars directly to the application classpath for your logging framework's configuration. Before you modify the classpath: Enable the JVM argument -javaagent on your app's Java agent. Verify which logging framework the application is using. Make sure you are able to change your logging framework's configuration. Add the following three jars to the classpath if they aren't already present. Generally, we recommend taking the latest versions published on Maven Central. Group ID com.newrelic.logging and Artifact ID: Select the artifact named after your application's logging framework in Maven. Group ID com.fasterxml.jackson.core and Artifact ID: Use jackson-core. Group ID com.newrelic.agent.java and Artifact ID: Use newrelic-api. Log4j 1.x We offer a Log4j 1.x extension extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with with the Log4j 1.x extension, you must configure the Log4j extension in code or via XML. Properties files are not supported because AsyncAppender instances can only be automatically configured via XML. Make sure you have the Log4j 1.x package installed and working on the application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Log4j 1.x extension as applicable. Gradle: Add the following to your build.gradle file: dependencies { compile(\"com.newrelic.logging:log4j1:2.0\") } Copy Maven: Add the following to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>log4j1</artifactId> <version>2.0</version> </dependency> </dependencies> Copy In your logging configuration XML file, update your <appender> element with a NewRelicLayout, adding <layout class=\"com.newrelic.logging.log4j1.NewRelicLayout\"/>: <appender name=\" TypicalFile \" class=\"org.apache.log4j.FileAppender\"> <param name=\"file\" value=\"logs/log4j1-app.log\"/> <param name=\"append\" value=\"false\"/> <layout class=\" com.newrelic.logging.log4j1.NewRelicLayout \"/> <!-- only this line needs to be added --> </appender> Copy Use NewRelicAsyncAppender to wrap any appenders that will target New Relic's log forwarder. For example: <appender name=\" NewRelicFile \" class=\"com.newrelic.logging.log4j1.NewRelicAsyncAppender\"> <appender-ref ref=\" TypicalFile \" /> </appender> Copy Use the async appender on the root logger. For example: <root> <appender-ref ref=\" NewRelicFile \" /> </root> Copy Example configuration file for the Log4j 1.x extension: <?xml version=\"1.0\" encoding=\"UTF-8\" ?> <!DOCTYPE log4j:configuration SYSTEM \"log4j.dtd\"> <log4j:configuration debug=\"false\"> <appender name=\"TypicalFile\" class=\"org.apache.log4j.FileAppender\"> <param name=\"file\" value=\"logs/log4j1-app.log\"/> <param name=\"append\" value=\"false\"/> <!-- layout has been replaced --> <layout class=\"com.newrelic.logging.log4j1.NewRelicLayout\"/> </appender> <!-- this appender was added --> <appender name=\"NewRelicFile\" class=\"com.newrelic.logging.log4j1.NewRelicAsyncAppender\"> <appender-ref ref=\"TypicalFile\" /> </appender> <appender name=\"TypicalConsole\" class=\"org.apache.log4j.ConsoleAppender\"> <layout class=\"org.apache.log4j.PatternLayout\"> <param name=\"ConversionPattern\" value=\"%-5p %c{1} - %m%n\"/> </layout> </appender> <root> ​ <!-- the new appender was used here -->​​ <appender-ref ref=\"NewRelicFile\" /> <appender-ref ref=\"TypicalConsole\" /> </root> </log4j:configuration> Copy Log4j 2.x We offer a Log4j 2.x extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with with the Log4j 2.x extension: Make sure you have the Log4j 2.x or Logs4j 2 binding package installed and working on your application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Log4j 2.x extension as applicable: Gradle: Add the highlighted section to your build.gradle file: dependencies { compile(\"com.newrelic.logging:log4j2:2.0\") } Copy Maven: Add the highlighted section to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>log4j2</artifactId> <version>2.0</version> </dependency> </dependencies> Copy In your logging configuration XML file, update your <configuration> element by adding the highlighted section: <Configuration xmlns=\"http://logging.apache.org/log4j/2.0/config\" packages=\"com.newrelic.logging.log4j2\" > Copy If you're using a properties file, add packages=com.newrelic.logging.log4j2. Add <NewRelicLayout/> to use a NewRelicLayout element within one of the appenders. For example: <File name=\"MyFile\" fileName=\"logs/app-log-file.log\"> <NewRelicLayout/> </File> Copy If you're using a properties file, only change the layout.type: appender.console.type = Console appender.console.name = STDOUT appender.console.layout.type = NewRelicLayout Copy If you only modified an existing appender, skip this step. If you added a new appender, add <AppenderRef/> within <Root> to use this appender. Use the ref attribute to refer to appender name you created in the previous step. For example: <Root level=\"info\"> <AppenderRef ref=\"MyFile\"/> </Root> Copy If you're using a properties file and added a new appender, add: rootLogger.level = info rootLogger.appenderRef.stdout.ref = STDOUT ​​​​​ Copy Logback version 1.2.0 or higher We offer a Logback extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with Logback: Make sure you have Logback version 1.2.0 or higher and the New Relic Java agent version 5.6.0 or higher installed and working on your application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Logback extension as applicable: Gradle: Add the highlighted section to your build.gradle file: dependencies { compile(\"com.newrelic.logging:logback:2.0\") } Copy Maven: Add the highlighted section to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>logback</artifactId> <version>2.0</version> </dependency> </dependencies> Copy Update your logging configuration xml to replace any existing <encoder> element. If you're logging to the console (stdout/stderr), look for ConsoleAppender and replace: <appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> Copy If you're logging to a file, look for FileAppender and replace <encoder>: <appender name=\"LOG_FILE\" class=\"ch.qos.logback.core.FileAppender\"> <file>logs/app-log-file.log</file> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> Copy Update your logging configuration xml with the NewRelicAsyncAppender. To ensure that NewRelicAsyncAppender wraps any appenders that will target New Relic's log forwarder, add the following section. Change \"LOG_FILE\" to the name of the appender you updated in the previous step. <appender name=\"ASYNC\" class=\"com.newrelic.logging.logback.NewRelicAsyncAppender\"> <appender-ref ref=\"LOG_FILE\" /> </appender> Copy Make sure NewRelicAsyncAppender is the first appender used in your logger. Replace your root logger’s appenders with the ASYNC appender created in the previous step. Then list any other appenders after the NewRelicAsyncAppender in the <root> list. <root> <appender-ref ref=\"ASYNC\" /> </root> Copy Here are examples of an updated logging .xml file for the Logback extension. You can also see a working example in GitHub. Single console appender example Example configuration file after adding in the logging extension information: <configuration> <appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"> <!-- changed the encoder --> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> <!-- added the ASYNC appender --> <appender name=\"ASYNC\" class=\"com.newrelic.logging.logback.NewRelicAsyncAppender\"> <appender-ref ref=\"STDOUT\" /> </appender> <root level=\"debug\"> <!-- changed the root logger --> <appender-ref ref=\"ASYNC\" /> </root> </configuration> Copy Two console appenders example This example sends New Relic logging to a file, but still sends standard logging to the console: <configuration> <appender name=\"FILE\" class=\"ch.qos.logback.core.FileAppender\"> <file>myApp.log</file> <!-- encoder changed --> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> <!-- this appender does normal console logging --> <appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder> <pattern>%msg%n</pattern> </encoder> </appender> <!-- The required New Relic ASYNC appender wraps the FILE appender --> <appender name=\"ASYNC\" class=\"com.newrelic.logging.logback.NewRelicAsyncAppender\"> <appender-ref ref=\"FILE\" /> </appender> <root level=\"debug\"> <!-- ASYNC is one of the main appenders --> <appender-ref ref=\"ASYNC\" /> <!-- Send every message to normal console logging, as well. --> <appender-ref ref=\"STDOUT\" /> </root> </configuration> Copy Spring and Springboot We offer extensions for current versions of Spring and Spring Boot. If you already know the logging library, you can skip directly to that documentation: java.util.logging log4j 1 log4j 2 logback The extensions support default configurations only on Spring Boot 2.0 and higher. With Spring Boot: Here are tips to determine which logging library you have: If you have spring-boot-starter-log4j2 in your dependencies, you're using log4j 2.x. Refer to the Spring Boot log4j 2.x documentation for basic configuration, and the New Relic log4j 2 extension for customizing your configuration. If you're using Spring Boot but not the starter-log4j2, you're using logback by default. Refer to Spring Boot logback documentation for basic configuration, and the New Relic logback extension for customizing your configuration. With Spring (but not Spring Boot): Spring 5 or higher: Spring implements a bridge to other logging libraries that will automatically find them. However, those individual libraries must be configured and explicitly included in your project dependencies. To identify your logging dependency, consult your Gradle, Maven, or other build tool's dependency tree. Then follow the procedures to configure logs in context for your Java app with that extension. Spring 4 or lower: Spring version 4 and lower uses Apache Commons Logging for its bridge. Refer to the Spring documentation for information on configuring its bridge. View logs in UI To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you don't see log data in the UI, follow the troubleshooting procedures. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 196.64496,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "sections": "Java: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in <em>context</em> for the Java agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Java app To <em>enable</em> <em>logs</em>"
      },
      "id": "612efca3e7b9d2f718b6f223"
    }
  ],
  "/docs/logs/logs-context/configure-logs-context-python": [
    {
      "sections": [
        "Ruby: Configure logs in context",
        "Set up your Ruby app",
        "Standard Ruby on Rails configuration",
        "Incompatible gems",
        "Rails advanced configuration",
        "Ruby configuration without Rails",
        "Lograge configuration",
        "Other logging extensions",
        "Troubleshooting",
        "What's next?"
      ],
      "title": "Ruby: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Logs in context for Ruby"
      ],
      "external_id": "d2100c0d6ddffe6b52a1f968f61f777a1e56a13c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/configure-logs-context-ruby/",
      "published_at": "2021-10-19T03:54:54Z",
      "updated_at": "2021-10-19T03:54:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Ruby agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Ruby app To enable logs in context for APM apps monitored by Ruby: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Ruby agent version, and enable distributed tracing. Use Ruby agent version 6.7.0 or higher for logs in context. For Rails applications, use a supported Rails version. Configure logs in context for Ruby. Standard Ruby on Rails configuration Rails logging is controlled by two components: A logger you can customize by setting config.logger A log formatter you can customize by setting config.log_formatter) In most cases, you should configure logs in context by setting config.log_formatter to the DecoratingFormatter in your Rails application. For more information about Rails configuration, see the rubyonrails.org documentation. In your application's config, require newrelic_rpm, then add the following line: module ________ class Application < Rails::Application ... config.log_formatter = ::NewRelic::Agent::Logging::DecoratingFormatter.new end end Copy This configuration uses the New Relic formatter for log messages, but the remaining configuration is provided by the other Rails settings. Incompatible gems New Relic's decorating logger is known to be incompatible with the following gems: logging semantic logger rails_stdout_logger rails_12factor Rails advanced configuration If setting the log_formatter option doesn't meet your needs, replace the entire Rails logger with an instance of the New Relic logger. Provide the parameters to the logger's constructor, like this: module ________ class Application < Rails::Application ... config.logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( \"log/application.log\", #etc... ) end end Copy Ruby configuration without Rails For non-Rails applications, use the DecoratingLogger in place of the Ruby standard ::Logger, like this: logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( 'log/application.log', #etc... ) ... logger.info(...) Copy The DecoratingLogger is a drop-in replacement for the Ruby standard ::Logger. Their constructors accept the same parameters. Lograge configuration To configure this extension with the lograge gem, follow standard procedures in this doc for Ruby on Rails configuration. No additional configuration is required for the lograge gem. Other logging extensions To use our logging extension with a different logging implementation or with your own custom logger, use the DecoratingFormatter. For example: module ________ class Application < Rails::Application ... config.logger = ::YourCustomLoggerImplementation.new( $stdout, formatter: ::NewRelic::Agent::Logging::DecoratingFormatter.new ) end end Copy To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you have configured your logging in /config/application.rb or in /config/environments/development.rb, run your application locally and check its logging output. You should see some output like this: {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"00fc7d46\",\"timestamp\":1567701375543,\"message\":\"example log message one\",\"log.level\":\"DEBUG\"} {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"6754870b\",\"timestamp\":1567702843604,\"message\":\"example log message two\",\"log.level\":\"DEBUG\"} Copy Troubleshooting If you don't see log data in the UI, follow the troubleshooting procedures. Also, if you see JSON logs in your application's output, but your query does not return logs, check your log forwarder. If the logs from your application are not formatted in JSON with fields like trace.id and span.id, there may be a problem with your log forwarding extension. In this situation: Check that the application is using a supported logging framework. Check that your logging configuration has been applied to all the environments where you want to forward and enrich the log data being sent to New Relic. Check that another logger is not configured later in your application's configuration. Logs in context for Ruby does not support tagged logging. If you are initializing your logger with a log_tags argument, your custom tags may not appear on the final version of your logs. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 285.7116,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Ruby: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "sections": "Ruby: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in <em>context</em> for the Ruby agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Ruby app To <em>enable</em> <em>logs</em>"
      },
      "id": "612d460128ccbc17bf56a863"
    },
    {
      "sections": [
        "Java: Configure logs in context",
        "Set up your Java app",
        "Dropwizard 1.3 or higher",
        "java.util.logging",
        "java.util.logging classpath additions",
        "Log4j 1.x",
        "Log4j 2.x",
        "Logback version 1.2.0 or higher",
        "Spring and Springboot",
        "View logs in UI",
        "What's next?"
      ],
      "title": "Java: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Configure logs in context"
      ],
      "external_id": "b9fbeafa564247287e5d6466630d5bdcdb85affb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/java-configure-logs-context-all/",
      "published_at": "2021-10-18T11:15:14Z",
      "updated_at": "2021-10-06T21:51:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Java agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Java app To enable logs in context for APM apps monitored by Java: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Java agent version. Use Java agent version 5.6.0 or higher for logs in context. Enable the JVM argument -javaagent, and enable distributed tracing. Configure logs in context for Java to enrich your log data, using any of the following extensions as applicable. If you use Spring or Spring Boot and aren't sure which extension you need, see our Spring documentation. Dropwizard 1.3 or higher We offer a Dropwizard extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with the DropWizard extension: Make sure you have the Dropwizard 1.3 or higher package installed and working on your application. Use the original Dropwizard appenders and logging factory installed and working on the application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Dropwizard 1.3 extension as applicable: Gradle: Add the following to your build.gradle file: dependencies { compile(\"com.newrelic.logging:dropwizard:2.0\") } Copy Maven: Add the following to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>dropwizard</artifactId> <version>2.0</version> </dependency> </dependencies> Copy Update your Dropwizard .yaml configuration file with a newrelic-json layout, replacing the currently used type: console or type: file with either type: newrelic-console or type: newrelic-file as appropriate. For example: logging: appenders: - type: newrelic-console # Add the two lines below if you don't have a layout specified on the appender. # If you have a layout, remove all parameters to the layout and set the type. layout: type: newrelic-json Copy The New Relic Dropwizard extension also supports a log-format layout type that uses the standard Dropwizard logging. For testing purposes, you can change the type of the layout with a one-line change: logging: appenders: - type: newrelic-file # This format will be ignored by the newrelic-json layout, but used by the log-format layout. logFormat: \"%date{ISO8601} %c %-5p: %m trace.id=%mdc{trace.id} span.id=%mdc{span.id}%n\" layout: # type: newrelic-json type: log-format Copy java.util.logging We offer a java.util.logging extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with with the java.util.logging extension: Make sure you have the java.util.logging package installed and working on the application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the java.util.logging extension as applicable. If you can't edit these files, you can instead add the jars directly to the application classpath. Gradle: Add the following to your build.gradle file: dependencies { compile(\"com.newrelic.logging:jul:2.0\") } Copy Maven: Add the following to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>jul</artifactId> <version>2.0</version> </dependency> </dependencies> Copy Check if your logging file's handlers property is set to something other than NewRelicMemoryHandler. Look for a line listing the root logger's handlers, like this: handlers = java.util.logging.FileHandler Copy Update your logging properties file to set the root logger's handler to NewRelicMemoryHandler so it intercepts messages destined for another handler: handlers = com.newrelic.logging.jul.NewRelicMemoryHandler Copy Configure the NewRelicMemoryHandler by setting the target to the handler that was previously assigned to the root logger, so it captures data New Relic needs on the thread the log message is coming from: com.newrelic.logging.jul.NewRelicMemoryHandler.target = java.util.logging.FileHandler Copy Use a NewRelicFormatter for the final handler. Update your logging properties file to set the formatter property like the following example. Make sure the handler where you set the formatter is the target handler from the previous step (java.util.logging.FileHandler in this example). java.util.logging.FileHandler.formatter = com.newrelic.logging.jul. NewRelicFormatter Copy The New Relic log format is JSON with telemetry metadata we use to correlate transactions and logs together. Currently we do not support any customization of that format. Once complete, JSON is logged instead of text. The JSON should be formatted as single objects, one per line, and should contain fields like log.level and thread.name. The trace.id, which is required for logs in context, should only have a value for log messages that occur within a transaction. java.util.logging classpath additions The most direct way to get the logs-in-context extensions is to add these dependencies to Maven's pom.xml or Gradle's build.gradle. This allows the packaging tools to pick up the correct dependencies. If you can't edit these files, you can instead add the jars directly to the application classpath for your logging framework's configuration. Before you modify the classpath: Enable the JVM argument -javaagent on your app's Java agent. Verify which logging framework the application is using. Make sure you are able to change your logging framework's configuration. Add the following three jars to the classpath if they aren't already present. Generally, we recommend taking the latest versions published on Maven Central. Group ID com.newrelic.logging and Artifact ID: Select the artifact named after your application's logging framework in Maven. Group ID com.fasterxml.jackson.core and Artifact ID: Use jackson-core. Group ID com.newrelic.agent.java and Artifact ID: Use newrelic-api. Log4j 1.x We offer a Log4j 1.x extension extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with with the Log4j 1.x extension, you must configure the Log4j extension in code or via XML. Properties files are not supported because AsyncAppender instances can only be automatically configured via XML. Make sure you have the Log4j 1.x package installed and working on the application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Log4j 1.x extension as applicable. Gradle: Add the following to your build.gradle file: dependencies { compile(\"com.newrelic.logging:log4j1:2.0\") } Copy Maven: Add the following to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>log4j1</artifactId> <version>2.0</version> </dependency> </dependencies> Copy In your logging configuration XML file, update your <appender> element with a NewRelicLayout, adding <layout class=\"com.newrelic.logging.log4j1.NewRelicLayout\"/>: <appender name=\" TypicalFile \" class=\"org.apache.log4j.FileAppender\"> <param name=\"file\" value=\"logs/log4j1-app.log\"/> <param name=\"append\" value=\"false\"/> <layout class=\" com.newrelic.logging.log4j1.NewRelicLayout \"/> <!-- only this line needs to be added --> </appender> Copy Use NewRelicAsyncAppender to wrap any appenders that will target New Relic's log forwarder. For example: <appender name=\" NewRelicFile \" class=\"com.newrelic.logging.log4j1.NewRelicAsyncAppender\"> <appender-ref ref=\" TypicalFile \" /> </appender> Copy Use the async appender on the root logger. For example: <root> <appender-ref ref=\" NewRelicFile \" /> </root> Copy Example configuration file for the Log4j 1.x extension: <?xml version=\"1.0\" encoding=\"UTF-8\" ?> <!DOCTYPE log4j:configuration SYSTEM \"log4j.dtd\"> <log4j:configuration debug=\"false\"> <appender name=\"TypicalFile\" class=\"org.apache.log4j.FileAppender\"> <param name=\"file\" value=\"logs/log4j1-app.log\"/> <param name=\"append\" value=\"false\"/> <!-- layout has been replaced --> <layout class=\"com.newrelic.logging.log4j1.NewRelicLayout\"/> </appender> <!-- this appender was added --> <appender name=\"NewRelicFile\" class=\"com.newrelic.logging.log4j1.NewRelicAsyncAppender\"> <appender-ref ref=\"TypicalFile\" /> </appender> <appender name=\"TypicalConsole\" class=\"org.apache.log4j.ConsoleAppender\"> <layout class=\"org.apache.log4j.PatternLayout\"> <param name=\"ConversionPattern\" value=\"%-5p %c{1} - %m%n\"/> </layout> </appender> <root> ​ <!-- the new appender was used here -->​​ <appender-ref ref=\"NewRelicFile\" /> <appender-ref ref=\"TypicalConsole\" /> </root> </log4j:configuration> Copy Log4j 2.x We offer a Log4j 2.x extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with with the Log4j 2.x extension: Make sure you have the Log4j 2.x or Logs4j 2 binding package installed and working on your application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Log4j 2.x extension as applicable: Gradle: Add the highlighted section to your build.gradle file: dependencies { compile(\"com.newrelic.logging:log4j2:2.0\") } Copy Maven: Add the highlighted section to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>log4j2</artifactId> <version>2.0</version> </dependency> </dependencies> Copy In your logging configuration XML file, update your <configuration> element by adding the highlighted section: <Configuration xmlns=\"http://logging.apache.org/log4j/2.0/config\" packages=\"com.newrelic.logging.log4j2\" > Copy If you're using a properties file, add packages=com.newrelic.logging.log4j2. Add <NewRelicLayout/> to use a NewRelicLayout element within one of the appenders. For example: <File name=\"MyFile\" fileName=\"logs/app-log-file.log\"> <NewRelicLayout/> </File> Copy If you're using a properties file, only change the layout.type: appender.console.type = Console appender.console.name = STDOUT appender.console.layout.type = NewRelicLayout Copy If you only modified an existing appender, skip this step. If you added a new appender, add <AppenderRef/> within <Root> to use this appender. Use the ref attribute to refer to appender name you created in the previous step. For example: <Root level=\"info\"> <AppenderRef ref=\"MyFile\"/> </Root> Copy If you're using a properties file and added a new appender, add: rootLogger.level = info rootLogger.appenderRef.stdout.ref = STDOUT ​​​​​ Copy Logback version 1.2.0 or higher We offer a Logback extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with Logback: Make sure you have Logback version 1.2.0 or higher and the New Relic Java agent version 5.6.0 or higher installed and working on your application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Logback extension as applicable: Gradle: Add the highlighted section to your build.gradle file: dependencies { compile(\"com.newrelic.logging:logback:2.0\") } Copy Maven: Add the highlighted section to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>logback</artifactId> <version>2.0</version> </dependency> </dependencies> Copy Update your logging configuration xml to replace any existing <encoder> element. If you're logging to the console (stdout/stderr), look for ConsoleAppender and replace: <appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> Copy If you're logging to a file, look for FileAppender and replace <encoder>: <appender name=\"LOG_FILE\" class=\"ch.qos.logback.core.FileAppender\"> <file>logs/app-log-file.log</file> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> Copy Update your logging configuration xml with the NewRelicAsyncAppender. To ensure that NewRelicAsyncAppender wraps any appenders that will target New Relic's log forwarder, add the following section. Change \"LOG_FILE\" to the name of the appender you updated in the previous step. <appender name=\"ASYNC\" class=\"com.newrelic.logging.logback.NewRelicAsyncAppender\"> <appender-ref ref=\"LOG_FILE\" /> </appender> Copy Make sure NewRelicAsyncAppender is the first appender used in your logger. Replace your root logger’s appenders with the ASYNC appender created in the previous step. Then list any other appenders after the NewRelicAsyncAppender in the <root> list. <root> <appender-ref ref=\"ASYNC\" /> </root> Copy Here are examples of an updated logging .xml file for the Logback extension. You can also see a working example in GitHub. Single console appender example Example configuration file after adding in the logging extension information: <configuration> <appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"> <!-- changed the encoder --> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> <!-- added the ASYNC appender --> <appender name=\"ASYNC\" class=\"com.newrelic.logging.logback.NewRelicAsyncAppender\"> <appender-ref ref=\"STDOUT\" /> </appender> <root level=\"debug\"> <!-- changed the root logger --> <appender-ref ref=\"ASYNC\" /> </root> </configuration> Copy Two console appenders example This example sends New Relic logging to a file, but still sends standard logging to the console: <configuration> <appender name=\"FILE\" class=\"ch.qos.logback.core.FileAppender\"> <file>myApp.log</file> <!-- encoder changed --> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> <!-- this appender does normal console logging --> <appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder> <pattern>%msg%n</pattern> </encoder> </appender> <!-- The required New Relic ASYNC appender wraps the FILE appender --> <appender name=\"ASYNC\" class=\"com.newrelic.logging.logback.NewRelicAsyncAppender\"> <appender-ref ref=\"FILE\" /> </appender> <root level=\"debug\"> <!-- ASYNC is one of the main appenders --> <appender-ref ref=\"ASYNC\" /> <!-- Send every message to normal console logging, as well. --> <appender-ref ref=\"STDOUT\" /> </root> </configuration> Copy Spring and Springboot We offer extensions for current versions of Spring and Spring Boot. If you already know the logging library, you can skip directly to that documentation: java.util.logging log4j 1 log4j 2 logback The extensions support default configurations only on Spring Boot 2.0 and higher. With Spring Boot: Here are tips to determine which logging library you have: If you have spring-boot-starter-log4j2 in your dependencies, you're using log4j 2.x. Refer to the Spring Boot log4j 2.x documentation for basic configuration, and the New Relic log4j 2 extension for customizing your configuration. If you're using Spring Boot but not the starter-log4j2, you're using logback by default. Refer to Spring Boot logback documentation for basic configuration, and the New Relic logback extension for customizing your configuration. With Spring (but not Spring Boot): Spring 5 or higher: Spring implements a bridge to other logging libraries that will automatically find them. However, those individual libraries must be configured and explicitly included in your project dependencies. To identify your logging dependency, consult your Gradle, Maven, or other build tool's dependency tree. Then follow the procedures to configure logs in context for your Java app with that extension. Spring 4 or lower: Spring version 4 and lower uses Apache Commons Logging for its bridge. Refer to the Spring documentation for information on configuring its bridge. View logs in UI To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you don't see log data in the UI, follow the troubleshooting procedures. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 196.64491,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "sections": "Java: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in <em>context</em> for the Java agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Java app To <em>enable</em> <em>logs</em>"
      },
      "id": "612efca3e7b9d2f718b6f223"
    },
    {
      "sections": [
        "C SDK: Configure logs in context"
      ],
      "title": "C SDK: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Configure logs in context"
      ],
      "external_id": "87eeada741d7f22204b37e7d6a906b9546ab8d4a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/c-sdk-configure-logs-context/",
      "published_at": "2021-10-18T05:57:03Z",
      "updated_at": "2021-10-06T21:51:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context is not available for apps monitored by the C SDK. Instead, you can build your own logging extension library by using the Log API. Make sure your app has distributed tracing enabled, so you can quickly understand what happens to requests as they travel through upstream and downstream services. To enable distributed tracing for apps monitored by the C SDK, install or update to the latest C SDK version. Distributed tracing requires C SDK version 1.1.0 or higher.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 196.64487,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "C SDK: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "sections": "C SDK: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in <em>context</em> is not available for apps monitored by the C SDK. Instead, you can build your own logging extension library by using the <em>Log</em> API. Make sure your app has distributed tracing enabled, so you can quickly understand what happens to requests as they travel through upstream and downstream"
      },
      "id": "6127279b28ccbcf0c6f2618d"
    }
  ],
  "/docs/logs/logs-context/configure-logs-context-ruby": [
    {
      "sections": [
        "Python: Configure logs in context",
        "Set up your Python app",
        "Python StreamHandler example",
        "What's next?"
      ],
      "title": "Python: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Logs in context for Python"
      ],
      "external_id": "a51497a54dbdd8c6ee16e8cf097a090347166d1f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/configure-logs-context-python/",
      "published_at": "2021-10-18T05:57:04Z",
      "updated_at": "2021-10-06T21:52:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Python agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Python app To enable logs in context for APM apps monitored by Python: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Python agent version, and enable distributed tracing. Use Python agent version 5.4.0 or higher for logs in context. Configure logs in context for your log handler. Python StreamHandler example Enabling logs in Python is as simple as instantiating a log formatter and adding it to your log handler. This example uses a StreamHandler which by default writes logs to sys.stderr, but any handler can be used. For more information about configuring log handlers, see the Python.org documentation. # Import the logging module and the New Relic log formatter import logging from newrelic.agent import NewRelicContextFormatter # Instantiate a new log handler handler = logging.StreamHandler() # Instantiate the log formatter and add it to the log handler formatter = NewRelicContextFormatter() handler.setFormatter(formatter) # Get the root logger and add the handler to it root_logger = logging.getLogger() root_logger.addHandler(handler) Copy To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you don't see log data in the UI, follow the troubleshooting procedures. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 196.64746,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Python: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "sections": "Python: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in <em>context</em> for the Python agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Python app To <em>enable</em> <em>logs</em>"
      },
      "id": "612d463564441f2c8042434e"
    },
    {
      "sections": [
        "Java: Configure logs in context",
        "Set up your Java app",
        "Dropwizard 1.3 or higher",
        "java.util.logging",
        "java.util.logging classpath additions",
        "Log4j 1.x",
        "Log4j 2.x",
        "Logback version 1.2.0 or higher",
        "Spring and Springboot",
        "View logs in UI",
        "What's next?"
      ],
      "title": "Java: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Configure logs in context"
      ],
      "external_id": "b9fbeafa564247287e5d6466630d5bdcdb85affb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/java-configure-logs-context-all/",
      "published_at": "2021-10-18T11:15:14Z",
      "updated_at": "2021-10-06T21:51:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Java agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Java app To enable logs in context for APM apps monitored by Java: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Java agent version. Use Java agent version 5.6.0 or higher for logs in context. Enable the JVM argument -javaagent, and enable distributed tracing. Configure logs in context for Java to enrich your log data, using any of the following extensions as applicable. If you use Spring or Spring Boot and aren't sure which extension you need, see our Spring documentation. Dropwizard 1.3 or higher We offer a Dropwizard extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with the DropWizard extension: Make sure you have the Dropwizard 1.3 or higher package installed and working on your application. Use the original Dropwizard appenders and logging factory installed and working on the application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Dropwizard 1.3 extension as applicable: Gradle: Add the following to your build.gradle file: dependencies { compile(\"com.newrelic.logging:dropwizard:2.0\") } Copy Maven: Add the following to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>dropwizard</artifactId> <version>2.0</version> </dependency> </dependencies> Copy Update your Dropwizard .yaml configuration file with a newrelic-json layout, replacing the currently used type: console or type: file with either type: newrelic-console or type: newrelic-file as appropriate. For example: logging: appenders: - type: newrelic-console # Add the two lines below if you don't have a layout specified on the appender. # If you have a layout, remove all parameters to the layout and set the type. layout: type: newrelic-json Copy The New Relic Dropwizard extension also supports a log-format layout type that uses the standard Dropwizard logging. For testing purposes, you can change the type of the layout with a one-line change: logging: appenders: - type: newrelic-file # This format will be ignored by the newrelic-json layout, but used by the log-format layout. logFormat: \"%date{ISO8601} %c %-5p: %m trace.id=%mdc{trace.id} span.id=%mdc{span.id}%n\" layout: # type: newrelic-json type: log-format Copy java.util.logging We offer a java.util.logging extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with with the java.util.logging extension: Make sure you have the java.util.logging package installed and working on the application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the java.util.logging extension as applicable. If you can't edit these files, you can instead add the jars directly to the application classpath. Gradle: Add the following to your build.gradle file: dependencies { compile(\"com.newrelic.logging:jul:2.0\") } Copy Maven: Add the following to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>jul</artifactId> <version>2.0</version> </dependency> </dependencies> Copy Check if your logging file's handlers property is set to something other than NewRelicMemoryHandler. Look for a line listing the root logger's handlers, like this: handlers = java.util.logging.FileHandler Copy Update your logging properties file to set the root logger's handler to NewRelicMemoryHandler so it intercepts messages destined for another handler: handlers = com.newrelic.logging.jul.NewRelicMemoryHandler Copy Configure the NewRelicMemoryHandler by setting the target to the handler that was previously assigned to the root logger, so it captures data New Relic needs on the thread the log message is coming from: com.newrelic.logging.jul.NewRelicMemoryHandler.target = java.util.logging.FileHandler Copy Use a NewRelicFormatter for the final handler. Update your logging properties file to set the formatter property like the following example. Make sure the handler where you set the formatter is the target handler from the previous step (java.util.logging.FileHandler in this example). java.util.logging.FileHandler.formatter = com.newrelic.logging.jul. NewRelicFormatter Copy The New Relic log format is JSON with telemetry metadata we use to correlate transactions and logs together. Currently we do not support any customization of that format. Once complete, JSON is logged instead of text. The JSON should be formatted as single objects, one per line, and should contain fields like log.level and thread.name. The trace.id, which is required for logs in context, should only have a value for log messages that occur within a transaction. java.util.logging classpath additions The most direct way to get the logs-in-context extensions is to add these dependencies to Maven's pom.xml or Gradle's build.gradle. This allows the packaging tools to pick up the correct dependencies. If you can't edit these files, you can instead add the jars directly to the application classpath for your logging framework's configuration. Before you modify the classpath: Enable the JVM argument -javaagent on your app's Java agent. Verify which logging framework the application is using. Make sure you are able to change your logging framework's configuration. Add the following three jars to the classpath if they aren't already present. Generally, we recommend taking the latest versions published on Maven Central. Group ID com.newrelic.logging and Artifact ID: Select the artifact named after your application's logging framework in Maven. Group ID com.fasterxml.jackson.core and Artifact ID: Use jackson-core. Group ID com.newrelic.agent.java and Artifact ID: Use newrelic-api. Log4j 1.x We offer a Log4j 1.x extension extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with with the Log4j 1.x extension, you must configure the Log4j extension in code or via XML. Properties files are not supported because AsyncAppender instances can only be automatically configured via XML. Make sure you have the Log4j 1.x package installed and working on the application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Log4j 1.x extension as applicable. Gradle: Add the following to your build.gradle file: dependencies { compile(\"com.newrelic.logging:log4j1:2.0\") } Copy Maven: Add the following to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>log4j1</artifactId> <version>2.0</version> </dependency> </dependencies> Copy In your logging configuration XML file, update your <appender> element with a NewRelicLayout, adding <layout class=\"com.newrelic.logging.log4j1.NewRelicLayout\"/>: <appender name=\" TypicalFile \" class=\"org.apache.log4j.FileAppender\"> <param name=\"file\" value=\"logs/log4j1-app.log\"/> <param name=\"append\" value=\"false\"/> <layout class=\" com.newrelic.logging.log4j1.NewRelicLayout \"/> <!-- only this line needs to be added --> </appender> Copy Use NewRelicAsyncAppender to wrap any appenders that will target New Relic's log forwarder. For example: <appender name=\" NewRelicFile \" class=\"com.newrelic.logging.log4j1.NewRelicAsyncAppender\"> <appender-ref ref=\" TypicalFile \" /> </appender> Copy Use the async appender on the root logger. For example: <root> <appender-ref ref=\" NewRelicFile \" /> </root> Copy Example configuration file for the Log4j 1.x extension: <?xml version=\"1.0\" encoding=\"UTF-8\" ?> <!DOCTYPE log4j:configuration SYSTEM \"log4j.dtd\"> <log4j:configuration debug=\"false\"> <appender name=\"TypicalFile\" class=\"org.apache.log4j.FileAppender\"> <param name=\"file\" value=\"logs/log4j1-app.log\"/> <param name=\"append\" value=\"false\"/> <!-- layout has been replaced --> <layout class=\"com.newrelic.logging.log4j1.NewRelicLayout\"/> </appender> <!-- this appender was added --> <appender name=\"NewRelicFile\" class=\"com.newrelic.logging.log4j1.NewRelicAsyncAppender\"> <appender-ref ref=\"TypicalFile\" /> </appender> <appender name=\"TypicalConsole\" class=\"org.apache.log4j.ConsoleAppender\"> <layout class=\"org.apache.log4j.PatternLayout\"> <param name=\"ConversionPattern\" value=\"%-5p %c{1} - %m%n\"/> </layout> </appender> <root> ​ <!-- the new appender was used here -->​​ <appender-ref ref=\"NewRelicFile\" /> <appender-ref ref=\"TypicalConsole\" /> </root> </log4j:configuration> Copy Log4j 2.x We offer a Log4j 2.x extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with with the Log4j 2.x extension: Make sure you have the Log4j 2.x or Logs4j 2 binding package installed and working on your application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Log4j 2.x extension as applicable: Gradle: Add the highlighted section to your build.gradle file: dependencies { compile(\"com.newrelic.logging:log4j2:2.0\") } Copy Maven: Add the highlighted section to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>log4j2</artifactId> <version>2.0</version> </dependency> </dependencies> Copy In your logging configuration XML file, update your <configuration> element by adding the highlighted section: <Configuration xmlns=\"http://logging.apache.org/log4j/2.0/config\" packages=\"com.newrelic.logging.log4j2\" > Copy If you're using a properties file, add packages=com.newrelic.logging.log4j2. Add <NewRelicLayout/> to use a NewRelicLayout element within one of the appenders. For example: <File name=\"MyFile\" fileName=\"logs/app-log-file.log\"> <NewRelicLayout/> </File> Copy If you're using a properties file, only change the layout.type: appender.console.type = Console appender.console.name = STDOUT appender.console.layout.type = NewRelicLayout Copy If you only modified an existing appender, skip this step. If you added a new appender, add <AppenderRef/> within <Root> to use this appender. Use the ref attribute to refer to appender name you created in the previous step. For example: <Root level=\"info\"> <AppenderRef ref=\"MyFile\"/> </Root> Copy If you're using a properties file and added a new appender, add: rootLogger.level = info rootLogger.appenderRef.stdout.ref = STDOUT ​​​​​ Copy Logback version 1.2.0 or higher We offer a Logback extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with Logback: Make sure you have Logback version 1.2.0 or higher and the New Relic Java agent version 5.6.0 or higher installed and working on your application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Logback extension as applicable: Gradle: Add the highlighted section to your build.gradle file: dependencies { compile(\"com.newrelic.logging:logback:2.0\") } Copy Maven: Add the highlighted section to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>logback</artifactId> <version>2.0</version> </dependency> </dependencies> Copy Update your logging configuration xml to replace any existing <encoder> element. If you're logging to the console (stdout/stderr), look for ConsoleAppender and replace: <appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> Copy If you're logging to a file, look for FileAppender and replace <encoder>: <appender name=\"LOG_FILE\" class=\"ch.qos.logback.core.FileAppender\"> <file>logs/app-log-file.log</file> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> Copy Update your logging configuration xml with the NewRelicAsyncAppender. To ensure that NewRelicAsyncAppender wraps any appenders that will target New Relic's log forwarder, add the following section. Change \"LOG_FILE\" to the name of the appender you updated in the previous step. <appender name=\"ASYNC\" class=\"com.newrelic.logging.logback.NewRelicAsyncAppender\"> <appender-ref ref=\"LOG_FILE\" /> </appender> Copy Make sure NewRelicAsyncAppender is the first appender used in your logger. Replace your root logger’s appenders with the ASYNC appender created in the previous step. Then list any other appenders after the NewRelicAsyncAppender in the <root> list. <root> <appender-ref ref=\"ASYNC\" /> </root> Copy Here are examples of an updated logging .xml file for the Logback extension. You can also see a working example in GitHub. Single console appender example Example configuration file after adding in the logging extension information: <configuration> <appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"> <!-- changed the encoder --> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> <!-- added the ASYNC appender --> <appender name=\"ASYNC\" class=\"com.newrelic.logging.logback.NewRelicAsyncAppender\"> <appender-ref ref=\"STDOUT\" /> </appender> <root level=\"debug\"> <!-- changed the root logger --> <appender-ref ref=\"ASYNC\" /> </root> </configuration> Copy Two console appenders example This example sends New Relic logging to a file, but still sends standard logging to the console: <configuration> <appender name=\"FILE\" class=\"ch.qos.logback.core.FileAppender\"> <file>myApp.log</file> <!-- encoder changed --> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> <!-- this appender does normal console logging --> <appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder> <pattern>%msg%n</pattern> </encoder> </appender> <!-- The required New Relic ASYNC appender wraps the FILE appender --> <appender name=\"ASYNC\" class=\"com.newrelic.logging.logback.NewRelicAsyncAppender\"> <appender-ref ref=\"FILE\" /> </appender> <root level=\"debug\"> <!-- ASYNC is one of the main appenders --> <appender-ref ref=\"ASYNC\" /> <!-- Send every message to normal console logging, as well. --> <appender-ref ref=\"STDOUT\" /> </root> </configuration> Copy Spring and Springboot We offer extensions for current versions of Spring and Spring Boot. If you already know the logging library, you can skip directly to that documentation: java.util.logging log4j 1 log4j 2 logback The extensions support default configurations only on Spring Boot 2.0 and higher. With Spring Boot: Here are tips to determine which logging library you have: If you have spring-boot-starter-log4j2 in your dependencies, you're using log4j 2.x. Refer to the Spring Boot log4j 2.x documentation for basic configuration, and the New Relic log4j 2 extension for customizing your configuration. If you're using Spring Boot but not the starter-log4j2, you're using logback by default. Refer to Spring Boot logback documentation for basic configuration, and the New Relic logback extension for customizing your configuration. With Spring (but not Spring Boot): Spring 5 or higher: Spring implements a bridge to other logging libraries that will automatically find them. However, those individual libraries must be configured and explicitly included in your project dependencies. To identify your logging dependency, consult your Gradle, Maven, or other build tool's dependency tree. Then follow the procedures to configure logs in context for your Java app with that extension. Spring 4 or lower: Spring version 4 and lower uses Apache Commons Logging for its bridge. Refer to the Spring documentation for information on configuring its bridge. View logs in UI To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you don't see log data in the UI, follow the troubleshooting procedures. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 196.64491,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "sections": "Java: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in <em>context</em> for the Java agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Java app To <em>enable</em> <em>logs</em>"
      },
      "id": "612efca3e7b9d2f718b6f223"
    },
    {
      "sections": [
        "C SDK: Configure logs in context"
      ],
      "title": "C SDK: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Configure logs in context"
      ],
      "external_id": "87eeada741d7f22204b37e7d6a906b9546ab8d4a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/c-sdk-configure-logs-context/",
      "published_at": "2021-10-18T05:57:03Z",
      "updated_at": "2021-10-06T21:51:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context is not available for apps monitored by the C SDK. Instead, you can build your own logging extension library by using the Log API. Make sure your app has distributed tracing enabled, so you can quickly understand what happens to requests as they travel through upstream and downstream services. To enable distributed tracing for apps monitored by the C SDK, install or update to the latest C SDK version. Distributed tracing requires C SDK version 1.1.0 or higher.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 196.64487,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "C SDK: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "sections": "C SDK: Configure <em>logs</em> <em>in</em> <em>context</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in <em>context</em> is not available for apps monitored by the C SDK. Instead, you can build your own logging extension library by using the <em>Log</em> API. Make sure your app has distributed tracing enabled, so you can quickly understand what happens to requests as they travel through upstream and downstream"
      },
      "id": "6127279b28ccbcf0c6f2618d"
    }
  ],
  "/docs/logs/logs-context/java-configure-logs-context-all": [
    {
      "sections": [
        "Ruby: Configure logs in context",
        "Set up your Ruby app",
        "Standard Ruby on Rails configuration",
        "Incompatible gems",
        "Rails advanced configuration",
        "Ruby configuration without Rails",
        "Lograge configuration",
        "Other logging extensions",
        "Troubleshooting",
        "What's next?"
      ],
      "title": "Ruby: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Logs in context for Ruby"
      ],
      "external_id": "d2100c0d6ddffe6b52a1f968f61f777a1e56a13c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/configure-logs-context-ruby/",
      "published_at": "2021-10-19T03:54:54Z",
      "updated_at": "2021-10-19T03:54:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Ruby agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Ruby app To enable logs in context for APM apps monitored by Ruby: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Ruby agent version, and enable distributed tracing. Use Ruby agent version 6.7.0 or higher for logs in context. For Rails applications, use a supported Rails version. Configure logs in context for Ruby. Standard Ruby on Rails configuration Rails logging is controlled by two components: A logger you can customize by setting config.logger A log formatter you can customize by setting config.log_formatter) In most cases, you should configure logs in context by setting config.log_formatter to the DecoratingFormatter in your Rails application. For more information about Rails configuration, see the rubyonrails.org documentation. In your application's config, require newrelic_rpm, then add the following line: module ________ class Application < Rails::Application ... config.log_formatter = ::NewRelic::Agent::Logging::DecoratingFormatter.new end end Copy This configuration uses the New Relic formatter for log messages, but the remaining configuration is provided by the other Rails settings. Incompatible gems New Relic's decorating logger is known to be incompatible with the following gems: logging semantic logger rails_stdout_logger rails_12factor Rails advanced configuration If setting the log_formatter option doesn't meet your needs, replace the entire Rails logger with an instance of the New Relic logger. Provide the parameters to the logger's constructor, like this: module ________ class Application < Rails::Application ... config.logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( \"log/application.log\", #etc... ) end end Copy Ruby configuration without Rails For non-Rails applications, use the DecoratingLogger in place of the Ruby standard ::Logger, like this: logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( 'log/application.log', #etc... ) ... logger.info(...) Copy The DecoratingLogger is a drop-in replacement for the Ruby standard ::Logger. Their constructors accept the same parameters. Lograge configuration To configure this extension with the lograge gem, follow standard procedures in this doc for Ruby on Rails configuration. No additional configuration is required for the lograge gem. Other logging extensions To use our logging extension with a different logging implementation or with your own custom logger, use the DecoratingFormatter. For example: module ________ class Application < Rails::Application ... config.logger = ::YourCustomLoggerImplementation.new( $stdout, formatter: ::NewRelic::Agent::Logging::DecoratingFormatter.new ) end end Copy To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you have configured your logging in /config/application.rb or in /config/environments/development.rb, run your application locally and check its logging output. You should see some output like this: {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"00fc7d46\",\"timestamp\":1567701375543,\"message\":\"example log message one\",\"log.level\":\"DEBUG\"} {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"6754870b\",\"timestamp\":1567702843604,\"message\":\"example log message two\",\"log.level\":\"DEBUG\"} Copy Troubleshooting If you don't see log data in the UI, follow the troubleshooting procedures. Also, if you see JSON logs in your application's output, but your query does not return logs, check your log forwarder. If the logs from your application are not formatted in JSON with fields like trace.id and span.id, there may be a problem with your log forwarding extension. In this situation: Check that the application is using a supported logging framework. Check that your logging configuration has been applied to all the environments where you want to forward and enrich the log data being sent to New Relic. Check that another logger is not configured later in your application's configuration. Logs in context for Ruby does not support tagged logging. If you are initializing your logger with a log_tags argument, your custom tags may not appear on the final version of your logs. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 383.25922,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Ruby: <em>Configure</em> <em>logs</em> <em>in</em> <em>context</em>",
        "sections": "Ruby: <em>Configure</em> <em>logs</em> <em>in</em> <em>context</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in <em>context</em> for the Ruby agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Ruby app To <em>enable</em> <em>logs</em>"
      },
      "id": "612d460128ccbc17bf56a863"
    },
    {
      "sections": [
        "C SDK: Configure logs in context"
      ],
      "title": "C SDK: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Configure logs in context"
      ],
      "external_id": "87eeada741d7f22204b37e7d6a906b9546ab8d4a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/c-sdk-configure-logs-context/",
      "published_at": "2021-10-18T05:57:03Z",
      "updated_at": "2021-10-06T21:51:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context is not available for apps monitored by the C SDK. Instead, you can build your own logging extension library by using the Log API. Make sure your app has distributed tracing enabled, so you can quickly understand what happens to requests as they travel through upstream and downstream services. To enable distributed tracing for apps monitored by the C SDK, install or update to the latest C SDK version. Distributed tracing requires C SDK version 1.1.0 or higher.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.7897,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "C SDK: <em>Configure</em> <em>logs</em> <em>in</em> <em>context</em>",
        "sections": "C SDK: <em>Configure</em> <em>logs</em> <em>in</em> <em>context</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in <em>context</em> is not available for apps monitored by the C SDK. Instead, you can build your own logging extension library by using the <em>Log</em> API. Make sure your app has distributed tracing enabled, so you can quickly understand what happens to requests as they travel through upstream and downstream"
      },
      "id": "6127279b28ccbcf0c6f2618d"
    },
    {
      "sections": [
        ".NET: Configure logs in context",
        "Set up your .NET app",
        "Configure log4net extension",
        "log4net workflow diagram",
        "log4net 2.0.8 or higher configuration",
        "Configure NLog extension",
        "Nlog workflow diagram",
        "Nlog 4.5 or higher configuration",
        "Nlog file-based configuration",
        "Configure Serilog 2.5 or higher extension",
        "Serilog workflow diagram",
        "Serilog 2.5 or higher configuration",
        "Serilog file-based configuration",
        "View logs in the UI",
        "What's next?"
      ],
      "title": ".NET: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Configure logs in context"
      ],
      "external_id": "69a3fc9dab232e3ebfdeccceace39b1014b70beb",
      "image": "https://docs.newrelic.com/static/90e8518fffeb7d9cc28f58f29fe749a5/e5166/LogsInContext-Log4Net.jpg",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/net-configure-logs-context-all/",
      "published_at": "2021-10-18T10:42:13Z",
      "updated_at": "2021-10-06T21:51:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the .NET agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your .NET app To enable logs in context for APM apps monitored by .NET: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest .NET agent version, and enable distributed tracing. Use .NET agent version 8.21 or higher and the New Relic .NET agent API version 8.21 or higher for logs in context. Install or update to Microsoft .NET Framework 4.5 or higher or .NET Core 2.0 or higher. Install and configure any of the following logging extensions to enrich your log data, including: log4net NLog Serilog Check your log data in the New Relic UI. Configure log4net extension You can use the Apache log4net version 2.0.8 or higher extension to link your log data with related data across the rest of the New Relic platform. log4net workflow diagram The following diagram illustrates the flow of log messages through Apache log4net, highlighting specific components of the New Relic log4net extension. Many log forwarders are available. This example uses Fluentd. Appender: The NewRelicAppender adds contextual information from the .NET agent (using the API) to the log events generated by the application. This contextual information, known as linking metadata, is used by New Relic to link log messages to the transactions and spans from which they were created. This appender will pass the enriched log events to downstream appenders for further processing. Since the NewRelicAppender is ForwardingAppender type, it needs to be the first appender in the chain. It also requires another appender that can write to an actual output destination as its child in order to work. Layout: The NewRelicLayout formats the enriched log events into the JSON format expected by New Relic. The appender, which this layout is assigned to, instructs log4net to output the JSON to a file in the location that the log forwarder expects. Log Forwarder: The log forwarder monitors an output folder and incrementally sends the properly formatted and enriched log information to the New Relic logging endpoint. log4net 2.0.8 or higher configuration Log4net uses appender and layout to store and format log messages. NewRelicAppender enriches log messages with contextual information from the New Relic .NET agent if it is attached to your application. The appender passes enriched log messages to downstream appenders to handle specific use cases for log messages. For more information about logging with log4net, see the Apache log4net Getting started documentation. To configure logs in context with the log4net extension: Using the Visual Studio NuGet Package Manager, locate and install the NewRelic.LogEnrichers.Log4Net package. In your log4net configuration file, update your logging configuration to use the NewRelicAppender as the first level appender, and reference your existing appenders as its children. Also replace the layout of the appender that writes log messages to an output destination with the NewRelicLayout. The following log4net configuration example enriches log events with New Relic linking metadata. In addition to the existing log files, it outputs new log files in a specific JSON format at C:\\logs\\log4netExample.log.json for consumption by the log forwarder: <log4net> <root> <level value=\"ALL\" /> <appender-ref ref=\"NewRelicAppender\" /> </root> <appender name=\"NewRelicAppender\" type=\"NewRelic.LogEnrichers.Log4Net.NewRelicAppender, NewRelic.LogEnrichers.Log4Net\" > <threshold value=\"ALL\"/> <appender-ref ref=\"FileAppender\" /> </appender> <appender name=\"FileAppender\" type=\"log4net.Appender.FileAppender\"> <file value=\"C:\\logs\\log4netExample.log.json\" /> <param name=\"AppendToFile\" value=\"true\" /> <layout type=\"NewRelic.LogEnrichers.Log4Net.NewRelicLayout, NewRelic.LogEnrichers.Log4Net\"> </layout> </appender> </log4net> Copy After you configure the log4net extension and update your logging file, you can configure your extension to send data to New Relic. Here is an example configuration using the Fluentd plugin for New Relic Logs: <!--NewRelicLoggingExample.conf--> <source> @type tail path C:\\logs\\log4netExample.log.json pos_file C:\\logs\\log4netExample.log.json.pos tag logfile.* <parse> @type json </parse> </source> <match **> @type newrelic license_key <YOUR NEW_RELIC_LICENSE_KEY> base_uri https://log-api.newrelic.com/log/v1 </match> Copy Configure NLog extension You can use our NLog 4.5 or higher extension to link to your log data with related data across the rest of the New Relic platform. Nlog workflow diagram The New Relic NLog extension provides a NewRelicJsonLayout that formats a log event in the way required by the New Relic logging endpoint. Next, it adds contextual information from the .NET agent when attached to your application. Then, a target can be configured to write logging data to an output folder. The log forwarder can monitor this folder and incrementally send log information to New Relic. The following diagram illustrates the flow of log messages through NLog, highlighting specific components of the New Relic NLog extension. New Relic JSON Layout: The NewRelicJsonLayout adds contextual information from the .NET agent (using the API) to the log events generated by the application, and outputs log messages in the JSON format expected by New Relic. This contextual information, known as linking metadata, is used by New Relic to link log messages to the transactions and spans where they were created. Since the NewRelicAppender is ForwardingAppender type, it needs to be the first appender in the chain. It also requires another appender that can write to an actual output destination as its child in order to work. File Target: A FileTarget defines a file on disk where log messages are written. Adding the NewRelicJsonLayout to that target allows the output to be formatted correctly for forwarding to New Relic. Log Forwarder: The log forwarder is configured to send the properly formatted and enriched log data from the FileTarget output to the New Relic logging endpoint. For more information about logging with NLog, see the nlog-project.org documentation. Nlog 4.5 or higher configuration Use our NLog 4.5 or higher extension to link to your log data with related data across the rest of the New Relic platform. Using the Visual Studio NuGet Package Manager, locate and install the NewRelic.LogEnrichers.NLog package. In your application code, update your logging configuration to add the NewRelicJsonLayout and decide if you want to collect MappedDiagnosticsContext (MDC) or the MappedDiagnosticsLogicalContext (MDLC) data. The following configuration examples result in new JSON files that are written to disk. Some of these configuration options may be useful for managing the amount of disk space used and/or the performance of the target. archiveAboveSize maxArchiveFiles bufferSize enableArchiveFileCompression autoFlush concurrentWrites Although the NLog AsyncWrapper Target is not required, it may help improve performance by performing formatting and output of log files on a different thread. Don't collect MDC or the MDLC data (default): The following code example enriches log events with New Relic linking metadata, but not with MDC or the MDLC data. In addition to the existing log files, it outputs new log files in a specific JSON format at C:\\logs\\NLogExample.log.json for consumption by the log forwarder: var loggerConfig = new LoggingConfiguration(); var newRelicFileTarget = new FileTarget(\"NewRelicFileTarget\"); newRelicFileTarget.Layout = new NewRelicJsonLayout(); newRelicFileTarget.FileName = \"C:\\logs\\NLogExample.json\"; loggerConfig.AddTarget(newRelicFileTarget); loggerConfig.AddRuleForAllLevels(\"NewRelicFileTarget\"); LogManager.Configuration = loggerConfig; var logger = LogManager.GetLogger(\"Example\"); Copy Collect MDC or the MDLC data: If your application uses the MDC or the MDLC, you can configure the NewRelicJsonLayout to include items in those collections. The following code example adds the additional configuration to enable collecting MDC and MDLC data. As in the previous example, it outputs new log files in a specific JSON format at C:\\logs\\NLogExample.log.json for consumption by the log forwarder: var loggerConfig = new LoggingConfiguration(); var newRelicFileTarget = new FileTarget(\"NewRelicFileTarget\"); var newRelicLayout = new NewRelicJsonLayout { IncludeMdc = `true,` IncludeMdlc = `true` }; newRelicFileTarget.Layout = newRelicLayout; newRelicFileTarget.FileName = \"C:\\logs\\NLogExample.json\"; loggerConfig.AddTarget(newRelicFileTarget); loggerConfig.AddRuleForAllLevels(\"NewRelicFileTarget\"); LogManager.Configuration = loggerConfig; var logger = LogManager.GetLogger(\"Example\"); Copy Once you have configured the NLog extension and updated your logging file, you can configure your extension to send data to New Relic. Here is an example configuration using the Fluentd plugin to forward logs to New Relic: <!--NewRelicLoggingExample.conf--> <source> @type tail path C:\\logs\\NLogExample.log.json pos_file C:\\logs\\NLogExample.log.json.pos tag logfile.* <parse> @type json </parse> </source> <match **> @type newrelic license_key <YOUR NEW_RELIC_LICENSE_KEY> base_uri https://log-api.newrelic.com/log/v1 </match> Copy Nlog file-based configuration You can also configure the New Relic NLog extension with file-based configuration providers. The folowing example code creates a logger based on settings contained in an App.config file. Instantiating Logger using .config file var logger = LogManager.GetLogger(\"NewRelicLog\"); logger.Info(\"Hello, New Relic!\"); Copy Sample App.config file <?xml version=\"1.0\" encoding=\"utf-8\" ?> <configuration> <configSections> <section name=\"nlog\" type=\"NLog.Config.ConfigSectionHandler, NLog\"/> </configSections> <startup> <supportedRuntime version=\"v4.0\" sku=\".NETFramework,Version=v4.5\" /> </startup> <nlog xmlns=\"http://www.nlog-project.org/schemas/NLog.xsd\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"> <extensions> <add assembly=\"NewRelic.LogEnrichers.NLog\" /> </extensions> <targets> <target name=\"NewRelicLogFile\" xsi:type=\"File\" fileName=\"C:/path/to/NewRelicLog.json\"> <layout xsi:type=\"newrelic-jsonlayout\"> </layout> </target> </targets> <rules> <logger name=\"NewRelicLog\" minlevel=\"Info\" writeTo=\"newRelicLogFile\" /> </rules> </nlog> </configuration> Copy Configure Serilog 2.5 or higher extension You can use our Serilog extension to link to your log data with related data across the rest of the New Relic platform. This requires: Serilog 2.5 or higher Serilog File Sinks v4.0 or higher Serilog workflow diagram Serilog is a structured logging framework that records log messages from your application and creates a LogEvent to store the message data. Using Enrichers, you can add additional information to the log events. Sinks and Formatters allow you to format and output those log events for downstream consumption and viewing. The following diagram illustrates the flow of log messages through Serilog, highlighting specific components of the New Relic Serilog extension. Many log forwarders are available. This example uses Fluentd. New Relic Enricher: The NewRelicEnricher adds contextual information from the .NET agent (using the API) to the log events generated by the application. This contextual information, called linking metadata, is used by New Relic to link log messages to the transactions and spans where they were created. New Relic Formatter: The NewRelicFormatter translates enriched log events into the JSON format expected by New Relic. A sink instructs Serilog to output the JSON to a file in the location that the log forwarder expects. New Relic Log Forwarder: The log forwarder is configured to send the properly formatted and enriched log data from the FileTarget output to the New Relic logging endpoint. For more information about Serilog log events, see the Serilog documentation on GitHub. Serilog 2.5 or higher configuration To configure logs in context with the Serilog extension: Use the Visual Studio NuGet Package Manager to locate and install the NewRelic.LogEnrichers.Serilog package. In your application code, update your logging configuration to add the NewRelicEnricher and NewRelicFormatter. The following code example enriches log events with New Relic linking metadata. In addition to the existing log files, it outputs new log files in a specific JSON format at C:\\logs\\SerilogExample.log.json for consumption by the log forwarder: var loggerConfig = new LoggerConfiguration() loggerConfig .Enrich.WithThreadName() .Enrich.WithThreadId() .Enrich.WithNewRelicLogsInContext() .WriteTo.File( path: @\"C:\\logs\\ExistingLoggingOutput.txt\") .WriteTo.File( formatter: new NewRelicFormatter(), path: @\"C:\\logs\\SerilogExample.log.json\"); var log = loggerConfig.CreateLogger(); Copy This configuration results in new JSON files that are written to disk. Some of these configuration options may be useful for managing the amount of disk space used and/or the performance of the sink. restrictedToMinimumLevel buffered rollingInterval rollOnFileSizeLimit retainedFileCountLimit Although not required, using the Serilog Asynchronous Sink Wrapper may help improve the performance by performing formatting and output of log files on a different thread. Once you have configured the Serilog extension and updated your logging file, you can configure your extension to send data to New Relic. Here is an example configuration using the Fluentd plugin to forward logs to New Relic: <!--NewRelicLoggingExample.conf--> <source> @type tail path C:\\logs\\SerilogExample.log.json pos_file C:\\logs\\SerilogExample.log.json.pos tag logfile.* <parse> @type json </parse> </source> <match **> @type newrelic license_key <YOUR NEW_RELIC_LICENSE_KEY> base_uri https://log-api.newrelic.com/log/v1 </match> Copy Serilog file-based configuration You can also configure the New Relic Serilog extension with file-based configuration providers.The following additional NuGet Packages are required: Microsoft.Extensions.Configuration](https://www.nuget.org/packages/Microsoft.Extensions.Configuration/) Serilog.Settings.Configuration The following example code creates a logger based on settings contained in an appSettings.json file. Instantiating logger using appsettings.json var builder = new ConfigurationBuilder() .AddJsonFile(\"appsettings.json\"); var configuration = builder.Build(); var logger = new LoggerConfiguration() .ReadFrom.Configuration(configuration) .CreateLogger(); Copy Sample appsettings.json file { \"Serilog\": { \"Using\": [ \"Serilog.Sinks.Console\", \"Serilog.Sinks.File\", \"NewRelic.LogEnrichers.Serilog\" ], \"MinimumLevel\": \"Debug\", \"Enrich\": [ \"WithNewRelicLogsInContext\" ], \"WriteTo\": [ { \"Name\": \"File\", \"Args\": { \"path\": \"C:\\\\Logs\\\\SerilogExample.log.json\", \"formatter\": \"NewRelic.LogEnrichers.Serilog.NewRelicFormatter, NewRelic.LogEnrichers.Serilog\" } } ], \"Properties\": { \"Application\": \"NewRelic Logging Serilog Example\" } } } Copy The following example code creates a logger based on settings contained in a web.config file. The Serilog.Settings.AppSettings NuGet Package is required. Instantiating logger using .config file var logger = new LoggerConfiguration() .ReadFrom.AppSettings() .CreateLogger(); Copy Sample web.config file <?xml version=\"1.0\" encoding=\"utf-8\"?> <configuration> <appSettings> <add key=\"serilog:using:NewRelic\" value=\"NewRelic.LogEnrichers.Serilog\" /> <add key=\"serilog:using:File\" value=\"Serilog.Sinks.File\" /> <!--Add other enrichers here--> <add key=\"serilog:enrich:WithNewRelicLogsInContext\" /> <add key=\"serilog:write-to:File.path\" value=\"C:\\logs\\SerilogExample.log.json\" /> <add key=\"serilog:write-to:File.formatter\" value=\"NewRelic.LogEnrichers.Serilog.NewRelicFormatter, NewRelic.LogEnrichers.Serilog\" /> </appSettings> Copy View logs in the UI To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you don't see log data in the UI, follow the troubleshooting procedures. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.7897,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": ".NET: <em>Configure</em> <em>logs</em> <em>in</em> <em>context</em>",
        "sections": ".NET: <em>Configure</em> <em>logs</em> <em>in</em> <em>context</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in <em>context</em> for the .NET agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your .NET app To <em>enable</em> <em>logs</em>"
      },
      "id": "612efe5764441ff155424352"
    }
  ],
  "/docs/logs/logs-context/net-configure-logs-context-all": [
    {
      "sections": [
        "Ruby: Configure logs in context",
        "Set up your Ruby app",
        "Standard Ruby on Rails configuration",
        "Incompatible gems",
        "Rails advanced configuration",
        "Ruby configuration without Rails",
        "Lograge configuration",
        "Other logging extensions",
        "Troubleshooting",
        "What's next?"
      ],
      "title": "Ruby: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Logs in context for Ruby"
      ],
      "external_id": "d2100c0d6ddffe6b52a1f968f61f777a1e56a13c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/configure-logs-context-ruby/",
      "published_at": "2021-10-19T03:54:54Z",
      "updated_at": "2021-10-19T03:54:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Ruby agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Ruby app To enable logs in context for APM apps monitored by Ruby: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Ruby agent version, and enable distributed tracing. Use Ruby agent version 6.7.0 or higher for logs in context. For Rails applications, use a supported Rails version. Configure logs in context for Ruby. Standard Ruby on Rails configuration Rails logging is controlled by two components: A logger you can customize by setting config.logger A log formatter you can customize by setting config.log_formatter) In most cases, you should configure logs in context by setting config.log_formatter to the DecoratingFormatter in your Rails application. For more information about Rails configuration, see the rubyonrails.org documentation. In your application's config, require newrelic_rpm, then add the following line: module ________ class Application < Rails::Application ... config.log_formatter = ::NewRelic::Agent::Logging::DecoratingFormatter.new end end Copy This configuration uses the New Relic formatter for log messages, but the remaining configuration is provided by the other Rails settings. Incompatible gems New Relic's decorating logger is known to be incompatible with the following gems: logging semantic logger rails_stdout_logger rails_12factor Rails advanced configuration If setting the log_formatter option doesn't meet your needs, replace the entire Rails logger with an instance of the New Relic logger. Provide the parameters to the logger's constructor, like this: module ________ class Application < Rails::Application ... config.logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( \"log/application.log\", #etc... ) end end Copy Ruby configuration without Rails For non-Rails applications, use the DecoratingLogger in place of the Ruby standard ::Logger, like this: logger = ::NewRelic::Agent::Logging::DecoratingLogger.new( 'log/application.log', #etc... ) ... logger.info(...) Copy The DecoratingLogger is a drop-in replacement for the Ruby standard ::Logger. Their constructors accept the same parameters. Lograge configuration To configure this extension with the lograge gem, follow standard procedures in this doc for Ruby on Rails configuration. No additional configuration is required for the lograge gem. Other logging extensions To use our logging extension with a different logging implementation or with your own custom logger, use the DecoratingFormatter. For example: module ________ class Application < Rails::Application ... config.logger = ::YourCustomLoggerImplementation.new( $stdout, formatter: ::NewRelic::Agent::Logging::DecoratingFormatter.new ) end end Copy To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you have configured your logging in /config/application.rb or in /config/environments/development.rb, run your application locally and check its logging output. You should see some output like this: {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"00fc7d46\",\"timestamp\":1567701375543,\"message\":\"example log message one\",\"log.level\":\"DEBUG\"} {\"entity.name\":\"your_app_name\",\"entity.type\":\"SERVICE\",\"hostname\":\"79bcbf8d\",\"trace.id\":\"79bcbf8d\",\"span.id\":\"6754870b\",\"timestamp\":1567702843604,\"message\":\"example log message two\",\"log.level\":\"DEBUG\"} Copy Troubleshooting If you don't see log data in the UI, follow the troubleshooting procedures. Also, if you see JSON logs in your application's output, but your query does not return logs, check your log forwarder. If the logs from your application are not formatted in JSON with fields like trace.id and span.id, there may be a problem with your log forwarding extension. In this situation: Check that the application is using a supported logging framework. Check that your logging configuration has been applied to all the environments where you want to forward and enrich the log data being sent to New Relic. Check that another logger is not configured later in your application's configuration. Logs in context for Ruby does not support tagged logging. If you are initializing your logger with a log_tags argument, your custom tags may not appear on the final version of your logs. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 383.25922,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Ruby: <em>Configure</em> <em>logs</em> <em>in</em> <em>context</em>",
        "sections": "Ruby: <em>Configure</em> <em>logs</em> <em>in</em> <em>context</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in <em>context</em> for the Ruby agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Ruby app To <em>enable</em> <em>logs</em>"
      },
      "id": "612d460128ccbc17bf56a863"
    },
    {
      "sections": [
        "Java: Configure logs in context",
        "Set up your Java app",
        "Dropwizard 1.3 or higher",
        "java.util.logging",
        "java.util.logging classpath additions",
        "Log4j 1.x",
        "Log4j 2.x",
        "Logback version 1.2.0 or higher",
        "Spring and Springboot",
        "View logs in UI",
        "What's next?"
      ],
      "title": "Java: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Configure logs in context"
      ],
      "external_id": "b9fbeafa564247287e5d6466630d5bdcdb85affb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/java-configure-logs-context-all/",
      "published_at": "2021-10-18T11:15:14Z",
      "updated_at": "2021-10-06T21:51:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context for the Java agent connects your logs and APM data in New Relic. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. Set up your Java app To enable logs in context for APM apps monitored by Java: Make sure you have already set up logging in New Relic. This includes configuring a supported log forwarder that collects your application logs and extends the metadata that is forwarded to New Relic. Install or update to the latest Java agent version. Use Java agent version 5.6.0 or higher for logs in context. Enable the JVM argument -javaagent, and enable distributed tracing. Configure logs in context for Java to enrich your log data, using any of the following extensions as applicable. If you use Spring or Spring Boot and aren't sure which extension you need, see our Spring documentation. Dropwizard 1.3 or higher We offer a Dropwizard extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with the DropWizard extension: Make sure you have the Dropwizard 1.3 or higher package installed and working on your application. Use the original Dropwizard appenders and logging factory installed and working on the application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Dropwizard 1.3 extension as applicable: Gradle: Add the following to your build.gradle file: dependencies { compile(\"com.newrelic.logging:dropwizard:2.0\") } Copy Maven: Add the following to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>dropwizard</artifactId> <version>2.0</version> </dependency> </dependencies> Copy Update your Dropwizard .yaml configuration file with a newrelic-json layout, replacing the currently used type: console or type: file with either type: newrelic-console or type: newrelic-file as appropriate. For example: logging: appenders: - type: newrelic-console # Add the two lines below if you don't have a layout specified on the appender. # If you have a layout, remove all parameters to the layout and set the type. layout: type: newrelic-json Copy The New Relic Dropwizard extension also supports a log-format layout type that uses the standard Dropwizard logging. For testing purposes, you can change the type of the layout with a one-line change: logging: appenders: - type: newrelic-file # This format will be ignored by the newrelic-json layout, but used by the log-format layout. logFormat: \"%date{ISO8601} %c %-5p: %m trace.id=%mdc{trace.id} span.id=%mdc{span.id}%n\" layout: # type: newrelic-json type: log-format Copy java.util.logging We offer a java.util.logging extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with with the java.util.logging extension: Make sure you have the java.util.logging package installed and working on the application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the java.util.logging extension as applicable. If you can't edit these files, you can instead add the jars directly to the application classpath. Gradle: Add the following to your build.gradle file: dependencies { compile(\"com.newrelic.logging:jul:2.0\") } Copy Maven: Add the following to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>jul</artifactId> <version>2.0</version> </dependency> </dependencies> Copy Check if your logging file's handlers property is set to something other than NewRelicMemoryHandler. Look for a line listing the root logger's handlers, like this: handlers = java.util.logging.FileHandler Copy Update your logging properties file to set the root logger's handler to NewRelicMemoryHandler so it intercepts messages destined for another handler: handlers = com.newrelic.logging.jul.NewRelicMemoryHandler Copy Configure the NewRelicMemoryHandler by setting the target to the handler that was previously assigned to the root logger, so it captures data New Relic needs on the thread the log message is coming from: com.newrelic.logging.jul.NewRelicMemoryHandler.target = java.util.logging.FileHandler Copy Use a NewRelicFormatter for the final handler. Update your logging properties file to set the formatter property like the following example. Make sure the handler where you set the formatter is the target handler from the previous step (java.util.logging.FileHandler in this example). java.util.logging.FileHandler.formatter = com.newrelic.logging.jul. NewRelicFormatter Copy The New Relic log format is JSON with telemetry metadata we use to correlate transactions and logs together. Currently we do not support any customization of that format. Once complete, JSON is logged instead of text. The JSON should be formatted as single objects, one per line, and should contain fields like log.level and thread.name. The trace.id, which is required for logs in context, should only have a value for log messages that occur within a transaction. java.util.logging classpath additions The most direct way to get the logs-in-context extensions is to add these dependencies to Maven's pom.xml or Gradle's build.gradle. This allows the packaging tools to pick up the correct dependencies. If you can't edit these files, you can instead add the jars directly to the application classpath for your logging framework's configuration. Before you modify the classpath: Enable the JVM argument -javaagent on your app's Java agent. Verify which logging framework the application is using. Make sure you are able to change your logging framework's configuration. Add the following three jars to the classpath if they aren't already present. Generally, we recommend taking the latest versions published on Maven Central. Group ID com.newrelic.logging and Artifact ID: Select the artifact named after your application's logging framework in Maven. Group ID com.fasterxml.jackson.core and Artifact ID: Use jackson-core. Group ID com.newrelic.agent.java and Artifact ID: Use newrelic-api. Log4j 1.x We offer a Log4j 1.x extension extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with with the Log4j 1.x extension, you must configure the Log4j extension in code or via XML. Properties files are not supported because AsyncAppender instances can only be automatically configured via XML. Make sure you have the Log4j 1.x package installed and working on the application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Log4j 1.x extension as applicable. Gradle: Add the following to your build.gradle file: dependencies { compile(\"com.newrelic.logging:log4j1:2.0\") } Copy Maven: Add the following to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>log4j1</artifactId> <version>2.0</version> </dependency> </dependencies> Copy In your logging configuration XML file, update your <appender> element with a NewRelicLayout, adding <layout class=\"com.newrelic.logging.log4j1.NewRelicLayout\"/>: <appender name=\" TypicalFile \" class=\"org.apache.log4j.FileAppender\"> <param name=\"file\" value=\"logs/log4j1-app.log\"/> <param name=\"append\" value=\"false\"/> <layout class=\" com.newrelic.logging.log4j1.NewRelicLayout \"/> <!-- only this line needs to be added --> </appender> Copy Use NewRelicAsyncAppender to wrap any appenders that will target New Relic's log forwarder. For example: <appender name=\" NewRelicFile \" class=\"com.newrelic.logging.log4j1.NewRelicAsyncAppender\"> <appender-ref ref=\" TypicalFile \" /> </appender> Copy Use the async appender on the root logger. For example: <root> <appender-ref ref=\" NewRelicFile \" /> </root> Copy Example configuration file for the Log4j 1.x extension: <?xml version=\"1.0\" encoding=\"UTF-8\" ?> <!DOCTYPE log4j:configuration SYSTEM \"log4j.dtd\"> <log4j:configuration debug=\"false\"> <appender name=\"TypicalFile\" class=\"org.apache.log4j.FileAppender\"> <param name=\"file\" value=\"logs/log4j1-app.log\"/> <param name=\"append\" value=\"false\"/> <!-- layout has been replaced --> <layout class=\"com.newrelic.logging.log4j1.NewRelicLayout\"/> </appender> <!-- this appender was added --> <appender name=\"NewRelicFile\" class=\"com.newrelic.logging.log4j1.NewRelicAsyncAppender\"> <appender-ref ref=\"TypicalFile\" /> </appender> <appender name=\"TypicalConsole\" class=\"org.apache.log4j.ConsoleAppender\"> <layout class=\"org.apache.log4j.PatternLayout\"> <param name=\"ConversionPattern\" value=\"%-5p %c{1} - %m%n\"/> </layout> </appender> <root> ​ <!-- the new appender was used here -->​​ <appender-ref ref=\"NewRelicFile\" /> <appender-ref ref=\"TypicalConsole\" /> </root> </log4j:configuration> Copy Log4j 2.x We offer a Log4j 2.x extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with with the Log4j 2.x extension: Make sure you have the Log4j 2.x or Logs4j 2 binding package installed and working on your application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Log4j 2.x extension as applicable: Gradle: Add the highlighted section to your build.gradle file: dependencies { compile(\"com.newrelic.logging:log4j2:2.0\") } Copy Maven: Add the highlighted section to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>log4j2</artifactId> <version>2.0</version> </dependency> </dependencies> Copy In your logging configuration XML file, update your <configuration> element by adding the highlighted section: <Configuration xmlns=\"http://logging.apache.org/log4j/2.0/config\" packages=\"com.newrelic.logging.log4j2\" > Copy If you're using a properties file, add packages=com.newrelic.logging.log4j2. Add <NewRelicLayout/> to use a NewRelicLayout element within one of the appenders. For example: <File name=\"MyFile\" fileName=\"logs/app-log-file.log\"> <NewRelicLayout/> </File> Copy If you're using a properties file, only change the layout.type: appender.console.type = Console appender.console.name = STDOUT appender.console.layout.type = NewRelicLayout Copy If you only modified an existing appender, skip this step. If you added a new appender, add <AppenderRef/> within <Root> to use this appender. Use the ref attribute to refer to appender name you created in the previous step. For example: <Root level=\"info\"> <AppenderRef ref=\"MyFile\"/> </Root> Copy If you're using a properties file and added a new appender, add: rootLogger.level = info rootLogger.appenderRef.stdout.ref = STDOUT ​​​​​ Copy Logback version 1.2.0 or higher We offer a Logback extension for logs in context with the Java agent. To get started, review the code and an example application on GitHub. To configure logs in context for your Java app with Logback: Make sure you have Logback version 1.2.0 or higher and the New Relic Java agent version 5.6.0 or higher installed and working on your application. Make sure you have the New Relic Java agent version 5.6.0 or higher installed on your application, and that you have enabled the JVM argument -javaagent. Update your project's dependencies to include the Logback extension as applicable: Gradle: Add the highlighted section to your build.gradle file: dependencies { compile(\"com.newrelic.logging:logback:2.0\") } Copy Maven: Add the highlighted section to your pom.xml file: <dependencies> <dependency> <groupId>com.newrelic.logging</groupId> <artifactId>logback</artifactId> <version>2.0</version> </dependency> </dependencies> Copy Update your logging configuration xml to replace any existing <encoder> element. If you're logging to the console (stdout/stderr), look for ConsoleAppender and replace: <appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> Copy If you're logging to a file, look for FileAppender and replace <encoder>: <appender name=\"LOG_FILE\" class=\"ch.qos.logback.core.FileAppender\"> <file>logs/app-log-file.log</file> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> Copy Update your logging configuration xml with the NewRelicAsyncAppender. To ensure that NewRelicAsyncAppender wraps any appenders that will target New Relic's log forwarder, add the following section. Change \"LOG_FILE\" to the name of the appender you updated in the previous step. <appender name=\"ASYNC\" class=\"com.newrelic.logging.logback.NewRelicAsyncAppender\"> <appender-ref ref=\"LOG_FILE\" /> </appender> Copy Make sure NewRelicAsyncAppender is the first appender used in your logger. Replace your root logger’s appenders with the ASYNC appender created in the previous step. Then list any other appenders after the NewRelicAsyncAppender in the <root> list. <root> <appender-ref ref=\"ASYNC\" /> </root> Copy Here are examples of an updated logging .xml file for the Logback extension. You can also see a working example in GitHub. Single console appender example Example configuration file after adding in the logging extension information: <configuration> <appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"> <!-- changed the encoder --> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> <!-- added the ASYNC appender --> <appender name=\"ASYNC\" class=\"com.newrelic.logging.logback.NewRelicAsyncAppender\"> <appender-ref ref=\"STDOUT\" /> </appender> <root level=\"debug\"> <!-- changed the root logger --> <appender-ref ref=\"ASYNC\" /> </root> </configuration> Copy Two console appenders example This example sends New Relic logging to a file, but still sends standard logging to the console: <configuration> <appender name=\"FILE\" class=\"ch.qos.logback.core.FileAppender\"> <file>myApp.log</file> <!-- encoder changed --> <encoder class=\"com.newrelic.logging.logback.NewRelicEncoder\"/> </appender> <!-- this appender does normal console logging --> <appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder> <pattern>%msg%n</pattern> </encoder> </appender> <!-- The required New Relic ASYNC appender wraps the FILE appender --> <appender name=\"ASYNC\" class=\"com.newrelic.logging.logback.NewRelicAsyncAppender\"> <appender-ref ref=\"FILE\" /> </appender> <root level=\"debug\"> <!-- ASYNC is one of the main appenders --> <appender-ref ref=\"ASYNC\" /> <!-- Send every message to normal console logging, as well. --> <appender-ref ref=\"STDOUT\" /> </root> </configuration> Copy Spring and Springboot We offer extensions for current versions of Spring and Spring Boot. If you already know the logging library, you can skip directly to that documentation: java.util.logging log4j 1 log4j 2 logback The extensions support default configurations only on Spring Boot 2.0 and higher. With Spring Boot: Here are tips to determine which logging library you have: If you have spring-boot-starter-log4j2 in your dependencies, you're using log4j 2.x. Refer to the Spring Boot log4j 2.x documentation for basic configuration, and the New Relic log4j 2 extension for customizing your configuration. If you're using Spring Boot but not the starter-log4j2, you're using logback by default. Refer to Spring Boot logback documentation for basic configuration, and the New Relic logback extension for customizing your configuration. With Spring (but not Spring Boot): Spring 5 or higher: Spring implements a bridge to other logging libraries that will automatically find them. However, those individual libraries must be configured and explicitly included in your project dependencies. To identify your logging dependency, consult your Gradle, Maven, or other build tool's dependency tree. Then follow the procedures to configure logs in context for your Java app with that extension. Spring 4 or lower: Spring version 4 and lower uses Apache Commons Logging for its bridge. Refer to the Spring documentation for information on configuring its bridge. View logs in UI To verify that you have configured the log appender correctly, run your application, then check your logs data in New Relic One using the query operator has:span.id has:trace.id. If everything is configured correctly and your data is being forwarded to New Relic with the enriched metadata, your logs should now be emitted as JSON and contain trace.id and span.id fields. If you don't see log data in the UI, follow the troubleshooting procedures. What's next? After you set up APM logs in context, make the most of your logging data: Explore the logging data across your platform with our Logs UI. See your logs in context of your app's performance in the APM UI. Troubleshoot errors with distributed tracing, stack traces, application logs, and more. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our infrastructure monitoring agent. Review your infrastructure logs in the UI. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.78976,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Java: <em>Configure</em> <em>logs</em> <em>in</em> <em>context</em>",
        "sections": "Java: <em>Configure</em> <em>logs</em> <em>in</em> <em>context</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in <em>context</em> for the Java agent connects your <em>logs</em> and APM data in <em>New</em> <em>Relic</em>. Bringing all of this data together in a single tool helps you quickly get to the root cause of an issue and find the <em>log</em> lines that you need to identify and resolve a problem. Set up your Java app To <em>enable</em> <em>logs</em>"
      },
      "id": "612efca3e7b9d2f718b6f223"
    },
    {
      "sections": [
        "C SDK: Configure logs in context"
      ],
      "title": "C SDK: Configure logs in context",
      "type": "docs",
      "tags": [
        "Logs",
        "Enable log management in New Relic",
        "Configure logs in context"
      ],
      "external_id": "87eeada741d7f22204b37e7d6a906b9546ab8d4a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/logs-context/c-sdk-configure-logs-context/",
      "published_at": "2021-10-18T05:57:03Z",
      "updated_at": "2021-10-06T21:51:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs in context is not available for apps monitored by the C SDK. Instead, you can build your own logging extension library by using the Log API. Make sure your app has distributed tracing enabled, so you can quickly understand what happens to requests as they travel through upstream and downstream services. To enable distributed tracing for apps monitored by the C SDK, install or update to the latest C SDK version. Distributed tracing requires C SDK version 1.1.0 or higher.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.7897,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "C SDK: <em>Configure</em> <em>logs</em> <em>in</em> <em>context</em>",
        "sections": "C SDK: <em>Configure</em> <em>logs</em> <em>in</em> <em>context</em>",
        "tags": "<em>Enable</em> <em>log</em> <em>management</em> <em>in</em> <em>New</em> <em>Relic</em>",
        "body": "<em>Logs</em> in <em>context</em> is not available for apps monitored by the C SDK. Instead, you can build your own logging extension library by using the <em>Log</em> API. Make sure your app has distributed tracing enabled, so you can quickly understand what happens to requests as they travel through upstream and downstream"
      },
      "id": "6127279b28ccbcf0c6f2618d"
    }
  ],
  "/docs/mobile-apps/new-relic-mobile-apps/android-app/android-app-ui": [
    {
      "sections": [
        "Introduction to New Relic Android app",
        "Requirements",
        "Install New Relic's mobile app",
        "View New Relic data",
        "New Relic product details",
        "Synthetics data",
        "Alerts",
        "Mobile app monitoring",
        "Details on setting time range",
        "Data privacy"
      ],
      "title": "Introduction to New Relic Android app",
      "type": "docs",
      "tags": [
        "Mobile apps",
        "New Relic mobile apps",
        "Android app"
      ],
      "external_id": "ff8415c00363a49eaa062f4b0b13c795b4717ea5",
      "image": "https://docs.newrelic.com/static/ea914fce17844b32fdabefd60efc457e/e5166/navigation_nexus.jpg",
      "url": "https://docs.newrelic.com/docs/mobile-apps/new-relic-mobile-apps/android-app/introduction-new-relic-android-app/",
      "published_at": "2021-10-18T15:35:57Z",
      "updated_at": "2021-09-14T07:28:16Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The user interface for New Relic's Android app provides functionality similar to New Relic's standard user interface, with customized details for mobile users. Requirements Requirements include: Android 4.0 (Ice Cream Sandwich) or higher Screen size of 7 inches or less Install New Relic's mobile app You can install the New Relic Android app from the Google Play Store or learn more from the New Relic website. Follow standard procedures to install any Android app, then sign in with your New Relic user name (account email) and password if applicable. Depending on your New Relic account, additional installation or user authentication steps may be required. View New Relic data To view details of your apps monitored by New Relic, select a product from the app's main menu. See below for details on how to use specific features of the app: New Relic product details The New Relic Android app includes data about these features: APM metrics, both real-time and historical data, including health maps. Select the transaction icon to see detailed transaction metrics, or an Overview chart to view summary charts of your top five transactions. Select the icon to filter by labels and categories. Browser monitoring metrics, including average page load time, Apdex, average throughput, and more. Infrastructure monitoring. Mobile monitoring, including crash reports, network errors, API calls, and active user count. Event notifications, including mobile alerts wherever you are, plus deployment notifications and notes. New Relic's Android app does not have the full feature set of the New Relic web interface. For more detailed analysis, sign in to your New Relic account with a web browser. Synthetics data You can use the Android app to view your synthetic monitoring data, including charts of your monitor's availability, load times, and load sizes. To view more detailed charts, select the caret icon. You can mute or disable your monitor, and view details of any recent errors. For scripted monitors, you can view and search the script log. Alerts When you log in to your New Relic account from the Android app, your device is automatically associated with your user channel. Then, you can add your user channel to your target policy to receive alerts. For Android alerts, notifications appear on your lock screen. To view them, tap the alert event. You can select any alert to view error details or acknowledge the alert. New Relic also sends a push notification when a colleague acknowledges an open event. Then, New Relic sends a final, closing notification when all Critical events end. Mobile app monitoring If you have installed mobile monitoring, you can monitor its performance directly from your Android device. Mobile monitoring includes network errors, API calls, and number of active users. You can also view detailed individual crash reports for a deeper understanding of a particular crash incident. Details on setting time range When viewing an application or host, you can change the visible time frame with the time picker. To move back and forth across the timeline, scrub the New Relic charts. To change the duration of the visible time slice, select the clock icon. To specify an end time other than now, slide the toggle from Ending Now to Custom Date. To save your changes and refresh the chart data, select the clock icon again. Data privacy New Relic's mobile apps only record information needed to help authenticate and troubleshoot: User's email address associated with your New Relic account, including first and last name (for authentication purposes only) IP address Device ID For more information, see our Mobile data privacy and security documentation.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 220.78497,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>New</em> <em>Relic</em> <em>Android</em> <em>app</em>",
        "sections": "Install <em>New</em> <em>Relic&#x27;s</em> <em>mobile</em> <em>app</em>",
        "tags": "<em>New</em> <em>Relic</em> <em>mobile</em> <em>apps</em>",
        "body": "The user interface for <em>New</em> <em>Relic</em>&#x27;s <em>Android</em> <em>app</em> provides functionality similar to <em>New</em> <em>Relic</em>&#x27;s standard user interface, with customized details for <em>mobile</em> users. Requirements Requirements include: <em>Android</em> 4.0 (Ice Cream Sandwich) or higher Screen size of 7 inches or less Install <em>New</em> <em>Relic</em>&#x27;s <em>mobile</em>"
      },
      "id": "604415e0196a67ff23960f46"
    },
    {
      "sections": [
        "Introduction to iOS mobile app",
        "Features",
        "Time range",
        "Synthetic monitoring",
        "Alerts",
        "Mobile monitoring",
        "Data privacy"
      ],
      "title": "Introduction to iOS mobile app",
      "type": "docs",
      "tags": [
        "Mobile apps",
        "New Relic mobile apps",
        "iOS app"
      ],
      "external_id": "371077582a50dfd2a1e7c57cfbbf9eeaf8013e1c",
      "image": "https://docs.newrelic.com/static/630c7a9a486540073ab96a2c9926e303/442cb/device-ios-synthetics-view-monitor.png",
      "url": "https://docs.newrelic.com/docs/mobile-apps/new-relic-mobile-apps/ios-app/introduction-ios-mobile-app/",
      "published_at": "2021-10-18T14:49:05Z",
      "updated_at": "2021-09-14T07:29:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The user interface for New Relic's iPhone and iPad app provides functionality similar to New Relic's standard user interface, with customized details for mobile users. The New Relic iOS apps show near real-time information about your apps, hosts, and more. Features New Relic's iOS app includes these New Relic products and features: New Relic's iOS app for iPhone and iPad includes these New Relic products and features: APM (iPhone and iPad). Includes real-time and historical data. Select the icon to see transaction details. Select Overview Charts to view summary charts of your top five transactions. Browser monitoring (iPhone and iPad). Provide overview dashboard, including average page load time, browser Apdex, average throughput, and more. Infrastructure monitoring (iPhone only). Alerts (iPhone and iPad). Get alert and deployment notifications. Synthetic monitoring (iPhone only). Mobile monitoring (iPhone and iPad). Includes crash reports, network errors, API calls, and active user count. New Relic's iOS app does not have all the features of the New Relic web application. For more detailed analysis, sign in to your New Relic account with a web browser. Time range When viewing an application or host, you can change the visible time frame by using the clock icon in the top right of the page. This feature is similar to the standard New Relic time picker. Features include: Scrub the New Relic charts to move back and forth across the timeline. Select the time picker to choose a time range that ends now (from 30 minutes to 90 days ago). For iPads: to specify an end time other than now, slide the toggle from Ending Now to Custom Date. Synthetic monitoring You can use the iOS app to view your synthetic monitoring data, including charts of your monitor's availability, load times, and load sizes. Select the caret icon to view more detailed charts. You can mute or disable your monitor, and view details of any recent errors. For scripted monitors, you can view and search the script log. Alerts When you connect the iOS app to your New Relic account, your device is automatically associated with your user channel. Then, you can add your user channel to your target policy to receive alerts. For iOS alerts, notifications appear on your lock screen and can be viewed by swiping the alert. You can select any alert to view error details or acknowledge the alert. New Relic also sends a push notification when a colleague acknowledges an open event. Then, New Relic sends a final, closing notification when all Critical events end. Mobile monitoring If you have a mobile application and have installed mobile monitoring, you can monitor its performance directly from your iPhone or iPad. Mobile monitoring includes network errors, API calls, and number of active users. You can also view detailed individual crash reports for a deeper understanding of a particular crash incident. Data privacy New Relic's mobile apps only record information needed to help authenticate and troubleshoot: User's email address associated with your New Relic account, including first and last name (for authentication purposes only) IP address Device ID For more information, see our Mobile data privacy and security documentation.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.95755,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to iOS <em>mobile</em> <em>app</em>",
        "sections": "Introduction to iOS <em>mobile</em> <em>app</em>",
        "tags": "<em>New</em> <em>Relic</em> <em>mobile</em> <em>apps</em>",
        "body": "The user interface for <em>New</em> <em>Relic</em>&#x27;s iPhone and iPad <em>app</em> provides functionality similar to <em>New</em> <em>Relic</em>&#x27;s standard user interface, with customized details for <em>mobile</em> users. The <em>New</em> <em>Relic</em> iOS <em>apps</em> show near real-time information about your <em>apps</em>, hosts, and more. Features <em>New</em> <em>Relic</em>&#x27;s iOS <em>app</em> includes"
      },
      "id": "6044161628ccbc96b62c6092"
    },
    {
      "sections": [
        "Mobile app authentication for New Relic partners",
        "Important",
        "Confirm your email address",
        "Troubleshoot email problems"
      ],
      "title": "Mobile app authentication for New Relic partners",
      "type": "docs",
      "tags": [
        "Mobile apps",
        "New Relic mobile apps",
        "Authentication and alerts"
      ],
      "external_id": "de6bdd35891dbbfea0ae914251a9d5c4487594a9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/mobile-monitoring/new-relic-mobile-apps/mobile-app-features/authentication-partner-saml-sso-accounts/",
      "published_at": "2021-10-18T06:25:52Z",
      "updated_at": "2021-03-13T03:57:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This resource is for New Relic partners. For authentication of users in regular New Relic accounts, see Authentication. Partner account users typically use SAML-SSO to sign in through your New Relic partner site. You may not have separate passwords or authentication information for your New Relic account. If you use an email address associated with a New Relic partner account when you first sign in to the New Relic mobile app, New Relic will send you a confirmation email for authentication. Android app users will also see a notification message. Important The authentication email expires 20 minutes after it is sent. Confirm your email address To authenticate using a SAML-SSO account provided through a New Relic partner: From the New Relic mobile app, type your email address associated with the partner account. Select I don't have a password. Retrieve the authentication email from your mobile device within 20 minutes. Select the Authenticate button (Android users) or email link (Android or iOS users) in the email to log in to New Relic. You will be redirected to the New Relic mobile app and logged in to your partner account. Troubleshoot email problems Here are some troubleshooting tips: If you cannot find the authentication message from New Relic in your mobile device's email in-box, check your Spam folder. If you miss the 20-minute deadline, sign in to the New Relic mobile app again, then select the link to resend the authentication email.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.36029,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Mobile</em> <em>app</em> authentication for <em>New</em> <em>Relic</em> partners",
        "sections": "<em>Mobile</em> <em>app</em> authentication for <em>New</em> <em>Relic</em> partners",
        "tags": "<em>New</em> <em>Relic</em> <em>mobile</em> <em>apps</em>",
        "body": " <em>New</em> <em>Relic</em> account. If you use an email address associated with a <em>New</em> <em>Relic</em> partner account when you first sign in to the <em>New</em> <em>Relic</em> <em>mobile</em> <em>app</em>, <em>New</em> <em>Relic</em> will send you a confirmation email for authentication. <em>Android</em> <em>app</em> users will also see a notification message. Important The authentication email"
      },
      "id": "604418de28ccbc28932c6071"
    }
  ],
  "/docs/mobile-apps/new-relic-mobile-apps/android-app/introduction-new-relic-android-app": [
    {
      "sections": [
        "Android app UI",
        "Pages",
        "Time range",
        "New Relic Synthetics",
        "Alerts",
        "Mobile apps",
        "For more help"
      ],
      "title": "Android app UI",
      "type": "docs",
      "tags": [
        "Mobile apps",
        "New Relic mobile apps",
        "Android app"
      ],
      "external_id": "8918a5a2454491a91421c55e26501a0e3f64cd3a",
      "image": "https://docs.newrelic.com/static/fc97ade0bbdbdef58b89495a0d91b734/edd00/deployment-markers_nexus.jpg",
      "url": "https://docs.newrelic.com/docs/mobile-apps/new-relic-mobile-apps/android-app/android-app-ui/",
      "published_at": "2021-10-18T15:36:53Z",
      "updated_at": "2021-09-14T07:28:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The UI for the New Relic Android app provides functionality similar to the standard user interface, with customized details for mobile users. Pages To view details of your New Relic apps, hosts, Synthetics monitors, Alerts, plugins, and key transactions, select a product from the main menu. The New Relic Android app includes: APM metrics, both real-time and historical data, including health maps. And, select the transaction icon for detailed transaction metrics, or an Overview Charts to view summary charts of your top five transactions. New Relic Infrastructure utilization. New Relic Plugins, including a list of their components or instances, and their charts and current values from the plugin's Summary. Mobile monitoring, including crash reports, network errors, API calls, and active user count. Select the filter icon to filter by labels and categories. Event notifications, including mobile alerts wherever you are, plus deployment notifications and notes. Note: New Relic's Android app does not have the full feature set of the New Relic web interface. For more detailed analysis, sign in to your New Relic account with a web browser. Time range When viewing an application or host, you can change the visible time frame by using the time picker icon in the top right of the page (the 7D in the screenshot). This feature is similar to the standard New Relic time picker. Features include: Scrub the New Relic charts to move back and forth in the timeline. Select the time picker to choose a time range that ends now (from 30 minutes to 90 days ago). New Relic Synthetics You can use the Android app to view your New Relic Synthetics data, including charts of your monitor's availability, load times, and load sizes. Select the caret icon to view more detailed charts. You can mute or disable your monitor, and view details of any recent errors. For scripted monitors, you can view and search the script log. Alerts When you log in to your New Relic account from the Android app, your device is automatically associated with your user channel. Then, you can add your user channel to your target policy to receive alerts. For Android alerts, notifications appear on your lock screen and can be viewed by tapping the alert. You can select any alert to view error details or acknowledge the alert. New Relic also sends a push notification when a colleague acknowledges an open event. Then, New Relic sends a final, closing notification when all Critical events end. Mobile apps If you have a mobile application and have installed mobile monitoring, you can monitor its performance directly from your Android device. Mobile monitoring includes network errors, API calls, and number of active users. You can also view detailed individual crash reports for a deeper understanding of a particular crash incident. For more help Additional documentation resources include: New Relic Android app (compatibility, requirements, installation) Android authentication (procedures to add or remove users, and for the users to authenticate with their Android device)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 221.41202,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Android</em> <em>app</em> UI",
        "sections": "<em>Mobile</em> <em>apps</em>",
        "tags": "<em>New</em> <em>Relic</em> <em>mobile</em> <em>apps</em>",
        "body": "The UI for the <em>New</em> <em>Relic</em> <em>Android</em> <em>app</em> provides functionality similar to the standard user interface, with customized details for <em>mobile</em> users. Pages To view details of your <em>New</em> <em>Relic</em> <em>apps</em>, hosts, Synthetics monitors, Alerts, plugins, and key transactions, select a product from the main menu. The <em>New</em>"
      },
      "id": "6044181d28ccbc9a522c60a5"
    },
    {
      "sections": [
        "Introduction to iOS mobile app",
        "Features",
        "Time range",
        "Synthetic monitoring",
        "Alerts",
        "Mobile monitoring",
        "Data privacy"
      ],
      "title": "Introduction to iOS mobile app",
      "type": "docs",
      "tags": [
        "Mobile apps",
        "New Relic mobile apps",
        "iOS app"
      ],
      "external_id": "371077582a50dfd2a1e7c57cfbbf9eeaf8013e1c",
      "image": "https://docs.newrelic.com/static/630c7a9a486540073ab96a2c9926e303/442cb/device-ios-synthetics-view-monitor.png",
      "url": "https://docs.newrelic.com/docs/mobile-apps/new-relic-mobile-apps/ios-app/introduction-ios-mobile-app/",
      "published_at": "2021-10-18T14:49:05Z",
      "updated_at": "2021-09-14T07:29:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The user interface for New Relic's iPhone and iPad app provides functionality similar to New Relic's standard user interface, with customized details for mobile users. The New Relic iOS apps show near real-time information about your apps, hosts, and more. Features New Relic's iOS app includes these New Relic products and features: New Relic's iOS app for iPhone and iPad includes these New Relic products and features: APM (iPhone and iPad). Includes real-time and historical data. Select the icon to see transaction details. Select Overview Charts to view summary charts of your top five transactions. Browser monitoring (iPhone and iPad). Provide overview dashboard, including average page load time, browser Apdex, average throughput, and more. Infrastructure monitoring (iPhone only). Alerts (iPhone and iPad). Get alert and deployment notifications. Synthetic monitoring (iPhone only). Mobile monitoring (iPhone and iPad). Includes crash reports, network errors, API calls, and active user count. New Relic's iOS app does not have all the features of the New Relic web application. For more detailed analysis, sign in to your New Relic account with a web browser. Time range When viewing an application or host, you can change the visible time frame by using the clock icon in the top right of the page. This feature is similar to the standard New Relic time picker. Features include: Scrub the New Relic charts to move back and forth across the timeline. Select the time picker to choose a time range that ends now (from 30 minutes to 90 days ago). For iPads: to specify an end time other than now, slide the toggle from Ending Now to Custom Date. Synthetic monitoring You can use the iOS app to view your synthetic monitoring data, including charts of your monitor's availability, load times, and load sizes. Select the caret icon to view more detailed charts. You can mute or disable your monitor, and view details of any recent errors. For scripted monitors, you can view and search the script log. Alerts When you connect the iOS app to your New Relic account, your device is automatically associated with your user channel. Then, you can add your user channel to your target policy to receive alerts. For iOS alerts, notifications appear on your lock screen and can be viewed by swiping the alert. You can select any alert to view error details or acknowledge the alert. New Relic also sends a push notification when a colleague acknowledges an open event. Then, New Relic sends a final, closing notification when all Critical events end. Mobile monitoring If you have a mobile application and have installed mobile monitoring, you can monitor its performance directly from your iPhone or iPad. Mobile monitoring includes network errors, API calls, and number of active users. You can also view detailed individual crash reports for a deeper understanding of a particular crash incident. Data privacy New Relic's mobile apps only record information needed to help authenticate and troubleshoot: User's email address associated with your New Relic account, including first and last name (for authentication purposes only) IP address Device ID For more information, see our Mobile data privacy and security documentation.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.95755,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to iOS <em>mobile</em> <em>app</em>",
        "sections": "Introduction to iOS <em>mobile</em> <em>app</em>",
        "tags": "<em>New</em> <em>Relic</em> <em>mobile</em> <em>apps</em>",
        "body": "The user interface for <em>New</em> <em>Relic</em>&#x27;s iPhone and iPad <em>app</em> provides functionality similar to <em>New</em> <em>Relic</em>&#x27;s standard user interface, with customized details for <em>mobile</em> users. The <em>New</em> <em>Relic</em> iOS <em>apps</em> show near real-time information about your <em>apps</em>, hosts, and more. Features <em>New</em> <em>Relic</em>&#x27;s iOS <em>app</em> includes"
      },
      "id": "6044161628ccbc96b62c6092"
    },
    {
      "sections": [
        "Mobile app authentication for New Relic partners",
        "Important",
        "Confirm your email address",
        "Troubleshoot email problems"
      ],
      "title": "Mobile app authentication for New Relic partners",
      "type": "docs",
      "tags": [
        "Mobile apps",
        "New Relic mobile apps",
        "Authentication and alerts"
      ],
      "external_id": "de6bdd35891dbbfea0ae914251a9d5c4487594a9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/mobile-monitoring/new-relic-mobile-apps/mobile-app-features/authentication-partner-saml-sso-accounts/",
      "published_at": "2021-10-18T06:25:52Z",
      "updated_at": "2021-03-13T03:57:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This resource is for New Relic partners. For authentication of users in regular New Relic accounts, see Authentication. Partner account users typically use SAML-SSO to sign in through your New Relic partner site. You may not have separate passwords or authentication information for your New Relic account. If you use an email address associated with a New Relic partner account when you first sign in to the New Relic mobile app, New Relic will send you a confirmation email for authentication. Android app users will also see a notification message. Important The authentication email expires 20 minutes after it is sent. Confirm your email address To authenticate using a SAML-SSO account provided through a New Relic partner: From the New Relic mobile app, type your email address associated with the partner account. Select I don't have a password. Retrieve the authentication email from your mobile device within 20 minutes. Select the Authenticate button (Android users) or email link (Android or iOS users) in the email to log in to New Relic. You will be redirected to the New Relic mobile app and logged in to your partner account. Troubleshoot email problems Here are some troubleshooting tips: If you cannot find the authentication message from New Relic in your mobile device's email in-box, check your Spam folder. If you miss the 20-minute deadline, sign in to the New Relic mobile app again, then select the link to resend the authentication email.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.36029,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Mobile</em> <em>app</em> authentication for <em>New</em> <em>Relic</em> partners",
        "sections": "<em>Mobile</em> <em>app</em> authentication for <em>New</em> <em>Relic</em> partners",
        "tags": "<em>New</em> <em>Relic</em> <em>mobile</em> <em>apps</em>",
        "body": " <em>New</em> <em>Relic</em> account. If you use an email address associated with a <em>New</em> <em>Relic</em> partner account when you first sign in to the <em>New</em> <em>Relic</em> <em>mobile</em> <em>app</em>, <em>New</em> <em>Relic</em> will send you a confirmation email for authentication. <em>Android</em> <em>app</em> users will also see a notification message. Important The authentication email"
      },
      "id": "604418de28ccbc28932c6071"
    }
  ],
  "/docs/mobile-apps/new-relic-mobile-apps/authentication-alerts/alerting-new-relic-mobile-apps": [
    {
      "sections": [
        "User settings and authentication",
        "User authentication",
        "User settings",
        "Sign in with additional username",
        "Switch between accounts",
        "Remove or re-add a user name"
      ],
      "title": "User settings and authentication",
      "type": "docs",
      "tags": [
        "Mobile apps",
        "New Relic mobile apps",
        "Authentication and alerts"
      ],
      "external_id": "0b40ac4c2e769279d25d0ebb2ea77cebda8d8ea7",
      "image": "https://docs.newrelic.com/static/88ff328efc4a127601923bc728fea229/8c557/device-ipad-switch-user.png",
      "url": "https://docs.newrelic.com/docs/mobile-apps/new-relic-mobile-apps/authentication-alerts/user-settings-authentication/",
      "published_at": "2021-10-18T14:48:15Z",
      "updated_at": "2021-05-16T06:27:19Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This explains how to authenticate your New Relic mobile app account, and how to add users to or remove them from your mobile device. User authentication Depending on your New Relic account, additional installation or authentication steps may be required when you install the New Relic mobile app. New Relic account Additional requirements New users If you do not already have a New Relic account: From your desktop web browser, create a New Relic account. Install your application with the appropriate New Relic agent. As part of new account setup, you will receive an email with a password reset link. The password reset link expires after 20 minutes for mobile apps. Existing New Relic users No additional requirements; your applications, hosts, installed plugins, and key transactions automatically appear after you sign in. Users with New Relic partner accounts Depending on the partner, you may need to complete a different authentication process. Azure Store users: Due to the deep integration between Azure Storefront and New Relic, Azure Storefront users cannot access their accounts on the New Relic Android or iOS apps. Users with SAML-SSO enabled accounts When you sign in to the New Relic mobile app, your session automatically redirects to your web browser. From there you can sign in to your New Relic SAML-SSO account. If you see any errors when using SAML-SSO accounts on your mobile device, verify that you are able to sign in to one.newrelic.com with a desktop web browser. If no, contact your administrator. If yes, get support at support.newrelic.com. User settings After you sign in, all New Relic accounts and applications associated with the user appear automatically. Sign in with additional username Follow the procedure for your mobile device. Mobile device To sign in to the app with an additional user name: Android To switch users: Log out from the Android device: Main menu > (selected username) > Logout > Confirm. Log in with a new account. iPhone From the app menu, select your account name, then select the Users menu. From the Users menu, select the plus icon. Sign in with the additional username. iPad To access the Users menu: Select the user icon or slide right. From the Users menu, select the plus icon. Sign in with the additional username. Switch between accounts To switch between accounts associated with your username: From the Users menu, select the user name. Select the account name. Remove or re-add a user name To remove a specific username from this device: From the Users menu, select Logout. To remove a user from this device, select the user's red minus icon. Select the user's Log out icon. To add a user again, sign in with that username again. Select the user icon or slide right to show the New Relic iPad app's Users menu.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 205.7964,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "User settings <em>and</em> <em>authentication</em>",
        "sections": "User settings <em>and</em> <em>authentication</em>",
        "tags": "<em>New</em> <em>Relic</em> <em>mobile</em> <em>apps</em>",
        "body": "This explains how to authenticate your <em>New</em> <em>Relic</em> <em>mobile</em> <em>app</em> account, and how to add users to or remove them from your <em>mobile</em> device. User <em>authentication</em> Depending on your <em>New</em> <em>Relic</em> account, additional installation or <em>authentication</em> steps may be required when you install the <em>New</em> <em>Relic</em> <em>mobile</em> <em>app</em>"
      },
      "id": "604415a728ccbc8fb52c6068"
    },
    {
      "sections": [
        "Troubleshoot SSO accounts using mobile devices",
        "No user name or password",
        "Errors after signing in",
        "Reauthentication problems"
      ],
      "title": "Troubleshoot SSO accounts using mobile devices",
      "type": "docs",
      "tags": [
        "Mobile apps",
        "New Relic mobile apps",
        "Authentication and alerts"
      ],
      "external_id": "9ebb373182ce5fea83ba5a6baa03b2c7bccf0174",
      "image": "",
      "url": "https://docs.newrelic.com/docs/mobile-apps/new-relic-mobile-apps/authentication-alerts/troubleshoot-sso-accounts-using-mobile-devices/",
      "published_at": "2021-10-18T15:36:50Z",
      "updated_at": "2021-05-16T06:23:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Typically when you sign in to the New Relic mobile app, your session redirects automatically to your web browser. From there you can sign in to your New Relic account. Here are troubleshooting tips if you have problems using the New Relic mobile app with your SAML-SSO enabled account. No user name or password You may not have a user name or password for New Relic because some SAML providers will overwrite your password, or because your administrator has not sent you this information. In these situations: From the mobile app's Log in, select the I don't have a password link. Use your mobile device to open your email account. From your email account, retrieve the New Relic authentication email within 20 minutes. Select the Authenticate button or the link below it in the email. Errors after signing in If you see any errors after successfully signing in to your SSO provider with your mobile device, verify that you are able to sign in to one.newrelic.com with a desktop web browser. If no, contact your administrator. If yes, get support at support.newrelic.com. Reauthentication problems If you are using reauthentication on a SAML-SSO account, you must log in to your default account. (All other accounts will be grayed out.) If you attempt to switch to a grayed-out account, an error message will appear, explaining this is currently not supported.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 205.79622,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Troubleshoot SSO accounts using <em>mobile</em> devices",
        "sections": "Troubleshoot SSO accounts using <em>mobile</em> devices",
        "tags": "<em>New</em> <em>Relic</em> <em>mobile</em> <em>apps</em>",
        "body": "Typically when you sign in to the <em>New</em> <em>Relic</em> <em>mobile</em> <em>app</em>, your session redirects automatically to your web browser. From there you can sign in to your <em>New</em> <em>Relic</em> account. Here are troubleshooting tips if you have problems using the <em>New</em> <em>Relic</em> <em>mobile</em> <em>app</em> with your SAML-SSO enabled account. No user name"
      },
      "id": "604415e0196a67fc3f960f42"
    },
    {
      "sections": [
        "Mobile app authentication for New Relic partners",
        "Important",
        "Confirm your email address",
        "Troubleshoot email problems"
      ],
      "title": "Mobile app authentication for New Relic partners",
      "type": "docs",
      "tags": [
        "Mobile apps",
        "New Relic mobile apps",
        "Authentication and alerts"
      ],
      "external_id": "de6bdd35891dbbfea0ae914251a9d5c4487594a9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/mobile-monitoring/new-relic-mobile-apps/mobile-app-features/authentication-partner-saml-sso-accounts/",
      "published_at": "2021-10-18T06:25:52Z",
      "updated_at": "2021-03-13T03:57:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This resource is for New Relic partners. For authentication of users in regular New Relic accounts, see Authentication. Partner account users typically use SAML-SSO to sign in through your New Relic partner site. You may not have separate passwords or authentication information for your New Relic account. If you use an email address associated with a New Relic partner account when you first sign in to the New Relic mobile app, New Relic will send you a confirmation email for authentication. Android app users will also see a notification message. Important The authentication email expires 20 minutes after it is sent. Confirm your email address To authenticate using a SAML-SSO account provided through a New Relic partner: From the New Relic mobile app, type your email address associated with the partner account. Select I don't have a password. Retrieve the authentication email from your mobile device within 20 minutes. Select the Authenticate button (Android users) or email link (Android or iOS users) in the email to log in to New Relic. You will be redirected to the New Relic mobile app and logged in to your partner account. Troubleshoot email problems Here are some troubleshooting tips: If you cannot find the authentication message from New Relic in your mobile device's email in-box, check your Spam folder. If you miss the 20-minute deadline, sign in to the New Relic mobile app again, then select the link to resend the authentication email.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 202.20352,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Mobile</em> <em>app</em> <em>authentication</em> for <em>New</em> <em>Relic</em> partners",
        "sections": "<em>Mobile</em> <em>app</em> <em>authentication</em> for <em>New</em> <em>Relic</em> partners",
        "tags": "<em>New</em> <em>Relic</em> <em>mobile</em> <em>apps</em>",
        "body": " <em>New</em> <em>Relic</em> account. If you use an email address associated with a <em>New</em> <em>Relic</em> partner account when you first sign in to the <em>New</em> <em>Relic</em> <em>mobile</em> <em>app</em>, <em>New</em> <em>Relic</em> will send you a confirmation email for <em>authentication</em>. Android <em>app</em> users will also see a notification message. Important The <em>authentication</em> email"
      },
      "id": "604418de28ccbc28932c6071"
    }
  ],
  "/docs/mobile-apps/new-relic-mobile-apps/authentication-alerts/troubleshoot-sso-accounts-using-mobile-devices": [
    {
      "sections": [
        "Alerting with New Relic mobile apps",
        "Requirements",
        "Turn notifications on or off",
        "View alert incident details",
        "Troubleshoot alert settings",
        "Check notification settings for your mobile device.",
        "Delete the Android or iOS device from your New Relic account.",
        "Uninstall the New Relic mobile app.",
        "Reinstall the New Relic mobile app."
      ],
      "title": "Alerting with New Relic mobile apps",
      "type": "docs",
      "tags": [
        "Mobile apps",
        "New Relic mobile apps",
        "Authentication and alerts"
      ],
      "external_id": "d55850dc642cc8ade20310e1d4654db61af1e809",
      "image": "https://docs.newrelic.com/static/f942198cbd9a41b7355ef7f01fa6cc66/e5166/alerts-incident-detail-nexus.jpg",
      "url": "https://docs.newrelic.com/docs/mobile-apps/new-relic-mobile-apps/authentication-alerts/alerting-new-relic-mobile-apps/",
      "published_at": "2021-10-18T14:48:16Z",
      "updated_at": "2021-07-09T12:24:33Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Account administrators can set up configuration to receive push notifications on Android and iOS devices from New Relic Alerts. You can receive alerts from any policy by attaching a user channel to the policy. Requirements This feature is available only to users on the original user model, not to users on the New Relic One user model. As a workaround, you can use the email notification channel. Turn notifications on or off When you log in to your New Relic account from an Android or iOS app, your device is automatically associated with your user channel. Be sure to add the associated user channel to the alert policy. View alert incident details The notification automatically appears on your device's lock screen. To start the New Relic app: Android devices: Tap the notification from the notification drawer. OR iOS devices: Swipe the screen. From the New Relic app's Alerts menu, select any alert to view error details for the associated application. Optional: Select Acknowledge. Optional: To view additional details, select Overview, Violations, or Event log. The main menu's Alerts list shows alerts in the following order, sorted by time: Active incidents Resolved incidents from today Resolved incidents and events from the past week, organized by day Troubleshoot alert settings If alerts are not working on your mobile device: Verify that you meet the requirements. Verify that alerts are enabled. Check your mobile device's notification settings, to ensure New Relic is permitted to send alerts. If the notification settings for your mobile device are correct, but you still do not receive notifications, delete the device from your account, then uninstall and reinstall the New Relic application. Check notification settings for your mobile device. Follow the procedure for your mobile device. Device To check notification settings: Android From your Android device's Settings, select Sound and notification. Check the settings for sound volume. Optional: Enable Also vibrate for calls. Check the settings for Interruptions. Check the settings for Notification. Check the settings for App notifications: Select the New Relic app, then check the settings for Block and Priority. iOS Ensure Do Not Disturb is off: From the iOS Settings app, select Do Not Disturb, and check that the Manual switch is off. Ensure the New Relic app is allowed to send notifications: From the iOS Settings app, select Notifications, and locate the New Relic app from the app list. Ensure that the Allow Notifications switch is on. Ensure that the alert style is set to Banners or Alerts. Optional: To enable audio alerts, set Sounds to on. Delete the Android or iOS device from your New Relic account. To delete the mobile device from your New Relic account, use the public graphql api api.newrelic.com/graphiql in a web browser: Query current devices by selecting actor -> mobilePushNotification -> devices and selecting appVersion, deviceId, and deviceName. Run this query to get the list of devices. Mutate to remove a device by selecting mutation -> mobilePushNotificationRemoveDevice, and passing in the deviceId from the list above. Or you can remove the device from the in-app Settings option from the menu -> Settings Look under Push notification devices, and remove from there. On iOS, slide from right to left to Delete a device, on Android, tap Delete Continue with the steps to reinstall the New Relic app from your device. Uninstall the New Relic mobile app. Follow the procedure to uninstall the New Relic app from your device, then reinstall it. Device To uninstall the New Relic app: Android From your Android device's Settings, select Apps, then select the New Relic app. Select Uninstall. Continue with the steps to reinstall the New Relic app. iOS From your iOS home screen, tap and hold the New Relic icon until it shakes. To delete the app, select the X icon. Continue with the steps to reinstall the New Relic app. Reinstall the New Relic mobile app. To reinstall the New Relic mobile app: From your Android device, select Google Play Store. OR From your iOS device's home screen, select App Store. Search for New Relic. Download the app. When the download finishes, sign in to your New Relic mobile app with your New Relic account.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 212.08412,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Alerting</em> with <em>New</em> <em>Relic</em> <em>mobile</em> <em>apps</em>",
        "sections": "<em>Alerting</em> with <em>New</em> <em>Relic</em> <em>mobile</em> <em>apps</em>",
        "tags": "<em>New</em> <em>Relic</em> <em>mobile</em> <em>apps</em>",
        "body": " channel to the <em>alert</em> policy. View <em>alert</em> incident details The notification automatically appears on your device&#x27;s lock screen. To start the <em>New</em> <em>Relic</em> <em>app</em>: Android devices: Tap the notification from the notification drawer. OR iOS devices: Swipe the screen. From the <em>New</em> <em>Relic</em> <em>app</em>&#x27;s <em>Alerts</em> menu, select"
      },
      "id": "603e9efd64441f19a14e88ab"
    },
    {
      "sections": [
        "User settings and authentication",
        "User authentication",
        "User settings",
        "Sign in with additional username",
        "Switch between accounts",
        "Remove or re-add a user name"
      ],
      "title": "User settings and authentication",
      "type": "docs",
      "tags": [
        "Mobile apps",
        "New Relic mobile apps",
        "Authentication and alerts"
      ],
      "external_id": "0b40ac4c2e769279d25d0ebb2ea77cebda8d8ea7",
      "image": "https://docs.newrelic.com/static/88ff328efc4a127601923bc728fea229/8c557/device-ipad-switch-user.png",
      "url": "https://docs.newrelic.com/docs/mobile-apps/new-relic-mobile-apps/authentication-alerts/user-settings-authentication/",
      "published_at": "2021-10-18T14:48:15Z",
      "updated_at": "2021-05-16T06:27:19Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This explains how to authenticate your New Relic mobile app account, and how to add users to or remove them from your mobile device. User authentication Depending on your New Relic account, additional installation or authentication steps may be required when you install the New Relic mobile app. New Relic account Additional requirements New users If you do not already have a New Relic account: From your desktop web browser, create a New Relic account. Install your application with the appropriate New Relic agent. As part of new account setup, you will receive an email with a password reset link. The password reset link expires after 20 minutes for mobile apps. Existing New Relic users No additional requirements; your applications, hosts, installed plugins, and key transactions automatically appear after you sign in. Users with New Relic partner accounts Depending on the partner, you may need to complete a different authentication process. Azure Store users: Due to the deep integration between Azure Storefront and New Relic, Azure Storefront users cannot access their accounts on the New Relic Android or iOS apps. Users with SAML-SSO enabled accounts When you sign in to the New Relic mobile app, your session automatically redirects to your web browser. From there you can sign in to your New Relic SAML-SSO account. If you see any errors when using SAML-SSO accounts on your mobile device, verify that you are able to sign in to one.newrelic.com with a desktop web browser. If no, contact your administrator. If yes, get support at support.newrelic.com. User settings After you sign in, all New Relic accounts and applications associated with the user appear automatically. Sign in with additional username Follow the procedure for your mobile device. Mobile device To sign in to the app with an additional user name: Android To switch users: Log out from the Android device: Main menu > (selected username) > Logout > Confirm. Log in with a new account. iPhone From the app menu, select your account name, then select the Users menu. From the Users menu, select the plus icon. Sign in with the additional username. iPad To access the Users menu: Select the user icon or slide right. From the Users menu, select the plus icon. Sign in with the additional username. Switch between accounts To switch between accounts associated with your username: From the Users menu, select the user name. Select the account name. Remove or re-add a user name To remove a specific username from this device: From the Users menu, select Logout. To remove a user from this device, select the user's red minus icon. Select the user's Log out icon. To add a user again, sign in with that username again. Select the user icon or slide right to show the New Relic iPad app's Users menu.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 205.7964,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "User settings <em>and</em> <em>authentication</em>",
        "sections": "User settings <em>and</em> <em>authentication</em>",
        "tags": "<em>New</em> <em>Relic</em> <em>mobile</em> <em>apps</em>",
        "body": "This explains how to authenticate your <em>New</em> <em>Relic</em> <em>mobile</em> <em>app</em> account, and how to add users to or remove them from your <em>mobile</em> device. User <em>authentication</em> Depending on your <em>New</em> <em>Relic</em> account, additional installation or <em>authentication</em> steps may be required when you install the <em>New</em> <em>Relic</em> <em>mobile</em> <em>app</em>"
      },
      "id": "604415a728ccbc8fb52c6068"
    },
    {
      "sections": [
        "Mobile app authentication for New Relic partners",
        "Important",
        "Confirm your email address",
        "Troubleshoot email problems"
      ],
      "title": "Mobile app authentication for New Relic partners",
      "type": "docs",
      "tags": [
        "Mobile apps",
        "New Relic mobile apps",
        "Authentication and alerts"
      ],
      "external_id": "de6bdd35891dbbfea0ae914251a9d5c4487594a9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/mobile-monitoring/new-relic-mobile-apps/mobile-app-features/authentication-partner-saml-sso-accounts/",
      "published_at": "2021-10-18T06:25:52Z",
      "updated_at": "2021-03-13T03:57:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This resource is for New Relic partners. For authentication of users in regular New Relic accounts, see Authentication. Partner account users typically use SAML-SSO to sign in through your New Relic partner site. You may not have separate passwords or authentication information for your New Relic account. If you use an email address associated with a New Relic partner account when you first sign in to the New Relic mobile app, New Relic will send you a confirmation email for authentication. Android app users will also see a notification message. Important The authentication email expires 20 minutes after it is sent. Confirm your email address To authenticate using a SAML-SSO account provided through a New Relic partner: From the New Relic mobile app, type your email address associated with the partner account. Select I don't have a password. Retrieve the authentication email from your mobile device within 20 minutes. Select the Authenticate button (Android users) or email link (Android or iOS users) in the email to log in to New Relic. You will be redirected to the New Relic mobile app and logged in to your partner account. Troubleshoot email problems Here are some troubleshooting tips: If you cannot find the authentication message from New Relic in your mobile device's email in-box, check your Spam folder. If you miss the 20-minute deadline, sign in to the New Relic mobile app again, then select the link to resend the authentication email.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 202.20352,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Mobile</em> <em>app</em> <em>authentication</em> for <em>New</em> <em>Relic</em> partners",
        "sections": "<em>Mobile</em> <em>app</em> <em>authentication</em> for <em>New</em> <em>Relic</em> partners",
        "tags": "<em>New</em> <em>Relic</em> <em>mobile</em> <em>apps</em>",
        "body": " <em>New</em> <em>Relic</em> account. If you use an email address associated with a <em>New</em> <em>Relic</em> partner account when you first sign in to the <em>New</em> <em>Relic</em> <em>mobile</em> <em>app</em>, <em>New</em> <em>Relic</em> will send you a confirmation email for <em>authentication</em>. Android <em>app</em> users will also see a notification message. Important The <em>authentication</em> email"
      },
      "id": "604418de28ccbc28932c6071"
    }
  ],
  "/docs/mobile-apps/new-relic-mobile-apps/authentication-alerts/user-settings-authentication": [
    {
      "sections": [
        "Alerting with New Relic mobile apps",
        "Requirements",
        "Turn notifications on or off",
        "View alert incident details",
        "Troubleshoot alert settings",
        "Check notification settings for your mobile device.",
        "Delete the Android or iOS device from your New Relic account.",
        "Uninstall the New Relic mobile app.",
        "Reinstall the New Relic mobile app."
      ],
      "title": "Alerting with New Relic mobile apps",
      "type": "docs",
      "tags": [
        "Mobile apps",
        "New Relic mobile apps",
        "Authentication and alerts"
      ],
      "external_id": "d55850dc642cc8ade20310e1d4654db61af1e809",
      "image": "https://docs.newrelic.com/static/f942198cbd9a41b7355ef7f01fa6cc66/e5166/alerts-incident-detail-nexus.jpg",
      "url": "https://docs.newrelic.com/docs/mobile-apps/new-relic-mobile-apps/authentication-alerts/alerting-new-relic-mobile-apps/",
      "published_at": "2021-10-18T14:48:16Z",
      "updated_at": "2021-07-09T12:24:33Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Account administrators can set up configuration to receive push notifications on Android and iOS devices from New Relic Alerts. You can receive alerts from any policy by attaching a user channel to the policy. Requirements This feature is available only to users on the original user model, not to users on the New Relic One user model. As a workaround, you can use the email notification channel. Turn notifications on or off When you log in to your New Relic account from an Android or iOS app, your device is automatically associated with your user channel. Be sure to add the associated user channel to the alert policy. View alert incident details The notification automatically appears on your device's lock screen. To start the New Relic app: Android devices: Tap the notification from the notification drawer. OR iOS devices: Swipe the screen. From the New Relic app's Alerts menu, select any alert to view error details for the associated application. Optional: Select Acknowledge. Optional: To view additional details, select Overview, Violations, or Event log. The main menu's Alerts list shows alerts in the following order, sorted by time: Active incidents Resolved incidents from today Resolved incidents and events from the past week, organized by day Troubleshoot alert settings If alerts are not working on your mobile device: Verify that you meet the requirements. Verify that alerts are enabled. Check your mobile device's notification settings, to ensure New Relic is permitted to send alerts. If the notification settings for your mobile device are correct, but you still do not receive notifications, delete the device from your account, then uninstall and reinstall the New Relic application. Check notification settings for your mobile device. Follow the procedure for your mobile device. Device To check notification settings: Android From your Android device's Settings, select Sound and notification. Check the settings for sound volume. Optional: Enable Also vibrate for calls. Check the settings for Interruptions. Check the settings for Notification. Check the settings for App notifications: Select the New Relic app, then check the settings for Block and Priority. iOS Ensure Do Not Disturb is off: From the iOS Settings app, select Do Not Disturb, and check that the Manual switch is off. Ensure the New Relic app is allowed to send notifications: From the iOS Settings app, select Notifications, and locate the New Relic app from the app list. Ensure that the Allow Notifications switch is on. Ensure that the alert style is set to Banners or Alerts. Optional: To enable audio alerts, set Sounds to on. Delete the Android or iOS device from your New Relic account. To delete the mobile device from your New Relic account, use the public graphql api api.newrelic.com/graphiql in a web browser: Query current devices by selecting actor -> mobilePushNotification -> devices and selecting appVersion, deviceId, and deviceName. Run this query to get the list of devices. Mutate to remove a device by selecting mutation -> mobilePushNotificationRemoveDevice, and passing in the deviceId from the list above. Or you can remove the device from the in-app Settings option from the menu -> Settings Look under Push notification devices, and remove from there. On iOS, slide from right to left to Delete a device, on Android, tap Delete Continue with the steps to reinstall the New Relic app from your device. Uninstall the New Relic mobile app. Follow the procedure to uninstall the New Relic app from your device, then reinstall it. Device To uninstall the New Relic app: Android From your Android device's Settings, select Apps, then select the New Relic app. Select Uninstall. Continue with the steps to reinstall the New Relic app. iOS From your iOS home screen, tap and hold the New Relic icon until it shakes. To delete the app, select the X icon. Continue with the steps to reinstall the New Relic app. Reinstall the New Relic mobile app. To reinstall the New Relic mobile app: From your Android device, select Google Play Store. OR From your iOS device's home screen, select App Store. Search for New Relic. Download the app. When the download finishes, sign in to your New Relic mobile app with your New Relic account.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 212.08412,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Alerting</em> with <em>New</em> <em>Relic</em> <em>mobile</em> <em>apps</em>",
        "sections": "<em>Alerting</em> with <em>New</em> <em>Relic</em> <em>mobile</em> <em>apps</em>",
        "tags": "<em>New</em> <em>Relic</em> <em>mobile</em> <em>apps</em>",
        "body": " channel to the <em>alert</em> policy. View <em>alert</em> incident details The notification automatically appears on your device&#x27;s lock screen. To start the <em>New</em> <em>Relic</em> <em>app</em>: Android devices: Tap the notification from the notification drawer. OR iOS devices: Swipe the screen. From the <em>New</em> <em>Relic</em> <em>app</em>&#x27;s <em>Alerts</em> menu, select"
      },
      "id": "603e9efd64441f19a14e88ab"
    },
    {
      "sections": [
        "Troubleshoot SSO accounts using mobile devices",
        "No user name or password",
        "Errors after signing in",
        "Reauthentication problems"
      ],
      "title": "Troubleshoot SSO accounts using mobile devices",
      "type": "docs",
      "tags": [
        "Mobile apps",
        "New Relic mobile apps",
        "Authentication and alerts"
      ],
      "external_id": "9ebb373182ce5fea83ba5a6baa03b2c7bccf0174",
      "image": "",
      "url": "https://docs.newrelic.com/docs/mobile-apps/new-relic-mobile-apps/authentication-alerts/troubleshoot-sso-accounts-using-mobile-devices/",
      "published_at": "2021-10-18T15:36:50Z",
      "updated_at": "2021-05-16T06:23:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Typically when you sign in to the New Relic mobile app, your session redirects automatically to your web browser. From there you can sign in to your New Relic account. Here are troubleshooting tips if you have problems using the New Relic mobile app with your SAML-SSO enabled account. No user name or password You may not have a user name or password for New Relic because some SAML providers will overwrite your password, or because your administrator has not sent you this information. In these situations: From the mobile app's Log in, select the I don't have a password link. Use your mobile device to open your email account. From your email account, retrieve the New Relic authentication email within 20 minutes. Select the Authenticate button or the link below it in the email. Errors after signing in If you see any errors after successfully signing in to your SSO provider with your mobile device, verify that you are able to sign in to one.newrelic.com with a desktop web browser. If no, contact your administrator. If yes, get support at support.newrelic.com. Reauthentication problems If you are using reauthentication on a SAML-SSO account, you must log in to your default account. (All other accounts will be grayed out.) If you attempt to switch to a grayed-out account, an error message will appear, explaining this is currently not supported.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 205.79622,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Troubleshoot SSO accounts using <em>mobile</em> devices",
        "sections": "Troubleshoot SSO accounts using <em>mobile</em> devices",
        "tags": "<em>New</em> <em>Relic</em> <em>mobile</em> <em>apps</em>",
        "body": "Typically when you sign in to the <em>New</em> <em>Relic</em> <em>mobile</em> <em>app</em>, your session redirects automatically to your web browser. From there you can sign in to your <em>New</em> <em>Relic</em> account. Here are troubleshooting tips if you have problems using the <em>New</em> <em>Relic</em> <em>mobile</em> <em>app</em> with your SAML-SSO enabled account. No user name"
      },
      "id": "604415e0196a67fc3f960f42"
    },
    {
      "sections": [
        "Mobile app authentication for New Relic partners",
        "Important",
        "Confirm your email address",
        "Troubleshoot email problems"
      ],
      "title": "Mobile app authentication for New Relic partners",
      "type": "docs",
      "tags": [
        "Mobile apps",
        "New Relic mobile apps",
        "Authentication and alerts"
      ],
      "external_id": "de6bdd35891dbbfea0ae914251a9d5c4487594a9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/mobile-monitoring/new-relic-mobile-apps/mobile-app-features/authentication-partner-saml-sso-accounts/",
      "published_at": "2021-10-18T06:25:52Z",
      "updated_at": "2021-03-13T03:57:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This resource is for New Relic partners. For authentication of users in regular New Relic accounts, see Authentication. Partner account users typically use SAML-SSO to sign in through your New Relic partner site. You may not have separate passwords or authentication information for your New Relic account. If you use an email address associated with a New Relic partner account when you first sign in to the New Relic mobile app, New Relic will send you a confirmation email for authentication. Android app users will also see a notification message. Important The authentication email expires 20 minutes after it is sent. Confirm your email address To authenticate using a SAML-SSO account provided through a New Relic partner: From the New Relic mobile app, type your email address associated with the partner account. Select I don't have a password. Retrieve the authentication email from your mobile device within 20 minutes. Select the Authenticate button (Android users) or email link (Android or iOS users) in the email to log in to New Relic. You will be redirected to the New Relic mobile app and logged in to your partner account. Troubleshoot email problems Here are some troubleshooting tips: If you cannot find the authentication message from New Relic in your mobile device's email in-box, check your Spam folder. If you miss the 20-minute deadline, sign in to the New Relic mobile app again, then select the link to resend the authentication email.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 202.20352,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Mobile</em> <em>app</em> <em>authentication</em> for <em>New</em> <em>Relic</em> partners",
        "sections": "<em>Mobile</em> <em>app</em> <em>authentication</em> for <em>New</em> <em>Relic</em> partners",
        "tags": "<em>New</em> <em>Relic</em> <em>mobile</em> <em>apps</em>",
        "body": " <em>New</em> <em>Relic</em> account. If you use an email address associated with a <em>New</em> <em>Relic</em> partner account when you first sign in to the <em>New</em> <em>Relic</em> <em>mobile</em> <em>app</em>, <em>New</em> <em>Relic</em> will send you a confirmation email for <em>authentication</em>. Android <em>app</em> users will also see a notification message. Important The <em>authentication</em> email"
      },
      "id": "604418de28ccbc28932c6071"
    }
  ],
  "/docs/mobile-apps/new-relic-mobile-apps/ios-app/install-new-relic-ios-mobile-app": [
    {
      "sections": [
        "Introduction to iOS mobile app",
        "Features",
        "Time range",
        "Synthetic monitoring",
        "Alerts",
        "Mobile monitoring",
        "Data privacy"
      ],
      "title": "Introduction to iOS mobile app",
      "type": "docs",
      "tags": [
        "Mobile apps",
        "New Relic mobile apps",
        "iOS app"
      ],
      "external_id": "371077582a50dfd2a1e7c57cfbbf9eeaf8013e1c",
      "image": "https://docs.newrelic.com/static/630c7a9a486540073ab96a2c9926e303/442cb/device-ios-synthetics-view-monitor.png",
      "url": "https://docs.newrelic.com/docs/mobile-apps/new-relic-mobile-apps/ios-app/introduction-ios-mobile-app/",
      "published_at": "2021-10-18T14:49:05Z",
      "updated_at": "2021-09-14T07:29:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The user interface for New Relic's iPhone and iPad app provides functionality similar to New Relic's standard user interface, with customized details for mobile users. The New Relic iOS apps show near real-time information about your apps, hosts, and more. Features New Relic's iOS app includes these New Relic products and features: New Relic's iOS app for iPhone and iPad includes these New Relic products and features: APM (iPhone and iPad). Includes real-time and historical data. Select the icon to see transaction details. Select Overview Charts to view summary charts of your top five transactions. Browser monitoring (iPhone and iPad). Provide overview dashboard, including average page load time, browser Apdex, average throughput, and more. Infrastructure monitoring (iPhone only). Alerts (iPhone and iPad). Get alert and deployment notifications. Synthetic monitoring (iPhone only). Mobile monitoring (iPhone and iPad). Includes crash reports, network errors, API calls, and active user count. New Relic's iOS app does not have all the features of the New Relic web application. For more detailed analysis, sign in to your New Relic account with a web browser. Time range When viewing an application or host, you can change the visible time frame by using the clock icon in the top right of the page. This feature is similar to the standard New Relic time picker. Features include: Scrub the New Relic charts to move back and forth across the timeline. Select the time picker to choose a time range that ends now (from 30 minutes to 90 days ago). For iPads: to specify an end time other than now, slide the toggle from Ending Now to Custom Date. Synthetic monitoring You can use the iOS app to view your synthetic monitoring data, including charts of your monitor's availability, load times, and load sizes. Select the caret icon to view more detailed charts. You can mute or disable your monitor, and view details of any recent errors. For scripted monitors, you can view and search the script log. Alerts When you connect the iOS app to your New Relic account, your device is automatically associated with your user channel. Then, you can add your user channel to your target policy to receive alerts. For iOS alerts, notifications appear on your lock screen and can be viewed by swiping the alert. You can select any alert to view error details or acknowledge the alert. New Relic also sends a push notification when a colleague acknowledges an open event. Then, New Relic sends a final, closing notification when all Critical events end. Mobile monitoring If you have a mobile application and have installed mobile monitoring, you can monitor its performance directly from your iPhone or iPad. Mobile monitoring includes network errors, API calls, and number of active users. You can also view detailed individual crash reports for a deeper understanding of a particular crash incident. Data privacy New Relic's mobile apps only record information needed to help authenticate and troubleshoot: User's email address associated with your New Relic account, including first and last name (for authentication purposes only) IP address Device ID For more information, see our Mobile data privacy and security documentation.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 277.44952,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Introduction</em> to <em>iOS</em> <em>mobile</em> <em>app</em>",
        "sections": "<em>Introduction</em> to <em>iOS</em> <em>mobile</em> <em>app</em>",
        "tags": "<em>New</em> <em>Relic</em> <em>mobile</em> <em>apps</em>",
        "body": "The user interface for <em>New</em> <em>Relic</em>&#x27;s <em>i</em>Phone and <em>i</em>Pad <em>app</em> provides functionality similar to <em>New</em> <em>Relic</em>&#x27;s standard user interface, with customized details for <em>mobile</em> users. The <em>New</em> <em>Relic</em> <em>iOS</em> <em>apps</em> show near real-time information about your <em>apps</em>, hosts, and more. Features <em>New</em> <em>Relic</em>&#x27;s <em>iOS</em> <em>app</em> includes"
      },
      "id": "6044161628ccbc96b62c6092"
    },
    {
      "sections": [
        "Alerting with New Relic mobile apps",
        "Requirements",
        "Turn notifications on or off",
        "View alert incident details",
        "Troubleshoot alert settings",
        "Check notification settings for your mobile device.",
        "Delete the Android or iOS device from your New Relic account.",
        "Uninstall the New Relic mobile app.",
        "Reinstall the New Relic mobile app."
      ],
      "title": "Alerting with New Relic mobile apps",
      "type": "docs",
      "tags": [
        "Mobile apps",
        "New Relic mobile apps",
        "Authentication and alerts"
      ],
      "external_id": "d55850dc642cc8ade20310e1d4654db61af1e809",
      "image": "https://docs.newrelic.com/static/f942198cbd9a41b7355ef7f01fa6cc66/e5166/alerts-incident-detail-nexus.jpg",
      "url": "https://docs.newrelic.com/docs/mobile-apps/new-relic-mobile-apps/authentication-alerts/alerting-new-relic-mobile-apps/",
      "published_at": "2021-10-18T14:48:16Z",
      "updated_at": "2021-07-09T12:24:33Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Account administrators can set up configuration to receive push notifications on Android and iOS devices from New Relic Alerts. You can receive alerts from any policy by attaching a user channel to the policy. Requirements This feature is available only to users on the original user model, not to users on the New Relic One user model. As a workaround, you can use the email notification channel. Turn notifications on or off When you log in to your New Relic account from an Android or iOS app, your device is automatically associated with your user channel. Be sure to add the associated user channel to the alert policy. View alert incident details The notification automatically appears on your device's lock screen. To start the New Relic app: Android devices: Tap the notification from the notification drawer. OR iOS devices: Swipe the screen. From the New Relic app's Alerts menu, select any alert to view error details for the associated application. Optional: Select Acknowledge. Optional: To view additional details, select Overview, Violations, or Event log. The main menu's Alerts list shows alerts in the following order, sorted by time: Active incidents Resolved incidents from today Resolved incidents and events from the past week, organized by day Troubleshoot alert settings If alerts are not working on your mobile device: Verify that you meet the requirements. Verify that alerts are enabled. Check your mobile device's notification settings, to ensure New Relic is permitted to send alerts. If the notification settings for your mobile device are correct, but you still do not receive notifications, delete the device from your account, then uninstall and reinstall the New Relic application. Check notification settings for your mobile device. Follow the procedure for your mobile device. Device To check notification settings: Android From your Android device's Settings, select Sound and notification. Check the settings for sound volume. Optional: Enable Also vibrate for calls. Check the settings for Interruptions. Check the settings for Notification. Check the settings for App notifications: Select the New Relic app, then check the settings for Block and Priority. iOS Ensure Do Not Disturb is off: From the iOS Settings app, select Do Not Disturb, and check that the Manual switch is off. Ensure the New Relic app is allowed to send notifications: From the iOS Settings app, select Notifications, and locate the New Relic app from the app list. Ensure that the Allow Notifications switch is on. Ensure that the alert style is set to Banners or Alerts. Optional: To enable audio alerts, set Sounds to on. Delete the Android or iOS device from your New Relic account. To delete the mobile device from your New Relic account, use the public graphql api api.newrelic.com/graphiql in a web browser: Query current devices by selecting actor -> mobilePushNotification -> devices and selecting appVersion, deviceId, and deviceName. Run this query to get the list of devices. Mutate to remove a device by selecting mutation -> mobilePushNotificationRemoveDevice, and passing in the deviceId from the list above. Or you can remove the device from the in-app Settings option from the menu -> Settings Look under Push notification devices, and remove from there. On iOS, slide from right to left to Delete a device, on Android, tap Delete Continue with the steps to reinstall the New Relic app from your device. Uninstall the New Relic mobile app. Follow the procedure to uninstall the New Relic app from your device, then reinstall it. Device To uninstall the New Relic app: Android From your Android device's Settings, select Apps, then select the New Relic app. Select Uninstall. Continue with the steps to reinstall the New Relic app. iOS From your iOS home screen, tap and hold the New Relic icon until it shakes. To delete the app, select the X icon. Continue with the steps to reinstall the New Relic app. Reinstall the New Relic mobile app. To reinstall the New Relic mobile app: From your Android device, select Google Play Store. OR From your iOS device's home screen, select App Store. Search for New Relic. Download the app. When the download finishes, sign in to your New Relic mobile app with your New Relic account.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 220.48737,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Alerting with <em>New</em> <em>Relic</em> <em>mobile</em> <em>apps</em>",
        "sections": "Alerting with <em>New</em> <em>Relic</em> <em>mobile</em> <em>apps</em>",
        "tags": "<em>New</em> <em>Relic</em> <em>mobile</em> <em>apps</em>",
        "body": ": Android From your Android device&#x27;s Settings, select <em>Apps</em>, then select the <em>New</em> <em>Relic</em> <em>app</em>. Select Uninstall. Continue with the steps to reinstall the <em>New</em> <em>Relic</em> <em>app</em>. <em>iOS</em> From your <em>iOS</em> home screen, tap and hold the <em>New</em> <em>Relic</em> icon until it shakes. To delete the <em>app</em>, select the X icon. Continue"
      },
      "id": "603e9efd64441f19a14e88ab"
    },
    {
      "sections": [
        "Introduction to New Relic Android app",
        "Requirements",
        "Install New Relic's mobile app",
        "View New Relic data",
        "New Relic product details",
        "Synthetics data",
        "Alerts",
        "Mobile app monitoring",
        "Details on setting time range",
        "Data privacy"
      ],
      "title": "Introduction to New Relic Android app",
      "type": "docs",
      "tags": [
        "Mobile apps",
        "New Relic mobile apps",
        "Android app"
      ],
      "external_id": "ff8415c00363a49eaa062f4b0b13c795b4717ea5",
      "image": "https://docs.newrelic.com/static/ea914fce17844b32fdabefd60efc457e/e5166/navigation_nexus.jpg",
      "url": "https://docs.newrelic.com/docs/mobile-apps/new-relic-mobile-apps/android-app/introduction-new-relic-android-app/",
      "published_at": "2021-10-18T15:35:57Z",
      "updated_at": "2021-09-14T07:28:16Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The user interface for New Relic's Android app provides functionality similar to New Relic's standard user interface, with customized details for mobile users. Requirements Requirements include: Android 4.0 (Ice Cream Sandwich) or higher Screen size of 7 inches or less Install New Relic's mobile app You can install the New Relic Android app from the Google Play Store or learn more from the New Relic website. Follow standard procedures to install any Android app, then sign in with your New Relic user name (account email) and password if applicable. Depending on your New Relic account, additional installation or user authentication steps may be required. View New Relic data To view details of your apps monitored by New Relic, select a product from the app's main menu. See below for details on how to use specific features of the app: New Relic product details The New Relic Android app includes data about these features: APM metrics, both real-time and historical data, including health maps. Select the transaction icon to see detailed transaction metrics, or an Overview chart to view summary charts of your top five transactions. Select the icon to filter by labels and categories. Browser monitoring metrics, including average page load time, Apdex, average throughput, and more. Infrastructure monitoring. Mobile monitoring, including crash reports, network errors, API calls, and active user count. Event notifications, including mobile alerts wherever you are, plus deployment notifications and notes. New Relic's Android app does not have the full feature set of the New Relic web interface. For more detailed analysis, sign in to your New Relic account with a web browser. Synthetics data You can use the Android app to view your synthetic monitoring data, including charts of your monitor's availability, load times, and load sizes. To view more detailed charts, select the caret icon. You can mute or disable your monitor, and view details of any recent errors. For scripted monitors, you can view and search the script log. Alerts When you log in to your New Relic account from the Android app, your device is automatically associated with your user channel. Then, you can add your user channel to your target policy to receive alerts. For Android alerts, notifications appear on your lock screen. To view them, tap the alert event. You can select any alert to view error details or acknowledge the alert. New Relic also sends a push notification when a colleague acknowledges an open event. Then, New Relic sends a final, closing notification when all Critical events end. Mobile app monitoring If you have installed mobile monitoring, you can monitor its performance directly from your Android device. Mobile monitoring includes network errors, API calls, and number of active users. You can also view detailed individual crash reports for a deeper understanding of a particular crash incident. Details on setting time range When viewing an application or host, you can change the visible time frame with the time picker. To move back and forth across the timeline, scrub the New Relic charts. To change the duration of the visible time slice, select the clock icon. To specify an end time other than now, slide the toggle from Ending Now to Custom Date. To save your changes and refresh the chart data, select the clock icon again. Data privacy New Relic's mobile apps only record information needed to help authenticate and troubleshoot: User's email address associated with your New Relic account, including first and last name (for authentication purposes only) IP address Device ID For more information, see our Mobile data privacy and security documentation.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.42714,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Introduction</em> to <em>New</em> <em>Relic</em> Android <em>app</em>",
        "sections": "<em>Install</em> <em>New</em> <em>Relic&#x27;s</em> <em>mobile</em> <em>app</em>",
        "tags": "<em>New</em> <em>Relic</em> <em>mobile</em> <em>apps</em>",
        "body": "The user interface for <em>New</em> <em>Relic</em>&#x27;s Android <em>app</em> provides functionality similar to <em>New</em> <em>Relic</em>&#x27;s standard user interface, with customized details for <em>mobile</em> users. Requirements Requirements include: Android 4.0 (Ice Cream Sandwich) or higher Screen size of 7 inches or less Install <em>New</em> <em>Relic</em>&#x27;s <em>mobile</em>"
      },
      "id": "604415e0196a67ff23960f46"
    }
  ],
  "/docs/mobile-apps/new-relic-mobile-apps/ios-app/introduction-ios-mobile-app": [
    {
      "sections": [
        "Install the New Relic iOS mobile app",
        "Compatibility and requirements",
        "Installation"
      ],
      "title": "Install the New Relic iOS mobile app",
      "type": "docs",
      "tags": [
        "Mobile apps",
        "New Relic mobile apps",
        "iOS app"
      ],
      "external_id": "a33650792e7ba24040db9a65d8d7fbb25c341d18",
      "image": "",
      "url": "https://docs.newrelic.com/docs/mobile-apps/new-relic-mobile-apps/ios-app/install-new-relic-ios-mobile-app/",
      "published_at": "2021-10-18T14:49:05Z",
      "updated_at": "2021-03-13T04:04:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This section provides information about compatibility and requirements, basic instructions on how to install and configure the New Relic iPhone and iPad apps, and links to more detailed information. Compatibility and requirements The New Relic iOS app allows you to view your New Relic applications, Infrastructure data, plugins you have installed from Plugin Central, key transactions, Synthetics monitors, and alerts from an Apple iPhone or iPad. Product requirements include: iOS 7 or higher iPhone users: iPhone 4S or higher iPad users: iPad 2 or higher You can also use an iPod touch, although resolution may be different. Installation You can install the New Relic app from the App Store or learn more from the New Relic website. Follow standard procedures to install any iOS app, and then sign in with your New Relic user name (account email) and password if applicable. Depending on your New Relic account, additional installation or authentication steps may be required. For more information, see User settings and authentication.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 230.61482,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the <em>New</em> <em>Relic</em> <em>iOS</em> <em>mobile</em> <em>app</em>",
        "sections": "<em>Install</em> the <em>New</em> <em>Relic</em> <em>iOS</em> <em>mobile</em> <em>app</em>",
        "tags": "<em>New</em> <em>Relic</em> <em>mobile</em> <em>apps</em>",
        "body": "This section provides information about compatibility and requirements, basic instructions on how to install and configure the <em>New</em> <em>Relic</em> <em>i</em>Phone and <em>i</em>Pad <em>apps</em>, and links to more detailed information. Compatibility and requirements The <em>New</em> <em>Relic</em> <em>iOS</em> <em>app</em> allows you to view your <em>New</em> <em>Relic</em> applications"
      },
      "id": "60441616196a67b070960f2b"
    },
    {
      "sections": [
        "Alerting with New Relic mobile apps",
        "Requirements",
        "Turn notifications on or off",
        "View alert incident details",
        "Troubleshoot alert settings",
        "Check notification settings for your mobile device.",
        "Delete the Android or iOS device from your New Relic account.",
        "Uninstall the New Relic mobile app.",
        "Reinstall the New Relic mobile app."
      ],
      "title": "Alerting with New Relic mobile apps",
      "type": "docs",
      "tags": [
        "Mobile apps",
        "New Relic mobile apps",
        "Authentication and alerts"
      ],
      "external_id": "d55850dc642cc8ade20310e1d4654db61af1e809",
      "image": "https://docs.newrelic.com/static/f942198cbd9a41b7355ef7f01fa6cc66/e5166/alerts-incident-detail-nexus.jpg",
      "url": "https://docs.newrelic.com/docs/mobile-apps/new-relic-mobile-apps/authentication-alerts/alerting-new-relic-mobile-apps/",
      "published_at": "2021-10-18T14:48:16Z",
      "updated_at": "2021-07-09T12:24:33Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Account administrators can set up configuration to receive push notifications on Android and iOS devices from New Relic Alerts. You can receive alerts from any policy by attaching a user channel to the policy. Requirements This feature is available only to users on the original user model, not to users on the New Relic One user model. As a workaround, you can use the email notification channel. Turn notifications on or off When you log in to your New Relic account from an Android or iOS app, your device is automatically associated with your user channel. Be sure to add the associated user channel to the alert policy. View alert incident details The notification automatically appears on your device's lock screen. To start the New Relic app: Android devices: Tap the notification from the notification drawer. OR iOS devices: Swipe the screen. From the New Relic app's Alerts menu, select any alert to view error details for the associated application. Optional: Select Acknowledge. Optional: To view additional details, select Overview, Violations, or Event log. The main menu's Alerts list shows alerts in the following order, sorted by time: Active incidents Resolved incidents from today Resolved incidents and events from the past week, organized by day Troubleshoot alert settings If alerts are not working on your mobile device: Verify that you meet the requirements. Verify that alerts are enabled. Check your mobile device's notification settings, to ensure New Relic is permitted to send alerts. If the notification settings for your mobile device are correct, but you still do not receive notifications, delete the device from your account, then uninstall and reinstall the New Relic application. Check notification settings for your mobile device. Follow the procedure for your mobile device. Device To check notification settings: Android From your Android device's Settings, select Sound and notification. Check the settings for sound volume. Optional: Enable Also vibrate for calls. Check the settings for Interruptions. Check the settings for Notification. Check the settings for App notifications: Select the New Relic app, then check the settings for Block and Priority. iOS Ensure Do Not Disturb is off: From the iOS Settings app, select Do Not Disturb, and check that the Manual switch is off. Ensure the New Relic app is allowed to send notifications: From the iOS Settings app, select Notifications, and locate the New Relic app from the app list. Ensure that the Allow Notifications switch is on. Ensure that the alert style is set to Banners or Alerts. Optional: To enable audio alerts, set Sounds to on. Delete the Android or iOS device from your New Relic account. To delete the mobile device from your New Relic account, use the public graphql api api.newrelic.com/graphiql in a web browser: Query current devices by selecting actor -> mobilePushNotification -> devices and selecting appVersion, deviceId, and deviceName. Run this query to get the list of devices. Mutate to remove a device by selecting mutation -> mobilePushNotificationRemoveDevice, and passing in the deviceId from the list above. Or you can remove the device from the in-app Settings option from the menu -> Settings Look under Push notification devices, and remove from there. On iOS, slide from right to left to Delete a device, on Android, tap Delete Continue with the steps to reinstall the New Relic app from your device. Uninstall the New Relic mobile app. Follow the procedure to uninstall the New Relic app from your device, then reinstall it. Device To uninstall the New Relic app: Android From your Android device's Settings, select Apps, then select the New Relic app. Select Uninstall. Continue with the steps to reinstall the New Relic app. iOS From your iOS home screen, tap and hold the New Relic icon until it shakes. To delete the app, select the X icon. Continue with the steps to reinstall the New Relic app. Reinstall the New Relic mobile app. To reinstall the New Relic mobile app: From your Android device, select Google Play Store. OR From your iOS device's home screen, select App Store. Search for New Relic. Download the app. When the download finishes, sign in to your New Relic mobile app with your New Relic account.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 220.48737,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Alerting with <em>New</em> <em>Relic</em> <em>mobile</em> <em>apps</em>",
        "sections": "Alerting with <em>New</em> <em>Relic</em> <em>mobile</em> <em>apps</em>",
        "tags": "<em>New</em> <em>Relic</em> <em>mobile</em> <em>apps</em>",
        "body": ": Android From your Android device&#x27;s Settings, select <em>Apps</em>, then select the <em>New</em> <em>Relic</em> <em>app</em>. Select Uninstall. Continue with the steps to reinstall the <em>New</em> <em>Relic</em> <em>app</em>. <em>iOS</em> From your <em>iOS</em> home screen, tap and hold the <em>New</em> <em>Relic</em> icon until it shakes. To delete the <em>app</em>, select the X icon. Continue"
      },
      "id": "603e9efd64441f19a14e88ab"
    },
    {
      "sections": [
        "Introduction to New Relic Android app",
        "Requirements",
        "Install New Relic's mobile app",
        "View New Relic data",
        "New Relic product details",
        "Synthetics data",
        "Alerts",
        "Mobile app monitoring",
        "Details on setting time range",
        "Data privacy"
      ],
      "title": "Introduction to New Relic Android app",
      "type": "docs",
      "tags": [
        "Mobile apps",
        "New Relic mobile apps",
        "Android app"
      ],
      "external_id": "ff8415c00363a49eaa062f4b0b13c795b4717ea5",
      "image": "https://docs.newrelic.com/static/ea914fce17844b32fdabefd60efc457e/e5166/navigation_nexus.jpg",
      "url": "https://docs.newrelic.com/docs/mobile-apps/new-relic-mobile-apps/android-app/introduction-new-relic-android-app/",
      "published_at": "2021-10-18T15:35:57Z",
      "updated_at": "2021-09-14T07:28:16Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The user interface for New Relic's Android app provides functionality similar to New Relic's standard user interface, with customized details for mobile users. Requirements Requirements include: Android 4.0 (Ice Cream Sandwich) or higher Screen size of 7 inches or less Install New Relic's mobile app You can install the New Relic Android app from the Google Play Store or learn more from the New Relic website. Follow standard procedures to install any Android app, then sign in with your New Relic user name (account email) and password if applicable. Depending on your New Relic account, additional installation or user authentication steps may be required. View New Relic data To view details of your apps monitored by New Relic, select a product from the app's main menu. See below for details on how to use specific features of the app: New Relic product details The New Relic Android app includes data about these features: APM metrics, both real-time and historical data, including health maps. Select the transaction icon to see detailed transaction metrics, or an Overview chart to view summary charts of your top five transactions. Select the icon to filter by labels and categories. Browser monitoring metrics, including average page load time, Apdex, average throughput, and more. Infrastructure monitoring. Mobile monitoring, including crash reports, network errors, API calls, and active user count. Event notifications, including mobile alerts wherever you are, plus deployment notifications and notes. New Relic's Android app does not have the full feature set of the New Relic web interface. For more detailed analysis, sign in to your New Relic account with a web browser. Synthetics data You can use the Android app to view your synthetic monitoring data, including charts of your monitor's availability, load times, and load sizes. To view more detailed charts, select the caret icon. You can mute or disable your monitor, and view details of any recent errors. For scripted monitors, you can view and search the script log. Alerts When you log in to your New Relic account from the Android app, your device is automatically associated with your user channel. Then, you can add your user channel to your target policy to receive alerts. For Android alerts, notifications appear on your lock screen. To view them, tap the alert event. You can select any alert to view error details or acknowledge the alert. New Relic also sends a push notification when a colleague acknowledges an open event. Then, New Relic sends a final, closing notification when all Critical events end. Mobile app monitoring If you have installed mobile monitoring, you can monitor its performance directly from your Android device. Mobile monitoring includes network errors, API calls, and number of active users. You can also view detailed individual crash reports for a deeper understanding of a particular crash incident. Details on setting time range When viewing an application or host, you can change the visible time frame with the time picker. To move back and forth across the timeline, scrub the New Relic charts. To change the duration of the visible time slice, select the clock icon. To specify an end time other than now, slide the toggle from Ending Now to Custom Date. To save your changes and refresh the chart data, select the clock icon again. Data privacy New Relic's mobile apps only record information needed to help authenticate and troubleshoot: User's email address associated with your New Relic account, including first and last name (for authentication purposes only) IP address Device ID For more information, see our Mobile data privacy and security documentation.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.42714,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Introduction</em> to <em>New</em> <em>Relic</em> Android <em>app</em>",
        "sections": "<em>Install</em> <em>New</em> <em>Relic&#x27;s</em> <em>mobile</em> <em>app</em>",
        "tags": "<em>New</em> <em>Relic</em> <em>mobile</em> <em>apps</em>",
        "body": "The user interface for <em>New</em> <em>Relic</em>&#x27;s Android <em>app</em> provides functionality similar to <em>New</em> <em>Relic</em>&#x27;s standard user interface, with customized details for <em>mobile</em> users. Requirements Requirements include: Android 4.0 (Ice Cream Sandwich) or higher Screen size of 7 inches or less Install <em>New</em> <em>Relic</em>&#x27;s <em>mobile</em>"
      },
      "id": "604415e0196a67ff23960f46"
    }
  ],
  "/docs/mobile-apps/new-relic-mobile-apps/tvos-app/introduction-apple-tv-app": [
    {
      "sections": [
        "Introduction to iOS mobile app",
        "Features",
        "Time range",
        "Synthetic monitoring",
        "Alerts",
        "Mobile monitoring",
        "Data privacy"
      ],
      "title": "Introduction to iOS mobile app",
      "type": "docs",
      "tags": [
        "Mobile apps",
        "New Relic mobile apps",
        "iOS app"
      ],
      "external_id": "371077582a50dfd2a1e7c57cfbbf9eeaf8013e1c",
      "image": "https://docs.newrelic.com/static/630c7a9a486540073ab96a2c9926e303/442cb/device-ios-synthetics-view-monitor.png",
      "url": "https://docs.newrelic.com/docs/mobile-apps/new-relic-mobile-apps/ios-app/introduction-ios-mobile-app/",
      "published_at": "2021-10-18T14:49:05Z",
      "updated_at": "2021-09-14T07:29:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The user interface for New Relic's iPhone and iPad app provides functionality similar to New Relic's standard user interface, with customized details for mobile users. The New Relic iOS apps show near real-time information about your apps, hosts, and more. Features New Relic's iOS app includes these New Relic products and features: New Relic's iOS app for iPhone and iPad includes these New Relic products and features: APM (iPhone and iPad). Includes real-time and historical data. Select the icon to see transaction details. Select Overview Charts to view summary charts of your top five transactions. Browser monitoring (iPhone and iPad). Provide overview dashboard, including average page load time, browser Apdex, average throughput, and more. Infrastructure monitoring (iPhone only). Alerts (iPhone and iPad). Get alert and deployment notifications. Synthetic monitoring (iPhone only). Mobile monitoring (iPhone and iPad). Includes crash reports, network errors, API calls, and active user count. New Relic's iOS app does not have all the features of the New Relic web application. For more detailed analysis, sign in to your New Relic account with a web browser. Time range When viewing an application or host, you can change the visible time frame by using the clock icon in the top right of the page. This feature is similar to the standard New Relic time picker. Features include: Scrub the New Relic charts to move back and forth across the timeline. Select the time picker to choose a time range that ends now (from 30 minutes to 90 days ago). For iPads: to specify an end time other than now, slide the toggle from Ending Now to Custom Date. Synthetic monitoring You can use the iOS app to view your synthetic monitoring data, including charts of your monitor's availability, load times, and load sizes. Select the caret icon to view more detailed charts. You can mute or disable your monitor, and view details of any recent errors. For scripted monitors, you can view and search the script log. Alerts When you connect the iOS app to your New Relic account, your device is automatically associated with your user channel. Then, you can add your user channel to your target policy to receive alerts. For iOS alerts, notifications appear on your lock screen and can be viewed by swiping the alert. You can select any alert to view error details or acknowledge the alert. New Relic also sends a push notification when a colleague acknowledges an open event. Then, New Relic sends a final, closing notification when all Critical events end. Mobile monitoring If you have a mobile application and have installed mobile monitoring, you can monitor its performance directly from your iPhone or iPad. Mobile monitoring includes network errors, API calls, and number of active users. You can also view detailed individual crash reports for a deeper understanding of a particular crash incident. Data privacy New Relic's mobile apps only record information needed to help authenticate and troubleshoot: User's email address associated with your New Relic account, including first and last name (for authentication purposes only) IP address Device ID For more information, see our Mobile data privacy and security documentation.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.95752,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to iOS <em>mobile</em> <em>app</em>",
        "sections": "Introduction to iOS <em>mobile</em> <em>app</em>",
        "tags": "<em>New</em> <em>Relic</em> <em>mobile</em> <em>apps</em>",
        "body": "The user interface for <em>New</em> <em>Relic</em>&#x27;s iPhone and iPad <em>app</em> provides functionality similar to <em>New</em> <em>Relic</em>&#x27;s standard user interface, with customized details for <em>mobile</em> users. The <em>New</em> <em>Relic</em> iOS <em>apps</em> show near real-time information about your <em>apps</em>, hosts, and more. Features <em>New</em> <em>Relic</em>&#x27;s iOS <em>app</em> includes"
      },
      "id": "6044161628ccbc96b62c6092"
    },
    {
      "sections": [
        "Introduction to New Relic Android app",
        "Requirements",
        "Install New Relic's mobile app",
        "View New Relic data",
        "New Relic product details",
        "Synthetics data",
        "Alerts",
        "Mobile app monitoring",
        "Details on setting time range",
        "Data privacy"
      ],
      "title": "Introduction to New Relic Android app",
      "type": "docs",
      "tags": [
        "Mobile apps",
        "New Relic mobile apps",
        "Android app"
      ],
      "external_id": "ff8415c00363a49eaa062f4b0b13c795b4717ea5",
      "image": "https://docs.newrelic.com/static/ea914fce17844b32fdabefd60efc457e/e5166/navigation_nexus.jpg",
      "url": "https://docs.newrelic.com/docs/mobile-apps/new-relic-mobile-apps/android-app/introduction-new-relic-android-app/",
      "published_at": "2021-10-18T15:35:57Z",
      "updated_at": "2021-09-14T07:28:16Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The user interface for New Relic's Android app provides functionality similar to New Relic's standard user interface, with customized details for mobile users. Requirements Requirements include: Android 4.0 (Ice Cream Sandwich) or higher Screen size of 7 inches or less Install New Relic's mobile app You can install the New Relic Android app from the Google Play Store or learn more from the New Relic website. Follow standard procedures to install any Android app, then sign in with your New Relic user name (account email) and password if applicable. Depending on your New Relic account, additional installation or user authentication steps may be required. View New Relic data To view details of your apps monitored by New Relic, select a product from the app's main menu. See below for details on how to use specific features of the app: New Relic product details The New Relic Android app includes data about these features: APM metrics, both real-time and historical data, including health maps. Select the transaction icon to see detailed transaction metrics, or an Overview chart to view summary charts of your top five transactions. Select the icon to filter by labels and categories. Browser monitoring metrics, including average page load time, Apdex, average throughput, and more. Infrastructure monitoring. Mobile monitoring, including crash reports, network errors, API calls, and active user count. Event notifications, including mobile alerts wherever you are, plus deployment notifications and notes. New Relic's Android app does not have the full feature set of the New Relic web interface. For more detailed analysis, sign in to your New Relic account with a web browser. Synthetics data You can use the Android app to view your synthetic monitoring data, including charts of your monitor's availability, load times, and load sizes. To view more detailed charts, select the caret icon. You can mute or disable your monitor, and view details of any recent errors. For scripted monitors, you can view and search the script log. Alerts When you log in to your New Relic account from the Android app, your device is automatically associated with your user channel. Then, you can add your user channel to your target policy to receive alerts. For Android alerts, notifications appear on your lock screen. To view them, tap the alert event. You can select any alert to view error details or acknowledge the alert. New Relic also sends a push notification when a colleague acknowledges an open event. Then, New Relic sends a final, closing notification when all Critical events end. Mobile app monitoring If you have installed mobile monitoring, you can monitor its performance directly from your Android device. Mobile monitoring includes network errors, API calls, and number of active users. You can also view detailed individual crash reports for a deeper understanding of a particular crash incident. Details on setting time range When viewing an application or host, you can change the visible time frame with the time picker. To move back and forth across the timeline, scrub the New Relic charts. To change the duration of the visible time slice, select the clock icon. To specify an end time other than now, slide the toggle from Ending Now to Custom Date. To save your changes and refresh the chart data, select the clock icon again. Data privacy New Relic's mobile apps only record information needed to help authenticate and troubleshoot: User's email address associated with your New Relic account, including first and last name (for authentication purposes only) IP address Device ID For more information, see our Mobile data privacy and security documentation.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.95674,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>New</em> <em>Relic</em> Android <em>app</em>",
        "sections": "Install <em>New</em> <em>Relic&#x27;s</em> <em>mobile</em> <em>app</em>",
        "tags": "<em>New</em> <em>Relic</em> <em>mobile</em> <em>apps</em>",
        "body": "The user interface for <em>New</em> <em>Relic</em>&#x27;s Android <em>app</em> provides functionality similar to <em>New</em> <em>Relic</em>&#x27;s standard user interface, with customized details for <em>mobile</em> users. Requirements Requirements include: Android 4.0 (Ice Cream Sandwich) or higher Screen size of 7 inches or less Install <em>New</em> <em>Relic</em>&#x27;s <em>mobile</em>"
      },
      "id": "604415e0196a67ff23960f46"
    },
    {
      "sections": [
        "Android app UI",
        "Pages",
        "Time range",
        "New Relic Synthetics",
        "Alerts",
        "Mobile apps",
        "For more help"
      ],
      "title": "Android app UI",
      "type": "docs",
      "tags": [
        "Mobile apps",
        "New Relic mobile apps",
        "Android app"
      ],
      "external_id": "8918a5a2454491a91421c55e26501a0e3f64cd3a",
      "image": "https://docs.newrelic.com/static/fc97ade0bbdbdef58b89495a0d91b734/edd00/deployment-markers_nexus.jpg",
      "url": "https://docs.newrelic.com/docs/mobile-apps/new-relic-mobile-apps/android-app/android-app-ui/",
      "published_at": "2021-10-18T15:36:53Z",
      "updated_at": "2021-09-14T07:28:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The UI for the New Relic Android app provides functionality similar to the standard user interface, with customized details for mobile users. Pages To view details of your New Relic apps, hosts, Synthetics monitors, Alerts, plugins, and key transactions, select a product from the main menu. The New Relic Android app includes: APM metrics, both real-time and historical data, including health maps. And, select the transaction icon for detailed transaction metrics, or an Overview Charts to view summary charts of your top five transactions. New Relic Infrastructure utilization. New Relic Plugins, including a list of their components or instances, and their charts and current values from the plugin's Summary. Mobile monitoring, including crash reports, network errors, API calls, and active user count. Select the filter icon to filter by labels and categories. Event notifications, including mobile alerts wherever you are, plus deployment notifications and notes. Note: New Relic's Android app does not have the full feature set of the New Relic web interface. For more detailed analysis, sign in to your New Relic account with a web browser. Time range When viewing an application or host, you can change the visible time frame by using the time picker icon in the top right of the page (the 7D in the screenshot). This feature is similar to the standard New Relic time picker. Features include: Scrub the New Relic charts to move back and forth in the timeline. Select the time picker to choose a time range that ends now (from 30 minutes to 90 days ago). New Relic Synthetics You can use the Android app to view your New Relic Synthetics data, including charts of your monitor's availability, load times, and load sizes. Select the caret icon to view more detailed charts. You can mute or disable your monitor, and view details of any recent errors. For scripted monitors, you can view and search the script log. Alerts When you log in to your New Relic account from the Android app, your device is automatically associated with your user channel. Then, you can add your user channel to your target policy to receive alerts. For Android alerts, notifications appear on your lock screen and can be viewed by tapping the alert. You can select any alert to view error details or acknowledge the alert. New Relic also sends a push notification when a colleague acknowledges an open event. Then, New Relic sends a final, closing notification when all Critical events end. Mobile apps If you have a mobile application and have installed mobile monitoring, you can monitor its performance directly from your Android device. Mobile monitoring includes network errors, API calls, and number of active users. You can also view detailed individual crash reports for a deeper understanding of a particular crash incident. For more help Additional documentation resources include: New Relic Android app (compatibility, requirements, installation) Android authentication (procedures to add or remove users, and for the users to authenticate with their Android device)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.95674,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Android <em>app</em> UI",
        "sections": "<em>Mobile</em> <em>apps</em>",
        "tags": "<em>New</em> <em>Relic</em> <em>mobile</em> <em>apps</em>",
        "body": "The UI for the <em>New</em> <em>Relic</em> Android <em>app</em> provides functionality similar to the standard user interface, with customized details for <em>mobile</em> users. Pages To view details of your <em>New</em> <em>Relic</em> <em>apps</em>, hosts, Synthetics monitors, Alerts, plugins, and key transactions, select a product from the main menu. The <em>New</em>"
      },
      "id": "6044181d28ccbc9a522c60a5"
    }
  ],
  "/docs/mobile-crash-rest-api-v1": [
    {
      "sections": [
        "Working with the New Relic REST API (v1) (deprecated)",
        "Important"
      ],
      "title": "Working with the New Relic REST API (v1) (deprecated)",
      "type": "docs",
      "tags": [
        "APIs",
        "REST API v1 deprecated",
        "New Relic REST API v1"
      ],
      "external_id": "9cb0f38eb95a8757624ddb63298ff9a32e1176e7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/apis/rest-api-v1-deprecated/new-relic-rest-api-v1/working-new-relic-rest-api-v1-deprecated/",
      "published_at": "2021-10-18T12:55:34Z",
      "updated_at": "2021-03-13T02:32:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Currently New Relic supports two versions of the REST API. Version 1 is deprecated and has been replaced with the newer v2. No termination date has been announced. However, no further development or modifications are being made to v1. Important Start new projects by referring to Getting started with API v2 and the New Relic REST API v2 examples. Also, begin migrating your v1 scripts to their v2 equivalent. To use the REST API v1 in any way, your API key is required. Then, from the command line, you can use: curl -gH \"x-api-key:REPLACE_WITH_YOUR_API_KEY\" 'ENDPOINT_URL' Copy OR wget -qO- --header \"x-api-key:REPLACE_WITH_YOUR_API_KEY\" 'ENDPOINT_URL' Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 355.11786,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Working with the New Relic <em>REST</em> <em>API</em> (<em>v1</em>) (deprecated)",
        "sections": "Working with the New Relic <em>REST</em> <em>API</em> (<em>v1</em>) (deprecated)",
        "tags": "<em>REST</em> <em>API</em> <em>v1</em> deprecated",
        "body": "Currently New Relic supports two versions of the <em>REST</em> <em>API</em>. Version <em>1</em> is deprecated and has been replaced with the newer <em>v</em>2. No termination date has been announced. However, no further development or modifications are being made to <em>v1</em>. Important Start new projects by referring to Getting started"
      },
      "id": "6043ff97e7b9d20358579a0d"
    },
    {
      "sections": [
        "Introduction to the Log API",
        "HTTP endpoint",
        "HTTP setup",
        "HTTP headers",
        "HTTP query parameters",
        "JSON body",
        "Simplified JSON body message",
        "Detailed JSON body message",
        "Limits and restricted characters",
        "Caution",
        "Important",
        "Rate limit violations",
        "HTTP requests per minute",
        "JSON bytes per minute",
        "Log payload format",
        "JSON message attributes",
        "Common block attributes",
        "Logs block attributes",
        "JSON message attribute parsing",
        "Log JSON examples",
        "Log POST message example",
        "JSON POST request example",
        "What's next?"
      ],
      "title": "Introduction to the Log API",
      "type": "docs",
      "tags": [
        "Logs",
        "Log management",
        "Log API"
      ],
      "external_id": "198ebbf54f4a13fdf2f5b0f19d8cc8677afd09a2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/logs/log-management/log-api/introduction-log-api/",
      "published_at": "2021-10-19T03:43:55Z",
      "updated_at": "2021-10-19T03:43:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If our log forwarding solutions don't meet your needs, you can use our Log API to send log data directly to New Relic via an HTTP endpoint. Want to try out our Log API? Create a New Relic account for free! No credit card required. HTTP endpoint Use the endpoint that's applicable for your New Relic account: United States (US) endpoint: https://log-api.newrelic.com/log/v1 Copy European Union (EU) endpoint: https://log-api.eu.newrelic.com/log/v1 Copy HTTP setup To send log data to your New Relic account via the Log API: Get your New Relic license key. Review the limits and restricted characters for your JSON payload. Generate the JSON message using the required headers and body fields. Ensure that your Api-Key or License-Key is included in your headers or query parameters. Refer to the log JSON examples. Send your JSON message to the appropriate HTTP endpoint for your New Relic account in a POST request. US: https://log-api.newrelic.com/log/v1 EU: https://log-api.eu.newrelic.com/log/v1 Generate some traffic and wait a few minutes, then check your account for data. If no data appears after you enable our log management capabilities, follow our troubleshooting procedures. HTTP headers When creating your HTTP headers, use these guidelines: Header Supported values Content-Type Required application/json json application/gzip gzip Api-Key Required A New Relic license key. You can also send this via query parameter. You can also use an Insights insert key but the license key is preferred. Gzipped JSON formatting is accepted. If sending compressed JSON, please include the Content-Type: application/json and Content-Encoding: gzip headers. HTTP query parameters The license key can also be passed as a query string parameter. This can be useful when sending logs from cloud-based sources that don't allow custom HTTP request headers. Query parameter Value Api-Key Your license key. Use this key whenever you send a header. You can also use an Insights insert key but the license key is preferred. JSON body You can send your JSON message using either a simplified or detailed set of attributes: Simplified JSON body message When using the simplified format to create your JSON message, send a single JSON object with the following: Field Value type Format Required Notes \"timestamp\" Integer Either milliseconds or seconds since epoch No If the field is not specific as millisecond or seconds since epoch, the message will be timestamped using the ingest time \"message\" String any string No This is the main log message field that is searched by default \"logtype\" String any string No Primary field for identifying logs and matching parsing rules other_fields (must not contain white space) String any string No These will become attributes of the log message Note: Log management does not support white space in attribute names Detailed JSON body message When using the detailed format to create your body, it must be a JSON array containing one or more JSON objects, each of which with the following format: Field Value type Format Required Notes \"common\" Object See common. No Any attributes that are common to all log messages \"logs\" Array See logs. Yes Array with the log entries Limits and restricted characters Caution Avoid calling our API from within the code of a customer-facing application. This can cause performance issues or block your application if response time is slow. If you need to do it this way, call our API asynchronously to avoid these performance issues. Restrictions on logs sent to the Log API: Payload total size: 1MB(10^6 bytes) maximum per POST. We highly recommend using compression. The payload must be encoded as UTF-8. Number of attributes per event: 255 maximum Length of attribute name: 255 characters Length of attribute value: 4096 maximum character length Some specific attributes have additional restrictions: accountId: This is a reserved attribute name. If it is included, it will be dropped during ingest. entity.guid, entity.name, and entity.type: These attributes are used internally to identify entities. Any values submitted with these keys in the attributes section of a metric data point may cause undefined behavior such as missing entities in the UI or telemetry not associating with the expected entities. For more information please refer to Entity synthesis. eventType: This is a reserved attribute name. If it is included, it will be dropped during ingest. timestamp: Must be a Unix epoch timestamp. You can define timestamps either in seconds or in milliseconds. Important Payloads with timestamps older than 48 hours may be dropped. Rate limits on logs sent to the Log API: Maximum rate for HTTP requests sent to the Log API: 300,000 requests per minute Maximum rate of uncompressed Log JSON bytes sent to the Log API: 10 GB per minute Rate limit violations Exceeding rate limits affects how the Log API behaves. Follow these instructions if this happens. HTTP requests per minute When the maximum request rate limit is exceeded for an account, the New Relic Log API returns a 429 response for the remainder of the minute. This response includes a Retry-After header indicating how long to wait in seconds before resubmitting or sending new data. To resolve this issue, either reduce the number of data points you are sending, or request a rate limit change. Subsequent subscription changes do not impact modified rate limits. If an account change impacts your rate limit, you must notify us to adjust your rate limit. To request rate limit changes, contact your New Relic account representative, or visit our Support portal. JSON bytes per minute When the maximum Log JSON byte limit is exceeded for an account, the New Relic Log API returns a 429 response for the remainder of the minute. This response includes a Retry-After header indicating how long to wait in seconds before resubmitting or sending new data. To resolve this issue, try to reduce the amount of log data you are sending, or spread it out over a larger period of time. To request rate limit changes, contact your New Relic account representative, or visit our Support portal. Log payload format We accept any valid JSON payload. The payload must encoded as UTF-8. Important Log management does not support white space in attribute names. For example, {\"Sample Attribute\": \"Value\"} would cause errors. JSON message attributes Common block attributes This is a block containing attributes that will be common to all log entries in logs: Field Value type Format Required Notes \"timestamp\" Integer Milliseconds or seconds since epoch No Message timestamp default to ingest time \"attributes\" Object JSON No This sub-object contains all other attributes of the message Logs block attributes This is an array containing log entries with the following format: Field Value type Format Required Notes \"timestamp\" Integer Milliseconds or seconds since epoch No Message timestamp default to ingest time \"attributes\" Object JSON No This sub-object contains all other attributes of the message \"message\" String (any string) Yes This is the main log message field that is searched by default \"log\" String (any string) No We will rewrite this string as the field message on ingest \"LOG\" String (any string) No We will rewrite this string as the field message on ingest \"MESSAGE\" String (any string) No We will rewrite this string as the field message on ingest JSON message attribute parsing Our log management capabilities will parse any message attribute as JSON. The resulting JSON attributes in the parsed message will be added to the event. If the message attribute is not JSON, it is left as is. Important New Relic's log management capabilities do not support white space in attribute names. For example, {\"Sample Attribute\": \"Value\"} would cause errors. Here is an example message attribute: { \"timestamp\": 1562767499238, \"message\": \"{\\\"service-name\\\": \\\"login-service\\\", \\\"user\\\": {\\\"id\\\": 123, \\\"name\\\": \\\"alice\\\"}}\" } Copy This will be treated as: { \"timestamp\": 1562767499238, \"message\": \"{\\\"service-name\\\": \\\"my-service\\\", \\\"user\\\": {\\\"id\\\": 123, \\\"name\\\": \\\"alice\\\"}}\", \"service-name\": \"my-service\", \"user\": { \"id\": 123, \"name\": \"alice\" } } Copy Log JSON examples Attributes can be scalar JSON types like string and number. They can also be compound (or nested) objects. Compound attributes will have their associated attributes stored with flattened names. For example, here is a compound user attribute in a log entry's attributes: \"attributes\": { \"action\": \"login\", \"user\": { \"id\": 123, \"name\": \"alice\" } } Copy This will result in the following attributes being stored with the log event: Attribute Value \"action\" \"login\" \"user.id\" 123 \"user.name\" \"alice\" Log POST message example Log POST message example: POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: <YOUR_LICENSE_KEY> Accept: */* Content-Length: 319 [{ \"common\": { \"attributes\": { \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } }, \"logs\": [{ \"timestamp\": <TIMESTAMP_IN_UNIX_EPOCH>, \"message\": \"User 'xyz' logged in\" },{ \"timestamp\": <TIMESTAMP_IN_UNIX_EPOCH>, \"message\": \"User 'xyz' logged out\", \"attributes\": { \"auditId\": 123 } }] }] Copy This POST message would result in the following log messages being stored in New Relic: Attribute Value \"logtype\" \"accesslogs\" \"service\" \"login-service\" \"hostname\" \"login.example.com\" Here's an example of stored logs block attributes: Attribute Value \"timestamp\" 1550086450124 \"message\" \"User 'xyz' logged out\" \"auditId\" 123 JSON POST request example Here's an example of a JSON POST request: POST /log/v1 HTTP/1.1 Host: log-api.newrelic.com Content-Type: application/json X-License-Key: <YOUR_LICENSE_KEY> Accept: */* Content-Length: 133 { \"timestamp\": <TIMESTAMP_IN_UNIX_EPOCH>, \"message\": \"User 'xyz' logged in\", \"logtype\": \"accesslogs\", \"service\": \"login-service\", \"hostname\": \"login.example.com\" } Copy What's next? Explore logging data across your platform with the New Relic One UI. Get deeper visibility into both your application and your platform performance data by forwarding your logs with our logs in context capabilities. Set up alerts. Query your data and create dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 273.04724,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to the Log <em>API</em>",
        "sections": "Introduction to the Log <em>API</em>",
        "tags": "Log <em>API</em>",
        "body": " account: United States (US) endpoint: https:&#x2F;&#x2F;log-<em>api</em>.newrelic.com&#x2F;log&#x2F;<em>v1</em> Copy European Union (EU) endpoint: https:&#x2F;&#x2F;log-<em>api</em>.eu.newrelic.com&#x2F;log&#x2F;<em>v1</em> Copy HTTP setup To send log data to your New Relic account via the Log <em>API</em>: Get your New Relic license key. Review the limits and restricted characters"
      },
      "id": "603ea832196a6726e7a83da1"
    },
    {
      "sections": [
        "Write synthetic API tests",
        "Tip",
        "Use API http-request module",
        "Important",
        "Configure request options",
        "Using optional metadata",
        "Using a SSL option or agentOptions",
        "Send a GET request",
        "Insights GET example",
        "Send a POST request",
        "Custom event POST example",
        "Validate results",
        "Event API validation example"
      ],
      "title": "Write synthetic API tests",
      "type": "docs",
      "tags": [
        "Synthetics",
        "Synthetic monitoring",
        "Scripting monitors"
      ],
      "external_id": "236593e91fbe7bb6af91ca5f10db1c01d2df0396",
      "image": "https://docs.newrelic.com/static/1f9113bc9e00a2a14593e27718f45c7c/baaa6/api-test-snap_0.png",
      "url": "https://docs.newrelic.com/docs/synthetics/synthetic-monitoring/scripting-monitors/write-synthetic-api-tests/",
      "published_at": "2021-10-19T03:57:25Z",
      "updated_at": "2021-10-19T03:57:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use synthetic monitoring's API tests to monitor your API endpoint to ensure it is functioning correctly. New Relic uses the http-request module internally to make HTTP calls to your endpoint and validate the results. Here we present some example functions showing how to use the $http object to submit your request. For detailed documentation on the options available for this object, see the http-request readme. (Note that Request is deprecated, but these options still apply.) Tip To view and share other API test examples, visit the synthetics scripts section in Explorers Hub. Use API http-request module API tests are powered by the http-request module, which is available through the $http object. Once each frequency interval, New Relic queries your endpoint from each of your selected locations. For instructions on creating a monitor, see Adding monitors. Read on to learn how to define metadata for your request, make a GET request, make a POST request, and how to validate the results. Important After a maximum run time of three minutes, New Relic manually stops the script. one.newrelic.com > Synthetics > Create monitor: The script editor suggests functions, selectors, and other elements to simplify scripting commands (available in GitHub). Configure request options To start your script: Declare a variable (such as options) to store your request options object. Define request options such as the URL endpoint, and custom headers. If you're setting SSL or agent options, see SSL and agentOptions requirements. We recommend using SSL to avoid exposing plain text credentials in your headers. Tip For a full list of supported request options, see request(options, callback) in the http-request documentation on GitHub. Here's an example of optional metadata in the options object: Using optional metadata //Declare optional metadata var options = { //Specify the endpoint URL url: 'https://api-endpoint.example.com', //Specify optional headers headers: { 'Endpoint-Key': 'uqNTC57Phe72pnnB8JuJmwAr7b09nKSKSz', 'Additional-Header': 'Additional-Header-Data' } }; Copy For SSL and agentOptions: If you are setting SSL options or providing an agentOptions object, the agent property in the request options object will need to be set to $globalAgents.https or $globalAgents.http to ensure your HTTP requests use the instrumented global agent. Here's an example of using a SSL option or agentOptions: Using a SSL option or agentOptions This example uses agentOptions: //Declare optional metadata var options = { //Specify the endpoint URL url: 'https://api-endpoint.example.com', //Specify optional headers headers: { 'Endpoint-Key': 'uqNTC57Phe72pnnB8JuJmwAr7b09nKSKSz', 'Additional-Header': 'Additional-Header-Data' } //Specify global agent as the http agent agent: $globalAgents.https, //Set SSL option strictSSL: true, //Specify http agent options agentOptions: { ​maxVersion: 'TLSv1.1' }, }; Copy Send a GET request To make a GET request, use the $http.get method to submit your request. Define your request options, make your request using $http.get, then validate the response to ensure your endpoint is returning the correct results. Insights GET example This example queries the Insights API by using GET: //Define your authentication credentials var myAccountID = '{YOUR_ACCOUNT_ID}'; var myQueryKey = '{YOUR_QUERY_KEY}'; var options = { //Define endpoint URI uri: 'https://insights-api.newrelic.com/v1/accounts/'+myAccountID+'/query?nrql=SELECT%20average(amount)%20FROM%20SyntheticsEvent', //Define query key and expected data type. headers: { 'X-Query-Key': myQueryKey, 'Accept': 'application/json' } }; //Define expected results using callback function. function callback (err, response, body){ //Log JSON results from endpoint to Synthetics console. console.log(JSON.parse(body)); console.log('done with script'); } //Make GET request, passing in options and callback. $http.get(options,callback); Copy Send a POST request To make a POST request, use the $http.post method to submit your request. Define your request options, make your request using $http.post, then validate the response to ensure your endpoint is returning the correct results. Custom event POST example This example POSTs a custom event containing static integers to the Event API: //Define your authentication credentials. var myAccountID = '{YOUR_ACCOUNT_ID}'; var myLicenseKey = '{YOUR_LICENSE_KEY}'; //Import the 'assert' module to validate results. var assert = require('assert'); var options = { //Define endpoint URL. url: \"https://insights-collector.newrelic.com/v1/accounts/\"+myAccountID+\"/events\", //Define body of POST request. body: '[{\"eventType\":\"SyntheticsEvent\",\"integer1\":1000,\"integer2\":2000}]', //Define New Relic license key and expected data type. headers: { 'Api-Key': myLicenseKey, 'Content-Type': 'application/json' } }; //Define expected results using callback function. function callback(error, response, body) { //Log status code to Synthetics console. console.log(response.statusCode + \" status code\") //Verify endpoint returns 200 (OK) response code. assert.ok(response.statusCode == 200, 'Expected 200 OK response'); //Parse JSON received from Insights into variable. var info = JSON.parse(body); //Verify that `info` contains element named `success` with a value of `true`. assert.ok(info.success == true, 'Expected True results in Response Body, result was ' + info.success); //Log end of script. console.log(\"End reached\"); } //Make POST request, passing in options and callback. $http.post(options, callback); Copy Validate results To validate your results, import the assert module to define your test case. Call an assert method to validate your endpoint's response. If any assert functions fail, the entire monitor will be considered a failed check. This may trigger alert notifications and affect your metrics. Important Synthetic monitoring does not allow thrown exceptions. Thrown exceptions result in script failure. Use the assert module to validate your results, and use console.log() to log results to the synthetic's console. Event API validation example This example POSTs to the Event API, then validates that the response is {\"success\":true}: //Define your authentication credentials. var myAccountID = '{YOUR_ACCOUNT_ID}'; var myLicenseKey = '{YOUR_LICENSE_KEY}'; //Import the `assert` module to validate results. var assert = require('assert'); var options = { //Define endpoint URL. url: \"https://insights-collector.newrelic.com/v1/accounts/\"+myAccountID+\"/events\", //Define body of POST request. body: '[{\"eventType\":\"SyntheticsEvent\",\"integer1\":1000,\"integer2\":2000}]', //Define New Relic license key and expected data type. headers: { 'Api-Key': myLicenseKey, 'Content-Type': 'application/json' } }; $http.post(options, function(error, response, body) { //Log status code to Synthetics console. The status code is logged before the `assert` function, //because a failed assert function ends the script. console.log(response.statusCode + \" status code\") //Call `assert` method, expecting a `200` response code. //If assertion fails, log `Expected 200 OK response` as error message to Synthetics console. assert.ok(response.statusCode == 200, 'Expected 200 OK response'); var info = JSON.parse(body); //Call `assert` method, expecting body to return `{\"success\":true}`. //If assertion fails, log `Expected True results in Response Body,` plus results as error message to Synthetics console. assert.ok(info.success == true, 'Expected True results in Response Body, result was ' + info.success); }); Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 247.28177,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Write synthetic <em>API</em> tests",
        "sections": "Event <em>API</em> <em>validation</em> example",
        "body": " = &#x27;{YOUR_ACCOUNT_ID}&#x27;; var myQueryKey = &#x27;{YOUR_QUERY_KEY}&#x27;; var options = { &#x2F;&#x2F;Define endpoint URI uri: &#x27;https:&#x2F;&#x2F;insights-<em>api</em>.newrelic.com&#x2F;<em>v1</em>&#x2F;accounts&#x2F;&#x27;+myAccountID+&#x27;&#x2F;query?nrql=SELECT%20average(amount)%20FROM%20SyntheticsEvent&#x27;, &#x2F;&#x2F;Define query key and expected data type. headers: { &#x27;X-Query-Key&#x27;: myQueryKey, &#x27;Accept"
      },
      "id": "603ecf4328ccbc9c48eba78f"
    }
  ],
  "/docs/mobile-monitoring/index": [
    {
      "sections": [
        "HTTP requests page",
        "Find and use HTTP requests page",
        "Understand HTTP request data",
        "Response time chart",
        "HTTP errors and network failures chart",
        "Total requests",
        "Group, sort, and filter HTTP requests",
        "View and share HTTP request data",
        "View legacy HTTP requests UI page",
        "View legacy HTTP requests UI",
        "View legacy drill-down details",
        "View legacy request data"
      ],
      "title": "HTTP requests page",
      "type": "docs",
      "tags": [
        "Mobile monitoring",
        "Mobile monitoring UI",
        "Network pages"
      ],
      "external_id": "56c27e3a1cad7439b752d38b4d00a60ab98f0e10",
      "image": "",
      "url": "https://docs.newrelic.com/docs/mobile-monitoring/mobile-monitoring-ui/network-pages/http-requests-page/",
      "published_at": "2021-10-18T12:51:14Z",
      "updated_at": "2021-10-07T20:06:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Mobile monitoring has an HTTP requests UI page that helps you better understand HTTP requests associated with your mobile app and how those network calls are affecting performance. This document describes the Enterprise-level HTTP requests page. Non-Enterprise accounts will see the legacy HTTP requests page. Find and use HTTP requests page To view mobile monitoring's HTTP requests page: Go to one.newrelic.com > Mobile > (select an app) > Network > HTTP requests. Use our standard page functions to look for trends in the HTTP analysis charts. Target specific request and response attributes by grouping, sorting, and filtering the data. Understand HTTP request data Here are some places to find the most important HTTP request information: Response time chart The response time chart shows how your app's network calls are performing across percentiles. Use it to compare the average response time to the 1st, 50th, and 99th percentile. Percentiles let you filter out outliers that may be making your average response time higher than expected. HTTP errors and network failures chart This chart shows the unsuccessful network calls your app is experiencing. Select the chart title to go to the HTTP errors page for more detail on the errors and failures. Total requests Sort by Total requests to identify which network requests are being used most frequently. The reason this can be helpful is because your slowest network calls may be only infrequently used, while more frequently used requests might be more worthy of optimization even if they are not the slowest. For a description of the non-Enterprise HTTP requests UI page, see Legacy HTTP requests. Group, sort, and filter HTTP requests If you want to... Do this... Group and sort HTTP requests in different ways Make selections from the Group by and Sort by dropdowns. By default, the HTTP requests page is grouped by request domain and sorted by average response time. Filter for specific HTTP requests Select an HTTP request from the Errors and failures list and/or select multiple filters from the Filter dropdown. See or remove applied filters The filters you select are displayed next to the filter dropdown. To clear filters, select the X icon on the filter you want to clear. Change the time window Select a new time period from the time picker dropdown. View information for a specific app version Using the Versions dropdown, select the version for which you want to see charts and lists. View and share HTTP request data To view any HTTP requests chart in Insights: Select for any chart. Select View query > View in Insights. Optional: Add the data to a dashboard, or share it by using a permalink. To delve deeper into your request data, query MobileRequest events and attributes. View legacy HTTP requests UI page Accounts that do not have an Enterprise-level subscription see a different HTTP requests UI page: View legacy HTTP requests UI To view your top five domains or drill down into details about specific HTTP requests: Go to one.newrelic.com > (select an app) > Network > HTTP requests. Optional: Select the Sort by and Hide < 1% throughput options. To view or hide all requests made by your app, select Expand all or Collapse all. To view details for a specific host or HTTP request (including request time, average throughput, and data transfer), select its name. View legacy drill-down details Use any of New Relic's standard page functions to drill down into detailed information. In addition, from the HTTP requests page, you can drill down into detailed information about specific requests, including: Top five HTTP request times Average throughput Average data transfer To view legacy details: one.newrelic.com > Mobile > (select an app) > Network > HTTP requests > (select a request): > Switch to legacy requests. If you want to... Do this View information to a specific version of your app Select Versions from the side bar (if applicable). Change the time period Use the time picker below the New Relic menu bar. View legacy request data You can dig deeper into your request data by querying and charting the MobileRequest event.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 516.8676,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Mobile</em> <em>monitoring</em>",
        "body": "<em>Mobile</em> <em>monitoring</em> has an HTTP requests UI page that helps you better understand HTTP requests associated with your <em>mobile</em> app and how those network calls are affecting performance. This document describes the Enterprise-level HTTP requests page. Non-Enterprise accounts will see the legacy HTTP"
      },
      "id": "60450de028ccbc42662c6083"
    },
    {
      "sections": [
        "Map page for mobile apps (deprecated)",
        "Important",
        "View a map of your mobile app services"
      ],
      "title": "Map page for mobile apps (deprecated)",
      "type": "docs",
      "tags": [
        "Mobile monitoring",
        "Mobile monitoring UI",
        "Network pages"
      ],
      "external_id": "a082467948ce481c9ecb544d26a802e8d5f3894b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/mobile-monitoring/mobile-monitoring-ui/network-pages/map-page-mobile-apps-deprecated/",
      "published_at": "2021-10-18T04:38:37Z",
      "updated_at": "2021-09-14T20:45:23Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important The mobile Maps UI is deprecated since December 22, 2020. Service maps are available in New Relic One's left navigation for each mobile entity, and they are a better way to visualize and customize representations of your architecture. For more information, see our Explorers Hub post. Maps help you find performance problems for a mobile app or its services. This gives you a clear picture of your app's relationships to other services and the influence of each service on the others. If one service fails, you can see at a glance which other services are affected. View a map of your mobile app services To view your mobile app and its related services as an architectural map, go to one.newrelic.com > Mobile > (select a mobile app) > Monitor > Service map. For more information, see the service maps documentation. If you need to use the deprecated mobile Map page, follow these steps: Go to one.newrelic.com > Mobile > (select an app) > Network > Map. To view HTTP request details for a service, select its name. To view details for an app monitored by APM that is related to the service, select the service's name below the associated hostname. To view throughput details as a chart, select the icon or the cpm bar below the service's name. To view detailed metrics for a service, mouse over the throughput chart. The Map page for mobile monitoring gives an architectural view of your mobile app and the services it uses,",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 421.53418,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Map page for <em>mobile</em> apps (deprecated)",
        "sections": "Map page for <em>mobile</em> apps (deprecated)",
        "tags": "<em>Mobile</em> <em>monitoring</em>",
        "body": " <em>mobile</em> app services To view your <em>mobile</em> app and its related services as an architectural map, go to one.newrelic.com &gt; <em>Mobile</em> &gt; (select a <em>mobile</em> app) &gt; <em>Monitor</em> &gt; Service map. For more information, see the service maps documentation. If you need to use the deprecated <em>mobile</em> Map page, follow these steps"
      },
      "id": "6044141828ccbc0f862c60ae"
    },
    {
      "sections": [
        "Mobile crash event trail",
        "Tip",
        "View events before mobile app crashes",
        "Use the event trail",
        "Difference between event trail and interaction trail"
      ],
      "title": "Mobile crash event trail",
      "type": "docs",
      "tags": [
        "Mobile monitoring",
        "Mobile monitoring UI",
        "Crashes"
      ],
      "external_id": "58795d499c8db6d8cc95bd6d2c645e970ea10d83",
      "image": "https://docs.newrelic.com/static/3731efca2d88ed92cc150d5f6c06830a/0d6fe/New-Relic-Mobile-crash-event-trail.png",
      "url": "https://docs.newrelic.com/docs/mobile-monitoring/mobile-monitoring-ui/crashes/mobile-crash-event-trail/",
      "published_at": "2021-10-18T04:36:19Z",
      "updated_at": "2021-07-22T01:05:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The mobile monitoring crash event trail shows you the events leading up to a crash of a mobile app, based on your subscription level's data retention policy. These can be events New Relic monitors by default, or custom events. This document explains what the crash event trail is and how to use it. Tip Access to this feature depends on your subscription level. View events before mobile app crashes When a mobile app crashes and you don't know why, you can study what happened right before the crash. The crash event trail shows you these events so that you can follow the \"breadcrumbs\" leading up to the crash and diagnose the cause of the failure. one.newrelic.com > Mobile > (select a mobile app) > Crash analysis > (selected crash type) > Event trail: The crash event trail shows the activity leading up to a mobile app crash. The crash event trail shows all mobile event types leading up to a crash. You can use the iOS SDK or Android SDK to create custom MobileBreadcrumb events that track whatever app activity you think would help you diagnose a crash. You can also use MobileHandledException events in the crash event trail to aid in debugging. Use the iOS and Android recordHandledException APIs for iOS or Android to annotate where exceptions are handled in your application. These events will automatically appear in the crash event trail. For more about annotating crash event trails with custom data, see Add custom data to mobile monitoring. Use the event trail To use the crash event trail: Go to one.newrelic.com > Mobile > (select a mobile app) > Crash analysis. On the lower right side of the Crash analysis page, select a crash type. On the Crash details page, beside the stack trace, select Event trail. Study the events leading up to a crash type for clues to the reasons for the crash. To expand details about an event's attributes, select it. To view the event trail results in New Relic, select Open session in Insights. To scroll through occurrences of the same crash type, use the event trail's left and right arrows. To make the most out of our crash analysis tools, use: The Android SDK API or iOS SDK API to create custom MobileBreadcrumb or MobileHandledException events Enable MobileRequest events Crash analysis page Interaction trail Difference between event trail and interaction trail The crash event trail is different from the interaction trail. The crash event trail shows all mobile event types leading up to a crash, whereas the interaction trail only shows interaction event types (Mobile events with the category interaction). The interaction trail has additional features, including stack traces and links to the associated interaction charts.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 374.0958,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Mobile</em> crash event trail",
        "sections": "<em>Mobile</em> crash event trail",
        "tags": "<em>Mobile</em> <em>monitoring</em>",
        "body": "The <em>mobile</em> <em>monitoring</em> crash event trail shows you the events leading up to a crash of a <em>mobile</em> app, based on your subscription level&#x27;s data retention policy. These can be events New Relic monitors by default, or custom events. This document explains what the crash event trail is and how to use"
      },
      "id": "604503b1e7b9d201e75799bb"
    }
  ],
  "/docs/mobile-monitoring/mobile-monitoring-ui/crashes/crash-analysis-group-filter-your-crashes": [
    {
      "sections": [
        "Mobile crash event trail",
        "Tip",
        "View events before mobile app crashes",
        "Use the event trail",
        "Difference between event trail and interaction trail"
      ],
      "title": "Mobile crash event trail",
      "type": "docs",
      "tags": [
        "Mobile monitoring",
        "Mobile monitoring UI",
        "Crashes"
      ],
      "external_id": "58795d499c8db6d8cc95bd6d2c645e970ea10d83",
      "image": "https://docs.newrelic.com/static/3731efca2d88ed92cc150d5f6c06830a/0d6fe/New-Relic-Mobile-crash-event-trail.png",
      "url": "https://docs.newrelic.com/docs/mobile-monitoring/mobile-monitoring-ui/crashes/mobile-crash-event-trail/",
      "published_at": "2021-10-18T04:36:19Z",
      "updated_at": "2021-07-22T01:05:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The mobile monitoring crash event trail shows you the events leading up to a crash of a mobile app, based on your subscription level's data retention policy. These can be events New Relic monitors by default, or custom events. This document explains what the crash event trail is and how to use it. Tip Access to this feature depends on your subscription level. View events before mobile app crashes When a mobile app crashes and you don't know why, you can study what happened right before the crash. The crash event trail shows you these events so that you can follow the \"breadcrumbs\" leading up to the crash and diagnose the cause of the failure. one.newrelic.com > Mobile > (select a mobile app) > Crash analysis > (selected crash type) > Event trail: The crash event trail shows the activity leading up to a mobile app crash. The crash event trail shows all mobile event types leading up to a crash. You can use the iOS SDK or Android SDK to create custom MobileBreadcrumb events that track whatever app activity you think would help you diagnose a crash. You can also use MobileHandledException events in the crash event trail to aid in debugging. Use the iOS and Android recordHandledException APIs for iOS or Android to annotate where exceptions are handled in your application. These events will automatically appear in the crash event trail. For more about annotating crash event trails with custom data, see Add custom data to mobile monitoring. Use the event trail To use the crash event trail: Go to one.newrelic.com > Mobile > (select a mobile app) > Crash analysis. On the lower right side of the Crash analysis page, select a crash type. On the Crash details page, beside the stack trace, select Event trail. Study the events leading up to a crash type for clues to the reasons for the crash. To expand details about an event's attributes, select it. To view the event trail results in New Relic, select Open session in Insights. To scroll through occurrences of the same crash type, use the event trail's left and right arrows. To make the most out of our crash analysis tools, use: The Android SDK API or iOS SDK API to create custom MobileBreadcrumb or MobileHandledException events Enable MobileRequest events Crash analysis page Interaction trail Difference between event trail and interaction trail The crash event trail is different from the interaction trail. The crash event trail shows all mobile event types leading up to a crash, whereas the interaction trail only shows interaction event types (Mobile events with the category interaction). The interaction trail has additional features, including stack traces and links to the associated interaction charts.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.3088,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Mobile</em> <em>crash</em> event trail",
        "sections": "View events before <em>mobile</em> app <em>crashes</em>",
        "tags": "<em>Mobile</em> <em>monitoring</em> <em>UI</em>",
        "body": "The <em>mobile</em> <em>monitoring</em> <em>crash</em> event trail shows you the events leading up to a <em>crash</em> of a <em>mobile</em> app, based on your subscription level&#x27;s data retention policy. These can be events New Relic monitors by default, or custom events. This document explains what the <em>crash</em> event trail is and how to use"
      },
      "id": "604503b1e7b9d201e75799bb"
    },
    {
      "sections": [
        "Introduction to mobile handled exceptions",
        "Features",
        "Requirements",
        "Tip",
        "Handled exceptions API and event type"
      ],
      "title": "Introduction to mobile handled exceptions",
      "type": "docs",
      "tags": [
        "Mobile monitoring",
        "Mobile monitoring UI",
        "Crashes"
      ],
      "external_id": "cfdf733d55bb89d157df675fa162b737bdad52c7",
      "image": "https://docs.newrelic.com/static/5891a9437b94b543d81ee04a70ebe876/8c557/mobile-handled-exceptions-ui.png",
      "url": "https://docs.newrelic.com/docs/mobile-monitoring/mobile-monitoring-ui/crashes/introduction-mobile-handled-exceptions/",
      "published_at": "2021-10-18T04:35:15Z",
      "updated_at": "2021-07-22T00:02:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Exceptions can contribute to invalid application states, resulting not only in application crashes but also in negative user reviews. This may lead to users deleting your app, which in turn may affect your organization's profitability. With the Handled exceptions UI, mobile development managers and their developer teams can identify significant factors affecting poor mobile app experience, and use filterable data to find a resolution more quickly. Features Handling exceptions as they occur can help improve your mobile app users' experience, but it's not enough to catch exceptions. You also need to know how to prevent them. For example: How many different types of handled exceptions are occurring? A high occurrence rate may necessitate changes to the back-end systems. Why does the user's app usage result in a try/catch? What is the context for the exceptions? When can a test environment's responses to handled exceptions indicate additional, more serious problems? What would have caused a crash if the exception had not been caught in production? What else (in the code or back-end API) is still affecting the users' experience? By using handled exceptions, you can identify and resolve these kinds of issues more quickly. one.newrelic.com > Mobile > (select an app) > Exceptions > Handled exceptions: As you explore the wealth of data in the charts and table, use groups and filters to discover patterns that help you determine the root cause of mobile app exceptions. Handled exception features Comments Slice and dice your exception data. You can view exception data via API as well as the UI: Use the recordHandledException() method within a try{...} catch(){...} block to help understand how often your application is throwing exceptions, and under what conditions. Use groups and filters to analyze trends leading to the exception. For example, you can group by OS Build, then filter a specific appVersion. Understand a particular user's experience. Examine the percentage charts to see overall trends with users and sessions at a glance. Then, use custom attributes to focus on exceptions related to paid accounts than free accounts. Pinpoint when most exceptions occur. For example, group on Last Interaction to get an overall view of problems. To drill down further, use filters, such as: To examine exceptions caused by network problems, filter by carrier and then select wifi. To examine exceptions caused by app releases, filter by appVersion. Align issues with common characteristics. For example: Use groups and filters to determine whether handled exception trends appear in networks (ASN, carrier, location, etc.) or in devices (device model, manufacturer, operating system build, version, etc.). Explore recurring patterns in stack traces with the top five exception locations table. Query data and share your findings. Explore the event trail before and after a crash. With mobile monitoring you get a more complete picture of events before and after crashes occur, so you can analyze and resolve problems from multiple angles: Use the handled exception's Occurrences page for expected exceptions. Use the Crash analysis UI and event trail for unanticipated exceptions. Requirements Tip Access to this feature depends on your subscription level and mobile data retention. Additional requirements include: Android: Android agent version 5.15.0 or higher iOS: iOS agent version 5.15.0 or higher Handled exceptions API and event type Mobile monitoring automatically includes default attributes that you can use to explore your handled exceptions data in the query builder and get specific details: Use the recordHandledExceptions() method for the Android or iOS SDK API. Query the MobileHandledException event type. For more information, see the NRQL examples for mobile monitoring. You can also create your own custom attributes and events. Then, select attributes in the Handled exceptions page, and query or share them.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.30135,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>mobile</em> handled exceptions",
        "sections": "Introduction to <em>mobile</em> handled exceptions",
        "tags": "<em>Mobile</em> <em>monitoring</em> <em>UI</em>",
        "body": " system build, version, etc.). Explore recurring patterns in stack traces with the top five exception locations table. Query data and share your findings. Explore the event trail before and after a <em>crash</em>. With <em>mobile</em> <em>monitoring</em> you get a more complete picture of events before and after <em>crashes</em> occur, so"
      },
      "id": "603e7e8228ccbc05f9eba770"
    },
    {
      "sections": [
        "Handled exceptions: Analyze trends, prevent crashes",
        "Handled exceptions workflow",
        "Exception percentage charts",
        "Exception percentage charts example",
        "Groups and filters",
        "Groups and filters example",
        "Top five exception locations",
        "Top five exception locations example",
        "Query builder links",
        "Exception locations table",
        "Exception locations table example"
      ],
      "title": "Handled exceptions: Analyze trends, prevent crashes",
      "type": "docs",
      "tags": [
        "Mobile monitoring",
        "Mobile monitoring UI",
        "Crashes"
      ],
      "external_id": "d325744648613b771d7dd39de3f1448fe8a54ab9",
      "image": "https://docs.newrelic.com/static/5891a9437b94b543d81ee04a70ebe876/8c557/mobile-handled-exceptions-ui.png",
      "url": "https://docs.newrelic.com/docs/mobile-monitoring/mobile-monitoring-ui/crashes/handled-exceptions-analyze-trends-prevent-crashes/",
      "published_at": "2021-10-18T14:50:51Z",
      "updated_at": "2021-07-21T21:33:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Handled exceptions help you identify significant factors contributing to poor mobile application experience, and use filterable data to find a resolution more quickly. You can also use the handled exceptions API to customize the data you send, and use NRQL to query and share the data. Handled exceptions workflow To get the most out of the Handled exceptions UI, use this basic workflow: Go to one.newrelic.com > Mobile > (select an app) > Exceptions > Handled exceptions. Use any of New Relic's standard page functions to drill down into detailed information; for example, zoom into any area of a chart. Look for obvious or general trends in the Users affected and Sessions affected percentage charts. Adjust the types of exceptions shown by using groups and filters. Optional: Query or share the chart data. Look for similar patterns where exceptions appear in stack traces with the Top 5 exception locations table. To view stack trace thread details for each occurrence of the exception, select a record from the Top 5 exceptions location table. one.newrelic.com > Mobile > (select an app) > Exceptions > Handled exceptions: As you explore the wealth of data in the charts and table, use groups and filters to discover patterns that help you determine the root cause of mobile app exceptions. Exception percentage charts Start with the Users affected and Sessions affected percentage charts to see at a glance whether there are any unexpected spikes, dips, or patterns with exceptions in general. (If the Users affected chart is empty, there were no user sessions during the selected time period.) For example: Are there any spikes near a recent version release? Is there a time period when the percentage of users has been affected significantly by the exception? Are there uneventful periods? To examine data in greater detail: Below any chart, select Expand chart. Exception percentage charts example one.newrelic.com > Mobile > (select an app) > Exceptions > Handled exceptions: The percentage charts help you quickly see any unexpected spikes, dips, or patterns with exceptions in general. Groups and filters Use the groups and filters to examine attributes for crashes, devices, locations, or other custom attributes in more detail. You can select a group, then filter to specific data. For example: Group the list by exception location (default), cause, app build or version, devices, connections, or other custom attributes. This lets you discover patterns in your exceptions to determine the root cause. Use the time picker to adjust the currently selected time period. Filter by a specific Version or by one or more attribute Filter, such as appVersion, exceptionLocationMethod, lastInteraction, or any of the longer list of standard and custom attributes. The currently selected filters appear at the top of the UI page. You can close them, add other filters, or select other groups and filters. Groups and filters example one.newrelic.com > Mobile > (select an app) > Exceptions > Handled exceptions: Group the data by attributes that matter to the most to you, then select one or more filters to help pinpoint specific causes behind the exceptions. Top five exception locations Use the Top 5 exception locations table to find or sort patterns in the type of exception you selected from the groups and filters. This includes: Recurring locations in the stack trace Mobile app version Number of occurrences Number of users affected during the selected time period For example, you can group by Exception Message, filter to timeout message, then select individual timeout locations from the table to review the stack trace thread and details about each occurrence. To filter or group by other attributes, use the table's search window, or select any of the available filters. For example, filter by type of occurrence, device, a specific location, or any custom attributes. To look for other historical patterns, change the selected time period. Top five exception locations example one.newrelic.com > Mobile > (select an app) > Exceptions > Handled exceptions: This example shows the Expand chart button and links to the query builder, where you can query, create dashboards, and share the handled exceptions data. Query builder links Handled exceptions charts use default attributes for mobile events (including MobileHandledException), along with any custom attributes you have added to this event type. When you mouse over the charts, direct links appear below them. These links to the query builder allow you to analyze your mobile app data even deeper. View query link: View the NRQL query used to calculate the chart data. View in query builder link: View the chart, and share it with others. Exception locations table The Exception locations table supplements the charts. It lists where the top five handled exceptions appear in their stack trace thread, and links them to relevant details. Each row helps you find answers to questions such as: How many of this exception occurred within the selected time period? Does a specific app version have a higher (or lower) number of users affected? Which exception has the fewest number of occurrences? You can change the sort order or filter options to focus on just the types of exceptions that matter the most to you and your teams. To view additional thread details for each occurrence of the exception, select a record from the Top 5 exceptions location table. Exception locations table example one.newrelic.com > Mobile > (select an app) > Exceptions > Handled exceptions: To continue to the handled exception's Occurrences page, select any row on the table.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.28369,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Handled exceptions: Analyze trends, prevent <em>crashes</em>",
        "sections": "Handled exceptions: Analyze trends, prevent <em>crashes</em>",
        "tags": "<em>Mobile</em> <em>monitoring</em> <em>UI</em>",
        "body": " exceptions workflow To get the most out of the Handled exceptions <em>UI</em>, use this basic workflow: Go to one.newrelic.com &gt; <em>Mobile</em> &gt; (select an app) &gt; Exceptions &gt; Handled exceptions. Use any of New Relic&#x27;s standard page functions to drill down into detailed information; for example, zoom into any area"
      },
      "id": "604505ae28ccbc783e2c6085"
    }
  ],
  "/docs/mobile-monitoring/mobile-monitoring-ui/crashes/find-build-uuids-unsymbolicated-crashes": [
    {
      "sections": [
        "Mobile crash event trail",
        "Tip",
        "View events before mobile app crashes",
        "Use the event trail",
        "Difference between event trail and interaction trail"
      ],
      "title": "Mobile crash event trail",
      "type": "docs",
      "tags": [
        "Mobile monitoring",
        "Mobile monitoring UI",
        "Crashes"
      ],
      "external_id": "58795d499c8db6d8cc95bd6d2c645e970ea10d83",
      "image": "https://docs.newrelic.com/static/3731efca2d88ed92cc150d5f6c06830a/0d6fe/New-Relic-Mobile-crash-event-trail.png",
      "url": "https://docs.newrelic.com/docs/mobile-monitoring/mobile-monitoring-ui/crashes/mobile-crash-event-trail/",
      "published_at": "2021-10-18T04:36:19Z",
      "updated_at": "2021-07-22T01:05:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The mobile monitoring crash event trail shows you the events leading up to a crash of a mobile app, based on your subscription level's data retention policy. These can be events New Relic monitors by default, or custom events. This document explains what the crash event trail is and how to use it. Tip Access to this feature depends on your subscription level. View events before mobile app crashes When a mobile app crashes and you don't know why, you can study what happened right before the crash. The crash event trail shows you these events so that you can follow the \"breadcrumbs\" leading up to the crash and diagnose the cause of the failure. one.newrelic.com > Mobile > (select a mobile app) > Crash analysis > (selected crash type) > Event trail: The crash event trail shows the activity leading up to a mobile app crash. The crash event trail shows all mobile event types leading up to a crash. You can use the iOS SDK or Android SDK to create custom MobileBreadcrumb events that track whatever app activity you think would help you diagnose a crash. You can also use MobileHandledException events in the crash event trail to aid in debugging. Use the iOS and Android recordHandledException APIs for iOS or Android to annotate where exceptions are handled in your application. These events will automatically appear in the crash event trail. For more about annotating crash event trails with custom data, see Add custom data to mobile monitoring. Use the event trail To use the crash event trail: Go to one.newrelic.com > Mobile > (select a mobile app) > Crash analysis. On the lower right side of the Crash analysis page, select a crash type. On the Crash details page, beside the stack trace, select Event trail. Study the events leading up to a crash type for clues to the reasons for the crash. To expand details about an event's attributes, select it. To view the event trail results in New Relic, select Open session in Insights. To scroll through occurrences of the same crash type, use the event trail's left and right arrows. To make the most out of our crash analysis tools, use: The Android SDK API or iOS SDK API to create custom MobileBreadcrumb or MobileHandledException events Enable MobileRequest events Crash analysis page Interaction trail Difference between event trail and interaction trail The crash event trail is different from the interaction trail. The crash event trail shows all mobile event types leading up to a crash, whereas the interaction trail only shows interaction event types (Mobile events with the category interaction). The interaction trail has additional features, including stack traces and links to the associated interaction charts.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.30879,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Mobile</em> <em>crash</em> event trail",
        "sections": "View events before <em>mobile</em> app <em>crashes</em>",
        "tags": "<em>Mobile</em> <em>monitoring</em> <em>UI</em>",
        "body": "The <em>mobile</em> <em>monitoring</em> <em>crash</em> event trail shows you the events leading up to a <em>crash</em> of a <em>mobile</em> app, based on your subscription level&#x27;s data retention policy. These can be events New Relic monitors by default, or custom events. This document explains what the <em>crash</em> event trail is and how to use"
      },
      "id": "604503b1e7b9d201e75799bb"
    },
    {
      "sections": [
        "Introduction to mobile handled exceptions",
        "Features",
        "Requirements",
        "Tip",
        "Handled exceptions API and event type"
      ],
      "title": "Introduction to mobile handled exceptions",
      "type": "docs",
      "tags": [
        "Mobile monitoring",
        "Mobile monitoring UI",
        "Crashes"
      ],
      "external_id": "cfdf733d55bb89d157df675fa162b737bdad52c7",
      "image": "https://docs.newrelic.com/static/5891a9437b94b543d81ee04a70ebe876/8c557/mobile-handled-exceptions-ui.png",
      "url": "https://docs.newrelic.com/docs/mobile-monitoring/mobile-monitoring-ui/crashes/introduction-mobile-handled-exceptions/",
      "published_at": "2021-10-18T04:35:15Z",
      "updated_at": "2021-07-22T00:02:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Exceptions can contribute to invalid application states, resulting not only in application crashes but also in negative user reviews. This may lead to users deleting your app, which in turn may affect your organization's profitability. With the Handled exceptions UI, mobile development managers and their developer teams can identify significant factors affecting poor mobile app experience, and use filterable data to find a resolution more quickly. Features Handling exceptions as they occur can help improve your mobile app users' experience, but it's not enough to catch exceptions. You also need to know how to prevent them. For example: How many different types of handled exceptions are occurring? A high occurrence rate may necessitate changes to the back-end systems. Why does the user's app usage result in a try/catch? What is the context for the exceptions? When can a test environment's responses to handled exceptions indicate additional, more serious problems? What would have caused a crash if the exception had not been caught in production? What else (in the code or back-end API) is still affecting the users' experience? By using handled exceptions, you can identify and resolve these kinds of issues more quickly. one.newrelic.com > Mobile > (select an app) > Exceptions > Handled exceptions: As you explore the wealth of data in the charts and table, use groups and filters to discover patterns that help you determine the root cause of mobile app exceptions. Handled exception features Comments Slice and dice your exception data. You can view exception data via API as well as the UI: Use the recordHandledException() method within a try{...} catch(){...} block to help understand how often your application is throwing exceptions, and under what conditions. Use groups and filters to analyze trends leading to the exception. For example, you can group by OS Build, then filter a specific appVersion. Understand a particular user's experience. Examine the percentage charts to see overall trends with users and sessions at a glance. Then, use custom attributes to focus on exceptions related to paid accounts than free accounts. Pinpoint when most exceptions occur. For example, group on Last Interaction to get an overall view of problems. To drill down further, use filters, such as: To examine exceptions caused by network problems, filter by carrier and then select wifi. To examine exceptions caused by app releases, filter by appVersion. Align issues with common characteristics. For example: Use groups and filters to determine whether handled exception trends appear in networks (ASN, carrier, location, etc.) or in devices (device model, manufacturer, operating system build, version, etc.). Explore recurring patterns in stack traces with the top five exception locations table. Query data and share your findings. Explore the event trail before and after a crash. With mobile monitoring you get a more complete picture of events before and after crashes occur, so you can analyze and resolve problems from multiple angles: Use the handled exception's Occurrences page for expected exceptions. Use the Crash analysis UI and event trail for unanticipated exceptions. Requirements Tip Access to this feature depends on your subscription level and mobile data retention. Additional requirements include: Android: Android agent version 5.15.0 or higher iOS: iOS agent version 5.15.0 or higher Handled exceptions API and event type Mobile monitoring automatically includes default attributes that you can use to explore your handled exceptions data in the query builder and get specific details: Use the recordHandledExceptions() method for the Android or iOS SDK API. Query the MobileHandledException event type. For more information, see the NRQL examples for mobile monitoring. You can also create your own custom attributes and events. Then, select attributes in the Handled exceptions page, and query or share them.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.30135,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>mobile</em> handled exceptions",
        "sections": "Introduction to <em>mobile</em> handled exceptions",
        "tags": "<em>Mobile</em> <em>monitoring</em> <em>UI</em>",
        "body": " system build, version, etc.). Explore recurring patterns in stack traces with the top five exception locations table. Query data and share your findings. Explore the event trail before and after a <em>crash</em>. With <em>mobile</em> <em>monitoring</em> you get a more complete picture of events before and after <em>crashes</em> occur, so"
      },
      "id": "603e7e8228ccbc05f9eba770"
    },
    {
      "sections": [
        "Handled exceptions: Analyze trends, prevent crashes",
        "Handled exceptions workflow",
        "Exception percentage charts",
        "Exception percentage charts example",
        "Groups and filters",
        "Groups and filters example",
        "Top five exception locations",
        "Top five exception locations example",
        "Query builder links",
        "Exception locations table",
        "Exception locations table example"
      ],
      "title": "Handled exceptions: Analyze trends, prevent crashes",
      "type": "docs",
      "tags": [
        "Mobile monitoring",
        "Mobile monitoring UI",
        "Crashes"
      ],
      "external_id": "d325744648613b771d7dd39de3f1448fe8a54ab9",
      "image": "https://docs.newrelic.com/static/5891a9437b94b543d81ee04a70ebe876/8c557/mobile-handled-exceptions-ui.png",
      "url": "https://docs.newrelic.com/docs/mobile-monitoring/mobile-monitoring-ui/crashes/handled-exceptions-analyze-trends-prevent-crashes/",
      "published_at": "2021-10-18T14:50:51Z",
      "updated_at": "2021-07-21T21:33:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Handled exceptions help you identify significant factors contributing to poor mobile application experience, and use filterable data to find a resolution more quickly. You can also use the handled exceptions API to customize the data you send, and use NRQL to query and share the data. Handled exceptions workflow To get the most out of the Handled exceptions UI, use this basic workflow: Go to one.newrelic.com > Mobile > (select an app) > Exceptions > Handled exceptions. Use any of New Relic's standard page functions to drill down into detailed information; for example, zoom into any area of a chart. Look for obvious or general trends in the Users affected and Sessions affected percentage charts. Adjust the types of exceptions shown by using groups and filters. Optional: Query or share the chart data. Look for similar patterns where exceptions appear in stack traces with the Top 5 exception locations table. To view stack trace thread details for each occurrence of the exception, select a record from the Top 5 exceptions location table. one.newrelic.com > Mobile > (select an app) > Exceptions > Handled exceptions: As you explore the wealth of data in the charts and table, use groups and filters to discover patterns that help you determine the root cause of mobile app exceptions. Exception percentage charts Start with the Users affected and Sessions affected percentage charts to see at a glance whether there are any unexpected spikes, dips, or patterns with exceptions in general. (If the Users affected chart is empty, there were no user sessions during the selected time period.) For example: Are there any spikes near a recent version release? Is there a time period when the percentage of users has been affected significantly by the exception? Are there uneventful periods? To examine data in greater detail: Below any chart, select Expand chart. Exception percentage charts example one.newrelic.com > Mobile > (select an app) > Exceptions > Handled exceptions: The percentage charts help you quickly see any unexpected spikes, dips, or patterns with exceptions in general. Groups and filters Use the groups and filters to examine attributes for crashes, devices, locations, or other custom attributes in more detail. You can select a group, then filter to specific data. For example: Group the list by exception location (default), cause, app build or version, devices, connections, or other custom attributes. This lets you discover patterns in your exceptions to determine the root cause. Use the time picker to adjust the currently selected time period. Filter by a specific Version or by one or more attribute Filter, such as appVersion, exceptionLocationMethod, lastInteraction, or any of the longer list of standard and custom attributes. The currently selected filters appear at the top of the UI page. You can close them, add other filters, or select other groups and filters. Groups and filters example one.newrelic.com > Mobile > (select an app) > Exceptions > Handled exceptions: Group the data by attributes that matter to the most to you, then select one or more filters to help pinpoint specific causes behind the exceptions. Top five exception locations Use the Top 5 exception locations table to find or sort patterns in the type of exception you selected from the groups and filters. This includes: Recurring locations in the stack trace Mobile app version Number of occurrences Number of users affected during the selected time period For example, you can group by Exception Message, filter to timeout message, then select individual timeout locations from the table to review the stack trace thread and details about each occurrence. To filter or group by other attributes, use the table's search window, or select any of the available filters. For example, filter by type of occurrence, device, a specific location, or any custom attributes. To look for other historical patterns, change the selected time period. Top five exception locations example one.newrelic.com > Mobile > (select an app) > Exceptions > Handled exceptions: This example shows the Expand chart button and links to the query builder, where you can query, create dashboards, and share the handled exceptions data. Query builder links Handled exceptions charts use default attributes for mobile events (including MobileHandledException), along with any custom attributes you have added to this event type. When you mouse over the charts, direct links appear below them. These links to the query builder allow you to analyze your mobile app data even deeper. View query link: View the NRQL query used to calculate the chart data. View in query builder link: View the chart, and share it with others. Exception locations table The Exception locations table supplements the charts. It lists where the top five handled exceptions appear in their stack trace thread, and links them to relevant details. Each row helps you find answers to questions such as: How many of this exception occurred within the selected time period? Does a specific app version have a higher (or lower) number of users affected? Which exception has the fewest number of occurrences? You can change the sort order or filter options to focus on just the types of exceptions that matter the most to you and your teams. To view additional thread details for each occurrence of the exception, select a record from the Top 5 exceptions location table. Exception locations table example one.newrelic.com > Mobile > (select an app) > Exceptions > Handled exceptions: To continue to the handled exception's Occurrences page, select any row on the table.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.28369,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Handled exceptions: Analyze trends, prevent <em>crashes</em>",
        "sections": "Handled exceptions: Analyze trends, prevent <em>crashes</em>",
        "tags": "<em>Mobile</em> <em>monitoring</em> <em>UI</em>",
        "body": " exceptions workflow To get the most out of the Handled exceptions <em>UI</em>, use this basic workflow: Go to one.newrelic.com &gt; <em>Mobile</em> &gt; (select an app) &gt; Exceptions &gt; Handled exceptions. Use any of New Relic&#x27;s standard page functions to drill down into detailed information; for example, zoom into any area"
      },
      "id": "604505ae28ccbc783e2c6085"
    }
  ],
  "/docs/mobile-monitoring/mobile-monitoring-ui/crashes/handled-exceptions-analyze-trends-prevent-crashes": [
    {
      "sections": [
        "Mobile crash event trail",
        "Tip",
        "View events before mobile app crashes",
        "Use the event trail",
        "Difference between event trail and interaction trail"
      ],
      "title": "Mobile crash event trail",
      "type": "docs",
      "tags": [
        "Mobile monitoring",
        "Mobile monitoring UI",
        "Crashes"
      ],
      "external_id": "58795d499c8db6d8cc95bd6d2c645e970ea10d83",
      "image": "https://docs.newrelic.com/static/3731efca2d88ed92cc150d5f6c06830a/0d6fe/New-Relic-Mobile-crash-event-trail.png",
      "url": "https://docs.newrelic.com/docs/mobile-monitoring/mobile-monitoring-ui/crashes/mobile-crash-event-trail/",
      "published_at": "2021-10-18T04:36:19Z",
      "updated_at": "2021-07-22T01:05:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The mobile monitoring crash event trail shows you the events leading up to a crash of a mobile app, based on your subscription level's data retention policy. These can be events New Relic monitors by default, or custom events. This document explains what the crash event trail is and how to use it. Tip Access to this feature depends on your subscription level. View events before mobile app crashes When a mobile app crashes and you don't know why, you can study what happened right before the crash. The crash event trail shows you these events so that you can follow the \"breadcrumbs\" leading up to the crash and diagnose the cause of the failure. one.newrelic.com > Mobile > (select a mobile app) > Crash analysis > (selected crash type) > Event trail: The crash event trail shows the activity leading up to a mobile app crash. The crash event trail shows all mobile event types leading up to a crash. You can use the iOS SDK or Android SDK to create custom MobileBreadcrumb events that track whatever app activity you think would help you diagnose a crash. You can also use MobileHandledException events in the crash event trail to aid in debugging. Use the iOS and Android recordHandledException APIs for iOS or Android to annotate where exceptions are handled in your application. These events will automatically appear in the crash event trail. For more about annotating crash event trails with custom data, see Add custom data to mobile monitoring. Use the event trail To use the crash event trail: Go to one.newrelic.com > Mobile > (select a mobile app) > Crash analysis. On the lower right side of the Crash analysis page, select a crash type. On the Crash details page, beside the stack trace, select Event trail. Study the events leading up to a crash type for clues to the reasons for the crash. To expand details about an event's attributes, select it. To view the event trail results in New Relic, select Open session in Insights. To scroll through occurrences of the same crash type, use the event trail's left and right arrows. To make the most out of our crash analysis tools, use: The Android SDK API or iOS SDK API to create custom MobileBreadcrumb or MobileHandledException events Enable MobileRequest events Crash analysis page Interaction trail Difference between event trail and interaction trail The crash event trail is different from the interaction trail. The crash event trail shows all mobile event types leading up to a crash, whereas the interaction trail only shows interaction event types (Mobile events with the category interaction). The interaction trail has additional features, including stack traces and links to the associated interaction charts.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.30879,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Mobile</em> <em>crash</em> event trail",
        "sections": "View events before <em>mobile</em> app <em>crashes</em>",
        "tags": "<em>Mobile</em> <em>monitoring</em> <em>UI</em>",
        "body": "The <em>mobile</em> <em>monitoring</em> <em>crash</em> event trail shows you the events leading up to a <em>crash</em> of a <em>mobile</em> app, based on your subscription level&#x27;s data retention policy. These can be events New Relic monitors by default, or custom events. This document explains what the <em>crash</em> event trail is and how to use"
      },
      "id": "604503b1e7b9d201e75799bb"
    },
    {
      "sections": [
        "Introduction to mobile handled exceptions",
        "Features",
        "Requirements",
        "Tip",
        "Handled exceptions API and event type"
      ],
      "title": "Introduction to mobile handled exceptions",
      "type": "docs",
      "tags": [
        "Mobile monitoring",
        "Mobile monitoring UI",
        "Crashes"
      ],
      "external_id": "cfdf733d55bb89d157df675fa162b737bdad52c7",
      "image": "https://docs.newrelic.com/static/5891a9437b94b543d81ee04a70ebe876/8c557/mobile-handled-exceptions-ui.png",
      "url": "https://docs.newrelic.com/docs/mobile-monitoring/mobile-monitoring-ui/crashes/introduction-mobile-handled-exceptions/",
      "published_at": "2021-10-18T04:35:15Z",
      "updated_at": "2021-07-22T00:02:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Exceptions can contribute to invalid application states, resulting not only in application crashes but also in negative user reviews. This may lead to users deleting your app, which in turn may affect your organization's profitability. With the Handled exceptions UI, mobile development managers and their developer teams can identify significant factors affecting poor mobile app experience, and use filterable data to find a resolution more quickly. Features Handling exceptions as they occur can help improve your mobile app users' experience, but it's not enough to catch exceptions. You also need to know how to prevent them. For example: How many different types of handled exceptions are occurring? A high occurrence rate may necessitate changes to the back-end systems. Why does the user's app usage result in a try/catch? What is the context for the exceptions? When can a test environment's responses to handled exceptions indicate additional, more serious problems? What would have caused a crash if the exception had not been caught in production? What else (in the code or back-end API) is still affecting the users' experience? By using handled exceptions, you can identify and resolve these kinds of issues more quickly. one.newrelic.com > Mobile > (select an app) > Exceptions > Handled exceptions: As you explore the wealth of data in the charts and table, use groups and filters to discover patterns that help you determine the root cause of mobile app exceptions. Handled exception features Comments Slice and dice your exception data. You can view exception data via API as well as the UI: Use the recordHandledException() method within a try{...} catch(){...} block to help understand how often your application is throwing exceptions, and under what conditions. Use groups and filters to analyze trends leading to the exception. For example, you can group by OS Build, then filter a specific appVersion. Understand a particular user's experience. Examine the percentage charts to see overall trends with users and sessions at a glance. Then, use custom attributes to focus on exceptions related to paid accounts than free accounts. Pinpoint when most exceptions occur. For example, group on Last Interaction to get an overall view of problems. To drill down further, use filters, such as: To examine exceptions caused by network problems, filter by carrier and then select wifi. To examine exceptions caused by app releases, filter by appVersion. Align issues with common characteristics. For example: Use groups and filters to determine whether handled exception trends appear in networks (ASN, carrier, location, etc.) or in devices (device model, manufacturer, operating system build, version, etc.). Explore recurring patterns in stack traces with the top five exception locations table. Query data and share your findings. Explore the event trail before and after a crash. With mobile monitoring you get a more complete picture of events before and after crashes occur, so you can analyze and resolve problems from multiple angles: Use the handled exception's Occurrences page for expected exceptions. Use the Crash analysis UI and event trail for unanticipated exceptions. Requirements Tip Access to this feature depends on your subscription level and mobile data retention. Additional requirements include: Android: Android agent version 5.15.0 or higher iOS: iOS agent version 5.15.0 or higher Handled exceptions API and event type Mobile monitoring automatically includes default attributes that you can use to explore your handled exceptions data in the query builder and get specific details: Use the recordHandledExceptions() method for the Android or iOS SDK API. Query the MobileHandledException event type. For more information, see the NRQL examples for mobile monitoring. You can also create your own custom attributes and events. Then, select attributes in the Handled exceptions page, and query or share them.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.30135,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>mobile</em> handled exceptions",
        "sections": "Introduction to <em>mobile</em> handled exceptions",
        "tags": "<em>Mobile</em> <em>monitoring</em> <em>UI</em>",
        "body": " system build, version, etc.). Explore recurring patterns in stack traces with the top five exception locations table. Query data and share your findings. Explore the event trail before and after a <em>crash</em>. With <em>mobile</em> <em>monitoring</em> you get a more complete picture of events before and after <em>crashes</em> occur, so"
      },
      "id": "603e7e8228ccbc05f9eba770"
    },
    {
      "sections": [
        "Crash analysis: Group and filter your crashes",
        "Use crash analysis features",
        "View the crash list",
        "Use the crash analysis workflow",
        "Crash percentages charts: See patterns immediately.",
        "Groups and filters: Slice and dice the crash data.",
        "Top five occurrences: Analyze specific characteristics.",
        "Queries and image links: Query crash data and share charts with others.",
        "Crash reports: Dive into the interaction trail, thread breakdown, and more.",
        "Crash profiles: Quickly see key differences between crashed and crash-free accounts.",
        "Crash event trail: See all the events leading up to a crash.",
        "Examine crash report details: Export to Xcode or resymbolicate your source code.",
        "Configure crash report email settings"
      ],
      "title": "Crash analysis: Group and filter your crashes",
      "type": "docs",
      "tags": [
        "Mobile monitoring",
        "Mobile monitoring UI",
        "Crashes"
      ],
      "external_id": "a3285a660dac893ee50bf03412a7f86cbf5d7b9a",
      "image": "https://docs.newrelic.com/static/4be0f954c4306f12ad168124f9241f6f/8c557/d0930crash-analytics.png",
      "url": "https://docs.newrelic.com/docs/mobile-monitoring/mobile-monitoring-ui/crashes/crash-analysis-group-filter-your-crashes/",
      "published_at": "2021-10-18T14:49:57Z",
      "updated_at": "2021-07-09T12:26:28Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Mobile app crashes can result in users submitting negative reviews, which can affect your organization's reputation. Crashes can also result in users deleting your app altogether, which affects your organization's profitability. With the Crash analysis UI, you can identify and deal with one of the largest bottlenecks in the development lifecycle: the time it takes to resolve unanticipated exceptions. Use crash analysis features When your development team receives an emailed crash notification, they can select the email link to review detailed information about the crash in our user interface, or analyze chart data directly through a query. The individual Crash details page also includes tracking tools for your team to follow the issue from reporting to resolution. Our crash analysis feature helps developers to: Slice and dice your crash and network data (using groups and filters) to analyze trends leading to the crash. Understand a particular user's experience by using a custom attribute. Pinpoint when most crashes occur; for example, by filtering by Last Interaction, or by viewing the interaction trail in the Crash report page. Align issues with common characteristics in networks or devices. Examine specific problems in a crash report that may be buried in thread breakdown data, trace details, or the trail of user interactions in the crashed session. Quickly see trends from the Crash profiles tab. View the crash list To view the filterable Crash list in the New Relic UI: Go to one.newrelic.com > Mobile > (select an app) > Exceptions > Crash analysis. OR To view the filterable Crash analysis page directly from the mobile app's Overview page, select the Crash occurrences chart's title. Use any of our standard page functions to drill down into detailed information; for example, zoom into any area of a chart. Use the crash analysis workflow To get the most out of mobile monitoring's Crash analysis UI, use this basic workflow. Start with the Crash rate percentage and Crash-free users percentage charts to see at a glance whether there are any unexpected spikes, dips, or patterns with crash time frames, or uneventful periods within the selected time period. Use the groups and filters to examine attributes (for crashes, devices, locations, or other custom attributes) in more detail. Use the Top 5 occurrences chart to identify patterns for the type of occurrence you selected from the groups and filters. Also correlate any general patterns for the selected type of occurrence to crash percentages that occurred during this time period. Optional: Query or share the chart data. Look for patterns in the Crash location table information, including location, exception type, date and number of occurrences, version, and number of users affected by the crash. Select a crash report from the table to view its interaction trail, its event trail, thread details for individual occurrences, attributes, and more. If necessary, resymbolicate or export the crash details to Xcode so you can debug your source code more easily. When finished, mark the crash occurrences as Resolved. Resolved crashes include a banner identifying who resolved the crash and when. Based on mobile monitoring's data retention policies, you can filter by resolved crashes when you need to track back to historical information. Crash percentages charts: See patterns immediately. Start with the Crash rate percentage and Crash-free users percentage charts to see at a glance whether there are any unexpected spikes, dips, or patterns with crashes in general. For example: Are there any spikes near a recent version release? Is there a time period when the percentage of users has been affected significantly by the crash? (If the Crash-free users percentage chart is empty, there were no user sessions during the selected time period.) Groups and filters: Slice and dice the crash data. You can select a group, and then filter to specific data, including: Group the crash list by type of crash, device, location, or other custom attributes. Show open crashes, resolved crashes, or all crashes. Use the time picker to adjust the currently selected time period. Filter by a specific Version or by one or more attribute Filter, such as Last Interaction, App Build, or any of the longer list of standard and custom attributes). The selected filters appear at the top of the UI page. Top five occurrences: Analyze specific characteristics. Use the Top 5 occurrences chart to identify what types of crashes and how many occurred during the selected time period. This chart shows the top five crash occurrences by the group and filters you select. For example, you can group by email address (if instrumented as a custom attribute), then filter down to a specific email to examine that user's experience. To filter or group by other attributes, use the search window, or select any of the available filters. For example, filter by type of crash, device, a specific location, or any custom attributes. To look for other historical patterns, change the selected time period. one.newrelic.com > Mobile > (select an app) > Exceptions > Crash analysis: Here is an example of the Top 5 occurrences chart filtered by the Crash locations group. Select any groups or filters to analyze your crash data any way you want. Queries and image links: Query crash data and share charts with others. Mobile monitoring's Crash analysis charts use default attributes for mobile events, along with any custom attributes you have added to this event type. To view or share the data, click the ellipsis icon. Add to dashboard link: View the chart, and copy it to a new or existing dashboard. View query link: View the NRQL query used to calculate the chart data. Get as image link: Select this option to get a public URL of the chart, then share it using any media. Crash reports: Dive into the interaction trail, thread breakdown, and more. Supplementing the charts, the Crash report table lists crash types by location, and links them to relevant crash report details. Each row helps you find answers to questions such as: How many of this crash type or exception occurred within the selected time period? What is the most recent exception message? Does a specific app version have a higher (or lower) number of users affected? When did the exception first and last occur? one.newrelic.com > Mobile > (select an app) > Exceptions > Crash analysis: Sort on any column to analyze patterns in the Crash reports table, then select any report to review crash details. Sometimes it may be more useful to examine crash report data from lowest to highest. For example: Which exception has the fewest number of occurrences? When did a particular exception start (Earliest occurrence)? You can change the sort order or filter options to focus on just the types of exceptions that matter the most to you and your teams. Crash profiles: Quickly see key differences between crashed and crash-free accounts. Crash profiles provide quick, clear insight into your mobile crash data by surfacing any anomalies in your mobile application’s performance. Unexpected differences between attributes and behaviors are highlighted, illuminating key differences between crashed and crash-free accounts. You can quickly pinpoint key issues through drill-downs and filters for a fast, streamlined troubleshooting experience. one.newrelic.com > Mobile > (select an app) > Exceptions> Crash analysis > Crash profiles: Compare crashed and crash-free sessions. Crash event trail: See all the events leading up to a crash. The mobile monitoring crash event trail shows you the events leading up to a crash of a mobile app. These can be events monitored by default, or custom events you've created. For more information, see Crash event trail. Examine crash report details: Export to Xcode or resymbolicate your source code. From the Crash report table, you can drill down into a specific Crash report. From here you can: Examine the interaction trail from session start through the crash event. Explore the related thread breakdown. Select Export crash details so you can examine source code using Xcode. Resymbolicate the crash report for easier debugging. File a ticket, and resolve the crash. one.newrelic.com > Mobile > (select an app) > Crashes > Crash report: This is an example of an interaction trail that includes the option to resymbolicate the crash occurrence. To analyze and debug your source code using Xcode, select Export crash details. Configure crash report email settings To learn how to configure email settings, see Email settings.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 165.40707,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Crash</em> analysis: Group and filter your <em>crashes</em>",
        "sections": "<em>Crash</em> analysis: Group and filter your <em>crashes</em>",
        "tags": "<em>Mobile</em> <em>monitoring</em> <em>UI</em>",
        "body": "<em>Mobile</em> app <em>crashes</em> can result in users submitting negative reviews, which can affect your organization&#x27;s reputation. <em>Crashes</em> can also result in users deleting your app altogether, which affects your organization&#x27;s profitability. With the <em>Crash</em> analysis <em>UI</em>, you can identify and deal with one"
      },
      "id": "60441616196a67b634960f1f"
    }
  ]
}