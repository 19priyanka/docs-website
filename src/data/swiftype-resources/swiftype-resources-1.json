{
  "/docs/accounts/accounts/account-maintenance/account-email-settings": [
    {
      "sections": [
        "Introduction to account settings",
        "Pricing, billing, and usage UI",
        "Manage users",
        "Add accounts",
        "Other account settings"
      ],
      "title": "Introduction to account settings",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "General account settings"
      ],
      "external_id": "1dbac104fa7e60637c32a61a80b8b709f6fd84c2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/general-account-settings/introduction-account-settings/",
      "published_at": "2021-12-19T13:41:30Z",
      "updated_at": "2021-11-13T02:21:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To view and edit account-related and user-management-related settings, go to one.newrelic.com and in the top right corner, select the account dropdown. Keep reading for tips on how to find different kinds of account settings in the UI. Pricing, billing, and usage UI Note that user permissions may impact your access to some account settings. To find billing-related UI, go to one.newrelic.com and, in the upper right corner, click the account dropdown to access various account settings. Here's what you can find there: Manage your plan: Options for viewing and managing billing-related settings. Manage your data: Options for managing ingest of data, which can be a billing factor. Learn more about managing data. View your usage: View of your billing-related usage. Administration: Options related to billing, usage, and user management (this UI available only for users on our New Relic One user model). Account settings: Options related to subscription and usage, and user management (this UI available only to users on our original user model). Related topics: Pricing details: Original pricing model | New Relic One pricing Manage data Manage users How you manage users depends on which user model you're on: New Relic One user model: see Manage users. Original user model: from one.newrelic.com, click the account dropdown, click Account settings, and then click Users and roles. For more on user management, see Original users. Not sure which user model you're on? See Determine user model. Add accounts Learn more about adding accounts. Other account settings See the account settings docs.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.82368,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>account</em> <em>settings</em>",
        "sections": "Introduction to <em>account</em> <em>settings</em>",
        "tags": "<em>General</em> <em>account</em> <em>settings</em>",
        "body": "To view and edit <em>account</em>-related and user-management-related <em>settings</em>, go to one.newrelic.com and in the top right corner, select the <em>account</em> dropdown. Keep reading for tips on how to find different kinds of <em>account</em> <em>settings</em> in the UI. Pricing, <em>billing</em>, and usage UI Note that user permissions may"
      },
      "id": "6043f38a196a679ae4960f5e"
    },
    {
      "sections": [
        "Non-profits and New Relic",
        "Exceptions",
        "Tip",
        "Additional requirements",
        "Signup procedures",
        "Program Benefits"
      ],
      "title": "Non-profits and New Relic",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "General account settings"
      ],
      "external_id": "441757ebbd6d8039278355701793bcbcae0fbc8a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/new-relic-one-pricing-billing/eligibility-guidelines-new-relic-nonprofit-program/",
      "published_at": "2021-12-19T17:31:50Z",
      "updated_at": "2021-11-06T19:59:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document provides New Relic's Observability for Good program eligibility guidelines for nonprofit, charitable, and NGO organizations. Exceptions Many global nonprofit, charity and NGO organizations are eligible for Observability for Good if the organization has a recognized legal status in their respective country equal to 501(c)(3) status under the United States Internal Revenue Code, with certain exceptions. In addition, all organizations must be verified by TechSoup or the local TechSoup partner. Examples of ineligible organizations include, but are not limited to: Ineligible nonprofits Comments Higher education institutions Private nonprofit and public higher education institutions do not qualify. This includes: Universities Colleges Trade schools Primary and secondary schools (public and private) Primary and secondary schools do not qualify. This includes, but is not limited to: K-12 public school districts Individual K-12 public schools within a state or federally funded school district Standalone K-12 charter schools K-12 Knowledge is Power Program (KiPP) schools Other K-12 schools that are not part of a state or federally funded school district Healthcare organizations providing patient care This includes hospitals, hospital auxiliaries, healthcare systems, and related health services organizations, such as: Nursing or convalescent homes Care and housing for the aged Pregnancy centers Tip Free clinics may qualify. Professional, commerce, mutual, and trade organizations This includes organizations such as: Credit Unions Regulation of business Industry trade shows Professional athletic leagues Tourist bureaus Employee or membership benefit organizations This includes organizations such as: Fraternal Beneficiary societies Associations of employees Employee or member welfare associations Pension and retirement benefits Tip Organizations focusing on the improvement of working conditions may qualify. Legislative or political organizations and advocacy groups Organizations focused on nonpartisan voter education may qualify. Organizations within countries sanctioned by the US This includes organizations within any sanctioned countries included on the US Department of Treasury's Office of Foreign Assets control list , which is updated periodically. Additional requirements In order to participate, approved organizations must also: Submit an application through newrelic.org/signup. Complete the eligibility validation process through TechSoup. Tip Learn about TechSoups local NGO definitions. Accept New Relic's standard terms of service. Be a direct customer of New Relic and have a direct billing relationship with New Relic. Signup procedures To learn more or to sign up as a new or existing New Relic customer, go to newrelic.org/signup. Program Benefits Observability for Good Standard: 4 additional free users per month (+1 free edition user: 5 total) 900 additional gb of data (+100 gb free edition: 1TB total) Observability for Good Pro: 2 free users per month 1 free TB data per month Tip A payment method on file is required for accounts using Observability for Good Pro.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 222.1981,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Non-profits <em>and</em> New Relic",
        "sections": "Non-profits <em>and</em> New Relic",
        "tags": "<em>General</em> <em>account</em> <em>settings</em>",
        "body": " through newrelic.org&#x2F;signup. Complete the eligibility validation process through TechSoup. Tip Learn about TechSoups local NGO definitions. Accept New Relic&#x27;s standard terms of service. Be a direct customer of New Relic and have a direct <em>billing</em> relationship with New Relic. Signup procedures To learn"
      },
      "id": "6186de8c196a678bfdf4378b"
    },
    {
      "sections": [
        "Set or change password",
        "Change your password",
        "Important",
        "Reset password"
      ],
      "title": "Set or change password",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "General account settings"
      ],
      "external_id": "817d8d17dca7f361429109d8364eff931a1cb392",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/account-maintenance/change-passwords-user-preferences/",
      "published_at": "2021-12-19T15:39:09Z",
      "updated_at": "2021-10-13T03:31:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document contains information on New Relic password requirements. Change your password New Relic account passwords don't expire. However, users can change their own password and other personal account information anytime. Your ability to change your password from the UI may depend on your user and organization level settings. To change your password from the UI: From one.newrelic.com, click the account dropdown and select User preferences. When creating a password, adhere to the password requirements below. Your password must meet these minimum requirements: 8 to 50 characters in length At least one letter (a-z, A-Z) At least one number (0-9), special character, or space Special characters allowed include ~ ` ! @ # $ % ^ & * ( ) _ - + = { [ } ] : ; \" ' < , > . ? / | \\. Spaces are also allowed. Important In addition to New Relic's requirements, follow your organization's guidelines for password length, use of upper or lower case letters, numbers, and special characters. Reset password If you forgot your own password or need to request a password reset, you can use New Relic's self-service options. Admins can't reset passwords for other users. If you need to reset someone else's password, get support at support.newrelic.com.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 209.1585,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Set</em> or change password",
        "sections": "<em>Set</em> or change password",
        "tags": "<em>General</em> <em>account</em> <em>settings</em>",
        "body": " and organization level <em>settings</em>. To change your password from the UI: From one.newrelic.com, click the <em>account</em> dropdown and select User preferences. When creating a password, adhere to the password requirements below. Your password must meet these minimum requirements: 8 to 50 characters in length"
      },
      "id": "603eb55a64441f9f8f4e889c"
    }
  ],
  "/docs/accounts/accounts/account-maintenance/change-passwords-user-preferences": [
    {
      "sections": [
        "Introduction to account settings",
        "Pricing, billing, and usage UI",
        "Manage users",
        "Add accounts",
        "Other account settings"
      ],
      "title": "Introduction to account settings",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "General account settings"
      ],
      "external_id": "1dbac104fa7e60637c32a61a80b8b709f6fd84c2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/general-account-settings/introduction-account-settings/",
      "published_at": "2021-12-19T13:41:30Z",
      "updated_at": "2021-11-13T02:21:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To view and edit account-related and user-management-related settings, go to one.newrelic.com and in the top right corner, select the account dropdown. Keep reading for tips on how to find different kinds of account settings in the UI. Pricing, billing, and usage UI Note that user permissions may impact your access to some account settings. To find billing-related UI, go to one.newrelic.com and, in the upper right corner, click the account dropdown to access various account settings. Here's what you can find there: Manage your plan: Options for viewing and managing billing-related settings. Manage your data: Options for managing ingest of data, which can be a billing factor. Learn more about managing data. View your usage: View of your billing-related usage. Administration: Options related to billing, usage, and user management (this UI available only for users on our New Relic One user model). Account settings: Options related to subscription and usage, and user management (this UI available only to users on our original user model). Related topics: Pricing details: Original pricing model | New Relic One pricing Manage data Manage users How you manage users depends on which user model you're on: New Relic One user model: see Manage users. Original user model: from one.newrelic.com, click the account dropdown, click Account settings, and then click Users and roles. For more on user management, see Original users. Not sure which user model you're on? See Determine user model. Add accounts Learn more about adding accounts. Other account settings See the account settings docs.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.82367,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>account</em> <em>settings</em>",
        "sections": "Introduction to <em>account</em> <em>settings</em>",
        "tags": "<em>General</em> <em>account</em> <em>settings</em>",
        "body": "To view and edit <em>account</em>-related and user-management-related <em>settings</em>, go to one.newrelic.com and in the top right corner, select the <em>account</em> dropdown. Keep reading for tips on how to find different kinds of <em>account</em> <em>settings</em> in the UI. Pricing, <em>billing</em>, and usage UI Note that user permissions may"
      },
      "id": "6043f38a196a679ae4960f5e"
    },
    {
      "sections": [
        "Non-profits and New Relic",
        "Exceptions",
        "Tip",
        "Additional requirements",
        "Signup procedures",
        "Program Benefits"
      ],
      "title": "Non-profits and New Relic",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "General account settings"
      ],
      "external_id": "441757ebbd6d8039278355701793bcbcae0fbc8a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/new-relic-one-pricing-billing/eligibility-guidelines-new-relic-nonprofit-program/",
      "published_at": "2021-12-19T17:31:50Z",
      "updated_at": "2021-11-06T19:59:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document provides New Relic's Observability for Good program eligibility guidelines for nonprofit, charitable, and NGO organizations. Exceptions Many global nonprofit, charity and NGO organizations are eligible for Observability for Good if the organization has a recognized legal status in their respective country equal to 501(c)(3) status under the United States Internal Revenue Code, with certain exceptions. In addition, all organizations must be verified by TechSoup or the local TechSoup partner. Examples of ineligible organizations include, but are not limited to: Ineligible nonprofits Comments Higher education institutions Private nonprofit and public higher education institutions do not qualify. This includes: Universities Colleges Trade schools Primary and secondary schools (public and private) Primary and secondary schools do not qualify. This includes, but is not limited to: K-12 public school districts Individual K-12 public schools within a state or federally funded school district Standalone K-12 charter schools K-12 Knowledge is Power Program (KiPP) schools Other K-12 schools that are not part of a state or federally funded school district Healthcare organizations providing patient care This includes hospitals, hospital auxiliaries, healthcare systems, and related health services organizations, such as: Nursing or convalescent homes Care and housing for the aged Pregnancy centers Tip Free clinics may qualify. Professional, commerce, mutual, and trade organizations This includes organizations such as: Credit Unions Regulation of business Industry trade shows Professional athletic leagues Tourist bureaus Employee or membership benefit organizations This includes organizations such as: Fraternal Beneficiary societies Associations of employees Employee or member welfare associations Pension and retirement benefits Tip Organizations focusing on the improvement of working conditions may qualify. Legislative or political organizations and advocacy groups Organizations focused on nonpartisan voter education may qualify. Organizations within countries sanctioned by the US This includes organizations within any sanctioned countries included on the US Department of Treasury's Office of Foreign Assets control list , which is updated periodically. Additional requirements In order to participate, approved organizations must also: Submit an application through newrelic.org/signup. Complete the eligibility validation process through TechSoup. Tip Learn about TechSoups local NGO definitions. Accept New Relic's standard terms of service. Be a direct customer of New Relic and have a direct billing relationship with New Relic. Signup procedures To learn more or to sign up as a new or existing New Relic customer, go to newrelic.org/signup. Program Benefits Observability for Good Standard: 4 additional free users per month (+1 free edition user: 5 total) 900 additional gb of data (+100 gb free edition: 1TB total) Observability for Good Pro: 2 free users per month 1 free TB data per month Tip A payment method on file is required for accounts using Observability for Good Pro.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 222.1981,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Non-profits <em>and</em> New Relic",
        "sections": "Non-profits <em>and</em> New Relic",
        "tags": "<em>General</em> <em>account</em> <em>settings</em>",
        "body": " through newrelic.org&#x2F;signup. Complete the eligibility validation process through TechSoup. Tip Learn about TechSoups local NGO definitions. Accept New Relic&#x27;s standard terms of service. Be a direct customer of New Relic and have a direct <em>billing</em> relationship with New Relic. Signup procedures To learn"
      },
      "id": "6186de8c196a678bfdf4378b"
    },
    {
      "sections": [
        "Change account or user name",
        "Change user name",
        "Change account name",
        "Important"
      ],
      "title": "Change account or user name",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "General account settings"
      ],
      "external_id": "328b3ae3afa62555d246baee53ec49add7ad470c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/account-maintenance/change-your-new-relic-account-name/",
      "published_at": "2021-12-19T15:39:09Z",
      "updated_at": "2021-08-26T14:42:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Change user name To change your user name: From the account dropdown, select User preferences. On the right side, click Full name. Change account name Only users on our original user model are able to change their New Relic account name. To do this: From the account dropdown, select Account settings. On the right side, edit the Name field. Important If you're on our New Relic One user model and want to change your account name, contact your account representative or support.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 198.82697,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Change <em>account</em> or user name",
        "sections": "Change <em>account</em> or user name",
        "tags": "<em>General</em> <em>account</em> <em>settings</em>",
        "body": " <em>settings</em>. On the right side, edit the Name field. Important If you&#x27;re on our New Relic One user model and want to change your <em>account</em> name, contact your <em>account</em> representative or support."
      },
      "id": "6043ce92e7b9d215e55799e4"
    }
  ],
  "/docs/accounts/accounts/account-maintenance/change-your-new-relic-account-name": [
    {
      "sections": [
        "Introduction to account settings",
        "Pricing, billing, and usage UI",
        "Manage users",
        "Add accounts",
        "Other account settings"
      ],
      "title": "Introduction to account settings",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "General account settings"
      ],
      "external_id": "1dbac104fa7e60637c32a61a80b8b709f6fd84c2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/general-account-settings/introduction-account-settings/",
      "published_at": "2021-12-19T13:41:30Z",
      "updated_at": "2021-11-13T02:21:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To view and edit account-related and user-management-related settings, go to one.newrelic.com and in the top right corner, select the account dropdown. Keep reading for tips on how to find different kinds of account settings in the UI. Pricing, billing, and usage UI Note that user permissions may impact your access to some account settings. To find billing-related UI, go to one.newrelic.com and, in the upper right corner, click the account dropdown to access various account settings. Here's what you can find there: Manage your plan: Options for viewing and managing billing-related settings. Manage your data: Options for managing ingest of data, which can be a billing factor. Learn more about managing data. View your usage: View of your billing-related usage. Administration: Options related to billing, usage, and user management (this UI available only for users on our New Relic One user model). Account settings: Options related to subscription and usage, and user management (this UI available only to users on our original user model). Related topics: Pricing details: Original pricing model | New Relic One pricing Manage data Manage users How you manage users depends on which user model you're on: New Relic One user model: see Manage users. Original user model: from one.newrelic.com, click the account dropdown, click Account settings, and then click Users and roles. For more on user management, see Original users. Not sure which user model you're on? See Determine user model. Add accounts Learn more about adding accounts. Other account settings See the account settings docs.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.82367,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>account</em> <em>settings</em>",
        "sections": "Introduction to <em>account</em> <em>settings</em>",
        "tags": "<em>General</em> <em>account</em> <em>settings</em>",
        "body": "To view and edit <em>account</em>-related and user-management-related <em>settings</em>, go to one.newrelic.com and in the top right corner, select the <em>account</em> dropdown. Keep reading for tips on how to find different kinds of <em>account</em> <em>settings</em> in the UI. Pricing, <em>billing</em>, and usage UI Note that user permissions may"
      },
      "id": "6043f38a196a679ae4960f5e"
    },
    {
      "sections": [
        "Non-profits and New Relic",
        "Exceptions",
        "Tip",
        "Additional requirements",
        "Signup procedures",
        "Program Benefits"
      ],
      "title": "Non-profits and New Relic",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "General account settings"
      ],
      "external_id": "441757ebbd6d8039278355701793bcbcae0fbc8a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/new-relic-one-pricing-billing/eligibility-guidelines-new-relic-nonprofit-program/",
      "published_at": "2021-12-19T17:31:50Z",
      "updated_at": "2021-11-06T19:59:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document provides New Relic's Observability for Good program eligibility guidelines for nonprofit, charitable, and NGO organizations. Exceptions Many global nonprofit, charity and NGO organizations are eligible for Observability for Good if the organization has a recognized legal status in their respective country equal to 501(c)(3) status under the United States Internal Revenue Code, with certain exceptions. In addition, all organizations must be verified by TechSoup or the local TechSoup partner. Examples of ineligible organizations include, but are not limited to: Ineligible nonprofits Comments Higher education institutions Private nonprofit and public higher education institutions do not qualify. This includes: Universities Colleges Trade schools Primary and secondary schools (public and private) Primary and secondary schools do not qualify. This includes, but is not limited to: K-12 public school districts Individual K-12 public schools within a state or federally funded school district Standalone K-12 charter schools K-12 Knowledge is Power Program (KiPP) schools Other K-12 schools that are not part of a state or federally funded school district Healthcare organizations providing patient care This includes hospitals, hospital auxiliaries, healthcare systems, and related health services organizations, such as: Nursing or convalescent homes Care and housing for the aged Pregnancy centers Tip Free clinics may qualify. Professional, commerce, mutual, and trade organizations This includes organizations such as: Credit Unions Regulation of business Industry trade shows Professional athletic leagues Tourist bureaus Employee or membership benefit organizations This includes organizations such as: Fraternal Beneficiary societies Associations of employees Employee or member welfare associations Pension and retirement benefits Tip Organizations focusing on the improvement of working conditions may qualify. Legislative or political organizations and advocacy groups Organizations focused on nonpartisan voter education may qualify. Organizations within countries sanctioned by the US This includes organizations within any sanctioned countries included on the US Department of Treasury's Office of Foreign Assets control list , which is updated periodically. Additional requirements In order to participate, approved organizations must also: Submit an application through newrelic.org/signup. Complete the eligibility validation process through TechSoup. Tip Learn about TechSoups local NGO definitions. Accept New Relic's standard terms of service. Be a direct customer of New Relic and have a direct billing relationship with New Relic. Signup procedures To learn more or to sign up as a new or existing New Relic customer, go to newrelic.org/signup. Program Benefits Observability for Good Standard: 4 additional free users per month (+1 free edition user: 5 total) 900 additional gb of data (+100 gb free edition: 1TB total) Observability for Good Pro: 2 free users per month 1 free TB data per month Tip A payment method on file is required for accounts using Observability for Good Pro.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 222.1981,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Non-profits <em>and</em> New Relic",
        "sections": "Non-profits <em>and</em> New Relic",
        "tags": "<em>General</em> <em>account</em> <em>settings</em>",
        "body": " through newrelic.org&#x2F;signup. Complete the eligibility validation process through TechSoup. Tip Learn about TechSoups local NGO definitions. Accept New Relic&#x27;s standard terms of service. Be a direct customer of New Relic and have a direct <em>billing</em> relationship with New Relic. Signup procedures To learn"
      },
      "id": "6186de8c196a678bfdf4378b"
    },
    {
      "sections": [
        "Set or change password",
        "Change your password",
        "Important",
        "Reset password"
      ],
      "title": "Set or change password",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "General account settings"
      ],
      "external_id": "817d8d17dca7f361429109d8364eff931a1cb392",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/account-maintenance/change-passwords-user-preferences/",
      "published_at": "2021-12-19T15:39:09Z",
      "updated_at": "2021-10-13T03:31:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document contains information on New Relic password requirements. Change your password New Relic account passwords don't expire. However, users can change their own password and other personal account information anytime. Your ability to change your password from the UI may depend on your user and organization level settings. To change your password from the UI: From one.newrelic.com, click the account dropdown and select User preferences. When creating a password, adhere to the password requirements below. Your password must meet these minimum requirements: 8 to 50 characters in length At least one letter (a-z, A-Z) At least one number (0-9), special character, or space Special characters allowed include ~ ` ! @ # $ % ^ & * ( ) _ - + = { [ } ] : ; \" ' < , > . ? / | \\. Spaces are also allowed. Important In addition to New Relic's requirements, follow your organization's guidelines for password length, use of upper or lower case letters, numbers, and special characters. Reset password If you forgot your own password or need to request a password reset, you can use New Relic's self-service options. Admins can't reset passwords for other users. If you need to reset someone else's password, get support at support.newrelic.com.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 209.1585,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Set</em> or change password",
        "sections": "<em>Set</em> or change password",
        "tags": "<em>General</em> <em>account</em> <em>settings</em>",
        "body": " and organization level <em>settings</em>. To change your password from the UI: From one.newrelic.com, click the <em>account</em> dropdown and select User preferences. When creating a password, adhere to the password requirements below. Your password must meet these minimum requirements: 8 to 50 characters in length"
      },
      "id": "603eb55a64441f9f8f4e889c"
    }
  ],
  "/docs/accounts/accounts/account-maintenance/set-session-timeouts": [
    {
      "sections": [
        "Overview of changes to pricing and user model",
        "Overview of how pricing model and user model relate",
        "Pricing plans explained",
        "Determine pricing model",
        "Convert to new pricing",
        "User models explained",
        "Requirements for new user model",
        "Determine which user model you're on",
        "Feature impacts of user model",
        "Transition to new models"
      ],
      "title": "Overview of changes to pricing and user model",
      "type": "docs",
      "tags": [
        "Accounts",
        "Original accounts and billing",
        "Original product-based pricing"
      ],
      "external_id": "b19d61b5a0c65ca352e2ce0e260e2b53391b94fc",
      "image": "https://docs.newrelic.com/static/1461470cc1ef0be91120c84d76a7ccdd/c1b63/new-relic-one-pricing-plan-billing-ui.png",
      "url": "https://docs.newrelic.com/docs/accounts/original-accounts-billing/original-product-based-pricing/overview-changes-pricing-user-model/",
      "published_at": "2021-12-19T14:16:05Z",
      "updated_at": "2021-11-24T14:34:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In 2020, New Relic released both a newer pricing model and a newer user model. Keep reading to learn about: How the pricing model and the user model relate to each other Pricing plans explained User models explained How to switch to the new models Overview of how pricing model and user model relate In 2020, we released both a new, improved pricing model and a new, improved user model. These models represent the future. All new sign-ups are on these new models and eventually all organizations will be on these models. But currently, our customers from before July 2020 may have one of several combinations of these as they either switch to the new pricing or migrate their users to the new user model. This table shows how pricing and user model relate to each other: Pricing plan factors User model factors Original pricing If your organization was created before July 30 2020, you remain on our original pricing model until you transition to New Relic One pricing. For original pricing accounts, you have users on the original user model unless you've migrated your users to the new model. Note that if you've added users via automated user management, they're on the new model. This means it's possible to have some users on both models. New Relic One pricing An organization is on the New Relic One pricing model: If it was created on or after July 30 2020, or If it's an older organization but has switched to New Relic One pricing. If your organization was created on or after July 30 2020, your users are on the New Relic One user model. If you have an older organization, you have users on the original user model unless you've migrated your users to the new model. Pricing plans explained New Relic organizations are on one of two pricing models: New Relic One pricing: Our new pricing model is simpler. It bills on a) which edition you're on (Standard, Pro, or Enterprise), b) the GBs of data ingested, and c) how many billable full platform users you have. All organizations created on or after July 30 2020 are on this pricing model, as are older organizations that have switched to this pricing. There are two versions of this pricing model. Our original product-based pricing model: this is based on subscriptions to specific products (for example, APM, Browser, Mobile, Infrastructure). Some organizations have the option to transition to the newer pricing model: in that case, their users remain on our original user model. Determine pricing model To determine which pricing model you’re on: go to one.newrelic.com, select the account dropdown, and select Manage your plan. If you see billing information about data ingested and the number of billable users, you’re on the new pricing model. If you're on New Relic One pricing, you'll see a billing UI like this. Convert to new pricing Some organizations are able to switch to new pricing. Learn more about switching your pricing model. User models explained In this context, the term \"user model\" refers to the structure of a New Relic user record and how it interacts with the broader New Relic organization that it's in. Our two user models are: New Relic One user model: this newer, improved user model has these major differences from the original user model: All your accounts and users are contained under a top-level organization. This gives an organization stronger control over managing users and what they can access. One impact of this is that users who work with multiple New Relic organizations may have an email address associated with multiple logins. Users on this model have a different system for managing permissions and account access. Using access grants, you assign users access to specific roles on specific accounts. Original user model: some aspects of this older model that are different from our newer model: There wasn't as much organization-level control over users. For example, a New Relic user had the ability to access multiple accounts they'd been granted access to using a single login. Users on the original user model have a different user management system. To learn more about benefits of the new model, see our blog post about user model changes and an explanation of the new account structure. For impacts and limitations, see Feature impacts. Requirements for new user model Here's a synopsis of the factors governing what organizations use the new model: All New Relic organizations that signed up after July 30 2020 have users on this model, and also have the new pricing model. Some older New Relic organizations have had their users migrated to the new model by New Relic or by using the user migration procedure. Note that switching to the new pricing model is independent from migrating users. Partner accounts (resellers, managed service providers), and organizations using the partnership account structure, cannot yet migrate their users to the new model. Determine which user model you're on To determine what user model you're on, from the account dropdown select User preferences. If you see the UI below, you're on the New Relic One user model. If you see a different UI with more content displayed, you're on the original user model (see original user docs). To determine if you can manage users on the New Relic One user model, see Manage users. The user model is independent of your pricing model. For how user model relates to pricing, see the Pricing and user model table. Feature impacts of user model The new user model offers many benefits. Here are some feature impacts of being on the New Relic One user model that may be not obvious to users accustomed to our previous user model: More separation of organizations/accounts: If you have an email address associated with multiple New Relic accounts, this may mean you need to log out and log back in. Learn more about account access. Adding accounts: Pro and Enterprise edition can have more than one account in their organization. Currently users in these organizations cannot, on their own, add accounts. They must get help from their New Relic account representative. To learn about adding users to accounts, see Manage users. Alerts-related impacts: Policy actions: For alert policies and conditions, some UI locations display the user that made the last modification. For users on the new model, it will display their user ID and not their user name. For example, a policy might read Last modified Sep 7, '20 4:26 pm by user 1234567 instead of by John Doe. If you have user management permissions, you can determine what user this is by going to the User management UI. Notifications: Users on the new model won’t show up as an option when choosing users to receive alert notifications. The result of this is that these users will not get New Relic mobile app push notifications. As a workaround, you can instead use their email address as the notification channel. You can't use the REST API to generate a list of users (only original model users). Instead you'd use the user management UI. Changing account name: These users cannot change the name of their organization after it has been created. Transition to new models Some New Relic organizations on the old models are able to transition to the new models. To learn more about this, see Transition to new models.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 288.2962,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Overview of changes to <em>pricing</em> <em>and</em> user model",
        "sections": "Overview of changes to <em>pricing</em> <em>and</em> user model",
        "tags": "<em>Original</em> <em>product</em>-<em>based</em> <em>pricing</em>",
        "body": " to this <em>pricing</em>. There are two versions of this <em>pricing</em> model. Our <em>original</em> <em>product</em>-<em>based</em> <em>pricing</em> model: this is <em>based</em> on subscriptions to specific products (for example, APM, Browser, Mobile, Infrastructure). Some organizations have the option to transition to the newer <em>pricing</em> model: in that case"
      },
      "id": "603e97fa28ccbcb7c2eba754"
    },
    {
      "sections": [
        "Original product-based pricing and billing",
        "Important",
        "Overview of original pricing",
        "Annual vs monthly pricing models",
        "APM and infrastructure: Compute-unit vs host-based pricing",
        "Compute unit pricing",
        "Host-based pricing",
        "Tip",
        "How is a \"host\" defined?",
        "Prorated billing",
        "Manage subscription and billing settings",
        "View summary information",
        "View or change current subscription",
        "View usage",
        "View or update billing information"
      ],
      "title": "Original product-based pricing and billing",
      "type": "docs",
      "tags": [
        "Accounts",
        "Original accounts and billing",
        "Original product-based pricing"
      ],
      "external_id": "92a9a2aaacf80af45767d6f8f15283c541b2bf08",
      "image": "https://docs.newrelic.com/static/a5a6fd548a3c62e03183f13e6be6688a/77a9e/Accounts_CU-calculation_0.png",
      "url": "https://docs.newrelic.com/docs/accounts/original-accounts-billing/product-pricing/product-based-pricing/",
      "published_at": "2021-12-19T14:24:45Z",
      "updated_at": "2021-11-14T09:27:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This doc explains our original product-based pricing model. For more on pricing and user-related changes, see Overview of changes. Overview of original pricing New Relic has two pricing models: a newer one called New Relic One pricing, and our original pricing model. Our original pricing model was based on subscriptions to specific products, like APM, Mobile, and Infrastructure. If you are on this pricing model, your users are likely on our original user model and use these original user docs. To understand more about the new pricing and user changes, see Overview of changes. For accounts on original pricing, this doc includes: Explanation of how our original pricing model works How to manage subscription and billing settings Annual vs monthly pricing models Here are the differences between billed-annually and billed-monthly plans: Pricing plans Details Annual (best price) New Relic charges your credit card each month for a year for a committed number of hosts or compute units. You can increase this amount at any time, and charges will adjust with the next monthly bill. Your account will automatically renew at the end of the year unless you change your subscription. Early termination, downgrade, or decrease in service: Unless your order form states otherwise, you will be charged at the level and quantity of service ordered until the end of the then-current term if you cancel or downgrade to a lower level of service or fewer hosts during your commitment year. Monthly (no commitment) New Relic charges your credit card each month for a specified number of hosts or compute units. The account Owner can change the credit card number. To edit billing settings, use the Billing UI. Adjustments to billing settings will take effect for the next billing period. Your account automatically renews each month unless you change your subscription. You can cancel service or downgrade to a lower level of service without penalty. APM and infrastructure: Compute-unit vs host-based pricing APM offers a choice between two pricing models: compute unit (CU) based pricing and host-based pricing. New Relic Infrastructure offers only CU-based pricing. This section shows how both options are calculated, and explains what \"host\" means in these pricing contexts: Compute unit pricing CU-based pricing is available for these New Relic products: APM (choice of either CU-based pricing or host-based pricing) Infrastructure: only CU-based pricing With CU-based pricing, your monthly price is determined by the size of the host (computing power and memory) running New Relic and the number of hours it connects to New Relic during the month. If a host is connected to New Relic at any time during an hour, that hour counts towards the CU calculation. Each host is counted separately for each New Relic account the host reports data to. For example, if you have a parent account with two children accounts, each running applications on the same host for 3,000 CUs in a given month, the usage for the parent account will be 6,000 CUs. For APM, CU-based pricing is the best choice if you have many cloud-based dynamic computing resources. For this reason, CU-based pricing is sometimes referred to as cloud pricing. CUs are calculated as follows: The maximum size of a given host (CPUs + GB RAM) is capped at 16. Examples: If a host has 2 CPU cores, 2GB RAM, and connects to New Relic for one hour (or less than one hour), it consumes 4 CUs. If a host has 2 CPU cores, 2GB RAM, and connects to New Relic for an entire month (750 hours used as standard month size), it consumes 3,000 CUs. You can purchase blocks of CUs to be consumed on a monthly basis. The total number of CUs purchased monthly is calculated by adding up the estimated CU consumption for all hosts for the month. There is no month-to-month rollover of unused CUs. Also, New Relic does not charge by JVMs, containers (such as Docker or Cloud Foundry), or application instances--it charges by the hosts running those containers or application instances. Price points vary, depending on the New Relic product and subscription level. You can view CU-based account usage from the New Relic UI. Host-based pricing Tip Pricing for your APM account can be either CU-based or host-based. New Relic Infrastructure uses only CU-based pricing. With host-based pricing, New Relic charges based on the number of equivalent hosts used in a month. One equivalent host is defined as: a host connected to New Relic for 750 hours (750 hours used as standard month size). If a host is connected to New Relic at any time during an hour, that hour counts towards the host calculation. These hours can be divided across multiple hosts. For example, you might have three hosts that are each connected to New Relic for 250 hours during one month: these hours would add up to equal one equivalent host. Each host is counted separately for each New Relic account the host reports data to. For example, if you have a parent account with two child accounts, each running applications on the same single host for 750 hours in a given month, the usage for the parent account will be 2 equivalent hosts. Once connected to New Relic, hosts are distinguished by their unique hostnames. A host is connected to New Relic when the language agent is active and is deployed on the host. New Relic does not charge by containers (such as Docker or Cloud Foundry), JVMs, or application instances; it charges by the hosts running those containers or application instances. New Relic APM gives you a choice between host-based pricing and CU-based pricing. Host-based pricing is ideal if you have mainly static environments, consisting of hosts you manage in your own data center. For specifics on pricing amounts, see the New Relic APM pricing page. How is a \"host\" defined? To understand how New Relic computes both host-based pricing and CU-based pricing, it's important to understand how the word host is used. A host can be one of the following: A physical machine is a hardware-based device with dedicated physical resources, including memory, processing, and storage. Each machine has its own OS which applications run on. A virtual machine (VM) is the software implementation of a physical machine that executes programs like a physical machine. One or more virtual machines can run on a physical machine. Each virtual machine has its own OS and allocated virtual machine resources such as RAM and CPU. A cloud instance is a type of virtual machine that is run in the public cloud. In this context, virtual machines and cloud instances are different from Java Virtual Machines (JVMs) and containers. For New Relic's pricing calculation purposes, a month is defined as 750 hours. Prorated billing If you upgrade your subscription partway through your billing period, you will be subject to a prorated charge for the higher level of service over the remainder of your billing period. This will be invoiced or charged to your credit card when the upgrade is submitted. You will be notified about this charge as part of the subscription change process. If you have questions, contact your New Relic account representative. If you need to report billing issues, contact New Relic's Billing Department. Manage subscription and billing settings Important Note that as of July 30 2020, we have a newer pricing model. To learn more, see Overview of pricing. The account Owner can perform many subscription self-service functions directly from the user interface: From one.newrelic.com, select the account dropdown. Select your choice of self-service options. When making subscription changes, be sure to save any changes, agree to New Relic's Terms of Service and Supplemental Payment Terms as appropriate, and select Pay now. Optional: If you downgrade your subscription, complete New Relic's survey. Here is a summary of the available options from your account dropdown in the New Relic user interface: View summary information To view summary information about your subscription, go to the billing UI. View or change current subscription To adjust your subscription settings, use the Billing UI. If you need more help, contact your New Relic account representative, or contact New Relic's Billing Department. View usage To view your usage, use the usage UI. View or update billing information To view or update your New Relic account's billing information, see the billing UI.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 271.0539,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Original</em> <em>product</em>-<em>based</em> <em>pricing</em> <em>and</em> <em>billing</em>",
        "sections": "<em>Original</em> <em>product</em>-<em>based</em> <em>pricing</em> <em>and</em> <em>billing</em>",
        "tags": "<em>Original</em> <em>product</em>-<em>based</em> <em>pricing</em>",
        "body": ". For <em>accounts</em> on <em>original</em> <em>pricing</em>, this doc includes: Explanation of how our <em>original</em> <em>pricing</em> model works How to manage subscription and <em>billing</em> settings Annual vs monthly <em>pricing</em> models Here are the differences between billed-annually and billed-monthly plans: <em>Pricing</em> plans Details Annual (best <em>price</em>) New"
      },
      "id": "6043f753e7b9d212085799da"
    },
    {
      "sections": [
        "Trial and Lite accounts",
        "Important",
        "Trial accounts",
        "Trial lengths",
        "End of trial period",
        "Caution"
      ],
      "title": "Trial and Lite accounts",
      "type": "docs",
      "tags": [
        "Accounts",
        "Original accounts and billing",
        "Original product-based pricing"
      ],
      "external_id": "050d5fa2eea990cf75a7d4de2c15bebd612860f3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/original-accounts-billing/product-based-pricing/trial-lite-accounts-deprecated/",
      "published_at": "2021-12-19T17:31:49Z",
      "updated_at": "2021-11-14T09:23:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This document applies to our original product-based pricing, which is no longer available for new accounts. For an explanation of pricing models, see Overview of pricing. Free trials for New Relic allow you to identify which products and subscription tier best fit your business. Accounts not upgraded with a subscription before the end of the trial period become Lite accounts, losing many key features and data. Trial accounts When you start a free trial, you gain access to all the features of a Pro account including full access to support. Our products allow you to view and track trends. Pro level data retention allows you to track how changes in your business, such as marketing approaches or new technology, affect trends. Trial lengths Trial lengths depend on the product: Product Trial Length Alerts 30 days APM 14 days Browser 14 days Infrastructure 30 days Insights 30 days Mobile 30 days Synthetics 14 days End of trial period Once the trial ends, your account becomes a Lite account. Lite accounts can access all of our products except Infrastructure and Insights, but lose access to most product features and support. Caution Lite accounts retain only very recent data, which could cause the loss of valuable trend data. Avoid this by subscribing before your trial ends.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 271.05096,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Trial <em>and</em> Lite <em>accounts</em>",
        "sections": "Trial <em>and</em> Lite <em>accounts</em>",
        "tags": "<em>Original</em> <em>product</em>-<em>based</em> <em>pricing</em>",
        "body": "Important This document applies to our <em>original</em> <em>product</em>-<em>based</em> <em>pricing</em>, which is no longer available for new <em>accounts</em>. For an explanation of <em>pricing</em> models, see Overview of <em>pricing</em>. Free trials for New Relic allow you to identify which products and subscription tier best fit your business. <em>Accounts</em>"
      },
      "id": "603ec29a196a67b153a83dad"
    }
  ],
  "/docs/accounts/accounts/automated-user-management/automated-user-provisioning-single-sign": [
    {
      "sections": [
        "Automated user management: SCIM API",
        "Requirements",
        "Tutorial",
        "SCIM service provider",
        "Authentication",
        "Supported resources",
        "Schemas",
        "New Relic user type (basic vs full platform) schema",
        "Supported actions",
        "Create group",
        "Get group",
        "Get group by query",
        "Update group with PUT",
        "Update group with PATCH (Non-member fields)",
        "Update group with PATCH (Add members)",
        "Update group with PATCH (Remove members)",
        "Delete group",
        "Create user",
        "Get user",
        "Get user by query",
        "Update user with PUT",
        "Update user with PATCH",
        "Delete user",
        "Deviations from the RFC",
        "Next steps when you're done",
        "Change your users' user type",
        "Assign access grants"
      ],
      "title": "Automated user management: SCIM API",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Automated user management"
      ],
      "external_id": "9e7992bec73759ba6c820721101618de28859b33",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/automated-user-management/scim-support-automated-user-management/",
      "published_at": "2021-12-19T15:40:05Z",
      "updated_at": "2021-11-24T11:26:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you want to implement New Relic's automated user management (AUM) and import your users from an identity provider, first read Introduction to AUM to learn about supported identity providers and when you'd want to use our SCIM API, documented below. Requirements If you have an identity provider that has a New Relic app (Azure AD, Okta, and OneLogin), see the guides for those. The SCIM API is meant for organizations that either aren't using those identity providers, or that want to use the SCIM API for additional configuration not available with the apps (for example, managing user type). Before using our SCIM API, you must first enable SCIM for an authentication domain. Note that after you set up an integration with the SCIM API, there are next steps to do, including downgrading some users to basic users, and granting user groups access to New Relic accounts. Tutorial See the SCIM API tutorial for more specific instructions on using this API. SCIM service provider New Relic’s SCIM service provider follows the SCIM 2.0 API as described in RFCs 7643 and 7644. You do not need to implement all aspects of the SCIM 2.0 specification to integrate your user information with New Relic. In fact, the New Relic service provider itself does not implement all aspects of the specification. This document describes the features from the specification available for an integration with New Relic. Authentication Authentication requires a bearer token. This bearer token is specific to your New Relic authentication domain and is displayed when first enabling SCIM for an authentication domain. Supported resources The New Relic service provider supports the following SCIM resources: Group , User , Service provider config , Resource type and Schema. Bulk and Search are not supported. For more information on how the RFC describes the resource endpoints, see RFC 7644 SCIM Protocol Specification. Schemas New Relic uses a subset of the available fields in the SCIM core schema. Other SCIM fields are ignored if they are included in incoming requests. The fields used by New Relic are: Group: SCIM field name Description displayName Required. Name of the group. members List of users in the group. User: SCIM Field Name Description externalId Unique identifier for the user used by your system. userName Required. Unique identifier for the user within New Relic’s system. Use the user’s email address. name.familyName Last name of the user. name.givenName First name of the user. emails or emails.value Required. Email address of the user. timezone Time zone of the user in IANA Time Zone database format, also known as the \"Olson\" time zone database format (for exmaple, \"America/Los_Angeles\"). active Required. Boolean indicating whether or not the user should be active or inactive within New Relic. groups List of groups to which the user belongs. New Relic user type (basic vs full platform) schema This is an optional schema extension for New Relic-specific user attributes. Currently this provides control only over a user's user type (basic user versus full platform user). For a tutorial showing how to use this, see the SCIM API tutorial. urn:ietf:params:scim:schemas:extension:newrelic:2.0:User: SCIM field name Description nrUserType The user's type: 'full user' (for full platform user) or 'basic user'. Supported actions SCIM provides several options for manipulating groups and users. The New Relic service provider supports the following options. When configuring, be aware that: Only simple filtering is supported. The eq operator may be used with a basic filter expression. For example, “displayName eq \"Example Group 1”. Other operators are not supported. PUT updates do not require that all fields be included. Fields that are not included will not be changed. When doing a PUT, if a required field already has the appropriate value, it is not necessary to include the field. Supported actions are: Create group Example request: POST /Groups { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"displayName\": \"Example Group 1\", \"members\": [] } Copy Example response: 201 Created { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"d0652232-b14f-434d-9c6f-36de7e1ab010\", \"displayName\": \"Example Group 1\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2019-11-08T21:33:13.055Z\", \"lastModified\": \"2019-11-08T21:33:13.055Z\" }, \"members\": [] } Copy Get group Example request: GET /Groups/YOUR_GROUP_ID Copy Example response: 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"d0652232-b14f-434d-9c6f-36de7e1ab010\", \"displayName\": \"Example Group 1\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2019-11-08T21:33:13.055Z\", \"lastModified\": \"2019-11-08T21:33:13.055Z\" }, \"members\": [] } Copy Get group by query Example request: GET /Groups?filter=displayName eq \"Example Group 1\" Copy Example response: 200 OK { \"totalResults\": 1, \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:ListResponse\" ], \"Resources\": [{ \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"d0652232-b14f-434d-9c6f-36de7e1ab010\", \"displayName\": \"Example Group 1\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2019-11-08T21:33:13.055Z\", \"lastModified\": \"2019-11-08T21:33:13.055Z\" }, \"members\": [] }] } Copy Update group with PUT In the request, include the fields that you want to change. If you include the members field, the group’s users will be adjusted to match the contents of the members field. Example request: PUT /Groups/YOUR_GROUP_ID { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"displayName\": \"Example Group 1a\" } Copy Example response: 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"d0652232-b14f-434d-9c6f-36de7e1ab010\", \"displayName\": \"Example Group 1a\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2019-11-08T21:33:13.055Z\", \"lastModified\": \"2019-11-08T22:47:14.019Z\" }, \"members\": [] } Copy Update group with PATCH (Non-member fields) Example request: PATCH /Groups/YOUR_GROUP_ID { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:PatchOp\" ], \"Operations\": [{ \"op\": \"Replace\", \"path\": \"displayName\", \"value\": \"Example Group 1b\" }] } Copy Example response: 204 No Content Copy Update group with PATCH (Add members) Example request: PATCH /Groups/YOUR_GROUP_ID { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:PatchOp\" ], \"Operations\": [{ \"op\": \"Add\", \"path\": \"members\", \"value\": [{ \"value\": \"f0cbc276-16c9-4d1a-abc0-1856b0c74224\" }] }] } Copy Example response: 204 No Content Copy Update group with PATCH (Remove members) Example request: PATCH /Groups/YOUR_GROUP_ID { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:PatchOp\" ], \"Operations\": [{ \"op\": \"Remove\", \"path\": \"members\", \"value\": [{ \"value\": \"f0cbc276-16c9-4d1a-abc0-1856b0c74224\" }] }] } Copy Example response: 204 No Content Copy Delete group Example request: DELETE /Groups/YOUR_GROUP_ID Copy Example response: 204 No Content Copy Create user Example request: POST /Users { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\" ], \"externalId\": \"external-id-1\", \"userName\": \"example-user-1@example.com\", \"name\": { \"familyName\": \"User 1\", \"givenName\": \"Example\" }, \"emails\": [{ \"value\": \"example-user-1@example.com\", \"primary\": true }], \"timezone\": \"America/Los_Angeles\", \"active\": true, \"groups\": [] } Copy Example response: 201 Created { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\" ], \"id\": \"f0cbc276-16c9-4d1a-abc0-1856b0c74224\", \"externalId\": \"external-id-1\", \"userName\": \"example-user-1@example.com\", \"name\": { \"familyName\": \"User 1\", \"givenName\": \"Example\" }, \"emails\": [{ \"value\": \"example-user-1@example.com\", \"primary\": true }], \"timezone\": \"America/Los_Angeles\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2019-11-08T22:07:12.477Z\", \"lastModified\": \"2019-11-08T22:07:12.477Z\" }, \"groups\": [] } Copy Get user Example request: GET /Users/YOUR_USER_ID Copy Example response: 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\" ], \"id\": \"f0cbc276-16c9-4d1a-abc0-1856b0c74224\", \"externalId\": \"external-id-1\", \"userName\": \"example-user-1@example.com\", \"name\": { \"familyName\": \"User 1\", \"givenName\": \"Example\" }, \"emails\": [{ \"value\": \"example-user-1@example.com\", \"primary\": true }], \"timezone\": \"America/Los_Angeles\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2019-11-08T22:07:12.477Z\", \"lastModified\": \"2019-11-08T22:07:12.477Z\" }, \"groups\": [] } Copy Get user by query Example request: GET /Users?filter=externalId eq \"external-id-1\" Copy Example response: 200 OK { \"totalResults\": 1, \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:ListResponse\" ], \"Resources\": [{ \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\" ], \"id\": \"f0cbc276-16c9-4d1a-abc0-1856b0c74224\", \"externalId\": \"external-id-1\", \"userName\": \"example-user-1@example.com\", \"name\": { \"familyName\": \"User 1\", \"givenName\": \"Example\" }, \"emails\": [{ \"value\": \"example-user-1@example.com\", \"primary\": true }], \"timezone\": \"America/Los_Angeles\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2019-11-08T22:07:12.477Z\", \"lastModified\": \"2019-11-08T22:07:12.477Z\" }, \"groups\": [] }] } Copy Update user with PUT In the request, include the fields that you want to change. If you include the groups field, the user’s groups will be adjusted to match the contents of the groups field. Example request: PUT /Users/YOUR_USER_ID { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\" ], \"name\": { \"familyName\": \"User 1A\", \"givenName\": \"Example\" } } Copy Example response: 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\" ], \"id\": \"f0cbc276-16c9-4d1a-abc0-1856b0c74224\", \"externalId\": \"external-id-1\", \"userName\": \"example-user-1@example.com\", \"name\": { \"familyName\": \"User 1A\", \"givenName\": \"Example\" }, \"emails\": [{ \"value\": \"example-user-1@example.com\", \"primary\": true }], \"timezone\": \"America/Los_Angeles\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2019-11-08T22:07:12.477Z\", \"lastModified\": \"2019-11-08T22:28:33.552Z\" }, \"groups\": [] } Copy Update user with PATCH Example request: PATCH /Users/YOUR_USER_ID { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:PatchOp\" ], \"Operations\": [{ \"op\": \"Replace\", \"path\": \"active\", \"value\": \"false\" }] } Copy Example response: 204 No Content Copy Delete user Example request: DELETE /Users/YOUR_USER_ID Copy Example response: 204 No Content Copy Deviations from the RFC This section describes areas where the New Relic SCIM service provider deviates from the SCIM protocol RFC 7644. Section numbers refer to RFC section numbers. Items in this section could change as we work to bring our service provider into full compliance with the RFC. RFC section name RFC section number Deviation description Creating Resources 3.3. The meta.location field is not set. Filtering 3.4.2.2. The only currently supported operator is eq. Field names are case sensitive. String attributes are compared in a case sensitive manner. Prefixing the field name with the schema is not supported. For example, filter=urn:ietf:params:scim:schemas:core:2.0:User:userName eq \"johnsmith\" will not work. /Me Authenticated Subject Alias 3.11. GET with the /Me resource returns a 404 Not Found. Service Provider Configuration Endpoints 4. The service provider feature discovery endpoints do not support filtering. Bearer Token and Cookie Considerations 7.4. Bearer tokens do not have a set expiration date. Next steps when you're done Change your users' user type When your users are provisioned in New Relic, you can see them in the User management UI. If you're adding users to New Relic via SCIM but not managing their user type via SCIM, they start out as basic users. To convert users to full platform users, you have two options: Use the User management UI. Use the SCIM API to manage user type. Assign access grants Once your users are in New Relic, you need to grant them access to specific New Relic accounts, specific groups, and specific roles. Without doing this, your users have no access to New Relic accounts. To learn how to do this, see: How access grants work The user management tutorial",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 228.91455,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Automated</em> <em>user</em> <em>management</em>: SCIM API",
        "sections": "<em>Automated</em> <em>user</em> <em>management</em>: SCIM API",
        "tags": "<em>Automated</em> <em>user</em> <em>management</em>",
        "body": "If you want to implement New Relic&#x27;s <em>automated</em> <em>user</em> <em>management</em> (AUM) and import your users from an identity provider, first read Introduction to AUM to learn about supported identity providers and when you&#x27;d want to use our SCIM API, documented below. Requirements If you have an identity provider"
      },
      "id": "6043f38a64441f7d39378f0b"
    },
    {
      "sections": [
        "Azure AD SCIM/SSO application configuration",
        "Requirements",
        "Step 1. Create authentication domain and enable SCIM",
        "Step 2. Set up Azure's New Relic app",
        "Step 3. Configure connection",
        "Step 4. Configure provisioning rules",
        "Tip",
        "Step 5. Set your users' user type",
        "Step 6. Assign access grants",
        "Step 7. Configure SAML SSO"
      ],
      "title": "Azure AD SCIM/SSO application configuration",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Automated user management"
      ],
      "external_id": "d6e7f7e95daa833451159a3db4e2c4257270b5e9",
      "image": "https://docs.newrelic.com/static/0a9a32fd5041e6e2ea37cc5f032b6910/8c557/Azure_AD_Provisioning_Attribute_Mapping_2_0.png",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/automated-user-management/azure-ad-scimsso-application-configuration/",
      "published_at": "2021-12-19T15:39:27Z",
      "updated_at": "2021-11-24T11:24:35Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our automated user management (AUM) allows allows you to import and configure your New Relic users from your identity provider via SCIM. This guide provides Azure AD-specific details on how to configure the New Relic Azure AD SCIM/SSO application. Requirements Before using this guide, read the requirements and procedure overview. Note that these instructions require going back and forth between your identity provider and New Relic. Step 1. Create authentication domain and enable SCIM To get to the New Relic authentication domain UI: From one.newrelic.com, click the account dropdown, click Organization and access, and then click Authentication domains. If you don't already have one, create a new authentication domain for your SCIM-provisioned users by clicking + Add new. For that authentication domain, under Source of users, select SCIM. Copy and save the API token for later use. Note that this will be shown only once. Step 2. Set up Azure's New Relic app Next, you'll set up Azure's New Relic SAML/SCIM app. Azure AD provides an application gallery, which includes various integrations for Azure AD, including the ones that New Relic offers. To set this up: Go to the Azure Active Directory admin center, and sign in if necessary. aad.portal.azure.com/ Click on All services in the left hand menu. In the main pane, click on Enterprise applications. Click on +New Application. Find our SCIM/SSO application by entering New Relic in the name search box, and click on the application New Relic by organization (not New Relic by account). Click on Add. Step 3. Configure connection Configure the New Relic SCIM/SSO application to automatically provision your users to New Relic. From the New Relic SCIM/SSO application page, click on the Provisioning link in the sidebar. In the main pane, click on Get started. In the Provisioning Mode pick-list, choose Automatic. In New Relic's authentication domain UI, set up a new domain with SCIM enabled. In Azure AD's New Relic SCIM/SSO app, in the Admin credentials section, fill out the Tenant URL and Secret token fields with the values provided in New Relic's authentication domain UI. To verify you can connect to New Relic, click Test Connection. When you see a message indicating verification success, click Save. The New Relic SCIM/SSO application can now connect with New Relic. Continue with the following section to configure the provisioning rules. Step 4. Configure provisioning rules Initially, nothing is configured to be sent to New Relic. You must configure Azure AD to send changes for user creation, updates, and deactivation. Go to the Provisioning page and complete the following: Expand the Mappings section. Click Provision Azure Active Directory Users. Verify the Target Object Actions Create Update and Delete checkboxes are all checked. Verify the Attribute Mappings look correct for your environment. Each of the New Relic attributes shown in the list must receive a value. Tip Ensure that the Azure Active Directory attributes shown in the list on the left are good sources for the information to send to New Relic. In particular, not all environments set the mail attribute. If your environment does not set the mail attribute, userPrincipalName could be a good alternative. Leave the switch for Enabled set to Off until you're done with the user and group configuration in the next section. Once all configuration is ready, return to this page and set the switch to On. Click Save. Here's an example of a filled-in attribute mapping page with the default values. Your values may be configured differently depending on your situation. After saving the provisioning rules, the New Relic SCIM/SSO application is ready to provision any changes made to users assigned to the application. Continue with the following section to assign users and groups to the New Relic SCIM/SSO application. Step 5. Set your users' user type When your users are provisioned in New Relic, you're able to see them in the User management UI. If you're adding users to New Relic via SCIM but not managing their user type via SCIM, they start out as basic users. To convert users to full platform users, you have two options: Use the User management UI to edit users. Configure the Azure app to manage user type. Step 6. Assign access grants Once these steps are completed, you should be able to see your users in New Relic by going to the User management UI. Now that your users are present in New Relic, you must grant them access to specific roles on specific accounts. If this is not done, your users don't yet have access to New Relic. To learn how to do this, see: How access grants work The user management tutorial Step 7. Configure SAML SSO To enable SAML SSO for your users, see the SAML instructions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 228.91199,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Step 5. Set your <em>users</em>&#x27; <em>user</em> type",
        "tags": "<em>Automated</em> <em>user</em> <em>management</em>",
        "body": "Our <em>automated</em> <em>user</em> <em>management</em> (AUM) allows allows you to import and configure your New Relic users from your identity provider via SCIM. This guide provides Azure AD-specific details on how to configure the New Relic Azure AD SCIM&#x2F;SSO application. Requirements Before using this guide, read"
      },
      "id": "6043f5c964441fcfb0378ef3"
    },
    {
      "sections": [
        "OneLogin SCIM/SSO application configuration",
        "Requirements",
        "Step 1. Create authentication domain and enable SCIM",
        "Step 2. Set up OneLogin's New Relic app",
        "Step 3. Configure SCIM/SSO application",
        "Fill in the configuration form",
        "Fill in the rules form",
        "A rule that only uses actions",
        "Fill in the provisioning form",
        "Tip",
        "Fill in the Parameters form",
        "Save your changes",
        "Step 4. Assign users",
        "Step 5. Set your users' user type",
        "Step 6. Assign access grants",
        "Step 7. Configure SAML SSO"
      ],
      "title": "OneLogin SCIM/SSO application configuration",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Automated user management"
      ],
      "external_id": "d298162c055490c99117f564c3ea9c2ea5dfd8d1",
      "image": "https://docs.newrelic.com/static/8f585557ea58f70d94a746e6439bf1ad/c1b63/onelogin_rules_conditions_actions.png",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/automated-user-management/onelogin-scimsso-application-configuration/",
      "published_at": "2021-12-19T15:39:45Z",
      "updated_at": "2021-11-24T11:24:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our automated user management (AUM) allows allows you to import and configure your New Relic users from your identity provider via SCIM. This guide provides OneLogin-specific details on how to configure the New Relic OneLogin SCIM/SSO application. Requirements Before using this guide, read our AUM requirements. Note that these instructions require going back and forth between your identity provider and New Relic. Step 1. Create authentication domain and enable SCIM To get to the New Relic authentication domain UI: From one.newrelic.com, click the account dropdown, click Organization and access, and then click Authentication domains. If you don't already have one, create a new authentication domain for your SCIM-provisioned users by clicking + Add new. For that authentication domain, under Source of users, select SCIM. Copy and save the API token for later use. Note that this will be shown only once. Step 2. Set up OneLogin's New Relic app Next, you'll be setting up OneLogin's New Relic SAML/SCIM app. To set this up: Go to the OneLogin web site and sign in with an account that has administrator permissions. From the OneLogin home page, click on Administration. From the OneLogin Administration page, choose the Applications menu. From the OneLogin Applications page, click on Add App. In the search field on the OneLogin Find Applications page, enter \"New Relic by organization\" (not \"New Relic by account\") and then click on the application when it shows in the search results. From the Add New Relic by Organization page, click on Save. Step 3. Configure SCIM/SSO application Configuration for the New Relic SCIM/SSO application is split across several forms. This section describes the different forms that need to be configured. From the New Relic by organization application page, fill in the following forms: Fill in the configuration form In the left pane, select Configuration and complete the following: Get the authentication domain ID (top of New Relic's authentication domain UI and SCIM bearer token (in authentication domain UI as \"SAML 2.0 endpoint) and input them into the appropriate fields in the OneLogin app. Leave the API Connection disabled until all the configuration described in the following sections is completed. After completing all the configuration, enable the connection. Fill in the rules form Configure the user groups to send to New Relic using rules. OneLogin provides this documentation which describes how to use rules to provision groups for users. Decide what type of groups to send along with your users to New Relic. If your organization is using Active Directory or LDAP, you might choose to use security groups to define your users capabilities at New Relic. Another reasonable group choice is OneLogin role. On the New Relic side, your user's groups define their capabilities. The groups that are sent with users will be mapped to New Relic capability groups. Note that at the moment, there is no way to delete a group from the OneLogin side. This is a known limitation from OneLogin. Removing or changing rules does not delete groups already sent to New Relic. If you wish to no longer use a group, removing all the users from the group will prevent it from being used at New Relic. A rule that only uses actions Here's an example rule configuration does not use any conditions. The conditions are left empty to avoid applying any filtering logic to the users. All users will be sent in this example. If you want to send only a subset of users, you need to specify conditions to select the subset. The actions describe where to retrieve the value for the group name and how to parse the value. In this example, we retrieve the group name from the OneLogin role field. The OneLogin role field only has a single value, but sometimes the source for the group name contains other fields besides group name. In other words, some sources give you a list of fields and values and only one of those fields has the value you want to use. In this case, you can insert a regular expression in with value that matches field to find and extract the value for the group name. This example uses the entire value of the For each field for the group name. Fill in the provisioning form In the left pane, select Provisioning and complete the following: Check Enable provisioning. Under Require admin approval before this action is performed, uncheck these options: Create user Delete user Update user Tip If you do not uncheck these options, SCIM provisioning requests will not be sent until an administrator approves them. Set When users are deleted in OneLogin, or the user's app access is removed, perform the below action to Delete. Set When user accounts are suspended in OneLogin, perform the following action to Suspend. Fill in the Parameters form In the left pane, select Parameters and complete the following: Click Groups field. Check Include in User Provisioning. Click Save. Save your changes After you complete the above forms, click Save. Then, return to the Configuration form and enable the API connection. Step 4. Assign users After New Relic SCIM/SSO application configuration is finished and New Relic side configuration is finished, you can begin to assign users to the application. Assign the New Relic SCIM/SSO application to a user: Go to the OneLogin web site and sign in with an account that has administrator permissions. From the OneLogin home page, click Administration. From the OneLogin Administration page, choose the Users menu Users item. From the OneLogin Users page, click the user you want to assign the application to. From the user's page, click Applications. From the user's application page, click the plus sign and select the \"New Relic by Organization\" application. Important: Updating users' time zones is important, as charts and other user assets display times. Default is UMT. From the Edit New Relic by Organization login for user page, enter the user's time zone in IANA Time Zone database format (also known as the Olson time zone database format) and click Save. If you're using Roles to define your New Relic capability groups, from the user's application page, click the proper role(s) for the user and then click Save User. Step 5. Set your users' user type When your users are provisioned in New Relic, you're able to see them in the User management UI. If you're adding users to New Relic via SCIM but not managing their user type via SCIM, they start out as basic users. To convert users to full platform users, you have two options: Use the User management UI to edit users. Configure the OneLogin app to manage user type. Step 6. Assign access grants Once these steps are completed, you should be able to see your users in New Relic by going to the User management UI. Now that your users are present in New Relic, you must grant them access to specific roles on specific accounts. If this is not done, your users don't yet have access to New Relic. To learn how to do this, see: How access grants work The user management tutorial Step 7. Configure SAML SSO To enable SAML SSO for your users, see the SAML instructions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 228.91142,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Step 4. Assign <em>users</em>",
        "tags": "<em>Automated</em> <em>user</em> <em>management</em>",
        "body": "Our <em>automated</em> <em>user</em> <em>management</em> (AUM) allows allows you to import and configure your New Relic users from your identity provider via SCIM. This guide provides OneLogin-specific details on how to configure the New Relic OneLogin SCIM&#x2F;SSO application. Requirements Before using this guide, read our AUM"
      },
      "id": "6043f34228ccbccafb2c606a"
    }
  ],
  "/docs/accounts/accounts/automated-user-management/azure-ad-scimsso-application-configuration": [
    {
      "sections": [
        "Automated user management: SCIM API",
        "Requirements",
        "Tutorial",
        "SCIM service provider",
        "Authentication",
        "Supported resources",
        "Schemas",
        "New Relic user type (basic vs full platform) schema",
        "Supported actions",
        "Create group",
        "Get group",
        "Get group by query",
        "Update group with PUT",
        "Update group with PATCH (Non-member fields)",
        "Update group with PATCH (Add members)",
        "Update group with PATCH (Remove members)",
        "Delete group",
        "Create user",
        "Get user",
        "Get user by query",
        "Update user with PUT",
        "Update user with PATCH",
        "Delete user",
        "Deviations from the RFC",
        "Next steps when you're done",
        "Change your users' user type",
        "Assign access grants"
      ],
      "title": "Automated user management: SCIM API",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Automated user management"
      ],
      "external_id": "9e7992bec73759ba6c820721101618de28859b33",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/automated-user-management/scim-support-automated-user-management/",
      "published_at": "2021-12-19T15:40:05Z",
      "updated_at": "2021-11-24T11:26:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you want to implement New Relic's automated user management (AUM) and import your users from an identity provider, first read Introduction to AUM to learn about supported identity providers and when you'd want to use our SCIM API, documented below. Requirements If you have an identity provider that has a New Relic app (Azure AD, Okta, and OneLogin), see the guides for those. The SCIM API is meant for organizations that either aren't using those identity providers, or that want to use the SCIM API for additional configuration not available with the apps (for example, managing user type). Before using our SCIM API, you must first enable SCIM for an authentication domain. Note that after you set up an integration with the SCIM API, there are next steps to do, including downgrading some users to basic users, and granting user groups access to New Relic accounts. Tutorial See the SCIM API tutorial for more specific instructions on using this API. SCIM service provider New Relic’s SCIM service provider follows the SCIM 2.0 API as described in RFCs 7643 and 7644. You do not need to implement all aspects of the SCIM 2.0 specification to integrate your user information with New Relic. In fact, the New Relic service provider itself does not implement all aspects of the specification. This document describes the features from the specification available for an integration with New Relic. Authentication Authentication requires a bearer token. This bearer token is specific to your New Relic authentication domain and is displayed when first enabling SCIM for an authentication domain. Supported resources The New Relic service provider supports the following SCIM resources: Group , User , Service provider config , Resource type and Schema. Bulk and Search are not supported. For more information on how the RFC describes the resource endpoints, see RFC 7644 SCIM Protocol Specification. Schemas New Relic uses a subset of the available fields in the SCIM core schema. Other SCIM fields are ignored if they are included in incoming requests. The fields used by New Relic are: Group: SCIM field name Description displayName Required. Name of the group. members List of users in the group. User: SCIM Field Name Description externalId Unique identifier for the user used by your system. userName Required. Unique identifier for the user within New Relic’s system. Use the user’s email address. name.familyName Last name of the user. name.givenName First name of the user. emails or emails.value Required. Email address of the user. timezone Time zone of the user in IANA Time Zone database format, also known as the \"Olson\" time zone database format (for exmaple, \"America/Los_Angeles\"). active Required. Boolean indicating whether or not the user should be active or inactive within New Relic. groups List of groups to which the user belongs. New Relic user type (basic vs full platform) schema This is an optional schema extension for New Relic-specific user attributes. Currently this provides control only over a user's user type (basic user versus full platform user). For a tutorial showing how to use this, see the SCIM API tutorial. urn:ietf:params:scim:schemas:extension:newrelic:2.0:User: SCIM field name Description nrUserType The user's type: 'full user' (for full platform user) or 'basic user'. Supported actions SCIM provides several options for manipulating groups and users. The New Relic service provider supports the following options. When configuring, be aware that: Only simple filtering is supported. The eq operator may be used with a basic filter expression. For example, “displayName eq \"Example Group 1”. Other operators are not supported. PUT updates do not require that all fields be included. Fields that are not included will not be changed. When doing a PUT, if a required field already has the appropriate value, it is not necessary to include the field. Supported actions are: Create group Example request: POST /Groups { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"displayName\": \"Example Group 1\", \"members\": [] } Copy Example response: 201 Created { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"d0652232-b14f-434d-9c6f-36de7e1ab010\", \"displayName\": \"Example Group 1\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2019-11-08T21:33:13.055Z\", \"lastModified\": \"2019-11-08T21:33:13.055Z\" }, \"members\": [] } Copy Get group Example request: GET /Groups/YOUR_GROUP_ID Copy Example response: 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"d0652232-b14f-434d-9c6f-36de7e1ab010\", \"displayName\": \"Example Group 1\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2019-11-08T21:33:13.055Z\", \"lastModified\": \"2019-11-08T21:33:13.055Z\" }, \"members\": [] } Copy Get group by query Example request: GET /Groups?filter=displayName eq \"Example Group 1\" Copy Example response: 200 OK { \"totalResults\": 1, \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:ListResponse\" ], \"Resources\": [{ \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"d0652232-b14f-434d-9c6f-36de7e1ab010\", \"displayName\": \"Example Group 1\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2019-11-08T21:33:13.055Z\", \"lastModified\": \"2019-11-08T21:33:13.055Z\" }, \"members\": [] }] } Copy Update group with PUT In the request, include the fields that you want to change. If you include the members field, the group’s users will be adjusted to match the contents of the members field. Example request: PUT /Groups/YOUR_GROUP_ID { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"displayName\": \"Example Group 1a\" } Copy Example response: 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"d0652232-b14f-434d-9c6f-36de7e1ab010\", \"displayName\": \"Example Group 1a\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2019-11-08T21:33:13.055Z\", \"lastModified\": \"2019-11-08T22:47:14.019Z\" }, \"members\": [] } Copy Update group with PATCH (Non-member fields) Example request: PATCH /Groups/YOUR_GROUP_ID { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:PatchOp\" ], \"Operations\": [{ \"op\": \"Replace\", \"path\": \"displayName\", \"value\": \"Example Group 1b\" }] } Copy Example response: 204 No Content Copy Update group with PATCH (Add members) Example request: PATCH /Groups/YOUR_GROUP_ID { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:PatchOp\" ], \"Operations\": [{ \"op\": \"Add\", \"path\": \"members\", \"value\": [{ \"value\": \"f0cbc276-16c9-4d1a-abc0-1856b0c74224\" }] }] } Copy Example response: 204 No Content Copy Update group with PATCH (Remove members) Example request: PATCH /Groups/YOUR_GROUP_ID { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:PatchOp\" ], \"Operations\": [{ \"op\": \"Remove\", \"path\": \"members\", \"value\": [{ \"value\": \"f0cbc276-16c9-4d1a-abc0-1856b0c74224\" }] }] } Copy Example response: 204 No Content Copy Delete group Example request: DELETE /Groups/YOUR_GROUP_ID Copy Example response: 204 No Content Copy Create user Example request: POST /Users { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\" ], \"externalId\": \"external-id-1\", \"userName\": \"example-user-1@example.com\", \"name\": { \"familyName\": \"User 1\", \"givenName\": \"Example\" }, \"emails\": [{ \"value\": \"example-user-1@example.com\", \"primary\": true }], \"timezone\": \"America/Los_Angeles\", \"active\": true, \"groups\": [] } Copy Example response: 201 Created { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\" ], \"id\": \"f0cbc276-16c9-4d1a-abc0-1856b0c74224\", \"externalId\": \"external-id-1\", \"userName\": \"example-user-1@example.com\", \"name\": { \"familyName\": \"User 1\", \"givenName\": \"Example\" }, \"emails\": [{ \"value\": \"example-user-1@example.com\", \"primary\": true }], \"timezone\": \"America/Los_Angeles\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2019-11-08T22:07:12.477Z\", \"lastModified\": \"2019-11-08T22:07:12.477Z\" }, \"groups\": [] } Copy Get user Example request: GET /Users/YOUR_USER_ID Copy Example response: 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\" ], \"id\": \"f0cbc276-16c9-4d1a-abc0-1856b0c74224\", \"externalId\": \"external-id-1\", \"userName\": \"example-user-1@example.com\", \"name\": { \"familyName\": \"User 1\", \"givenName\": \"Example\" }, \"emails\": [{ \"value\": \"example-user-1@example.com\", \"primary\": true }], \"timezone\": \"America/Los_Angeles\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2019-11-08T22:07:12.477Z\", \"lastModified\": \"2019-11-08T22:07:12.477Z\" }, \"groups\": [] } Copy Get user by query Example request: GET /Users?filter=externalId eq \"external-id-1\" Copy Example response: 200 OK { \"totalResults\": 1, \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:ListResponse\" ], \"Resources\": [{ \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\" ], \"id\": \"f0cbc276-16c9-4d1a-abc0-1856b0c74224\", \"externalId\": \"external-id-1\", \"userName\": \"example-user-1@example.com\", \"name\": { \"familyName\": \"User 1\", \"givenName\": \"Example\" }, \"emails\": [{ \"value\": \"example-user-1@example.com\", \"primary\": true }], \"timezone\": \"America/Los_Angeles\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2019-11-08T22:07:12.477Z\", \"lastModified\": \"2019-11-08T22:07:12.477Z\" }, \"groups\": [] }] } Copy Update user with PUT In the request, include the fields that you want to change. If you include the groups field, the user’s groups will be adjusted to match the contents of the groups field. Example request: PUT /Users/YOUR_USER_ID { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\" ], \"name\": { \"familyName\": \"User 1A\", \"givenName\": \"Example\" } } Copy Example response: 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\" ], \"id\": \"f0cbc276-16c9-4d1a-abc0-1856b0c74224\", \"externalId\": \"external-id-1\", \"userName\": \"example-user-1@example.com\", \"name\": { \"familyName\": \"User 1A\", \"givenName\": \"Example\" }, \"emails\": [{ \"value\": \"example-user-1@example.com\", \"primary\": true }], \"timezone\": \"America/Los_Angeles\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2019-11-08T22:07:12.477Z\", \"lastModified\": \"2019-11-08T22:28:33.552Z\" }, \"groups\": [] } Copy Update user with PATCH Example request: PATCH /Users/YOUR_USER_ID { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:PatchOp\" ], \"Operations\": [{ \"op\": \"Replace\", \"path\": \"active\", \"value\": \"false\" }] } Copy Example response: 204 No Content Copy Delete user Example request: DELETE /Users/YOUR_USER_ID Copy Example response: 204 No Content Copy Deviations from the RFC This section describes areas where the New Relic SCIM service provider deviates from the SCIM protocol RFC 7644. Section numbers refer to RFC section numbers. Items in this section could change as we work to bring our service provider into full compliance with the RFC. RFC section name RFC section number Deviation description Creating Resources 3.3. The meta.location field is not set. Filtering 3.4.2.2. The only currently supported operator is eq. Field names are case sensitive. String attributes are compared in a case sensitive manner. Prefixing the field name with the schema is not supported. For example, filter=urn:ietf:params:scim:schemas:core:2.0:User:userName eq \"johnsmith\" will not work. /Me Authenticated Subject Alias 3.11. GET with the /Me resource returns a 404 Not Found. Service Provider Configuration Endpoints 4. The service provider feature discovery endpoints do not support filtering. Bearer Token and Cookie Considerations 7.4. Bearer tokens do not have a set expiration date. Next steps when you're done Change your users' user type When your users are provisioned in New Relic, you can see them in the User management UI. If you're adding users to New Relic via SCIM but not managing their user type via SCIM, they start out as basic users. To convert users to full platform users, you have two options: Use the User management UI. Use the SCIM API to manage user type. Assign access grants Once your users are in New Relic, you need to grant them access to specific New Relic accounts, specific groups, and specific roles. Without doing this, your users have no access to New Relic accounts. To learn how to do this, see: How access grants work The user management tutorial",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 228.91454,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Automated</em> <em>user</em> <em>management</em>: SCIM API",
        "sections": "<em>Automated</em> <em>user</em> <em>management</em>: SCIM API",
        "tags": "<em>Automated</em> <em>user</em> <em>management</em>",
        "body": "If you want to implement New Relic&#x27;s <em>automated</em> <em>user</em> <em>management</em> (AUM) and import your users from an identity provider, first read Introduction to AUM to learn about supported identity providers and when you&#x27;d want to use our SCIM API, documented below. Requirements If you have an identity provider"
      },
      "id": "6043f38a64441f7d39378f0b"
    },
    {
      "sections": [
        "OneLogin SCIM/SSO application configuration",
        "Requirements",
        "Step 1. Create authentication domain and enable SCIM",
        "Step 2. Set up OneLogin's New Relic app",
        "Step 3. Configure SCIM/SSO application",
        "Fill in the configuration form",
        "Fill in the rules form",
        "A rule that only uses actions",
        "Fill in the provisioning form",
        "Tip",
        "Fill in the Parameters form",
        "Save your changes",
        "Step 4. Assign users",
        "Step 5. Set your users' user type",
        "Step 6. Assign access grants",
        "Step 7. Configure SAML SSO"
      ],
      "title": "OneLogin SCIM/SSO application configuration",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Automated user management"
      ],
      "external_id": "d298162c055490c99117f564c3ea9c2ea5dfd8d1",
      "image": "https://docs.newrelic.com/static/8f585557ea58f70d94a746e6439bf1ad/c1b63/onelogin_rules_conditions_actions.png",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/automated-user-management/onelogin-scimsso-application-configuration/",
      "published_at": "2021-12-19T15:39:45Z",
      "updated_at": "2021-11-24T11:24:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our automated user management (AUM) allows allows you to import and configure your New Relic users from your identity provider via SCIM. This guide provides OneLogin-specific details on how to configure the New Relic OneLogin SCIM/SSO application. Requirements Before using this guide, read our AUM requirements. Note that these instructions require going back and forth between your identity provider and New Relic. Step 1. Create authentication domain and enable SCIM To get to the New Relic authentication domain UI: From one.newrelic.com, click the account dropdown, click Organization and access, and then click Authentication domains. If you don't already have one, create a new authentication domain for your SCIM-provisioned users by clicking + Add new. For that authentication domain, under Source of users, select SCIM. Copy and save the API token for later use. Note that this will be shown only once. Step 2. Set up OneLogin's New Relic app Next, you'll be setting up OneLogin's New Relic SAML/SCIM app. To set this up: Go to the OneLogin web site and sign in with an account that has administrator permissions. From the OneLogin home page, click on Administration. From the OneLogin Administration page, choose the Applications menu. From the OneLogin Applications page, click on Add App. In the search field on the OneLogin Find Applications page, enter \"New Relic by organization\" (not \"New Relic by account\") and then click on the application when it shows in the search results. From the Add New Relic by Organization page, click on Save. Step 3. Configure SCIM/SSO application Configuration for the New Relic SCIM/SSO application is split across several forms. This section describes the different forms that need to be configured. From the New Relic by organization application page, fill in the following forms: Fill in the configuration form In the left pane, select Configuration and complete the following: Get the authentication domain ID (top of New Relic's authentication domain UI and SCIM bearer token (in authentication domain UI as \"SAML 2.0 endpoint) and input them into the appropriate fields in the OneLogin app. Leave the API Connection disabled until all the configuration described in the following sections is completed. After completing all the configuration, enable the connection. Fill in the rules form Configure the user groups to send to New Relic using rules. OneLogin provides this documentation which describes how to use rules to provision groups for users. Decide what type of groups to send along with your users to New Relic. If your organization is using Active Directory or LDAP, you might choose to use security groups to define your users capabilities at New Relic. Another reasonable group choice is OneLogin role. On the New Relic side, your user's groups define their capabilities. The groups that are sent with users will be mapped to New Relic capability groups. Note that at the moment, there is no way to delete a group from the OneLogin side. This is a known limitation from OneLogin. Removing or changing rules does not delete groups already sent to New Relic. If you wish to no longer use a group, removing all the users from the group will prevent it from being used at New Relic. A rule that only uses actions Here's an example rule configuration does not use any conditions. The conditions are left empty to avoid applying any filtering logic to the users. All users will be sent in this example. If you want to send only a subset of users, you need to specify conditions to select the subset. The actions describe where to retrieve the value for the group name and how to parse the value. In this example, we retrieve the group name from the OneLogin role field. The OneLogin role field only has a single value, but sometimes the source for the group name contains other fields besides group name. In other words, some sources give you a list of fields and values and only one of those fields has the value you want to use. In this case, you can insert a regular expression in with value that matches field to find and extract the value for the group name. This example uses the entire value of the For each field for the group name. Fill in the provisioning form In the left pane, select Provisioning and complete the following: Check Enable provisioning. Under Require admin approval before this action is performed, uncheck these options: Create user Delete user Update user Tip If you do not uncheck these options, SCIM provisioning requests will not be sent until an administrator approves them. Set When users are deleted in OneLogin, or the user's app access is removed, perform the below action to Delete. Set When user accounts are suspended in OneLogin, perform the following action to Suspend. Fill in the Parameters form In the left pane, select Parameters and complete the following: Click Groups field. Check Include in User Provisioning. Click Save. Save your changes After you complete the above forms, click Save. Then, return to the Configuration form and enable the API connection. Step 4. Assign users After New Relic SCIM/SSO application configuration is finished and New Relic side configuration is finished, you can begin to assign users to the application. Assign the New Relic SCIM/SSO application to a user: Go to the OneLogin web site and sign in with an account that has administrator permissions. From the OneLogin home page, click Administration. From the OneLogin Administration page, choose the Users menu Users item. From the OneLogin Users page, click the user you want to assign the application to. From the user's page, click Applications. From the user's application page, click the plus sign and select the \"New Relic by Organization\" application. Important: Updating users' time zones is important, as charts and other user assets display times. Default is UMT. From the Edit New Relic by Organization login for user page, enter the user's time zone in IANA Time Zone database format (also known as the Olson time zone database format) and click Save. If you're using Roles to define your New Relic capability groups, from the user's application page, click the proper role(s) for the user and then click Save User. Step 5. Set your users' user type When your users are provisioned in New Relic, you're able to see them in the User management UI. If you're adding users to New Relic via SCIM but not managing their user type via SCIM, they start out as basic users. To convert users to full platform users, you have two options: Use the User management UI to edit users. Configure the OneLogin app to manage user type. Step 6. Assign access grants Once these steps are completed, you should be able to see your users in New Relic by going to the User management UI. Now that your users are present in New Relic, you must grant them access to specific roles on specific accounts. If this is not done, your users don't yet have access to New Relic. To learn how to do this, see: How access grants work The user management tutorial Step 7. Configure SAML SSO To enable SAML SSO for your users, see the SAML instructions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 228.9114,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Step 4. Assign <em>users</em>",
        "tags": "<em>Automated</em> <em>user</em> <em>management</em>",
        "body": "Our <em>automated</em> <em>user</em> <em>management</em> (AUM) allows allows you to import and configure your New Relic users from your identity provider via SCIM. This guide provides OneLogin-specific details on how to configure the New Relic OneLogin SCIM&#x2F;SSO application. Requirements Before using this guide, read our AUM"
      },
      "id": "6043f34228ccbccafb2c606a"
    },
    {
      "sections": [
        "Okta SCIM/SSO application configuration",
        "Requirements",
        "Step 1. Create authentication domain and enable SCIM",
        "Step 2. Set up Okta's New Relic app",
        "Step 3. Configure provisioning",
        "Step 4. Assign users and groups",
        "Assignments tab",
        "Push groups tab",
        "Step 5. Set your users' user type",
        "Step 6. Assign access grants",
        "Step 7. Configure SAML SSO",
        "Additional considerations",
        "Moving users between groups"
      ],
      "title": "Okta SCIM/SSO application configuration",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Automated user management"
      ],
      "external_id": "7a00399a6ce11aaa2cb52046f994a80f5986c0e4",
      "image": "https://docs.newrelic.com/static/3f3318e1dc8c9049231c207a7b4e5c54/c1b63/okta-add-user-type-to-profile.png",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/automated-user-management/okta-scimsso-application-configuration/",
      "published_at": "2021-12-19T15:39:45Z",
      "updated_at": "2021-11-24T11:23:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our automated user management (AUM) allows allows you to import and configure your New Relic users from your identity provider via SCIM. This guide provides Okta specific details on how to configure the New Relic Okta SCIM/SSO application. Requirements Before using this guide, read our AUM requirements. Note that these instructions require going back and forth between your identity provider and New Relic. Step 1. Create authentication domain and enable SCIM To get to the New Relic authentication domain UI: From one.newrelic.com, click the account dropdown, click Organization and access, and then click Authentication domains. If you don't already have one, create a new authentication domain for your SCIM-provisioned users by clicking + Add new. For that authentication domain, under Source of users, select SCIM. Copy and save the API token for later use. Note that this will be shown only once. Step 2. Set up Okta's New Relic app Next, set up Okta's New Relic SCIM/SSO application: Go to okta.com/ and sign in with an account that has administrator permissions. From the Okta home page, click on Admin. From the Okta admin Dashboard, choose the Applications page. Click Browse app catalog and search for \"New Relic by organization\" (not \"New Relic by account\") and choose that from the results. From the New Relic by Organization page, click on Add. From the Add New Relic by organization page, check the two Application visibility \"Do not display...\" checkboxes and click on Done. We will make the application visible later after configuration is complete and provisioning has begun. Step 3. Configure provisioning Configure Okta's New Relic SCIM/SSO application to automatically provision your users to New Relic: From the app, click on the Provisioning tab. From the Integration form, click on Configure API integration. Check the Enable API integration checkbox. Take the API token you saved in Step 1 and input it in the Okta New Relic app's API token field. Optional: click on Test API credentials to verify a SCIM connection can be established to New Relic. If a connection can be established, a success message is displayed. If a connection was not established, re-enter the API Token and try the test again. Click Save. Note that the save process does a test of the API credentials. If a connection is not established to New Relic, the save will fail. On the newly displayed To App form, click on Edit. Check the Enable checkbox in the Create users, Update user attributes, and Deactivate users sections. Click Save. Step 4. Assign users and groups If you don't already have your user groups set up in Okta, you'll need to create them. These will be the groups that you'll later assign access grants to in New Relic, which will be what gives those groups access to specific roles on specific accounts. To learn how to create groups, see Okta's documentation on groups. Assignments tab Next, you'll assign users. Assigning users is done using two different tabs in the app. We recommend having your New Relic users selected on the Assignments tab and their associated groups selected on the Push groups tab. In the app, click on the Assignments tab. From the Assignments form, click on Assign. From the pop up menu, click on Assign to groups. From the Assign ... to groups form, click on Assign for the group you wish to assign to the application. Highly recommended: Set your users' time zones in Okta. The time zone affects how date/times for that user are shown in New Relic. Users without a time zone configured will be shown in UTC time in New Relic. Time zone is specified in IANA Time Zone database format, also known as the \"Olson\" time zone database format (for example, \"America/Los_Angeles\"). There are several ways in Okta to assign users' time zone, so consult the Okta docs for more information if needed. Here is one way to do this in the Assignments tab: In the Time zone field, enter the default time zone for members of the group. Click on Save and go back. Repeat the steps to add a group until all desired groups have been assigned to the application. Click Done. Push groups tab In the app, click on the Push groups tab. From the Push groups form, click on Push groups. From the pop up menu, click on Find groups by name. From the Push groups to... form, in the search field enter the first few characters of the name of the group you want to send to New Relic. Leave the Push group memberships immediately checkbox checked. Click on your group in the pop up search results list. In the Match result & push action section, No match found should be displayed, meaning that the group does not yet exist at New Relic. Leave the selector set to Create group and leave the default name for the group. The intent here is to have a group of the same name created at New Relic. If this is the last group you wish to send to New Relic, click on Save. Otherwise, if you have more groups to configure, click on Save & add another and repeat the steps to add a group. When you've added one or more groups, you should be able to see the users you've added by going to the User management UI page. Step 5. Set your users' user type When your users are provisioned in New Relic, you're able to see them in the User management UI. If you're adding users to New Relic via SCIM but not managing their user type via SCIM, they start out as basic users. To convert users to full platform users, you have two options: Use the User management UI to edit users. Manage user type from Okta (described below). To manage your users' user type from Okta: Go to the New Relic authentication domain UI and click Enable Manage user type with SCIM. Note that when this is enabled, you can’t manage user type from the New Relic UI and can only manage it from Okta. Go into your Okta instance. The rest of these instructions are done from Okta. Next, you'll configure Okta to be able to send a new attribute nrUserType. Steps: Go to the Profile editor. In the Attributes section, click Add attribute. Set your settings to match the screenshot below. The only two fields that must match exactly are External name (value: nrUserType) and External namespace (value: urn:ietf:params:scim:schemas:extension:newrelic:2.0:User). The variable value can be any value. Next, you'll configure your Okta user profile to have this field. Steps: In the Profile editor, go to Users and click the User (default) profile. Add a new New Relic user type attribute to that profile (see Okta user profile instructions). How you set this will depend on your own setup and preferences for defining user type. Note that the expected values for user type are Basic user for basic user and Full user for full platform user. Below is an example with information filled in. In the People section, define the user type for your users. How you do this will depend on your setup and preferences. For example, you may choose to set this manually by setting each user’s user type, or you may use Okta to manage these in bulk. Next, you’ll set up mapping for that attribute. Steps: In the app's Provisioning section, click Unmapped attributes. Go into edit mode for the unmapped New Relic user type attribute. Configure it based on how you want to set the user type. To learn about why you'd set them as basic users or full platform users, see User type. Learn more about Okta attribute mappings. Step 6. Assign access grants Once these steps are completed, you're able to see your users in New Relic by going to the User management UI. Now that your users are present in New Relic, you must grant them access to specific roles on specific accounts. If this is not done, your users don't yet have access to New Relic. To learn how to do this, see: How access grants work The user management tutorial Step 7. Configure SAML SSO To enable SAML SSO, see the SAML instructions. Additional considerations In this section we discuss other important things to know when using the New Relic SCIM/SSO application. This section includes tips to work around potential issues that could cause undesired results when integrating between Okta and New Relic. Moving users between groups When moving a user between groups, you must manually synchronize the old group's membership with New Relic. This is because Okta does not send a SCIM request to remove a user from a group. So, the admin needs to push the old group's membership to New Relic manually to inform New Relic that the user is no longer a member of the old group. Here are the steps to manually synchronize a group's membership: From the New Relic SCIM/SSO application page, click on the Push groups tab. From the Push groups form, open the pick list on the desired group's button under the Push Status column. From the displayed pick list on the button, click Push now. This causes an immediate synchronization of the group's membership with New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 228.91006,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Step 4. Assign <em>users</em> <em>and</em> groups",
        "tags": "<em>Automated</em> <em>user</em> <em>management</em>",
        "body": "Our <em>automated</em> <em>user</em> <em>management</em> (AUM) allows allows you to import and configure your New Relic users from your identity provider via SCIM. This guide provides Okta specific details on how to configure the New Relic Okta SCIM&#x2F;SSO application. Requirements Before using this guide, read our AUM"
      },
      "id": "6043f5cae7b9d2758b579a0c"
    }
  ],
  "/docs/accounts/accounts/automated-user-management/okta-scimsso-application-configuration": [
    {
      "sections": [
        "Automated user management: SCIM API",
        "Requirements",
        "Tutorial",
        "SCIM service provider",
        "Authentication",
        "Supported resources",
        "Schemas",
        "New Relic user type (basic vs full platform) schema",
        "Supported actions",
        "Create group",
        "Get group",
        "Get group by query",
        "Update group with PUT",
        "Update group with PATCH (Non-member fields)",
        "Update group with PATCH (Add members)",
        "Update group with PATCH (Remove members)",
        "Delete group",
        "Create user",
        "Get user",
        "Get user by query",
        "Update user with PUT",
        "Update user with PATCH",
        "Delete user",
        "Deviations from the RFC",
        "Next steps when you're done",
        "Change your users' user type",
        "Assign access grants"
      ],
      "title": "Automated user management: SCIM API",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Automated user management"
      ],
      "external_id": "9e7992bec73759ba6c820721101618de28859b33",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/automated-user-management/scim-support-automated-user-management/",
      "published_at": "2021-12-19T15:40:05Z",
      "updated_at": "2021-11-24T11:26:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you want to implement New Relic's automated user management (AUM) and import your users from an identity provider, first read Introduction to AUM to learn about supported identity providers and when you'd want to use our SCIM API, documented below. Requirements If you have an identity provider that has a New Relic app (Azure AD, Okta, and OneLogin), see the guides for those. The SCIM API is meant for organizations that either aren't using those identity providers, or that want to use the SCIM API for additional configuration not available with the apps (for example, managing user type). Before using our SCIM API, you must first enable SCIM for an authentication domain. Note that after you set up an integration with the SCIM API, there are next steps to do, including downgrading some users to basic users, and granting user groups access to New Relic accounts. Tutorial See the SCIM API tutorial for more specific instructions on using this API. SCIM service provider New Relic’s SCIM service provider follows the SCIM 2.0 API as described in RFCs 7643 and 7644. You do not need to implement all aspects of the SCIM 2.0 specification to integrate your user information with New Relic. In fact, the New Relic service provider itself does not implement all aspects of the specification. This document describes the features from the specification available for an integration with New Relic. Authentication Authentication requires a bearer token. This bearer token is specific to your New Relic authentication domain and is displayed when first enabling SCIM for an authentication domain. Supported resources The New Relic service provider supports the following SCIM resources: Group , User , Service provider config , Resource type and Schema. Bulk and Search are not supported. For more information on how the RFC describes the resource endpoints, see RFC 7644 SCIM Protocol Specification. Schemas New Relic uses a subset of the available fields in the SCIM core schema. Other SCIM fields are ignored if they are included in incoming requests. The fields used by New Relic are: Group: SCIM field name Description displayName Required. Name of the group. members List of users in the group. User: SCIM Field Name Description externalId Unique identifier for the user used by your system. userName Required. Unique identifier for the user within New Relic’s system. Use the user’s email address. name.familyName Last name of the user. name.givenName First name of the user. emails or emails.value Required. Email address of the user. timezone Time zone of the user in IANA Time Zone database format, also known as the \"Olson\" time zone database format (for exmaple, \"America/Los_Angeles\"). active Required. Boolean indicating whether or not the user should be active or inactive within New Relic. groups List of groups to which the user belongs. New Relic user type (basic vs full platform) schema This is an optional schema extension for New Relic-specific user attributes. Currently this provides control only over a user's user type (basic user versus full platform user). For a tutorial showing how to use this, see the SCIM API tutorial. urn:ietf:params:scim:schemas:extension:newrelic:2.0:User: SCIM field name Description nrUserType The user's type: 'full user' (for full platform user) or 'basic user'. Supported actions SCIM provides several options for manipulating groups and users. The New Relic service provider supports the following options. When configuring, be aware that: Only simple filtering is supported. The eq operator may be used with a basic filter expression. For example, “displayName eq \"Example Group 1”. Other operators are not supported. PUT updates do not require that all fields be included. Fields that are not included will not be changed. When doing a PUT, if a required field already has the appropriate value, it is not necessary to include the field. Supported actions are: Create group Example request: POST /Groups { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"displayName\": \"Example Group 1\", \"members\": [] } Copy Example response: 201 Created { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"d0652232-b14f-434d-9c6f-36de7e1ab010\", \"displayName\": \"Example Group 1\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2019-11-08T21:33:13.055Z\", \"lastModified\": \"2019-11-08T21:33:13.055Z\" }, \"members\": [] } Copy Get group Example request: GET /Groups/YOUR_GROUP_ID Copy Example response: 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"d0652232-b14f-434d-9c6f-36de7e1ab010\", \"displayName\": \"Example Group 1\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2019-11-08T21:33:13.055Z\", \"lastModified\": \"2019-11-08T21:33:13.055Z\" }, \"members\": [] } Copy Get group by query Example request: GET /Groups?filter=displayName eq \"Example Group 1\" Copy Example response: 200 OK { \"totalResults\": 1, \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:ListResponse\" ], \"Resources\": [{ \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"d0652232-b14f-434d-9c6f-36de7e1ab010\", \"displayName\": \"Example Group 1\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2019-11-08T21:33:13.055Z\", \"lastModified\": \"2019-11-08T21:33:13.055Z\" }, \"members\": [] }] } Copy Update group with PUT In the request, include the fields that you want to change. If you include the members field, the group’s users will be adjusted to match the contents of the members field. Example request: PUT /Groups/YOUR_GROUP_ID { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"displayName\": \"Example Group 1a\" } Copy Example response: 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"d0652232-b14f-434d-9c6f-36de7e1ab010\", \"displayName\": \"Example Group 1a\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2019-11-08T21:33:13.055Z\", \"lastModified\": \"2019-11-08T22:47:14.019Z\" }, \"members\": [] } Copy Update group with PATCH (Non-member fields) Example request: PATCH /Groups/YOUR_GROUP_ID { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:PatchOp\" ], \"Operations\": [{ \"op\": \"Replace\", \"path\": \"displayName\", \"value\": \"Example Group 1b\" }] } Copy Example response: 204 No Content Copy Update group with PATCH (Add members) Example request: PATCH /Groups/YOUR_GROUP_ID { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:PatchOp\" ], \"Operations\": [{ \"op\": \"Add\", \"path\": \"members\", \"value\": [{ \"value\": \"f0cbc276-16c9-4d1a-abc0-1856b0c74224\" }] }] } Copy Example response: 204 No Content Copy Update group with PATCH (Remove members) Example request: PATCH /Groups/YOUR_GROUP_ID { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:PatchOp\" ], \"Operations\": [{ \"op\": \"Remove\", \"path\": \"members\", \"value\": [{ \"value\": \"f0cbc276-16c9-4d1a-abc0-1856b0c74224\" }] }] } Copy Example response: 204 No Content Copy Delete group Example request: DELETE /Groups/YOUR_GROUP_ID Copy Example response: 204 No Content Copy Create user Example request: POST /Users { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\" ], \"externalId\": \"external-id-1\", \"userName\": \"example-user-1@example.com\", \"name\": { \"familyName\": \"User 1\", \"givenName\": \"Example\" }, \"emails\": [{ \"value\": \"example-user-1@example.com\", \"primary\": true }], \"timezone\": \"America/Los_Angeles\", \"active\": true, \"groups\": [] } Copy Example response: 201 Created { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\" ], \"id\": \"f0cbc276-16c9-4d1a-abc0-1856b0c74224\", \"externalId\": \"external-id-1\", \"userName\": \"example-user-1@example.com\", \"name\": { \"familyName\": \"User 1\", \"givenName\": \"Example\" }, \"emails\": [{ \"value\": \"example-user-1@example.com\", \"primary\": true }], \"timezone\": \"America/Los_Angeles\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2019-11-08T22:07:12.477Z\", \"lastModified\": \"2019-11-08T22:07:12.477Z\" }, \"groups\": [] } Copy Get user Example request: GET /Users/YOUR_USER_ID Copy Example response: 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\" ], \"id\": \"f0cbc276-16c9-4d1a-abc0-1856b0c74224\", \"externalId\": \"external-id-1\", \"userName\": \"example-user-1@example.com\", \"name\": { \"familyName\": \"User 1\", \"givenName\": \"Example\" }, \"emails\": [{ \"value\": \"example-user-1@example.com\", \"primary\": true }], \"timezone\": \"America/Los_Angeles\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2019-11-08T22:07:12.477Z\", \"lastModified\": \"2019-11-08T22:07:12.477Z\" }, \"groups\": [] } Copy Get user by query Example request: GET /Users?filter=externalId eq \"external-id-1\" Copy Example response: 200 OK { \"totalResults\": 1, \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:ListResponse\" ], \"Resources\": [{ \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\" ], \"id\": \"f0cbc276-16c9-4d1a-abc0-1856b0c74224\", \"externalId\": \"external-id-1\", \"userName\": \"example-user-1@example.com\", \"name\": { \"familyName\": \"User 1\", \"givenName\": \"Example\" }, \"emails\": [{ \"value\": \"example-user-1@example.com\", \"primary\": true }], \"timezone\": \"America/Los_Angeles\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2019-11-08T22:07:12.477Z\", \"lastModified\": \"2019-11-08T22:07:12.477Z\" }, \"groups\": [] }] } Copy Update user with PUT In the request, include the fields that you want to change. If you include the groups field, the user’s groups will be adjusted to match the contents of the groups field. Example request: PUT /Users/YOUR_USER_ID { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\" ], \"name\": { \"familyName\": \"User 1A\", \"givenName\": \"Example\" } } Copy Example response: 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\" ], \"id\": \"f0cbc276-16c9-4d1a-abc0-1856b0c74224\", \"externalId\": \"external-id-1\", \"userName\": \"example-user-1@example.com\", \"name\": { \"familyName\": \"User 1A\", \"givenName\": \"Example\" }, \"emails\": [{ \"value\": \"example-user-1@example.com\", \"primary\": true }], \"timezone\": \"America/Los_Angeles\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2019-11-08T22:07:12.477Z\", \"lastModified\": \"2019-11-08T22:28:33.552Z\" }, \"groups\": [] } Copy Update user with PATCH Example request: PATCH /Users/YOUR_USER_ID { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:PatchOp\" ], \"Operations\": [{ \"op\": \"Replace\", \"path\": \"active\", \"value\": \"false\" }] } Copy Example response: 204 No Content Copy Delete user Example request: DELETE /Users/YOUR_USER_ID Copy Example response: 204 No Content Copy Deviations from the RFC This section describes areas where the New Relic SCIM service provider deviates from the SCIM protocol RFC 7644. Section numbers refer to RFC section numbers. Items in this section could change as we work to bring our service provider into full compliance with the RFC. RFC section name RFC section number Deviation description Creating Resources 3.3. The meta.location field is not set. Filtering 3.4.2.2. The only currently supported operator is eq. Field names are case sensitive. String attributes are compared in a case sensitive manner. Prefixing the field name with the schema is not supported. For example, filter=urn:ietf:params:scim:schemas:core:2.0:User:userName eq \"johnsmith\" will not work. /Me Authenticated Subject Alias 3.11. GET with the /Me resource returns a 404 Not Found. Service Provider Configuration Endpoints 4. The service provider feature discovery endpoints do not support filtering. Bearer Token and Cookie Considerations 7.4. Bearer tokens do not have a set expiration date. Next steps when you're done Change your users' user type When your users are provisioned in New Relic, you can see them in the User management UI. If you're adding users to New Relic via SCIM but not managing their user type via SCIM, they start out as basic users. To convert users to full platform users, you have two options: Use the User management UI. Use the SCIM API to manage user type. Assign access grants Once your users are in New Relic, you need to grant them access to specific New Relic accounts, specific groups, and specific roles. Without doing this, your users have no access to New Relic accounts. To learn how to do this, see: How access grants work The user management tutorial",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 228.91454,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Automated</em> <em>user</em> <em>management</em>: SCIM API",
        "sections": "<em>Automated</em> <em>user</em> <em>management</em>: SCIM API",
        "tags": "<em>Automated</em> <em>user</em> <em>management</em>",
        "body": "If you want to implement New Relic&#x27;s <em>automated</em> <em>user</em> <em>management</em> (AUM) and import your users from an identity provider, first read Introduction to AUM to learn about supported identity providers and when you&#x27;d want to use our SCIM API, documented below. Requirements If you have an identity provider"
      },
      "id": "6043f38a64441f7d39378f0b"
    },
    {
      "sections": [
        "Azure AD SCIM/SSO application configuration",
        "Requirements",
        "Step 1. Create authentication domain and enable SCIM",
        "Step 2. Set up Azure's New Relic app",
        "Step 3. Configure connection",
        "Step 4. Configure provisioning rules",
        "Tip",
        "Step 5. Set your users' user type",
        "Step 6. Assign access grants",
        "Step 7. Configure SAML SSO"
      ],
      "title": "Azure AD SCIM/SSO application configuration",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Automated user management"
      ],
      "external_id": "d6e7f7e95daa833451159a3db4e2c4257270b5e9",
      "image": "https://docs.newrelic.com/static/0a9a32fd5041e6e2ea37cc5f032b6910/8c557/Azure_AD_Provisioning_Attribute_Mapping_2_0.png",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/automated-user-management/azure-ad-scimsso-application-configuration/",
      "published_at": "2021-12-19T15:39:27Z",
      "updated_at": "2021-11-24T11:24:35Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our automated user management (AUM) allows allows you to import and configure your New Relic users from your identity provider via SCIM. This guide provides Azure AD-specific details on how to configure the New Relic Azure AD SCIM/SSO application. Requirements Before using this guide, read the requirements and procedure overview. Note that these instructions require going back and forth between your identity provider and New Relic. Step 1. Create authentication domain and enable SCIM To get to the New Relic authentication domain UI: From one.newrelic.com, click the account dropdown, click Organization and access, and then click Authentication domains. If you don't already have one, create a new authentication domain for your SCIM-provisioned users by clicking + Add new. For that authentication domain, under Source of users, select SCIM. Copy and save the API token for later use. Note that this will be shown only once. Step 2. Set up Azure's New Relic app Next, you'll set up Azure's New Relic SAML/SCIM app. Azure AD provides an application gallery, which includes various integrations for Azure AD, including the ones that New Relic offers. To set this up: Go to the Azure Active Directory admin center, and sign in if necessary. aad.portal.azure.com/ Click on All services in the left hand menu. In the main pane, click on Enterprise applications. Click on +New Application. Find our SCIM/SSO application by entering New Relic in the name search box, and click on the application New Relic by organization (not New Relic by account). Click on Add. Step 3. Configure connection Configure the New Relic SCIM/SSO application to automatically provision your users to New Relic. From the New Relic SCIM/SSO application page, click on the Provisioning link in the sidebar. In the main pane, click on Get started. In the Provisioning Mode pick-list, choose Automatic. In New Relic's authentication domain UI, set up a new domain with SCIM enabled. In Azure AD's New Relic SCIM/SSO app, in the Admin credentials section, fill out the Tenant URL and Secret token fields with the values provided in New Relic's authentication domain UI. To verify you can connect to New Relic, click Test Connection. When you see a message indicating verification success, click Save. The New Relic SCIM/SSO application can now connect with New Relic. Continue with the following section to configure the provisioning rules. Step 4. Configure provisioning rules Initially, nothing is configured to be sent to New Relic. You must configure Azure AD to send changes for user creation, updates, and deactivation. Go to the Provisioning page and complete the following: Expand the Mappings section. Click Provision Azure Active Directory Users. Verify the Target Object Actions Create Update and Delete checkboxes are all checked. Verify the Attribute Mappings look correct for your environment. Each of the New Relic attributes shown in the list must receive a value. Tip Ensure that the Azure Active Directory attributes shown in the list on the left are good sources for the information to send to New Relic. In particular, not all environments set the mail attribute. If your environment does not set the mail attribute, userPrincipalName could be a good alternative. Leave the switch for Enabled set to Off until you're done with the user and group configuration in the next section. Once all configuration is ready, return to this page and set the switch to On. Click Save. Here's an example of a filled-in attribute mapping page with the default values. Your values may be configured differently depending on your situation. After saving the provisioning rules, the New Relic SCIM/SSO application is ready to provision any changes made to users assigned to the application. Continue with the following section to assign users and groups to the New Relic SCIM/SSO application. Step 5. Set your users' user type When your users are provisioned in New Relic, you're able to see them in the User management UI. If you're adding users to New Relic via SCIM but not managing their user type via SCIM, they start out as basic users. To convert users to full platform users, you have two options: Use the User management UI to edit users. Configure the Azure app to manage user type. Step 6. Assign access grants Once these steps are completed, you should be able to see your users in New Relic by going to the User management UI. Now that your users are present in New Relic, you must grant them access to specific roles on specific accounts. If this is not done, your users don't yet have access to New Relic. To learn how to do this, see: How access grants work The user management tutorial Step 7. Configure SAML SSO To enable SAML SSO for your users, see the SAML instructions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 228.91196,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Step 5. Set your <em>users</em>&#x27; <em>user</em> type",
        "tags": "<em>Automated</em> <em>user</em> <em>management</em>",
        "body": "Our <em>automated</em> <em>user</em> <em>management</em> (AUM) allows allows you to import and configure your New Relic users from your identity provider via SCIM. This guide provides Azure AD-specific details on how to configure the New Relic Azure AD SCIM&#x2F;SSO application. Requirements Before using this guide, read"
      },
      "id": "6043f5c964441fcfb0378ef3"
    },
    {
      "sections": [
        "OneLogin SCIM/SSO application configuration",
        "Requirements",
        "Step 1. Create authentication domain and enable SCIM",
        "Step 2. Set up OneLogin's New Relic app",
        "Step 3. Configure SCIM/SSO application",
        "Fill in the configuration form",
        "Fill in the rules form",
        "A rule that only uses actions",
        "Fill in the provisioning form",
        "Tip",
        "Fill in the Parameters form",
        "Save your changes",
        "Step 4. Assign users",
        "Step 5. Set your users' user type",
        "Step 6. Assign access grants",
        "Step 7. Configure SAML SSO"
      ],
      "title": "OneLogin SCIM/SSO application configuration",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Automated user management"
      ],
      "external_id": "d298162c055490c99117f564c3ea9c2ea5dfd8d1",
      "image": "https://docs.newrelic.com/static/8f585557ea58f70d94a746e6439bf1ad/c1b63/onelogin_rules_conditions_actions.png",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/automated-user-management/onelogin-scimsso-application-configuration/",
      "published_at": "2021-12-19T15:39:45Z",
      "updated_at": "2021-11-24T11:24:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our automated user management (AUM) allows allows you to import and configure your New Relic users from your identity provider via SCIM. This guide provides OneLogin-specific details on how to configure the New Relic OneLogin SCIM/SSO application. Requirements Before using this guide, read our AUM requirements. Note that these instructions require going back and forth between your identity provider and New Relic. Step 1. Create authentication domain and enable SCIM To get to the New Relic authentication domain UI: From one.newrelic.com, click the account dropdown, click Organization and access, and then click Authentication domains. If you don't already have one, create a new authentication domain for your SCIM-provisioned users by clicking + Add new. For that authentication domain, under Source of users, select SCIM. Copy and save the API token for later use. Note that this will be shown only once. Step 2. Set up OneLogin's New Relic app Next, you'll be setting up OneLogin's New Relic SAML/SCIM app. To set this up: Go to the OneLogin web site and sign in with an account that has administrator permissions. From the OneLogin home page, click on Administration. From the OneLogin Administration page, choose the Applications menu. From the OneLogin Applications page, click on Add App. In the search field on the OneLogin Find Applications page, enter \"New Relic by organization\" (not \"New Relic by account\") and then click on the application when it shows in the search results. From the Add New Relic by Organization page, click on Save. Step 3. Configure SCIM/SSO application Configuration for the New Relic SCIM/SSO application is split across several forms. This section describes the different forms that need to be configured. From the New Relic by organization application page, fill in the following forms: Fill in the configuration form In the left pane, select Configuration and complete the following: Get the authentication domain ID (top of New Relic's authentication domain UI and SCIM bearer token (in authentication domain UI as \"SAML 2.0 endpoint) and input them into the appropriate fields in the OneLogin app. Leave the API Connection disabled until all the configuration described in the following sections is completed. After completing all the configuration, enable the connection. Fill in the rules form Configure the user groups to send to New Relic using rules. OneLogin provides this documentation which describes how to use rules to provision groups for users. Decide what type of groups to send along with your users to New Relic. If your organization is using Active Directory or LDAP, you might choose to use security groups to define your users capabilities at New Relic. Another reasonable group choice is OneLogin role. On the New Relic side, your user's groups define their capabilities. The groups that are sent with users will be mapped to New Relic capability groups. Note that at the moment, there is no way to delete a group from the OneLogin side. This is a known limitation from OneLogin. Removing or changing rules does not delete groups already sent to New Relic. If you wish to no longer use a group, removing all the users from the group will prevent it from being used at New Relic. A rule that only uses actions Here's an example rule configuration does not use any conditions. The conditions are left empty to avoid applying any filtering logic to the users. All users will be sent in this example. If you want to send only a subset of users, you need to specify conditions to select the subset. The actions describe where to retrieve the value for the group name and how to parse the value. In this example, we retrieve the group name from the OneLogin role field. The OneLogin role field only has a single value, but sometimes the source for the group name contains other fields besides group name. In other words, some sources give you a list of fields and values and only one of those fields has the value you want to use. In this case, you can insert a regular expression in with value that matches field to find and extract the value for the group name. This example uses the entire value of the For each field for the group name. Fill in the provisioning form In the left pane, select Provisioning and complete the following: Check Enable provisioning. Under Require admin approval before this action is performed, uncheck these options: Create user Delete user Update user Tip If you do not uncheck these options, SCIM provisioning requests will not be sent until an administrator approves them. Set When users are deleted in OneLogin, or the user's app access is removed, perform the below action to Delete. Set When user accounts are suspended in OneLogin, perform the following action to Suspend. Fill in the Parameters form In the left pane, select Parameters and complete the following: Click Groups field. Check Include in User Provisioning. Click Save. Save your changes After you complete the above forms, click Save. Then, return to the Configuration form and enable the API connection. Step 4. Assign users After New Relic SCIM/SSO application configuration is finished and New Relic side configuration is finished, you can begin to assign users to the application. Assign the New Relic SCIM/SSO application to a user: Go to the OneLogin web site and sign in with an account that has administrator permissions. From the OneLogin home page, click Administration. From the OneLogin Administration page, choose the Users menu Users item. From the OneLogin Users page, click the user you want to assign the application to. From the user's page, click Applications. From the user's application page, click the plus sign and select the \"New Relic by Organization\" application. Important: Updating users' time zones is important, as charts and other user assets display times. Default is UMT. From the Edit New Relic by Organization login for user page, enter the user's time zone in IANA Time Zone database format (also known as the Olson time zone database format) and click Save. If you're using Roles to define your New Relic capability groups, from the user's application page, click the proper role(s) for the user and then click Save User. Step 5. Set your users' user type When your users are provisioned in New Relic, you're able to see them in the User management UI. If you're adding users to New Relic via SCIM but not managing their user type via SCIM, they start out as basic users. To convert users to full platform users, you have two options: Use the User management UI to edit users. Configure the OneLogin app to manage user type. Step 6. Assign access grants Once these steps are completed, you should be able to see your users in New Relic by going to the User management UI. Now that your users are present in New Relic, you must grant them access to specific roles on specific accounts. If this is not done, your users don't yet have access to New Relic. To learn how to do this, see: How access grants work The user management tutorial Step 7. Configure SAML SSO To enable SAML SSO for your users, see the SAML instructions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 228.9114,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Step 4. Assign <em>users</em>",
        "tags": "<em>Automated</em> <em>user</em> <em>management</em>",
        "body": "Our <em>automated</em> <em>user</em> <em>management</em> (AUM) allows allows you to import and configure your New Relic users from your identity provider via SCIM. This guide provides OneLogin-specific details on how to configure the New Relic OneLogin SCIM&#x2F;SSO application. Requirements Before using this guide, read our AUM"
      },
      "id": "6043f34228ccbccafb2c606a"
    }
  ],
  "/docs/accounts/accounts/automated-user-management/onelogin-scimsso-application-configuration": [
    {
      "sections": [
        "Automated user management: SCIM API",
        "Requirements",
        "Tutorial",
        "SCIM service provider",
        "Authentication",
        "Supported resources",
        "Schemas",
        "New Relic user type (basic vs full platform) schema",
        "Supported actions",
        "Create group",
        "Get group",
        "Get group by query",
        "Update group with PUT",
        "Update group with PATCH (Non-member fields)",
        "Update group with PATCH (Add members)",
        "Update group with PATCH (Remove members)",
        "Delete group",
        "Create user",
        "Get user",
        "Get user by query",
        "Update user with PUT",
        "Update user with PATCH",
        "Delete user",
        "Deviations from the RFC",
        "Next steps when you're done",
        "Change your users' user type",
        "Assign access grants"
      ],
      "title": "Automated user management: SCIM API",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Automated user management"
      ],
      "external_id": "9e7992bec73759ba6c820721101618de28859b33",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/automated-user-management/scim-support-automated-user-management/",
      "published_at": "2021-12-19T15:40:05Z",
      "updated_at": "2021-11-24T11:26:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you want to implement New Relic's automated user management (AUM) and import your users from an identity provider, first read Introduction to AUM to learn about supported identity providers and when you'd want to use our SCIM API, documented below. Requirements If you have an identity provider that has a New Relic app (Azure AD, Okta, and OneLogin), see the guides for those. The SCIM API is meant for organizations that either aren't using those identity providers, or that want to use the SCIM API for additional configuration not available with the apps (for example, managing user type). Before using our SCIM API, you must first enable SCIM for an authentication domain. Note that after you set up an integration with the SCIM API, there are next steps to do, including downgrading some users to basic users, and granting user groups access to New Relic accounts. Tutorial See the SCIM API tutorial for more specific instructions on using this API. SCIM service provider New Relic’s SCIM service provider follows the SCIM 2.0 API as described in RFCs 7643 and 7644. You do not need to implement all aspects of the SCIM 2.0 specification to integrate your user information with New Relic. In fact, the New Relic service provider itself does not implement all aspects of the specification. This document describes the features from the specification available for an integration with New Relic. Authentication Authentication requires a bearer token. This bearer token is specific to your New Relic authentication domain and is displayed when first enabling SCIM for an authentication domain. Supported resources The New Relic service provider supports the following SCIM resources: Group , User , Service provider config , Resource type and Schema. Bulk and Search are not supported. For more information on how the RFC describes the resource endpoints, see RFC 7644 SCIM Protocol Specification. Schemas New Relic uses a subset of the available fields in the SCIM core schema. Other SCIM fields are ignored if they are included in incoming requests. The fields used by New Relic are: Group: SCIM field name Description displayName Required. Name of the group. members List of users in the group. User: SCIM Field Name Description externalId Unique identifier for the user used by your system. userName Required. Unique identifier for the user within New Relic’s system. Use the user’s email address. name.familyName Last name of the user. name.givenName First name of the user. emails or emails.value Required. Email address of the user. timezone Time zone of the user in IANA Time Zone database format, also known as the \"Olson\" time zone database format (for exmaple, \"America/Los_Angeles\"). active Required. Boolean indicating whether or not the user should be active or inactive within New Relic. groups List of groups to which the user belongs. New Relic user type (basic vs full platform) schema This is an optional schema extension for New Relic-specific user attributes. Currently this provides control only over a user's user type (basic user versus full platform user). For a tutorial showing how to use this, see the SCIM API tutorial. urn:ietf:params:scim:schemas:extension:newrelic:2.0:User: SCIM field name Description nrUserType The user's type: 'full user' (for full platform user) or 'basic user'. Supported actions SCIM provides several options for manipulating groups and users. The New Relic service provider supports the following options. When configuring, be aware that: Only simple filtering is supported. The eq operator may be used with a basic filter expression. For example, “displayName eq \"Example Group 1”. Other operators are not supported. PUT updates do not require that all fields be included. Fields that are not included will not be changed. When doing a PUT, if a required field already has the appropriate value, it is not necessary to include the field. Supported actions are: Create group Example request: POST /Groups { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"displayName\": \"Example Group 1\", \"members\": [] } Copy Example response: 201 Created { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"d0652232-b14f-434d-9c6f-36de7e1ab010\", \"displayName\": \"Example Group 1\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2019-11-08T21:33:13.055Z\", \"lastModified\": \"2019-11-08T21:33:13.055Z\" }, \"members\": [] } Copy Get group Example request: GET /Groups/YOUR_GROUP_ID Copy Example response: 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"d0652232-b14f-434d-9c6f-36de7e1ab010\", \"displayName\": \"Example Group 1\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2019-11-08T21:33:13.055Z\", \"lastModified\": \"2019-11-08T21:33:13.055Z\" }, \"members\": [] } Copy Get group by query Example request: GET /Groups?filter=displayName eq \"Example Group 1\" Copy Example response: 200 OK { \"totalResults\": 1, \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:ListResponse\" ], \"Resources\": [{ \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"d0652232-b14f-434d-9c6f-36de7e1ab010\", \"displayName\": \"Example Group 1\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2019-11-08T21:33:13.055Z\", \"lastModified\": \"2019-11-08T21:33:13.055Z\" }, \"members\": [] }] } Copy Update group with PUT In the request, include the fields that you want to change. If you include the members field, the group’s users will be adjusted to match the contents of the members field. Example request: PUT /Groups/YOUR_GROUP_ID { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"displayName\": \"Example Group 1a\" } Copy Example response: 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"d0652232-b14f-434d-9c6f-36de7e1ab010\", \"displayName\": \"Example Group 1a\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2019-11-08T21:33:13.055Z\", \"lastModified\": \"2019-11-08T22:47:14.019Z\" }, \"members\": [] } Copy Update group with PATCH (Non-member fields) Example request: PATCH /Groups/YOUR_GROUP_ID { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:PatchOp\" ], \"Operations\": [{ \"op\": \"Replace\", \"path\": \"displayName\", \"value\": \"Example Group 1b\" }] } Copy Example response: 204 No Content Copy Update group with PATCH (Add members) Example request: PATCH /Groups/YOUR_GROUP_ID { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:PatchOp\" ], \"Operations\": [{ \"op\": \"Add\", \"path\": \"members\", \"value\": [{ \"value\": \"f0cbc276-16c9-4d1a-abc0-1856b0c74224\" }] }] } Copy Example response: 204 No Content Copy Update group with PATCH (Remove members) Example request: PATCH /Groups/YOUR_GROUP_ID { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:PatchOp\" ], \"Operations\": [{ \"op\": \"Remove\", \"path\": \"members\", \"value\": [{ \"value\": \"f0cbc276-16c9-4d1a-abc0-1856b0c74224\" }] }] } Copy Example response: 204 No Content Copy Delete group Example request: DELETE /Groups/YOUR_GROUP_ID Copy Example response: 204 No Content Copy Create user Example request: POST /Users { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\" ], \"externalId\": \"external-id-1\", \"userName\": \"example-user-1@example.com\", \"name\": { \"familyName\": \"User 1\", \"givenName\": \"Example\" }, \"emails\": [{ \"value\": \"example-user-1@example.com\", \"primary\": true }], \"timezone\": \"America/Los_Angeles\", \"active\": true, \"groups\": [] } Copy Example response: 201 Created { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\" ], \"id\": \"f0cbc276-16c9-4d1a-abc0-1856b0c74224\", \"externalId\": \"external-id-1\", \"userName\": \"example-user-1@example.com\", \"name\": { \"familyName\": \"User 1\", \"givenName\": \"Example\" }, \"emails\": [{ \"value\": \"example-user-1@example.com\", \"primary\": true }], \"timezone\": \"America/Los_Angeles\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2019-11-08T22:07:12.477Z\", \"lastModified\": \"2019-11-08T22:07:12.477Z\" }, \"groups\": [] } Copy Get user Example request: GET /Users/YOUR_USER_ID Copy Example response: 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\" ], \"id\": \"f0cbc276-16c9-4d1a-abc0-1856b0c74224\", \"externalId\": \"external-id-1\", \"userName\": \"example-user-1@example.com\", \"name\": { \"familyName\": \"User 1\", \"givenName\": \"Example\" }, \"emails\": [{ \"value\": \"example-user-1@example.com\", \"primary\": true }], \"timezone\": \"America/Los_Angeles\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2019-11-08T22:07:12.477Z\", \"lastModified\": \"2019-11-08T22:07:12.477Z\" }, \"groups\": [] } Copy Get user by query Example request: GET /Users?filter=externalId eq \"external-id-1\" Copy Example response: 200 OK { \"totalResults\": 1, \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:ListResponse\" ], \"Resources\": [{ \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\" ], \"id\": \"f0cbc276-16c9-4d1a-abc0-1856b0c74224\", \"externalId\": \"external-id-1\", \"userName\": \"example-user-1@example.com\", \"name\": { \"familyName\": \"User 1\", \"givenName\": \"Example\" }, \"emails\": [{ \"value\": \"example-user-1@example.com\", \"primary\": true }], \"timezone\": \"America/Los_Angeles\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2019-11-08T22:07:12.477Z\", \"lastModified\": \"2019-11-08T22:07:12.477Z\" }, \"groups\": [] }] } Copy Update user with PUT In the request, include the fields that you want to change. If you include the groups field, the user’s groups will be adjusted to match the contents of the groups field. Example request: PUT /Users/YOUR_USER_ID { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\" ], \"name\": { \"familyName\": \"User 1A\", \"givenName\": \"Example\" } } Copy Example response: 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\" ], \"id\": \"f0cbc276-16c9-4d1a-abc0-1856b0c74224\", \"externalId\": \"external-id-1\", \"userName\": \"example-user-1@example.com\", \"name\": { \"familyName\": \"User 1A\", \"givenName\": \"Example\" }, \"emails\": [{ \"value\": \"example-user-1@example.com\", \"primary\": true }], \"timezone\": \"America/Los_Angeles\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2019-11-08T22:07:12.477Z\", \"lastModified\": \"2019-11-08T22:28:33.552Z\" }, \"groups\": [] } Copy Update user with PATCH Example request: PATCH /Users/YOUR_USER_ID { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:PatchOp\" ], \"Operations\": [{ \"op\": \"Replace\", \"path\": \"active\", \"value\": \"false\" }] } Copy Example response: 204 No Content Copy Delete user Example request: DELETE /Users/YOUR_USER_ID Copy Example response: 204 No Content Copy Deviations from the RFC This section describes areas where the New Relic SCIM service provider deviates from the SCIM protocol RFC 7644. Section numbers refer to RFC section numbers. Items in this section could change as we work to bring our service provider into full compliance with the RFC. RFC section name RFC section number Deviation description Creating Resources 3.3. The meta.location field is not set. Filtering 3.4.2.2. The only currently supported operator is eq. Field names are case sensitive. String attributes are compared in a case sensitive manner. Prefixing the field name with the schema is not supported. For example, filter=urn:ietf:params:scim:schemas:core:2.0:User:userName eq \"johnsmith\" will not work. /Me Authenticated Subject Alias 3.11. GET with the /Me resource returns a 404 Not Found. Service Provider Configuration Endpoints 4. The service provider feature discovery endpoints do not support filtering. Bearer Token and Cookie Considerations 7.4. Bearer tokens do not have a set expiration date. Next steps when you're done Change your users' user type When your users are provisioned in New Relic, you can see them in the User management UI. If you're adding users to New Relic via SCIM but not managing their user type via SCIM, they start out as basic users. To convert users to full platform users, you have two options: Use the User management UI. Use the SCIM API to manage user type. Assign access grants Once your users are in New Relic, you need to grant them access to specific New Relic accounts, specific groups, and specific roles. Without doing this, your users have no access to New Relic accounts. To learn how to do this, see: How access grants work The user management tutorial",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 228.91452,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Automated</em> <em>user</em> <em>management</em>: SCIM API",
        "sections": "<em>Automated</em> <em>user</em> <em>management</em>: SCIM API",
        "tags": "<em>Automated</em> <em>user</em> <em>management</em>",
        "body": "If you want to implement New Relic&#x27;s <em>automated</em> <em>user</em> <em>management</em> (AUM) and import your users from an identity provider, first read Introduction to AUM to learn about supported identity providers and when you&#x27;d want to use our SCIM API, documented below. Requirements If you have an identity provider"
      },
      "id": "6043f38a64441f7d39378f0b"
    },
    {
      "sections": [
        "Azure AD SCIM/SSO application configuration",
        "Requirements",
        "Step 1. Create authentication domain and enable SCIM",
        "Step 2. Set up Azure's New Relic app",
        "Step 3. Configure connection",
        "Step 4. Configure provisioning rules",
        "Tip",
        "Step 5. Set your users' user type",
        "Step 6. Assign access grants",
        "Step 7. Configure SAML SSO"
      ],
      "title": "Azure AD SCIM/SSO application configuration",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Automated user management"
      ],
      "external_id": "d6e7f7e95daa833451159a3db4e2c4257270b5e9",
      "image": "https://docs.newrelic.com/static/0a9a32fd5041e6e2ea37cc5f032b6910/8c557/Azure_AD_Provisioning_Attribute_Mapping_2_0.png",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/automated-user-management/azure-ad-scimsso-application-configuration/",
      "published_at": "2021-12-19T15:39:27Z",
      "updated_at": "2021-11-24T11:24:35Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our automated user management (AUM) allows allows you to import and configure your New Relic users from your identity provider via SCIM. This guide provides Azure AD-specific details on how to configure the New Relic Azure AD SCIM/SSO application. Requirements Before using this guide, read the requirements and procedure overview. Note that these instructions require going back and forth between your identity provider and New Relic. Step 1. Create authentication domain and enable SCIM To get to the New Relic authentication domain UI: From one.newrelic.com, click the account dropdown, click Organization and access, and then click Authentication domains. If you don't already have one, create a new authentication domain for your SCIM-provisioned users by clicking + Add new. For that authentication domain, under Source of users, select SCIM. Copy and save the API token for later use. Note that this will be shown only once. Step 2. Set up Azure's New Relic app Next, you'll set up Azure's New Relic SAML/SCIM app. Azure AD provides an application gallery, which includes various integrations for Azure AD, including the ones that New Relic offers. To set this up: Go to the Azure Active Directory admin center, and sign in if necessary. aad.portal.azure.com/ Click on All services in the left hand menu. In the main pane, click on Enterprise applications. Click on +New Application. Find our SCIM/SSO application by entering New Relic in the name search box, and click on the application New Relic by organization (not New Relic by account). Click on Add. Step 3. Configure connection Configure the New Relic SCIM/SSO application to automatically provision your users to New Relic. From the New Relic SCIM/SSO application page, click on the Provisioning link in the sidebar. In the main pane, click on Get started. In the Provisioning Mode pick-list, choose Automatic. In New Relic's authentication domain UI, set up a new domain with SCIM enabled. In Azure AD's New Relic SCIM/SSO app, in the Admin credentials section, fill out the Tenant URL and Secret token fields with the values provided in New Relic's authentication domain UI. To verify you can connect to New Relic, click Test Connection. When you see a message indicating verification success, click Save. The New Relic SCIM/SSO application can now connect with New Relic. Continue with the following section to configure the provisioning rules. Step 4. Configure provisioning rules Initially, nothing is configured to be sent to New Relic. You must configure Azure AD to send changes for user creation, updates, and deactivation. Go to the Provisioning page and complete the following: Expand the Mappings section. Click Provision Azure Active Directory Users. Verify the Target Object Actions Create Update and Delete checkboxes are all checked. Verify the Attribute Mappings look correct for your environment. Each of the New Relic attributes shown in the list must receive a value. Tip Ensure that the Azure Active Directory attributes shown in the list on the left are good sources for the information to send to New Relic. In particular, not all environments set the mail attribute. If your environment does not set the mail attribute, userPrincipalName could be a good alternative. Leave the switch for Enabled set to Off until you're done with the user and group configuration in the next section. Once all configuration is ready, return to this page and set the switch to On. Click Save. Here's an example of a filled-in attribute mapping page with the default values. Your values may be configured differently depending on your situation. After saving the provisioning rules, the New Relic SCIM/SSO application is ready to provision any changes made to users assigned to the application. Continue with the following section to assign users and groups to the New Relic SCIM/SSO application. Step 5. Set your users' user type When your users are provisioned in New Relic, you're able to see them in the User management UI. If you're adding users to New Relic via SCIM but not managing their user type via SCIM, they start out as basic users. To convert users to full platform users, you have two options: Use the User management UI to edit users. Configure the Azure app to manage user type. Step 6. Assign access grants Once these steps are completed, you should be able to see your users in New Relic by going to the User management UI. Now that your users are present in New Relic, you must grant them access to specific roles on specific accounts. If this is not done, your users don't yet have access to New Relic. To learn how to do this, see: How access grants work The user management tutorial Step 7. Configure SAML SSO To enable SAML SSO for your users, see the SAML instructions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 228.91194,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Step 5. Set your <em>users</em>&#x27; <em>user</em> type",
        "tags": "<em>Automated</em> <em>user</em> <em>management</em>",
        "body": "Our <em>automated</em> <em>user</em> <em>management</em> (AUM) allows allows you to import and configure your New Relic users from your identity provider via SCIM. This guide provides Azure AD-specific details on how to configure the New Relic Azure AD SCIM&#x2F;SSO application. Requirements Before using this guide, read"
      },
      "id": "6043f5c964441fcfb0378ef3"
    },
    {
      "sections": [
        "Okta SCIM/SSO application configuration",
        "Requirements",
        "Step 1. Create authentication domain and enable SCIM",
        "Step 2. Set up Okta's New Relic app",
        "Step 3. Configure provisioning",
        "Step 4. Assign users and groups",
        "Assignments tab",
        "Push groups tab",
        "Step 5. Set your users' user type",
        "Step 6. Assign access grants",
        "Step 7. Configure SAML SSO",
        "Additional considerations",
        "Moving users between groups"
      ],
      "title": "Okta SCIM/SSO application configuration",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Automated user management"
      ],
      "external_id": "7a00399a6ce11aaa2cb52046f994a80f5986c0e4",
      "image": "https://docs.newrelic.com/static/3f3318e1dc8c9049231c207a7b4e5c54/c1b63/okta-add-user-type-to-profile.png",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/automated-user-management/okta-scimsso-application-configuration/",
      "published_at": "2021-12-19T15:39:45Z",
      "updated_at": "2021-11-24T11:23:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our automated user management (AUM) allows allows you to import and configure your New Relic users from your identity provider via SCIM. This guide provides Okta specific details on how to configure the New Relic Okta SCIM/SSO application. Requirements Before using this guide, read our AUM requirements. Note that these instructions require going back and forth between your identity provider and New Relic. Step 1. Create authentication domain and enable SCIM To get to the New Relic authentication domain UI: From one.newrelic.com, click the account dropdown, click Organization and access, and then click Authentication domains. If you don't already have one, create a new authentication domain for your SCIM-provisioned users by clicking + Add new. For that authentication domain, under Source of users, select SCIM. Copy and save the API token for later use. Note that this will be shown only once. Step 2. Set up Okta's New Relic app Next, set up Okta's New Relic SCIM/SSO application: Go to okta.com/ and sign in with an account that has administrator permissions. From the Okta home page, click on Admin. From the Okta admin Dashboard, choose the Applications page. Click Browse app catalog and search for \"New Relic by organization\" (not \"New Relic by account\") and choose that from the results. From the New Relic by Organization page, click on Add. From the Add New Relic by organization page, check the two Application visibility \"Do not display...\" checkboxes and click on Done. We will make the application visible later after configuration is complete and provisioning has begun. Step 3. Configure provisioning Configure Okta's New Relic SCIM/SSO application to automatically provision your users to New Relic: From the app, click on the Provisioning tab. From the Integration form, click on Configure API integration. Check the Enable API integration checkbox. Take the API token you saved in Step 1 and input it in the Okta New Relic app's API token field. Optional: click on Test API credentials to verify a SCIM connection can be established to New Relic. If a connection can be established, a success message is displayed. If a connection was not established, re-enter the API Token and try the test again. Click Save. Note that the save process does a test of the API credentials. If a connection is not established to New Relic, the save will fail. On the newly displayed To App form, click on Edit. Check the Enable checkbox in the Create users, Update user attributes, and Deactivate users sections. Click Save. Step 4. Assign users and groups If you don't already have your user groups set up in Okta, you'll need to create them. These will be the groups that you'll later assign access grants to in New Relic, which will be what gives those groups access to specific roles on specific accounts. To learn how to create groups, see Okta's documentation on groups. Assignments tab Next, you'll assign users. Assigning users is done using two different tabs in the app. We recommend having your New Relic users selected on the Assignments tab and their associated groups selected on the Push groups tab. In the app, click on the Assignments tab. From the Assignments form, click on Assign. From the pop up menu, click on Assign to groups. From the Assign ... to groups form, click on Assign for the group you wish to assign to the application. Highly recommended: Set your users' time zones in Okta. The time zone affects how date/times for that user are shown in New Relic. Users without a time zone configured will be shown in UTC time in New Relic. Time zone is specified in IANA Time Zone database format, also known as the \"Olson\" time zone database format (for example, \"America/Los_Angeles\"). There are several ways in Okta to assign users' time zone, so consult the Okta docs for more information if needed. Here is one way to do this in the Assignments tab: In the Time zone field, enter the default time zone for members of the group. Click on Save and go back. Repeat the steps to add a group until all desired groups have been assigned to the application. Click Done. Push groups tab In the app, click on the Push groups tab. From the Push groups form, click on Push groups. From the pop up menu, click on Find groups by name. From the Push groups to... form, in the search field enter the first few characters of the name of the group you want to send to New Relic. Leave the Push group memberships immediately checkbox checked. Click on your group in the pop up search results list. In the Match result & push action section, No match found should be displayed, meaning that the group does not yet exist at New Relic. Leave the selector set to Create group and leave the default name for the group. The intent here is to have a group of the same name created at New Relic. If this is the last group you wish to send to New Relic, click on Save. Otherwise, if you have more groups to configure, click on Save & add another and repeat the steps to add a group. When you've added one or more groups, you should be able to see the users you've added by going to the User management UI page. Step 5. Set your users' user type When your users are provisioned in New Relic, you're able to see them in the User management UI. If you're adding users to New Relic via SCIM but not managing their user type via SCIM, they start out as basic users. To convert users to full platform users, you have two options: Use the User management UI to edit users. Manage user type from Okta (described below). To manage your users' user type from Okta: Go to the New Relic authentication domain UI and click Enable Manage user type with SCIM. Note that when this is enabled, you can’t manage user type from the New Relic UI and can only manage it from Okta. Go into your Okta instance. The rest of these instructions are done from Okta. Next, you'll configure Okta to be able to send a new attribute nrUserType. Steps: Go to the Profile editor. In the Attributes section, click Add attribute. Set your settings to match the screenshot below. The only two fields that must match exactly are External name (value: nrUserType) and External namespace (value: urn:ietf:params:scim:schemas:extension:newrelic:2.0:User). The variable value can be any value. Next, you'll configure your Okta user profile to have this field. Steps: In the Profile editor, go to Users and click the User (default) profile. Add a new New Relic user type attribute to that profile (see Okta user profile instructions). How you set this will depend on your own setup and preferences for defining user type. Note that the expected values for user type are Basic user for basic user and Full user for full platform user. Below is an example with information filled in. In the People section, define the user type for your users. How you do this will depend on your setup and preferences. For example, you may choose to set this manually by setting each user’s user type, or you may use Okta to manage these in bulk. Next, you’ll set up mapping for that attribute. Steps: In the app's Provisioning section, click Unmapped attributes. Go into edit mode for the unmapped New Relic user type attribute. Configure it based on how you want to set the user type. To learn about why you'd set them as basic users or full platform users, see User type. Learn more about Okta attribute mappings. Step 6. Assign access grants Once these steps are completed, you're able to see your users in New Relic by going to the User management UI. Now that your users are present in New Relic, you must grant them access to specific roles on specific accounts. If this is not done, your users don't yet have access to New Relic. To learn how to do this, see: How access grants work The user management tutorial Step 7. Configure SAML SSO To enable SAML SSO, see the SAML instructions. Additional considerations In this section we discuss other important things to know when using the New Relic SCIM/SSO application. This section includes tips to work around potential issues that could cause undesired results when integrating between Okta and New Relic. Moving users between groups When moving a user between groups, you must manually synchronize the old group's membership with New Relic. This is because Okta does not send a SCIM request to remove a user from a group. So, the admin needs to push the old group's membership to New Relic manually to inform New Relic that the user is no longer a member of the old group. Here are the steps to manually synchronize a group's membership: From the New Relic SCIM/SSO application page, click on the Push groups tab. From the Push groups form, open the pick list on the desired group's button under the Push Status column. From the displayed pick list on the button, click Push now. This causes an immediate synchronization of the group's membership with New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 228.91005,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Step 4. Assign <em>users</em> <em>and</em> groups",
        "tags": "<em>Automated</em> <em>user</em> <em>management</em>",
        "body": "Our <em>automated</em> <em>user</em> <em>management</em> (AUM) allows allows you to import and configure your New Relic users from your identity provider via SCIM. This guide provides Okta specific details on how to configure the New Relic Okta SCIM&#x2F;SSO application. Requirements Before using this guide, read our AUM"
      },
      "id": "6043f5cae7b9d2758b579a0c"
    }
  ],
  "/docs/accounts/accounts/automated-user-management/roles-permissions-automated-user-management": [
    {
      "sections": [
        "Automated user management: SCIM API",
        "Requirements",
        "Tutorial",
        "SCIM service provider",
        "Authentication",
        "Supported resources",
        "Schemas",
        "New Relic user type (basic vs full platform) schema",
        "Supported actions",
        "Create group",
        "Get group",
        "Get group by query",
        "Update group with PUT",
        "Update group with PATCH (Non-member fields)",
        "Update group with PATCH (Add members)",
        "Update group with PATCH (Remove members)",
        "Delete group",
        "Create user",
        "Get user",
        "Get user by query",
        "Update user with PUT",
        "Update user with PATCH",
        "Delete user",
        "Deviations from the RFC",
        "Next steps when you're done",
        "Change your users' user type",
        "Assign access grants"
      ],
      "title": "Automated user management: SCIM API",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Automated user management"
      ],
      "external_id": "9e7992bec73759ba6c820721101618de28859b33",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/automated-user-management/scim-support-automated-user-management/",
      "published_at": "2021-12-19T15:40:05Z",
      "updated_at": "2021-11-24T11:26:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you want to implement New Relic's automated user management (AUM) and import your users from an identity provider, first read Introduction to AUM to learn about supported identity providers and when you'd want to use our SCIM API, documented below. Requirements If you have an identity provider that has a New Relic app (Azure AD, Okta, and OneLogin), see the guides for those. The SCIM API is meant for organizations that either aren't using those identity providers, or that want to use the SCIM API for additional configuration not available with the apps (for example, managing user type). Before using our SCIM API, you must first enable SCIM for an authentication domain. Note that after you set up an integration with the SCIM API, there are next steps to do, including downgrading some users to basic users, and granting user groups access to New Relic accounts. Tutorial See the SCIM API tutorial for more specific instructions on using this API. SCIM service provider New Relic’s SCIM service provider follows the SCIM 2.0 API as described in RFCs 7643 and 7644. You do not need to implement all aspects of the SCIM 2.0 specification to integrate your user information with New Relic. In fact, the New Relic service provider itself does not implement all aspects of the specification. This document describes the features from the specification available for an integration with New Relic. Authentication Authentication requires a bearer token. This bearer token is specific to your New Relic authentication domain and is displayed when first enabling SCIM for an authentication domain. Supported resources The New Relic service provider supports the following SCIM resources: Group , User , Service provider config , Resource type and Schema. Bulk and Search are not supported. For more information on how the RFC describes the resource endpoints, see RFC 7644 SCIM Protocol Specification. Schemas New Relic uses a subset of the available fields in the SCIM core schema. Other SCIM fields are ignored if they are included in incoming requests. The fields used by New Relic are: Group: SCIM field name Description displayName Required. Name of the group. members List of users in the group. User: SCIM Field Name Description externalId Unique identifier for the user used by your system. userName Required. Unique identifier for the user within New Relic’s system. Use the user’s email address. name.familyName Last name of the user. name.givenName First name of the user. emails or emails.value Required. Email address of the user. timezone Time zone of the user in IANA Time Zone database format, also known as the \"Olson\" time zone database format (for exmaple, \"America/Los_Angeles\"). active Required. Boolean indicating whether or not the user should be active or inactive within New Relic. groups List of groups to which the user belongs. New Relic user type (basic vs full platform) schema This is an optional schema extension for New Relic-specific user attributes. Currently this provides control only over a user's user type (basic user versus full platform user). For a tutorial showing how to use this, see the SCIM API tutorial. urn:ietf:params:scim:schemas:extension:newrelic:2.0:User: SCIM field name Description nrUserType The user's type: 'full user' (for full platform user) or 'basic user'. Supported actions SCIM provides several options for manipulating groups and users. The New Relic service provider supports the following options. When configuring, be aware that: Only simple filtering is supported. The eq operator may be used with a basic filter expression. For example, “displayName eq \"Example Group 1”. Other operators are not supported. PUT updates do not require that all fields be included. Fields that are not included will not be changed. When doing a PUT, if a required field already has the appropriate value, it is not necessary to include the field. Supported actions are: Create group Example request: POST /Groups { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"displayName\": \"Example Group 1\", \"members\": [] } Copy Example response: 201 Created { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"d0652232-b14f-434d-9c6f-36de7e1ab010\", \"displayName\": \"Example Group 1\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2019-11-08T21:33:13.055Z\", \"lastModified\": \"2019-11-08T21:33:13.055Z\" }, \"members\": [] } Copy Get group Example request: GET /Groups/YOUR_GROUP_ID Copy Example response: 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"d0652232-b14f-434d-9c6f-36de7e1ab010\", \"displayName\": \"Example Group 1\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2019-11-08T21:33:13.055Z\", \"lastModified\": \"2019-11-08T21:33:13.055Z\" }, \"members\": [] } Copy Get group by query Example request: GET /Groups?filter=displayName eq \"Example Group 1\" Copy Example response: 200 OK { \"totalResults\": 1, \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:ListResponse\" ], \"Resources\": [{ \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"d0652232-b14f-434d-9c6f-36de7e1ab010\", \"displayName\": \"Example Group 1\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2019-11-08T21:33:13.055Z\", \"lastModified\": \"2019-11-08T21:33:13.055Z\" }, \"members\": [] }] } Copy Update group with PUT In the request, include the fields that you want to change. If you include the members field, the group’s users will be adjusted to match the contents of the members field. Example request: PUT /Groups/YOUR_GROUP_ID { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"displayName\": \"Example Group 1a\" } Copy Example response: 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"d0652232-b14f-434d-9c6f-36de7e1ab010\", \"displayName\": \"Example Group 1a\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2019-11-08T21:33:13.055Z\", \"lastModified\": \"2019-11-08T22:47:14.019Z\" }, \"members\": [] } Copy Update group with PATCH (Non-member fields) Example request: PATCH /Groups/YOUR_GROUP_ID { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:PatchOp\" ], \"Operations\": [{ \"op\": \"Replace\", \"path\": \"displayName\", \"value\": \"Example Group 1b\" }] } Copy Example response: 204 No Content Copy Update group with PATCH (Add members) Example request: PATCH /Groups/YOUR_GROUP_ID { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:PatchOp\" ], \"Operations\": [{ \"op\": \"Add\", \"path\": \"members\", \"value\": [{ \"value\": \"f0cbc276-16c9-4d1a-abc0-1856b0c74224\" }] }] } Copy Example response: 204 No Content Copy Update group with PATCH (Remove members) Example request: PATCH /Groups/YOUR_GROUP_ID { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:PatchOp\" ], \"Operations\": [{ \"op\": \"Remove\", \"path\": \"members\", \"value\": [{ \"value\": \"f0cbc276-16c9-4d1a-abc0-1856b0c74224\" }] }] } Copy Example response: 204 No Content Copy Delete group Example request: DELETE /Groups/YOUR_GROUP_ID Copy Example response: 204 No Content Copy Create user Example request: POST /Users { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\" ], \"externalId\": \"external-id-1\", \"userName\": \"example-user-1@example.com\", \"name\": { \"familyName\": \"User 1\", \"givenName\": \"Example\" }, \"emails\": [{ \"value\": \"example-user-1@example.com\", \"primary\": true }], \"timezone\": \"America/Los_Angeles\", \"active\": true, \"groups\": [] } Copy Example response: 201 Created { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\" ], \"id\": \"f0cbc276-16c9-4d1a-abc0-1856b0c74224\", \"externalId\": \"external-id-1\", \"userName\": \"example-user-1@example.com\", \"name\": { \"familyName\": \"User 1\", \"givenName\": \"Example\" }, \"emails\": [{ \"value\": \"example-user-1@example.com\", \"primary\": true }], \"timezone\": \"America/Los_Angeles\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2019-11-08T22:07:12.477Z\", \"lastModified\": \"2019-11-08T22:07:12.477Z\" }, \"groups\": [] } Copy Get user Example request: GET /Users/YOUR_USER_ID Copy Example response: 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\" ], \"id\": \"f0cbc276-16c9-4d1a-abc0-1856b0c74224\", \"externalId\": \"external-id-1\", \"userName\": \"example-user-1@example.com\", \"name\": { \"familyName\": \"User 1\", \"givenName\": \"Example\" }, \"emails\": [{ \"value\": \"example-user-1@example.com\", \"primary\": true }], \"timezone\": \"America/Los_Angeles\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2019-11-08T22:07:12.477Z\", \"lastModified\": \"2019-11-08T22:07:12.477Z\" }, \"groups\": [] } Copy Get user by query Example request: GET /Users?filter=externalId eq \"external-id-1\" Copy Example response: 200 OK { \"totalResults\": 1, \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:ListResponse\" ], \"Resources\": [{ \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\" ], \"id\": \"f0cbc276-16c9-4d1a-abc0-1856b0c74224\", \"externalId\": \"external-id-1\", \"userName\": \"example-user-1@example.com\", \"name\": { \"familyName\": \"User 1\", \"givenName\": \"Example\" }, \"emails\": [{ \"value\": \"example-user-1@example.com\", \"primary\": true }], \"timezone\": \"America/Los_Angeles\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2019-11-08T22:07:12.477Z\", \"lastModified\": \"2019-11-08T22:07:12.477Z\" }, \"groups\": [] }] } Copy Update user with PUT In the request, include the fields that you want to change. If you include the groups field, the user’s groups will be adjusted to match the contents of the groups field. Example request: PUT /Users/YOUR_USER_ID { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\" ], \"name\": { \"familyName\": \"User 1A\", \"givenName\": \"Example\" } } Copy Example response: 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\" ], \"id\": \"f0cbc276-16c9-4d1a-abc0-1856b0c74224\", \"externalId\": \"external-id-1\", \"userName\": \"example-user-1@example.com\", \"name\": { \"familyName\": \"User 1A\", \"givenName\": \"Example\" }, \"emails\": [{ \"value\": \"example-user-1@example.com\", \"primary\": true }], \"timezone\": \"America/Los_Angeles\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2019-11-08T22:07:12.477Z\", \"lastModified\": \"2019-11-08T22:28:33.552Z\" }, \"groups\": [] } Copy Update user with PATCH Example request: PATCH /Users/YOUR_USER_ID { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:PatchOp\" ], \"Operations\": [{ \"op\": \"Replace\", \"path\": \"active\", \"value\": \"false\" }] } Copy Example response: 204 No Content Copy Delete user Example request: DELETE /Users/YOUR_USER_ID Copy Example response: 204 No Content Copy Deviations from the RFC This section describes areas where the New Relic SCIM service provider deviates from the SCIM protocol RFC 7644. Section numbers refer to RFC section numbers. Items in this section could change as we work to bring our service provider into full compliance with the RFC. RFC section name RFC section number Deviation description Creating Resources 3.3. The meta.location field is not set. Filtering 3.4.2.2. The only currently supported operator is eq. Field names are case sensitive. String attributes are compared in a case sensitive manner. Prefixing the field name with the schema is not supported. For example, filter=urn:ietf:params:scim:schemas:core:2.0:User:userName eq \"johnsmith\" will not work. /Me Authenticated Subject Alias 3.11. GET with the /Me resource returns a 404 Not Found. Service Provider Configuration Endpoints 4. The service provider feature discovery endpoints do not support filtering. Bearer Token and Cookie Considerations 7.4. Bearer tokens do not have a set expiration date. Next steps when you're done Change your users' user type When your users are provisioned in New Relic, you can see them in the User management UI. If you're adding users to New Relic via SCIM but not managing their user type via SCIM, they start out as basic users. To convert users to full platform users, you have two options: Use the User management UI. Use the SCIM API to manage user type. Assign access grants Once your users are in New Relic, you need to grant them access to specific New Relic accounts, specific groups, and specific roles. Without doing this, your users have no access to New Relic accounts. To learn how to do this, see: How access grants work The user management tutorial",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 228.91452,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Automated</em> <em>user</em> <em>management</em>: SCIM API",
        "sections": "<em>Automated</em> <em>user</em> <em>management</em>: SCIM API",
        "tags": "<em>Automated</em> <em>user</em> <em>management</em>",
        "body": "If you want to implement New Relic&#x27;s <em>automated</em> <em>user</em> <em>management</em> (AUM) and import your users from an identity provider, first read Introduction to AUM to learn about supported identity providers and when you&#x27;d want to use our SCIM API, documented below. Requirements If you have an identity provider"
      },
      "id": "6043f38a64441f7d39378f0b"
    },
    {
      "sections": [
        "Azure AD SCIM/SSO application configuration",
        "Requirements",
        "Step 1. Create authentication domain and enable SCIM",
        "Step 2. Set up Azure's New Relic app",
        "Step 3. Configure connection",
        "Step 4. Configure provisioning rules",
        "Tip",
        "Step 5. Set your users' user type",
        "Step 6. Assign access grants",
        "Step 7. Configure SAML SSO"
      ],
      "title": "Azure AD SCIM/SSO application configuration",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Automated user management"
      ],
      "external_id": "d6e7f7e95daa833451159a3db4e2c4257270b5e9",
      "image": "https://docs.newrelic.com/static/0a9a32fd5041e6e2ea37cc5f032b6910/8c557/Azure_AD_Provisioning_Attribute_Mapping_2_0.png",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/automated-user-management/azure-ad-scimsso-application-configuration/",
      "published_at": "2021-12-19T15:39:27Z",
      "updated_at": "2021-11-24T11:24:35Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our automated user management (AUM) allows allows you to import and configure your New Relic users from your identity provider via SCIM. This guide provides Azure AD-specific details on how to configure the New Relic Azure AD SCIM/SSO application. Requirements Before using this guide, read the requirements and procedure overview. Note that these instructions require going back and forth between your identity provider and New Relic. Step 1. Create authentication domain and enable SCIM To get to the New Relic authentication domain UI: From one.newrelic.com, click the account dropdown, click Organization and access, and then click Authentication domains. If you don't already have one, create a new authentication domain for your SCIM-provisioned users by clicking + Add new. For that authentication domain, under Source of users, select SCIM. Copy and save the API token for later use. Note that this will be shown only once. Step 2. Set up Azure's New Relic app Next, you'll set up Azure's New Relic SAML/SCIM app. Azure AD provides an application gallery, which includes various integrations for Azure AD, including the ones that New Relic offers. To set this up: Go to the Azure Active Directory admin center, and sign in if necessary. aad.portal.azure.com/ Click on All services in the left hand menu. In the main pane, click on Enterprise applications. Click on +New Application. Find our SCIM/SSO application by entering New Relic in the name search box, and click on the application New Relic by organization (not New Relic by account). Click on Add. Step 3. Configure connection Configure the New Relic SCIM/SSO application to automatically provision your users to New Relic. From the New Relic SCIM/SSO application page, click on the Provisioning link in the sidebar. In the main pane, click on Get started. In the Provisioning Mode pick-list, choose Automatic. In New Relic's authentication domain UI, set up a new domain with SCIM enabled. In Azure AD's New Relic SCIM/SSO app, in the Admin credentials section, fill out the Tenant URL and Secret token fields with the values provided in New Relic's authentication domain UI. To verify you can connect to New Relic, click Test Connection. When you see a message indicating verification success, click Save. The New Relic SCIM/SSO application can now connect with New Relic. Continue with the following section to configure the provisioning rules. Step 4. Configure provisioning rules Initially, nothing is configured to be sent to New Relic. You must configure Azure AD to send changes for user creation, updates, and deactivation. Go to the Provisioning page and complete the following: Expand the Mappings section. Click Provision Azure Active Directory Users. Verify the Target Object Actions Create Update and Delete checkboxes are all checked. Verify the Attribute Mappings look correct for your environment. Each of the New Relic attributes shown in the list must receive a value. Tip Ensure that the Azure Active Directory attributes shown in the list on the left are good sources for the information to send to New Relic. In particular, not all environments set the mail attribute. If your environment does not set the mail attribute, userPrincipalName could be a good alternative. Leave the switch for Enabled set to Off until you're done with the user and group configuration in the next section. Once all configuration is ready, return to this page and set the switch to On. Click Save. Here's an example of a filled-in attribute mapping page with the default values. Your values may be configured differently depending on your situation. After saving the provisioning rules, the New Relic SCIM/SSO application is ready to provision any changes made to users assigned to the application. Continue with the following section to assign users and groups to the New Relic SCIM/SSO application. Step 5. Set your users' user type When your users are provisioned in New Relic, you're able to see them in the User management UI. If you're adding users to New Relic via SCIM but not managing their user type via SCIM, they start out as basic users. To convert users to full platform users, you have two options: Use the User management UI to edit users. Configure the Azure app to manage user type. Step 6. Assign access grants Once these steps are completed, you should be able to see your users in New Relic by going to the User management UI. Now that your users are present in New Relic, you must grant them access to specific roles on specific accounts. If this is not done, your users don't yet have access to New Relic. To learn how to do this, see: How access grants work The user management tutorial Step 7. Configure SAML SSO To enable SAML SSO for your users, see the SAML instructions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 228.91194,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Step 5. Set your <em>users</em>&#x27; <em>user</em> type",
        "tags": "<em>Automated</em> <em>user</em> <em>management</em>",
        "body": "Our <em>automated</em> <em>user</em> <em>management</em> (AUM) allows allows you to import and configure your New Relic users from your identity provider via SCIM. This guide provides Azure AD-specific details on how to configure the New Relic Azure AD SCIM&#x2F;SSO application. Requirements Before using this guide, read"
      },
      "id": "6043f5c964441fcfb0378ef3"
    },
    {
      "sections": [
        "OneLogin SCIM/SSO application configuration",
        "Requirements",
        "Step 1. Create authentication domain and enable SCIM",
        "Step 2. Set up OneLogin's New Relic app",
        "Step 3. Configure SCIM/SSO application",
        "Fill in the configuration form",
        "Fill in the rules form",
        "A rule that only uses actions",
        "Fill in the provisioning form",
        "Tip",
        "Fill in the Parameters form",
        "Save your changes",
        "Step 4. Assign users",
        "Step 5. Set your users' user type",
        "Step 6. Assign access grants",
        "Step 7. Configure SAML SSO"
      ],
      "title": "OneLogin SCIM/SSO application configuration",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Automated user management"
      ],
      "external_id": "d298162c055490c99117f564c3ea9c2ea5dfd8d1",
      "image": "https://docs.newrelic.com/static/8f585557ea58f70d94a746e6439bf1ad/c1b63/onelogin_rules_conditions_actions.png",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/automated-user-management/onelogin-scimsso-application-configuration/",
      "published_at": "2021-12-19T15:39:45Z",
      "updated_at": "2021-11-24T11:24:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our automated user management (AUM) allows allows you to import and configure your New Relic users from your identity provider via SCIM. This guide provides OneLogin-specific details on how to configure the New Relic OneLogin SCIM/SSO application. Requirements Before using this guide, read our AUM requirements. Note that these instructions require going back and forth between your identity provider and New Relic. Step 1. Create authentication domain and enable SCIM To get to the New Relic authentication domain UI: From one.newrelic.com, click the account dropdown, click Organization and access, and then click Authentication domains. If you don't already have one, create a new authentication domain for your SCIM-provisioned users by clicking + Add new. For that authentication domain, under Source of users, select SCIM. Copy and save the API token for later use. Note that this will be shown only once. Step 2. Set up OneLogin's New Relic app Next, you'll be setting up OneLogin's New Relic SAML/SCIM app. To set this up: Go to the OneLogin web site and sign in with an account that has administrator permissions. From the OneLogin home page, click on Administration. From the OneLogin Administration page, choose the Applications menu. From the OneLogin Applications page, click on Add App. In the search field on the OneLogin Find Applications page, enter \"New Relic by organization\" (not \"New Relic by account\") and then click on the application when it shows in the search results. From the Add New Relic by Organization page, click on Save. Step 3. Configure SCIM/SSO application Configuration for the New Relic SCIM/SSO application is split across several forms. This section describes the different forms that need to be configured. From the New Relic by organization application page, fill in the following forms: Fill in the configuration form In the left pane, select Configuration and complete the following: Get the authentication domain ID (top of New Relic's authentication domain UI and SCIM bearer token (in authentication domain UI as \"SAML 2.0 endpoint) and input them into the appropriate fields in the OneLogin app. Leave the API Connection disabled until all the configuration described in the following sections is completed. After completing all the configuration, enable the connection. Fill in the rules form Configure the user groups to send to New Relic using rules. OneLogin provides this documentation which describes how to use rules to provision groups for users. Decide what type of groups to send along with your users to New Relic. If your organization is using Active Directory or LDAP, you might choose to use security groups to define your users capabilities at New Relic. Another reasonable group choice is OneLogin role. On the New Relic side, your user's groups define their capabilities. The groups that are sent with users will be mapped to New Relic capability groups. Note that at the moment, there is no way to delete a group from the OneLogin side. This is a known limitation from OneLogin. Removing or changing rules does not delete groups already sent to New Relic. If you wish to no longer use a group, removing all the users from the group will prevent it from being used at New Relic. A rule that only uses actions Here's an example rule configuration does not use any conditions. The conditions are left empty to avoid applying any filtering logic to the users. All users will be sent in this example. If you want to send only a subset of users, you need to specify conditions to select the subset. The actions describe where to retrieve the value for the group name and how to parse the value. In this example, we retrieve the group name from the OneLogin role field. The OneLogin role field only has a single value, but sometimes the source for the group name contains other fields besides group name. In other words, some sources give you a list of fields and values and only one of those fields has the value you want to use. In this case, you can insert a regular expression in with value that matches field to find and extract the value for the group name. This example uses the entire value of the For each field for the group name. Fill in the provisioning form In the left pane, select Provisioning and complete the following: Check Enable provisioning. Under Require admin approval before this action is performed, uncheck these options: Create user Delete user Update user Tip If you do not uncheck these options, SCIM provisioning requests will not be sent until an administrator approves them. Set When users are deleted in OneLogin, or the user's app access is removed, perform the below action to Delete. Set When user accounts are suspended in OneLogin, perform the following action to Suspend. Fill in the Parameters form In the left pane, select Parameters and complete the following: Click Groups field. Check Include in User Provisioning. Click Save. Save your changes After you complete the above forms, click Save. Then, return to the Configuration form and enable the API connection. Step 4. Assign users After New Relic SCIM/SSO application configuration is finished and New Relic side configuration is finished, you can begin to assign users to the application. Assign the New Relic SCIM/SSO application to a user: Go to the OneLogin web site and sign in with an account that has administrator permissions. From the OneLogin home page, click Administration. From the OneLogin Administration page, choose the Users menu Users item. From the OneLogin Users page, click the user you want to assign the application to. From the user's page, click Applications. From the user's application page, click the plus sign and select the \"New Relic by Organization\" application. Important: Updating users' time zones is important, as charts and other user assets display times. Default is UMT. From the Edit New Relic by Organization login for user page, enter the user's time zone in IANA Time Zone database format (also known as the Olson time zone database format) and click Save. If you're using Roles to define your New Relic capability groups, from the user's application page, click the proper role(s) for the user and then click Save User. Step 5. Set your users' user type When your users are provisioned in New Relic, you're able to see them in the User management UI. If you're adding users to New Relic via SCIM but not managing their user type via SCIM, they start out as basic users. To convert users to full platform users, you have two options: Use the User management UI to edit users. Configure the OneLogin app to manage user type. Step 6. Assign access grants Once these steps are completed, you should be able to see your users in New Relic by going to the User management UI. Now that your users are present in New Relic, you must grant them access to specific roles on specific accounts. If this is not done, your users don't yet have access to New Relic. To learn how to do this, see: How access grants work The user management tutorial Step 7. Configure SAML SSO To enable SAML SSO for your users, see the SAML instructions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 228.91138,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Step 4. Assign <em>users</em>",
        "tags": "<em>Automated</em> <em>user</em> <em>management</em>",
        "body": "Our <em>automated</em> <em>user</em> <em>management</em> (AUM) allows allows you to import and configure your New Relic users from your identity provider via SCIM. This guide provides OneLogin-specific details on how to configure the New Relic OneLogin SCIM&#x2F;SSO application. Requirements Before using this guide, read our AUM"
      },
      "id": "6043f34228ccbccafb2c606a"
    }
  ],
  "/docs/accounts/accounts/automated-user-management/scim-support-automated-user-management": [
    {
      "sections": [
        "Azure AD SCIM/SSO application configuration",
        "Requirements",
        "Step 1. Create authentication domain and enable SCIM",
        "Step 2. Set up Azure's New Relic app",
        "Step 3. Configure connection",
        "Step 4. Configure provisioning rules",
        "Tip",
        "Step 5. Set your users' user type",
        "Step 6. Assign access grants",
        "Step 7. Configure SAML SSO"
      ],
      "title": "Azure AD SCIM/SSO application configuration",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Automated user management"
      ],
      "external_id": "d6e7f7e95daa833451159a3db4e2c4257270b5e9",
      "image": "https://docs.newrelic.com/static/0a9a32fd5041e6e2ea37cc5f032b6910/8c557/Azure_AD_Provisioning_Attribute_Mapping_2_0.png",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/automated-user-management/azure-ad-scimsso-application-configuration/",
      "published_at": "2021-12-19T15:39:27Z",
      "updated_at": "2021-11-24T11:24:35Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our automated user management (AUM) allows allows you to import and configure your New Relic users from your identity provider via SCIM. This guide provides Azure AD-specific details on how to configure the New Relic Azure AD SCIM/SSO application. Requirements Before using this guide, read the requirements and procedure overview. Note that these instructions require going back and forth between your identity provider and New Relic. Step 1. Create authentication domain and enable SCIM To get to the New Relic authentication domain UI: From one.newrelic.com, click the account dropdown, click Organization and access, and then click Authentication domains. If you don't already have one, create a new authentication domain for your SCIM-provisioned users by clicking + Add new. For that authentication domain, under Source of users, select SCIM. Copy and save the API token for later use. Note that this will be shown only once. Step 2. Set up Azure's New Relic app Next, you'll set up Azure's New Relic SAML/SCIM app. Azure AD provides an application gallery, which includes various integrations for Azure AD, including the ones that New Relic offers. To set this up: Go to the Azure Active Directory admin center, and sign in if necessary. aad.portal.azure.com/ Click on All services in the left hand menu. In the main pane, click on Enterprise applications. Click on +New Application. Find our SCIM/SSO application by entering New Relic in the name search box, and click on the application New Relic by organization (not New Relic by account). Click on Add. Step 3. Configure connection Configure the New Relic SCIM/SSO application to automatically provision your users to New Relic. From the New Relic SCIM/SSO application page, click on the Provisioning link in the sidebar. In the main pane, click on Get started. In the Provisioning Mode pick-list, choose Automatic. In New Relic's authentication domain UI, set up a new domain with SCIM enabled. In Azure AD's New Relic SCIM/SSO app, in the Admin credentials section, fill out the Tenant URL and Secret token fields with the values provided in New Relic's authentication domain UI. To verify you can connect to New Relic, click Test Connection. When you see a message indicating verification success, click Save. The New Relic SCIM/SSO application can now connect with New Relic. Continue with the following section to configure the provisioning rules. Step 4. Configure provisioning rules Initially, nothing is configured to be sent to New Relic. You must configure Azure AD to send changes for user creation, updates, and deactivation. Go to the Provisioning page and complete the following: Expand the Mappings section. Click Provision Azure Active Directory Users. Verify the Target Object Actions Create Update and Delete checkboxes are all checked. Verify the Attribute Mappings look correct for your environment. Each of the New Relic attributes shown in the list must receive a value. Tip Ensure that the Azure Active Directory attributes shown in the list on the left are good sources for the information to send to New Relic. In particular, not all environments set the mail attribute. If your environment does not set the mail attribute, userPrincipalName could be a good alternative. Leave the switch for Enabled set to Off until you're done with the user and group configuration in the next section. Once all configuration is ready, return to this page and set the switch to On. Click Save. Here's an example of a filled-in attribute mapping page with the default values. Your values may be configured differently depending on your situation. After saving the provisioning rules, the New Relic SCIM/SSO application is ready to provision any changes made to users assigned to the application. Continue with the following section to assign users and groups to the New Relic SCIM/SSO application. Step 5. Set your users' user type When your users are provisioned in New Relic, you're able to see them in the User management UI. If you're adding users to New Relic via SCIM but not managing their user type via SCIM, they start out as basic users. To convert users to full platform users, you have two options: Use the User management UI to edit users. Configure the Azure app to manage user type. Step 6. Assign access grants Once these steps are completed, you should be able to see your users in New Relic by going to the User management UI. Now that your users are present in New Relic, you must grant them access to specific roles on specific accounts. If this is not done, your users don't yet have access to New Relic. To learn how to do this, see: How access grants work The user management tutorial Step 7. Configure SAML SSO To enable SAML SSO for your users, see the SAML instructions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 228.91193,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Step 5. Set your <em>users</em>&#x27; <em>user</em> type",
        "tags": "<em>Automated</em> <em>user</em> <em>management</em>",
        "body": "Our <em>automated</em> <em>user</em> <em>management</em> (AUM) allows allows you to import and configure your New Relic users from your identity provider via SCIM. This guide provides Azure AD-specific details on how to configure the New Relic Azure AD SCIM&#x2F;SSO application. Requirements Before using this guide, read"
      },
      "id": "6043f5c964441fcfb0378ef3"
    },
    {
      "sections": [
        "OneLogin SCIM/SSO application configuration",
        "Requirements",
        "Step 1. Create authentication domain and enable SCIM",
        "Step 2. Set up OneLogin's New Relic app",
        "Step 3. Configure SCIM/SSO application",
        "Fill in the configuration form",
        "Fill in the rules form",
        "A rule that only uses actions",
        "Fill in the provisioning form",
        "Tip",
        "Fill in the Parameters form",
        "Save your changes",
        "Step 4. Assign users",
        "Step 5. Set your users' user type",
        "Step 6. Assign access grants",
        "Step 7. Configure SAML SSO"
      ],
      "title": "OneLogin SCIM/SSO application configuration",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Automated user management"
      ],
      "external_id": "d298162c055490c99117f564c3ea9c2ea5dfd8d1",
      "image": "https://docs.newrelic.com/static/8f585557ea58f70d94a746e6439bf1ad/c1b63/onelogin_rules_conditions_actions.png",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/automated-user-management/onelogin-scimsso-application-configuration/",
      "published_at": "2021-12-19T15:39:45Z",
      "updated_at": "2021-11-24T11:24:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our automated user management (AUM) allows allows you to import and configure your New Relic users from your identity provider via SCIM. This guide provides OneLogin-specific details on how to configure the New Relic OneLogin SCIM/SSO application. Requirements Before using this guide, read our AUM requirements. Note that these instructions require going back and forth between your identity provider and New Relic. Step 1. Create authentication domain and enable SCIM To get to the New Relic authentication domain UI: From one.newrelic.com, click the account dropdown, click Organization and access, and then click Authentication domains. If you don't already have one, create a new authentication domain for your SCIM-provisioned users by clicking + Add new. For that authentication domain, under Source of users, select SCIM. Copy and save the API token for later use. Note that this will be shown only once. Step 2. Set up OneLogin's New Relic app Next, you'll be setting up OneLogin's New Relic SAML/SCIM app. To set this up: Go to the OneLogin web site and sign in with an account that has administrator permissions. From the OneLogin home page, click on Administration. From the OneLogin Administration page, choose the Applications menu. From the OneLogin Applications page, click on Add App. In the search field on the OneLogin Find Applications page, enter \"New Relic by organization\" (not \"New Relic by account\") and then click on the application when it shows in the search results. From the Add New Relic by Organization page, click on Save. Step 3. Configure SCIM/SSO application Configuration for the New Relic SCIM/SSO application is split across several forms. This section describes the different forms that need to be configured. From the New Relic by organization application page, fill in the following forms: Fill in the configuration form In the left pane, select Configuration and complete the following: Get the authentication domain ID (top of New Relic's authentication domain UI and SCIM bearer token (in authentication domain UI as \"SAML 2.0 endpoint) and input them into the appropriate fields in the OneLogin app. Leave the API Connection disabled until all the configuration described in the following sections is completed. After completing all the configuration, enable the connection. Fill in the rules form Configure the user groups to send to New Relic using rules. OneLogin provides this documentation which describes how to use rules to provision groups for users. Decide what type of groups to send along with your users to New Relic. If your organization is using Active Directory or LDAP, you might choose to use security groups to define your users capabilities at New Relic. Another reasonable group choice is OneLogin role. On the New Relic side, your user's groups define their capabilities. The groups that are sent with users will be mapped to New Relic capability groups. Note that at the moment, there is no way to delete a group from the OneLogin side. This is a known limitation from OneLogin. Removing or changing rules does not delete groups already sent to New Relic. If you wish to no longer use a group, removing all the users from the group will prevent it from being used at New Relic. A rule that only uses actions Here's an example rule configuration does not use any conditions. The conditions are left empty to avoid applying any filtering logic to the users. All users will be sent in this example. If you want to send only a subset of users, you need to specify conditions to select the subset. The actions describe where to retrieve the value for the group name and how to parse the value. In this example, we retrieve the group name from the OneLogin role field. The OneLogin role field only has a single value, but sometimes the source for the group name contains other fields besides group name. In other words, some sources give you a list of fields and values and only one of those fields has the value you want to use. In this case, you can insert a regular expression in with value that matches field to find and extract the value for the group name. This example uses the entire value of the For each field for the group name. Fill in the provisioning form In the left pane, select Provisioning and complete the following: Check Enable provisioning. Under Require admin approval before this action is performed, uncheck these options: Create user Delete user Update user Tip If you do not uncheck these options, SCIM provisioning requests will not be sent until an administrator approves them. Set When users are deleted in OneLogin, or the user's app access is removed, perform the below action to Delete. Set When user accounts are suspended in OneLogin, perform the following action to Suspend. Fill in the Parameters form In the left pane, select Parameters and complete the following: Click Groups field. Check Include in User Provisioning. Click Save. Save your changes After you complete the above forms, click Save. Then, return to the Configuration form and enable the API connection. Step 4. Assign users After New Relic SCIM/SSO application configuration is finished and New Relic side configuration is finished, you can begin to assign users to the application. Assign the New Relic SCIM/SSO application to a user: Go to the OneLogin web site and sign in with an account that has administrator permissions. From the OneLogin home page, click Administration. From the OneLogin Administration page, choose the Users menu Users item. From the OneLogin Users page, click the user you want to assign the application to. From the user's page, click Applications. From the user's application page, click the plus sign and select the \"New Relic by Organization\" application. Important: Updating users' time zones is important, as charts and other user assets display times. Default is UMT. From the Edit New Relic by Organization login for user page, enter the user's time zone in IANA Time Zone database format (also known as the Olson time zone database format) and click Save. If you're using Roles to define your New Relic capability groups, from the user's application page, click the proper role(s) for the user and then click Save User. Step 5. Set your users' user type When your users are provisioned in New Relic, you're able to see them in the User management UI. If you're adding users to New Relic via SCIM but not managing their user type via SCIM, they start out as basic users. To convert users to full platform users, you have two options: Use the User management UI to edit users. Configure the OneLogin app to manage user type. Step 6. Assign access grants Once these steps are completed, you should be able to see your users in New Relic by going to the User management UI. Now that your users are present in New Relic, you must grant them access to specific roles on specific accounts. If this is not done, your users don't yet have access to New Relic. To learn how to do this, see: How access grants work The user management tutorial Step 7. Configure SAML SSO To enable SAML SSO for your users, see the SAML instructions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 228.91136,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Step 4. Assign <em>users</em>",
        "tags": "<em>Automated</em> <em>user</em> <em>management</em>",
        "body": "Our <em>automated</em> <em>user</em> <em>management</em> (AUM) allows allows you to import and configure your New Relic users from your identity provider via SCIM. This guide provides OneLogin-specific details on how to configure the New Relic OneLogin SCIM&#x2F;SSO application. Requirements Before using this guide, read our AUM"
      },
      "id": "6043f34228ccbccafb2c606a"
    },
    {
      "sections": [
        "Okta SCIM/SSO application configuration",
        "Requirements",
        "Step 1. Create authentication domain and enable SCIM",
        "Step 2. Set up Okta's New Relic app",
        "Step 3. Configure provisioning",
        "Step 4. Assign users and groups",
        "Assignments tab",
        "Push groups tab",
        "Step 5. Set your users' user type",
        "Step 6. Assign access grants",
        "Step 7. Configure SAML SSO",
        "Additional considerations",
        "Moving users between groups"
      ],
      "title": "Okta SCIM/SSO application configuration",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Automated user management"
      ],
      "external_id": "7a00399a6ce11aaa2cb52046f994a80f5986c0e4",
      "image": "https://docs.newrelic.com/static/3f3318e1dc8c9049231c207a7b4e5c54/c1b63/okta-add-user-type-to-profile.png",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/automated-user-management/okta-scimsso-application-configuration/",
      "published_at": "2021-12-19T15:39:45Z",
      "updated_at": "2021-11-24T11:23:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our automated user management (AUM) allows allows you to import and configure your New Relic users from your identity provider via SCIM. This guide provides Okta specific details on how to configure the New Relic Okta SCIM/SSO application. Requirements Before using this guide, read our AUM requirements. Note that these instructions require going back and forth between your identity provider and New Relic. Step 1. Create authentication domain and enable SCIM To get to the New Relic authentication domain UI: From one.newrelic.com, click the account dropdown, click Organization and access, and then click Authentication domains. If you don't already have one, create a new authentication domain for your SCIM-provisioned users by clicking + Add new. For that authentication domain, under Source of users, select SCIM. Copy and save the API token for later use. Note that this will be shown only once. Step 2. Set up Okta's New Relic app Next, set up Okta's New Relic SCIM/SSO application: Go to okta.com/ and sign in with an account that has administrator permissions. From the Okta home page, click on Admin. From the Okta admin Dashboard, choose the Applications page. Click Browse app catalog and search for \"New Relic by organization\" (not \"New Relic by account\") and choose that from the results. From the New Relic by Organization page, click on Add. From the Add New Relic by organization page, check the two Application visibility \"Do not display...\" checkboxes and click on Done. We will make the application visible later after configuration is complete and provisioning has begun. Step 3. Configure provisioning Configure Okta's New Relic SCIM/SSO application to automatically provision your users to New Relic: From the app, click on the Provisioning tab. From the Integration form, click on Configure API integration. Check the Enable API integration checkbox. Take the API token you saved in Step 1 and input it in the Okta New Relic app's API token field. Optional: click on Test API credentials to verify a SCIM connection can be established to New Relic. If a connection can be established, a success message is displayed. If a connection was not established, re-enter the API Token and try the test again. Click Save. Note that the save process does a test of the API credentials. If a connection is not established to New Relic, the save will fail. On the newly displayed To App form, click on Edit. Check the Enable checkbox in the Create users, Update user attributes, and Deactivate users sections. Click Save. Step 4. Assign users and groups If you don't already have your user groups set up in Okta, you'll need to create them. These will be the groups that you'll later assign access grants to in New Relic, which will be what gives those groups access to specific roles on specific accounts. To learn how to create groups, see Okta's documentation on groups. Assignments tab Next, you'll assign users. Assigning users is done using two different tabs in the app. We recommend having your New Relic users selected on the Assignments tab and their associated groups selected on the Push groups tab. In the app, click on the Assignments tab. From the Assignments form, click on Assign. From the pop up menu, click on Assign to groups. From the Assign ... to groups form, click on Assign for the group you wish to assign to the application. Highly recommended: Set your users' time zones in Okta. The time zone affects how date/times for that user are shown in New Relic. Users without a time zone configured will be shown in UTC time in New Relic. Time zone is specified in IANA Time Zone database format, also known as the \"Olson\" time zone database format (for example, \"America/Los_Angeles\"). There are several ways in Okta to assign users' time zone, so consult the Okta docs for more information if needed. Here is one way to do this in the Assignments tab: In the Time zone field, enter the default time zone for members of the group. Click on Save and go back. Repeat the steps to add a group until all desired groups have been assigned to the application. Click Done. Push groups tab In the app, click on the Push groups tab. From the Push groups form, click on Push groups. From the pop up menu, click on Find groups by name. From the Push groups to... form, in the search field enter the first few characters of the name of the group you want to send to New Relic. Leave the Push group memberships immediately checkbox checked. Click on your group in the pop up search results list. In the Match result & push action section, No match found should be displayed, meaning that the group does not yet exist at New Relic. Leave the selector set to Create group and leave the default name for the group. The intent here is to have a group of the same name created at New Relic. If this is the last group you wish to send to New Relic, click on Save. Otherwise, if you have more groups to configure, click on Save & add another and repeat the steps to add a group. When you've added one or more groups, you should be able to see the users you've added by going to the User management UI page. Step 5. Set your users' user type When your users are provisioned in New Relic, you're able to see them in the User management UI. If you're adding users to New Relic via SCIM but not managing their user type via SCIM, they start out as basic users. To convert users to full platform users, you have two options: Use the User management UI to edit users. Manage user type from Okta (described below). To manage your users' user type from Okta: Go to the New Relic authentication domain UI and click Enable Manage user type with SCIM. Note that when this is enabled, you can’t manage user type from the New Relic UI and can only manage it from Okta. Go into your Okta instance. The rest of these instructions are done from Okta. Next, you'll configure Okta to be able to send a new attribute nrUserType. Steps: Go to the Profile editor. In the Attributes section, click Add attribute. Set your settings to match the screenshot below. The only two fields that must match exactly are External name (value: nrUserType) and External namespace (value: urn:ietf:params:scim:schemas:extension:newrelic:2.0:User). The variable value can be any value. Next, you'll configure your Okta user profile to have this field. Steps: In the Profile editor, go to Users and click the User (default) profile. Add a new New Relic user type attribute to that profile (see Okta user profile instructions). How you set this will depend on your own setup and preferences for defining user type. Note that the expected values for user type are Basic user for basic user and Full user for full platform user. Below is an example with information filled in. In the People section, define the user type for your users. How you do this will depend on your setup and preferences. For example, you may choose to set this manually by setting each user’s user type, or you may use Okta to manage these in bulk. Next, you’ll set up mapping for that attribute. Steps: In the app's Provisioning section, click Unmapped attributes. Go into edit mode for the unmapped New Relic user type attribute. Configure it based on how you want to set the user type. To learn about why you'd set them as basic users or full platform users, see User type. Learn more about Okta attribute mappings. Step 6. Assign access grants Once these steps are completed, you're able to see your users in New Relic by going to the User management UI. Now that your users are present in New Relic, you must grant them access to specific roles on specific accounts. If this is not done, your users don't yet have access to New Relic. To learn how to do this, see: How access grants work The user management tutorial Step 7. Configure SAML SSO To enable SAML SSO, see the SAML instructions. Additional considerations In this section we discuss other important things to know when using the New Relic SCIM/SSO application. This section includes tips to work around potential issues that could cause undesired results when integrating between Okta and New Relic. Moving users between groups When moving a user between groups, you must manually synchronize the old group's membership with New Relic. This is because Okta does not send a SCIM request to remove a user from a group. So, the admin needs to push the old group's membership to New Relic manually to inform New Relic that the user is no longer a member of the old group. Here are the steps to manually synchronize a group's membership: From the New Relic SCIM/SSO application page, click on the Push groups tab. From the Push groups form, open the pick list on the desired group's button under the Push Status column. From the displayed pick list on the button, click Push now. This causes an immediate synchronization of the group's membership with New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 228.91002,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Step 4. Assign <em>users</em> <em>and</em> groups",
        "tags": "<em>Automated</em> <em>user</em> <em>management</em>",
        "body": "Our <em>automated</em> <em>user</em> <em>management</em> (AUM) allows allows you to import and configure your New Relic users from your identity provider via SCIM. This guide provides Okta specific details on how to configure the New Relic Okta SCIM&#x2F;SSO application. Requirements Before using this guide, read our AUM"
      },
      "id": "6043f5cae7b9d2758b579a0c"
    }
  ],
  "/docs/accounts/accounts/automated-user-management/tutorial-manage-users-groups-scim": [
    {
      "sections": [
        "How to manage users",
        "Important",
        "Requirements",
        "Manage users in the UI",
        "Overview of user management concepts",
        "Common user management tasks",
        "Add, edit, and delete users",
        "Edit user type (basic and full platform)",
        "Give users access to accounts and roles (access grants)",
        "Create new custom groups and roles",
        "Set up SAML SSO and/or SCIM provisioning",
        "Grant users ability to manage other users",
        "Control how basic users upgrade to full platform users",
        "Track changes",
        "User management terms and definitions"
      ],
      "title": "How to manage users",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "New Relic One user management"
      ],
      "external_id": "9e37836740ce56302734d4af636bdbe087b4dbc3",
      "image": "https://docs.newrelic.com/static/a78dad5ff794da5deaaf3b514e334ea7/c1b63/new-relic-one-user-mgmt.png",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/new-relic-one-user-management/add-manage-users-groups-roles/",
      "published_at": "2021-12-19T15:22:19Z",
      "updated_at": "2021-11-24T14:23:16Z",
      "document_type": "page",
      "popularity": 1,
      "body": "For users on our New Relic One user model, we provide various user management features, including the ability to: Use role based access control (RBAC) to assign default or custom roles to user groups Create custom user groups Grant user groups access to specific roles and accounts Important This doc applies to users on the New Relic One user model. For managing users on our original user model, see Original users. Not sure which user model you're on? See User models. Requirements To see if you can access these user management features, go to the user management UI and see what you have access to. Access requirements: These features allow managing of users on the New Relic One user model. To learn more, see User models. To avoid configuration conflicts, ensure that only one user is managing users at a time. Simultaneous configuration by more than one user may result in errors and unexpected results. Most capabilities require the Authentication domain manager role and some require the Organization manager role. For details, see Standard roles. Pricing edition requirements: To manage user groups, roles, and access grants: Pro or Enterprise edition is required. To import users from an identity provider: Enterprise is required. A New Relic user can have a maximum of either three concurrent active sessions, or three unique IP addresses in use at any given time. Manage users in the UI For users on the New Relic One user model, to find your user management options: From the account dropdown, select Administration. There are two user management UI pages there: User management: Use this to add users, update user type (basic versus full platform) and other information, and approve user type upgrade requests. Organization and access: Use this page to create and manage groups, roles, and access grants, and to set up SAML SSO and SCIM provisioning. Want to see videos of the user management UI in action? See our user management videos. Important If you can't see these UI pages, it may be because you're on our original user model or because you don't have the required user management role. Overview of user management concepts If your organization has Pro or Enterprise edition, you can create and configure access grants. An access grant gives a group of users access to a specific role on a specific account. Here's a diagram showing how access grants work and how they relate to the broader organization: A diagram explaining the concept of how access grants give a user group access to a) a role on b) a specific account. (Note that this applies to users on our New Relic One user model (and not our original user model).) When your New Relic organization is created, it starts out with some default access grants for our default User or Admin groups. Each of these groups is assigned one or more roles and granted access to the primary (first created) account: A view of the Organization and access UI, showing the default access grants associated with our default groups. If you have a relatively flat organizational structure, and are okay with all or many of your users having wide administrative access and access to all accounts, you'll probably only need at most a few access grants. For example, you might decide to add new access grants to the existing default Admin or User groups, giving those roles access to other accounts. Or, if you need more granular definition over roles and permissions, you'd create access grants that define new groups that have access to specific roles (either our standard roles or custom-defined roles). For a tutorial on how to create access grants and create custom roles, see the User management tutorial. For other examples of some common user management tasks, see Example tasks. To see the UI in action, see our user management videos. Some tips on setting up access grants: It may help to first plan out how your access grants will be organized. How many accounts will you have? What user groups will get access to which roles and which accounts? Will you use our default groups and roles or create your own custom groups and roles? If you've used automated user management to provision users via SCIM, you will have to create access grants to give those users access. A common configuration for organizations with many accounts (roughly 20 or more) is setting up groups with the more organization-scoped roles (Organization manager, Authentication domain manager, and Billing user) on the primary account, and then on other accounts, setting up groups with the more product-scoped roles (like All product admin, Standard user, or custom roles). Common user management tasks In the Organization and access UI, you can create access grants, custom groups, custom roles, and configure an authentication domain. Here are some example user management procedures: Add, edit, and delete users See the user management tutorial. Edit user type (basic and full platform) Note that there are limits around how many times full platform users can be changed to basic users. Important If you're using automated user management to provision and manage your users, you have other options for managing user type. To change the user type of one or more users: On the User management page, click the checkboxes for the users whose user type you want to edit. Once you start selecting users, an option will appear for Edit type. You can also edit the user type and group of a specific user by clicking on that user. Give users access to accounts and roles (access grants) See our user management tutorial. Create new custom groups and roles See the user management tutorial. Set up SAML SSO and/or SCIM provisioning See Get started with SAML SSO or SCIM. Grant users ability to manage other users To grant your users the ability to manage other users, you'll need to add users to a group that has the Authentication domain manager and Organization manager role. You have two options: From the User management UI, you can add a user to the default Admin group, which includes both of those roles. OR You can create a custom group and assign it these roles. For a tutorial on creating new access grants, groups, and roles, see the user management tutorial. Control how basic users upgrade to full platform users See the authentication domain settings. Track changes To see an audit log of changes to your account, including user management actions, you can query the NrAuditEvent. User management terms and definitions For an explanation of how user access to accounts and roles works, see User management concepts explained. Here are some definitions for the terms we use there: A New Relic organization is the representation of your organization, containing all your accounts, users, and data. For more information, see Organization and account structure. A capability is an ability to use or edit a specific, granular New Relic feature. Examples of capabilities: The ability to modify APM settings The ability to delete alert conditions A role is a set of capabilities. It is what gives a user their permissions. Our default standard roles have various capability sets, and you can create custom roles that have a custom set of capabilities. See some specific New Relic capabilities. A user group has one or more roles associated with it. You assign your users to a group. We have default user groups (Admin and User), and you can make your own groups. An access grant is what grants a user group access to roles and to specific New Relic accounts. An access grant essentially states, \"This group is assigned this role on this New Relic account.\" Adding a user to a group doesn’t do anything unless that group is included in an access grant. An authentication domain contains a set of users who are added to New Relic and who log in to New Relic in the same way. For example, you may have one authentication domain for users who log in via username/password and another authentication domain for users who log in via SAML. If a user is a basic user, this takes precedence over any role-related limitations. For more on this, see Basic user and roles.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 540.6752,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "How to <em>manage</em> <em>users</em>",
        "sections": "Give <em>users</em> access to <em>accounts</em> <em>and</em> roles (access grants)",
        "tags": "<em>New</em> <em>Relic</em> <em>One</em> <em>user</em> <em>management</em>",
        "body": "For users on our <em>New</em> <em>Relic</em> <em>One</em> <em>user</em> model, we provide various <em>user</em> <em>management</em> features, including the ability to: Use role based access control (RBAC) to assign default or custom roles to <em>user</em> groups Create custom <em>user</em> groups Grant <em>user</em> groups access to specific roles and <em>accounts</em> Important"
      },
      "id": "603e7bce28ccbc415beba74c"
    },
    {
      "sections": [
        "Tutorials on user management tasks: access grants, custom roles, and adding users (New Relic One user model)",
        "Requirements",
        "Overview",
        "Add users",
        "View available accounts",
        "Grant access to accounts and roles",
        "Create custom role",
        "Add users to groups"
      ],
      "title": "Tutorials on user management tasks: access grants, custom roles, and adding users (New Relic One user model)",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "New Relic One user management"
      ],
      "external_id": "b1e5f303b6446f264c9d8c5020871a6990e052fd",
      "image": "https://docs.newrelic.com/static/a78dad5ff794da5deaaf3b514e334ea7/c1b63/new-relic-one-user-mgmt.png",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/new-relic-one-user-management/tutorial-add-new-user-groups-roles-new-relic-one-user-model/",
      "published_at": "2021-12-19T16:26:00Z",
      "updated_at": "2021-11-24T14:27:52Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This tutorial will walk you through some common procedures for managing users on the New Relic One user model. Here's a quick overview of the process (3:24 minutes): Requirements Some notes on requirements: This tutorial is for users on our New Relic One user model. Creating access grants requires Pro or Enterprise edition. For details, see user management requirements. Role requirements: Using the Organization and access UI to manage group access requires Authentication domain manager and Organization manager roles. Using the User management UI to add/edit users requires Authentication domain manager role. Overview Before using this tutorial, we recommend you check out: Understand what an access grant is Videos that show the user management UI in action This tutorial will walk you through how to: Add users View available accounts Grant groups access to roles and accounts Create custom roles Add users to groups Add users Adding users to New Relic is a separate process from creating access grants. In other words, you can do either procedure first. The key point is that if you're trying to give a user access to a role or an account that they don't yet have, they won't have that access until they're both a) in a group and b) that group has the correct access grant (access to a specific role on a specific account). There are two ways to add users: With the user management UI (described below) With automated user management, also known as SCIM provisioning: use of this method means that groups are defined in your identity provider and so you can't edit a user's group from the New Relic UI. To add users from the UI: From the top right of the New Relic UI, click the account dropdown, click Administration, and click User management. If you have multiple authentication domains, choose one from the authentication domain dropdown. Click Add user. Complete the process, including choosing user type and user group. Relevant tips: For how to bulk edit users' user type, see Edit user type. When choosing a group, you can choose either one of our default groups or a custom group that you've defined an access grant for. For more on creating access grants, keep reading. View available accounts When thinking about creating access grants for granting access to different accounts, it can help you to understand what accounts there are in your organization. To view the accounts in your organization: go to the account dropdown, click Administration, click Organizations and access, and then click Accounts. Grant access to accounts and roles Groups are used to group your users and manage what your users are able to do in New Relic: by creating an access grant, you assign a group access to a) a specific role on b) a specific account. By default, organizations on the New Relic One user model have two available groups: Admin and User. These default groups automatically have access to specific standard roles and are assigned to the account in which they were initially added. To view existing access grants: from the account dropdown, click Administration, and then click Organization and access. Even if you haven't created any custom access grants, you will see the default-created grants there that are present for our default groups. This is what you might see when you go to the Organization and access UI and view groups. The default available groups of Admin and User have access grants automatically created that grant users in those groups access to the roles associated with those groups and to the initial account those users were added in. To create a new access grant that gives a user group access to a role and an account: From the Organization and access UI, click Group access. If you don't see that UI, it may be because you're logged in with an original user model record, or because you don't have the ability to manage users. For more on such factors, see Factors affecting access. Choose one of the following: Existing: If there is already a group you want to add an access grant to, you can use this. For example, if you want to gives users in the default Admin or User group access to new accounts, you might choose this and then select the Admin Default or Admin User role. New: If you need to create a new group, choose this. You'll also need to choose the authentication domain that group is inside of (for an explanation of what \"Default\" means, see Default groups. If your organization strategy requires a good amount of restrictions over access to accounts and access to roles, you'll probably need to create a good amount of access grants. Next, under Access grant, you'll choose one of following: Account: Choose this to be able to select from the roles that are account-scoped. These are the roles that have to do with using and configuring our platform features (and not about organization and user management). Organization: Choose this to be able to select from the roles that are organization-scoped. These are the roles that govern organization- and user management. (Note that these users must also already belong to an account-scoped role. This is true for most users but if it's not, you may see a message that the user doesn't belong to an organization.) Select the Role you want to assign. Roles are organization-wide, so regardless of the authentication domain you're in, you have access to our standard roles and any custom roles you've created. For tips on selecting roles, see the tips after these instructions. Select the Account you want to add access to from the dropdown. If you don't see an account that you'd expect to see, this may be for a few reasons. One is that you yourself don't have the proper permissions for that account. Another is that that account is not actually in your organization. For more information, see Factors affecting access. If you are still having problems, talk to your account representative. If you want to continue adding more grants for that same group, select Add another at the bottom before clicking Add access. When you're done, if your users are already in the group you've added the grant to, they should have access within a few minutes (although for EU region New Relic accounts, this can take up to twenty minutes or so). If your users are not yet in that group (which would be true if you just created an access grant with a new group), you'll need to go to the User management UI and add one or more users to that group. Some tips for using this UI: If your users are managed via automated user management, you can't use the User management UI to add users to groups because your groups are imported from your identity provider. You will need to create access grants for those groups once they are in New Relic, though, to give those groups access. Note that if a user has the organization-scoped Organization manager and/or Authentication domain manager roles (which is true of users in our default Admin group) those users will always have those capabilities because those are organization-scoped abilities. This means that when you go to add those users to another account, you only have to add an account-scoped role, and not an organization-scoped role. In other words, once the users in a group have those organization-scoped roles, they will always have them for that organization unless removed. When selecting from amongst our standard roles, it's important to understand the difference between All product admin and Standard user. In short, All product admin is more popular a choice because it gives the ability to configure platform features. If you wanted to have your users be able to use platform features but not configure them, you'd choose Standard user. Create custom role When creating an access grant, you can use our standard roles, or you can create your own roles with unique sets of capabilities. To view existing roles: from the account dropdown, click Administration, then click Organization and access, and then click Roles. To create a custom role, click Add new custom role. Review the list of available capabilities and decide which ones your custom role needs. For more information about how roles and capabilities work, see Capabilities. Or check out this NerdByte video (4:07 minutes). Once you're finished creating a custom role, you will probably want to use it in a new access grant. Add users to groups In the user management UI, you can see your users and the groups they've been assigned to. Your users only have access to the access grants associated with the groups they've been added to. To view users and see their groups: from the account dropdown, click Administration, and click User management. If you don’t see that option, review the requirements. Groups reside within the boundaries of an authentication domain. If your organization has more than one authentication domain, the domain switcher in the top left will show which one you’re in. To add a user, click Add user. Complete the prompts in the UI, including choosing the user type and group. Any custom groups you’ve added should be available from the group dropdown. If the custom group you choose has been granted access to a role and an account, once you add the user to that group, that user will now have access. To edit a user’s group or other details: click on the user you want to edit and make changes.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 527.61755,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Tutorials on <em>user</em> <em>management</em> tasks: access grants, custom roles, <em>and</em> adding <em>users</em> (<em>New</em> <em>Relic</em> <em>One</em> <em>user</em> model)",
        "sections": "Tutorials on <em>user</em> <em>management</em> tasks: access grants, custom roles, <em>and</em> adding <em>users</em> (<em>New</em> <em>Relic</em> <em>One</em> <em>user</em> model)",
        "tags": "<em>New</em> <em>Relic</em> <em>One</em> <em>user</em> <em>management</em>",
        "body": " means that groups are defined in your identity provider and so you can&#x27;t edit a <em>user</em>&#x27;s group from the <em>New</em> <em>Relic</em> UI. To add users from the UI: From the top right of the <em>New</em> <em>Relic</em> UI, click the <em>account</em> dropdown, click Administration, and click <em>User</em> <em>management</em>. If you have multiple authentication domains"
      },
      "id": "603e7d67196a671e26a83dc5"
    },
    {
      "sections": [
        "OneLogin SCIM/SSO application configuration",
        "Requirements",
        "Step 1. Create authentication domain and enable SCIM",
        "Step 2. Set up OneLogin's New Relic app",
        "Step 3. Configure SCIM/SSO application",
        "Fill in the configuration form",
        "Fill in the rules form",
        "A rule that only uses actions",
        "Fill in the provisioning form",
        "Tip",
        "Fill in the Parameters form",
        "Save your changes",
        "Step 4. Assign users",
        "Step 5. Set your users' user type",
        "Step 6. Assign access grants",
        "Step 7. Configure SAML SSO"
      ],
      "title": "OneLogin SCIM/SSO application configuration",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Automated user management"
      ],
      "external_id": "d298162c055490c99117f564c3ea9c2ea5dfd8d1",
      "image": "https://docs.newrelic.com/static/8f585557ea58f70d94a746e6439bf1ad/c1b63/onelogin_rules_conditions_actions.png",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/automated-user-management/onelogin-scimsso-application-configuration/",
      "published_at": "2021-12-19T15:39:45Z",
      "updated_at": "2021-11-24T11:24:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our automated user management (AUM) allows allows you to import and configure your New Relic users from your identity provider via SCIM. This guide provides OneLogin-specific details on how to configure the New Relic OneLogin SCIM/SSO application. Requirements Before using this guide, read our AUM requirements. Note that these instructions require going back and forth between your identity provider and New Relic. Step 1. Create authentication domain and enable SCIM To get to the New Relic authentication domain UI: From one.newrelic.com, click the account dropdown, click Organization and access, and then click Authentication domains. If you don't already have one, create a new authentication domain for your SCIM-provisioned users by clicking + Add new. For that authentication domain, under Source of users, select SCIM. Copy and save the API token for later use. Note that this will be shown only once. Step 2. Set up OneLogin's New Relic app Next, you'll be setting up OneLogin's New Relic SAML/SCIM app. To set this up: Go to the OneLogin web site and sign in with an account that has administrator permissions. From the OneLogin home page, click on Administration. From the OneLogin Administration page, choose the Applications menu. From the OneLogin Applications page, click on Add App. In the search field on the OneLogin Find Applications page, enter \"New Relic by organization\" (not \"New Relic by account\") and then click on the application when it shows in the search results. From the Add New Relic by Organization page, click on Save. Step 3. Configure SCIM/SSO application Configuration for the New Relic SCIM/SSO application is split across several forms. This section describes the different forms that need to be configured. From the New Relic by organization application page, fill in the following forms: Fill in the configuration form In the left pane, select Configuration and complete the following: Get the authentication domain ID (top of New Relic's authentication domain UI and SCIM bearer token (in authentication domain UI as \"SAML 2.0 endpoint) and input them into the appropriate fields in the OneLogin app. Leave the API Connection disabled until all the configuration described in the following sections is completed. After completing all the configuration, enable the connection. Fill in the rules form Configure the user groups to send to New Relic using rules. OneLogin provides this documentation which describes how to use rules to provision groups for users. Decide what type of groups to send along with your users to New Relic. If your organization is using Active Directory or LDAP, you might choose to use security groups to define your users capabilities at New Relic. Another reasonable group choice is OneLogin role. On the New Relic side, your user's groups define their capabilities. The groups that are sent with users will be mapped to New Relic capability groups. Note that at the moment, there is no way to delete a group from the OneLogin side. This is a known limitation from OneLogin. Removing or changing rules does not delete groups already sent to New Relic. If you wish to no longer use a group, removing all the users from the group will prevent it from being used at New Relic. A rule that only uses actions Here's an example rule configuration does not use any conditions. The conditions are left empty to avoid applying any filtering logic to the users. All users will be sent in this example. If you want to send only a subset of users, you need to specify conditions to select the subset. The actions describe where to retrieve the value for the group name and how to parse the value. In this example, we retrieve the group name from the OneLogin role field. The OneLogin role field only has a single value, but sometimes the source for the group name contains other fields besides group name. In other words, some sources give you a list of fields and values and only one of those fields has the value you want to use. In this case, you can insert a regular expression in with value that matches field to find and extract the value for the group name. This example uses the entire value of the For each field for the group name. Fill in the provisioning form In the left pane, select Provisioning and complete the following: Check Enable provisioning. Under Require admin approval before this action is performed, uncheck these options: Create user Delete user Update user Tip If you do not uncheck these options, SCIM provisioning requests will not be sent until an administrator approves them. Set When users are deleted in OneLogin, or the user's app access is removed, perform the below action to Delete. Set When user accounts are suspended in OneLogin, perform the following action to Suspend. Fill in the Parameters form In the left pane, select Parameters and complete the following: Click Groups field. Check Include in User Provisioning. Click Save. Save your changes After you complete the above forms, click Save. Then, return to the Configuration form and enable the API connection. Step 4. Assign users After New Relic SCIM/SSO application configuration is finished and New Relic side configuration is finished, you can begin to assign users to the application. Assign the New Relic SCIM/SSO application to a user: Go to the OneLogin web site and sign in with an account that has administrator permissions. From the OneLogin home page, click Administration. From the OneLogin Administration page, choose the Users menu Users item. From the OneLogin Users page, click the user you want to assign the application to. From the user's page, click Applications. From the user's application page, click the plus sign and select the \"New Relic by Organization\" application. Important: Updating users' time zones is important, as charts and other user assets display times. Default is UMT. From the Edit New Relic by Organization login for user page, enter the user's time zone in IANA Time Zone database format (also known as the Olson time zone database format) and click Save. If you're using Roles to define your New Relic capability groups, from the user's application page, click the proper role(s) for the user and then click Save User. Step 5. Set your users' user type When your users are provisioned in New Relic, you're able to see them in the User management UI. If you're adding users to New Relic via SCIM but not managing their user type via SCIM, they start out as basic users. To convert users to full platform users, you have two options: Use the User management UI to edit users. Configure the OneLogin app to manage user type. Step 6. Assign access grants Once these steps are completed, you should be able to see your users in New Relic by going to the User management UI. Now that your users are present in New Relic, you must grant them access to specific roles on specific accounts. If this is not done, your users don't yet have access to New Relic. To learn how to do this, see: How access grants work The user management tutorial Step 7. Configure SAML SSO To enable SAML SSO for your users, see the SAML instructions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 455.72546,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OneLogin</em> SCIM&#x2F;SSO application configuration",
        "sections": "Step 2. Set up <em>OneLogin&#x27;s</em> <em>New</em> <em>Relic</em> app",
        "tags": "<em>Automated</em> <em>user</em> <em>management</em>",
        "body": "Our <em>automated</em> <em>user</em> <em>management</em> (AUM) allows allows you to import and configure your <em>New</em> <em>Relic</em> users from your identity provider via SCIM. This guide provides <em>One</em>Login-specific details on how to configure the <em>New</em> <em>Relic</em> <em>One</em>Login SCIM&#x2F;SSO application. Requirements Before using this guide, read our AUM"
      },
      "id": "6043f34228ccbccafb2c606a"
    }
  ],
  "/docs/accounts/accounts/billing/view-or-change-account-tax-information": [
    {
      "sections": [
        "Introduction to account settings",
        "Pricing, billing, and usage UI",
        "Manage users",
        "Add accounts",
        "Other account settings"
      ],
      "title": "Introduction to account settings",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "General account settings"
      ],
      "external_id": "1dbac104fa7e60637c32a61a80b8b709f6fd84c2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/general-account-settings/introduction-account-settings/",
      "published_at": "2021-12-19T13:41:30Z",
      "updated_at": "2021-11-13T02:21:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To view and edit account-related and user-management-related settings, go to one.newrelic.com and in the top right corner, select the account dropdown. Keep reading for tips on how to find different kinds of account settings in the UI. Pricing, billing, and usage UI Note that user permissions may impact your access to some account settings. To find billing-related UI, go to one.newrelic.com and, in the upper right corner, click the account dropdown to access various account settings. Here's what you can find there: Manage your plan: Options for viewing and managing billing-related settings. Manage your data: Options for managing ingest of data, which can be a billing factor. Learn more about managing data. View your usage: View of your billing-related usage. Administration: Options related to billing, usage, and user management (this UI available only for users on our New Relic One user model). Account settings: Options related to subscription and usage, and user management (this UI available only to users on our original user model). Related topics: Pricing details: Original pricing model | New Relic One pricing Manage data Manage users How you manage users depends on which user model you're on: New Relic One user model: see Manage users. Original user model: from one.newrelic.com, click the account dropdown, click Account settings, and then click Users and roles. For more on user management, see Original users. Not sure which user model you're on? See Determine user model. Add accounts Learn more about adding accounts. Other account settings See the account settings docs.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.82361,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>account</em> <em>settings</em>",
        "sections": "Introduction to <em>account</em> <em>settings</em>",
        "tags": "<em>General</em> <em>account</em> <em>settings</em>",
        "body": "To view and edit <em>account</em>-related and user-management-related <em>settings</em>, go to one.newrelic.com and in the top right corner, select the <em>account</em> dropdown. Keep reading for tips on how to find different kinds of <em>account</em> <em>settings</em> in the UI. Pricing, <em>billing</em>, and usage UI Note that user permissions may"
      },
      "id": "6043f38a196a679ae4960f5e"
    },
    {
      "sections": [
        "Non-profits and New Relic",
        "Exceptions",
        "Tip",
        "Additional requirements",
        "Signup procedures",
        "Program Benefits"
      ],
      "title": "Non-profits and New Relic",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "General account settings"
      ],
      "external_id": "441757ebbd6d8039278355701793bcbcae0fbc8a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/new-relic-one-pricing-billing/eligibility-guidelines-new-relic-nonprofit-program/",
      "published_at": "2021-12-19T17:31:50Z",
      "updated_at": "2021-11-06T19:59:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document provides New Relic's Observability for Good program eligibility guidelines for nonprofit, charitable, and NGO organizations. Exceptions Many global nonprofit, charity and NGO organizations are eligible for Observability for Good if the organization has a recognized legal status in their respective country equal to 501(c)(3) status under the United States Internal Revenue Code, with certain exceptions. In addition, all organizations must be verified by TechSoup or the local TechSoup partner. Examples of ineligible organizations include, but are not limited to: Ineligible nonprofits Comments Higher education institutions Private nonprofit and public higher education institutions do not qualify. This includes: Universities Colleges Trade schools Primary and secondary schools (public and private) Primary and secondary schools do not qualify. This includes, but is not limited to: K-12 public school districts Individual K-12 public schools within a state or federally funded school district Standalone K-12 charter schools K-12 Knowledge is Power Program (KiPP) schools Other K-12 schools that are not part of a state or federally funded school district Healthcare organizations providing patient care This includes hospitals, hospital auxiliaries, healthcare systems, and related health services organizations, such as: Nursing or convalescent homes Care and housing for the aged Pregnancy centers Tip Free clinics may qualify. Professional, commerce, mutual, and trade organizations This includes organizations such as: Credit Unions Regulation of business Industry trade shows Professional athletic leagues Tourist bureaus Employee or membership benefit organizations This includes organizations such as: Fraternal Beneficiary societies Associations of employees Employee or member welfare associations Pension and retirement benefits Tip Organizations focusing on the improvement of working conditions may qualify. Legislative or political organizations and advocacy groups Organizations focused on nonpartisan voter education may qualify. Organizations within countries sanctioned by the US This includes organizations within any sanctioned countries included on the US Department of Treasury's Office of Foreign Assets control list , which is updated periodically. Additional requirements In order to participate, approved organizations must also: Submit an application through newrelic.org/signup. Complete the eligibility validation process through TechSoup. Tip Learn about TechSoups local NGO definitions. Accept New Relic's standard terms of service. Be a direct customer of New Relic and have a direct billing relationship with New Relic. Signup procedures To learn more or to sign up as a new or existing New Relic customer, go to newrelic.org/signup. Program Benefits Observability for Good Standard: 4 additional free users per month (+1 free edition user: 5 total) 900 additional gb of data (+100 gb free edition: 1TB total) Observability for Good Pro: 2 free users per month 1 free TB data per month Tip A payment method on file is required for accounts using Observability for Good Pro.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 222.19806,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Non-profits <em>and</em> New Relic",
        "sections": "Non-profits <em>and</em> New Relic",
        "tags": "<em>General</em> <em>account</em> <em>settings</em>",
        "body": " through newrelic.org&#x2F;signup. Complete the eligibility validation process through TechSoup. Tip Learn about TechSoups local NGO definitions. Accept New Relic&#x27;s standard terms of service. Be a direct customer of New Relic and have a direct <em>billing</em> relationship with New Relic. Signup procedures To learn"
      },
      "id": "6186de8c196a678bfdf4378b"
    },
    {
      "sections": [
        "Set or change password",
        "Change your password",
        "Important",
        "Reset password"
      ],
      "title": "Set or change password",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "General account settings"
      ],
      "external_id": "817d8d17dca7f361429109d8364eff931a1cb392",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/account-maintenance/change-passwords-user-preferences/",
      "published_at": "2021-12-19T15:39:09Z",
      "updated_at": "2021-10-13T03:31:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document contains information on New Relic password requirements. Change your password New Relic account passwords don't expire. However, users can change their own password and other personal account information anytime. Your ability to change your password from the UI may depend on your user and organization level settings. To change your password from the UI: From one.newrelic.com, click the account dropdown and select User preferences. When creating a password, adhere to the password requirements below. Your password must meet these minimum requirements: 8 to 50 characters in length At least one letter (a-z, A-Z) At least one number (0-9), special character, or space Special characters allowed include ~ ` ! @ # $ % ^ & * ( ) _ - + = { [ } ] : ; \" ' < , > . ? / | \\. Spaces are also allowed. Important In addition to New Relic's requirements, follow your organization's guidelines for password length, use of upper or lower case letters, numbers, and special characters. Reset password If you forgot your own password or need to request a password reset, you can use New Relic's self-service options. Admins can't reset passwords for other users. If you need to reset someone else's password, get support at support.newrelic.com.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 209.15848,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Set</em> or change password",
        "sections": "<em>Set</em> or change password",
        "tags": "<em>General</em> <em>account</em> <em>settings</em>",
        "body": " and organization level <em>settings</em>. To change your password from the UI: From one.newrelic.com, click the <em>account</em> dropdown and select User preferences. When creating a password, adhere to the password requirements below. Your password must meet these minimum requirements: 8 to 50 characters in length"
      },
      "id": "603eb55a64441f9f8f4e889c"
    }
  ],
  "/docs/accounts/accounts/saml-single-sign/saml-service-providers": [
    {
      "sections": [
        "Overview of data retention (original pricing model)",
        "Important",
        "Limits on editing and deleting data",
        "Product and API data retention policies",
        "APM",
        "APM data retention policies",
        "Legacy APM data retention policies",
        "Browser",
        "Infrastructure",
        "Tip",
        "Infrastructure downsampling and data retention",
        "Integration compute unit event limits",
        "Insights",
        "Logs",
        "Logs in context data retention",
        "Mobile",
        "Mobile data retention policies",
        "Standard Mobile (legacy) data retention policies",
        "Synthetics",
        "Synthetics data retention policies",
        "Metric API",
        "Trace API",
        "Data components",
        "Event data: reported by most products",
        "Metric timeslice data: reported by APM, Browser, and Mobile",
        "Aggregate metric timeslice data: reported by APM, Browser, and Mobile",
        "Key metrics: reported by APM, Browser, and Mobile",
        "Trace data: reported by APM, Browser, and Mobile"
      ],
      "title": "Overview of data retention (original pricing model)",
      "type": "docs",
      "tags": [
        "Accounts",
        "Original accounts and billing",
        "Original data retention"
      ],
      "external_id": "22d5ab9f4d623ead28ee7bb82c118d91804dee22",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/original-accounts-billing/product-based-pricing/overview-data-retention-components/",
      "published_at": "2021-12-19T15:14:48Z",
      "updated_at": "2021-11-14T09:24:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This doc is for accounts on our original Product-based pricing. If you're on our New Relic One pricing model, see Manage your data. Not sure which you're on? See Overview of pricing models. If you're on the original product-based pricing model, you retain your existing subscriptions and data retention values. You manage these existing retention settings from the Data management hub in New Relic One. To manage your retention settings, go to the Data management UI. You'll see your existing retention settings. Adjust retention values by clicking Edit retention. New Relic stores different types of data for different periods of time. The retention period for a type of data will vary depending on the product, the subscription level, and the feature. Limits on editing and deleting data Once telemetry data (events, metrics, logs, traces) is reported to New Relic and available for querying, that data cannot be edited or deleted. This is a purposeful design decision that optimizes New Relic's speed and performance. Data will expire after its data retention ends. If you sent unwanted data or sensitive data to New Relic that must be removed, contact your account representative at New Relic, or get support at support.newrelic.com. Product and API data retention policies Select a New Relic product to see details about its data retention: APM Specific retention policies apply to each APM subscription level, including Lite, Essentials, Pro, and Enterprise. This includes metric timeslice data, key metrics, trace data, and event data. In addition to retention limits, your data is subject to aggregation (averaging) after certain elapsed time periods. For more information, see the aggregate metric data description. APM data retention policies For accounts on our original product-based pricing, APM data retention policies depend on your APM product subscription level. Component Lite Essentials Pro Metric timeslice data 24 hours 3 days 90 days Key metrics none forever forever Distributed tracing and logs in context none none 8 days Other trace data * 1 day 3 days 7 days Event data * * 1 day 3 days 8 days * If you view a transaction trace in the New Relic UI, New Relic stores that trace data for up to one year. * * Learn about how to extend the retention of event data. Legacy APM data retention policies Component Standard Startup Small Business Enterprise Metric timeslice data 7 days 14 days 30 days 90 days Key metrics none none none forever Trace data 7 days 7 days 7 days 7 days Event data none 8 days 8 days 8 days Browser For accounts on our original product-based pricing, Browser data is stored depending on your subscription level: Component Lite Pro Metric data 24 hours 90 days Key metrics 24 hours forever Trace data 7 days 7 days Event data * 1 day 8 days SPA data Unavailable 8 days * Learn about how to extend the retention of event data. Infrastructure For accounts on our original product-based pricing, Infrastructure data retention policies depend on your Infrastructure subscription level and your New Relic Infrastructure compute units pricing model. Data retention rules apply the same whether that data is displayed in the UI or queried. Tip Infrastructure data retention is not governed by your Insights subscription. Infrastructure downsampling and data retention Types of data are stored depending on your subscription level: Component Essentials Pro Infrastructure data 3 months 13 months Host count Host count will stop reflecting a host whose agent is no longer reporting after three minutes. However, host data will be available, subject to other retention criteria. 3 minutes 3 minutes Inventory attributes removed Inventory attributes for a host are retained for 24 hours after the agent stops reporting. 24 hours 24 hours Integration data Not available with Essentials Limit of 2275 integration events per compute unit per month In addition, Infrastructure downsamples your data on the fly, as it's generated. All Infrastructure metric data types (including On-Host Integrations metrics) will display different granularity depending on the age of the data and the size of the time window. The following table illustrates when different downsampled buckets will be used, both in the Infrastructure UI and for queries: Bucket Size Used For Time Windows Covering... Data Retained For... Raw (5, 10 or 15 second) 0 to 59 minutes 7 days 1 minute 60 minutes to 6 hours 30 days 10 minutes 6 hours to 3 days Full account retention period 1 hour 3 days to 14 days Full account retention period 3 hours 14+ days Full account retention period Integration compute unit event limits The 2275 limit on integration events per compute unit per month is a limit on total Infrastructure integration events. It's not a limit of 2275 for each integration's events. Additional details and clarifications about this limit: This limit applies to all events from all integrations (cloud integrations and on-host integrations). The events are all handled the same. Default data received by the Infrastructure agent does not count against the 2275 event per compute unit limit. If you exceed your limit, we do not enforce this limit. If you exceed your limit, we'll review pricing options with you to ensure you get the most cost-effective pricing for your organization's integrations. Insights For accounts on our original product-based pricing, an Insights subscription extends your event data retention. An Insights Pro subscription allows you to customize the length of your event data retention. Logs For accounts on our original product-based pricing, log data can be retained for up to 30 days by New Relic. Shorter retention periods of 8 or 15 days are also available. Logs in context data retention New Relic Logs logs in context data retention policy is based on your current APM product subscription level. For more information, see APM data retention. Mobile For accounts on our original product-based pricing, Mobile data retention policies depend on your New Relic Mobile product subscription level. This includes data components such as metric data, aggregate metrics, key metrics, trace data, and event data. For information about your subscription-related data usage, see Mobile subscription usage. Mobile data retention policies Component Lite Enterprise Overview page data 24 hours 90 days Crash data and stack traces 24 hours 90 days HTTP requests (except errors) as metric data Unavailable 90 days HTTP request errors as metric data Unavailable 3 days Interaction traces Unavailable 90 days Custom events * Unavailable 1 day Mobile events * 1 day 8 days MobileBreadcrumb events * Unavailable 3 days MobileCrash events * 1 day 90 days MobileHandledException events * Unavailable 3 days MobileJSError events (React Native beta) * Unavailable 3 days MobileRequest events * Unavailable 3 days MobileRequestError events * Unavailable 3 days MobileSession events * 1 day 90 days * Learn how to extend retention of event data. Standard Mobile (legacy) data retention policies Unless otherwise noted, Insights event data is unavailable for Standard subscriptions. Component Standard Overview page data 7 days Crash data and stack traces 7 days HTTP requests (except errors) as metric data 7 days HTTP request errors as metric data 3 days Interaction traces Unavailable MobileCrash events 8 days MobileSession events 8 days Synthetics For accounts on our original product-based pricing, Synthetics data retention policies depend on your Synthetics product subscription level. This includes data components such as metric data, aggregate metrics, key metrics, trace data, and Insights events. Tip Synthetics events do not count against an Insights Pro subscription. Your current subscription level appears in the right column of the Account summary page: Go to synthetics.newrelic.com > (account dropdown) > Account settings > Account > Summary. Synthetics data retention policies Component Lite Pro Synthetics monitor data 2 weeks 13 months Event data 2 weeks 13 months Also see the data retention details for APIs, including: Metric API All raw metric data points will be retained for 30 days. All additional aggregated data derived from the raw metric data points (for example, one-minute rollups) will be retained for 13 months. Any change to the retention period beyond such periods may result in a charge to you. Trace API See Trace API general requirements and limits. Data components For accounts on our original product-based pricing, the section below provides an explanation of some types of data components governed by the data retention rules of specific products: Event data: reported by most products See Event data retention for information on the event data type. Metric timeslice data: reported by APM, Browser, and Mobile Metric timeslice data is a specific type of data that is used for most metric charts and reports in APM, mobile, and browser. Important Note that metric timeslice data differs from other metric data types. All metric timeslice data is aggregated, but New Relic deals with fresh data and old data in different ways. Fresh data has specific policies applied to the data to keep granular views of performance as aggregate metrics. As data ages and becomes less useful in a granular state, we summarize that data and only keep key metrics. Aggregate metric timeslice data: reported by APM, Browser, and Mobile Aggregate metric timeslice data summarizes calls to specific methods in your application: how many times each one was called and response times. In the New Relic UI, you see the class and method names along with their aggregate numbers. Metric data aggregation depends on your subscription level. Subscription Level Aggregate retention Enterprise Aggregates (averages) to 1-hour periods after 8 days. After 90 days, the permanent metric data continues to be stored in 1-hour periods. This means you cannot obtain data granularity of less than 1 hour after 8 days, and only a subset of metrics are available after 90 days. Pro After 8 days Essentials After 3 days. Legacy Small Business, Startup, and Standard After 3 days. Lite After 2 hours. We retain your most recent data in one-minute increments. We also aggregate permanent metric data to day-size increments after 90 days. When looking at older data in small time windows, you may notice that charts show less detail. As data ages, it is aggregated into larger segments. Key metrics: reported by APM, Browser, and Mobile New Relic retains certain \"key\" metric timeslice data aggregations forever for Enterprise and Pro customers, for any number of applications. New Relic aggregates permanent key metric data to hour-size increments after 90 days. Product Key metrics APM Apdex, for app server Throughput and page views Response time, plus breakdown into tiers or categories on your app's main Overview chart Page load time, plus breakdown into segments Error rate CPU usage Memory usage Browser Apdex, for browser Browser page load time Throughput total. After 90 days, there is no breakout by browser type, and only the combined average value is available for the time range requested. Mobile Crash rate: For iOS and for Android Network throughput Network response time Network error rates Interaction traces Trace data: reported by APM, Browser, and Mobile Depending on the product, New Relic retains different types of trace data: Product Trace data APM Types of trace data: Transaction traces Distributed tracing Slow query samples Error details See APM data retention details. Browser Types of trace data: Session traces Browser traces JavaScript errors* See Browser data retention details. Mobile Types of trace data: App crash stack traces Interaction traces See Mobile data retention details. * JavaScript errors in the stack trace UI are saved as trace data. JS errors are also saved as events.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 148.27058,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Overview of data retention (<em>original</em> pricing model)",
        "sections": "Overview of data retention (<em>original</em> pricing model)",
        "tags": "<em>Original</em> <em>accounts</em> <em>and</em> <em>billing</em>",
        "body": "Important This doc is for <em>accounts</em> on our <em>original</em> Product-based pricing. If you&#x27;re on our New Relic One pricing model, see Manage your data. Not sure which you&#x27;re on? See Overview of pricing models. If you&#x27;re on the <em>original</em> product-based pricing model, you retain your existing subscriptions"
      },
      "id": "6043f75364441f6967378ec6"
    },
    {
      "sections": [
        "Event data retention (original pricing model)",
        "Important",
        "Data retention UI",
        "Overview of event data retention",
        "Extend your event retention",
        "Insights Pro",
        "How number of events stored is calculated",
        "Insights Pro event overage example",
        "Disable/enable Transaction and Pageview event reporting",
        "Tip",
        "Flexible data retention",
        "How it works",
        "Manage retention via UI",
        "Glossary"
      ],
      "title": "Event data retention (original pricing model)",
      "type": "docs",
      "tags": [
        "Accounts",
        "Original accounts and billing",
        "Original data retention"
      ],
      "external_id": "76d1289aad7de08b355bb8c313f9e7a42a5779d8",
      "image": "https://docs.newrelic.com/static/e53a1e416eb6116545627d3ec880d08e/e9c9b/flex-2.png",
      "url": "https://docs.newrelic.com/docs/accounts/original-accounts-billing/original-data-retention/event-data-retention-original-pricing-plan/",
      "published_at": "2021-12-19T14:16:05Z",
      "updated_at": "2021-11-14T09:17:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This doc is for accounts on our original pricing model, not our New Relic One pricing model. Not sure which you're on? See Overview of pricing models. For organizations on New Relic One pricing, our various New Relic products report a wide range of event data. Different products have different data retention periods, and different ways to extend event data retention. You can customize the length of your event data retention through flexible event retention. Data retention UI For how to find the data retention UI, see Manage data. Overview of event data retention All New Relic product subscriptions come with a certain level of data retention that governs how long different types of data are retained. One type of data governed by data retention rules is event data. Event data is available in some UI charts and tables, and also available for querying via NRQL, our querying language. There are events reported from products by default, and there are custom events: each have their own retention rules, depending on the product and subscription level. Here are some examples of how different product subscriptions can affect event data retention: Free/Lite APM subscription: default-reported events available for 1 day. No custom events available. Pro APM subscription: default-reported events available for 8 days. Custom events available for 1 day (and able to be extended with Insight Pro). To see your subscriptions, go to the Account summary page. Extend your event retention Product Method APM, Browser, and Mobile Event data retention can be extended with a paid subscription to these products (see product data retention). To extend retention of both default-reported events and custom events further, you need an Insights Pro subscription. Infrastructure Event data retention can be extended with a paid Infrastructure subscription. See Infrastructure data retention rules. Synthetics Event data retention can be extended with a paid Synthetics subscription. See Synthetics data retention rules. Custom events Custom events reported by agent APIs or the Event API: Extension requires an Insights Pro subscription. Insights Pro Important As of April 12, 2021, we are upgrading Insights to an improved web and mobile experience! All of your Insights URLs will be redirected automatically to the corresponding dashboards in New Relic One. For more details about this migration and how you can easily plan for this transition, see our Explorers Hub post. A paid Insights subscription is what governs the extension of event data retention for: Our APM, Browser, Mobile, and Serverless products Custom events that come from an agent API or from the Event API Important Note that having an Insights Pro subscription doesn't require use of the Insights UI (insights.newrelic.com) to query your data: there are other querying options available. To see the data retention governed by your Insights subscription: go to the usage UI and select Insights usage. With an Insights Pro subscription, you can use flexible retention to customize how your event data is retained. This lets you keep only the data you need, for as long as you need it. How number of events stored is calculated This is an explanation of how the number of stored events are calculated by default for an Insights Pro subscription. (Note that with flexible retention, you have more fine-grained control over the retention period.) The events stored is calculated based on 1) total events stored over time (calculated based on the events generated per week) and 2) the weeks of data retention available. This equation can be represented like this: events stored = (events generated per week) * (weeks of retention) Copy An Insights Pro subscription provides a given number of weeks of data retention as well as a given number of events over that retention period. For example: (200M transactions per week) * (4 weeks of retention) = 800M events stored in Insights (16M transactions per week) * (50 weeks of retention) = 800M events stored in Insights For Insights Pro subscriptions, data is purged based on retention window, not volume. It is deleted from the system once it's past the retention window. For example: If your Insights license is for 800 million events with a 4 week retention period, your data would start being purged after it is older than four weeks. Temporary spikes in data exceeding your subscription level will still be recorded, but consistent overage should be solved by upgrading your subscription level or decreasing data collected. For customers without an Insights Pro subscription, New Relic may throttle or downsample events to a limit of not more than than 4,000 events per host per minute. Insights Pro event overage example In this example, you have an Insights Pro subscription with a license for 800 million events over 4 weeks, a rate of 200 million events per week. You have APM Pro, Browser Pro, and Mobile Enterprise. A fifth week of data is added via your subscriptions, bumping you to a total of 1 billion events stored within your plan: If you are using 975 million events, you are not over your retention. If you are using 1.25 billion events, you are over your retention. Disable/enable Transaction and Pageview event reporting Tip Owners or Admins The Insights Data summary UI page is used to see the types of events being reported. You can also use this page to enable and disable the reporting of PageView and Transaction events. To view Data summary: Go to insights.newrelic.com > Manage data. Select the Summary tab. Note: if you disable PageView or Transaction event reporting, this can affect some New Relic UI elements. You may see some empty charts on some UI pages that rely on this data. Go to insights.newrelic.com > Manage data > Summary. From the Summary tab, select Configure data sources. Toggle the appropriate switch on or off, then save. Toggling Transaction on or off will cause reporting agents to restart themselves. For more about configuring event reporting, see Event data retention. Flexible data retention With an Insights Pro subscription, you get access to flexible retention, which lets you define how some types of event data are retained. This lets you keep only the event data you need, for as long as you need it. You can manage your flexible retention through the UI or through our GraphQL API. Requirements to use this feature: An Insights Pro subscription or equivalent trial. Applies only for events governed by an Insights Pro subscription. To use this feature, you must be an account Owner or data retention add-on manager for your account. How it works To understand how standard event data retention works, first read Event data retention. With flexible retention, you specify the data retention for applicable event namespaces across your accounts. This gives you per-event namespace control of your data. The retention that you specify for an event namespace will be shared by all the event types under that namespace. If some namespaces are not relevant to you, you can avoid collecting their event data entirely. Your retention value can’t be lower than the included retention or higher than the default retention. You can control data retention either in our UI or by API. Manage retention via UI You can control data retention either using our GraphQL API or in the UI. To do this with the UI, go to the data retention UI. Your retention changes take effect within 24 hours after updating. Glossary To understand the terms used with flexible retention, see the following: Term Description Event namespace An event's namespace corresponds to one or more event types that share a single data retention value. For more information, see Event namespaces (types). You can also use NerdGraph to get the list of customizable event namespaces. Retention value The number (in days) that specifies how long your event data is stored. Retention rule The event namespace and retention value pair that you specify to override the current retention. Licensed retention Retention period that’s determined in weeks by your Insights Pro subscription contract. Included retention Retention period for which your data is stored but not charged under the Insights Pro subscription. For details, see the data retention details for a specific product. Paid retention Retention period for which your data is stored and is charged under the Insights Pro subscription. By default, your licensed retention determines this value but Flexible retention lets you override it. Default retention Retention period that comes out of the box. This is based on the total of included retention plus licensed retention. For information on managing retention settings with APIs, see the Manage data retention documentation.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 148.26698,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Event data retention (<em>original</em> pricing model)",
        "sections": "Event data retention (<em>original</em> pricing model)",
        "tags": "<em>Original</em> <em>accounts</em> <em>and</em> <em>billing</em>",
        "body": "Important This doc is for <em>accounts</em> on our <em>original</em> pricing model, not our New Relic One pricing model. Not sure which you&#x27;re on? See Overview of pricing models. For organizations on New Relic One pricing, our various New Relic products report a wide range of event data. Different products have"
      },
      "id": "6043f713e7b9d2ccee579a1d"
    },
    {
      "sections": [
        "Overview of changes to pricing and user model",
        "Overview of how pricing model and user model relate",
        "Pricing plans explained",
        "Determine pricing model",
        "Convert to new pricing",
        "User models explained",
        "Requirements for new user model",
        "Determine which user model you're on",
        "Feature impacts of user model",
        "Transition to new models"
      ],
      "title": "Overview of changes to pricing and user model",
      "type": "docs",
      "tags": [
        "Accounts",
        "Original accounts and billing",
        "Original product-based pricing"
      ],
      "external_id": "b19d61b5a0c65ca352e2ce0e260e2b53391b94fc",
      "image": "https://docs.newrelic.com/static/1461470cc1ef0be91120c84d76a7ccdd/c1b63/new-relic-one-pricing-plan-billing-ui.png",
      "url": "https://docs.newrelic.com/docs/accounts/original-accounts-billing/original-product-based-pricing/overview-changes-pricing-user-model/",
      "published_at": "2021-12-19T14:16:05Z",
      "updated_at": "2021-11-24T14:34:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In 2020, New Relic released both a newer pricing model and a newer user model. Keep reading to learn about: How the pricing model and the user model relate to each other Pricing plans explained User models explained How to switch to the new models Overview of how pricing model and user model relate In 2020, we released both a new, improved pricing model and a new, improved user model. These models represent the future. All new sign-ups are on these new models and eventually all organizations will be on these models. But currently, our customers from before July 2020 may have one of several combinations of these as they either switch to the new pricing or migrate their users to the new user model. This table shows how pricing and user model relate to each other: Pricing plan factors User model factors Original pricing If your organization was created before July 30 2020, you remain on our original pricing model until you transition to New Relic One pricing. For original pricing accounts, you have users on the original user model unless you've migrated your users to the new model. Note that if you've added users via automated user management, they're on the new model. This means it's possible to have some users on both models. New Relic One pricing An organization is on the New Relic One pricing model: If it was created on or after July 30 2020, or If it's an older organization but has switched to New Relic One pricing. If your organization was created on or after July 30 2020, your users are on the New Relic One user model. If you have an older organization, you have users on the original user model unless you've migrated your users to the new model. Pricing plans explained New Relic organizations are on one of two pricing models: New Relic One pricing: Our new pricing model is simpler. It bills on a) which edition you're on (Standard, Pro, or Enterprise), b) the GBs of data ingested, and c) how many billable full platform users you have. All organizations created on or after July 30 2020 are on this pricing model, as are older organizations that have switched to this pricing. There are two versions of this pricing model. Our original product-based pricing model: this is based on subscriptions to specific products (for example, APM, Browser, Mobile, Infrastructure). Some organizations have the option to transition to the newer pricing model: in that case, their users remain on our original user model. Determine pricing model To determine which pricing model you’re on: go to one.newrelic.com, select the account dropdown, and select Manage your plan. If you see billing information about data ingested and the number of billable users, you’re on the new pricing model. If you're on New Relic One pricing, you'll see a billing UI like this. Convert to new pricing Some organizations are able to switch to new pricing. Learn more about switching your pricing model. User models explained In this context, the term \"user model\" refers to the structure of a New Relic user record and how it interacts with the broader New Relic organization that it's in. Our two user models are: New Relic One user model: this newer, improved user model has these major differences from the original user model: All your accounts and users are contained under a top-level organization. This gives an organization stronger control over managing users and what they can access. One impact of this is that users who work with multiple New Relic organizations may have an email address associated with multiple logins. Users on this model have a different system for managing permissions and account access. Using access grants, you assign users access to specific roles on specific accounts. Original user model: some aspects of this older model that are different from our newer model: There wasn't as much organization-level control over users. For example, a New Relic user had the ability to access multiple accounts they'd been granted access to using a single login. Users on the original user model have a different user management system. To learn more about benefits of the new model, see our blog post about user model changes and an explanation of the new account structure. For impacts and limitations, see Feature impacts. Requirements for new user model Here's a synopsis of the factors governing what organizations use the new model: All New Relic organizations that signed up after July 30 2020 have users on this model, and also have the new pricing model. Some older New Relic organizations have had their users migrated to the new model by New Relic or by using the user migration procedure. Note that switching to the new pricing model is independent from migrating users. Partner accounts (resellers, managed service providers), and organizations using the partnership account structure, cannot yet migrate their users to the new model. Determine which user model you're on To determine what user model you're on, from the account dropdown select User preferences. If you see the UI below, you're on the New Relic One user model. If you see a different UI with more content displayed, you're on the original user model (see original user docs). To determine if you can manage users on the New Relic One user model, see Manage users. The user model is independent of your pricing model. For how user model relates to pricing, see the Pricing and user model table. Feature impacts of user model The new user model offers many benefits. Here are some feature impacts of being on the New Relic One user model that may be not obvious to users accustomed to our previous user model: More separation of organizations/accounts: If you have an email address associated with multiple New Relic accounts, this may mean you need to log out and log back in. Learn more about account access. Adding accounts: Pro and Enterprise edition can have more than one account in their organization. Currently users in these organizations cannot, on their own, add accounts. They must get help from their New Relic account representative. To learn about adding users to accounts, see Manage users. Alerts-related impacts: Policy actions: For alert policies and conditions, some UI locations display the user that made the last modification. For users on the new model, it will display their user ID and not their user name. For example, a policy might read Last modified Sep 7, '20 4:26 pm by user 1234567 instead of by John Doe. If you have user management permissions, you can determine what user this is by going to the User management UI. Notifications: Users on the new model won’t show up as an option when choosing users to receive alert notifications. The result of this is that these users will not get New Relic mobile app push notifications. As a workaround, you can instead use their email address as the notification channel. You can't use the REST API to generate a list of users (only original model users). Instead you'd use the user management UI. Changing account name: These users cannot change the name of their organization after it has been created. Transition to new models Some New Relic organizations on the old models are able to transition to the new models. To learn more about this, see Transition to new models.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 137.87303,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Overview of changes to pricing <em>and</em> <em>user</em> model",
        "sections": "Overview of changes to pricing <em>and</em> <em>user</em> model",
        "tags": "<em>Original</em> <em>accounts</em> <em>and</em> <em>billing</em>",
        "body": ", their <em>users</em> remain on our <em>original</em> <em>user</em> model. Determine pricing model To determine which pricing model you’re on: go to one.newrelic.com, select the <em>account</em> dropdown, and select Manage your plan. If you see <em>billing</em> information about data ingested and the number of billable <em>users</em>, you’re on the new"
      },
      "id": "603e97fa28ccbcb7c2eba754"
    }
  ],
  "/docs/accounts/accounts-billing/account-setup/choose-your-data-center": [
    {
      "sections": [
        "Glossary of New Relic terms",
        "account dropdown",
        "account switcher",
        "administrator",
        "agent",
        "agent API",
        "aggregated metrics",
        "aggregation delay",
        "aggregation function",
        "aggregation method",
        "aggregation timer",
        "aggregation window",
        "alert",
        "alert condition",
        "alert evaluation",
        "alert policy",
        "apdex",
        "apdex_f",
        "apdex_t",
        "API (application programming interface)",
        "APM",
        "application",
        "application ID",
        "application name",
        "Applied Intelligence (AI)",
        "attribute",
        "availability monitoring",
        "browser",
        "Browser monitoring",
        "background external",
        "child account",
        "cloud-based integration",
        "collector",
        "Command line interface (CLI)",
        "compute unit (CU)",
        "condition_id",
        "CPM (calls per minute)",
        "CPU burn",
        "custom attribute",
        "custom dashboard",
        "custom event",
        "custom instrumentation",
        "custom metric",
        "data collector",
        "data explorer",
        "degradation period",
        "dimensional metric",
        "Docker",
        "downtime",
        "entity",
        "event",
        "expected error",
        "exporter",
        "Flex",
        "framework",
        "harvest cycle",
        "health status indicator",
        "host",
        "host ID",
        "ignored error",
        "incident",
        "Infrastructure monitoring",
        "Insights",
        "instance ID",
        "instrumentation",
        "integration",
        "interaction",
        "interaction trace",
        "inventory data",
        "key transaction",
        "launcher",
        "log",
        "Log monitoring",
        "Logs",
        "Logs in context",
        "master account",
        "metric",
        "metric timeslice",
        "metric grouping issue",
        "minion",
        "Mobile monitoring",
        "monitor",
        "NerdGraph",
        "Nerdlet",
        "Nerdpack",
        "New Relic Edge with Infinite Tracing",
        "New Relic One",
        "New Relic One catalog",
        "NRQL (New Relic query language)",
        "non-web transaction",
        "notification",
        "notification channel",
        "on-host integration",
        "owner",
        "page load timing",
        "parameter",
        "parent account",
        "permalink",
        "pinger",
        "polling interval (AWS)",
        "PPM (pages per minute)",
        "private location",
        "recovery period",
        "response time",
        "restricted user",
        "rollup",
        "root span",
        "RPM",
        "RUM (real user monitoring)",
        "runbook",
        "SAML (Security Assertion Markup Language)",
        "Selenium",
        "service",
        "signal",
        "signal filter",
        "span",
        "SSL certificate",
        "SSO (single sign on)",
        "streaming algorithm",
        "sub-accounts",
        "Synthetic monitoring",
        "target",
        "tag",
        "thresholds",
        "throughput",
        "tier",
        "time picker",
        "time range",
        "timeslice data",
        "trace",
        "traffic light",
        "transaction",
        "transaction trace",
        "UI",
        "user",
        "UTC",
        "value function (metrics)",
        "violation",
        "web external",
        "web transaction",
        "WebDriverJS",
        "workload"
      ],
      "title": "Glossary of New Relic terms",
      "type": "docs",
      "tags": [
        "Using New Relic",
        "Welcome to New Relic",
        "Get started"
      ],
      "external_id": "8f8fc1ec9f41e6a4d6b4e986e9b0589bc2ca1f86",
      "image": "https://docs.newrelic.com/docs/glossary/glossary/images/account-dropdown.png",
      "url": "https://docs.newrelic.com/docs/glossary/glossary/",
      "published_at": "2021-12-19T13:38:30Z",
      "updated_at": "2021-12-18T01:39:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Whether you're considering New Relic One or you're already using our capabilities, this glossary of common terminology can help. And if you don't already have a New Relic account, don't hesitate to sign up at newrelic.com/signup. It's free, forever! account dropdown In the upper right of the New Relic UI, the account dropdown gives you access to your account settings. If you're trying to switch between accounts, use the account switcher. account switcher If you have access to more than one account in a multi-account organization, you can use the account switcher to switch between accounts. This is located in the top right of most New Relic UI pages. For more on factors that affect access to accounts, see Factors affecting access. To find account settings, use the account dropdown. administrator A type of user role on a New Relic account. For more information, see Users. agent At New Relic, an agent is a piece of monitoring software that provides integrations with various technologies (for example, web frameworks, host operating systems, or database types). The agents send that data to New Relic, usually on a specific cadence. For more information, see: New Relic Instant Observability Install agents agent API Some New Relic agents have agent APIs that allow you to extend the functionality of an agent. You can use the API to control, customize and extend the functionality of the agent. Here are some agent API docs: APM agents: C SDK API Go agent API Java agent API .NET agent API Node.js agent API PHP agent API Ruby agent API Python agent API Browser agent: Browser agent API Mobile agents: iOS SDK API Android SDK API aggregated metrics Aggregated metric data summarizes calls to specific methods in your application, including how many times each one was called and response times. In the New Relic UI, you see the class and method names along with their aggregate numbers. Metric data aggregation depends on the New Relic tool and your subscription level. For more information, see the documentation about data retention. aggregation delay The length of time in seconds to wait for the aggregation window to fill with data. Required when using CADENCE or EVENT_FLOW aggreation_method types. aggregation function You can use NRQL query functions, such as sum(), average(), or latest() to choose how the data points in an aggregation window should be processed into a single data point. The single aggregated data point is what's passed through the alert evaluation process. aggregation method New Relic aggregates data into windows, and needs to determine when the current window ends and the next one begins. The aggregation_method is the logic that tells us when we have all the data for a given aggregation window. Once the window is closed, the data is aggregated into a single point and evaluated against the threshold. This field is optional. One of the following three values can be specified: EVENT_FLOW: (Default) Each aggregation window will wait until it starts to see timestamps arrive that are past its own delay setting. Once this occurs, the data is published. Relies on the timestamps of arriving data, so wall-clock time is no longer relevant. Works best for sources that come in frequently and with low event spread (high througput metrics) CADENCE: Classic New Relic logic where each evaluation window waits exactly as long as the aggregation_delay setting, using the wall-clock time as a timer. aggregation_delay is required when using this option. Data arriving too late will be dropped, which can cause false alerts. EVENT_TIMER: Each aggregation window has a timer on it, set to the aggregation_timer setting. The Timer starts running as soon as the first data point appears for that aggregation window (based on the data point’s timestamp). The aggregation_timer is reset for each new data point that arrives for that window. Once the aggregation_timer reaches 0, the aggregation window is published. Ideal for sparse and batched data, such as cloud integrations and infrequent error logs. aggregation timer The length of time in seconds to wait after each data point received, to ensure the entire batch is processed. Required when using EVENT_TIMER aggregation_method type. aggregation window Streaming alerts gathers data together into specific amounts of time. These windows of time are customizable. Data points are collected together based their timestamps and reported as a batch. The customizable aggregation window provides greater flexibility and fewer false violations when alerting on irregular or less frequent data points. alert An alert communicates an event or incident that designated personnel can track through Alerts. For an explanation of how basic alerts concepts are related, see Concepts and workflow. alert condition An alert condition (or condition), identified by its unique numeric condition_id, contains the criteria for creating a violation. The condition includes the threshold that is set for a metric timeslice or a custom metric over time on a chosen target. For an explanation of how a condition relates to other basic alerts concepts, see Concepts and workflow. alert evaluation Streaming data is assessed on a set of aggregation windows to determine if an alert condition is violating or recovering. The aggregation window time is how long we'll collect data before running the NRQL query condition. The offset evaluation time is how long you want us to wait for late data before assessing it. If a window doesn't have any data points, it's treated as a gap for loss of signal. alert policy A collection of one or more conditions, one or more notification channels, and an Incident preference setting. If a condition contained within the policy opens a violation, an incident may be opened depending on the Incident preference setting. Notifications will then be sent to all channels attached to the policy. For an explanation of how a policy relates to other basic alerts concepts, see Concepts and workflow. apdex Apdex is an industry-standard way to measure users' satisfaction with the response time of an application or service. New Relic rates each response as Satisfied, Tolerated, or Frustrated, and uses these ratings to calculate an overall user satisfaction score. For more information, see Apdex: Measure user satisfaction. apdex_f The response time above which a transaction are rated frustrating. Defaults to four times apdex_t. Requests that complete in less than apdex_t are rated satisfied. Requests that take longer than apdex_t, but less than four times apdex_t (apdex_f), are tolerated. Any requests that take longer than apdex_f are rated frustrating. For more information, see Apdex: Measure user satisfaction. apdex_t The response time above which a transaction is considered tolerable. The default value is 0.5 seconds, but you can change this in your Apdex settings. Requests that complete in less than apdex_t are rated satisfied. Requests that take more than apdex_t, but less than apdex_f, are tolerated. Any requests that take longer than apdex_f are rated frustrating. For more information, see Apdex: Measure user satisfaction. API (application programming interface) New Relic offers a variety of APIs and SDKs. For more information, see the introduction to New Relic's APIs. APM New Relic's APM (application performance monitoring) provides monitoring of your web or non-web application's performance. APM supports apps using several programming languages. application For New Relic purposes, any program instrumented by New Relic. application ID Some New Relic solutions assign a monitored application a unique application ID, often shortened to app ID. When present, this ID is available in the UI. It is also reported as an attribute and can be queried. For how to determine this, see Find app ID. application name The name that New Relic combines with your license key to uniquely identify a particular app. For more information, see Name your application. Applied Intelligence (AI) Applied Intelligence (AI) helps you find, troubleshoot, and resolve problems more quickly. Specifically, it’s a hybrid machine learning engine that reduces alert noise, correlates incidents, and automatically detects anomalies. Applied Intelligence includes Alerts, Incident Intelligence, and Proactive Detection. attribute Attributes are key-value pairs attached to data objects reported to New Relic. Attributes add detail, and they're similar to tags or labels in other SaaS software. You can explore this data by querying or searching via the UI or by using the data dictionary. Examples: APM reports a Transaction event. This includes timing data for the transaction in a duration attribute, which might have a value of .002. Our Infrastructure Monitoring reports a ProcessSample event. This includes a variety of CPU usage attributes, including a cpuSystemPercent attribute, which might have a value of .01. Our Telemetry SDK reports a Metric data type for storing metrics, with attached attributes like metricName and newrelic.source. Some New Relic tools allow you to report custom attributes to enhance your monitoring. For more information about attributes in APM, see Agent attributes. availability monitoring See Types of Synthetics monitors. browser The New Relic UI supports most browsers. For more information, see Supported browsers. For our end-user browser monitoring tool, see Browser Monitoring. Browser monitoring A Real User Monitoring (RUM) solution that measures the speed and performance of your end users as they navigate to your site from different web browsers, devices, operating systems, and networks. background external See web external. child account See parent account. cloud-based integration New Relic offers cloud-based integrations with providers such as Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform. collector The component that collects data from New Relic agents running on an app server, mobile device, or end-user browser. While the agent is installed on a user's app server, the collectors are centrally located in New Relic's data center. In order to contact the collector, the agent must be able to reach New Relic's domains and IP addresses. (The exact domain or IP depends on the New Relic monitoring tool.) The collector receives and interprets this data, and stores it in a database. The data is then retrieved and presented in the New Relic UI and by our various REST APIs. Command line interface (CLI) Our command line interface (CLI) is a tool you can use to build a New Relic application. This is the same tool our own engineers use. Go here for quick start instructions. Go to our Developer site for sample apps and guides. compute unit (CU) A unit of measurement that determines your pricing for some New Relic products governed by our original product-based pricing model. For more information, see Compute unit pricing. condition_id See alert condition. CPM (calls per minute) The number of calls your application receives each minute. This usually corresponds to the number of page views or external connections, and is usually the same as RPM (requests per minute). CPU burn The time consumed by code minus the wait time for a transaction. This is the time actually spent processing the transaction. It appears in the New Relic UI at the top of the transaction view for the agents that provide it (Ruby and PHP only). custom attribute A key-value pair added to a transaction or event in order to gain additional information about it. For more information, see custom attributes. custom dashboard A customizable dashboard with charts and tables that includes data from multiple New Relic data sources. For more information, see dashboards. custom event An event, in New Relic terms, is a data object with attached attributes. New Relic reports default event types, like Transaction and TransactionError. You can also create your own events. Events can be queried, and are used in some other features. You can generate custom events with APM agents, the browser monitoring agent, the mobile monitoring agents, and via the Event API. Alternatively, you can add custom attributes to some existing default New Relic events. custom instrumentation Custom instrumentation allows you to extend New Relic's monitoring to instrument code elements New Relic doesn't automatically instrument. Custom instrumentation is useful when your framework is not supported by New Relic, or when New Relic fails to pick up some element of your program. You can also use custom instrumentation to block a transaction from being reported entirely. For more information, see Custom instrumentation. custom metric Metric timeslice data that is manually recorded via an API call. Custom metrics allow you to record arbitrary metrics; for example, timing or computer resource data. All custom metric names must be prefixed with Custom/. For more information, see Custom metrics. Not to be confused with custom instrumentation data. data collector See collector. data explorer Use the data explorer to access, query and customize your data, create visualizations, and make connections between your services in a consistent and curated experience. For more on using the data explorer, see Introduction to the data explorer. degradation period When a data source enters a violating state, a degradation period of time begins. The degradation period is set in the condition's threshold. A violation will open if the source stays in a violating state for the entire degradation period. In addition: If the data source enters a non-violating state before the entire time has elapsed, the degradation period countdown is reset, and a violation does not open. If your alert condition threshold is configured as at least once in, the degradation period always lasts a single minute. dimensional metric A dimensional metric is a metric that has multiple attributes, also known as dimensions. At New Relic, we report dimensional metrics using the Metric data type. For more on other metric data types, see Metric data. Docker An open platform for distributed applications, which allows you to assemble multi-container portable apps. Infrastructure Monitoring includes integrated Docker monitoring. For more information about Docker, see the Docker website. downtime The period of time when customers cannot access your site and your app is not reporting to New Relic. For more information, see Synthetic Monitoring and Types of synthetic monitors. entity In New Relic, an entity is anything we can identify that has data you can monitor. An entity can be something you monitor directly, like applications and microservices, or indirectly, like data centers. You can identify one or more entities to be targets for alert conditions. In the Alerts API, the entity being monitored is identified with an entity_id. For more on this, see What are entities? event The word event is a general term that can have many meanings. At New Relic, event can have several meanings: At New Relic, event data is one of our core data types. Event data represents a record of a single event at a particular moment in time. Events can vary by type (for example, Transaction or Mobile, and will have associated attributes (for example, timestamp or transactionName). For more details, see Event data. For our infrastructure monitoring, the word event can be used to refer to important system and host activity. For example, a configuration change for a monitored host would be registered on Infrastructure's Events UI page. For alerts, the Events UI page displays a list of alerts-related incidents for your monitored entities. Events are reported for a violation opening and for closing. In some contexts, event can refer to any NRQL-queryable data type. For example, when you run a NRQL query, you will see a count of inspected events: this refers to a count of all data types queried. expected error An expected error is a common error that you don't want to affect your Apdex score or error rate. For more information, see Manage errors in APM. exporter At New Relic, an exporter is a type of integration that reports telemetry data to New Relic from a third-party (non-New Relic) telemetry tool. For examples, see Exporters, or search our integration quickstarts in New Relic I/O. Flex New Relic Flex is an application-agnostic, all-in-one infrastructure integration. With it, you can build your own integration that collects metric data from a wide variety of services, and that can instrument any app that exposes metrics over a standard protocol (HTTP, file, shell) in a standard format (for example, JSON or plain text) to the terminal. It's a recommended way to create a custom integration, because it doesn't require coding skills. framework A framework is a structured collection of pre-defined functions, into which an application builder inserts their own code to build their application. A framework is not the same as a library. While a library is a collection of functions you can call as needed, a framework is a skeleton for your application. The functions in that framework then call your functions. For more about the distinction between a framework and a library, see What is the difference between a framework and a library?. New Relic automatically instruments many common frameworks. For more about the frameworks New Relic supports, see the agent-specific documentation: C SDK supported frameworks Go supported frameworks Java supported frameworks .NET supported frameworks Node.js supported frameworks PHP supported frameworks Python supported frameworks Ruby supported frameworks harvest cycle The period of time between each connection from a New Relic agent to the collector. Between harvest cycles, an agent collects and caches data. At the end of the cycle an agent reports those data to the collector, then begins a new harvest cycle. health status indicator Some New Relic UI pages have a health status indicator appearing next to an index of monitored entities. This is a colored bar (generally green, yellow, red, or gray) indicating the status of your app or other entity monitored by New Relic. It also indicates whether the entity has any alert policies assigned to it and whether there are any policy violations. In general, the colored bar will be green, yellow, red, or gray to indicate the health status. Exceptions: Our REST API (v2) uses orange instead of yellow for the application's health and reporting status. Service maps use different criteria for reporting the health of a connection between an app and an external service not monitored by New Relic (for example, a third party API). host At New Relic, a host means one of the following: A physical machine is a hardware-based device with dedicated physical resources, including memory, processing, and storage. Each machine has its own OS which applications run on. A virtual machine (VM) is the software implementation of a physical machine that executes programs like a physical machine. One or more virtual machines can run on a physical machine. Each virtual machine has its own OS and allocated virtual machine resources such as RAM and CPU. A cloud instance is a type of virtual machine that is run in the public cloud. In this context, virtual machines and cloud instances are different from Java Virtual Machines (JVMs) and containers. host ID Each host identified by APM is assigned a host ID. This ID is used to uniquely identify it, and to retrieve data about that host via the REST API. For more information, see List host ID. ignored error An error that you have told the APM agent not to report to the collector. For more information, see Manage errors in APM. incident An incident is a collection of one or more violations of the conditions defined in an alert policy. An incident record includes all of the open and close time stamps for each violation, as well as chart snapshots of the data being evaluated around the time of each violation. You can view detailed information from the Incidents pages in the user interface. You can also select your preference for how we roll up violations into the incident. For an explanation of how an incident relates to other basic alerts concepts, see Concepts and workflow. Infrastructure monitoring By connecting changes in host performance to changes in your configuration, infrastructure monitoring provides real-time metrics and powerful analytics that reduce your mean-time-to-resolution (MTTR). Infrastructure is specifically designed for complex environments that need flexible, dynamic server monitoring, from a physical datacenter to thousands of Amazon Elastic Compute Cloud (Amazon EC2) instances and other types of integrations. Insights Insights was the name for the New Relic product that previously governed the reporting of custom events, as well as the ability to query and chart your New Relic data. These features are now a fundamental part of the New Relic One platform and are no longer governed by the Insights product or name. To learn more about these features: Event API for reporting custom events Query and chart data For historical reasons, the word \"Insights\" is still used in some places. For example: Some APM agents still have Insights language in their codebase. For example, the Java agent custom_insights_events configuration. For New Relic organizations on our original pricing model, Insights Pro is still the product name governing custom event data ingest and retention. There is an API key called the Insights insert key. instance ID Each instance identified by New Relic is assigned a unique instance ID. Instance IDs are most commonly found for JVMs (Java Virtual Machines), but can exist for each agent. This ID is used to uniquely identify it, and to retrieve data about that instance via the REST API. For more information, see List instance IDs. instrumentation The collection of data from an application or host. When New Relic instruments a framework, it detects the methods and calls used by that framework, and intelligently groups them together. integration At New Relic, an integration refers to a solution that integrates with a specific technology (like a web framework or a type of database). All our integrations can be found as quickstarts in New Relic Instant Observability. interaction In our mobile monitoring, an interaction is a specific code path initiated by a user interaction (usually a button press). An interaction is the mobile equivalent of a transaction, and like a transaction an interaction can be traced and monitored. You can see much of the data included in an interaction in the BrowserInteraction event. interaction trace An interaction trace is a complete picture of a single interaction. With interaction traces, New Relic gives you much deeper visibility into a single slow interaction, which can help you understand a broader problem. Interaction traces are the mobile equivalent of a transaction trace. For more information, see Creating interactions (iOS) and Creating interactions (Android). inventory data Inventory data is information about the status or configuration of a service or host. Examples of inventory data include: Configuration settings Name of the host the service is on Amazon AWS region Port being used For more information, see Understand and use data. key transaction A web transaction that the user has marked as particularly important; for example, key business events (such as signups or purchase confirmations), or transactions with a high performance impact (such as searches). Key transactions have their own pages in the UI and other customized values. For more information, see Key transactions. launcher A launcher is a specific piece of code you can include when you create a New Relic One app. It creates the tile on the homepage that you click to launch the app. For more information, see the documentation about core UI components. log A log is a message about a system used to understand the activity of the system and to diagnose problems. For more information on how we use log data, see Log management. Log monitoring Our log management and monitoring features give you the tools to collect, process, explore, visualize, and alert on your log data using your existing log forwarder. With all of your log data in one place, you'll be able to make better decisions, detect and resolve problems more quickly, and see your logs in context to troubleshoot faster. Logs Our Logs feature is a scalable log management platform that allows you to connect your log data with the rest of your telemetry data. Pre-built plugins with some of the most common open-source logging tools make it simple to send your data from anywhere to New Relic. Logs in context Logs in context makes it easy to link to your log data with related data across the rest of our platform. Bringing all of this data together in a single tool allows you to quickly get to the root cause of an issue and find the log lines that you need to identify and resolve a problem. master account See parent account. metric A metric is a numeric measurement. Metric data is a broad category because there are several ways to make and report measurements. For more about how metrics are reported at New Relic, see New Relic data types. metric timeslice New Relic reports metrics in several ways. One variety of metric data is called metric timeslice data; this is the type of data used to generate many of the charts in APM, mobile monitoring, and browser monitoring (for more details, see metric timeslice data). Over time, metric timeslice data is aggregated into longer timeslice data records for more efficient storage. For more about how we aggregate this type of data, see Data aggregation. For how to query this type of data, see Query metric timeslice data. metric grouping issue A metric grouping issue occurs when an account sends too many differently named metric timeslice data points to New Relic, and those individual web transactions are not properly aggregated. For example, rather than a single /user/controlpanel/ metric name, you might see /user/controlpanel/alice, /user/controlpanel/bob, and /user/controlpanel/carol. For more information, see Metric grouping issues. minion The software that accepts monitor jobs from a private location. A minion is a packaged virtual appliance that runs in your hypervisor. For more information, see Private locations overview and install and configure private minions. Mobile monitoring Mobile monitoring allows you to monitor and manage the performance of your mobile apps on Android, iOS, tvOS, and other systems. Mobile monitoring provides end-to-end details, including crashes, throughput, HTTP requests, error traces, and more. Not to be confused with New Relic's own mobile apps for Android, iPhone, and iPad. monitor For our Synthetic Monitoring, a monitor ensures your website or API endpoint is available. For more information, see Adding and editing monitors. NerdGraph NerdGraph is our GraphQL API, an efficient and flexible query language that lets you request exactly the data you need, without over-fetching or under-fetching. NerdGraph calls get all the data you need in a single request. NerdGraph also makes it easier to evolve APIs over time and enables powerful developer tools. You can use our NerdGraph GraphiQL explorer to explore the schema and find definitions. With valid New Relic API key, you can try it out yourself at api.newrelic.com/graphiql. Nerdlet A Nerdlet is a component of a New Relic One application. It's a specific UI view, represented by a React JavaScript package. For more information, see Nerdpack file structure. Nerdpack A Nerdpack is a component of a New Relic One application. It's the package containing all the files needed by that application. For more information, see Nerdpack file structure. New Relic Edge with Infinite Tracing New Relic Edge with Infinite Tracing is a fully managed, distributed tracing service that observes 100% of your application traces, then provides actionable data so you can solve issues faster. For more information, see /docs/understand-dependencies/distributed-tracing/get-started/how-new-relic-distributed-tracing-works. New Relic One For more information, see Introduction to New Relic One. New Relic One catalog Our catalog is a collection of applications built on the New Relic One platform. The catalog includes custom apps we've built, public open source apps, and any apps that you buid. You can browse the catalog on New Relic One. NRQL (New Relic query language) NRQL is a query language, similar in form to SQL, that allows you to query the data stored in your New Relic account. non-web transaction APM identifies transactions as either web or non-web. When New Relic does not detect a transaction was initiated by a web request, this is called a non-web transaction. For more information, see Background processes and other non-web transactions. notification The message sent when an incident opens, is acknowledged, or closes. The type of notification is defined by the alert policy's notification channel. For an explanation of how notifications relate to other basic alerts concepts, see Concepts and workflow. notification channel Where we send a notification when an incident opens, is acknowledged, or closes. Available channels include email, mobile push notifications, webhooks, and more. on-host integration On-host integrations refer to integrations that reside on your own servers or hosts and that communicate with our infrastructure agent. For more information, see Introduction to on-host integrations. owner For accounts on our original pricing model, this is a type of user role: the user who initially created the account. For more information, see Users. page load timing With page load timing, New Relic monitors the full load time for end-user browsers. New Relic's application agents dynamically inject JavaScript into the page, then capture the following key load points: Navigation start: The user initiates the transaction. First byte: The browser receives the requested page. DOM ready: The browser has finished parsing DOM. Page ready: Page loading is complete. Page load timing is sometimes referred to as RUM, or real user monitoring. Unlike standard RUM, page load timing also captures JavaScript errors and AJAX requests. For more information, see Page load timing process. parameter Deprecated term; see attribute. parent account New Relic organizations can have a parent/child account structure. This structure was much more important for organizations on our original user model, but is still used for some features for organizations on the New Relic One user model. Learn more about account structure. Parent accounts were previously referred to as \"master accounts\", and child accounts were previously referred to as \"sub-accounts\". permalink A unique URL that links to a view of your application at a specific point in time. Permalinks are useful for troubleshooting and for sharing interesting time windows with colleagues. pinger The component of New Relic that connects to your website to verify your website is accessible. New Relic has pingers in Europe, Asia, and the United States. Each pinger attempts to contact your website at least once every two minutes. If enough pingers are unable to reach your website, your application will be considered down. For in-depth scriptable testing, including real browser tests and tests of API endpoints, see Synthetic Monitoring. Synthetic Monitoring includes free ping monitoring, which allows you to monitor your website from locations around the world. For more information, see Types of Synthetic monitors. polling interval (AWS) Our Amazon integrations query your AWS services according to a polling interval, which varies depending on the integration. Each polling interval occurs for every AWS entity. For example, if you have thirteen Elastic Load Balancers (ELB), each one will be polled every five minutes. Depending on the AWS integration, there may be delays in the timing between the API request and the metric data returned. If you notice unusual delays, follow the integration troubleshooting procedures. PPM (pages per minute) The number of pages per minute your application serves. private location A Synthetic monitor feature that allows you to run Synthetic monitors from within your own systems by creating private minions. Private locations allow you to extend your Synthetic coverage to new geographical locations, and to monitor websites behind your firewall such as an intranet site. For more information, see Private locations overview. recovery period A recovery period of time begins when a data source enters a non-violating state after being in a violating state. The recovery period is set in the condition's threshold. A violation will close when a source remains in a non-violating state and the recovery period time has elapsed. If the data source enters a violating state before the time has elapsed, the recovery period clock will reset and the violation won't close. response time The duration of time between a request for service and a response. For more information, see Response time. restricted user A type of user role on a New Relic account. For more information, see Users. rollup Using the same application name for multiple applications. This allows you to combine data in APM, either from multiple applications, or from multiple instances of an application. For more information, see Rolling up app data. root span For distributed tracing, the root span is the first span in a trace. In many cases, the root span duration will represent the duration of the entire trace, or be very close to it. However, for more complex, modern systems that use a lot of asynchronous, non-blocking processes, this will not be true. For those systems, the root span’s duration may be significantly less than the duration of the trace. RPM The term RPM usually refers to the number of requests per minute your application receives from users. This is usually the same as CPM (calls per minute). Historically, some New Relic monitoring solutions, like APM and Browser Monitoring, used to contain RPM in the URL; for example, https://rpm.newrelic.com. This language use originally referred to Rails performance management because the first iteration of our product monitored Ruby on Rails applications. We monitor many more languages and systems than Ruby now. RUM (real user monitoring) See page load timing. runbook A runbook contains standard procedures and operations typically used by system administrators, network operations staff, and other personnel to handle outages, alert incidents, and other situations. If your organization stores runbook instructions as URLs, you can link this information to an alerts policy so your personnel has easy access to this information when an incident violates the defined policy thresholds. SAML (Security Assertion Markup Language) SAML is an XML-based data format for sharing authentication data between two parties. New Relic accounts must obtain a SAML certificate in order to enable Single Sign On for their users. For more information, see SAML service providers. Selenium Selenium is an open-source browser testing suite. Synthetics uses Selenium to test monitored websites with real browsers. For more information, see monitor types. service A service is a cluster of runtime server processes that accomplish a particular task, usually service requests. Unlike an application, a service is not usually invoked by a human. New Relic offers a variety of integrations that allow you to report data from your services. signal The stream of telemetry data that's watched and alerted on. You use NRQL queries to define a signal. signal filter When we receive data and it's routed to the streaming alerts platform, your NRQL WHERE clause will filter the data coming in. The filtered streaming data is what's evaluated for loss of signal violations, for example. span In a distributed trace, a span is a \"named, timed operation representing a contiguous segment of work in that trace\" (from OpenTracing.io definition). For distributed tracing, spans are displayed in the distributed tracing UI, and the data type Span is available to be queried. See also root span. SSL certificate SSL certificates encrypt data that is being transmitted. While New Relic refers to security certificates as SSL because it is a more commonly used term, all certificates adhere to industry standards for secure encryption in transit. SSO (single sign on) SSO (single sign on) allows you to manage user authentication in New Relic using an external SSO provider. For more information, see Setting up SSO. streaming algorithm This is what determines when the data in an aggregation window is processed. The streaming algorithm uses your server's clock time and the aggregation window size to trigger the alert evaluation process. sub-accounts See master account. Synthetic monitoring Synthetic monitoring allows you to monitor your website or API endpoint via automated, scriptable tools. Use free ping monitor to ensure your website is accessible, or expand your monitoring with browser monitors, which test your website with real browsers. Go further with scripting, to script browsers or API monitors for sophisticated testing. target A target is a resource or component monitored by a New Relic monitoring tool that has been identified in an alert condition. When the data source for that target crosses the defined critical threshold, we will open a violation. Depending on your policy's Incident preference setting, Alerts may create an incident record and send notifications through the defined channels. See also entity. tag Tags are key:value metadata added to monitored apps, hosts, dashboards, and other entities to help you organize your data at a high level. For details, see Tags. thresholds Thresholds are alert condition settings that define a violation. Threshold values include the value a data source must pass to trigger a violation and the time-related settings that define a violation; for example: Passing a certain value for at least x minutes Passing a certain value only once in x minutes While the data source passes a certain value, a degradation period starts. Likewise, when that data source stops passing a certain value, a recovery period starts. The durations of these two time periods are defined in the alert condition threshold settings. Thresholds have a required critical (red) threshold and an optional warning (yellow) threshold. In the UI, the entity's health status indicator will change to yellow or red when a threshold has been crossed and a violation will open. For more information, see Define thresholds. For an explanation of how thresholds relate to other basic Alerts concepts, see Concepts and workflow. throughput Throughput is a measurement of user activity for a monitored application. APM throughput and Browser Monitoring throughput are measured in different ways: APM: requests per minute (RPM) Browser: page views per minute (PPM) tier A tier can refer to how New Relic categorizes or visualizes the various agent language ecosystems that we support. For example: In APM, the color-coded categories that appear on your app's main Overview chart show response time spent in various functions, processes, or agents as tiers; for example, request queuing, garbage collection, Middleware, JVMs, etc. In New Relic labels, TIER can be used to define or classify the client-server architecture; for example, front-end and back-end tiers. \"Tier\" may sometimes be used to refer to our pricing editions. time picker By default the New Relic UI shows data for the past 30 minutes, ending now. To change the time window, use the time picker. time range A time range can refer to a length of time selected in the New Relic UI. New Relic displays a time range depending on the range you select using the time picker. timeslice data See metric timeslice data. trace A trace is a description of how a request travels through a system. Trace data helps you understand the performance of your system and diagnose problems. For more information on how we use trace data, see New Relic data types. traffic light See health status. transaction A transaction is defined as one logical unit of work in an application. This term primarily refers to server-side transactions monitored by APM. For more information, see documentation about web transactions and non-web transactions. The term transaction is also sometimes used in Browser Monitoring. In that case, it primarily refers to activity beginning with a browser-side web request and ending with a complete page load. transaction trace A transaction trace is a complete picture of a single transaction, down to the database queries and exact invocation patterns. With transaction traces, New Relic gives you much deeper visibility into a single slow transaction, which can help you understand a broader problem. For more information, see Transaction traces. UI The New Relic user interface. For more information, see Standard page functions. user A user can refer to a specific user role in a New Relic account. For more information, see Users. UTC Universal Time Coordinated (UTC), or Coordinated Universal Time, is a standard timestamp for synchronizing time around the world. value function (metrics) The numeric value obtained from metric timeslice data; for example, an average, minimum, maximum, total, sample size, etc. violation A violation occurs when the entity monitored by an alert condition reports a value that crosses the thresholds defined in that condition. For an explanation of how violations relate to other basic alerts concepts, see Concepts and workflow. You can view a summary of the violations for a selected incident's page. You can also view the violations for a specific entity from the product's UI. web external Web external is the term applied to the portion of time spent in transactions to external applications from within the code of the application you are monitoring. That time can be a call to a third party company (a payment provider, for example) or it could be a call to another microservice within your own company. Web external demonstrates how performance is impacted by your code executing outside the application you are measuring. web transaction A transaction is defined as one logical unit of work in an application. This term primarily refers to server-side transactions monitored by APM. Web transactions are initiated with an HTTP request. For most organizations, these represent customer-centric interactions and thus are the most important transactions to monitor. For more information, see Web transactions and Non-web transactions. WebDriverJS WebDriver is a Selenium component, used to control Synthetics scripted browsers. Specifically, Synthetics uses WebDriverJS, a Node.js-based flavor of Selenium. For more information, see Writing scripted browsers and Scripted browser examples. workload A workload represents a group of entities that work together to provide a digital service. For more information, see Workloads.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 309.73672,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Glossary of <em>New</em> <em>Relic</em> terms",
        "sections": "Glossary of <em>New</em> <em>Relic</em> terms",
        "tags": "<em>Welcome</em> <em>to</em> <em>New</em> <em>Relic</em>",
        "body": " line interface (CLI) is a tool you can <em>use</em> to build a <em>New</em> <em>Relic</em> application. This is the same tool our own engineers <em>use</em>. Go here for quick <em>start</em> instructions. Go to our Developer site for sample apps and guides. compute unit (CU) A unit of measurement that determines your pricing for some <em>New</em> <em>Relic</em>"
      },
      "id": "61b40189196a672dd0a5aa8c"
    },
    {
      "sections": [
        "Find help and use the Support portal",
        "Ask in New Relic's Explorers Hub, our free forum",
        "Run the New Relic Diagnostics tool",
        "Find answers in New Relic Docs and New Relic University",
        "Contribute to our documentation",
        "Don't find what you need? File a documentation issue",
        "File a ticket in the support portal",
        "Important",
        "Check the status of our systems",
        "Licenses and security information"
      ],
      "title": "Find help and use the Support portal",
      "type": "docs",
      "tags": [
        "Using New Relic",
        "Welcome to New Relic",
        "Get started"
      ],
      "external_id": "ce14a732f0aae92aa495e61aac701f6880378a3d",
      "image": "https://docs.newrelic.com/docs/new-relic-solutions/solve-common-issues/find-help-use-support-portal/images/new-relic-explorers-hub.png",
      "url": "https://docs.newrelic.com/docs/new-relic-solutions/solve-common-issues/find-help-use-support-portal/",
      "published_at": "2021-12-19T14:22:31Z",
      "updated_at": "2021-12-14T04:10:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers a variety of support options, including online help, a troubleshooting tool, open source documentation with detailed procedures and troubleshooting tips, and support assistance. Ask in New Relic's Explorers Hub. Run the New Relic Diagnostics tool. Find answers in New Relic Docs and New Relic University. Contribute to our documentation. Don't find what you need? File a documentation issue. File a ticket in the support portal. Check the status of our systems. Read about our licenses, data security, and compliance information. Ask in New Relic's Explorers Hub, our free forum New Relic's Explorer Hub is our forum that's free for all users. New Relic users and employees engage every day in conversations to troubleshoot and solve issues, find workarounds, and discuss new features. discuss.newrelic.com: The Explorer Hub is our public forum. Use it to ask questions and find answers. Join our community of users to learn more about New Relic and get some inspiration. Run the New Relic Diagnostics tool New Relic Diagnostics is our automated diagnostic tool for Linux, Windows, and Mac. If it detects a problem with any of our agents, it suggests solutions and saves troubleshooting logs that you can attach to tickets. Find answers in New Relic Docs and New Relic University New Relic's docs site contains helpful installation, configuration, and troubleshooting tips. From the main page, select from frequently-used categories and topics, like release notes. Or, search from any page. For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources. Contribute to our documentation Our documentation is open source and available in GitHub, and we encourage you to contribute! We really care about ensuring our docs are helpful, complete, and accurate. To edit a page, click the Edit page button in any document to create a pull request with the edit you think is needed. We don't want you to worry about style. When you edit a file, tech writers on our team review it for style, grammar, and formatting. docs.newrelic.com: At the right-hand side of each page you'll find a feedback widget. Use it to tell us whether a page is helpful, create an issue, or edit the page. Don't find what you need? File a documentation issue If you can't find an answer in the documentation, you can file an issue to ask us for help. When you find places the docs could be better, let us know too! To do it, click the Create issue button in any document and we'll look into your problem to find a solution. docs.newrelic.com: At the right-hand side of each page you'll find a feedback widget. Use it to tell us whether a page is helpful, create an issue, or edit the page. File a ticket in the support portal If none of the above methods worked, go to support.newrelic.com. The Support portal gives you access to unified search across all of New Relic's help resources. If you can't find what you are looking for and your subscription level includes technical support, you can file a support ticket. Important Support for beta or limited release features may not be available. To file a new ticket: Go to support.newrelic.com > Login. From the Support portal, select the area of New Relic where you need help. Select your account. Provide as many details as possible. Include the URL, if applicable, or select Attach file to include a log file, a New Relic Diagnostics file, screenshots, or other useful attachments. Click Submit. Check the status of our systems It's always a good idea to visit status.newrelic.com to check the status of our systems. If there are open incidents, you'll be able to find more information. Licenses and security information Review New Relic's licenses, attributions, and other notices. Read about our data security, privacy, and compliance policies.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 259.95816,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Find help and <em>use</em> the Support portal",
        "sections": "Ask in <em>New</em> <em>Relic&#x27;s</em> Explorers Hub, our free forum",
        "tags": "<em>Welcome</em> <em>to</em> <em>New</em> <em>Relic</em>",
        "body": " <em>Relic</em>&#x27;s Explorer Hub is our forum that&#x27;s free for all users. <em>New</em> <em>Relic</em> users and employees engage every day in conversations to troubleshoot and solve issues, find workarounds, and discuss <em>new</em> features. discuss.newrelic.com: The Explorer Hub is our public forum. <em>Use</em> it to ask questions and find"
      },
      "id": "603eb6b5e7b9d299072a07e5"
    },
    {
      "sections": [
        "View our UI in dark mode",
        "Switch between light and dark mode"
      ],
      "title": "View our UI in dark mode",
      "type": "docs",
      "tags": [
        "Using New Relic",
        "Welcome to New Relic",
        "Get started"
      ],
      "external_id": "b36ae1720e821d90136751f9a2f90d3d1f030c16",
      "image": "https://docs.newrelic.com/static/cbbd29dd03ca29a197b3cee88707141b/c1b63/APM_distributed_tracing_dark-Mode.png",
      "url": "https://docs.newrelic.com/docs/new-relic-one/use-new-relic-one/ui-data/view-our-ui-dark-mode/",
      "published_at": "2021-12-19T17:36:44Z",
      "updated_at": "2021-12-14T03:44:46Z",
      "document_type": "page",
      "popularity": 1,
      "body": "When you're checking on incidents in the middle of the night, or just want to avoid straining your eyes, you can view our UI in dark mode. To switch between light mode and dark mode in New Relic's UI, click the account dropdown. Switch between light and dark mode To view our UI in dark mode: From one.newrelic.com, click the account dropdown. [ NOTE: Currently dark mode is not supported in the Firefox browser.] Click Theme: to switch between the following options: Theme: auto (default): matches the UI with what you already have enabled with your OS. Theme: dark: keeps the UI in dark mode. Theme: light: keeps the UI in light mode.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 259.79626,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Welcome</em> <em>to</em> <em>New</em> <em>Relic</em>",
        "body": "When you&#x27;re checking on incidents in the middle of the night, or just want to avoid straining your eyes, you can view our UI in dark mode. To switch between light mode and dark mode in <em>New</em> <em>Relic</em>&#x27;s UI, click the account dropdown. Switch between light and dark mode To view our UI in dark mode: From"
      },
      "id": "603e987964441ff7db4e8876"
    }
  ],
  "/docs/accounts/accounts-billing/account-setup/create-your-new-relic-account": [
    {
      "sections": [
        "Install New Relic",
        "Install APM",
        "Install browser monitoring",
        "Install infrastructure monitoring",
        "Install mobile monitoring",
        "Install synthetic monitors",
        "Troubleshooting"
      ],
      "title": "Install New Relic ",
      "type": "docs",
      "tags": [
        "Using New Relic",
        "Cross-product functions",
        "Install and configure"
      ],
      "external_id": "819ccfd8df22ff322271245ca0831bf53609b91f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/new-relic-one/use-new-relic-one/cross-product-functions/install-configure/install-new-relic/",
      "published_at": "2021-12-16T01:39:49Z",
      "updated_at": "2021-12-14T03:50:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you sign up for a New Relic account (it's free, forever!) and install any of our monitoring services, you can start working with your data. Get started quickly with our New Relic Instant Observability quickstarts. Here are links to instructions on how to install New Relic monitoring services: APM Browser Infrastructure Mobile Synthetic monitors Integrations for third-party telemetry services Data ingest APIs (metrics, events, logs, traces) Install APM C Go Java .NET For Windows .NET applications on IIS, use our guided install in New Relic One to get started with APM. If you're on an EU server, use our launcher for EU accounts instead. Node.js PHP Python Ruby Install browser monitoring See browser monitoring install. Install infrastructure monitoring Linux guided install for infrastructure monitoring Linux procedures for infrastructure monitoring Windows Kubernetes Prometheus On-host integrations (for services like NGINX, StatsD, MySQL, etc.) AWS cloud integrations Azure cloud integrations Google Cloud Platform Install mobile monitoring Android iOS Install synthetic monitors Synthetic monitoring doesn't require installation, except for its private minions feature. Troubleshooting You should start seeing your data in the New Relic UI after installing the agent, generating some traffic, and waiting a few minutes. If no data appears, follow our troubleshooting procedures.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 555.05774,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install <em>New</em> <em>Relic</em> ",
        "sections": "Install <em>New</em> <em>Relic</em>",
        "tags": "Using <em>New</em> <em>Relic</em>",
        "body": "After you <em>sign</em> <em>up</em> for a <em>New</em> <em>Relic</em> account (it&#x27;s free, forever!) and install any of our monitoring services, you can start working with your data. Get started quickly with our <em>New</em> <em>Relic</em> Instant Observability quickstarts. Here are links to instructions on how to install <em>New</em> <em>Relic</em> monitoring services"
      },
      "id": "61b8148ce7b9d22373ef3a8b"
    },
    {
      "image": "https://docs.newrelic.com/static/d2a9c929c7541b67b6fe4c87844fc01b/ae694/prometheus_grafana_dashboard.png",
      "url": "https://docs.newrelic.com/whats-new/2020/08/create-grafana-dashboards-prometheus-data-stored-new-relic/",
      "sections": [
        "Create Grafana dashboards with Prometheus data stored in New Relic",
        "Step 1: Get data flowing into New Relic with the Prometheus remote write integration",
        "Step 2: Configure your Grafana dashboards to use Prometheus data stored in New Relic"
      ],
      "published_at": "2021-12-14T23:02:34Z",
      "title": "Create Grafana dashboards with Prometheus data stored in New Relic",
      "updated_at": "2021-10-19T05:58:32Z",
      "type": "docs",
      "external_id": "da09ab47a2ac806ad3ed1fa67e3a02dd54394383",
      "document_type": "nr1_announcement",
      "popularity": 1,
      "body": "We’ve teamed up with Grafana Labs so you can use our platform as a data source for Prometheus metrics and see them in your existing dashboards, seamlessly tapping into the reliability, scale, and security provided by New Relic. Follow the steps below or use this more detailed walkthrough to send Prometheus data to New Relic, so that Grafana can populate your existing Prometheus-specific dashboards with that data. This process requires Prometheus version 2.15.0 or higher and Grafana version 6.7.0 or higher. You’ll also need to sign up for New Relic. Here's an example of how these Grafana dashboards with Prometheus data look in our new dark mode. Step 1: Get data flowing into New Relic with the Prometheus remote write integration Go to Instrument Everything – US or Instrument Everything – EU, then click the Prometheus tile. You can also go to the Prometheus remote write setup page to get your remote_write URL. For more information on how to set up the Prometheus remote write integration, check out our docs. Step 2: Configure your Grafana dashboards to use Prometheus data stored in New Relic For more information on how to configure New Relic as a Prometheus data source for Grafana, check out our docs.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 533.48517,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create Grafana dashboards with Prometheus data stored in <em>New</em> <em>Relic</em>",
        "sections": "Create Grafana dashboards with Prometheus data stored in <em>New</em> <em>Relic</em>",
        "body": " Prometheus data to <em>New</em> <em>Relic</em>, so that Grafana can populate your existing Prometheus-specific dashboards with that data. This process requires Prometheus version 2.15.0 or higher and Grafana version 6.7.0 or higher. You’ll also need to <em>sign</em> <em>up</em> for <em>New</em> <em>Relic</em>. Here&#x27;s an example of how these Grafana"
      },
      "id": "60445821e7b9d23b585799e4"
    },
    {
      "sections": [
        "Introduction to APM",
        "Tip",
        "Ready to get started?",
        "Identify problems before your users do",
        "Monitor all aspects of your business"
      ],
      "title": "Introduction to APM",
      "type": "docs",
      "tags": [
        "APM",
        "Getting started"
      ],
      "external_id": "317c07d4b32daa51186fd7d2cb857c392b595b5c",
      "image": "https://docs.newrelic.com/static/576604215ce4407c00e97b1072e557b5/c1b63/apm-screenshot.png",
      "url": "https://docs.newrelic.com/docs/apm/new-relic-apm/getting-started/introduction-apm/",
      "published_at": "2021-12-19T13:44:26Z",
      "updated_at": "2021-12-04T01:47:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Application performance monitoring (APM) is exactly what it sounds like: monitoring of your web or non-web application's performance. APM supports apps for several programming languages, including Go, Java, .NET, Node.js, PHP, Python, and Ruby, as well as a C SDK. View the status of all your services at a glance with APM. Tip This doc gives you detailed information about why APM matters to your business. But if you want to skip ahead, just sign up for a New Relic account. (It's free, forever!) Then, after you install the agent, you can start working with your data. Ready to get started? Start benefiting from APM in five simple steps (and just a few minutes!). Sign up for a New Relic account (it's free, forever!). Install and customize the language agent for your app. Generate some traffic for your app. Wait a few minutes for New Relic to start receiving your data. Log in to your account, and start exploring New Relic! Use the Explorer in New Relic One to access and observe the full stack of your software, including your apps, see performance data and alerting status at a glance, and check relationships. We provide you with a simple, yet powerful visual tool to monitor all your entities, that is, anything we can identify that reports data. In the New Relic ecosystem, entities include basic components like applications, hosts, containers, or database services, but it can also refer to custom groupings of such elements. You can also create your own entities. If data does not appear after waiting a few minutes, follow the troubleshooting tips for your APM agent. Identify problems before your users do Our APM agents report and store the state of your application so you can quickly identify potential problems before they affect your end users. With APM, your DevOps teams don't need to guess whether a performance blocker comes from the app itself, CPU availability, database loads, or something entirely unexpected. Use our APM solutions to gather both current and historical information about memory usage, CPU utilization, database query performance, web browser rendering performance, app availability, error analysis, external services, and other useful metrics. Monitor all aspects of your business Take advantage of these APM features, and more: Features Description App performance at a glance When you sign in to APM and select an app for your account, use the Summary page to quickly examine relationships across different aspects of your environment; for example: Web transactions response time: Where is the most time being spent? In the request queue, during different stages of page rendering and execution, from external services, or something else? Transaction traces: Which transactions are the slowest, and why? Error rate and throughput: What relationship is there between a spike in errors or slower throughput for a particular time period? Was there a deployment or outage at that time? Hosts: What kind of impact does this have on CPU usage, memory, etc.? Apdex: How are these events affecting customers' satisfaction with the site? Web and non-web transactions Start by comparing the top twenty web transactions or non-web transactions in terms of most time consuming, slowest average response time, highest throughput, or worst Apdex. From there, drill down into deeper trace levels for individual transactions, which in turn break down into smaller segments and components, from HTTPS requests on down to SQL queries. Want to explore even deeper? Set up distributed tracing to see how requests move across a distributed system. Select the transactions that are most important to your business (key transactions). APM and Infrastructure When your APM and Infrastructure accounts are linked, you will have access to APM data charts on these Infrastructure UI pages: Hosts, Network, Storage, and Processes. Distributed tracing Distributed tracing gives you visibility across distributed systems, showing you the path of a request as it travels between services. This feature is especially valuable for large, distributed systems that rely on many small services and microservices. Logs Bring your logs and application's data together to make troubleshooting easier and faster. With logs in context, you can see log messages related to your errors and traces directly in your app's UI. You can also see logs in context of your infrastructure data, such as Kubernetes clusters. No need to switch to another UI page in New Relic One. Service maps APM's service maps show your app's connections and dependencies, including databases and external services. Health indicators and performance metrics show you the current operational status for every part of your architecture. You can create and customize service maps to visualize and monitor your complex architectures. Query your data You can query and visualize your data: Sample and search APM transaction and error data to gain full understanding of the data being collected. Browse your data visually with the data explorer. Create custom SQL-like queries of your data using the New Relic Query Language (NRQL), or using our PromQL-style queries. Use dashboards to build advanced data visualizations, contextualize data, and understand what's going on in your system, real-time. These are just a few of APM's features. To find out more, see the table of contents for APM.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 460.09543,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": " services at a glance with APM. Tip This doc gives you detailed information about why APM matters to your business. But if you want to skip ahead, just <em>sign</em> <em>up</em> for a <em>New</em> <em>Relic</em> account. (It&#x27;s free, forever!) Then, after you install the agent, you can start working with your data. Ready to get started? Start"
      },
      "id": "60440835e7b9d29c2f5799e0"
    }
  ],
  "/docs/accounts/accounts-billing/account-setup/downgradecancel-account": [
    {
      "sections": [
        "Troubleshoot password, email address, and login issues",
        "Tip",
        "Account access issues",
        "Password solutions",
        "Forgot your password, or your password does not work.",
        "Important",
        "Reset password.",
        "Password error messages.",
        "Password reset link expired.",
        "Added existing user to another New Relic account.",
        "Email solutions",
        "Did not receive new account confirmation email.",
        "Email address does not work or email error messages.",
        "Received message that your email account already exists.",
        "Email link does not redirect to the password reset page.",
        "Asked to verify email during login.",
        "General system solutions",
        "Delete an account you recently created.",
        "SAML single sign-on (SSO) problems prevent login.",
        "System failure errors prevent login.",
        "Mobile device solutions",
        "Solutions for other situations",
        "Unable to log in from your New Relic partner account."
      ],
      "title": "Troubleshoot password, email address, and login issues",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Account setup"
      ],
      "external_id": "fc93486097659dacf04d90cfc6b435cc6791ce6d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/account-setup/troubleshoot-new-relics-password-email-address-login-problems/",
      "published_at": "2021-12-19T15:40:46Z",
      "updated_at": "2021-11-14T08:56:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have problems with your email address or password when logging in to New Relic, review these troubleshooting tips. (Don't have a New Relic account yet? Sign up here. It's free!) Tip To learn about factors affecting access to features and data, see Factors affecting access. Account access issues If you're logged into New Relic but don't see the accounts or data you expect, see Factors affecting access. Password solutions New Relic requires a valid email address and a valid password in order for you to log in and to receive information from New Relic. If you are unable to create your account successfully, or if you have password or other login problems, review these troubleshooting suggestions to try to identify a resolution. Forgot your password, or your password does not work. If you have forgotten your New Relic account password, or if your password does not work, request a password reset from New Relic. If you have access to multiple New Relic logins, also ensure you are using the correct password for the login method you're using Go to New Relic's login page. Select Forgot your password. Type your account email address, and select Send my reset link. When you receive a confirmation email from New Relic, select the password reset link, and follow New Relic's password requirements to complete the process to reset your password. Important The password reset link expires after 12 hours. If you do not quickly receive an email from New Relic, check your spam filters, contact your organization's email administrator for troubleshooting suggestions, get support at support.newrelic.com. Reset password. If you forgot your own password or need to request a password reset for your account email, you can use New Relic's self-service options. Admin-level users cannot reset passwords for other users. If you need to reset someone else's password, get support at support.newrelic.com. Password error messages. If you complete the signup process and are unable to log in to your account due to password error messages, get support at support.newrelic.com. Password reset link expired. If you want to change your password but you see a message that the password reset link expired, try using a private browser or clearing your browser cache and cookies. Important The password reset link expires after 12 hours. If you do not quickly receive an email from New Relic after you select the reset link, check your spam filters, contact your organization's email administrator for troubleshooting suggestions, get support at support.newrelic.com. Added existing user to another New Relic account. If you're an existing New Relic account user and you have been added to another account, you don't need to create a new password. Your existing password will work for each account, and you can switch between accounts after logging in. Email solutions New Relic requires a valid email address and a valid password in order for you to log in and to receive information from New Relic. If you're unable to create your account successfully, or if you have email or other login problems, review these troubleshooting suggestions to try to identify a resolution. Did not receive new account confirmation email. When you first create your account, New Relic sends a confirmation email so you can complete the setup process and sign in. If you cannot locate your original account confirmation email: Go to New Relic's login page at login.newrelic.com/login. Select Forgot your password. Type your account email address, and select Send my password. When New Relic's system returns an email message, select the link in it to confirm your account again. If you don't receive an email from New Relic: Check your spam filters. If applicable, add New Relic to your email allow list. Get support at support.newrelic.com. Email address does not work or email error messages. If you complete the signup process and are unable to log in to your account due to email or password error messages, get support at support.newrelic.com. Received message that your email account already exists. If you are trying to create a new account and receive a message that your email account already exists, get support at support.newrelic.com. Email link does not redirect to the password reset page. If you have used your password reset link but are not redirected to the password reset page, troubleshoot the following: Your password reset link has expired. You will need to request a new link from the New Relic website. You are experiencing a caching issue. Clear your browsing cache or use a private browsing window before trying the link again. Tip Private browsing, also known as incognito mode, is a privacy feature to disable browsing history and the web cache. To open a private browsing window, you can use the keyboard shortcut CTRL+Shift+N on Windows and Command+Shift+N on Mac for most browsing applications. If this still doesn't solve your problem, get support at support.newrelic.com. Asked to verify email during login. If your email is associated with multiple accounts, you will be given the option to verify your email during login. This will allow you to choose which account to access. If you do not verify your email address, New Relic will attempt to log you in with the most recently created user record associated with your email address. To avoid verifying your email during each login, click the Remember Me checkbox in the login screen. General system solutions Some general system problems and solutions: Delete an account you recently created. If you created a New Relic account unnecessarily and want to delete it, and if that account is a simple one, you may be able to delete it yourself. See Delete simple organization. SAML single sign-on (SSO) problems prevent login. If your organization uses a SAML Single Sign On (SSO) solution, you can skip the Password field when you log in. If you need to reset your password, contact your organization's system administrator or IT department as applicable. If you're an administrator who has recently enabled or made changes to your SAML SSO settings and are unable to log in, there might be an issue with your configuration. Customers on the New Relic One user model can use a special recovery flow to fix any issues. Visit login.newrelic.com/recovery_access to bypass SSO and gain one-time access to your organization. System failure errors prevent login. If you receive failure errors while trying to sign up, a third-party password manager may be triggering New Relic's spam trap. To work around this, try these solutions: Bypass your password manager. Use a different browser to sign up with New Relic. Get support at support.newrelic.com. Mobile device solutions If you are unable to log in from your mobile device, the new user authentication time frame may have expired. You must complete this process within 20 minutes of receiving New Relic's confirmation message for your mobile device. To solve this problem, request another confirmation message to be sent to your device. Also, depending on your New Relic account, additional installation or authentication steps may be required for your iOS or Android app account. Solutions for other situations Here are suggestions for other unique situations. Unable to log in from your New Relic partner account. With partner accounts, SAML SSO authentication to sign in to New Relic is controlled by the partnership. Depending on the partnership, you may or may not be able to log in directly to New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 211.69841,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Troubleshoot password, email address, <em>and</em> login issues",
        "sections": "<em>Account</em> access issues",
        "tags": "<em>Accounts</em> <em>and</em> <em>billing</em>",
        "body": " <em>account</em>, you don&#x27;t need to create a new password. Your existing password will work for each <em>account</em>, and you can switch between <em>accounts</em> after logging in. Email solutions New Relic requires a valid email address and a valid password in order for you to log in and to receive information from New Relic"
      },
      "id": "6043f38b28ccbc04a22fd1aa"
    },
    {
      "sections": [
        "Account ID"
      ],
      "title": "Account ID",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Account setup"
      ],
      "external_id": "3771d53b48cfcf2f29d303b1d77a3884e4bae480",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/account-structure/account-id/",
      "published_at": "2021-12-19T14:37:48Z",
      "updated_at": "2021-09-13T20:41:52Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some New Relic procedures require use of your account ID (for example, some API calls). Your account ID is the ID number we assign to a New Relic account. Note that some New Relic organizations contain multiple accounts. Options for finding your account ID: If your organization has multiple accounts: From one.newrelic.com, use the account selector dropdown near the top of the page to switch between accounts and see their IDs. There are other options, depending on which user model you're on: New Relic One user model: Click the account dropdown and then click Administration. Click Organization and access and then Accounts to see account IDs. Original user model: Click the account dropdown, click Account settings, and then click API keys. The account ID is displayed there. For more on how account access works, see Factors affecting access.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.30705,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Account</em> ID",
        "sections": "<em>Account</em> ID",
        "tags": "<em>Accounts</em> <em>and</em> <em>billing</em>",
        "body": "Some New Relic procedures require use of your <em>account</em> ID (for example, some API calls). Your <em>account</em> ID is the ID number we assign to a New Relic <em>account</em>. Note that some New Relic organizations contain multiple <em>accounts</em>. Options for finding your <em>account</em> ID: If your organization has multiple"
      },
      "id": "61271c84196a67ed0c00b367"
    },
    {
      "sections": [
        "Factors affecting access to features and data",
        "User permissions",
        "Account access",
        "Pricing plan or edition",
        "The entities being monitored",
        "Data retention"
      ],
      "title": "Factors affecting access to features and data",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Account structure"
      ],
      "external_id": "98bf366604050b491ee35c8422414c70bda452b4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/account-structure/factors-affecting-access-features-data/",
      "published_at": "2021-12-19T14:14:41Z",
      "updated_at": "2021-11-24T11:11:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "There are several factors that can affect your access to a New Relic feature, or New Relic data. User permissions User-related settings can impact what features or data you have access to. Potential impacts include: Your user type (basic user or full platform user). Your assigned roles. How your user roles and permissions work depends on which user model you're on: Original user model roles New Relic One user model roles If you think your permissions are preventing you from accessing something, talk to your New Relic administrators. Account access If you're having trouble finding an account, here are some causes and solutions: If your organization has multiple accounts and you have access to those accounts, you will see an account switcher at the top left of most New Relic UI pages that will let you switch between accounts. If you're logged into New Relic but can't find an account or its data, it may be for one of these reasons: You may need to be added to that account. How you do this will depend on your user model: Original use model | New Relic One user model. You may have more than one New Relic login associated with the same email address. If you think this may be the case, log out and log back in. When you input your email address, the login UI notes if you have multiple user records and gives you an option for verifying your email to see all available accounts when you log in. Other details about multiple logins: To see all your available login options the next time you log in, select Remember me when logging in. If one of your login options reads \"Original account\", that means it's a user record on our original user model. For more information, see this Explorers Hub post about multiple accounts. Related docs: User permission factors Account structure Login and password troubleshooting Delete accounts Pricing plan or edition We have two pricing models available, and this can impact feature availability. Pricing plans: Our original product-based pricing model: This plan separates our offerings by product. If you’re on this plan, access to some features may depend on the products you pay for. Our New Relic One pricing model: This newer pricing model gives more cross-platform access. The main factors affecting access are your edition and your user type. We also have several pricing editions: Standard, Pro, and Enterprise. Learn more about our pricing models. The entities being monitored Some of our solutions enable functionality that isn’t available to all users, like: Additional UI components, and Data available for querying For example, enabling distributed tracing results in you seeing trace data in the UI and in having trace data available for querying. If you are missing data that you expect to see, see Missing data. Data retention Different types of New Relic data have different data retention periods. Once data has passed a given data retention point, it may be deleted or be aggregated for longer term storage. For details, see Data retention.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 133.33368,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Factors affecting access to features <em>and</em> data",
        "sections": "<em>Account</em> access",
        "tags": "<em>Accounts</em> <em>and</em> <em>billing</em>",
        "body": ", here are some causes and solutions: If your organization has multiple <em>accounts</em> and you have access to those <em>accounts</em>, you will see an <em>account</em> switcher at the top left of most New Relic UI pages that will let you switch between <em>accounts</em>. If you&#x27;re logged into New Relic but can&#x27;t find an <em>account</em>"
      },
      "id": "60bee5c064441f0505d543bb"
    }
  ],
  "/docs/accounts/accounts-billing/account-setup/troubleshoot-new-relics-password-email-address-login-problems": [
    {
      "sections": [
        "Downgrade or cancel account/organization",
        "Reduce data ingest",
        "Downgrade organization",
        "Pro or Enterprise edition downgrade",
        "Standard edition downgrade",
        "Cancel organization",
        "Pro or Enterprise edition cancel",
        "Standard edition: cancel simple organization",
        "Standard edition: cancel other organizations"
      ],
      "title": "Downgrade or cancel account/organization",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Account setup"
      ],
      "external_id": "3a75d74fd723f17050d69fe6da4fe162b6965bc8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/account-setup/downgradecancel-account/",
      "published_at": "2021-12-19T15:40:46Z",
      "updated_at": "2021-11-14T08:52:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Depending on your pricing edition and other factors, we offer various options for reducing data ingest, deleting or uninstalling an agent or integration, or downgrading or cancelling your organization (your account or group of accounts). Reduce data ingest If you want to stop reporting some data to New Relic without downgrading or cancelling, you can configure our tools to send less data. For how to manage data ingest, see Manage your data. To uninstall agents or integrations, here are some recommended procedures: Remove APM, browser, and mobile apps Remove infrastructure agent For how to disable other New Relic tools, see their specific docs. You can search New Relic quickstarts here. Downgrade organization Options for downgrading your New Relic organization will differ depending on your pricing edition: Pro or Enterprise edition downgrade To downgrade your Pro or Enterprise organization, contact your New Relic account representative. Standard edition downgrade Options for downgrading for Standard edition: If you're being billed and want to downgrade: From one.newrelic.com, click the account dropdown, click Manage your plan, and then click Downgrade account. Once you are downgraded, you’ll still be able to access New Relic via the login page. If you're not being billed and haven't input your credit card, you don’t need to downgrade and can continue using New Relic at a free level. Cancel organization If you've accidentally signed up for New Relic and want to delete that account, see Delete simple organization. Other options for cancelling your organization depend on your pricing edition: Pro or Enterprise edition cancel To cancel your Pro or Enterprise organization, contact your New Relic account representative. Standard edition: cancel simple organization If you've created a new New Relic organization that you don't need, and if it meets some requirements (below), you can delete that organization with these steps: click the account dropdown, select Organization and access, and then click Delete organization. Requirements for being able to self-delete an organization: Organization has a single account, and single user. Was created after July 2020 (and hence is on the new pricing model and the New Relic One user model). Standard edition: cancel other organizations First consider removing monitoring tools or downgrading. If you still have questions, contact support.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 211.69571,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Downgrade or cancel <em>account</em>&#x2F;organization",
        "sections": "Downgrade or cancel <em>account</em>&#x2F;organization",
        "tags": "<em>Accounts</em> <em>and</em> <em>billing</em>",
        "body": "Depending on your pricing edition and other factors, we offer various options for reducing data ingest, deleting or uninstalling an agent or integration, or downgrading or cancelling your organization (your <em>account</em> or group of <em>accounts</em>). Reduce data ingest If you want to stop reporting some data"
      },
      "id": "6043f3c4196a677821960f65"
    },
    {
      "sections": [
        "Account ID"
      ],
      "title": "Account ID",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Account setup"
      ],
      "external_id": "3771d53b48cfcf2f29d303b1d77a3884e4bae480",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/account-structure/account-id/",
      "published_at": "2021-12-19T14:37:48Z",
      "updated_at": "2021-09-13T20:41:52Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some New Relic procedures require use of your account ID (for example, some API calls). Your account ID is the ID number we assign to a New Relic account. Note that some New Relic organizations contain multiple accounts. Options for finding your account ID: If your organization has multiple accounts: From one.newrelic.com, use the account selector dropdown near the top of the page to switch between accounts and see their IDs. There are other options, depending on which user model you're on: New Relic One user model: Click the account dropdown and then click Administration. Click Organization and access and then Accounts to see account IDs. Original user model: Click the account dropdown, click Account settings, and then click API keys. The account ID is displayed there. For more on how account access works, see Factors affecting access.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.30705,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Account</em> ID",
        "sections": "<em>Account</em> ID",
        "tags": "<em>Accounts</em> <em>and</em> <em>billing</em>",
        "body": "Some New Relic procedures require use of your <em>account</em> ID (for example, some API calls). Your <em>account</em> ID is the ID number we assign to a New Relic <em>account</em>. Note that some New Relic organizations contain multiple <em>accounts</em>. Options for finding your <em>account</em> ID: If your organization has multiple"
      },
      "id": "61271c84196a67ed0c00b367"
    },
    {
      "sections": [
        "Factors affecting access to features and data",
        "User permissions",
        "Account access",
        "Pricing plan or edition",
        "The entities being monitored",
        "Data retention"
      ],
      "title": "Factors affecting access to features and data",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Account structure"
      ],
      "external_id": "98bf366604050b491ee35c8422414c70bda452b4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/account-structure/factors-affecting-access-features-data/",
      "published_at": "2021-12-19T14:14:41Z",
      "updated_at": "2021-11-24T11:11:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "There are several factors that can affect your access to a New Relic feature, or New Relic data. User permissions User-related settings can impact what features or data you have access to. Potential impacts include: Your user type (basic user or full platform user). Your assigned roles. How your user roles and permissions work depends on which user model you're on: Original user model roles New Relic One user model roles If you think your permissions are preventing you from accessing something, talk to your New Relic administrators. Account access If you're having trouble finding an account, here are some causes and solutions: If your organization has multiple accounts and you have access to those accounts, you will see an account switcher at the top left of most New Relic UI pages that will let you switch between accounts. If you're logged into New Relic but can't find an account or its data, it may be for one of these reasons: You may need to be added to that account. How you do this will depend on your user model: Original use model | New Relic One user model. You may have more than one New Relic login associated with the same email address. If you think this may be the case, log out and log back in. When you input your email address, the login UI notes if you have multiple user records and gives you an option for verifying your email to see all available accounts when you log in. Other details about multiple logins: To see all your available login options the next time you log in, select Remember me when logging in. If one of your login options reads \"Original account\", that means it's a user record on our original user model. For more information, see this Explorers Hub post about multiple accounts. Related docs: User permission factors Account structure Login and password troubleshooting Delete accounts Pricing plan or edition We have two pricing models available, and this can impact feature availability. Pricing plans: Our original product-based pricing model: This plan separates our offerings by product. If you’re on this plan, access to some features may depend on the products you pay for. Our New Relic One pricing model: This newer pricing model gives more cross-platform access. The main factors affecting access are your edition and your user type. We also have several pricing editions: Standard, Pro, and Enterprise. Learn more about our pricing models. The entities being monitored Some of our solutions enable functionality that isn’t available to all users, like: Additional UI components, and Data available for querying For example, enabling distributed tracing results in you seeing trace data in the UI and in having trace data available for querying. If you are missing data that you expect to see, see Missing data. Data retention Different types of New Relic data have different data retention periods. Once data has passed a given data retention point, it may be deleted or be aggregated for longer term storage. For details, see Data retention.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 133.33368,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Factors affecting access to features <em>and</em> data",
        "sections": "<em>Account</em> access",
        "tags": "<em>Accounts</em> <em>and</em> <em>billing</em>",
        "body": ", here are some causes and solutions: If your organization has multiple <em>accounts</em> and you have access to those <em>accounts</em>, you will see an <em>account</em> switcher at the top left of most New Relic UI pages that will let you switch between <em>accounts</em>. If you&#x27;re logged into New Relic but can&#x27;t find an <em>account</em>"
      },
      "id": "60bee5c064441f0505d543bb"
    }
  ],
  "/docs/accounts/accounts-billing/account-structure/account-id": [
    {
      "sections": [
        "Troubleshoot password, email address, and login issues",
        "Tip",
        "Account access issues",
        "Password solutions",
        "Forgot your password, or your password does not work.",
        "Important",
        "Reset password.",
        "Password error messages.",
        "Password reset link expired.",
        "Added existing user to another New Relic account.",
        "Email solutions",
        "Did not receive new account confirmation email.",
        "Email address does not work or email error messages.",
        "Received message that your email account already exists.",
        "Email link does not redirect to the password reset page.",
        "Asked to verify email during login.",
        "General system solutions",
        "Delete an account you recently created.",
        "SAML single sign-on (SSO) problems prevent login.",
        "System failure errors prevent login.",
        "Mobile device solutions",
        "Solutions for other situations",
        "Unable to log in from your New Relic partner account."
      ],
      "title": "Troubleshoot password, email address, and login issues",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Account setup"
      ],
      "external_id": "fc93486097659dacf04d90cfc6b435cc6791ce6d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/account-setup/troubleshoot-new-relics-password-email-address-login-problems/",
      "published_at": "2021-12-19T15:40:46Z",
      "updated_at": "2021-11-14T08:56:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have problems with your email address or password when logging in to New Relic, review these troubleshooting tips. (Don't have a New Relic account yet? Sign up here. It's free!) Tip To learn about factors affecting access to features and data, see Factors affecting access. Account access issues If you're logged into New Relic but don't see the accounts or data you expect, see Factors affecting access. Password solutions New Relic requires a valid email address and a valid password in order for you to log in and to receive information from New Relic. If you are unable to create your account successfully, or if you have password or other login problems, review these troubleshooting suggestions to try to identify a resolution. Forgot your password, or your password does not work. If you have forgotten your New Relic account password, or if your password does not work, request a password reset from New Relic. If you have access to multiple New Relic logins, also ensure you are using the correct password for the login method you're using Go to New Relic's login page. Select Forgot your password. Type your account email address, and select Send my reset link. When you receive a confirmation email from New Relic, select the password reset link, and follow New Relic's password requirements to complete the process to reset your password. Important The password reset link expires after 12 hours. If you do not quickly receive an email from New Relic, check your spam filters, contact your organization's email administrator for troubleshooting suggestions, get support at support.newrelic.com. Reset password. If you forgot your own password or need to request a password reset for your account email, you can use New Relic's self-service options. Admin-level users cannot reset passwords for other users. If you need to reset someone else's password, get support at support.newrelic.com. Password error messages. If you complete the signup process and are unable to log in to your account due to password error messages, get support at support.newrelic.com. Password reset link expired. If you want to change your password but you see a message that the password reset link expired, try using a private browser or clearing your browser cache and cookies. Important The password reset link expires after 12 hours. If you do not quickly receive an email from New Relic after you select the reset link, check your spam filters, contact your organization's email administrator for troubleshooting suggestions, get support at support.newrelic.com. Added existing user to another New Relic account. If you're an existing New Relic account user and you have been added to another account, you don't need to create a new password. Your existing password will work for each account, and you can switch between accounts after logging in. Email solutions New Relic requires a valid email address and a valid password in order for you to log in and to receive information from New Relic. If you're unable to create your account successfully, or if you have email or other login problems, review these troubleshooting suggestions to try to identify a resolution. Did not receive new account confirmation email. When you first create your account, New Relic sends a confirmation email so you can complete the setup process and sign in. If you cannot locate your original account confirmation email: Go to New Relic's login page at login.newrelic.com/login. Select Forgot your password. Type your account email address, and select Send my password. When New Relic's system returns an email message, select the link in it to confirm your account again. If you don't receive an email from New Relic: Check your spam filters. If applicable, add New Relic to your email allow list. Get support at support.newrelic.com. Email address does not work or email error messages. If you complete the signup process and are unable to log in to your account due to email or password error messages, get support at support.newrelic.com. Received message that your email account already exists. If you are trying to create a new account and receive a message that your email account already exists, get support at support.newrelic.com. Email link does not redirect to the password reset page. If you have used your password reset link but are not redirected to the password reset page, troubleshoot the following: Your password reset link has expired. You will need to request a new link from the New Relic website. You are experiencing a caching issue. Clear your browsing cache or use a private browsing window before trying the link again. Tip Private browsing, also known as incognito mode, is a privacy feature to disable browsing history and the web cache. To open a private browsing window, you can use the keyboard shortcut CTRL+Shift+N on Windows and Command+Shift+N on Mac for most browsing applications. If this still doesn't solve your problem, get support at support.newrelic.com. Asked to verify email during login. If your email is associated with multiple accounts, you will be given the option to verify your email during login. This will allow you to choose which account to access. If you do not verify your email address, New Relic will attempt to log you in with the most recently created user record associated with your email address. To avoid verifying your email during each login, click the Remember Me checkbox in the login screen. General system solutions Some general system problems and solutions: Delete an account you recently created. If you created a New Relic account unnecessarily and want to delete it, and if that account is a simple one, you may be able to delete it yourself. See Delete simple organization. SAML single sign-on (SSO) problems prevent login. If your organization uses a SAML Single Sign On (SSO) solution, you can skip the Password field when you log in. If you need to reset your password, contact your organization's system administrator or IT department as applicable. If you're an administrator who has recently enabled or made changes to your SAML SSO settings and are unable to log in, there might be an issue with your configuration. Customers on the New Relic One user model can use a special recovery flow to fix any issues. Visit login.newrelic.com/recovery_access to bypass SSO and gain one-time access to your organization. System failure errors prevent login. If you receive failure errors while trying to sign up, a third-party password manager may be triggering New Relic's spam trap. To work around this, try these solutions: Bypass your password manager. Use a different browser to sign up with New Relic. Get support at support.newrelic.com. Mobile device solutions If you are unable to log in from your mobile device, the new user authentication time frame may have expired. You must complete this process within 20 minutes of receiving New Relic's confirmation message for your mobile device. To solve this problem, request another confirmation message to be sent to your device. Also, depending on your New Relic account, additional installation or authentication steps may be required for your iOS or Android app account. Solutions for other situations Here are suggestions for other unique situations. Unable to log in from your New Relic partner account. With partner accounts, SAML SSO authentication to sign in to New Relic is controlled by the partnership. Depending on the partnership, you may or may not be able to log in directly to New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 211.6984,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Troubleshoot password, email address, <em>and</em> login issues",
        "sections": "<em>Account</em> access issues",
        "tags": "<em>Accounts</em> <em>and</em> <em>billing</em>",
        "body": " <em>account</em>, you don&#x27;t need to create a new password. Your existing password will work for each <em>account</em>, and you can switch between <em>accounts</em> after logging in. Email solutions New Relic requires a valid email address and a valid password in order for you to log in and to receive information from New Relic"
      },
      "id": "6043f38b28ccbc04a22fd1aa"
    },
    {
      "sections": [
        "Downgrade or cancel account/organization",
        "Reduce data ingest",
        "Downgrade organization",
        "Pro or Enterprise edition downgrade",
        "Standard edition downgrade",
        "Cancel organization",
        "Pro or Enterprise edition cancel",
        "Standard edition: cancel simple organization",
        "Standard edition: cancel other organizations"
      ],
      "title": "Downgrade or cancel account/organization",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Account setup"
      ],
      "external_id": "3a75d74fd723f17050d69fe6da4fe162b6965bc8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/account-setup/downgradecancel-account/",
      "published_at": "2021-12-19T15:40:46Z",
      "updated_at": "2021-11-14T08:52:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Depending on your pricing edition and other factors, we offer various options for reducing data ingest, deleting or uninstalling an agent or integration, or downgrading or cancelling your organization (your account or group of accounts). Reduce data ingest If you want to stop reporting some data to New Relic without downgrading or cancelling, you can configure our tools to send less data. For how to manage data ingest, see Manage your data. To uninstall agents or integrations, here are some recommended procedures: Remove APM, browser, and mobile apps Remove infrastructure agent For how to disable other New Relic tools, see their specific docs. You can search New Relic quickstarts here. Downgrade organization Options for downgrading your New Relic organization will differ depending on your pricing edition: Pro or Enterprise edition downgrade To downgrade your Pro or Enterprise organization, contact your New Relic account representative. Standard edition downgrade Options for downgrading for Standard edition: If you're being billed and want to downgrade: From one.newrelic.com, click the account dropdown, click Manage your plan, and then click Downgrade account. Once you are downgraded, you’ll still be able to access New Relic via the login page. If you're not being billed and haven't input your credit card, you don’t need to downgrade and can continue using New Relic at a free level. Cancel organization If you've accidentally signed up for New Relic and want to delete that account, see Delete simple organization. Other options for cancelling your organization depend on your pricing edition: Pro or Enterprise edition cancel To cancel your Pro or Enterprise organization, contact your New Relic account representative. Standard edition: cancel simple organization If you've created a new New Relic organization that you don't need, and if it meets some requirements (below), you can delete that organization with these steps: click the account dropdown, select Organization and access, and then click Delete organization. Requirements for being able to self-delete an organization: Organization has a single account, and single user. Was created after July 2020 (and hence is on the new pricing model and the New Relic One user model). Standard edition: cancel other organizations First consider removing monitoring tools or downgrading. If you still have questions, contact support.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 211.69571,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Downgrade or cancel <em>account</em>&#x2F;organization",
        "sections": "Downgrade or cancel <em>account</em>&#x2F;organization",
        "tags": "<em>Accounts</em> <em>and</em> <em>billing</em>",
        "body": "Depending on your pricing edition and other factors, we offer various options for reducing data ingest, deleting or uninstalling an agent or integration, or downgrading or cancelling your organization (your <em>account</em> or group of <em>accounts</em>). Reduce data ingest If you want to stop reporting some data"
      },
      "id": "6043f3c4196a677821960f65"
    },
    {
      "sections": [
        "Factors affecting access to features and data",
        "User permissions",
        "Account access",
        "Pricing plan or edition",
        "The entities being monitored",
        "Data retention"
      ],
      "title": "Factors affecting access to features and data",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Account structure"
      ],
      "external_id": "98bf366604050b491ee35c8422414c70bda452b4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/account-structure/factors-affecting-access-features-data/",
      "published_at": "2021-12-19T14:14:41Z",
      "updated_at": "2021-11-24T11:11:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "There are several factors that can affect your access to a New Relic feature, or New Relic data. User permissions User-related settings can impact what features or data you have access to. Potential impacts include: Your user type (basic user or full platform user). Your assigned roles. How your user roles and permissions work depends on which user model you're on: Original user model roles New Relic One user model roles If you think your permissions are preventing you from accessing something, talk to your New Relic administrators. Account access If you're having trouble finding an account, here are some causes and solutions: If your organization has multiple accounts and you have access to those accounts, you will see an account switcher at the top left of most New Relic UI pages that will let you switch between accounts. If you're logged into New Relic but can't find an account or its data, it may be for one of these reasons: You may need to be added to that account. How you do this will depend on your user model: Original use model | New Relic One user model. You may have more than one New Relic login associated with the same email address. If you think this may be the case, log out and log back in. When you input your email address, the login UI notes if you have multiple user records and gives you an option for verifying your email to see all available accounts when you log in. Other details about multiple logins: To see all your available login options the next time you log in, select Remember me when logging in. If one of your login options reads \"Original account\", that means it's a user record on our original user model. For more information, see this Explorers Hub post about multiple accounts. Related docs: User permission factors Account structure Login and password troubleshooting Delete accounts Pricing plan or edition We have two pricing models available, and this can impact feature availability. Pricing plans: Our original product-based pricing model: This plan separates our offerings by product. If you’re on this plan, access to some features may depend on the products you pay for. Our New Relic One pricing model: This newer pricing model gives more cross-platform access. The main factors affecting access are your edition and your user type. We also have several pricing editions: Standard, Pro, and Enterprise. Learn more about our pricing models. The entities being monitored Some of our solutions enable functionality that isn’t available to all users, like: Additional UI components, and Data available for querying For example, enabling distributed tracing results in you seeing trace data in the UI and in having trace data available for querying. If you are missing data that you expect to see, see Missing data. Data retention Different types of New Relic data have different data retention periods. Once data has passed a given data retention point, it may be deleted or be aggregated for longer term storage. For details, see Data retention.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 133.33366,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Factors affecting access to features <em>and</em> data",
        "sections": "<em>Account</em> access",
        "tags": "<em>Accounts</em> <em>and</em> <em>billing</em>",
        "body": ", here are some causes and solutions: If your organization has multiple <em>accounts</em> and you have access to those <em>accounts</em>, you will see an <em>account</em> switcher at the top left of most New Relic UI pages that will let you switch between <em>accounts</em>. If you&#x27;re logged into New Relic but can&#x27;t find an <em>account</em>"
      },
      "id": "60bee5c064441f0505d543bb"
    }
  ],
  "/docs/accounts/accounts-billing/account-structure/add-accounts": [
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/account-setup/create-your-new-relic-account/",
      "sections": [
        "Sign up for New Relic",
        "Next steps",
        "Add accounts to an existing New Relic organization",
        "Login problems?",
        "Delete an accidental sign-up"
      ],
      "published_at": "2021-12-19T15:58:22Z",
      "title": "Sign up for New Relic",
      "updated_at": "2021-11-24T14:20:48Z",
      "type": "docs",
      "external_id": "d3bcfb1dde6566ad655368040ede0d75d0089df8",
      "document_type": "page",
      "popularity": 1,
      "body": "At New Relic, our mission is to help you create more perfect software. So we give you free access to our observability platform, forever. For free, you can ingest up to 100 GBs per month, and have one full platform user and unlimited basic users. Ready to get started? Sign up for New Relic Want to learn more first? As just one example of what you can do with New Relic, imagine you are a Kubernetes administrator overseeing many clusters and pods of software containers. Where do you start troubleshooting? This short video shows how you can locate a problem cluster and use distributed tracing to find relevant logs: Learn more: See pricing details Learn what New Relic does Browse our solutions Next steps Here are some next steps after signing up: Start reporting data using our Add your data UI Get ideas on how to bring all your data together Watch New Relic University videos If you unintentionally signed up, see Delete account Add accounts to an existing New Relic organization If you already have an existing New Relic organization and want to add accounts to it, see Account structure. Login problems? If you're having login or password problems, see Login troubleshooting. Delete an accidental sign-up If you've unintentionally signed up and need to delete that organization, see Delete organization.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 283.80347,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Add</em> <em>accounts</em> to an existing New Relic organization",
        "body": " unintentionally signed up, see Delete <em>account</em> <em>Add</em> <em>accounts</em> to an existing New Relic organization If you already have an existing New Relic organization and want to <em>add</em> <em>accounts</em> to it, see <em>Account</em> structure. Login problems? If you&#x27;re having login or password problems, see Login troubleshooting. Delete an accidental sign-up If you&#x27;ve unintentionally signed up and need to delete that organization, see Delete organization."
      },
      "id": "6043f64a64441fa576378eca"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/account-structure/new-relic-account-structure/",
      "sections": [
        "Organization and account structure",
        "Important",
        "New Relic One user model",
        "How users access accounts",
        "Original user model"
      ],
      "published_at": "2021-12-19T15:58:31Z",
      "title": "Organization and account structure",
      "updated_at": "2021-11-24T11:32:06Z",
      "type": "docs",
      "external_id": "4f5a4cde293d0b599f489eff010f69c021ccb539",
      "document_type": "page",
      "popularity": 1,
      "body": "Depending on your user model, you have different options for adding and managing accounts and assigning users to them. We have two models: New Relic One user model Original user model Important Note that the user model is not directly related to our two pricing models. New Relic One user model Important This section is about organizations on the New Relic One user model, not the original model. Learn more about the difference. At New Relic, an \"organization\" represents a New Relic customer. The organization contains everything relevant to a New Relic customer: its accounts, its users, and its data. A New Relic \"account\" can be considered a workspace. For example, you might have an account for a specific app, or a set of related hosts and services for a specific initiative or project, or you might have an account for a specific team. Each account has its own account ID, and that ID is used for some account-specific tasks, like making API calls. Our Standard edition allows for a single account per organization. Pro and Enterprise editions allow for multiple accounts per organization. Currently you can't add accounts to your organization on your own. To add accounts, talk to your New Relic account representative. How users access accounts In your organization, your New Relic users are granted access to specific accounts that are relevant to their duties and responsibilities. To manage users’ access to accounts, you create access grants, which assign a group of users to a specific role on a specific account. For example, you may assign a user group the ability to manage billing on some accounts with the Billing manager role, and assign some users as non-admin full platform users on some accounts, and assign some users as basic users on some accounts. Our user management system allows you to create the user access you need, whether that’s a relatively simple setup with just a few roles across a few accounts, or a complex one with many roles across many accounts. Learn more about user management. Note that some features, like dashboards and workloads, can display data from across different accounts in an organization. This means that if a user isn’t granted access to all relevant accounts, they may experience missing data. To learn more about access issues, see Factors affecting access. Original user model See Original user model structure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Organization and <em>account</em> structure",
        "sections": "How users access <em>accounts</em>",
        "body": " is used for some <em>account</em>-specific tasks, like making API calls. Our Standard edition allows for a single <em>account</em> per organization. Pro and Enterprise editions allow for multiple <em>accounts</em> per organization. Currently you can&#x27;t <em>add</em> <em>accounts</em> to your organization on your own. To <em>add</em> <em>accounts</em>, talk to your New"
      },
      "id": "60bee5c028ccbc2413e667e4"
    },
    {
      "sections": [
        "Factors affecting access to features and data",
        "User permissions",
        "Account access",
        "Pricing plan or edition",
        "The entities being monitored",
        "Data retention"
      ],
      "title": "Factors affecting access to features and data",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Account structure"
      ],
      "external_id": "98bf366604050b491ee35c8422414c70bda452b4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/account-structure/factors-affecting-access-features-data/",
      "published_at": "2021-12-19T14:14:41Z",
      "updated_at": "2021-11-24T11:11:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "There are several factors that can affect your access to a New Relic feature, or New Relic data. User permissions User-related settings can impact what features or data you have access to. Potential impacts include: Your user type (basic user or full platform user). Your assigned roles. How your user roles and permissions work depends on which user model you're on: Original user model roles New Relic One user model roles If you think your permissions are preventing you from accessing something, talk to your New Relic administrators. Account access If you're having trouble finding an account, here are some causes and solutions: If your organization has multiple accounts and you have access to those accounts, you will see an account switcher at the top left of most New Relic UI pages that will let you switch between accounts. If you're logged into New Relic but can't find an account or its data, it may be for one of these reasons: You may need to be added to that account. How you do this will depend on your user model: Original use model | New Relic One user model. You may have more than one New Relic login associated with the same email address. If you think this may be the case, log out and log back in. When you input your email address, the login UI notes if you have multiple user records and gives you an option for verifying your email to see all available accounts when you log in. Other details about multiple logins: To see all your available login options the next time you log in, select Remember me when logging in. If one of your login options reads \"Original account\", that means it's a user record on our original user model. For more information, see this Explorers Hub post about multiple accounts. Related docs: User permission factors Account structure Login and password troubleshooting Delete accounts Pricing plan or edition We have two pricing models available, and this can impact feature availability. Pricing plans: Our original product-based pricing model: This plan separates our offerings by product. If you’re on this plan, access to some features may depend on the products you pay for. Our New Relic One pricing model: This newer pricing model gives more cross-platform access. The main factors affecting access are your edition and your user type. We also have several pricing editions: Standard, Pro, and Enterprise. Learn more about our pricing models. The entities being monitored Some of our solutions enable functionality that isn’t available to all users, like: Additional UI components, and Data available for querying For example, enabling distributed tracing results in you seeing trace data in the UI and in having trace data available for querying. If you are missing data that you expect to see, see Missing data. Data retention Different types of New Relic data have different data retention periods. Once data has passed a given data retention point, it may be deleted or be aggregated for longer term storage. For details, see Data retention.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Account</em> access",
        "tags": "<em>Accounts</em>",
        "body": ", here are some causes and solutions: If your organization has multiple <em>accounts</em> and you have access to those <em>accounts</em>, you will see an <em>account</em> switcher at the top left of most New Relic UI pages that will let you switch between <em>accounts</em>. If you&#x27;re logged into New Relic but can&#x27;t find an <em>account</em>"
      },
      "id": "60bee5c064441f0505d543bb"
    }
  ],
  "/docs/accounts/accounts-billing/account-structure/factors-affecting-access-features-data": [
    {
      "sections": [
        "Overview of changes to pricing and user model",
        "Overview of how pricing model and user model relate",
        "Pricing plans explained",
        "Determine pricing model",
        "Convert to new pricing",
        "User models explained",
        "Requirements for new user model",
        "Determine which user model you're on",
        "Feature impacts of user model",
        "Transition to new models"
      ],
      "title": "Overview of changes to pricing and user model",
      "type": "docs",
      "tags": [
        "Accounts",
        "Original accounts and billing",
        "Original product-based pricing"
      ],
      "external_id": "b19d61b5a0c65ca352e2ce0e260e2b53391b94fc",
      "image": "https://docs.newrelic.com/static/1461470cc1ef0be91120c84d76a7ccdd/c1b63/new-relic-one-pricing-plan-billing-ui.png",
      "url": "https://docs.newrelic.com/docs/accounts/original-accounts-billing/original-product-based-pricing/overview-changes-pricing-user-model/",
      "published_at": "2021-12-19T14:16:05Z",
      "updated_at": "2021-11-24T14:34:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In 2020, New Relic released both a newer pricing model and a newer user model. Keep reading to learn about: How the pricing model and the user model relate to each other Pricing plans explained User models explained How to switch to the new models Overview of how pricing model and user model relate In 2020, we released both a new, improved pricing model and a new, improved user model. These models represent the future. All new sign-ups are on these new models and eventually all organizations will be on these models. But currently, our customers from before July 2020 may have one of several combinations of these as they either switch to the new pricing or migrate their users to the new user model. This table shows how pricing and user model relate to each other: Pricing plan factors User model factors Original pricing If your organization was created before July 30 2020, you remain on our original pricing model until you transition to New Relic One pricing. For original pricing accounts, you have users on the original user model unless you've migrated your users to the new model. Note that if you've added users via automated user management, they're on the new model. This means it's possible to have some users on both models. New Relic One pricing An organization is on the New Relic One pricing model: If it was created on or after July 30 2020, or If it's an older organization but has switched to New Relic One pricing. If your organization was created on or after July 30 2020, your users are on the New Relic One user model. If you have an older organization, you have users on the original user model unless you've migrated your users to the new model. Pricing plans explained New Relic organizations are on one of two pricing models: New Relic One pricing: Our new pricing model is simpler. It bills on a) which edition you're on (Standard, Pro, or Enterprise), b) the GBs of data ingested, and c) how many billable full platform users you have. All organizations created on or after July 30 2020 are on this pricing model, as are older organizations that have switched to this pricing. There are two versions of this pricing model. Our original product-based pricing model: this is based on subscriptions to specific products (for example, APM, Browser, Mobile, Infrastructure). Some organizations have the option to transition to the newer pricing model: in that case, their users remain on our original user model. Determine pricing model To determine which pricing model you’re on: go to one.newrelic.com, select the account dropdown, and select Manage your plan. If you see billing information about data ingested and the number of billable users, you’re on the new pricing model. If you're on New Relic One pricing, you'll see a billing UI like this. Convert to new pricing Some organizations are able to switch to new pricing. Learn more about switching your pricing model. User models explained In this context, the term \"user model\" refers to the structure of a New Relic user record and how it interacts with the broader New Relic organization that it's in. Our two user models are: New Relic One user model: this newer, improved user model has these major differences from the original user model: All your accounts and users are contained under a top-level organization. This gives an organization stronger control over managing users and what they can access. One impact of this is that users who work with multiple New Relic organizations may have an email address associated with multiple logins. Users on this model have a different system for managing permissions and account access. Using access grants, you assign users access to specific roles on specific accounts. Original user model: some aspects of this older model that are different from our newer model: There wasn't as much organization-level control over users. For example, a New Relic user had the ability to access multiple accounts they'd been granted access to using a single login. Users on the original user model have a different user management system. To learn more about benefits of the new model, see our blog post about user model changes and an explanation of the new account structure. For impacts and limitations, see Feature impacts. Requirements for new user model Here's a synopsis of the factors governing what organizations use the new model: All New Relic organizations that signed up after July 30 2020 have users on this model, and also have the new pricing model. Some older New Relic organizations have had their users migrated to the new model by New Relic or by using the user migration procedure. Note that switching to the new pricing model is independent from migrating users. Partner accounts (resellers, managed service providers), and organizations using the partnership account structure, cannot yet migrate their users to the new model. Determine which user model you're on To determine what user model you're on, from the account dropdown select User preferences. If you see the UI below, you're on the New Relic One user model. If you see a different UI with more content displayed, you're on the original user model (see original user docs). To determine if you can manage users on the New Relic One user model, see Manage users. The user model is independent of your pricing model. For how user model relates to pricing, see the Pricing and user model table. Feature impacts of user model The new user model offers many benefits. Here are some feature impacts of being on the New Relic One user model that may be not obvious to users accustomed to our previous user model: More separation of organizations/accounts: If you have an email address associated with multiple New Relic accounts, this may mean you need to log out and log back in. Learn more about account access. Adding accounts: Pro and Enterprise edition can have more than one account in their organization. Currently users in these organizations cannot, on their own, add accounts. They must get help from their New Relic account representative. To learn about adding users to accounts, see Manage users. Alerts-related impacts: Policy actions: For alert policies and conditions, some UI locations display the user that made the last modification. For users on the new model, it will display their user ID and not their user name. For example, a policy might read Last modified Sep 7, '20 4:26 pm by user 1234567 instead of by John Doe. If you have user management permissions, you can determine what user this is by going to the User management UI. Notifications: Users on the new model won’t show up as an option when choosing users to receive alert notifications. The result of this is that these users will not get New Relic mobile app push notifications. As a workaround, you can instead use their email address as the notification channel. You can't use the REST API to generate a list of users (only original model users). Instead you'd use the user management UI. Changing account name: These users cannot change the name of their organization after it has been created. Transition to new models Some New Relic organizations on the old models are able to transition to the new models. To learn more about this, see Transition to new models.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 141.12035,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Overview of changes to pricing <em>and</em> user model",
        "sections": "Overview of changes to pricing <em>and</em> user model",
        "tags": "Original <em>accounts</em> <em>and</em> <em>billing</em>",
        "body": ", their users remain on our original user model. Determine pricing model To determine which pricing model you’re on: go to one.newrelic.com, select the <em>account</em> dropdown, and select Manage your plan. If you see <em>billing</em> information about data ingested and the number of billable users, you’re on the new"
      },
      "id": "603e97fa28ccbcb7c2eba754"
    },
    {
      "sections": [
        "Query and alert on billing/usage data",
        "Available data types",
        "Query examples",
        "Data usage queries",
        "Daily data usage",
        "Daily usage by source",
        "Metrics ingest by source",
        "Month-to-date data usage",
        "Month-to-date estimated data cost",
        "User count queries",
        "Month-to-date full platform users",
        "Projected monthly full platform user count",
        "Count full platform users and basic users",
        "Set usage alerts",
        "Caution",
        "Ingested gigabytes exceed a fixed value",
        "Usage exceeds fixed threshold for GBs",
        "Usage exceeds fixed threshold for users",
        "Usage exceeds fixed threshold for estimated cost",
        "Available attributes"
      ],
      "title": "Query and alert on billing/usage data",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "New Relic One pricing and billing"
      ],
      "external_id": "e22ae9e26686d11726a82ad4036ff58520b4a439",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/new-relic-one-pricing-billing/usage-queries-alerts/",
      "published_at": "2021-12-19T15:22:20Z",
      "updated_at": "2021-12-04T21:08:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "For accounts on our New Relic One pricing model, we provide a View your usage UI for understanding billing-related usage and a Manage your data UI for managing billing-related data. Additionally, you can: Query your usage data to get more detail than is available in the UI. Set up NRQL alert conditions to get notifications about changes in your usage. Note that account hierarchy may affect queried data. See Account structure. Available data types Usage data is attached to these events: NrConsumption records usage every hour, and is the equivalent of \"real-time\" usage. Use this event to observe usage trends over time. NrMTDConsumption generates aggregate values from the NrConsumption event. Use this event to see usage or estimated cost for a billing period. NrUsage records usage every hour and is used to see usage reported per product. To see changes made to your account (for example, user management changes), you can query NrAuditEvent. Query examples The View your usage UI displays your data usage and billable user count. If you need more detail than that UI provides, you can use these NRQL queries. For definitions of some of the attributes used in these queries, see Attributes. Data usage queries Here are some data usage query examples: Daily data usage This query totals your billable ingested data, and displays a daily value for the past three months: FROM NrConsumption SELECT sum(GigabytesIngested) WHERE productLine = 'DataPlatform' SINCE 3 months ago TIMESERIES 1 day Copy Daily usage by source This query totals your billable ingested data, and displays a daily value for the past three months faceted by the source: FROM NrConsumption SELECT sum(GigabytesIngested) WHERE productLine = 'DataPlatform' SINCE 3 months ago FACET usageMetric TIMESERIES 1 day Copy Metrics ingest by source This query breaks down Metric data by the top ten metric names. You could also facet by appName or host to adjust the analysis. FROM Metric SELECT bytecountestimate()/10e8 as 'GB Estimate' SINCE '2021-04-01' UNTIL '2021-04-08' FACET metricName LIMIT 10 TIMESERIES 1 day Copy Month-to-date data usage This query shows the current full platform user count. In other words, it shows how much you'd be billed for your data for that month if you were billed right now. FROM NrMTDConsumption SELECT latest(GigabytesIngested) WHERE productLine = 'DataPlatform' SINCE this month Copy Month-to-date estimated data cost This query shows the estimated cost of your ingested data: FROM NrMTDConsumption SELECT latest(estimatedCost) WHERE productLine = 'DataPlatform' SINCE this month Copy User count queries Here are some user-related query examples. For details on how users are counted, see User count calculations. Month-to-date full platform users This query shows the billable full platform users for the month. In other words, it shows how much you'd be billed for your users for that month if you were billed right now. FROM NrMTDConsumption SELECT latest(usersBillable) SINCE this month Copy This query shows how many full platform users were counted by hour. This is useful for seeing how the full platform user count changed over time. from NrConsumption SELECT max(FullUsers) SINCE 10 days ago TIMESERIES 1 hour Copy Projected monthly full platform user count This query shows a projected count of monthly users. This query would not be good for using in a dashboard; it requires values based on a) the days remaining in the month, b) the start of the month. Here's an example querying the projected end-of-month count with 10 days left in that month: FROM NrMTDConsumption SELECT predictLinear(FullUsers, 10 days) SINCE '2020-09-01' Copy Count full platform users and basic users The usage UI shows the count of full platform users and basic users. The query used is: FROM NrUsage SELECT max(usage) SINCE 10 days ago WHERE productLine='FullStackObservability' WHERE metric in ('FullUsers', 'BasicUsers') FACET metric Copy To see the count of full and basic users over time: FROM NrUsage SELECT max(usage) SINCE 10 days ago WHERE productLine='FullStackObservability' WHERE metric in ('FullUsers', 'BasicUsers') FACET metric TIMESERIES 1 hour Copy Set usage alerts To help manage your billable data, you can set alerts to notify you of unexpected increases in usage. Learn how to create alerts with NRQL queries here. Caution When creating alert conditions, you should use the Event Timer method, which works very well with infrequent data. Here are some NRQL alert condition examples. For attribute definitions, see Attributes. Ingested gigabytes exceed a fixed value This query will create an alert when your hourly usage exceeds a fixed value: FROM NrConsumption SELECT sum(GigabytesIngested) WHERE productLine = 'DataPlatform' Copy If you have multiple child accounts, you may want to set threshold alerts for a specific subaccount: FROM NrConsumption SELECT sum(GigabytesIngested) WHERE productLine = 'DataPlatform' AND consumingAccountId = YOUR_CHILD-ACCOUNT_ID Copy Usage exceeds fixed threshold for GBs This query will create an alert when your usage exceeds fixed monthly threshold for GBs: FROM NrMTDConsumption SELECT latest(GigabytesIngested) WHERE productLine = 'DataPlatform' Copy Usage exceeds fixed threshold for users This query will create an alert when your usage exceeds fixed monthly threshold for billable users: FROM NrMTDConsumption SELECT latest(usersBillable) Copy Usage exceeds fixed threshold for estimated cost This query will create an alert when your usage exceeds fixed threshold for estimated cost: FROM NrMTDConsumption SELECT latest(estimatedCost) WHERE productLine = 'DataPlatform' Copy Available attributes Below are some of the important attributes attached to usage events. Attribute Description productLine The category of usage. There are three options: DataPlatform, FullStackObservability, and ProactiveDetection. (Starting November 1, 2021, IncidentIntelligence is no longer a billing factor). For more details about these categories, see New Relic platform. metric Consolidates multiple categories of usage into a single metric. Helpful when faceting by productLine. consumingAccountId ID of the New Relic account that is directly responsible for the stored events, as determined from the license key used. estimatedCost Calculates a cost estimate based on usage and metric cost. This is an estimate of costs to date, not your monthly invoice. For more attributes, see the data dictionary.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 136.86691,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Query <em>and</em> alert on <em>billing</em>&#x2F;usage data",
        "sections": "Query <em>and</em> alert on <em>billing</em>&#x2F;usage data",
        "tags": "<em>Accounts</em> <em>and</em> <em>billing</em>",
        "body": "For <em>accounts</em> on our New Relic One pricing model, we provide a View your usage UI for understanding <em>billing</em>-related usage and a Manage your data UI for managing <em>billing</em>-related data. Additionally, you can: Query your usage data to get more detail than is available in the UI. Set up NRQL alert"
      },
      "id": "6175f12b64441f53a35fc21c"
    },
    {
      "sections": [
        "How to manage users",
        "Important",
        "Requirements",
        "Manage users in the UI",
        "Overview of user management concepts",
        "Common user management tasks",
        "Add, edit, and delete users",
        "Edit user type (basic and full platform)",
        "Give users access to accounts and roles (access grants)",
        "Create new custom groups and roles",
        "Set up SAML SSO and/or SCIM provisioning",
        "Grant users ability to manage other users",
        "Control how basic users upgrade to full platform users",
        "Track changes",
        "User management terms and definitions"
      ],
      "title": "How to manage users",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "New Relic One user management"
      ],
      "external_id": "9e37836740ce56302734d4af636bdbe087b4dbc3",
      "image": "https://docs.newrelic.com/static/a78dad5ff794da5deaaf3b514e334ea7/c1b63/new-relic-one-user-mgmt.png",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/new-relic-one-user-management/add-manage-users-groups-roles/",
      "published_at": "2021-12-19T15:22:19Z",
      "updated_at": "2021-11-24T14:23:16Z",
      "document_type": "page",
      "popularity": 1,
      "body": "For users on our New Relic One user model, we provide various user management features, including the ability to: Use role based access control (RBAC) to assign default or custom roles to user groups Create custom user groups Grant user groups access to specific roles and accounts Important This doc applies to users on the New Relic One user model. For managing users on our original user model, see Original users. Not sure which user model you're on? See User models. Requirements To see if you can access these user management features, go to the user management UI and see what you have access to. Access requirements: These features allow managing of users on the New Relic One user model. To learn more, see User models. To avoid configuration conflicts, ensure that only one user is managing users at a time. Simultaneous configuration by more than one user may result in errors and unexpected results. Most capabilities require the Authentication domain manager role and some require the Organization manager role. For details, see Standard roles. Pricing edition requirements: To manage user groups, roles, and access grants: Pro or Enterprise edition is required. To import users from an identity provider: Enterprise is required. A New Relic user can have a maximum of either three concurrent active sessions, or three unique IP addresses in use at any given time. Manage users in the UI For users on the New Relic One user model, to find your user management options: From the account dropdown, select Administration. There are two user management UI pages there: User management: Use this to add users, update user type (basic versus full platform) and other information, and approve user type upgrade requests. Organization and access: Use this page to create and manage groups, roles, and access grants, and to set up SAML SSO and SCIM provisioning. Want to see videos of the user management UI in action? See our user management videos. Important If you can't see these UI pages, it may be because you're on our original user model or because you don't have the required user management role. Overview of user management concepts If your organization has Pro or Enterprise edition, you can create and configure access grants. An access grant gives a group of users access to a specific role on a specific account. Here's a diagram showing how access grants work and how they relate to the broader organization: A diagram explaining the concept of how access grants give a user group access to a) a role on b) a specific account. (Note that this applies to users on our New Relic One user model (and not our original user model).) When your New Relic organization is created, it starts out with some default access grants for our default User or Admin groups. Each of these groups is assigned one or more roles and granted access to the primary (first created) account: A view of the Organization and access UI, showing the default access grants associated with our default groups. If you have a relatively flat organizational structure, and are okay with all or many of your users having wide administrative access and access to all accounts, you'll probably only need at most a few access grants. For example, you might decide to add new access grants to the existing default Admin or User groups, giving those roles access to other accounts. Or, if you need more granular definition over roles and permissions, you'd create access grants that define new groups that have access to specific roles (either our standard roles or custom-defined roles). For a tutorial on how to create access grants and create custom roles, see the User management tutorial. For other examples of some common user management tasks, see Example tasks. To see the UI in action, see our user management videos. Some tips on setting up access grants: It may help to first plan out how your access grants will be organized. How many accounts will you have? What user groups will get access to which roles and which accounts? Will you use our default groups and roles or create your own custom groups and roles? If you've used automated user management to provision users via SCIM, you will have to create access grants to give those users access. A common configuration for organizations with many accounts (roughly 20 or more) is setting up groups with the more organization-scoped roles (Organization manager, Authentication domain manager, and Billing user) on the primary account, and then on other accounts, setting up groups with the more product-scoped roles (like All product admin, Standard user, or custom roles). Common user management tasks In the Organization and access UI, you can create access grants, custom groups, custom roles, and configure an authentication domain. Here are some example user management procedures: Add, edit, and delete users See the user management tutorial. Edit user type (basic and full platform) Note that there are limits around how many times full platform users can be changed to basic users. Important If you're using automated user management to provision and manage your users, you have other options for managing user type. To change the user type of one or more users: On the User management page, click the checkboxes for the users whose user type you want to edit. Once you start selecting users, an option will appear for Edit type. You can also edit the user type and group of a specific user by clicking on that user. Give users access to accounts and roles (access grants) See our user management tutorial. Create new custom groups and roles See the user management tutorial. Set up SAML SSO and/or SCIM provisioning See Get started with SAML SSO or SCIM. Grant users ability to manage other users To grant your users the ability to manage other users, you'll need to add users to a group that has the Authentication domain manager and Organization manager role. You have two options: From the User management UI, you can add a user to the default Admin group, which includes both of those roles. OR You can create a custom group and assign it these roles. For a tutorial on creating new access grants, groups, and roles, see the user management tutorial. Control how basic users upgrade to full platform users See the authentication domain settings. Track changes To see an audit log of changes to your account, including user management actions, you can query the NrAuditEvent. User management terms and definitions For an explanation of how user access to accounts and roles works, see User management concepts explained. Here are some definitions for the terms we use there: A New Relic organization is the representation of your organization, containing all your accounts, users, and data. For more information, see Organization and account structure. A capability is an ability to use or edit a specific, granular New Relic feature. Examples of capabilities: The ability to modify APM settings The ability to delete alert conditions A role is a set of capabilities. It is what gives a user their permissions. Our default standard roles have various capability sets, and you can create custom roles that have a custom set of capabilities. See some specific New Relic capabilities. A user group has one or more roles associated with it. You assign your users to a group. We have default user groups (Admin and User), and you can make your own groups. An access grant is what grants a user group access to roles and to specific New Relic accounts. An access grant essentially states, \"This group is assigned this role on this New Relic account.\" Adding a user to a group doesn’t do anything unless that group is included in an access grant. An authentication domain contains a set of users who are added to New Relic and who log in to New Relic in the same way. For example, you may have one authentication domain for users who log in via username/password and another authentication domain for users who log in via SAML. If a user is a basic user, this takes precedence over any role-related limitations. For more on this, see Basic user and roles.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 126.27004,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Give users access to <em>accounts</em> <em>and</em> roles (access grants)",
        "tags": "<em>Accounts</em> <em>and</em> <em>billing</em>",
        "body": " access grants to give those users access. A common configuration for organizations with many <em>accounts</em> (roughly 20 or more) is setting up groups with the more organization-scoped roles (Organization manager, Authentication domain manager, and <em>Billing</em> user) on the primary <em>account</em>, and then on other"
      },
      "id": "603e7bce28ccbc415beba74c"
    }
  ],
  "/docs/accounts/accounts-billing/account-structure/new-relic-account-structure": [
    {
      "sections": [
        "How to manage users",
        "Important",
        "Requirements",
        "Manage users in the UI",
        "Overview of user management concepts",
        "Common user management tasks",
        "Add, edit, and delete users",
        "Edit user type (basic and full platform)",
        "Give users access to accounts and roles (access grants)",
        "Create new custom groups and roles",
        "Set up SAML SSO and/or SCIM provisioning",
        "Grant users ability to manage other users",
        "Control how basic users upgrade to full platform users",
        "Track changes",
        "User management terms and definitions"
      ],
      "title": "How to manage users",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "New Relic One user management"
      ],
      "external_id": "9e37836740ce56302734d4af636bdbe087b4dbc3",
      "image": "https://docs.newrelic.com/static/a78dad5ff794da5deaaf3b514e334ea7/c1b63/new-relic-one-user-mgmt.png",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/new-relic-one-user-management/add-manage-users-groups-roles/",
      "published_at": "2021-12-19T15:22:19Z",
      "updated_at": "2021-11-24T14:23:16Z",
      "document_type": "page",
      "popularity": 1,
      "body": "For users on our New Relic One user model, we provide various user management features, including the ability to: Use role based access control (RBAC) to assign default or custom roles to user groups Create custom user groups Grant user groups access to specific roles and accounts Important This doc applies to users on the New Relic One user model. For managing users on our original user model, see Original users. Not sure which user model you're on? See User models. Requirements To see if you can access these user management features, go to the user management UI and see what you have access to. Access requirements: These features allow managing of users on the New Relic One user model. To learn more, see User models. To avoid configuration conflicts, ensure that only one user is managing users at a time. Simultaneous configuration by more than one user may result in errors and unexpected results. Most capabilities require the Authentication domain manager role and some require the Organization manager role. For details, see Standard roles. Pricing edition requirements: To manage user groups, roles, and access grants: Pro or Enterprise edition is required. To import users from an identity provider: Enterprise is required. A New Relic user can have a maximum of either three concurrent active sessions, or three unique IP addresses in use at any given time. Manage users in the UI For users on the New Relic One user model, to find your user management options: From the account dropdown, select Administration. There are two user management UI pages there: User management: Use this to add users, update user type (basic versus full platform) and other information, and approve user type upgrade requests. Organization and access: Use this page to create and manage groups, roles, and access grants, and to set up SAML SSO and SCIM provisioning. Want to see videos of the user management UI in action? See our user management videos. Important If you can't see these UI pages, it may be because you're on our original user model or because you don't have the required user management role. Overview of user management concepts If your organization has Pro or Enterprise edition, you can create and configure access grants. An access grant gives a group of users access to a specific role on a specific account. Here's a diagram showing how access grants work and how they relate to the broader organization: A diagram explaining the concept of how access grants give a user group access to a) a role on b) a specific account. (Note that this applies to users on our New Relic One user model (and not our original user model).) When your New Relic organization is created, it starts out with some default access grants for our default User or Admin groups. Each of these groups is assigned one or more roles and granted access to the primary (first created) account: A view of the Organization and access UI, showing the default access grants associated with our default groups. If you have a relatively flat organizational structure, and are okay with all or many of your users having wide administrative access and access to all accounts, you'll probably only need at most a few access grants. For example, you might decide to add new access grants to the existing default Admin or User groups, giving those roles access to other accounts. Or, if you need more granular definition over roles and permissions, you'd create access grants that define new groups that have access to specific roles (either our standard roles or custom-defined roles). For a tutorial on how to create access grants and create custom roles, see the User management tutorial. For other examples of some common user management tasks, see Example tasks. To see the UI in action, see our user management videos. Some tips on setting up access grants: It may help to first plan out how your access grants will be organized. How many accounts will you have? What user groups will get access to which roles and which accounts? Will you use our default groups and roles or create your own custom groups and roles? If you've used automated user management to provision users via SCIM, you will have to create access grants to give those users access. A common configuration for organizations with many accounts (roughly 20 or more) is setting up groups with the more organization-scoped roles (Organization manager, Authentication domain manager, and Billing user) on the primary account, and then on other accounts, setting up groups with the more product-scoped roles (like All product admin, Standard user, or custom roles). Common user management tasks In the Organization and access UI, you can create access grants, custom groups, custom roles, and configure an authentication domain. Here are some example user management procedures: Add, edit, and delete users See the user management tutorial. Edit user type (basic and full platform) Note that there are limits around how many times full platform users can be changed to basic users. Important If you're using automated user management to provision and manage your users, you have other options for managing user type. To change the user type of one or more users: On the User management page, click the checkboxes for the users whose user type you want to edit. Once you start selecting users, an option will appear for Edit type. You can also edit the user type and group of a specific user by clicking on that user. Give users access to accounts and roles (access grants) See our user management tutorial. Create new custom groups and roles See the user management tutorial. Set up SAML SSO and/or SCIM provisioning See Get started with SAML SSO or SCIM. Grant users ability to manage other users To grant your users the ability to manage other users, you'll need to add users to a group that has the Authentication domain manager and Organization manager role. You have two options: From the User management UI, you can add a user to the default Admin group, which includes both of those roles. OR You can create a custom group and assign it these roles. For a tutorial on creating new access grants, groups, and roles, see the user management tutorial. Control how basic users upgrade to full platform users See the authentication domain settings. Track changes To see an audit log of changes to your account, including user management actions, you can query the NrAuditEvent. User management terms and definitions For an explanation of how user access to accounts and roles works, see User management concepts explained. Here are some definitions for the terms we use there: A New Relic organization is the representation of your organization, containing all your accounts, users, and data. For more information, see Organization and account structure. A capability is an ability to use or edit a specific, granular New Relic feature. Examples of capabilities: The ability to modify APM settings The ability to delete alert conditions A role is a set of capabilities. It is what gives a user their permissions. Our default standard roles have various capability sets, and you can create custom roles that have a custom set of capabilities. See some specific New Relic capabilities. A user group has one or more roles associated with it. You assign your users to a group. We have default user groups (Admin and User), and you can make your own groups. An access grant is what grants a user group access to roles and to specific New Relic accounts. An access grant essentially states, \"This group is assigned this role on this New Relic account.\" Adding a user to a group doesn’t do anything unless that group is included in an access grant. An authentication domain contains a set of users who are added to New Relic and who log in to New Relic in the same way. For example, you may have one authentication domain for users who log in via username/password and another authentication domain for users who log in via SAML. If a user is a basic user, this takes precedence over any role-related limitations. For more on this, see Basic user and roles.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 676.1882,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Give users access to <em>accounts</em> <em>and</em> roles (access grants)",
        "tags": "<em>Accounts</em> <em>and</em> billing",
        "body": " definitions for the terms we use there: A New Relic <em>organization</em> is the representation of your <em>organization</em>, containing all your accounts, users, and data. For more information, see <em>Organization</em> and <em>account</em> <em>structure</em>. A capability is an ability to use or edit a specific, granular New Relic feature. Examples"
      },
      "id": "603e7bce28ccbc415beba74c"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/original-accounts-billing/original-users-roles/original-account-structure/",
      "sections": [
        "Organization and account structure (original user model)",
        "Important"
      ],
      "published_at": "2021-12-19T14:33:11Z",
      "title": "Organization and account structure (original user model)",
      "updated_at": "2021-11-14T09:20:42Z",
      "type": "docs",
      "external_id": "5fba4aaa95de18df4cd4ce1dabcb7e81477718c2",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This doc is about organizations on our original user model, not the New Relic One model. Learn more about the difference. Since July 30, 2020, new signups to New Relic are on our New Relic One user model. Older customers who have not yet migrated to the new model are still on our original user model. The original user model will be increasingly deprecated as more New Relic customers are migrated to the new model. Important Note that the user model is not directly related to our two different pricing models. Here are some important aspects of the original user model and links to important docs: Our original user model did not have firm boundaries between organizations/customers, and was much more user-centric. This meant that a user could be granted access to data from different organizations, and could access that data from a single login. Read more about the differences between the old and new model. On the original user model, a user’s access to accounts is based on: a) being added to a specific account, or b) having access to a parent account and inheriting access to that account’s child accounts. See our original user management docs. See an explanation of how parent/child accounts work.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 534.1498,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Organization</em> <em>and</em> <em>account</em> <em>structure</em> (original user model)",
        "sections": "<em>Organization</em> <em>and</em> <em>account</em> <em>structure</em> (original user model)",
        "body": " between the old and new model. On the original user model, a user’s access to accounts is based on: a) being added to a specific <em>account</em>, or b) having access to a parent <em>account</em> and inheriting access to that <em>account</em>’s child accounts. See our original user management docs. See an explanation of how parent&#x2F;child accounts work."
      },
      "id": "60d60855196a67b29e5e1629"
    },
    {
      "sections": [
        "Rules and limits for alerts",
        "Permission levels",
        "New Relic One pricing model",
        "Original pricing model",
        "Limits",
        "Tip"
      ],
      "title": "Rules and limits for alerts",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Rules, limits, and glossary"
      ],
      "external_id": "21a79b6a8acf57efc16c3fae83e5167367b82452",
      "image": "",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/learn-alerts/rules-limits-alerts/",
      "published_at": "2021-12-19T16:03:21Z",
      "updated_at": "2021-11-14T07:53:31Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document contains some technical rules and limits for alerts. Permission levels Permissions differ depending on whether you're on our original product-based pricing model or our New Relic One pricing model: New Relic One pricing model See Users and roles. Original pricing model For accounts on our original product-based pricing model, the user role determines the Alerts features available: Owner and Admins can add, change, disable, and delete alert policies, conditions, and channels. Users and Restricted Users can only view alert policies and conditions and their settings. Any user role can acknowledge an incident or close a violation. Limits If your organization has a parent/child account structure, child accounts do not inherit a parent account's alert policies: You must create policies separately for all child accounts. The following rules apply both to the New Relic One user interface and to the REST API (v2). New Relic Alerts Minimum value Maximum value Alert policies: Alert policy name 1 character 64 characters Policies per account n/a 10000 policies Products per policy any New Relic product (APM, mobile monitoring, synthetic monitoring, etc.) any New Relic product Alert conditions: Condition name 1 character 128 characters Conditions per policy 0 conditions 500 conditions Infrastructure alert conditions 0 conditions 3700 conditions NRQL query conditions OR Web app response percentiles per account 0 conditions 4000 conditions Targets (product entities) per condition 1 target 1000 targets from 1 or more products Thresholds per condition 0 Warnings, 1 Critical 1 Warning, 1 Critical Alert violations: Custom violation descriptions 4000 characters Duration for condition violation 5 minutes Exception: 1 minute for at least once conditions, because the violation could occur during the first minute (or any minute). 2 hours Violations per incident 1 violation 10,000 violations Violations beyond this limit will not be persisted. Violation Search API - Page Size 1 page (less than or equal to 25 violations) 1000 pages (25K violations) Tip Only use the only-open parameter to retrieve all open violations. If you have more than 25K open violations and need to retrieve them via the REST API, please contact New Relic Support. Notification channels: Notification channel name 1 character 64 characters Channels per account n/a 2500 channels per type Exception: No limits for user channels Channel limitations Depends on channel Depends on channel",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 448.30478,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Rules <em>and</em> limits for alerts",
        "sections": "Rules <em>and</em> limits for alerts",
        "tags": "Alerts <em>and</em> Applied Intelligence",
        "body": " can acknowledge an incident or close a violation. Limits If your <em>organization</em> has a parent&#x2F;child <em>account</em> <em>structure</em>, child accounts do not inherit a parent <em>account</em>&#x27;s alert policies: You must create policies separately for all child accounts. The following rules apply both to the New Relic One user"
      },
      "id": "60442974196a678217960f33"
    }
  ],
  "/docs/accounts/accounts-billing/general-account-settings/default-time-zone-setting": [
    {
      "sections": [
        "Introduction to account settings",
        "Pricing, billing, and usage UI",
        "Manage users",
        "Add accounts",
        "Other account settings"
      ],
      "title": "Introduction to account settings",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "General account settings"
      ],
      "external_id": "1dbac104fa7e60637c32a61a80b8b709f6fd84c2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/general-account-settings/introduction-account-settings/",
      "published_at": "2021-12-19T13:41:30Z",
      "updated_at": "2021-11-13T02:21:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To view and edit account-related and user-management-related settings, go to one.newrelic.com and in the top right corner, select the account dropdown. Keep reading for tips on how to find different kinds of account settings in the UI. Pricing, billing, and usage UI Note that user permissions may impact your access to some account settings. To find billing-related UI, go to one.newrelic.com and, in the upper right corner, click the account dropdown to access various account settings. Here's what you can find there: Manage your plan: Options for viewing and managing billing-related settings. Manage your data: Options for managing ingest of data, which can be a billing factor. Learn more about managing data. View your usage: View of your billing-related usage. Administration: Options related to billing, usage, and user management (this UI available only for users on our New Relic One user model). Account settings: Options related to subscription and usage, and user management (this UI available only to users on our original user model). Related topics: Pricing details: Original pricing model | New Relic One pricing Manage data Manage users How you manage users depends on which user model you're on: New Relic One user model: see Manage users. Original user model: from one.newrelic.com, click the account dropdown, click Account settings, and then click Users and roles. For more on user management, see Original users. Not sure which user model you're on? See Determine user model. Add accounts Learn more about adding accounts. Other account settings See the account settings docs.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.82355,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>account</em> <em>settings</em>",
        "sections": "Introduction to <em>account</em> <em>settings</em>",
        "tags": "<em>General</em> <em>account</em> <em>settings</em>",
        "body": "To view and edit <em>account</em>-related and user-management-related <em>settings</em>, go to one.newrelic.com and in the top right corner, select the <em>account</em> dropdown. Keep reading for tips on how to find different kinds of <em>account</em> <em>settings</em> in the UI. Pricing, <em>billing</em>, and usage UI Note that user permissions may"
      },
      "id": "6043f38a196a679ae4960f5e"
    },
    {
      "sections": [
        "Non-profits and New Relic",
        "Exceptions",
        "Tip",
        "Additional requirements",
        "Signup procedures",
        "Program Benefits"
      ],
      "title": "Non-profits and New Relic",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "General account settings"
      ],
      "external_id": "441757ebbd6d8039278355701793bcbcae0fbc8a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/new-relic-one-pricing-billing/eligibility-guidelines-new-relic-nonprofit-program/",
      "published_at": "2021-12-19T17:31:50Z",
      "updated_at": "2021-11-06T19:59:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document provides New Relic's Observability for Good program eligibility guidelines for nonprofit, charitable, and NGO organizations. Exceptions Many global nonprofit, charity and NGO organizations are eligible for Observability for Good if the organization has a recognized legal status in their respective country equal to 501(c)(3) status under the United States Internal Revenue Code, with certain exceptions. In addition, all organizations must be verified by TechSoup or the local TechSoup partner. Examples of ineligible organizations include, but are not limited to: Ineligible nonprofits Comments Higher education institutions Private nonprofit and public higher education institutions do not qualify. This includes: Universities Colleges Trade schools Primary and secondary schools (public and private) Primary and secondary schools do not qualify. This includes, but is not limited to: K-12 public school districts Individual K-12 public schools within a state or federally funded school district Standalone K-12 charter schools K-12 Knowledge is Power Program (KiPP) schools Other K-12 schools that are not part of a state or federally funded school district Healthcare organizations providing patient care This includes hospitals, hospital auxiliaries, healthcare systems, and related health services organizations, such as: Nursing or convalescent homes Care and housing for the aged Pregnancy centers Tip Free clinics may qualify. Professional, commerce, mutual, and trade organizations This includes organizations such as: Credit Unions Regulation of business Industry trade shows Professional athletic leagues Tourist bureaus Employee or membership benefit organizations This includes organizations such as: Fraternal Beneficiary societies Associations of employees Employee or member welfare associations Pension and retirement benefits Tip Organizations focusing on the improvement of working conditions may qualify. Legislative or political organizations and advocacy groups Organizations focused on nonpartisan voter education may qualify. Organizations within countries sanctioned by the US This includes organizations within any sanctioned countries included on the US Department of Treasury's Office of Foreign Assets control list , which is updated periodically. Additional requirements In order to participate, approved organizations must also: Submit an application through newrelic.org/signup. Complete the eligibility validation process through TechSoup. Tip Learn about TechSoups local NGO definitions. Accept New Relic's standard terms of service. Be a direct customer of New Relic and have a direct billing relationship with New Relic. Signup procedures To learn more or to sign up as a new or existing New Relic customer, go to newrelic.org/signup. Program Benefits Observability for Good Standard: 4 additional free users per month (+1 free edition user: 5 total) 900 additional gb of data (+100 gb free edition: 1TB total) Observability for Good Pro: 2 free users per month 1 free TB data per month Tip A payment method on file is required for accounts using Observability for Good Pro.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 222.19801,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Non-profits <em>and</em> New Relic",
        "sections": "Non-profits <em>and</em> New Relic",
        "tags": "<em>General</em> <em>account</em> <em>settings</em>",
        "body": " through newrelic.org&#x2F;signup. Complete the eligibility validation process through TechSoup. Tip Learn about TechSoups local NGO definitions. Accept New Relic&#x27;s standard terms of service. Be a direct customer of New Relic and have a direct <em>billing</em> relationship with New Relic. Signup procedures To learn"
      },
      "id": "6186de8c196a678bfdf4378b"
    },
    {
      "sections": [
        "Set or change password",
        "Change your password",
        "Important",
        "Reset password"
      ],
      "title": "Set or change password",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "General account settings"
      ],
      "external_id": "817d8d17dca7f361429109d8364eff931a1cb392",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/account-maintenance/change-passwords-user-preferences/",
      "published_at": "2021-12-19T15:39:09Z",
      "updated_at": "2021-10-13T03:31:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document contains information on New Relic password requirements. Change your password New Relic account passwords don't expire. However, users can change their own password and other personal account information anytime. Your ability to change your password from the UI may depend on your user and organization level settings. To change your password from the UI: From one.newrelic.com, click the account dropdown and select User preferences. When creating a password, adhere to the password requirements below. Your password must meet these minimum requirements: 8 to 50 characters in length At least one letter (a-z, A-Z) At least one number (0-9), special character, or space Special characters allowed include ~ ` ! @ # $ % ^ & * ( ) _ - + = { [ } ] : ; \" ' < , > . ? / | \\. Spaces are also allowed. Important In addition to New Relic's requirements, follow your organization's guidelines for password length, use of upper or lower case letters, numbers, and special characters. Reset password If you forgot your own password or need to request a password reset, you can use New Relic's self-service options. Admins can't reset passwords for other users. If you need to reset someone else's password, get support at support.newrelic.com.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 209.15845,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Set</em> or change password",
        "sections": "<em>Set</em> or change password",
        "tags": "<em>General</em> <em>account</em> <em>settings</em>",
        "body": " and organization level <em>settings</em>. To change your password from the UI: From one.newrelic.com, click the <em>account</em> dropdown and select User preferences. When creating a password, adhere to the password requirements below. Your password must meet these minimum requirements: 8 to 50 characters in length"
      },
      "id": "603eb55a64441f9f8f4e889c"
    }
  ],
  "/docs/accounts/accounts-billing/general-account-settings/introduction-account-settings": [
    {
      "sections": [
        "Non-profits and New Relic",
        "Exceptions",
        "Tip",
        "Additional requirements",
        "Signup procedures",
        "Program Benefits"
      ],
      "title": "Non-profits and New Relic",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "General account settings"
      ],
      "external_id": "441757ebbd6d8039278355701793bcbcae0fbc8a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/new-relic-one-pricing-billing/eligibility-guidelines-new-relic-nonprofit-program/",
      "published_at": "2021-12-19T17:31:50Z",
      "updated_at": "2021-11-06T19:59:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document provides New Relic's Observability for Good program eligibility guidelines for nonprofit, charitable, and NGO organizations. Exceptions Many global nonprofit, charity and NGO organizations are eligible for Observability for Good if the organization has a recognized legal status in their respective country equal to 501(c)(3) status under the United States Internal Revenue Code, with certain exceptions. In addition, all organizations must be verified by TechSoup or the local TechSoup partner. Examples of ineligible organizations include, but are not limited to: Ineligible nonprofits Comments Higher education institutions Private nonprofit and public higher education institutions do not qualify. This includes: Universities Colleges Trade schools Primary and secondary schools (public and private) Primary and secondary schools do not qualify. This includes, but is not limited to: K-12 public school districts Individual K-12 public schools within a state or federally funded school district Standalone K-12 charter schools K-12 Knowledge is Power Program (KiPP) schools Other K-12 schools that are not part of a state or federally funded school district Healthcare organizations providing patient care This includes hospitals, hospital auxiliaries, healthcare systems, and related health services organizations, such as: Nursing or convalescent homes Care and housing for the aged Pregnancy centers Tip Free clinics may qualify. Professional, commerce, mutual, and trade organizations This includes organizations such as: Credit Unions Regulation of business Industry trade shows Professional athletic leagues Tourist bureaus Employee or membership benefit organizations This includes organizations such as: Fraternal Beneficiary societies Associations of employees Employee or member welfare associations Pension and retirement benefits Tip Organizations focusing on the improvement of working conditions may qualify. Legislative or political organizations and advocacy groups Organizations focused on nonpartisan voter education may qualify. Organizations within countries sanctioned by the US This includes organizations within any sanctioned countries included on the US Department of Treasury's Office of Foreign Assets control list , which is updated periodically. Additional requirements In order to participate, approved organizations must also: Submit an application through newrelic.org/signup. Complete the eligibility validation process through TechSoup. Tip Learn about TechSoups local NGO definitions. Accept New Relic's standard terms of service. Be a direct customer of New Relic and have a direct billing relationship with New Relic. Signup procedures To learn more or to sign up as a new or existing New Relic customer, go to newrelic.org/signup. Program Benefits Observability for Good Standard: 4 additional free users per month (+1 free edition user: 5 total) 900 additional gb of data (+100 gb free edition: 1TB total) Observability for Good Pro: 2 free users per month 1 free TB data per month Tip A payment method on file is required for accounts using Observability for Good Pro.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 222.19801,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Non-profits <em>and</em> New Relic",
        "sections": "Non-profits <em>and</em> New Relic",
        "tags": "<em>General</em> <em>account</em> <em>settings</em>",
        "body": " through newrelic.org&#x2F;signup. Complete the eligibility validation process through TechSoup. Tip Learn about TechSoups local NGO definitions. Accept New Relic&#x27;s standard terms of service. Be a direct customer of New Relic and have a direct <em>billing</em> relationship with New Relic. Signup procedures To learn"
      },
      "id": "6186de8c196a678bfdf4378b"
    },
    {
      "sections": [
        "Set or change password",
        "Change your password",
        "Important",
        "Reset password"
      ],
      "title": "Set or change password",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "General account settings"
      ],
      "external_id": "817d8d17dca7f361429109d8364eff931a1cb392",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/account-maintenance/change-passwords-user-preferences/",
      "published_at": "2021-12-19T15:39:09Z",
      "updated_at": "2021-10-13T03:31:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document contains information on New Relic password requirements. Change your password New Relic account passwords don't expire. However, users can change their own password and other personal account information anytime. Your ability to change your password from the UI may depend on your user and organization level settings. To change your password from the UI: From one.newrelic.com, click the account dropdown and select User preferences. When creating a password, adhere to the password requirements below. Your password must meet these minimum requirements: 8 to 50 characters in length At least one letter (a-z, A-Z) At least one number (0-9), special character, or space Special characters allowed include ~ ` ! @ # $ % ^ & * ( ) _ - + = { [ } ] : ; \" ' < , > . ? / | \\. Spaces are also allowed. Important In addition to New Relic's requirements, follow your organization's guidelines for password length, use of upper or lower case letters, numbers, and special characters. Reset password If you forgot your own password or need to request a password reset, you can use New Relic's self-service options. Admins can't reset passwords for other users. If you need to reset someone else's password, get support at support.newrelic.com.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 209.15845,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Set</em> or change password",
        "sections": "<em>Set</em> or change password",
        "tags": "<em>General</em> <em>account</em> <em>settings</em>",
        "body": " and organization level <em>settings</em>. To change your password from the UI: From one.newrelic.com, click the <em>account</em> dropdown and select User preferences. When creating a password, adhere to the password requirements below. Your password must meet these minimum requirements: 8 to 50 characters in length"
      },
      "id": "603eb55a64441f9f8f4e889c"
    },
    {
      "sections": [
        "Change account or user name",
        "Change user name",
        "Change account name",
        "Important"
      ],
      "title": "Change account or user name",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "General account settings"
      ],
      "external_id": "328b3ae3afa62555d246baee53ec49add7ad470c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/account-maintenance/change-your-new-relic-account-name/",
      "published_at": "2021-12-19T15:39:09Z",
      "updated_at": "2021-08-26T14:42:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Change user name To change your user name: From the account dropdown, select User preferences. On the right side, click Full name. Change account name Only users on our original user model are able to change their New Relic account name. To do this: From the account dropdown, select Account settings. On the right side, edit the Name field. Important If you're on our New Relic One user model and want to change your account name, contact your account representative or support.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 198.82697,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Change <em>account</em> or user name",
        "sections": "Change <em>account</em> or user name",
        "tags": "<em>General</em> <em>account</em> <em>settings</em>",
        "body": " <em>settings</em>. On the right side, edit the Name field. Important If you&#x27;re on our New Relic One user model and want to change your <em>account</em> name, contact your <em>account</em> representative or support."
      },
      "id": "6043ce92e7b9d215e55799e4"
    }
  ],
  "/docs/accounts/accounts-billing/new-relic-one-pricing-billing/eligibility-guidelines-new-relic-nonprofit-program": [
    {
      "sections": [
        "Introduction to account settings",
        "Pricing, billing, and usage UI",
        "Manage users",
        "Add accounts",
        "Other account settings"
      ],
      "title": "Introduction to account settings",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "General account settings"
      ],
      "external_id": "1dbac104fa7e60637c32a61a80b8b709f6fd84c2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/general-account-settings/introduction-account-settings/",
      "published_at": "2021-12-19T13:41:30Z",
      "updated_at": "2021-11-13T02:21:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To view and edit account-related and user-management-related settings, go to one.newrelic.com and in the top right corner, select the account dropdown. Keep reading for tips on how to find different kinds of account settings in the UI. Pricing, billing, and usage UI Note that user permissions may impact your access to some account settings. To find billing-related UI, go to one.newrelic.com and, in the upper right corner, click the account dropdown to access various account settings. Here's what you can find there: Manage your plan: Options for viewing and managing billing-related settings. Manage your data: Options for managing ingest of data, which can be a billing factor. Learn more about managing data. View your usage: View of your billing-related usage. Administration: Options related to billing, usage, and user management (this UI available only for users on our New Relic One user model). Account settings: Options related to subscription and usage, and user management (this UI available only to users on our original user model). Related topics: Pricing details: Original pricing model | New Relic One pricing Manage data Manage users How you manage users depends on which user model you're on: New Relic One user model: see Manage users. Original user model: from one.newrelic.com, click the account dropdown, click Account settings, and then click Users and roles. For more on user management, see Original users. Not sure which user model you're on? See Determine user model. Add accounts Learn more about adding accounts. Other account settings See the account settings docs.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.82355,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>account</em> <em>settings</em>",
        "sections": "Introduction to <em>account</em> <em>settings</em>",
        "tags": "<em>General</em> <em>account</em> <em>settings</em>",
        "body": "To view and edit <em>account</em>-related and user-management-related <em>settings</em>, go to one.newrelic.com and in the top right corner, select the <em>account</em> dropdown. Keep reading for tips on how to find different kinds of <em>account</em> <em>settings</em> in the UI. Pricing, <em>billing</em>, and usage UI Note that user permissions may"
      },
      "id": "6043f38a196a679ae4960f5e"
    },
    {
      "sections": [
        "Set or change password",
        "Change your password",
        "Important",
        "Reset password"
      ],
      "title": "Set or change password",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "General account settings"
      ],
      "external_id": "817d8d17dca7f361429109d8364eff931a1cb392",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/account-maintenance/change-passwords-user-preferences/",
      "published_at": "2021-12-19T15:39:09Z",
      "updated_at": "2021-10-13T03:31:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document contains information on New Relic password requirements. Change your password New Relic account passwords don't expire. However, users can change their own password and other personal account information anytime. Your ability to change your password from the UI may depend on your user and organization level settings. To change your password from the UI: From one.newrelic.com, click the account dropdown and select User preferences. When creating a password, adhere to the password requirements below. Your password must meet these minimum requirements: 8 to 50 characters in length At least one letter (a-z, A-Z) At least one number (0-9), special character, or space Special characters allowed include ~ ` ! @ # $ % ^ & * ( ) _ - + = { [ } ] : ; \" ' < , > . ? / | \\. Spaces are also allowed. Important In addition to New Relic's requirements, follow your organization's guidelines for password length, use of upper or lower case letters, numbers, and special characters. Reset password If you forgot your own password or need to request a password reset, you can use New Relic's self-service options. Admins can't reset passwords for other users. If you need to reset someone else's password, get support at support.newrelic.com.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 209.15845,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Set</em> or change password",
        "sections": "<em>Set</em> or change password",
        "tags": "<em>General</em> <em>account</em> <em>settings</em>",
        "body": " and organization level <em>settings</em>. To change your password from the UI: From one.newrelic.com, click the <em>account</em> dropdown and select User preferences. When creating a password, adhere to the password requirements below. Your password must meet these minimum requirements: 8 to 50 characters in length"
      },
      "id": "603eb55a64441f9f8f4e889c"
    },
    {
      "sections": [
        "Change account or user name",
        "Change user name",
        "Change account name",
        "Important"
      ],
      "title": "Change account or user name",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "General account settings"
      ],
      "external_id": "328b3ae3afa62555d246baee53ec49add7ad470c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/account-maintenance/change-your-new-relic-account-name/",
      "published_at": "2021-12-19T15:39:09Z",
      "updated_at": "2021-08-26T14:42:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Change user name To change your user name: From the account dropdown, select User preferences. On the right side, click Full name. Change account name Only users on our original user model are able to change their New Relic account name. To do this: From the account dropdown, select Account settings. On the right side, edit the Name field. Important If you're on our New Relic One user model and want to change your account name, contact your account representative or support.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 198.82695,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Change <em>account</em> or user name",
        "sections": "Change <em>account</em> or user name",
        "tags": "<em>General</em> <em>account</em> <em>settings</em>",
        "body": " <em>settings</em>. On the right side, edit the Name field. Important If you&#x27;re on our New Relic One user model and want to change your <em>account</em> name, contact your <em>account</em> representative or support."
      },
      "id": "6043ce92e7b9d215e55799e4"
    }
  ],
  "/docs/accounts/accounts-billing/new-relic-one-pricing-billing/new-relic-one-pricing-billing": [
    {
      "sections": [
        "Query and alert on billing/usage data",
        "Available data types",
        "Query examples",
        "Data usage queries",
        "Daily data usage",
        "Daily usage by source",
        "Metrics ingest by source",
        "Month-to-date data usage",
        "Month-to-date estimated data cost",
        "User count queries",
        "Month-to-date full platform users",
        "Projected monthly full platform user count",
        "Count full platform users and basic users",
        "Set usage alerts",
        "Caution",
        "Ingested gigabytes exceed a fixed value",
        "Usage exceeds fixed threshold for GBs",
        "Usage exceeds fixed threshold for users",
        "Usage exceeds fixed threshold for estimated cost",
        "Available attributes"
      ],
      "title": "Query and alert on billing/usage data",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "New Relic One pricing and billing"
      ],
      "external_id": "e22ae9e26686d11726a82ad4036ff58520b4a439",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/new-relic-one-pricing-billing/usage-queries-alerts/",
      "published_at": "2021-12-19T15:22:20Z",
      "updated_at": "2021-12-04T21:08:00Z",
      "document_type": "page",
      "popularity": 1,
      "body": "For accounts on our New Relic One pricing model, we provide a View your usage UI for understanding billing-related usage and a Manage your data UI for managing billing-related data. Additionally, you can: Query your usage data to get more detail than is available in the UI. Set up NRQL alert conditions to get notifications about changes in your usage. Note that account hierarchy may affect queried data. See Account structure. Available data types Usage data is attached to these events: NrConsumption records usage every hour, and is the equivalent of \"real-time\" usage. Use this event to observe usage trends over time. NrMTDConsumption generates aggregate values from the NrConsumption event. Use this event to see usage or estimated cost for a billing period. NrUsage records usage every hour and is used to see usage reported per product. To see changes made to your account (for example, user management changes), you can query NrAuditEvent. Query examples The View your usage UI displays your data usage and billable user count. If you need more detail than that UI provides, you can use these NRQL queries. For definitions of some of the attributes used in these queries, see Attributes. Data usage queries Here are some data usage query examples: Daily data usage This query totals your billable ingested data, and displays a daily value for the past three months: FROM NrConsumption SELECT sum(GigabytesIngested) WHERE productLine = 'DataPlatform' SINCE 3 months ago TIMESERIES 1 day Copy Daily usage by source This query totals your billable ingested data, and displays a daily value for the past three months faceted by the source: FROM NrConsumption SELECT sum(GigabytesIngested) WHERE productLine = 'DataPlatform' SINCE 3 months ago FACET usageMetric TIMESERIES 1 day Copy Metrics ingest by source This query breaks down Metric data by the top ten metric names. You could also facet by appName or host to adjust the analysis. FROM Metric SELECT bytecountestimate()/10e8 as 'GB Estimate' SINCE '2021-04-01' UNTIL '2021-04-08' FACET metricName LIMIT 10 TIMESERIES 1 day Copy Month-to-date data usage This query shows the current full platform user count. In other words, it shows how much you'd be billed for your data for that month if you were billed right now. FROM NrMTDConsumption SELECT latest(GigabytesIngested) WHERE productLine = 'DataPlatform' SINCE this month Copy Month-to-date estimated data cost This query shows the estimated cost of your ingested data: FROM NrMTDConsumption SELECT latest(estimatedCost) WHERE productLine = 'DataPlatform' SINCE this month Copy User count queries Here are some user-related query examples. For details on how users are counted, see User count calculations. Month-to-date full platform users This query shows the billable full platform users for the month. In other words, it shows how much you'd be billed for your users for that month if you were billed right now. FROM NrMTDConsumption SELECT latest(usersBillable) SINCE this month Copy This query shows how many full platform users were counted by hour. This is useful for seeing how the full platform user count changed over time. from NrConsumption SELECT max(FullUsers) SINCE 10 days ago TIMESERIES 1 hour Copy Projected monthly full platform user count This query shows a projected count of monthly users. This query would not be good for using in a dashboard; it requires values based on a) the days remaining in the month, b) the start of the month. Here's an example querying the projected end-of-month count with 10 days left in that month: FROM NrMTDConsumption SELECT predictLinear(FullUsers, 10 days) SINCE '2020-09-01' Copy Count full platform users and basic users The usage UI shows the count of full platform users and basic users. The query used is: FROM NrUsage SELECT max(usage) SINCE 10 days ago WHERE productLine='FullStackObservability' WHERE metric in ('FullUsers', 'BasicUsers') FACET metric Copy To see the count of full and basic users over time: FROM NrUsage SELECT max(usage) SINCE 10 days ago WHERE productLine='FullStackObservability' WHERE metric in ('FullUsers', 'BasicUsers') FACET metric TIMESERIES 1 hour Copy Set usage alerts To help manage your billable data, you can set alerts to notify you of unexpected increases in usage. Learn how to create alerts with NRQL queries here. Caution When creating alert conditions, you should use the Event Timer method, which works very well with infrequent data. Here are some NRQL alert condition examples. For attribute definitions, see Attributes. Ingested gigabytes exceed a fixed value This query will create an alert when your hourly usage exceeds a fixed value: FROM NrConsumption SELECT sum(GigabytesIngested) WHERE productLine = 'DataPlatform' Copy If you have multiple child accounts, you may want to set threshold alerts for a specific subaccount: FROM NrConsumption SELECT sum(GigabytesIngested) WHERE productLine = 'DataPlatform' AND consumingAccountId = YOUR_CHILD-ACCOUNT_ID Copy Usage exceeds fixed threshold for GBs This query will create an alert when your usage exceeds fixed monthly threshold for GBs: FROM NrMTDConsumption SELECT latest(GigabytesIngested) WHERE productLine = 'DataPlatform' Copy Usage exceeds fixed threshold for users This query will create an alert when your usage exceeds fixed monthly threshold for billable users: FROM NrMTDConsumption SELECT latest(usersBillable) Copy Usage exceeds fixed threshold for estimated cost This query will create an alert when your usage exceeds fixed threshold for estimated cost: FROM NrMTDConsumption SELECT latest(estimatedCost) WHERE productLine = 'DataPlatform' Copy Available attributes Below are some of the important attributes attached to usage events. Attribute Description productLine The category of usage. There are three options: DataPlatform, FullStackObservability, and ProactiveDetection. (Starting November 1, 2021, IncidentIntelligence is no longer a billing factor). For more details about these categories, see New Relic platform. metric Consolidates multiple categories of usage into a single metric. Helpful when faceting by productLine. consumingAccountId ID of the New Relic account that is directly responsible for the stored events, as determined from the license key used. estimatedCost Calculates a cost estimate based on usage and metric cost. This is an estimate of costs to date, not your monthly invoice. For more attributes, see the data dictionary.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 479.80417,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Query <em>and</em> alert on <em>billing</em>&#x2F;usage data",
        "sections": "Query <em>and</em> alert on <em>billing</em>&#x2F;usage data",
        "tags": "<em>New</em> <em>Relic</em> <em>One</em> <em>pricing</em> <em>and</em> <em>billing</em>",
        "body": "For <em>accounts</em> on our <em>New</em> <em>Relic</em> <em>One</em> <em>pricing</em> model, we provide a View your usage UI for understanding <em>billing</em>-related usage and a Manage your data UI for managing <em>billing</em>-related data. Additionally, you can: Query your usage data to get more detail than is available in the UI. Set up NRQL alert"
      },
      "id": "6175f12b64441f53a35fc21c"
    },
    {
      "sections": [
        "Tutorials on user management tasks: access grants, custom roles, and adding users (New Relic One user model)",
        "Requirements",
        "Overview",
        "Add users",
        "View available accounts",
        "Grant access to accounts and roles",
        "Create custom role",
        "Add users to groups"
      ],
      "title": "Tutorials on user management tasks: access grants, custom roles, and adding users (New Relic One user model)",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "New Relic One user management"
      ],
      "external_id": "b1e5f303b6446f264c9d8c5020871a6990e052fd",
      "image": "https://docs.newrelic.com/static/a78dad5ff794da5deaaf3b514e334ea7/c1b63/new-relic-one-user-mgmt.png",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/new-relic-one-user-management/tutorial-add-new-user-groups-roles-new-relic-one-user-model/",
      "published_at": "2021-12-19T16:26:00Z",
      "updated_at": "2021-11-24T14:27:52Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This tutorial will walk you through some common procedures for managing users on the New Relic One user model. Here's a quick overview of the process (3:24 minutes): Requirements Some notes on requirements: This tutorial is for users on our New Relic One user model. Creating access grants requires Pro or Enterprise edition. For details, see user management requirements. Role requirements: Using the Organization and access UI to manage group access requires Authentication domain manager and Organization manager roles. Using the User management UI to add/edit users requires Authentication domain manager role. Overview Before using this tutorial, we recommend you check out: Understand what an access grant is Videos that show the user management UI in action This tutorial will walk you through how to: Add users View available accounts Grant groups access to roles and accounts Create custom roles Add users to groups Add users Adding users to New Relic is a separate process from creating access grants. In other words, you can do either procedure first. The key point is that if you're trying to give a user access to a role or an account that they don't yet have, they won't have that access until they're both a) in a group and b) that group has the correct access grant (access to a specific role on a specific account). There are two ways to add users: With the user management UI (described below) With automated user management, also known as SCIM provisioning: use of this method means that groups are defined in your identity provider and so you can't edit a user's group from the New Relic UI. To add users from the UI: From the top right of the New Relic UI, click the account dropdown, click Administration, and click User management. If you have multiple authentication domains, choose one from the authentication domain dropdown. Click Add user. Complete the process, including choosing user type and user group. Relevant tips: For how to bulk edit users' user type, see Edit user type. When choosing a group, you can choose either one of our default groups or a custom group that you've defined an access grant for. For more on creating access grants, keep reading. View available accounts When thinking about creating access grants for granting access to different accounts, it can help you to understand what accounts there are in your organization. To view the accounts in your organization: go to the account dropdown, click Administration, click Organizations and access, and then click Accounts. Grant access to accounts and roles Groups are used to group your users and manage what your users are able to do in New Relic: by creating an access grant, you assign a group access to a) a specific role on b) a specific account. By default, organizations on the New Relic One user model have two available groups: Admin and User. These default groups automatically have access to specific standard roles and are assigned to the account in which they were initially added. To view existing access grants: from the account dropdown, click Administration, and then click Organization and access. Even if you haven't created any custom access grants, you will see the default-created grants there that are present for our default groups. This is what you might see when you go to the Organization and access UI and view groups. The default available groups of Admin and User have access grants automatically created that grant users in those groups access to the roles associated with those groups and to the initial account those users were added in. To create a new access grant that gives a user group access to a role and an account: From the Organization and access UI, click Group access. If you don't see that UI, it may be because you're logged in with an original user model record, or because you don't have the ability to manage users. For more on such factors, see Factors affecting access. Choose one of the following: Existing: If there is already a group you want to add an access grant to, you can use this. For example, if you want to gives users in the default Admin or User group access to new accounts, you might choose this and then select the Admin Default or Admin User role. New: If you need to create a new group, choose this. You'll also need to choose the authentication domain that group is inside of (for an explanation of what \"Default\" means, see Default groups. If your organization strategy requires a good amount of restrictions over access to accounts and access to roles, you'll probably need to create a good amount of access grants. Next, under Access grant, you'll choose one of following: Account: Choose this to be able to select from the roles that are account-scoped. These are the roles that have to do with using and configuring our platform features (and not about organization and user management). Organization: Choose this to be able to select from the roles that are organization-scoped. These are the roles that govern organization- and user management. (Note that these users must also already belong to an account-scoped role. This is true for most users but if it's not, you may see a message that the user doesn't belong to an organization.) Select the Role you want to assign. Roles are organization-wide, so regardless of the authentication domain you're in, you have access to our standard roles and any custom roles you've created. For tips on selecting roles, see the tips after these instructions. Select the Account you want to add access to from the dropdown. If you don't see an account that you'd expect to see, this may be for a few reasons. One is that you yourself don't have the proper permissions for that account. Another is that that account is not actually in your organization. For more information, see Factors affecting access. If you are still having problems, talk to your account representative. If you want to continue adding more grants for that same group, select Add another at the bottom before clicking Add access. When you're done, if your users are already in the group you've added the grant to, they should have access within a few minutes (although for EU region New Relic accounts, this can take up to twenty minutes or so). If your users are not yet in that group (which would be true if you just created an access grant with a new group), you'll need to go to the User management UI and add one or more users to that group. Some tips for using this UI: If your users are managed via automated user management, you can't use the User management UI to add users to groups because your groups are imported from your identity provider. You will need to create access grants for those groups once they are in New Relic, though, to give those groups access. Note that if a user has the organization-scoped Organization manager and/or Authentication domain manager roles (which is true of users in our default Admin group) those users will always have those capabilities because those are organization-scoped abilities. This means that when you go to add those users to another account, you only have to add an account-scoped role, and not an organization-scoped role. In other words, once the users in a group have those organization-scoped roles, they will always have them for that organization unless removed. When selecting from amongst our standard roles, it's important to understand the difference between All product admin and Standard user. In short, All product admin is more popular a choice because it gives the ability to configure platform features. If you wanted to have your users be able to use platform features but not configure them, you'd choose Standard user. Create custom role When creating an access grant, you can use our standard roles, or you can create your own roles with unique sets of capabilities. To view existing roles: from the account dropdown, click Administration, then click Organization and access, and then click Roles. To create a custom role, click Add new custom role. Review the list of available capabilities and decide which ones your custom role needs. For more information about how roles and capabilities work, see Capabilities. Or check out this NerdByte video (4:07 minutes). Once you're finished creating a custom role, you will probably want to use it in a new access grant. Add users to groups In the user management UI, you can see your users and the groups they've been assigned to. Your users only have access to the access grants associated with the groups they've been added to. To view users and see their groups: from the account dropdown, click Administration, and click User management. If you don’t see that option, review the requirements. Groups reside within the boundaries of an authentication domain. If your organization has more than one authentication domain, the domain switcher in the top left will show which one you’re in. To add a user, click Add user. Complete the prompts in the UI, including choosing the user type and group. Any custom groups you’ve added should be available from the group dropdown. If the custom group you choose has been granted access to a role and an account, once you add the user to that group, that user will now have access. To edit a user’s group or other details: click on the user you want to edit and make changes.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 252.26945,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Tutorials on user management tasks: access grants, custom roles, <em>and</em> adding users (<em>New</em> <em>Relic</em> <em>One</em> user model)",
        "sections": "Tutorials on user management tasks: access grants, custom roles, <em>and</em> adding users (<em>New</em> <em>Relic</em> <em>One</em> user model)",
        "tags": "<em>Accounts</em> <em>and</em> <em>billing</em>",
        "body": " in your organization: go to the <em>account</em> dropdown, click Administration, click Organizations and access, and then click <em>Accounts</em>. Grant access to <em>accounts</em> and roles Groups are used to group your users and manage what your users are able to do in <em>New</em> <em>Relic</em>: by creating an access grant, you assign a group"
      },
      "id": "603e7d67196a671e26a83dc5"
    },
    {
      "sections": [
        "How to manage users",
        "Important",
        "Requirements",
        "Manage users in the UI",
        "Overview of user management concepts",
        "Common user management tasks",
        "Add, edit, and delete users",
        "Edit user type (basic and full platform)",
        "Give users access to accounts and roles (access grants)",
        "Create new custom groups and roles",
        "Set up SAML SSO and/or SCIM provisioning",
        "Grant users ability to manage other users",
        "Control how basic users upgrade to full platform users",
        "Track changes",
        "User management terms and definitions"
      ],
      "title": "How to manage users",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "New Relic One user management"
      ],
      "external_id": "9e37836740ce56302734d4af636bdbe087b4dbc3",
      "image": "https://docs.newrelic.com/static/a78dad5ff794da5deaaf3b514e334ea7/c1b63/new-relic-one-user-mgmt.png",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/new-relic-one-user-management/add-manage-users-groups-roles/",
      "published_at": "2021-12-19T15:22:19Z",
      "updated_at": "2021-11-24T14:23:16Z",
      "document_type": "page",
      "popularity": 1,
      "body": "For users on our New Relic One user model, we provide various user management features, including the ability to: Use role based access control (RBAC) to assign default or custom roles to user groups Create custom user groups Grant user groups access to specific roles and accounts Important This doc applies to users on the New Relic One user model. For managing users on our original user model, see Original users. Not sure which user model you're on? See User models. Requirements To see if you can access these user management features, go to the user management UI and see what you have access to. Access requirements: These features allow managing of users on the New Relic One user model. To learn more, see User models. To avoid configuration conflicts, ensure that only one user is managing users at a time. Simultaneous configuration by more than one user may result in errors and unexpected results. Most capabilities require the Authentication domain manager role and some require the Organization manager role. For details, see Standard roles. Pricing edition requirements: To manage user groups, roles, and access grants: Pro or Enterprise edition is required. To import users from an identity provider: Enterprise is required. A New Relic user can have a maximum of either three concurrent active sessions, or three unique IP addresses in use at any given time. Manage users in the UI For users on the New Relic One user model, to find your user management options: From the account dropdown, select Administration. There are two user management UI pages there: User management: Use this to add users, update user type (basic versus full platform) and other information, and approve user type upgrade requests. Organization and access: Use this page to create and manage groups, roles, and access grants, and to set up SAML SSO and SCIM provisioning. Want to see videos of the user management UI in action? See our user management videos. Important If you can't see these UI pages, it may be because you're on our original user model or because you don't have the required user management role. Overview of user management concepts If your organization has Pro or Enterprise edition, you can create and configure access grants. An access grant gives a group of users access to a specific role on a specific account. Here's a diagram showing how access grants work and how they relate to the broader organization: A diagram explaining the concept of how access grants give a user group access to a) a role on b) a specific account. (Note that this applies to users on our New Relic One user model (and not our original user model).) When your New Relic organization is created, it starts out with some default access grants for our default User or Admin groups. Each of these groups is assigned one or more roles and granted access to the primary (first created) account: A view of the Organization and access UI, showing the default access grants associated with our default groups. If you have a relatively flat organizational structure, and are okay with all or many of your users having wide administrative access and access to all accounts, you'll probably only need at most a few access grants. For example, you might decide to add new access grants to the existing default Admin or User groups, giving those roles access to other accounts. Or, if you need more granular definition over roles and permissions, you'd create access grants that define new groups that have access to specific roles (either our standard roles or custom-defined roles). For a tutorial on how to create access grants and create custom roles, see the User management tutorial. For other examples of some common user management tasks, see Example tasks. To see the UI in action, see our user management videos. Some tips on setting up access grants: It may help to first plan out how your access grants will be organized. How many accounts will you have? What user groups will get access to which roles and which accounts? Will you use our default groups and roles or create your own custom groups and roles? If you've used automated user management to provision users via SCIM, you will have to create access grants to give those users access. A common configuration for organizations with many accounts (roughly 20 or more) is setting up groups with the more organization-scoped roles (Organization manager, Authentication domain manager, and Billing user) on the primary account, and then on other accounts, setting up groups with the more product-scoped roles (like All product admin, Standard user, or custom roles). Common user management tasks In the Organization and access UI, you can create access grants, custom groups, custom roles, and configure an authentication domain. Here are some example user management procedures: Add, edit, and delete users See the user management tutorial. Edit user type (basic and full platform) Note that there are limits around how many times full platform users can be changed to basic users. Important If you're using automated user management to provision and manage your users, you have other options for managing user type. To change the user type of one or more users: On the User management page, click the checkboxes for the users whose user type you want to edit. Once you start selecting users, an option will appear for Edit type. You can also edit the user type and group of a specific user by clicking on that user. Give users access to accounts and roles (access grants) See our user management tutorial. Create new custom groups and roles See the user management tutorial. Set up SAML SSO and/or SCIM provisioning See Get started with SAML SSO or SCIM. Grant users ability to manage other users To grant your users the ability to manage other users, you'll need to add users to a group that has the Authentication domain manager and Organization manager role. You have two options: From the User management UI, you can add a user to the default Admin group, which includes both of those roles. OR You can create a custom group and assign it these roles. For a tutorial on creating new access grants, groups, and roles, see the user management tutorial. Control how basic users upgrade to full platform users See the authentication domain settings. Track changes To see an audit log of changes to your account, including user management actions, you can query the NrAuditEvent. User management terms and definitions For an explanation of how user access to accounts and roles works, see User management concepts explained. Here are some definitions for the terms we use there: A New Relic organization is the representation of your organization, containing all your accounts, users, and data. For more information, see Organization and account structure. A capability is an ability to use or edit a specific, granular New Relic feature. Examples of capabilities: The ability to modify APM settings The ability to delete alert conditions A role is a set of capabilities. It is what gives a user their permissions. Our default standard roles have various capability sets, and you can create custom roles that have a custom set of capabilities. See some specific New Relic capabilities. A user group has one or more roles associated with it. You assign your users to a group. We have default user groups (Admin and User), and you can make your own groups. An access grant is what grants a user group access to roles and to specific New Relic accounts. An access grant essentially states, \"This group is assigned this role on this New Relic account.\" Adding a user to a group doesn’t do anything unless that group is included in an access grant. An authentication domain contains a set of users who are added to New Relic and who log in to New Relic in the same way. For example, you may have one authentication domain for users who log in via username/password and another authentication domain for users who log in via SAML. If a user is a basic user, this takes precedence over any role-related limitations. For more on this, see Basic user and roles.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 252.26328,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Give users access to <em>accounts</em> <em>and</em> roles (access grants)",
        "tags": "<em>Accounts</em> <em>and</em> <em>billing</em>",
        "body": "For users on our <em>New</em> <em>Relic</em> <em>One</em> user model, we provide various user management features, including the ability to: Use role based access control (RBAC) to assign default or custom roles to user groups Create custom user groups Grant user groups access to specific roles and <em>accounts</em> Important"
      },
      "id": "603e7bce28ccbc415beba74c"
    }
  ],
  "/docs/accounts/accounts-billing/new-relic-one-pricing-billing/usage-queries-alerts": [
    {
      "sections": [
        "New Relic One pricing and billing",
        "Important",
        "How the New Relic One pricing model works",
        "Billing and usage in the UI",
        "Billing calculation details",
        "Data usage calculation",
        "Full platform user count billing details",
        "Data retention",
        "Billing periods",
        "Usage plan details",
        "Query and alert on usage data",
        "Free tier",
        "Non-profit use of New Relic",
        "Cancel or downgrade"
      ],
      "title": "New Relic One pricing and billing ",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "New Relic One pricing and billing"
      ],
      "external_id": "03d43f14ae24579c81b601571242aef540833c8c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/new-relic-one-pricing-billing/new-relic-one-pricing-billing/",
      "published_at": "2021-12-19T14:23:17Z",
      "updated_at": "2021-11-24T14:22:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "An explanation of how New Relic One pricing works, and how to view and manage billing. Important This document explains the New Relic One pricing model. If you’re on our original pricing model, see Original pricing. Not sure which you're on? See Overview of pricing. How the New Relic One pricing model works Starting July 30, 2020, all of our new customers are on a pricing model that we call New Relic One pricing. Customers on our original pricing model are able to transition to this pricing. For New Relic One pricing, billing is based on these factors: The amount of data ingested. 100 GBs per month is free. $0.25 per GB ingested above that. The number of provisioned full platform users, who have access to our more curated UI experiences. Basic users are free. The cost of each full platform user depends on your edition: Standard, Pro, or Enterprise. Standard edition includes one full platform user for free, and a max of five. Pro and Enterprise give access to more account and user management features, more support, longer data retention, and other features. For a summary of what's included for free, see Free edition. For an overview of pricing, see our Pricing page. Keep reading for details about New Relic One pricing and billing. Billing and usage in the UI For how to view and manage billing and usage in the UI, see Pricing and billing UI. If you need more detail than the usage UI shows, you can also run queries of your usage data and set up alerts. Billing calculation details For accounts on New Relic One pricing, some high-level billing information is displayed in the UI. Here are some more details about how billing works: Data usage calculation One pricing factor is your ingested data. In this context, “ingested” refers to the data actually saved to your account after we apply various data trimming and data transformation rules. In other words, it’s not the size of the raw data sent to New Relic, but the size of the data that actually ends up stored. To view and manage your usage, go to the usage UI. Other calculation details: In the context of our pricing model, a GB is defined as 1 billion bytes. Monthly data ingested is rounded down to the nearest integer. For example, if your account uses 100.9 GBs during a month, that’s counted as 100 GBs. For more on how data is ingested, see Manage data ingest. For how to query usage, see Query and alert on usage. Full platform user count billing details For accounts with New Relic One pricing, the monthly count of provisioned full platform users is one billing factor. To give an example: if you're on the Pro pricing edition and your organization has 100 full platform users during the month of January, you'd be billed for 100 full platform users for that month. A full platform user counts as a billable user the moment they're added to a New Relic organization (provisioned), whether or not that user has logged into or used New Relic yet. A user's user type is meant to be long-term setting determined by a user's expected New Relic duties and responsibilities. Because user type is a billing factor, we have restrictions around how often a full platform user can be downgraded to a basic user: a full platform user can downgrade to a basic user a maximum of two times in a rolling 12-month period. If a full platform user has been changed to a basic user two times in that 12-month period, that user won't be able to return to being a basic user until the start of the next 12-month period. To learn reasons for assigning one user type or another, see Tips on assigning user type. Here are more user-related billing details and caveats: You can see your full platform user count in the UI. We de-duplicate users based on email address. If there are multiple users in an organization that have the same email address, those user records count as a single user for billing purposes. The count of full platform users is prorated based on the start of a New Relic subscription, or based on when a user is created as a full platform user or converted to a full platform user. A New Relic user can have a maximum of either three concurrent active sessions, or three unique IP addresses in use at any given time. For organizations on our original user model that have a parent/child account structure, the count of billable users in the UI may differ from the users you can see. For more on this, see User count discrepancy. The Standard edition of the New Relic One pricing model includes one free full platform user. For organizations on our original user model, because the organization-related settings aren't as robust as on our newer model, a user may be set as a basic user in one account and as a full platform user in another account. In such cases, the full platform user status takes precedence and that user is considered a full platform user. For how to query usage data, see Query and alert on usage. For more on user capabilities, see Users and roles. Data retention See Data retention. Billing periods For pay-as-you-go customers, billing occurs at the end of the month (UTC), and you can see this tracked in the UI. When you input your credit card and start to be charged, your end-of-month bill will take into account all activity (billable data usage and users) that occurred since the beginning of that month. For example: if you input your credit card in the middle of the month, and so far at that point your account has 200 GBs of usage for that month, that 200 GBs counts towards your end-of-month bill. For how to query user-related usage, see Query and alert on usage. Usage plan details There are two New Relic One pricing usage plans: Pay-as-you-go: This plan bills at the end of each month. There are no commitments and you can cancel at any time. For details, see Usage plans. Annual pool of funds: This plan applies to some customers who have subscribed for a year or more. For details, see Usage plans. For some frequently asked questions, see Pricing FAQs. Query and alert on usage data To create detailed queries of your usage, and get notifications when you are close to hitting certain usage levels, see Query usage data. Free tier If your organization is on New Relic One pricing and on the Standard pricing edition, you can use New Relic free, forever, if you stay under the free allowed limits. Here's a summary of what Standard edition gets access to for free: A single account (Pro and Enterprise editions can have multiple accounts per organization). Up to 100 GBs of ingested data per month. One full platform user, and unlimited basic users. To upgrade to Pro or Enterprise, or to learn more about pricing, see New Relic pricing. Non-profit use of New Relic If you’re a non-profit and want to use New Relic at special pricing, see our Non-profit eligibility docs. Cancel or downgrade See Downgrade account.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 430.26495,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>New</em> <em>Relic</em> <em>One</em> <em>pricing</em> <em>and</em> <em>billing</em> ",
        "sections": "<em>New</em> <em>Relic</em> <em>One</em> <em>pricing</em> <em>and</em> <em>billing</em>",
        "tags": "<em>New</em> <em>Relic</em> <em>One</em> <em>pricing</em> <em>and</em> <em>billing</em>",
        "body": " details For <em>accounts</em> on <em>New</em> <em>Relic</em> <em>One</em> <em>pricing</em>, some high-level <em>billing</em> information is displayed in the UI. Here are some more details about how <em>billing</em> works: Data usage calculation <em>One</em> <em>pricing</em> factor is your ingested data. In this context, “ingested” refers to the data actually saved to your <em>account</em>"
      },
      "id": "6043f69a64441f7b26378eda"
    },
    {
      "sections": [
        "Tutorials on user management tasks: access grants, custom roles, and adding users (New Relic One user model)",
        "Requirements",
        "Overview",
        "Add users",
        "View available accounts",
        "Grant access to accounts and roles",
        "Create custom role",
        "Add users to groups"
      ],
      "title": "Tutorials on user management tasks: access grants, custom roles, and adding users (New Relic One user model)",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "New Relic One user management"
      ],
      "external_id": "b1e5f303b6446f264c9d8c5020871a6990e052fd",
      "image": "https://docs.newrelic.com/static/a78dad5ff794da5deaaf3b514e334ea7/c1b63/new-relic-one-user-mgmt.png",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/new-relic-one-user-management/tutorial-add-new-user-groups-roles-new-relic-one-user-model/",
      "published_at": "2021-12-19T16:26:00Z",
      "updated_at": "2021-11-24T14:27:52Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This tutorial will walk you through some common procedures for managing users on the New Relic One user model. Here's a quick overview of the process (3:24 minutes): Requirements Some notes on requirements: This tutorial is for users on our New Relic One user model. Creating access grants requires Pro or Enterprise edition. For details, see user management requirements. Role requirements: Using the Organization and access UI to manage group access requires Authentication domain manager and Organization manager roles. Using the User management UI to add/edit users requires Authentication domain manager role. Overview Before using this tutorial, we recommend you check out: Understand what an access grant is Videos that show the user management UI in action This tutorial will walk you through how to: Add users View available accounts Grant groups access to roles and accounts Create custom roles Add users to groups Add users Adding users to New Relic is a separate process from creating access grants. In other words, you can do either procedure first. The key point is that if you're trying to give a user access to a role or an account that they don't yet have, they won't have that access until they're both a) in a group and b) that group has the correct access grant (access to a specific role on a specific account). There are two ways to add users: With the user management UI (described below) With automated user management, also known as SCIM provisioning: use of this method means that groups are defined in your identity provider and so you can't edit a user's group from the New Relic UI. To add users from the UI: From the top right of the New Relic UI, click the account dropdown, click Administration, and click User management. If you have multiple authentication domains, choose one from the authentication domain dropdown. Click Add user. Complete the process, including choosing user type and user group. Relevant tips: For how to bulk edit users' user type, see Edit user type. When choosing a group, you can choose either one of our default groups or a custom group that you've defined an access grant for. For more on creating access grants, keep reading. View available accounts When thinking about creating access grants for granting access to different accounts, it can help you to understand what accounts there are in your organization. To view the accounts in your organization: go to the account dropdown, click Administration, click Organizations and access, and then click Accounts. Grant access to accounts and roles Groups are used to group your users and manage what your users are able to do in New Relic: by creating an access grant, you assign a group access to a) a specific role on b) a specific account. By default, organizations on the New Relic One user model have two available groups: Admin and User. These default groups automatically have access to specific standard roles and are assigned to the account in which they were initially added. To view existing access grants: from the account dropdown, click Administration, and then click Organization and access. Even if you haven't created any custom access grants, you will see the default-created grants there that are present for our default groups. This is what you might see when you go to the Organization and access UI and view groups. The default available groups of Admin and User have access grants automatically created that grant users in those groups access to the roles associated with those groups and to the initial account those users were added in. To create a new access grant that gives a user group access to a role and an account: From the Organization and access UI, click Group access. If you don't see that UI, it may be because you're logged in with an original user model record, or because you don't have the ability to manage users. For more on such factors, see Factors affecting access. Choose one of the following: Existing: If there is already a group you want to add an access grant to, you can use this. For example, if you want to gives users in the default Admin or User group access to new accounts, you might choose this and then select the Admin Default or Admin User role. New: If you need to create a new group, choose this. You'll also need to choose the authentication domain that group is inside of (for an explanation of what \"Default\" means, see Default groups. If your organization strategy requires a good amount of restrictions over access to accounts and access to roles, you'll probably need to create a good amount of access grants. Next, under Access grant, you'll choose one of following: Account: Choose this to be able to select from the roles that are account-scoped. These are the roles that have to do with using and configuring our platform features (and not about organization and user management). Organization: Choose this to be able to select from the roles that are organization-scoped. These are the roles that govern organization- and user management. (Note that these users must also already belong to an account-scoped role. This is true for most users but if it's not, you may see a message that the user doesn't belong to an organization.) Select the Role you want to assign. Roles are organization-wide, so regardless of the authentication domain you're in, you have access to our standard roles and any custom roles you've created. For tips on selecting roles, see the tips after these instructions. Select the Account you want to add access to from the dropdown. If you don't see an account that you'd expect to see, this may be for a few reasons. One is that you yourself don't have the proper permissions for that account. Another is that that account is not actually in your organization. For more information, see Factors affecting access. If you are still having problems, talk to your account representative. If you want to continue adding more grants for that same group, select Add another at the bottom before clicking Add access. When you're done, if your users are already in the group you've added the grant to, they should have access within a few minutes (although for EU region New Relic accounts, this can take up to twenty minutes or so). If your users are not yet in that group (which would be true if you just created an access grant with a new group), you'll need to go to the User management UI and add one or more users to that group. Some tips for using this UI: If your users are managed via automated user management, you can't use the User management UI to add users to groups because your groups are imported from your identity provider. You will need to create access grants for those groups once they are in New Relic, though, to give those groups access. Note that if a user has the organization-scoped Organization manager and/or Authentication domain manager roles (which is true of users in our default Admin group) those users will always have those capabilities because those are organization-scoped abilities. This means that when you go to add those users to another account, you only have to add an account-scoped role, and not an organization-scoped role. In other words, once the users in a group have those organization-scoped roles, they will always have them for that organization unless removed. When selecting from amongst our standard roles, it's important to understand the difference between All product admin and Standard user. In short, All product admin is more popular a choice because it gives the ability to configure platform features. If you wanted to have your users be able to use platform features but not configure them, you'd choose Standard user. Create custom role When creating an access grant, you can use our standard roles, or you can create your own roles with unique sets of capabilities. To view existing roles: from the account dropdown, click Administration, then click Organization and access, and then click Roles. To create a custom role, click Add new custom role. Review the list of available capabilities and decide which ones your custom role needs. For more information about how roles and capabilities work, see Capabilities. Or check out this NerdByte video (4:07 minutes). Once you're finished creating a custom role, you will probably want to use it in a new access grant. Add users to groups In the user management UI, you can see your users and the groups they've been assigned to. Your users only have access to the access grants associated with the groups they've been added to. To view users and see their groups: from the account dropdown, click Administration, and click User management. If you don’t see that option, review the requirements. Groups reside within the boundaries of an authentication domain. If your organization has more than one authentication domain, the domain switcher in the top left will show which one you’re in. To add a user, click Add user. Complete the prompts in the UI, including choosing the user type and group. Any custom groups you’ve added should be available from the group dropdown. If the custom group you choose has been granted access to a role and an account, once you add the user to that group, that user will now have access. To edit a user’s group or other details: click on the user you want to edit and make changes.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 252.26944,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Tutorials on user management tasks: access grants, custom roles, <em>and</em> adding users (<em>New</em> <em>Relic</em> <em>One</em> user model)",
        "sections": "Tutorials on user management tasks: access grants, custom roles, <em>and</em> adding users (<em>New</em> <em>Relic</em> <em>One</em> user model)",
        "tags": "<em>Accounts</em> <em>and</em> <em>billing</em>",
        "body": " in your organization: go to the <em>account</em> dropdown, click Administration, click Organizations and access, and then click <em>Accounts</em>. Grant access to <em>accounts</em> and roles Groups are used to group your users and manage what your users are able to do in <em>New</em> <em>Relic</em>: by creating an access grant, you assign a group"
      },
      "id": "603e7d67196a671e26a83dc5"
    },
    {
      "sections": [
        "How to manage users",
        "Important",
        "Requirements",
        "Manage users in the UI",
        "Overview of user management concepts",
        "Common user management tasks",
        "Add, edit, and delete users",
        "Edit user type (basic and full platform)",
        "Give users access to accounts and roles (access grants)",
        "Create new custom groups and roles",
        "Set up SAML SSO and/or SCIM provisioning",
        "Grant users ability to manage other users",
        "Control how basic users upgrade to full platform users",
        "Track changes",
        "User management terms and definitions"
      ],
      "title": "How to manage users",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "New Relic One user management"
      ],
      "external_id": "9e37836740ce56302734d4af636bdbe087b4dbc3",
      "image": "https://docs.newrelic.com/static/a78dad5ff794da5deaaf3b514e334ea7/c1b63/new-relic-one-user-mgmt.png",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/new-relic-one-user-management/add-manage-users-groups-roles/",
      "published_at": "2021-12-19T15:22:19Z",
      "updated_at": "2021-11-24T14:23:16Z",
      "document_type": "page",
      "popularity": 1,
      "body": "For users on our New Relic One user model, we provide various user management features, including the ability to: Use role based access control (RBAC) to assign default or custom roles to user groups Create custom user groups Grant user groups access to specific roles and accounts Important This doc applies to users on the New Relic One user model. For managing users on our original user model, see Original users. Not sure which user model you're on? See User models. Requirements To see if you can access these user management features, go to the user management UI and see what you have access to. Access requirements: These features allow managing of users on the New Relic One user model. To learn more, see User models. To avoid configuration conflicts, ensure that only one user is managing users at a time. Simultaneous configuration by more than one user may result in errors and unexpected results. Most capabilities require the Authentication domain manager role and some require the Organization manager role. For details, see Standard roles. Pricing edition requirements: To manage user groups, roles, and access grants: Pro or Enterprise edition is required. To import users from an identity provider: Enterprise is required. A New Relic user can have a maximum of either three concurrent active sessions, or three unique IP addresses in use at any given time. Manage users in the UI For users on the New Relic One user model, to find your user management options: From the account dropdown, select Administration. There are two user management UI pages there: User management: Use this to add users, update user type (basic versus full platform) and other information, and approve user type upgrade requests. Organization and access: Use this page to create and manage groups, roles, and access grants, and to set up SAML SSO and SCIM provisioning. Want to see videos of the user management UI in action? See our user management videos. Important If you can't see these UI pages, it may be because you're on our original user model or because you don't have the required user management role. Overview of user management concepts If your organization has Pro or Enterprise edition, you can create and configure access grants. An access grant gives a group of users access to a specific role on a specific account. Here's a diagram showing how access grants work and how they relate to the broader organization: A diagram explaining the concept of how access grants give a user group access to a) a role on b) a specific account. (Note that this applies to users on our New Relic One user model (and not our original user model).) When your New Relic organization is created, it starts out with some default access grants for our default User or Admin groups. Each of these groups is assigned one or more roles and granted access to the primary (first created) account: A view of the Organization and access UI, showing the default access grants associated with our default groups. If you have a relatively flat organizational structure, and are okay with all or many of your users having wide administrative access and access to all accounts, you'll probably only need at most a few access grants. For example, you might decide to add new access grants to the existing default Admin or User groups, giving those roles access to other accounts. Or, if you need more granular definition over roles and permissions, you'd create access grants that define new groups that have access to specific roles (either our standard roles or custom-defined roles). For a tutorial on how to create access grants and create custom roles, see the User management tutorial. For other examples of some common user management tasks, see Example tasks. To see the UI in action, see our user management videos. Some tips on setting up access grants: It may help to first plan out how your access grants will be organized. How many accounts will you have? What user groups will get access to which roles and which accounts? Will you use our default groups and roles or create your own custom groups and roles? If you've used automated user management to provision users via SCIM, you will have to create access grants to give those users access. A common configuration for organizations with many accounts (roughly 20 or more) is setting up groups with the more organization-scoped roles (Organization manager, Authentication domain manager, and Billing user) on the primary account, and then on other accounts, setting up groups with the more product-scoped roles (like All product admin, Standard user, or custom roles). Common user management tasks In the Organization and access UI, you can create access grants, custom groups, custom roles, and configure an authentication domain. Here are some example user management procedures: Add, edit, and delete users See the user management tutorial. Edit user type (basic and full platform) Note that there are limits around how many times full platform users can be changed to basic users. Important If you're using automated user management to provision and manage your users, you have other options for managing user type. To change the user type of one or more users: On the User management page, click the checkboxes for the users whose user type you want to edit. Once you start selecting users, an option will appear for Edit type. You can also edit the user type and group of a specific user by clicking on that user. Give users access to accounts and roles (access grants) See our user management tutorial. Create new custom groups and roles See the user management tutorial. Set up SAML SSO and/or SCIM provisioning See Get started with SAML SSO or SCIM. Grant users ability to manage other users To grant your users the ability to manage other users, you'll need to add users to a group that has the Authentication domain manager and Organization manager role. You have two options: From the User management UI, you can add a user to the default Admin group, which includes both of those roles. OR You can create a custom group and assign it these roles. For a tutorial on creating new access grants, groups, and roles, see the user management tutorial. Control how basic users upgrade to full platform users See the authentication domain settings. Track changes To see an audit log of changes to your account, including user management actions, you can query the NrAuditEvent. User management terms and definitions For an explanation of how user access to accounts and roles works, see User management concepts explained. Here are some definitions for the terms we use there: A New Relic organization is the representation of your organization, containing all your accounts, users, and data. For more information, see Organization and account structure. A capability is an ability to use or edit a specific, granular New Relic feature. Examples of capabilities: The ability to modify APM settings The ability to delete alert conditions A role is a set of capabilities. It is what gives a user their permissions. Our default standard roles have various capability sets, and you can create custom roles that have a custom set of capabilities. See some specific New Relic capabilities. A user group has one or more roles associated with it. You assign your users to a group. We have default user groups (Admin and User), and you can make your own groups. An access grant is what grants a user group access to roles and to specific New Relic accounts. An access grant essentially states, \"This group is assigned this role on this New Relic account.\" Adding a user to a group doesn’t do anything unless that group is included in an access grant. An authentication domain contains a set of users who are added to New Relic and who log in to New Relic in the same way. For example, you may have one authentication domain for users who log in via username/password and another authentication domain for users who log in via SAML. If a user is a basic user, this takes precedence over any role-related limitations. For more on this, see Basic user and roles.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 252.26324,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Give users access to <em>accounts</em> <em>and</em> roles (access grants)",
        "tags": "<em>Accounts</em> <em>and</em> <em>billing</em>",
        "body": "For users on our <em>New</em> <em>Relic</em> <em>One</em> user model, we provide various user management features, including the ability to: Use role based access control (RBAC) to assign default or custom roles to user groups Create custom user groups Grant user groups access to specific roles and <em>accounts</em> Important"
      },
      "id": "603e7bce28ccbc415beba74c"
    }
  ],
  "/docs/accounts/accounts-billing/new-relic-one-user-management/add-manage-users-groups-roles": [
    {
      "sections": [
        "Tutorials on user management tasks: access grants, custom roles, and adding users (New Relic One user model)",
        "Requirements",
        "Overview",
        "Add users",
        "View available accounts",
        "Grant access to accounts and roles",
        "Create custom role",
        "Add users to groups"
      ],
      "title": "Tutorials on user management tasks: access grants, custom roles, and adding users (New Relic One user model)",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "New Relic One user management"
      ],
      "external_id": "b1e5f303b6446f264c9d8c5020871a6990e052fd",
      "image": "https://docs.newrelic.com/static/a78dad5ff794da5deaaf3b514e334ea7/c1b63/new-relic-one-user-mgmt.png",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/new-relic-one-user-management/tutorial-add-new-user-groups-roles-new-relic-one-user-model/",
      "published_at": "2021-12-19T16:26:00Z",
      "updated_at": "2021-11-24T14:27:52Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This tutorial will walk you through some common procedures for managing users on the New Relic One user model. Here's a quick overview of the process (3:24 minutes): Requirements Some notes on requirements: This tutorial is for users on our New Relic One user model. Creating access grants requires Pro or Enterprise edition. For details, see user management requirements. Role requirements: Using the Organization and access UI to manage group access requires Authentication domain manager and Organization manager roles. Using the User management UI to add/edit users requires Authentication domain manager role. Overview Before using this tutorial, we recommend you check out: Understand what an access grant is Videos that show the user management UI in action This tutorial will walk you through how to: Add users View available accounts Grant groups access to roles and accounts Create custom roles Add users to groups Add users Adding users to New Relic is a separate process from creating access grants. In other words, you can do either procedure first. The key point is that if you're trying to give a user access to a role or an account that they don't yet have, they won't have that access until they're both a) in a group and b) that group has the correct access grant (access to a specific role on a specific account). There are two ways to add users: With the user management UI (described below) With automated user management, also known as SCIM provisioning: use of this method means that groups are defined in your identity provider and so you can't edit a user's group from the New Relic UI. To add users from the UI: From the top right of the New Relic UI, click the account dropdown, click Administration, and click User management. If you have multiple authentication domains, choose one from the authentication domain dropdown. Click Add user. Complete the process, including choosing user type and user group. Relevant tips: For how to bulk edit users' user type, see Edit user type. When choosing a group, you can choose either one of our default groups or a custom group that you've defined an access grant for. For more on creating access grants, keep reading. View available accounts When thinking about creating access grants for granting access to different accounts, it can help you to understand what accounts there are in your organization. To view the accounts in your organization: go to the account dropdown, click Administration, click Organizations and access, and then click Accounts. Grant access to accounts and roles Groups are used to group your users and manage what your users are able to do in New Relic: by creating an access grant, you assign a group access to a) a specific role on b) a specific account. By default, organizations on the New Relic One user model have two available groups: Admin and User. These default groups automatically have access to specific standard roles and are assigned to the account in which they were initially added. To view existing access grants: from the account dropdown, click Administration, and then click Organization and access. Even if you haven't created any custom access grants, you will see the default-created grants there that are present for our default groups. This is what you might see when you go to the Organization and access UI and view groups. The default available groups of Admin and User have access grants automatically created that grant users in those groups access to the roles associated with those groups and to the initial account those users were added in. To create a new access grant that gives a user group access to a role and an account: From the Organization and access UI, click Group access. If you don't see that UI, it may be because you're logged in with an original user model record, or because you don't have the ability to manage users. For more on such factors, see Factors affecting access. Choose one of the following: Existing: If there is already a group you want to add an access grant to, you can use this. For example, if you want to gives users in the default Admin or User group access to new accounts, you might choose this and then select the Admin Default or Admin User role. New: If you need to create a new group, choose this. You'll also need to choose the authentication domain that group is inside of (for an explanation of what \"Default\" means, see Default groups. If your organization strategy requires a good amount of restrictions over access to accounts and access to roles, you'll probably need to create a good amount of access grants. Next, under Access grant, you'll choose one of following: Account: Choose this to be able to select from the roles that are account-scoped. These are the roles that have to do with using and configuring our platform features (and not about organization and user management). Organization: Choose this to be able to select from the roles that are organization-scoped. These are the roles that govern organization- and user management. (Note that these users must also already belong to an account-scoped role. This is true for most users but if it's not, you may see a message that the user doesn't belong to an organization.) Select the Role you want to assign. Roles are organization-wide, so regardless of the authentication domain you're in, you have access to our standard roles and any custom roles you've created. For tips on selecting roles, see the tips after these instructions. Select the Account you want to add access to from the dropdown. If you don't see an account that you'd expect to see, this may be for a few reasons. One is that you yourself don't have the proper permissions for that account. Another is that that account is not actually in your organization. For more information, see Factors affecting access. If you are still having problems, talk to your account representative. If you want to continue adding more grants for that same group, select Add another at the bottom before clicking Add access. When you're done, if your users are already in the group you've added the grant to, they should have access within a few minutes (although for EU region New Relic accounts, this can take up to twenty minutes or so). If your users are not yet in that group (which would be true if you just created an access grant with a new group), you'll need to go to the User management UI and add one or more users to that group. Some tips for using this UI: If your users are managed via automated user management, you can't use the User management UI to add users to groups because your groups are imported from your identity provider. You will need to create access grants for those groups once they are in New Relic, though, to give those groups access. Note that if a user has the organization-scoped Organization manager and/or Authentication domain manager roles (which is true of users in our default Admin group) those users will always have those capabilities because those are organization-scoped abilities. This means that when you go to add those users to another account, you only have to add an account-scoped role, and not an organization-scoped role. In other words, once the users in a group have those organization-scoped roles, they will always have them for that organization unless removed. When selecting from amongst our standard roles, it's important to understand the difference between All product admin and Standard user. In short, All product admin is more popular a choice because it gives the ability to configure platform features. If you wanted to have your users be able to use platform features but not configure them, you'd choose Standard user. Create custom role When creating an access grant, you can use our standard roles, or you can create your own roles with unique sets of capabilities. To view existing roles: from the account dropdown, click Administration, then click Organization and access, and then click Roles. To create a custom role, click Add new custom role. Review the list of available capabilities and decide which ones your custom role needs. For more information about how roles and capabilities work, see Capabilities. Or check out this NerdByte video (4:07 minutes). Once you're finished creating a custom role, you will probably want to use it in a new access grant. Add users to groups In the user management UI, you can see your users and the groups they've been assigned to. Your users only have access to the access grants associated with the groups they've been added to. To view users and see their groups: from the account dropdown, click Administration, and click User management. If you don’t see that option, review the requirements. Groups reside within the boundaries of an authentication domain. If your organization has more than one authentication domain, the domain switcher in the top left will show which one you’re in. To add a user, click Add user. Complete the prompts in the UI, including choosing the user type and group. Any custom groups you’ve added should be available from the group dropdown. If the custom group you choose has been granted access to a role and an account, once you add the user to that group, that user will now have access. To edit a user’s group or other details: click on the user you want to edit and make changes.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 431.95624,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Tutorials on <em>user</em> <em>management</em> tasks: access grants, custom roles, <em>and</em> adding <em>users</em> (<em>New</em> <em>Relic</em> <em>One</em> <em>user</em> model)",
        "sections": "Tutorials on <em>user</em> <em>management</em> tasks: access grants, custom roles, <em>and</em> adding <em>users</em> (<em>New</em> <em>Relic</em> <em>One</em> <em>user</em> model)",
        "tags": "<em>New</em> <em>Relic</em> <em>One</em> <em>user</em> <em>management</em>",
        "body": " means that groups are defined in your identity provider and so you can&#x27;t edit a <em>user</em>&#x27;s group from the <em>New</em> <em>Relic</em> UI. To add users from the UI: From the top right of the <em>New</em> <em>Relic</em> UI, click the <em>account</em> dropdown, click Administration, and click <em>User</em> <em>management</em>. If you have multiple authentication domains"
      },
      "id": "603e7d67196a671e26a83dc5"
    },
    {
      "sections": [
        "Introduction to user management",
        "New pricing model",
        "User management docs"
      ],
      "title": "Introduction to user management",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "New Relic One user management"
      ],
      "external_id": "6c2f39333fa3c6931fe616669244cb44f183a167",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/new-relic-one-user-management/introduction-managing-users/",
      "published_at": "2021-12-19T15:02:53Z",
      "updated_at": "2021-11-24T13:09:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New pricing model This doc is for users on our New Relic One user model. Some important things to note before starting: The docs in this section are for managing users on the New Relic One user model. Learn about user models. Note that this is a separate topic from our two different pricing models. For managing users on our original user model, see Original users. User management docs Here are our main docs for managing these users: User model/structure: learn some basic aspects of our user model, such as what basic users and full platform users are, how groups (like Admin and User) work, and how roles and capabilities work. How to manage users: an overview of user management concepts, where to manage users in the UI, and some common user management tasks. Authentication domain settings: configure an authentication domain, which governs how your users are added to New Relic (manually versus SCIM provisioning), the authentication method they use (manual login versus SAML SSO), managing how basic users become full platform users, and user session settings. For an overview of SAML SSO and SCIM options, see Introduction to SAML and SCIM. A tutorial on how to create access grants, which is how you give users access to roles and accounts. Want to understand how user count affects billing? See User-related billing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 431.77606,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>user</em> <em>management</em>",
        "sections": "Introduction to <em>user</em> <em>management</em>",
        "tags": "<em>New</em> <em>Relic</em> <em>One</em> <em>user</em> <em>management</em>",
        "body": "<em>New</em> pricing model This doc is for users on our <em>New</em> <em>Relic</em> <em>One</em> <em>user</em> model. Some important things to note before starting: The docs in this section are for managing users on the <em>New</em> <em>Relic</em> <em>One</em> <em>user</em> model. Learn about <em>user</em> models. Note that this is a separate topic from our two different pricing models"
      },
      "id": "6043f3c4196a67a215960f3c"
    },
    {
      "sections": [
        "SCIM API tutorial",
        "Requirements",
        "Overview",
        "Configure your authentication domain for SCIM",
        "Tip",
        "Create users and user groups in your system",
        "Connect to the SCIM API",
        "401 Unauthorized",
        "Create users in your authentication domain",
        "Important",
        "Example responses",
        "201 Created",
        "400 Bad Request",
        "409 Conflict",
        "Create groups in your authentication domain",
        "View users and groups in your authentication domain",
        "200 OK",
        "Update a user's attributes",
        "Update a group's members",
        "Delete users and groups",
        "Next steps",
        "Optional: Manage user type"
      ],
      "title": "SCIM API tutorial",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Automated user management",
        "New Relic One user management"
      ],
      "external_id": "ef29f5444770fc05d762fd27c4872a92660ec681",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/automated-user-management/tutorial-manage-users-groups-scim/",
      "published_at": "2021-12-19T15:40:05Z",
      "updated_at": "2021-11-24T11:08:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This tutorial will walk you through some common procedures for managing users on the New Relic One user model using the SCIM API. The SCIM API allows you to view, create, update, and delete users and groups programmatically, outside of the User management UI. Requirements Before using this tutorial, we recommend you read: The requirements for automated user management and using the SCIM API. Important user management concepts The primary SCIM API reference Other related resources: Some SCIM 2.0 RFC documents from the Internet Engineering Task Force that are most relevant: RFC 7643 - SCIM Core Resources and Extensions, RFC 7643 - JSON Representation, and RFC 7644 - SCIM Protocol. Overview This tutorial shows you how to accomplish some of the most common tasks needed for adding users to New Relic from an identity provider service and managing them from there. It is meant to supplement our primary SCIM API resource. Note that using automated user management means that your groups of users are imported into New Relic. This means that you can't use our user management UI to add users to groups. The groups are created and managed from your identity provider side. Once you're done with getting your user groups into New Relic, you must use our Organization and access UI to create access grants, which give your groups access to specific roles and/or accounts. For more information, see user management concepts. Configure your authentication domain for SCIM Before you can use the SCIM API, you must first enable SCIM for your authentication domain. Note that the API access token is displayed only once after you save the configuration, so save it somewhere safe for later user. Tip If you need to view a bearer token later, the only way to do this is to generate a new one, and that will invalidate the old one and any integrations using the old token. Create users and user groups in your system The SCIM API is typically used by scripts for importing users and groups into New Relic from a database or a third-party identity provider that doesn't have pre-configured configs for New Relic. If you want to use the SCIM API custom application or for ad-hoc requests, proceed to learn how to connect to the SCIM API. Connect to the SCIM API The SCIM API is available at https://scim-provisioning.service.newrelic.com/scim/v2 and this URL is viewable in the authentication domain settings page. To access the SCIM API, your client must include a bearer token with each request. The token is displayed after saving your Authentication Domain configuration. If you're using a third-party identity provider, configure it to use Bearer token authorization and plug in your API access token. Refer to your identity provider's documentation for help configuring this. Once configured, you're all set to import users and groups. Rather than reading the entire SCIM protocol RFCs, there are three specific sections you may find valuable: See RFC 7643 - SCIM Core Resources and Extensions and RFC 7643 - JSON Representation for the specifics. Refer to RFC 7644 - SCIM Protocol for more information about the protocol used in this tutorial. For all requests to the SCIM API, you must provide the bearer token in an Authorization header. Here's an example with curl: curl -X 'GET' -H 'Accept: application/json' -H \"Authorization: Bearer $TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Users' Copy Any request in the rest of this tutorial will receive a 401 Unauthorized response if the API access token is missing or invalid. Example response: 401 Unauthorized { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"detail\": \"invalid authentication token\", \"status\": \"401\" } Copy Create users in your authentication domain You can use the SCIM API to send a POST request to /scim/v2/Users to create a user. The following user attributes are required: userName This identifier must be unique within an authentication domain. Use the user's email address. emails Same as userName. The email address of the user. (Despite it being called emails, for this procedure enter only one.) active Boolean indicating whether or not the user should be active or inactive within New Relic. We recommend providing the following attributes for the best user experience: name.givenName The user's first or given name. name.familyName The user's last or family name. timezone The user's timezone in IANA Time Zone database format. curl -X 'POST' -H 'Content-Type: application/json' -H \"Authorization: Bearer $TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Users' --data-binary @- <<EOF { \"schemas\": [\"urn:ietf:params:scim:schemas:core:2.0:User\"], \"userName\": \"bjensen@example.com\", \"name\": { \"familyName\": \"Jensen\", \"givenName\": \"Barbara\" }, \"emails\": [ { \"primary\": true, \"value\": \"bjensen@example.com\" } ], \"active\": true, \"timezone\": \"America/Los_Angeles\" } EOF Copy Important Take note of the returned user id. To update a user in the future you'll need to supply the same ID with the request. Example responses 201 Created { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\", ], \"id\": \"5a1d580f-323c-450c-8c62-479b5c9085d6\", \"externalId\": null, \"userName\": \"bjensen@example.com\", \"name\": { \"familyName\": \"Jensen\", \"givenName\": \"Barbara\" }, \"emails\": [ { \"value\": \"bjensen@example.com\", \"primary\": true } ], \"timezone\": \"America/Los_Angeles\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2021-07-20T21:32:58.167Z\", \"lastModified\": \"2021-07-20T21:32:58.167Z\" }, \"groups\": [] } Copy 400 Bad Request { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"scimType\": \"invalidValue\", \"detail\": \"Validation failed: Username can't be blank\", \"status\": \"400\" } Copy 409 Conflict { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"scimType\": \"uniqueness\", \"detail\": \"Resource already exists\", \"status\": \"409\" } Copy Create groups in your authentication domain You can use the SCIM API to send a POST request to /scim/v2/Groups to create a group. The only group attribute required is: displayName The group name. curl -X 'POST' -H 'Content-Type: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Groups' --data-binary @- <<EOF { \"schemas\": [\"urn:ietf:params:scim:schemas:core:2.0:Group\"], \"displayName\": \"Example Group\" } EOF Copy Important Take note of the returned group id. To update a group or its members in the future you will need to supply the same ID with the request. Example responses 201 Created { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"df2b9a04-0426-4a3e-bf5f-54d5341f4e5b\", \"displayName\": \"Example Group\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2021-07-20T22:41:50.414Z\", \"lastModified\": \"2021-07-20T22:41:50.414Z\" }, \"members\": [] } Copy 400 Bad Request { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"scimType\": \"invalidValue\", \"detail\": \"Validation failed: Display name can't be blank\", \"status\": \"400\" } Copy 409 Conflict { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"scimType\": \"uniqueness\", \"detail\": \"Resource already exists\", \"status\": \"409\" } Copy View users and groups in your authentication domain After you've created some users and groups you'll see them in the User management UI. You can also retrieve them from the SCIM API. In this tutorial, you'll be searching for specific users and groups, but that's not the only way to view users and groups. Refer to the SCIM API reference and RFC 7644 for all the available query options. To retrieve a user by email, send a GET request to /scim/v2/Users with a filter query parameter. The filter parameter must be URL encoded. curl -X 'GET' -H 'Accept: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Users' --get --data-urlencode 'filter=userName eq \"bjensen@example.com\"' Copy Example response: 200 OK { \"totalResults\": 1, \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:ListResponse\" ], \"Resources\": [ { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\", ], \"id\": \"5a1d580f-323c-450c-8c62-479b5c9085d6\", \"externalId\": null, \"userName\": \"bjensen@example.com\", \"name\": { \"familyName\": \"Jensen\", \"givenName\": \"Barbara\" }, \"emails\": [ { \"value\": \"bjensen@example.com\", \"primary\": true } ], \"timezone\": \"America/Los_Angeles\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2021-07-20T21:32:58.167Z\", \"lastModified\": \"2021-07-20T21:32:58.167Z\" }, \"groups\": [] } ] } Copy Similarly, send a GET request to /scim/v2/Groups with a filter query parameter to retrieve a group by name. curl -X 'GET' -H 'Accept: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Groups' --get --data-urlencode 'filter=displayName eq \"Example Group\"' Copy Example response: 200 OK { \"totalResults\": 1, \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:ListResponse\" ], \"Resources\": [ { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"df2b9a04-0426-4a3e-bf5f-54d5341f4e5b\", \"displayName\": \"Example Group\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2021-07-20T22:41:50.414Z\", \"lastModified\": \"2021-07-20T22:41:50.414Z\" }, \"members\": [] } ] } Copy Update a user's attributes The SCIM API supports both PUT and PATCH methods for updating users. Refer to the SCIM API supported actions and RFC 7644 for details on using PATCH. This tutorial demonstrates updating a user's attributes with the PUT method. New Relic does not require all user attributes be included in the request body, only the attributes you want to update are necessary. Send a PUT request to /scim/v2/Users/${ID} to update the user. curl -X 'PUT' -H 'Content-Type: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Users/5a1d580f-323c-450c-8c62-479b5c9085d6' --data-binary @- <<EOF { \"schemas\": [\"urn:ietf:params:scim:schemas:core:2.0:User\"], \"timezone\": \"America/Chicago\" } EOF Copy Example responses 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\", ], \"id\": \"5a1d580f-323c-450c-8c62-479b5c9085d6\", \"externalId\": null, \"userName\": \"bjensen@example.com\", \"name\": { \"familyName\": \"Jensen\", \"givenName\": \"Barbara\" }, \"emails\": [ { \"value\": \"bjensen@example.com\", \"primary\": true } ], \"timezone\": \"America/Chicago\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2021-07-20T21:32:58.167Z\", \"lastModified\": \"2021-07-21T02:12:05.348Z\" }, \"groups\": [] } Copy 409 Conflict { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"scimType\": \"uniqueness\", \"detail\": \"Resource already exists\", \"status\": \"409\" } Copy Update a group's members The SCIM API supports both PUT and PATCH methods for updating groups. This tutorial will show how to update a group's members with the PATCH method. Refer to the SCIM API supported actions and RFC 7644 for details on using PUT. PATCH is convenient for adding or removing group members without needing to specify the full member list in the request. To add a user to a group, use the following operation parameters: op set to Add path set to members value set to a list of {\"value\": \"${USER_ID}\"} with each user ID to add to the group Send a PATCH request to /scim/v2/Groups/${ID} to update group members. curl -X 'PATCH' -H 'Content-Type: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Groups/df2b9a04-0426-4a3e-bf5f-54d5341f4e5b' --data-binary @- <<EOF { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:PatchOp\" ], \"Operations\": [{ \"op\": \"Add\", \"path\": \"members\", \"value\": [{ \"value\": \"5a1d580f-323c-450c-8c62-479b5c9085d6\" }] }] } EOF Copy Example response: 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"df2b9a04-0426-4a3e-bf5f-54d5341f4e5b\", \"displayName\": \"Example Group\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2021-07-20T22:41:50.414Z\", \"lastModified\": \"2021-07-20T22:41:50.414Z\" }, \"members\": [ { \"type\": \"User\", \"value\": \"5a1d580f-323c-450c-8c62-479b5c9085d6\" } ] } Copy To remove a user from a group, use the following operation parameters: op set to Remove path set to members value set to a list of {\"value\": \"${USER_ID}\"} with each user ID to remove from the group curl -X 'PATCH' -H 'Accept: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Groups/df2b9a04-0426-4a3e-bf5f-54d5341f4e5b' --data-binary @- <<EOF { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:PatchOp\" ], \"Operations\": [{ \"op\": \"Remove\", \"path\": \"members\", \"value\": [{ \"value\": \"5a1d580f-323c-450c-8c62-479b5c9085d6\" }] }] } EOF Copy Example response: 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"df2b9a04-0426-4a3e-bf5f-54d5341f4e5b\", \"displayName\": \"Example Group\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2021-07-20T22:41:50.414Z\", \"lastModified\": \"2021-07-20T22:41:50.414Z\" }, \"members\": [] } Copy Delete users and groups To remove a user from an authentication domain, send a DELETE request to /scim/v2/Users/${ID}. curl -X 'DELETE' -H 'Accept: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Users/d0f4d8e3-5413-4894-a8f9-de709994e18c' Copy Example response: 204 No Content Copy Similarly, to remove a group from your authentication domain, send a DELETE request to /scim/v2/Groups/${ID}. curl -X 'DELETE' -H 'Accept: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Groups/df2b9a04-0426-4a3e-bf5f-54d5341f4e5b' Copy Example response: 204 No Content Copy Next steps Once your integration is complete, potential next steps include: Your New Relic users will by default start out as basic users and you have the option to change some of them to full platform users. To do this, you can use the User management UI or use the SCIM API. Set up SAML SSO. Once your user groups are in New Relic, you'll need to assign access grants, which are what give your users access to specific roles and specific accounts. Learn more about access grants. Optional: Manage user type Once your SCIM API integration is complete, all users brought into New Relic start out as basic users. You can use our default method for managing user type, which is using the User management UI. Optionally, you can use our SCIM API instead. To do this, you can set update your authentication domain configuration to Delegate control of user type to your identity provider or custom application. The user's type attribute is defined in the custom schema urn:ietf:params:scim:schemas:extension:newrelic:2.0:User. Include this schema and the nrUserType string attribute in your create or update request to set a user's type. Valid values for nrUserType include: Full User (for full platform user) Basic User To create a new Basic User send a POST request /scim/v2/Users and include the custom New Relic schema extension: curl -X 'POST' -H 'Content-Type: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Users' --data-binary @- <<EOF { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\", \"urn:ietf:params:scim:schemas:extension:newrelic:2.0:User\" ], \"userName\": \"jbenson@example.com\", \"name\": { \"givenName\": \"James\", \"familyName\": \"Benson\" }, \"emails\": [{ \"primary\": true, \"value\": \"jbenson@example.com\", \"type\": \"work\" }], \"active\": true, \"timezone\": \"America/Chicago\", \"urn:ietf:params:scim:schemas:extension:newrelic:2.0:User\": { \"nrUserType\": \"Basic User\" } } EOF Copy Example responses 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\", \"urn:ietf:params:scim:schemas:extension:newrelic:2.0:User\" ], \"id\": \"8ab6c464-983c-4bb4-9507-720f28763a43\", \"externalId\": null, \"userName\": \"jbenson@example.com\", \"name\": { \"familyName\": \"Benson\", \"givenName\": \"James\" }, \"emails\": [ { \"value\": \"jbenson@example.com\", \"primary\": true } ], \"timezone\": \"America/Chicago\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2021-07-21T19:53:33.470Z\", \"lastModified\": \"2021-07-21T19:53:33.470Z\" }, \"groups\": [], \"urn:ietf:params:scim:schemas:extension:newrelic:2.0:User\": { \"nrUserType\": \"Basic User\" } } Copy 409 Conflict { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"scimType\": \"uniqueness\", \"detail\": \"Resource already exists\", \"status\": \"409\" } Copy 400 Bad Request { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"scimType\": \"invalidValue\", \"detail\": \"Validation failed: Username can't be blank\", \"status\": \"400\" } Copy To update a user's type, send a PUT request scim/v2/Users/${ID} and include the custom New Relic schema extension: curl -X 'PUT' -H 'Content-Type: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Users' --data-binary @- <<EOF { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\", \"urn:ietf:params:scim:schemas:extension:newrelic:2.0:User\" ], \"urn:ietf:params:scim:schemas:extension:newrelic:2.0:User\": { \"nrUserType\": \"Full User\" } } EOF Copy Example responses 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\", \"urn:ietf:params:scim:schemas:extension:newrelic:2.0:User\" ], \"id\": \"8ab6c464-983c-4bb4-9507-720f28763a43\", \"externalId\": null, \"userName\": \"jbenson@example.com\", \"name\": { \"familyName\": \"Benson\", \"givenName\": \"James\" }, \"emails\": [ { \"value\": \"jbenson@example.com\", \"primary\": true } ], \"timezone\": \"America/Chicago\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2021-07-21T19:53:33.470Z\", \"lastModified\": \"2021-07-21T20:15:56.718Z\" }, \"groups\": [], \"urn:ietf:params:scim:schemas:extension:newrelic:2.0:User\": { \"nrUserType\": \"Full User\" } } Copy 400 Bad Request { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"scimType\": \"invalidValue\", \"detail\": \"Validation failed: invalid nrUserType value provided\", \"status\": \"400\" } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 362.11212,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Create <em>users</em> <em>and</em> <em>user</em> groups in your system",
        "tags": "<em>New</em> <em>Relic</em> <em>One</em> <em>user</em> <em>management</em>",
        "body": "This tutorial will walk you through some common procedures for managing users on the <em>New</em> <em>Relic</em> <em>One</em> <em>user</em> model using the SCIM API. The SCIM API allows you to view, create, update, and delete users and groups programmatically, outside of the <em>User</em> <em>management</em> UI. Requirements Before using this tutorial"
      },
      "id": "611fd560196a6791717ab5f9"
    }
  ],
  "/docs/accounts/accounts-billing/new-relic-one-user-management/authentication-domains-saml-sso-scim-more": [
    {
      "sections": [
        "Introduction to user management",
        "New pricing model",
        "User management docs"
      ],
      "title": "Introduction to user management",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "New Relic One user management"
      ],
      "external_id": "6c2f39333fa3c6931fe616669244cb44f183a167",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/new-relic-one-user-management/introduction-managing-users/",
      "published_at": "2021-12-19T15:02:53Z",
      "updated_at": "2021-11-24T13:09:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New pricing model This doc is for users on our New Relic One user model. Some important things to note before starting: The docs in this section are for managing users on the New Relic One user model. Learn about user models. Note that this is a separate topic from our two different pricing models. For managing users on our original user model, see Original users. User management docs Here are our main docs for managing these users: User model/structure: learn some basic aspects of our user model, such as what basic users and full platform users are, how groups (like Admin and User) work, and how roles and capabilities work. How to manage users: an overview of user management concepts, where to manage users in the UI, and some common user management tasks. Authentication domain settings: configure an authentication domain, which governs how your users are added to New Relic (manually versus SCIM provisioning), the authentication method they use (manual login versus SAML SSO), managing how basic users become full platform users, and user session settings. For an overview of SAML SSO and SCIM options, see Introduction to SAML and SCIM. A tutorial on how to create access grants, which is how you give users access to roles and accounts. Want to understand how user count affects billing? See User-related billing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 324.26364,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "Accounts <em>and</em> billing",
        "body": " versus <em>SCIM</em> provisioning), the <em>authentication</em> method they use (manual login versus <em>SAML</em> <em>SSO</em>), managing how basic users become full platform users, and user session <em>settings</em>. For an overview of <em>SAML</em> <em>SSO</em> and <em>SCIM</em> options, see Introduction to <em>SAML</em> and <em>SCIM</em>. A tutorial on how to create access grants, which is how you give users access to roles and accounts. Want to understand how user count affects billing? See User-related billing."
      },
      "id": "6043f3c4196a67a215960f3c"
    },
    {
      "sections": [
        "Azure AD SCIM/SSO application configuration",
        "Requirements",
        "Step 1. Create authentication domain and enable SCIM",
        "Step 2. Set up Azure's New Relic app",
        "Step 3. Configure connection",
        "Step 4. Configure provisioning rules",
        "Tip",
        "Step 5. Set your users' user type",
        "Step 6. Assign access grants",
        "Step 7. Configure SAML SSO"
      ],
      "title": "Azure AD SCIM/SSO application configuration",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Automated user management"
      ],
      "external_id": "d6e7f7e95daa833451159a3db4e2c4257270b5e9",
      "image": "https://docs.newrelic.com/static/0a9a32fd5041e6e2ea37cc5f032b6910/8c557/Azure_AD_Provisioning_Attribute_Mapping_2_0.png",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/automated-user-management/azure-ad-scimsso-application-configuration/",
      "published_at": "2021-12-19T15:39:27Z",
      "updated_at": "2021-11-24T11:24:35Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our automated user management (AUM) allows allows you to import and configure your New Relic users from your identity provider via SCIM. This guide provides Azure AD-specific details on how to configure the New Relic Azure AD SCIM/SSO application. Requirements Before using this guide, read the requirements and procedure overview. Note that these instructions require going back and forth between your identity provider and New Relic. Step 1. Create authentication domain and enable SCIM To get to the New Relic authentication domain UI: From one.newrelic.com, click the account dropdown, click Organization and access, and then click Authentication domains. If you don't already have one, create a new authentication domain for your SCIM-provisioned users by clicking + Add new. For that authentication domain, under Source of users, select SCIM. Copy and save the API token for later use. Note that this will be shown only once. Step 2. Set up Azure's New Relic app Next, you'll set up Azure's New Relic SAML/SCIM app. Azure AD provides an application gallery, which includes various integrations for Azure AD, including the ones that New Relic offers. To set this up: Go to the Azure Active Directory admin center, and sign in if necessary. aad.portal.azure.com/ Click on All services in the left hand menu. In the main pane, click on Enterprise applications. Click on +New Application. Find our SCIM/SSO application by entering New Relic in the name search box, and click on the application New Relic by organization (not New Relic by account). Click on Add. Step 3. Configure connection Configure the New Relic SCIM/SSO application to automatically provision your users to New Relic. From the New Relic SCIM/SSO application page, click on the Provisioning link in the sidebar. In the main pane, click on Get started. In the Provisioning Mode pick-list, choose Automatic. In New Relic's authentication domain UI, set up a new domain with SCIM enabled. In Azure AD's New Relic SCIM/SSO app, in the Admin credentials section, fill out the Tenant URL and Secret token fields with the values provided in New Relic's authentication domain UI. To verify you can connect to New Relic, click Test Connection. When you see a message indicating verification success, click Save. The New Relic SCIM/SSO application can now connect with New Relic. Continue with the following section to configure the provisioning rules. Step 4. Configure provisioning rules Initially, nothing is configured to be sent to New Relic. You must configure Azure AD to send changes for user creation, updates, and deactivation. Go to the Provisioning page and complete the following: Expand the Mappings section. Click Provision Azure Active Directory Users. Verify the Target Object Actions Create Update and Delete checkboxes are all checked. Verify the Attribute Mappings look correct for your environment. Each of the New Relic attributes shown in the list must receive a value. Tip Ensure that the Azure Active Directory attributes shown in the list on the left are good sources for the information to send to New Relic. In particular, not all environments set the mail attribute. If your environment does not set the mail attribute, userPrincipalName could be a good alternative. Leave the switch for Enabled set to Off until you're done with the user and group configuration in the next section. Once all configuration is ready, return to this page and set the switch to On. Click Save. Here's an example of a filled-in attribute mapping page with the default values. Your values may be configured differently depending on your situation. After saving the provisioning rules, the New Relic SCIM/SSO application is ready to provision any changes made to users assigned to the application. Continue with the following section to assign users and groups to the New Relic SCIM/SSO application. Step 5. Set your users' user type When your users are provisioned in New Relic, you're able to see them in the User management UI. If you're adding users to New Relic via SCIM but not managing their user type via SCIM, they start out as basic users. To convert users to full platform users, you have two options: Use the User management UI to edit users. Configure the Azure app to manage user type. Step 6. Assign access grants Once these steps are completed, you should be able to see your users in New Relic by going to the User management UI. Now that your users are present in New Relic, you must grant them access to specific roles on specific accounts. If this is not done, your users don't yet have access to New Relic. To learn how to do this, see: How access grants work The user management tutorial Step 7. Configure SAML SSO To enable SAML SSO for your users, see the SAML instructions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 317.87592,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Azure AD <em>SCIM</em>&#x2F;<em>SSO</em> application configuration",
        "sections": "Step 1. Create <em>authentication</em> <em>domain</em> <em>and</em> enable <em>SCIM</em>",
        "tags": "Accounts <em>and</em> billing",
        "body": ", click on Get started. In the Provisioning Mode pick-list, choose Automatic. In New Relic&#x27;s <em>authentication</em> <em>domain</em> UI, <em>set</em> up a new <em>domain</em> with <em>SCIM</em> enabled. In Azure AD&#x27;s New Relic <em>SCIM</em>&#x2F;<em>SSO</em> app, in the Admin credentials section, fill out the Tenant URL and Secret token fields with the values"
      },
      "id": "6043f5c964441fcfb0378ef3"
    },
    {
      "sections": [
        "OneLogin SCIM/SSO application configuration",
        "Requirements",
        "Step 1. Create authentication domain and enable SCIM",
        "Step 2. Set up OneLogin's New Relic app",
        "Step 3. Configure SCIM/SSO application",
        "Fill in the configuration form",
        "Fill in the rules form",
        "A rule that only uses actions",
        "Fill in the provisioning form",
        "Tip",
        "Fill in the Parameters form",
        "Save your changes",
        "Step 4. Assign users",
        "Step 5. Set your users' user type",
        "Step 6. Assign access grants",
        "Step 7. Configure SAML SSO"
      ],
      "title": "OneLogin SCIM/SSO application configuration",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Automated user management"
      ],
      "external_id": "d298162c055490c99117f564c3ea9c2ea5dfd8d1",
      "image": "https://docs.newrelic.com/static/8f585557ea58f70d94a746e6439bf1ad/c1b63/onelogin_rules_conditions_actions.png",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/automated-user-management/onelogin-scimsso-application-configuration/",
      "published_at": "2021-12-19T15:39:45Z",
      "updated_at": "2021-11-24T11:24:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our automated user management (AUM) allows allows you to import and configure your New Relic users from your identity provider via SCIM. This guide provides OneLogin-specific details on how to configure the New Relic OneLogin SCIM/SSO application. Requirements Before using this guide, read our AUM requirements. Note that these instructions require going back and forth between your identity provider and New Relic. Step 1. Create authentication domain and enable SCIM To get to the New Relic authentication domain UI: From one.newrelic.com, click the account dropdown, click Organization and access, and then click Authentication domains. If you don't already have one, create a new authentication domain for your SCIM-provisioned users by clicking + Add new. For that authentication domain, under Source of users, select SCIM. Copy and save the API token for later use. Note that this will be shown only once. Step 2. Set up OneLogin's New Relic app Next, you'll be setting up OneLogin's New Relic SAML/SCIM app. To set this up: Go to the OneLogin web site and sign in with an account that has administrator permissions. From the OneLogin home page, click on Administration. From the OneLogin Administration page, choose the Applications menu. From the OneLogin Applications page, click on Add App. In the search field on the OneLogin Find Applications page, enter \"New Relic by organization\" (not \"New Relic by account\") and then click on the application when it shows in the search results. From the Add New Relic by Organization page, click on Save. Step 3. Configure SCIM/SSO application Configuration for the New Relic SCIM/SSO application is split across several forms. This section describes the different forms that need to be configured. From the New Relic by organization application page, fill in the following forms: Fill in the configuration form In the left pane, select Configuration and complete the following: Get the authentication domain ID (top of New Relic's authentication domain UI and SCIM bearer token (in authentication domain UI as \"SAML 2.0 endpoint) and input them into the appropriate fields in the OneLogin app. Leave the API Connection disabled until all the configuration described in the following sections is completed. After completing all the configuration, enable the connection. Fill in the rules form Configure the user groups to send to New Relic using rules. OneLogin provides this documentation which describes how to use rules to provision groups for users. Decide what type of groups to send along with your users to New Relic. If your organization is using Active Directory or LDAP, you might choose to use security groups to define your users capabilities at New Relic. Another reasonable group choice is OneLogin role. On the New Relic side, your user's groups define their capabilities. The groups that are sent with users will be mapped to New Relic capability groups. Note that at the moment, there is no way to delete a group from the OneLogin side. This is a known limitation from OneLogin. Removing or changing rules does not delete groups already sent to New Relic. If you wish to no longer use a group, removing all the users from the group will prevent it from being used at New Relic. A rule that only uses actions Here's an example rule configuration does not use any conditions. The conditions are left empty to avoid applying any filtering logic to the users. All users will be sent in this example. If you want to send only a subset of users, you need to specify conditions to select the subset. The actions describe where to retrieve the value for the group name and how to parse the value. In this example, we retrieve the group name from the OneLogin role field. The OneLogin role field only has a single value, but sometimes the source for the group name contains other fields besides group name. In other words, some sources give you a list of fields and values and only one of those fields has the value you want to use. In this case, you can insert a regular expression in with value that matches field to find and extract the value for the group name. This example uses the entire value of the For each field for the group name. Fill in the provisioning form In the left pane, select Provisioning and complete the following: Check Enable provisioning. Under Require admin approval before this action is performed, uncheck these options: Create user Delete user Update user Tip If you do not uncheck these options, SCIM provisioning requests will not be sent until an administrator approves them. Set When users are deleted in OneLogin, or the user's app access is removed, perform the below action to Delete. Set When user accounts are suspended in OneLogin, perform the following action to Suspend. Fill in the Parameters form In the left pane, select Parameters and complete the following: Click Groups field. Check Include in User Provisioning. Click Save. Save your changes After you complete the above forms, click Save. Then, return to the Configuration form and enable the API connection. Step 4. Assign users After New Relic SCIM/SSO application configuration is finished and New Relic side configuration is finished, you can begin to assign users to the application. Assign the New Relic SCIM/SSO application to a user: Go to the OneLogin web site and sign in with an account that has administrator permissions. From the OneLogin home page, click Administration. From the OneLogin Administration page, choose the Users menu Users item. From the OneLogin Users page, click the user you want to assign the application to. From the user's page, click Applications. From the user's application page, click the plus sign and select the \"New Relic by Organization\" application. Important: Updating users' time zones is important, as charts and other user assets display times. Default is UMT. From the Edit New Relic by Organization login for user page, enter the user's time zone in IANA Time Zone database format (also known as the Olson time zone database format) and click Save. If you're using Roles to define your New Relic capability groups, from the user's application page, click the proper role(s) for the user and then click Save User. Step 5. Set your users' user type When your users are provisioned in New Relic, you're able to see them in the User management UI. If you're adding users to New Relic via SCIM but not managing their user type via SCIM, they start out as basic users. To convert users to full platform users, you have two options: Use the User management UI to edit users. Configure the OneLogin app to manage user type. Step 6. Assign access grants Once these steps are completed, you should be able to see your users in New Relic by going to the User management UI. Now that your users are present in New Relic, you must grant them access to specific roles on specific accounts. If this is not done, your users don't yet have access to New Relic. To learn how to do this, see: How access grants work The user management tutorial Step 7. Configure SAML SSO To enable SAML SSO for your users, see the SAML instructions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 298.2743,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "OneLogin <em>SCIM</em>&#x2F;<em>SSO</em> application configuration",
        "sections": "Step 1. Create <em>authentication</em> <em>domain</em> <em>and</em> enable <em>SCIM</em>",
        "tags": "Accounts <em>and</em> billing",
        "body": " page, fill in the following forms: Fill in the configuration form In the left pane, select Configuration and complete the following: Get the <em>authentication</em> <em>domain</em> ID (top of New Relic&#x27;s <em>authentication</em> <em>domain</em> UI and <em>SCIM</em> bearer token (in <em>authentication</em> <em>domain</em> UI as &quot;<em>SAML</em> 2.0 endpoint) and input them"
      },
      "id": "6043f34228ccbccafb2c606a"
    }
  ],
  "/docs/accounts/accounts-billing/new-relic-one-user-management/introduction-managing-users": [
    {
      "sections": [
        "How to manage users",
        "Important",
        "Requirements",
        "Manage users in the UI",
        "Overview of user management concepts",
        "Common user management tasks",
        "Add, edit, and delete users",
        "Edit user type (basic and full platform)",
        "Give users access to accounts and roles (access grants)",
        "Create new custom groups and roles",
        "Set up SAML SSO and/or SCIM provisioning",
        "Grant users ability to manage other users",
        "Control how basic users upgrade to full platform users",
        "Track changes",
        "User management terms and definitions"
      ],
      "title": "How to manage users",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "New Relic One user management"
      ],
      "external_id": "9e37836740ce56302734d4af636bdbe087b4dbc3",
      "image": "https://docs.newrelic.com/static/a78dad5ff794da5deaaf3b514e334ea7/c1b63/new-relic-one-user-mgmt.png",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/new-relic-one-user-management/add-manage-users-groups-roles/",
      "published_at": "2021-12-19T15:22:19Z",
      "updated_at": "2021-11-24T14:23:16Z",
      "document_type": "page",
      "popularity": 1,
      "body": "For users on our New Relic One user model, we provide various user management features, including the ability to: Use role based access control (RBAC) to assign default or custom roles to user groups Create custom user groups Grant user groups access to specific roles and accounts Important This doc applies to users on the New Relic One user model. For managing users on our original user model, see Original users. Not sure which user model you're on? See User models. Requirements To see if you can access these user management features, go to the user management UI and see what you have access to. Access requirements: These features allow managing of users on the New Relic One user model. To learn more, see User models. To avoid configuration conflicts, ensure that only one user is managing users at a time. Simultaneous configuration by more than one user may result in errors and unexpected results. Most capabilities require the Authentication domain manager role and some require the Organization manager role. For details, see Standard roles. Pricing edition requirements: To manage user groups, roles, and access grants: Pro or Enterprise edition is required. To import users from an identity provider: Enterprise is required. A New Relic user can have a maximum of either three concurrent active sessions, or three unique IP addresses in use at any given time. Manage users in the UI For users on the New Relic One user model, to find your user management options: From the account dropdown, select Administration. There are two user management UI pages there: User management: Use this to add users, update user type (basic versus full platform) and other information, and approve user type upgrade requests. Organization and access: Use this page to create and manage groups, roles, and access grants, and to set up SAML SSO and SCIM provisioning. Want to see videos of the user management UI in action? See our user management videos. Important If you can't see these UI pages, it may be because you're on our original user model or because you don't have the required user management role. Overview of user management concepts If your organization has Pro or Enterprise edition, you can create and configure access grants. An access grant gives a group of users access to a specific role on a specific account. Here's a diagram showing how access grants work and how they relate to the broader organization: A diagram explaining the concept of how access grants give a user group access to a) a role on b) a specific account. (Note that this applies to users on our New Relic One user model (and not our original user model).) When your New Relic organization is created, it starts out with some default access grants for our default User or Admin groups. Each of these groups is assigned one or more roles and granted access to the primary (first created) account: A view of the Organization and access UI, showing the default access grants associated with our default groups. If you have a relatively flat organizational structure, and are okay with all or many of your users having wide administrative access and access to all accounts, you'll probably only need at most a few access grants. For example, you might decide to add new access grants to the existing default Admin or User groups, giving those roles access to other accounts. Or, if you need more granular definition over roles and permissions, you'd create access grants that define new groups that have access to specific roles (either our standard roles or custom-defined roles). For a tutorial on how to create access grants and create custom roles, see the User management tutorial. For other examples of some common user management tasks, see Example tasks. To see the UI in action, see our user management videos. Some tips on setting up access grants: It may help to first plan out how your access grants will be organized. How many accounts will you have? What user groups will get access to which roles and which accounts? Will you use our default groups and roles or create your own custom groups and roles? If you've used automated user management to provision users via SCIM, you will have to create access grants to give those users access. A common configuration for organizations with many accounts (roughly 20 or more) is setting up groups with the more organization-scoped roles (Organization manager, Authentication domain manager, and Billing user) on the primary account, and then on other accounts, setting up groups with the more product-scoped roles (like All product admin, Standard user, or custom roles). Common user management tasks In the Organization and access UI, you can create access grants, custom groups, custom roles, and configure an authentication domain. Here are some example user management procedures: Add, edit, and delete users See the user management tutorial. Edit user type (basic and full platform) Note that there are limits around how many times full platform users can be changed to basic users. Important If you're using automated user management to provision and manage your users, you have other options for managing user type. To change the user type of one or more users: On the User management page, click the checkboxes for the users whose user type you want to edit. Once you start selecting users, an option will appear for Edit type. You can also edit the user type and group of a specific user by clicking on that user. Give users access to accounts and roles (access grants) See our user management tutorial. Create new custom groups and roles See the user management tutorial. Set up SAML SSO and/or SCIM provisioning See Get started with SAML SSO or SCIM. Grant users ability to manage other users To grant your users the ability to manage other users, you'll need to add users to a group that has the Authentication domain manager and Organization manager role. You have two options: From the User management UI, you can add a user to the default Admin group, which includes both of those roles. OR You can create a custom group and assign it these roles. For a tutorial on creating new access grants, groups, and roles, see the user management tutorial. Control how basic users upgrade to full platform users See the authentication domain settings. Track changes To see an audit log of changes to your account, including user management actions, you can query the NrAuditEvent. User management terms and definitions For an explanation of how user access to accounts and roles works, see User management concepts explained. Here are some definitions for the terms we use there: A New Relic organization is the representation of your organization, containing all your accounts, users, and data. For more information, see Organization and account structure. A capability is an ability to use or edit a specific, granular New Relic feature. Examples of capabilities: The ability to modify APM settings The ability to delete alert conditions A role is a set of capabilities. It is what gives a user their permissions. Our default standard roles have various capability sets, and you can create custom roles that have a custom set of capabilities. See some specific New Relic capabilities. A user group has one or more roles associated with it. You assign your users to a group. We have default user groups (Admin and User), and you can make your own groups. An access grant is what grants a user group access to roles and to specific New Relic accounts. An access grant essentially states, \"This group is assigned this role on this New Relic account.\" Adding a user to a group doesn’t do anything unless that group is included in an access grant. An authentication domain contains a set of users who are added to New Relic and who log in to New Relic in the same way. For example, you may have one authentication domain for users who log in via username/password and another authentication domain for users who log in via SAML. If a user is a basic user, this takes precedence over any role-related limitations. For more on this, see Basic user and roles.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 432.46667,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "How to <em>manage</em> <em>users</em>",
        "sections": "Give <em>users</em> access to <em>accounts</em> <em>and</em> roles (access grants)",
        "tags": "<em>New</em> <em>Relic</em> <em>One</em> <em>user</em> <em>management</em>",
        "body": "For users on our <em>New</em> <em>Relic</em> <em>One</em> <em>user</em> model, we provide various <em>user</em> <em>management</em> features, including the ability to: Use role based access control (RBAC) to assign default or custom roles to <em>user</em> groups Create custom <em>user</em> groups Grant <em>user</em> groups access to specific roles and <em>accounts</em> Important"
      },
      "id": "603e7bce28ccbc415beba74c"
    },
    {
      "sections": [
        "Tutorials on user management tasks: access grants, custom roles, and adding users (New Relic One user model)",
        "Requirements",
        "Overview",
        "Add users",
        "View available accounts",
        "Grant access to accounts and roles",
        "Create custom role",
        "Add users to groups"
      ],
      "title": "Tutorials on user management tasks: access grants, custom roles, and adding users (New Relic One user model)",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "New Relic One user management"
      ],
      "external_id": "b1e5f303b6446f264c9d8c5020871a6990e052fd",
      "image": "https://docs.newrelic.com/static/a78dad5ff794da5deaaf3b514e334ea7/c1b63/new-relic-one-user-mgmt.png",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/new-relic-one-user-management/tutorial-add-new-user-groups-roles-new-relic-one-user-model/",
      "published_at": "2021-12-19T16:26:00Z",
      "updated_at": "2021-11-24T14:27:52Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This tutorial will walk you through some common procedures for managing users on the New Relic One user model. Here's a quick overview of the process (3:24 minutes): Requirements Some notes on requirements: This tutorial is for users on our New Relic One user model. Creating access grants requires Pro or Enterprise edition. For details, see user management requirements. Role requirements: Using the Organization and access UI to manage group access requires Authentication domain manager and Organization manager roles. Using the User management UI to add/edit users requires Authentication domain manager role. Overview Before using this tutorial, we recommend you check out: Understand what an access grant is Videos that show the user management UI in action This tutorial will walk you through how to: Add users View available accounts Grant groups access to roles and accounts Create custom roles Add users to groups Add users Adding users to New Relic is a separate process from creating access grants. In other words, you can do either procedure first. The key point is that if you're trying to give a user access to a role or an account that they don't yet have, they won't have that access until they're both a) in a group and b) that group has the correct access grant (access to a specific role on a specific account). There are two ways to add users: With the user management UI (described below) With automated user management, also known as SCIM provisioning: use of this method means that groups are defined in your identity provider and so you can't edit a user's group from the New Relic UI. To add users from the UI: From the top right of the New Relic UI, click the account dropdown, click Administration, and click User management. If you have multiple authentication domains, choose one from the authentication domain dropdown. Click Add user. Complete the process, including choosing user type and user group. Relevant tips: For how to bulk edit users' user type, see Edit user type. When choosing a group, you can choose either one of our default groups or a custom group that you've defined an access grant for. For more on creating access grants, keep reading. View available accounts When thinking about creating access grants for granting access to different accounts, it can help you to understand what accounts there are in your organization. To view the accounts in your organization: go to the account dropdown, click Administration, click Organizations and access, and then click Accounts. Grant access to accounts and roles Groups are used to group your users and manage what your users are able to do in New Relic: by creating an access grant, you assign a group access to a) a specific role on b) a specific account. By default, organizations on the New Relic One user model have two available groups: Admin and User. These default groups automatically have access to specific standard roles and are assigned to the account in which they were initially added. To view existing access grants: from the account dropdown, click Administration, and then click Organization and access. Even if you haven't created any custom access grants, you will see the default-created grants there that are present for our default groups. This is what you might see when you go to the Organization and access UI and view groups. The default available groups of Admin and User have access grants automatically created that grant users in those groups access to the roles associated with those groups and to the initial account those users were added in. To create a new access grant that gives a user group access to a role and an account: From the Organization and access UI, click Group access. If you don't see that UI, it may be because you're logged in with an original user model record, or because you don't have the ability to manage users. For more on such factors, see Factors affecting access. Choose one of the following: Existing: If there is already a group you want to add an access grant to, you can use this. For example, if you want to gives users in the default Admin or User group access to new accounts, you might choose this and then select the Admin Default or Admin User role. New: If you need to create a new group, choose this. You'll also need to choose the authentication domain that group is inside of (for an explanation of what \"Default\" means, see Default groups. If your organization strategy requires a good amount of restrictions over access to accounts and access to roles, you'll probably need to create a good amount of access grants. Next, under Access grant, you'll choose one of following: Account: Choose this to be able to select from the roles that are account-scoped. These are the roles that have to do with using and configuring our platform features (and not about organization and user management). Organization: Choose this to be able to select from the roles that are organization-scoped. These are the roles that govern organization- and user management. (Note that these users must also already belong to an account-scoped role. This is true for most users but if it's not, you may see a message that the user doesn't belong to an organization.) Select the Role you want to assign. Roles are organization-wide, so regardless of the authentication domain you're in, you have access to our standard roles and any custom roles you've created. For tips on selecting roles, see the tips after these instructions. Select the Account you want to add access to from the dropdown. If you don't see an account that you'd expect to see, this may be for a few reasons. One is that you yourself don't have the proper permissions for that account. Another is that that account is not actually in your organization. For more information, see Factors affecting access. If you are still having problems, talk to your account representative. If you want to continue adding more grants for that same group, select Add another at the bottom before clicking Add access. When you're done, if your users are already in the group you've added the grant to, they should have access within a few minutes (although for EU region New Relic accounts, this can take up to twenty minutes or so). If your users are not yet in that group (which would be true if you just created an access grant with a new group), you'll need to go to the User management UI and add one or more users to that group. Some tips for using this UI: If your users are managed via automated user management, you can't use the User management UI to add users to groups because your groups are imported from your identity provider. You will need to create access grants for those groups once they are in New Relic, though, to give those groups access. Note that if a user has the organization-scoped Organization manager and/or Authentication domain manager roles (which is true of users in our default Admin group) those users will always have those capabilities because those are organization-scoped abilities. This means that when you go to add those users to another account, you only have to add an account-scoped role, and not an organization-scoped role. In other words, once the users in a group have those organization-scoped roles, they will always have them for that organization unless removed. When selecting from amongst our standard roles, it's important to understand the difference between All product admin and Standard user. In short, All product admin is more popular a choice because it gives the ability to configure platform features. If you wanted to have your users be able to use platform features but not configure them, you'd choose Standard user. Create custom role When creating an access grant, you can use our standard roles, or you can create your own roles with unique sets of capabilities. To view existing roles: from the account dropdown, click Administration, then click Organization and access, and then click Roles. To create a custom role, click Add new custom role. Review the list of available capabilities and decide which ones your custom role needs. For more information about how roles and capabilities work, see Capabilities. Or check out this NerdByte video (4:07 minutes). Once you're finished creating a custom role, you will probably want to use it in a new access grant. Add users to groups In the user management UI, you can see your users and the groups they've been assigned to. Your users only have access to the access grants associated with the groups they've been added to. To view users and see their groups: from the account dropdown, click Administration, and click User management. If you don’t see that option, review the requirements. Groups reside within the boundaries of an authentication domain. If your organization has more than one authentication domain, the domain switcher in the top left will show which one you’re in. To add a user, click Add user. Complete the prompts in the UI, including choosing the user type and group. Any custom groups you’ve added should be available from the group dropdown. If the custom group you choose has been granted access to a role and an account, once you add the user to that group, that user will now have access. To edit a user’s group or other details: click on the user you want to edit and make changes.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 431.9562,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Tutorials on <em>user</em> <em>management</em> tasks: access grants, custom roles, <em>and</em> adding <em>users</em> (<em>New</em> <em>Relic</em> <em>One</em> <em>user</em> model)",
        "sections": "Tutorials on <em>user</em> <em>management</em> tasks: access grants, custom roles, <em>and</em> adding <em>users</em> (<em>New</em> <em>Relic</em> <em>One</em> <em>user</em> model)",
        "tags": "<em>New</em> <em>Relic</em> <em>One</em> <em>user</em> <em>management</em>",
        "body": " means that groups are defined in your identity provider and so you can&#x27;t edit a <em>user</em>&#x27;s group from the <em>New</em> <em>Relic</em> UI. To add users from the UI: From the top right of the <em>New</em> <em>Relic</em> UI, click the <em>account</em> dropdown, click Administration, and click <em>User</em> <em>management</em>. If you have multiple authentication domains"
      },
      "id": "603e7d67196a671e26a83dc5"
    },
    {
      "sections": [
        "SCIM API tutorial",
        "Requirements",
        "Overview",
        "Configure your authentication domain for SCIM",
        "Tip",
        "Create users and user groups in your system",
        "Connect to the SCIM API",
        "401 Unauthorized",
        "Create users in your authentication domain",
        "Important",
        "Example responses",
        "201 Created",
        "400 Bad Request",
        "409 Conflict",
        "Create groups in your authentication domain",
        "View users and groups in your authentication domain",
        "200 OK",
        "Update a user's attributes",
        "Update a group's members",
        "Delete users and groups",
        "Next steps",
        "Optional: Manage user type"
      ],
      "title": "SCIM API tutorial",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Automated user management",
        "New Relic One user management"
      ],
      "external_id": "ef29f5444770fc05d762fd27c4872a92660ec681",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/automated-user-management/tutorial-manage-users-groups-scim/",
      "published_at": "2021-12-19T15:40:05Z",
      "updated_at": "2021-11-24T11:08:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This tutorial will walk you through some common procedures for managing users on the New Relic One user model using the SCIM API. The SCIM API allows you to view, create, update, and delete users and groups programmatically, outside of the User management UI. Requirements Before using this tutorial, we recommend you read: The requirements for automated user management and using the SCIM API. Important user management concepts The primary SCIM API reference Other related resources: Some SCIM 2.0 RFC documents from the Internet Engineering Task Force that are most relevant: RFC 7643 - SCIM Core Resources and Extensions, RFC 7643 - JSON Representation, and RFC 7644 - SCIM Protocol. Overview This tutorial shows you how to accomplish some of the most common tasks needed for adding users to New Relic from an identity provider service and managing them from there. It is meant to supplement our primary SCIM API resource. Note that using automated user management means that your groups of users are imported into New Relic. This means that you can't use our user management UI to add users to groups. The groups are created and managed from your identity provider side. Once you're done with getting your user groups into New Relic, you must use our Organization and access UI to create access grants, which give your groups access to specific roles and/or accounts. For more information, see user management concepts. Configure your authentication domain for SCIM Before you can use the SCIM API, you must first enable SCIM for your authentication domain. Note that the API access token is displayed only once after you save the configuration, so save it somewhere safe for later user. Tip If you need to view a bearer token later, the only way to do this is to generate a new one, and that will invalidate the old one and any integrations using the old token. Create users and user groups in your system The SCIM API is typically used by scripts for importing users and groups into New Relic from a database or a third-party identity provider that doesn't have pre-configured configs for New Relic. If you want to use the SCIM API custom application or for ad-hoc requests, proceed to learn how to connect to the SCIM API. Connect to the SCIM API The SCIM API is available at https://scim-provisioning.service.newrelic.com/scim/v2 and this URL is viewable in the authentication domain settings page. To access the SCIM API, your client must include a bearer token with each request. The token is displayed after saving your Authentication Domain configuration. If you're using a third-party identity provider, configure it to use Bearer token authorization and plug in your API access token. Refer to your identity provider's documentation for help configuring this. Once configured, you're all set to import users and groups. Rather than reading the entire SCIM protocol RFCs, there are three specific sections you may find valuable: See RFC 7643 - SCIM Core Resources and Extensions and RFC 7643 - JSON Representation for the specifics. Refer to RFC 7644 - SCIM Protocol for more information about the protocol used in this tutorial. For all requests to the SCIM API, you must provide the bearer token in an Authorization header. Here's an example with curl: curl -X 'GET' -H 'Accept: application/json' -H \"Authorization: Bearer $TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Users' Copy Any request in the rest of this tutorial will receive a 401 Unauthorized response if the API access token is missing or invalid. Example response: 401 Unauthorized { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"detail\": \"invalid authentication token\", \"status\": \"401\" } Copy Create users in your authentication domain You can use the SCIM API to send a POST request to /scim/v2/Users to create a user. The following user attributes are required: userName This identifier must be unique within an authentication domain. Use the user's email address. emails Same as userName. The email address of the user. (Despite it being called emails, for this procedure enter only one.) active Boolean indicating whether or not the user should be active or inactive within New Relic. We recommend providing the following attributes for the best user experience: name.givenName The user's first or given name. name.familyName The user's last or family name. timezone The user's timezone in IANA Time Zone database format. curl -X 'POST' -H 'Content-Type: application/json' -H \"Authorization: Bearer $TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Users' --data-binary @- <<EOF { \"schemas\": [\"urn:ietf:params:scim:schemas:core:2.0:User\"], \"userName\": \"bjensen@example.com\", \"name\": { \"familyName\": \"Jensen\", \"givenName\": \"Barbara\" }, \"emails\": [ { \"primary\": true, \"value\": \"bjensen@example.com\" } ], \"active\": true, \"timezone\": \"America/Los_Angeles\" } EOF Copy Important Take note of the returned user id. To update a user in the future you'll need to supply the same ID with the request. Example responses 201 Created { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\", ], \"id\": \"5a1d580f-323c-450c-8c62-479b5c9085d6\", \"externalId\": null, \"userName\": \"bjensen@example.com\", \"name\": { \"familyName\": \"Jensen\", \"givenName\": \"Barbara\" }, \"emails\": [ { \"value\": \"bjensen@example.com\", \"primary\": true } ], \"timezone\": \"America/Los_Angeles\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2021-07-20T21:32:58.167Z\", \"lastModified\": \"2021-07-20T21:32:58.167Z\" }, \"groups\": [] } Copy 400 Bad Request { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"scimType\": \"invalidValue\", \"detail\": \"Validation failed: Username can't be blank\", \"status\": \"400\" } Copy 409 Conflict { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"scimType\": \"uniqueness\", \"detail\": \"Resource already exists\", \"status\": \"409\" } Copy Create groups in your authentication domain You can use the SCIM API to send a POST request to /scim/v2/Groups to create a group. The only group attribute required is: displayName The group name. curl -X 'POST' -H 'Content-Type: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Groups' --data-binary @- <<EOF { \"schemas\": [\"urn:ietf:params:scim:schemas:core:2.0:Group\"], \"displayName\": \"Example Group\" } EOF Copy Important Take note of the returned group id. To update a group or its members in the future you will need to supply the same ID with the request. Example responses 201 Created { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"df2b9a04-0426-4a3e-bf5f-54d5341f4e5b\", \"displayName\": \"Example Group\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2021-07-20T22:41:50.414Z\", \"lastModified\": \"2021-07-20T22:41:50.414Z\" }, \"members\": [] } Copy 400 Bad Request { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"scimType\": \"invalidValue\", \"detail\": \"Validation failed: Display name can't be blank\", \"status\": \"400\" } Copy 409 Conflict { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"scimType\": \"uniqueness\", \"detail\": \"Resource already exists\", \"status\": \"409\" } Copy View users and groups in your authentication domain After you've created some users and groups you'll see them in the User management UI. You can also retrieve them from the SCIM API. In this tutorial, you'll be searching for specific users and groups, but that's not the only way to view users and groups. Refer to the SCIM API reference and RFC 7644 for all the available query options. To retrieve a user by email, send a GET request to /scim/v2/Users with a filter query parameter. The filter parameter must be URL encoded. curl -X 'GET' -H 'Accept: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Users' --get --data-urlencode 'filter=userName eq \"bjensen@example.com\"' Copy Example response: 200 OK { \"totalResults\": 1, \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:ListResponse\" ], \"Resources\": [ { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\", ], \"id\": \"5a1d580f-323c-450c-8c62-479b5c9085d6\", \"externalId\": null, \"userName\": \"bjensen@example.com\", \"name\": { \"familyName\": \"Jensen\", \"givenName\": \"Barbara\" }, \"emails\": [ { \"value\": \"bjensen@example.com\", \"primary\": true } ], \"timezone\": \"America/Los_Angeles\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2021-07-20T21:32:58.167Z\", \"lastModified\": \"2021-07-20T21:32:58.167Z\" }, \"groups\": [] } ] } Copy Similarly, send a GET request to /scim/v2/Groups with a filter query parameter to retrieve a group by name. curl -X 'GET' -H 'Accept: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Groups' --get --data-urlencode 'filter=displayName eq \"Example Group\"' Copy Example response: 200 OK { \"totalResults\": 1, \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:ListResponse\" ], \"Resources\": [ { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"df2b9a04-0426-4a3e-bf5f-54d5341f4e5b\", \"displayName\": \"Example Group\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2021-07-20T22:41:50.414Z\", \"lastModified\": \"2021-07-20T22:41:50.414Z\" }, \"members\": [] } ] } Copy Update a user's attributes The SCIM API supports both PUT and PATCH methods for updating users. Refer to the SCIM API supported actions and RFC 7644 for details on using PATCH. This tutorial demonstrates updating a user's attributes with the PUT method. New Relic does not require all user attributes be included in the request body, only the attributes you want to update are necessary. Send a PUT request to /scim/v2/Users/${ID} to update the user. curl -X 'PUT' -H 'Content-Type: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Users/5a1d580f-323c-450c-8c62-479b5c9085d6' --data-binary @- <<EOF { \"schemas\": [\"urn:ietf:params:scim:schemas:core:2.0:User\"], \"timezone\": \"America/Chicago\" } EOF Copy Example responses 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\", ], \"id\": \"5a1d580f-323c-450c-8c62-479b5c9085d6\", \"externalId\": null, \"userName\": \"bjensen@example.com\", \"name\": { \"familyName\": \"Jensen\", \"givenName\": \"Barbara\" }, \"emails\": [ { \"value\": \"bjensen@example.com\", \"primary\": true } ], \"timezone\": \"America/Chicago\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2021-07-20T21:32:58.167Z\", \"lastModified\": \"2021-07-21T02:12:05.348Z\" }, \"groups\": [] } Copy 409 Conflict { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"scimType\": \"uniqueness\", \"detail\": \"Resource already exists\", \"status\": \"409\" } Copy Update a group's members The SCIM API supports both PUT and PATCH methods for updating groups. This tutorial will show how to update a group's members with the PATCH method. Refer to the SCIM API supported actions and RFC 7644 for details on using PUT. PATCH is convenient for adding or removing group members without needing to specify the full member list in the request. To add a user to a group, use the following operation parameters: op set to Add path set to members value set to a list of {\"value\": \"${USER_ID}\"} with each user ID to add to the group Send a PATCH request to /scim/v2/Groups/${ID} to update group members. curl -X 'PATCH' -H 'Content-Type: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Groups/df2b9a04-0426-4a3e-bf5f-54d5341f4e5b' --data-binary @- <<EOF { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:PatchOp\" ], \"Operations\": [{ \"op\": \"Add\", \"path\": \"members\", \"value\": [{ \"value\": \"5a1d580f-323c-450c-8c62-479b5c9085d6\" }] }] } EOF Copy Example response: 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"df2b9a04-0426-4a3e-bf5f-54d5341f4e5b\", \"displayName\": \"Example Group\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2021-07-20T22:41:50.414Z\", \"lastModified\": \"2021-07-20T22:41:50.414Z\" }, \"members\": [ { \"type\": \"User\", \"value\": \"5a1d580f-323c-450c-8c62-479b5c9085d6\" } ] } Copy To remove a user from a group, use the following operation parameters: op set to Remove path set to members value set to a list of {\"value\": \"${USER_ID}\"} with each user ID to remove from the group curl -X 'PATCH' -H 'Accept: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Groups/df2b9a04-0426-4a3e-bf5f-54d5341f4e5b' --data-binary @- <<EOF { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:PatchOp\" ], \"Operations\": [{ \"op\": \"Remove\", \"path\": \"members\", \"value\": [{ \"value\": \"5a1d580f-323c-450c-8c62-479b5c9085d6\" }] }] } EOF Copy Example response: 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"df2b9a04-0426-4a3e-bf5f-54d5341f4e5b\", \"displayName\": \"Example Group\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2021-07-20T22:41:50.414Z\", \"lastModified\": \"2021-07-20T22:41:50.414Z\" }, \"members\": [] } Copy Delete users and groups To remove a user from an authentication domain, send a DELETE request to /scim/v2/Users/${ID}. curl -X 'DELETE' -H 'Accept: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Users/d0f4d8e3-5413-4894-a8f9-de709994e18c' Copy Example response: 204 No Content Copy Similarly, to remove a group from your authentication domain, send a DELETE request to /scim/v2/Groups/${ID}. curl -X 'DELETE' -H 'Accept: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Groups/df2b9a04-0426-4a3e-bf5f-54d5341f4e5b' Copy Example response: 204 No Content Copy Next steps Once your integration is complete, potential next steps include: Your New Relic users will by default start out as basic users and you have the option to change some of them to full platform users. To do this, you can use the User management UI or use the SCIM API. Set up SAML SSO. Once your user groups are in New Relic, you'll need to assign access grants, which are what give your users access to specific roles and specific accounts. Learn more about access grants. Optional: Manage user type Once your SCIM API integration is complete, all users brought into New Relic start out as basic users. You can use our default method for managing user type, which is using the User management UI. Optionally, you can use our SCIM API instead. To do this, you can set update your authentication domain configuration to Delegate control of user type to your identity provider or custom application. The user's type attribute is defined in the custom schema urn:ietf:params:scim:schemas:extension:newrelic:2.0:User. Include this schema and the nrUserType string attribute in your create or update request to set a user's type. Valid values for nrUserType include: Full User (for full platform user) Basic User To create a new Basic User send a POST request /scim/v2/Users and include the custom New Relic schema extension: curl -X 'POST' -H 'Content-Type: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Users' --data-binary @- <<EOF { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\", \"urn:ietf:params:scim:schemas:extension:newrelic:2.0:User\" ], \"userName\": \"jbenson@example.com\", \"name\": { \"givenName\": \"James\", \"familyName\": \"Benson\" }, \"emails\": [{ \"primary\": true, \"value\": \"jbenson@example.com\", \"type\": \"work\" }], \"active\": true, \"timezone\": \"America/Chicago\", \"urn:ietf:params:scim:schemas:extension:newrelic:2.0:User\": { \"nrUserType\": \"Basic User\" } } EOF Copy Example responses 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\", \"urn:ietf:params:scim:schemas:extension:newrelic:2.0:User\" ], \"id\": \"8ab6c464-983c-4bb4-9507-720f28763a43\", \"externalId\": null, \"userName\": \"jbenson@example.com\", \"name\": { \"familyName\": \"Benson\", \"givenName\": \"James\" }, \"emails\": [ { \"value\": \"jbenson@example.com\", \"primary\": true } ], \"timezone\": \"America/Chicago\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2021-07-21T19:53:33.470Z\", \"lastModified\": \"2021-07-21T19:53:33.470Z\" }, \"groups\": [], \"urn:ietf:params:scim:schemas:extension:newrelic:2.0:User\": { \"nrUserType\": \"Basic User\" } } Copy 409 Conflict { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"scimType\": \"uniqueness\", \"detail\": \"Resource already exists\", \"status\": \"409\" } Copy 400 Bad Request { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"scimType\": \"invalidValue\", \"detail\": \"Validation failed: Username can't be blank\", \"status\": \"400\" } Copy To update a user's type, send a PUT request scim/v2/Users/${ID} and include the custom New Relic schema extension: curl -X 'PUT' -H 'Content-Type: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Users' --data-binary @- <<EOF { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\", \"urn:ietf:params:scim:schemas:extension:newrelic:2.0:User\" ], \"urn:ietf:params:scim:schemas:extension:newrelic:2.0:User\": { \"nrUserType\": \"Full User\" } } EOF Copy Example responses 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\", \"urn:ietf:params:scim:schemas:extension:newrelic:2.0:User\" ], \"id\": \"8ab6c464-983c-4bb4-9507-720f28763a43\", \"externalId\": null, \"userName\": \"jbenson@example.com\", \"name\": { \"familyName\": \"Benson\", \"givenName\": \"James\" }, \"emails\": [ { \"value\": \"jbenson@example.com\", \"primary\": true } ], \"timezone\": \"America/Chicago\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2021-07-21T19:53:33.470Z\", \"lastModified\": \"2021-07-21T20:15:56.718Z\" }, \"groups\": [], \"urn:ietf:params:scim:schemas:extension:newrelic:2.0:User\": { \"nrUserType\": \"Full User\" } } Copy 400 Bad Request { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"scimType\": \"invalidValue\", \"detail\": \"Validation failed: invalid nrUserType value provided\", \"status\": \"400\" } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 362.11206,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Create <em>users</em> <em>and</em> <em>user</em> groups in your system",
        "tags": "<em>New</em> <em>Relic</em> <em>One</em> <em>user</em> <em>management</em>",
        "body": "This tutorial will walk you through some common procedures for managing users on the <em>New</em> <em>Relic</em> <em>One</em> <em>user</em> model using the SCIM API. The SCIM API allows you to view, create, update, and delete users and groups programmatically, outside of the <em>User</em> <em>management</em> UI. Requirements Before using this tutorial"
      },
      "id": "611fd560196a6791717ab5f9"
    }
  ],
  "/docs/accounts/accounts-billing/new-relic-one-user-management/introduction-saml-scim": [
    {
      "sections": [
        "Introduction to automated user management (AUM) and single-sign on (SSO)",
        "Benefits",
        "Requirements and recommendations",
        "Set up automated user management (AUM)"
      ],
      "title": "Introduction to automated user management (AUM) and single-sign on (SSO)",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Automated user management"
      ],
      "external_id": "831a5f1137eccac9540d716302645b4e976a6332",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/automated-user-management/automated-user-provisioning-single-sign/",
      "published_at": "2021-12-19T15:56:00Z",
      "updated_at": "2021-11-14T11:21:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic lets you set up automated user management (AUM), which allows you to import, update, and deactivate your New Relic users from an identity provider, like Azure AD, Okta, or OneLogin. Benefits Before reading the benefits of automated user management, we recommend reading Get started with SAML SSO and SCIM. Benefits of enabling automated user management include: Time and cost efficiency: When you make changes in your identity provider, such as creating, updating, and removing users, these changes are automatically reflected in New Relic. By being able to manage a large set of users from your identity provider, it reduces the workload of your admins who'd otherwise need to do a significant amount of work in New Relic to accomplish the same thing. Increased productivity: By having a more automatic way to set up users and groups, they're enabled and ready to use New Relic more quickly. Enhanced security: SCIM is an industry standard protocol for maintaining groups of users. Use of this feature requires SAML SSO, so once your users are added to New Relic, they can log in using your identity provider. Popular identity providers Azure AD, Okta, and OneLogin have dedicated New Relic apps, improving ease of enablement. Requirements and recommendations Requirements and recommendations: Requires Pro or Enterprise edition. Supports SAML 2.0 standard for single sign on (SSO). Supports SCIM 2.0 standard. User model-related requirements: This feature requires you to be on our New Relic One user model and creates users on that model. If you're on our original user model (or otherwise can't seem to implement this feature), talk to your New Relic account representative. Configuring AUM requires that a user have the Authentication domain manager and the Organization manager role (users in the default group Admin have these). There are three identity providers that have a New Relic app: Azure AD, Okta, and OneLogin. For other identity providers, you can use our SCIM API. Before enabling this, it helps to first set up user groups in your identity provider service and think about which New Relic roles and accounts those groups will have access to. Set up automated user management (AUM) For an explanation of how your identity provider groups map over to New Relic groups, see Group and role mapping. To use automated user management to import users from your identity provider: It's important to first review the requirements. In the authentication domain UI, create a new authentication domain. If you use Azure AD, Okta, or OneLogin, use the applicable guide: Azure AD | Okta | OneLogin. If you don't use one of the above services, you'll need to: Use the authentication domain UI to enable SCIM as the source of users. Use our SCIM API to integrate with your identity provider service. See the SCIM API tutorial for all the steps involved. Highly recommended: Set a time zone for your users in your identity provider. How you do this will vary by identity provider. If not set in your identity provider, our UI shows UTC time zone dates/times. Time zone is specified in IANA Time Zone database format, also known as the \"Olson\" time zone database format (for example, \"America/Los_Angeles\"). If you have issues, contact your account representative. After being provisioned, your users can click on the New Relic SCIM/SSO application tile in their identity provider to be logged into New Relic. To learn more about New Relic's roles and capabilities, see Standard roles.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 2001.7899,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to automated user management (AUM) <em>and</em> single-sign on (<em>SSO</em>)",
        "sections": "Introduction to automated user management (AUM) <em>and</em> single-sign on (<em>SSO</em>)",
        "tags": "Accounts <em>and</em> billing",
        "body": " with <em>SAML</em> <em>SSO</em> and <em>SCIM</em>. Benefits of enabling automated user management include: Time and cost efficiency: When you make changes in your identity provider, such as creating, updating, and removing users, these changes are automatically reflected in New Relic. By being able to manage a large set of users"
      },
      "id": "6043d60e64441ff8f5378f37"
    },
    {
      "sections": [
        "How to manage users",
        "Important",
        "Requirements",
        "Manage users in the UI",
        "Overview of user management concepts",
        "Common user management tasks",
        "Add, edit, and delete users",
        "Edit user type (basic and full platform)",
        "Give users access to accounts and roles (access grants)",
        "Create new custom groups and roles",
        "Set up SAML SSO and/or SCIM provisioning",
        "Grant users ability to manage other users",
        "Control how basic users upgrade to full platform users",
        "Track changes",
        "User management terms and definitions"
      ],
      "title": "How to manage users",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "New Relic One user management"
      ],
      "external_id": "9e37836740ce56302734d4af636bdbe087b4dbc3",
      "image": "https://docs.newrelic.com/static/a78dad5ff794da5deaaf3b514e334ea7/c1b63/new-relic-one-user-mgmt.png",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/new-relic-one-user-management/add-manage-users-groups-roles/",
      "published_at": "2021-12-19T15:22:19Z",
      "updated_at": "2021-11-24T14:23:16Z",
      "document_type": "page",
      "popularity": 1,
      "body": "For users on our New Relic One user model, we provide various user management features, including the ability to: Use role based access control (RBAC) to assign default or custom roles to user groups Create custom user groups Grant user groups access to specific roles and accounts Important This doc applies to users on the New Relic One user model. For managing users on our original user model, see Original users. Not sure which user model you're on? See User models. Requirements To see if you can access these user management features, go to the user management UI and see what you have access to. Access requirements: These features allow managing of users on the New Relic One user model. To learn more, see User models. To avoid configuration conflicts, ensure that only one user is managing users at a time. Simultaneous configuration by more than one user may result in errors and unexpected results. Most capabilities require the Authentication domain manager role and some require the Organization manager role. For details, see Standard roles. Pricing edition requirements: To manage user groups, roles, and access grants: Pro or Enterprise edition is required. To import users from an identity provider: Enterprise is required. A New Relic user can have a maximum of either three concurrent active sessions, or three unique IP addresses in use at any given time. Manage users in the UI For users on the New Relic One user model, to find your user management options: From the account dropdown, select Administration. There are two user management UI pages there: User management: Use this to add users, update user type (basic versus full platform) and other information, and approve user type upgrade requests. Organization and access: Use this page to create and manage groups, roles, and access grants, and to set up SAML SSO and SCIM provisioning. Want to see videos of the user management UI in action? See our user management videos. Important If you can't see these UI pages, it may be because you're on our original user model or because you don't have the required user management role. Overview of user management concepts If your organization has Pro or Enterprise edition, you can create and configure access grants. An access grant gives a group of users access to a specific role on a specific account. Here's a diagram showing how access grants work and how they relate to the broader organization: A diagram explaining the concept of how access grants give a user group access to a) a role on b) a specific account. (Note that this applies to users on our New Relic One user model (and not our original user model).) When your New Relic organization is created, it starts out with some default access grants for our default User or Admin groups. Each of these groups is assigned one or more roles and granted access to the primary (first created) account: A view of the Organization and access UI, showing the default access grants associated with our default groups. If you have a relatively flat organizational structure, and are okay with all or many of your users having wide administrative access and access to all accounts, you'll probably only need at most a few access grants. For example, you might decide to add new access grants to the existing default Admin or User groups, giving those roles access to other accounts. Or, if you need more granular definition over roles and permissions, you'd create access grants that define new groups that have access to specific roles (either our standard roles or custom-defined roles). For a tutorial on how to create access grants and create custom roles, see the User management tutorial. For other examples of some common user management tasks, see Example tasks. To see the UI in action, see our user management videos. Some tips on setting up access grants: It may help to first plan out how your access grants will be organized. How many accounts will you have? What user groups will get access to which roles and which accounts? Will you use our default groups and roles or create your own custom groups and roles? If you've used automated user management to provision users via SCIM, you will have to create access grants to give those users access. A common configuration for organizations with many accounts (roughly 20 or more) is setting up groups with the more organization-scoped roles (Organization manager, Authentication domain manager, and Billing user) on the primary account, and then on other accounts, setting up groups with the more product-scoped roles (like All product admin, Standard user, or custom roles). Common user management tasks In the Organization and access UI, you can create access grants, custom groups, custom roles, and configure an authentication domain. Here are some example user management procedures: Add, edit, and delete users See the user management tutorial. Edit user type (basic and full platform) Note that there are limits around how many times full platform users can be changed to basic users. Important If you're using automated user management to provision and manage your users, you have other options for managing user type. To change the user type of one or more users: On the User management page, click the checkboxes for the users whose user type you want to edit. Once you start selecting users, an option will appear for Edit type. You can also edit the user type and group of a specific user by clicking on that user. Give users access to accounts and roles (access grants) See our user management tutorial. Create new custom groups and roles See the user management tutorial. Set up SAML SSO and/or SCIM provisioning See Get started with SAML SSO or SCIM. Grant users ability to manage other users To grant your users the ability to manage other users, you'll need to add users to a group that has the Authentication domain manager and Organization manager role. You have two options: From the User management UI, you can add a user to the default Admin group, which includes both of those roles. OR You can create a custom group and assign it these roles. For a tutorial on creating new access grants, groups, and roles, see the user management tutorial. Control how basic users upgrade to full platform users See the authentication domain settings. Track changes To see an audit log of changes to your account, including user management actions, you can query the NrAuditEvent. User management terms and definitions For an explanation of how user access to accounts and roles works, see User management concepts explained. Here are some definitions for the terms we use there: A New Relic organization is the representation of your organization, containing all your accounts, users, and data. For more information, see Organization and account structure. A capability is an ability to use or edit a specific, granular New Relic feature. Examples of capabilities: The ability to modify APM settings The ability to delete alert conditions A role is a set of capabilities. It is what gives a user their permissions. Our default standard roles have various capability sets, and you can create custom roles that have a custom set of capabilities. See some specific New Relic capabilities. A user group has one or more roles associated with it. You assign your users to a group. We have default user groups (Admin and User), and you can make your own groups. An access grant is what grants a user group access to roles and to specific New Relic accounts. An access grant essentially states, \"This group is assigned this role on this New Relic account.\" Adding a user to a group doesn’t do anything unless that group is included in an access grant. An authentication domain contains a set of users who are added to New Relic and who log in to New Relic in the same way. For example, you may have one authentication domain for users who log in via username/password and another authentication domain for users who log in via SAML. If a user is a basic user, this takes precedence over any role-related limitations. For more on this, see Basic user and roles.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1359.2722,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Set up <em>SAML</em> <em>SSO</em> <em>and</em>&#x2F;<em>or</em> <em>SCIM</em> provisioning",
        "tags": "Accounts <em>and</em> billing",
        "body": " the user type and group of a specific user by clicking on that user. Give users access to accounts and roles (access grants) See our user management tutorial. Create new custom groups and roles See the user management tutorial. Set up <em>SAML</em> <em>SSO</em> and&#x2F;or <em>SCIM</em> provisioning See <em>Get</em> <em>started</em> with <em>SAML</em> <em>SSO</em>"
      },
      "id": "603e7bce28ccbc415beba74c"
    },
    {
      "image": "https://docs.newrelic.com/static/49612c40721bfa27afa90fafcba0e95c/c1b63/login-multiple-accounts-found.png",
      "url": "https://docs.newrelic.com/docs/accounts/original-accounts-billing/original-users-roles/users-roles-original-user-model/",
      "sections": [
        "Users, roles, permissions (original user model)",
        "Requirements",
        "Updates about our new user model",
        "View and manage users in UI",
        "Tip",
        "Add a new user",
        "Important",
        "Manage user type (basic vs full) and full platform user upgrades",
        "Determine full platform user count",
        "Enable SAML SSO and/or SCIM",
        "View pending SAML SSO users",
        "Update account roles",
        "Delete a user",
        "Update the account Owner",
        "User types: basic user and full platform user",
        "Account roles",
        "Add-on roles",
        "View roles",
        "Assign a managed role",
        "Create a custom role",
        "Assign a custom role",
        "Edit or delete a custom role",
        "Account permissions",
        "Alert permissions",
        "APM permissions",
        "Browser permissions",
        "Infrastructure permissions",
        "Insights permissions",
        "Mobile permissions",
        "Synthetics permissions",
        "Workloads permissions",
        "Bulk user management",
        "Update users in bulk",
        "Example CSV file",
        "Bulk user management troubleshooting",
        "If you have a backup CSV file",
        "If no backup file exists"
      ],
      "published_at": "2021-12-19T15:01:08Z",
      "title": "Users, roles, permissions (original user model)",
      "updated_at": "2021-11-24T02:16:28Z",
      "type": "docs",
      "external_id": "95ae42f3474b43dec394245cfc3e23628449a1ed",
      "document_type": "page",
      "popularity": 1,
      "body": "For users on our original user model, an introduction to how the user model works, including user roles and permissions, and how to add and manage users. Requirements This doc and the surrounding section of docs shows you how to manage users on our original user model. If you were a New Relic customer before July 30 2020 and haven't migrated your users to the new model, your users are on our original user model (and not the New Relic One model). If you're an admin and want to see if you have users on the original model: If you can see users in the Users and roles UI, those users are on our original user model. Updates about our new user model In July of 2020, we released a new user model called the New Relic One user model, which offers many benefits in terms of how you manage your organization and users. At first this was only available to new sign-ups but over time we've been migrating more older customers to the new model. Some older customers are able to migrate their users on their own. We'll continue working on migrating users to the new model until the original model is fully deprecated. One impact of the new user model is that it's possible now for users to have multiple logins associated with the same email. For example, a user with access to multiple organizations (like a contractor) may have their user record updated to the new user model in one organization, resulting in them having their original login method and records and a New Relic One user model record. This may result in the user being logged in to New Relic and not being able to find an account they're looking for. For more on that, see Factors affecting access. If a user's email is associated with more than one login, they'll see a \"multiple accounts found\" note when logging in. View and manage users in UI If your New Relic account has users on our original user model, you can use the Users and roles UI. To access this: Click the account dropdown, click Account settings, and then click Users and roles. Some features in the UI are visible only to account Owners and Admins. Tip You can also use the New Relic REST API to obtain a list of everyone and their roles in your New Relic account. Here are some instructions and tips for adding and managing users via the UI: Add a new user Tip Owner or Admins To add a new user to your New Relic account: Go to: account dropdown > Account settings > Users and roles > Users. In the upper right corner, click New user. Enter the appropriate name and email address. Select their base role as either Admin, User, or Restricted. Select Add user. The new user will receive an email notification automatically from New Relic. Important New Relic recommends a maximum of 1,000 accounts per user. Additional accounts may result in limited access to some New Relic features. Manage user type (basic vs full) and full platform user upgrades Note that billing-related aspects of your count of full platform users only apply if you're on New Relic One pricing. If you're on our original pricing model, billing impacts do not apply. To update a user's type (basic user versus full platform user): Go to: account dropdown > Account settings > Users and roles > Users. Either select a user and edit their type or bulk update the type for multiple users. To control how basic users upgrade to become full platform users, from the Users and roles UI you can select Access requests. You have two options: Automatic approval: With this option, basic users can automatically upgrade to be full platform users. This option allows your users to more easily troubleshoot problems. Require review: With this option, your admins get a notification when basic users request an upgrade and must upgrade them first. You can approve them either from the notification email or from the user's entry in the Users and roles UI. For more about user type, see User type. Determine full platform user count If you're on New Relic One pricing model, your count of full platform users is a factor in your billing. To see your count of full platform users, click the account dropdown and then click View your usage. If you have a parent/child account structure (including a customer partnership), your count of full platform users may not match what you see when you go to Account settings > Users and roles. To examine users on a parent account's children accounts, go to a parent account's Account settings UI page, click on a child account, and go to that account's Users and roles UI page. Enable SAML SSO and/or SCIM For an introduction to using SAML SSO and/or SCIM provisioning, see Get started with SAML SSO or SCIM. View pending SAML SSO users New Relic accounts with SAML Single Sign On (SSO) may have a list of Pending users. These are individuals who have been added to the SAML-enabled account but have not yet confirmed. Update account roles Tip Owner or Admins To update a person's role and capabilities: Go to: account dropdown > Account settings > Users and roles > Users. Select the person's name. Under Roles and capabilities, select their base role as Admin, User, or Restricted. The account Owner must update the Owner role. Delete a user Tip Owner or Admins To remove a user from your New Relic account: Go to: account dropdown > Account settings > Users and roles > Users. Click on the name of the person you would like to update. Click Delete User. Tip You can also add, update, or delete users in bulk via CSV file. Update the account Owner A New Relic account can have only one Owner role at any time. You must be the current account Owner to change your role to someone who currently has an Admin role for the account. If the current Owner is unavailable, contact your account representative at New Relic, or get support at support.newrelic.com. You cannot delete or remove your assigned Owner role. However, if the account has one or more Admin role, you can change an Owner to an Admin. Go to: account dropdown > Account settings > Account > Users and roles. Above the Active users list, select Change owner. If an account has no Admins, this button won't be available. Select someone who currently has an Admin role for the account. Refresh the page for changes to take effect. Your previous Owner role automatically changes to an Admin role. To find out who is the current assigned Owner: Go to: account dropdown > Account settings > Account > Users and roles. View the Base role column to locate your account Owner. The Change owner button is only visible to the current account Owner. If the current Owner is unable to change the role (for example, that person no longer is with your organization), contact your account representative at New Relic, or get support at support.newrelic.com. User types: basic user and full platform user This section is for users on our original user model. If you're on our New Relic One user model, see our New Relic One user docs. The user type (basic user or full platform user) determines what features a user has access to. Below are details on the two user types. Note that billing-related aspects only apply if you're on New Relic One pricing. If you're on our original pricing model, billing impacts do not apply. Basic user. Details: These users have access to basic features like setting up reporting of data, running queries of data, making custom charts and dashboards, and setting up alerts. They do not have access to our more curated observability UI experiences (for more details on feature access, see Capabilities). Depending on access request settings, basic users can either upgrade themselves to be full platform users or request upgrade access from admins. Full platform user (also called full user). Details: Full platform users have access to everything (dependent on role restrictions), which includes our curated observability UI experiences, such as APM, infrastructure monitoring, browser monitoring, mobile monitoring, and synthetic monitors. For details, see Capabilities. For organizations on New Relic One pricing: these users are billable. The Standard edition includes one free full platform user and up to five total. If a user in your organization is set as a basic user in one account and a full platform user in another, the user is considered a full platform user and has full platform user access on all accounts in that organization. For how to edit a user's type, see Manage users. Account roles A New Relic account can have only one Owner. To share an account with other users in your organization, create Admins, Users, or Restricted Users. Account role Description Owner The person who initially creates the New Relic account and receives all billing queries. The Owner has complete access to all of the account information. Admin Can add, edit, and delete users, and can enable or set up features. User Can use (and optionally set up) New Relic features. In general, Admins take responsibility for setting up features, and Users and Restricted Users can use them. Restricted User One or more individuals who can view (but not set up or change) any New Relic features. The Restricted User role is useful, for example, for demos. You can change your New Relic session settings so that Restricted User logins do not time out, and then set the user interface to Kiosk mode. Add-on roles With add-on roles, you can grant variable levels of access to all users in your account, across the entire platform of New Relic products. This allows you to tailor your account permissions levels to suit the needs of Users and Restricted Users within your account. Giving a User or Restricted User add-on manager access to a product grants them the equivalent of Admin capabilities within the product. They will continue to have User or Restricted User capabilities for all other New Relic products. For example, you could make a software engineer in your company a User in most products, but assign Admin-level access to APM. For another example, you might assign the Nerdpack manager role to a user, and that gives them the ability to subscribe and unsubscribe New Relic One applications to an account. There are two types of add-on roles: Add-on Manager roles are available to grant permissions on a per-product basis. Giving a User or Restricted User managed add-on access to a product grants them the equivalent of Admin capabilities within the product. Custom add-on roles can grant feature-specific permissions across different New Relic products. For example, a group of Users could have the ability to acknowledge incidents and close violations in New Relic Alerts, but not have the ability to modify your existing alert preferences. Individuals on a parent account automatically have the same level of access for all the child accounts of the parent account. Below are options for managing both managed add-on roles and custom add-on roles: View roles To view the list of individuals assigned to your account and their current roles: Go to account dropdown > Account settings > Users and roles. Assign a managed role Tip Owner and Admins Managed add-on roles are available by default for each New Relic product. Adding a managed role for a user grants them Admin-level permissions for the assigned product. They cannot be edited or deleted. To assign a managed add-on role for a User or Restricted User in your account: Go to account dropdown > Account settings > Users and roles. From the list of users associated with your account, select their name. Under Add-on roles, select the type of manager role for the user. To understand which capabilities may be added, use the Capabilities preview chart. Features in the Capabilities preview chart may not exactly match what features are available for your subscription level. Tip You can also add, update, or delete users in bulk by using a CSV file. Create a custom role To create a custom add-on role for your account: Go to account dropdown > Account settings > Users and roles > Roles. Select New custom add-on role. Select the capabilities necessary for the new custom role, then Create role. Assign a custom role Tip Owners and Admins You must create a custom role before assigning it to a user. To assign a custom add-on role for a User or Restricted User in your account: Go to account dropdown > Account settings > Users and roles > Users. From the list of users associated with your account, select their name ]. Under Add-on roles, select a custom role for the user. Click Update user. Edit or delete a custom role Tip Owners and Admins You cannot edit or delete New Relic's default roles. However, you can edit or delete custom add-on roles for your account: Go to account dropdown > Account settings > Users and roles > Roles. From the Add-on roles list, select the custom add-on role, then select Edit role or Delete role as appropriate. Account permissions Here is a summary of user permissions. Individuals on a parent account automatically have the same level of access for all the child accounts of that parent account. However, they won't receive email notifications for alerts or weekly reports for child accounts unless they are explicitly granted permission on those accounts. Function Owner Admin User Restricted Maintain billing information. Change the account Owner. Add, update, and delete account Admins, Users, and Restricted Users. When the account Owner and Admins add individuals to the account, New Relic automatically sends them an email message. Update users' job titles and roles from Account settings in the New Relic UI. Create, modify and delete child accounts from Account settings in the New Relic UI. Update your own account information (name, password change or password reset request, default account, email preferences, etc.) from User preferences in the New Relic UI. Change someone else's password. You cannot reset passwords for anyone else on the account, even if you are an Owner or Admin. Instead, follow standard procedures to request a password reset from New Relic. View the list of individuals on the account from (account dropdown) > Account settings > Account > Summary in the New Relic UI. Manage flexible data retention. Subscribe and unsubscribe applications to New Relic One Add, update, and delete Proactive Detection configurations. Alert permissions Here is a summary of Admin and Add-on manager capabilities with New Relic Alerts. To allow a User or Restricted User to execute any of these functions in New Relic Alerts, assign an Alerts add-on manager role. Admin and manager capabilities for Alerts include: Create or name alert policies. Specify incident preferences. Disable or define alert conditions. Provide runbook instructions. Select product targets. Alter alert condition thresholds. Create, modify, or delete notification channels. APM permissions Here is a summary of Admin and Add-on manager capabilities with APM. To allow a User or Restricted User to execute any of these functions in APM, assign an APM add-on manager role. Admin and manager capabilities for APM include: Remove applications from the New Relic UI. Delete app traces and error traces. Browser permissions Here is a summary of Admin and Add-on manager capabilities with New Relic Browser. To allow a User or Restricted User to execute any of these functions in New Relic Browser, assign a Browser add-on manager role. Admin and manager capabilities for Browser include: Add, rename, or delete applications. Manage whitelists. Manage domain conditions. Infrastructure permissions Here is a summary of Admin and Add-on manager capabilities with New Relic Infrastructure. To allow a User or Restricted User to execute any of these functions in New Relic Infrastructure, assign an Infrastructure manager role. Admin and manager capabilities for Infrastructure include: Create alert conditions in New Relic Infrastructure, including conditions for host not reporting. Add or modify integrations. Insights permissions Here is a summary of Admin and Add-on manager capabilities with New Relic Insights. To allow a User or Restricted User to execute any of these functions, assign an Insights manager role. These functions include: Create, view, modify, or delete Query API keys or Insert API keys. Tip New Relic Insights includes permission levels to share your Insights dashboards with others. Mobile permissions To give permission to delete a mobile app from New Relic, you can assign an Admin or Mobile manager role. Synthetics permissions Here's a summary of Admin and Add-on manager capabilities with New Relic Synthetics. To allow a User or Restricted User to execute any of these functions in New Relic Synthetics, assign a Synthetics add-on manager role. Admin and manager capabilities for Synthetics include: Create, edit, or delete monitors. Edit monitor scripts. Create, edit, or delete private locations. Create, edit, or delete monitor downtimes. Create, view, edit, or delete secure credentials. For more information, see User roles in Synthetics. Workloads permissions Here's a summary of Admin and Add-on manager capabilities with New Relic One workloads: Create, duplicate, modify, or delete workloads. Link dashboards to workloads and save filters. To allow a User or Restricted User to execute these functions, assign the workloads manager add-on role. Bulk user management With the Bulk user actions feature, you can add, update, or delete multiple users at once. This can be helpful for: adding roles when multiple new employees start deleting roles when multiple employees leave giving multiple employees Admin roles Update users in bulk Some important rules and recommendations for making bulk user actions: You cannot make updates to your own role or an Owner role. You cannot edit an existing user's email address or name. You should avoid editing an existing user by deleting and re-adding them because this can have unintended consequences (for example, API keys associated with the original user will be lost). To add new user roles, update existing user roles, or delete user roles for users on the original user model: Go to: account dropdown > Account settings > Users and roles, and add /bulk_actions at the end of the URL. Example URL: https://account.newrelic.com/accounts/123456789/users/bulk_actions Copy Download a Backup CSV file. Downloading a backup file keeps a record of the users in your account prior to changes being made, and allows you to easily re-add any users that may be removed accidentally. Download a CSV of users or a CSV template. Each bulk action (add, update, or delete) will require its own CSV file. New Relic recommends saving your files with an account number, date, and the bulk action being performed. For example: account_123456789_delete_users_2018-06-29 Populate that sheet with only the users whose roles you'll be applying the chosen bulk action for. Remove users from the spreadsheet whose roles you do not want to change. Bulk action Fields Add Required fields: user email, name, type, base role Optional field: add-on role Update Required fields: user email (do not edit), name (do not edit), base role Optional field: add-on role Delete Required fields: only user email Example CSV file The following is an example downloaded CSV of users that lists four users on the New Relic account. In this example, we want to delete the user Alex Datanerd. All other users must be removed before uploading the CSV. Email Name Type Base role Add-on roles Last active User1 @Company.com Jane Datanerd full Owner 2/6/20 User2 @Company.com Jamie Datanerd full Admin 6/6/20 User3 @Company.com Alex Datanerd full User apm_admin, browser_admin 7/25/20 User4 @Company.com Pat Datanerd basic User alerts_admin, insights_admin, apm_admin 4/6/20 The other three users, whose roles will remain unchanged, are removed. The final CSV only shows Alex's name. This file would then be uploaded using the Delete users in CSV option in the UI. Email Name Type Base role Add-on roles Last active User3 @Company.com Alex Datanerd full User apm_admin, browser_admin In the UI, select a CSV action: Add, Update, or Delete the users listed within the CSV file. Upload the new CSV, and select Save changes. Bulk user management troubleshooting If a user is removed or changed during your CSV file upload by mistake, you can add them back through another CSV file upload. Important Be aware that associated permissions may be lost when a user is deleted and re-added. For example, associated API keys will need to be re-added. If you have a backup CSV file If you have a backup CSV file saved: Open the backup CSV file. Populate the backup CSV file with the users whose roles will be modified. Select a CSV action for the new CSV file: add, update, or delete Upload the new CSV, and select Save changes. If no backup file exists If no backup CSV file has been previously downloaded: Download the CSV file template. Populate the spreadsheet with the information required for the user to be restored. Action Required fields Add User email, name, type, base role. Optional: Add-on role Update User email, name, type, base role. Optional: Add-on role Delete User email Select a CSV action for the new CSV file: Add, Update, or Delete. Upload the new CSV, and select Save changes.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 487.00195,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Users, roles, permissions (<em>original</em> user model)",
        "sections": "Enable <em>SAML</em> <em>SSO</em> <em>and</em>&#x2F;<em>or</em> <em>SCIM</em>",
        "body": " page. Enable <em>SAML</em> <em>SSO</em> and&#x2F;or <em>SCIM</em> For an introduction to using <em>SAML</em> <em>SSO</em> and&#x2F;or <em>SCIM</em> provisioning, see <em>Get</em> <em>started</em> with <em>SAML</em> <em>SSO</em> or <em>SCIM</em>. View pending <em>SAML</em> <em>SSO</em> users New Relic accounts with <em>SAML</em> Single Sign On (<em>SSO</em>) may have a list of Pending users. These are individuals who have been added"
      },
      "id": "603e88b2e7b9d2a3f12a07d5"
    }
  ],
  "/docs/accounts/accounts-billing/new-relic-one-user-management/new-relic-one-user-model-understand-user-structure": [
    {
      "sections": [
        "How to manage users",
        "Important",
        "Requirements",
        "Manage users in the UI",
        "Overview of user management concepts",
        "Common user management tasks",
        "Add, edit, and delete users",
        "Edit user type (basic and full platform)",
        "Give users access to accounts and roles (access grants)",
        "Create new custom groups and roles",
        "Set up SAML SSO and/or SCIM provisioning",
        "Grant users ability to manage other users",
        "Control how basic users upgrade to full platform users",
        "Track changes",
        "User management terms and definitions"
      ],
      "title": "How to manage users",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "New Relic One user management"
      ],
      "external_id": "9e37836740ce56302734d4af636bdbe087b4dbc3",
      "image": "https://docs.newrelic.com/static/a78dad5ff794da5deaaf3b514e334ea7/c1b63/new-relic-one-user-mgmt.png",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/new-relic-one-user-management/add-manage-users-groups-roles/",
      "published_at": "2021-12-19T15:22:19Z",
      "updated_at": "2021-11-24T14:23:16Z",
      "document_type": "page",
      "popularity": 1,
      "body": "For users on our New Relic One user model, we provide various user management features, including the ability to: Use role based access control (RBAC) to assign default or custom roles to user groups Create custom user groups Grant user groups access to specific roles and accounts Important This doc applies to users on the New Relic One user model. For managing users on our original user model, see Original users. Not sure which user model you're on? See User models. Requirements To see if you can access these user management features, go to the user management UI and see what you have access to. Access requirements: These features allow managing of users on the New Relic One user model. To learn more, see User models. To avoid configuration conflicts, ensure that only one user is managing users at a time. Simultaneous configuration by more than one user may result in errors and unexpected results. Most capabilities require the Authentication domain manager role and some require the Organization manager role. For details, see Standard roles. Pricing edition requirements: To manage user groups, roles, and access grants: Pro or Enterprise edition is required. To import users from an identity provider: Enterprise is required. A New Relic user can have a maximum of either three concurrent active sessions, or three unique IP addresses in use at any given time. Manage users in the UI For users on the New Relic One user model, to find your user management options: From the account dropdown, select Administration. There are two user management UI pages there: User management: Use this to add users, update user type (basic versus full platform) and other information, and approve user type upgrade requests. Organization and access: Use this page to create and manage groups, roles, and access grants, and to set up SAML SSO and SCIM provisioning. Want to see videos of the user management UI in action? See our user management videos. Important If you can't see these UI pages, it may be because you're on our original user model or because you don't have the required user management role. Overview of user management concepts If your organization has Pro or Enterprise edition, you can create and configure access grants. An access grant gives a group of users access to a specific role on a specific account. Here's a diagram showing how access grants work and how they relate to the broader organization: A diagram explaining the concept of how access grants give a user group access to a) a role on b) a specific account. (Note that this applies to users on our New Relic One user model (and not our original user model).) When your New Relic organization is created, it starts out with some default access grants for our default User or Admin groups. Each of these groups is assigned one or more roles and granted access to the primary (first created) account: A view of the Organization and access UI, showing the default access grants associated with our default groups. If you have a relatively flat organizational structure, and are okay with all or many of your users having wide administrative access and access to all accounts, you'll probably only need at most a few access grants. For example, you might decide to add new access grants to the existing default Admin or User groups, giving those roles access to other accounts. Or, if you need more granular definition over roles and permissions, you'd create access grants that define new groups that have access to specific roles (either our standard roles or custom-defined roles). For a tutorial on how to create access grants and create custom roles, see the User management tutorial. For other examples of some common user management tasks, see Example tasks. To see the UI in action, see our user management videos. Some tips on setting up access grants: It may help to first plan out how your access grants will be organized. How many accounts will you have? What user groups will get access to which roles and which accounts? Will you use our default groups and roles or create your own custom groups and roles? If you've used automated user management to provision users via SCIM, you will have to create access grants to give those users access. A common configuration for organizations with many accounts (roughly 20 or more) is setting up groups with the more organization-scoped roles (Organization manager, Authentication domain manager, and Billing user) on the primary account, and then on other accounts, setting up groups with the more product-scoped roles (like All product admin, Standard user, or custom roles). Common user management tasks In the Organization and access UI, you can create access grants, custom groups, custom roles, and configure an authentication domain. Here are some example user management procedures: Add, edit, and delete users See the user management tutorial. Edit user type (basic and full platform) Note that there are limits around how many times full platform users can be changed to basic users. Important If you're using automated user management to provision and manage your users, you have other options for managing user type. To change the user type of one or more users: On the User management page, click the checkboxes for the users whose user type you want to edit. Once you start selecting users, an option will appear for Edit type. You can also edit the user type and group of a specific user by clicking on that user. Give users access to accounts and roles (access grants) See our user management tutorial. Create new custom groups and roles See the user management tutorial. Set up SAML SSO and/or SCIM provisioning See Get started with SAML SSO or SCIM. Grant users ability to manage other users To grant your users the ability to manage other users, you'll need to add users to a group that has the Authentication domain manager and Organization manager role. You have two options: From the User management UI, you can add a user to the default Admin group, which includes both of those roles. OR You can create a custom group and assign it these roles. For a tutorial on creating new access grants, groups, and roles, see the user management tutorial. Control how basic users upgrade to full platform users See the authentication domain settings. Track changes To see an audit log of changes to your account, including user management actions, you can query the NrAuditEvent. User management terms and definitions For an explanation of how user access to accounts and roles works, see User management concepts explained. Here are some definitions for the terms we use there: A New Relic organization is the representation of your organization, containing all your accounts, users, and data. For more information, see Organization and account structure. A capability is an ability to use or edit a specific, granular New Relic feature. Examples of capabilities: The ability to modify APM settings The ability to delete alert conditions A role is a set of capabilities. It is what gives a user their permissions. Our default standard roles have various capability sets, and you can create custom roles that have a custom set of capabilities. See some specific New Relic capabilities. A user group has one or more roles associated with it. You assign your users to a group. We have default user groups (Admin and User), and you can make your own groups. An access grant is what grants a user group access to roles and to specific New Relic accounts. An access grant essentially states, \"This group is assigned this role on this New Relic account.\" Adding a user to a group doesn’t do anything unless that group is included in an access grant. An authentication domain contains a set of users who are added to New Relic and who log in to New Relic in the same way. For example, you may have one authentication domain for users who log in via username/password and another authentication domain for users who log in via SAML. If a user is a basic user, this takes precedence over any role-related limitations. For more on this, see Basic user and roles.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.44208,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "How to manage <em>users</em>",
        "sections": "Give <em>users</em> access to accounts and <em>roles</em> (access grants)",
        "tags": "<em>New</em> <em>Relic</em> <em>One</em> <em>user</em> management",
        "body": "For <em>users</em> on our <em>New</em> <em>Relic</em> <em>One</em> <em>user</em> <em>model</em>, we provide various <em>user</em> management features, including the ability to: Use <em>role</em> based access control (RBAC) to assign default or custom <em>roles</em> to <em>user</em> groups Create custom <em>user</em> groups Grant <em>user</em> groups access to specific <em>roles</em> and accounts Important"
      },
      "id": "603e7bce28ccbc415beba74c"
    },
    {
      "sections": [
        "Metric normalization rules",
        "Metric normalization rules management"
      ],
      "title": "Metric normalization rules",
      "type": "docs",
      "tags": [
        "New Relic One",
        "Use New Relic One",
        "UI and data"
      ],
      "external_id": "3c55e4717f145ac7ae0d88e860878f4e8d18cd6b",
      "image": "https://docs.newrelic.com/static/83edfb6f5b1b68712cac34d138bb8cb8/3996e/create-new-rule-window.png",
      "url": "https://docs.newrelic.com/docs/new-relic-one/use-new-relic-one/ui-data/metric-normalization-rules/",
      "published_at": "2021-12-19T17:35:57Z",
      "updated_at": "2021-12-04T18:10:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "There may be cases where an application sends many individual metrics that could be better managed in groups. Most of these occur with web transactions metrics named from URLs. For more information on this issue, see Metric grouping issues (MGIs). To reduce high cardinality and prevent metric grouping issues, New Relic supports rules for grouping or filtering out metrics (normalization). In some cases, when MGIs with a significant high number of metrics are detected, a rule is created automatically to protect the platform from performance degradation. Metric normalization rules management You can create and manage new rules that deal with the noise produced from high cardinality metrics by using the metric normalization tool, which is accessible from each service dashboard in the New Relic Explorer. Once there, select Metric Normalization in the left sidebar. There you can see the existing rules or create new ones. Click a rule to modify it, or click Create a new rule to create a new one. A new pane to configure the rule will be displayed. Available fields are: Match expression: enter the regular expression to group all the metrics you want to include in the rule. Matches: here you will see a preview of the metrics matched by the regular expression above. Action: the action you want to perform on the metrics. Replace: replace the matched metrics by the regular expression with the value described in the Replacement field. Ignore: ignore any metric that matches the regular expression. Deny new metrics: only write metrics that have already been reported, and ignore those that match the regular expression. Replacement: only active when Replace is enabled. Matched metrics are replaced with the field's value. If the regular expression is capturing groups, you can use placeholders for them with \\1 or \\2 for the groups 1 and 2 respectively. Active: rules can’t be deleted, but can be deactivated. Click the toggle to enable or disable the rule. If you want the rule to be removed, reach out to New Relic's support. Terminate: When enabled, the rules waterfall is exited when the associated pattern is matched. Notes: internal notes on the rule. Has no effect on the rule. Once you have set up the fields, click Create (or Edit in case you are editing an existing rule), and the rule will be applied immediately as long as it's Active.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.17416,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>New</em> <em>Relic</em> <em>One</em>",
        "body": " grouping issues, <em>New</em> <em>Relic</em> supports rules for grouping or filtering out metrics (normalization). In some cases, when MGIs with a significant high number of metrics are detected, a rule is created automatically to protect the platform from performance degradation. Metric normalization rules management You"
      },
      "id": "603e810b64441ff3a74e8862"
    },
    {
      "sections": [
        "Tutorials on user management tasks: access grants, custom roles, and adding users (New Relic One user model)",
        "Requirements",
        "Overview",
        "Add users",
        "View available accounts",
        "Grant access to accounts and roles",
        "Create custom role",
        "Add users to groups"
      ],
      "title": "Tutorials on user management tasks: access grants, custom roles, and adding users (New Relic One user model)",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "New Relic One user management"
      ],
      "external_id": "b1e5f303b6446f264c9d8c5020871a6990e052fd",
      "image": "https://docs.newrelic.com/static/a78dad5ff794da5deaaf3b514e334ea7/c1b63/new-relic-one-user-mgmt.png",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/new-relic-one-user-management/tutorial-add-new-user-groups-roles-new-relic-one-user-model/",
      "published_at": "2021-12-19T16:26:00Z",
      "updated_at": "2021-11-24T14:27:52Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This tutorial will walk you through some common procedures for managing users on the New Relic One user model. Here's a quick overview of the process (3:24 minutes): Requirements Some notes on requirements: This tutorial is for users on our New Relic One user model. Creating access grants requires Pro or Enterprise edition. For details, see user management requirements. Role requirements: Using the Organization and access UI to manage group access requires Authentication domain manager and Organization manager roles. Using the User management UI to add/edit users requires Authentication domain manager role. Overview Before using this tutorial, we recommend you check out: Understand what an access grant is Videos that show the user management UI in action This tutorial will walk you through how to: Add users View available accounts Grant groups access to roles and accounts Create custom roles Add users to groups Add users Adding users to New Relic is a separate process from creating access grants. In other words, you can do either procedure first. The key point is that if you're trying to give a user access to a role or an account that they don't yet have, they won't have that access until they're both a) in a group and b) that group has the correct access grant (access to a specific role on a specific account). There are two ways to add users: With the user management UI (described below) With automated user management, also known as SCIM provisioning: use of this method means that groups are defined in your identity provider and so you can't edit a user's group from the New Relic UI. To add users from the UI: From the top right of the New Relic UI, click the account dropdown, click Administration, and click User management. If you have multiple authentication domains, choose one from the authentication domain dropdown. Click Add user. Complete the process, including choosing user type and user group. Relevant tips: For how to bulk edit users' user type, see Edit user type. When choosing a group, you can choose either one of our default groups or a custom group that you've defined an access grant for. For more on creating access grants, keep reading. View available accounts When thinking about creating access grants for granting access to different accounts, it can help you to understand what accounts there are in your organization. To view the accounts in your organization: go to the account dropdown, click Administration, click Organizations and access, and then click Accounts. Grant access to accounts and roles Groups are used to group your users and manage what your users are able to do in New Relic: by creating an access grant, you assign a group access to a) a specific role on b) a specific account. By default, organizations on the New Relic One user model have two available groups: Admin and User. These default groups automatically have access to specific standard roles and are assigned to the account in which they were initially added. To view existing access grants: from the account dropdown, click Administration, and then click Organization and access. Even if you haven't created any custom access grants, you will see the default-created grants there that are present for our default groups. This is what you might see when you go to the Organization and access UI and view groups. The default available groups of Admin and User have access grants automatically created that grant users in those groups access to the roles associated with those groups and to the initial account those users were added in. To create a new access grant that gives a user group access to a role and an account: From the Organization and access UI, click Group access. If you don't see that UI, it may be because you're logged in with an original user model record, or because you don't have the ability to manage users. For more on such factors, see Factors affecting access. Choose one of the following: Existing: If there is already a group you want to add an access grant to, you can use this. For example, if you want to gives users in the default Admin or User group access to new accounts, you might choose this and then select the Admin Default or Admin User role. New: If you need to create a new group, choose this. You'll also need to choose the authentication domain that group is inside of (for an explanation of what \"Default\" means, see Default groups. If your organization strategy requires a good amount of restrictions over access to accounts and access to roles, you'll probably need to create a good amount of access grants. Next, under Access grant, you'll choose one of following: Account: Choose this to be able to select from the roles that are account-scoped. These are the roles that have to do with using and configuring our platform features (and not about organization and user management). Organization: Choose this to be able to select from the roles that are organization-scoped. These are the roles that govern organization- and user management. (Note that these users must also already belong to an account-scoped role. This is true for most users but if it's not, you may see a message that the user doesn't belong to an organization.) Select the Role you want to assign. Roles are organization-wide, so regardless of the authentication domain you're in, you have access to our standard roles and any custom roles you've created. For tips on selecting roles, see the tips after these instructions. Select the Account you want to add access to from the dropdown. If you don't see an account that you'd expect to see, this may be for a few reasons. One is that you yourself don't have the proper permissions for that account. Another is that that account is not actually in your organization. For more information, see Factors affecting access. If you are still having problems, talk to your account representative. If you want to continue adding more grants for that same group, select Add another at the bottom before clicking Add access. When you're done, if your users are already in the group you've added the grant to, they should have access within a few minutes (although for EU region New Relic accounts, this can take up to twenty minutes or so). If your users are not yet in that group (which would be true if you just created an access grant with a new group), you'll need to go to the User management UI and add one or more users to that group. Some tips for using this UI: If your users are managed via automated user management, you can't use the User management UI to add users to groups because your groups are imported from your identity provider. You will need to create access grants for those groups once they are in New Relic, though, to give those groups access. Note that if a user has the organization-scoped Organization manager and/or Authentication domain manager roles (which is true of users in our default Admin group) those users will always have those capabilities because those are organization-scoped abilities. This means that when you go to add those users to another account, you only have to add an account-scoped role, and not an organization-scoped role. In other words, once the users in a group have those organization-scoped roles, they will always have them for that organization unless removed. When selecting from amongst our standard roles, it's important to understand the difference between All product admin and Standard user. In short, All product admin is more popular a choice because it gives the ability to configure platform features. If you wanted to have your users be able to use platform features but not configure them, you'd choose Standard user. Create custom role When creating an access grant, you can use our standard roles, or you can create your own roles with unique sets of capabilities. To view existing roles: from the account dropdown, click Administration, then click Organization and access, and then click Roles. To create a custom role, click Add new custom role. Review the list of available capabilities and decide which ones your custom role needs. For more information about how roles and capabilities work, see Capabilities. Or check out this NerdByte video (4:07 minutes). Once you're finished creating a custom role, you will probably want to use it in a new access grant. Add users to groups In the user management UI, you can see your users and the groups they've been assigned to. Your users only have access to the access grants associated with the groups they've been added to. To view users and see their groups: from the account dropdown, click Administration, and click User management. If you don’t see that option, review the requirements. Groups reside within the boundaries of an authentication domain. If your organization has more than one authentication domain, the domain switcher in the top left will show which one you’re in. To add a user, click Add user. Complete the prompts in the UI, including choosing the user type and group. Any custom groups you’ve added should be available from the group dropdown. If the custom group you choose has been granted access to a role and an account, once you add the user to that group, that user will now have access. To edit a user’s group or other details: click on the user you want to edit and make changes.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 249.86966,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Tutorials on <em>user</em> management tasks: access grants, custom <em>roles</em>, and adding <em>users</em> (<em>New</em> <em>Relic</em> <em>One</em> <em>user</em> <em>model</em>)",
        "sections": "Tutorials on <em>user</em> management tasks: access grants, custom <em>roles</em>, and adding <em>users</em> (<em>New</em> <em>Relic</em> <em>One</em> <em>user</em> <em>model</em>)",
        "tags": "<em>New</em> <em>Relic</em> <em>One</em> <em>user</em> management",
        "body": " access to a) a specific <em>role</em> on b) a specific account. By default, organizations on the <em>New</em> <em>Relic</em> <em>One</em> <em>user</em> <em>model</em> have two available groups: Admin and <em>User</em>. These default groups automatically have access to specific standard <em>roles</em> and are assigned to the account in which they were initially added"
      },
      "id": "603e7d67196a671e26a83dc5"
    }
  ],
  "/docs/accounts/accounts-billing/new-relic-one-user-management/tutorial-add-new-user-groups-roles-new-relic-one-user-model": [
    {
      "sections": [
        "How to manage users",
        "Important",
        "Requirements",
        "Manage users in the UI",
        "Overview of user management concepts",
        "Common user management tasks",
        "Add, edit, and delete users",
        "Edit user type (basic and full platform)",
        "Give users access to accounts and roles (access grants)",
        "Create new custom groups and roles",
        "Set up SAML SSO and/or SCIM provisioning",
        "Grant users ability to manage other users",
        "Control how basic users upgrade to full platform users",
        "Track changes",
        "User management terms and definitions"
      ],
      "title": "How to manage users",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "New Relic One user management"
      ],
      "external_id": "9e37836740ce56302734d4af636bdbe087b4dbc3",
      "image": "https://docs.newrelic.com/static/a78dad5ff794da5deaaf3b514e334ea7/c1b63/new-relic-one-user-mgmt.png",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/new-relic-one-user-management/add-manage-users-groups-roles/",
      "published_at": "2021-12-19T15:22:19Z",
      "updated_at": "2021-11-24T14:23:16Z",
      "document_type": "page",
      "popularity": 1,
      "body": "For users on our New Relic One user model, we provide various user management features, including the ability to: Use role based access control (RBAC) to assign default or custom roles to user groups Create custom user groups Grant user groups access to specific roles and accounts Important This doc applies to users on the New Relic One user model. For managing users on our original user model, see Original users. Not sure which user model you're on? See User models. Requirements To see if you can access these user management features, go to the user management UI and see what you have access to. Access requirements: These features allow managing of users on the New Relic One user model. To learn more, see User models. To avoid configuration conflicts, ensure that only one user is managing users at a time. Simultaneous configuration by more than one user may result in errors and unexpected results. Most capabilities require the Authentication domain manager role and some require the Organization manager role. For details, see Standard roles. Pricing edition requirements: To manage user groups, roles, and access grants: Pro or Enterprise edition is required. To import users from an identity provider: Enterprise is required. A New Relic user can have a maximum of either three concurrent active sessions, or three unique IP addresses in use at any given time. Manage users in the UI For users on the New Relic One user model, to find your user management options: From the account dropdown, select Administration. There are two user management UI pages there: User management: Use this to add users, update user type (basic versus full platform) and other information, and approve user type upgrade requests. Organization and access: Use this page to create and manage groups, roles, and access grants, and to set up SAML SSO and SCIM provisioning. Want to see videos of the user management UI in action? See our user management videos. Important If you can't see these UI pages, it may be because you're on our original user model or because you don't have the required user management role. Overview of user management concepts If your organization has Pro or Enterprise edition, you can create and configure access grants. An access grant gives a group of users access to a specific role on a specific account. Here's a diagram showing how access grants work and how they relate to the broader organization: A diagram explaining the concept of how access grants give a user group access to a) a role on b) a specific account. (Note that this applies to users on our New Relic One user model (and not our original user model).) When your New Relic organization is created, it starts out with some default access grants for our default User or Admin groups. Each of these groups is assigned one or more roles and granted access to the primary (first created) account: A view of the Organization and access UI, showing the default access grants associated with our default groups. If you have a relatively flat organizational structure, and are okay with all or many of your users having wide administrative access and access to all accounts, you'll probably only need at most a few access grants. For example, you might decide to add new access grants to the existing default Admin or User groups, giving those roles access to other accounts. Or, if you need more granular definition over roles and permissions, you'd create access grants that define new groups that have access to specific roles (either our standard roles or custom-defined roles). For a tutorial on how to create access grants and create custom roles, see the User management tutorial. For other examples of some common user management tasks, see Example tasks. To see the UI in action, see our user management videos. Some tips on setting up access grants: It may help to first plan out how your access grants will be organized. How many accounts will you have? What user groups will get access to which roles and which accounts? Will you use our default groups and roles or create your own custom groups and roles? If you've used automated user management to provision users via SCIM, you will have to create access grants to give those users access. A common configuration for organizations with many accounts (roughly 20 or more) is setting up groups with the more organization-scoped roles (Organization manager, Authentication domain manager, and Billing user) on the primary account, and then on other accounts, setting up groups with the more product-scoped roles (like All product admin, Standard user, or custom roles). Common user management tasks In the Organization and access UI, you can create access grants, custom groups, custom roles, and configure an authentication domain. Here are some example user management procedures: Add, edit, and delete users See the user management tutorial. Edit user type (basic and full platform) Note that there are limits around how many times full platform users can be changed to basic users. Important If you're using automated user management to provision and manage your users, you have other options for managing user type. To change the user type of one or more users: On the User management page, click the checkboxes for the users whose user type you want to edit. Once you start selecting users, an option will appear for Edit type. You can also edit the user type and group of a specific user by clicking on that user. Give users access to accounts and roles (access grants) See our user management tutorial. Create new custom groups and roles See the user management tutorial. Set up SAML SSO and/or SCIM provisioning See Get started with SAML SSO or SCIM. Grant users ability to manage other users To grant your users the ability to manage other users, you'll need to add users to a group that has the Authentication domain manager and Organization manager role. You have two options: From the User management UI, you can add a user to the default Admin group, which includes both of those roles. OR You can create a custom group and assign it these roles. For a tutorial on creating new access grants, groups, and roles, see the user management tutorial. Control how basic users upgrade to full platform users See the authentication domain settings. Track changes To see an audit log of changes to your account, including user management actions, you can query the NrAuditEvent. User management terms and definitions For an explanation of how user access to accounts and roles works, see User management concepts explained. Here are some definitions for the terms we use there: A New Relic organization is the representation of your organization, containing all your accounts, users, and data. For more information, see Organization and account structure. A capability is an ability to use or edit a specific, granular New Relic feature. Examples of capabilities: The ability to modify APM settings The ability to delete alert conditions A role is a set of capabilities. It is what gives a user their permissions. Our default standard roles have various capability sets, and you can create custom roles that have a custom set of capabilities. See some specific New Relic capabilities. A user group has one or more roles associated with it. You assign your users to a group. We have default user groups (Admin and User), and you can make your own groups. An access grant is what grants a user group access to roles and to specific New Relic accounts. An access grant essentially states, \"This group is assigned this role on this New Relic account.\" Adding a user to a group doesn’t do anything unless that group is included in an access grant. An authentication domain contains a set of users who are added to New Relic and who log in to New Relic in the same way. For example, you may have one authentication domain for users who log in via username/password and another authentication domain for users who log in via SAML. If a user is a basic user, this takes precedence over any role-related limitations. For more on this, see Basic user and roles.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 432.46658,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "How to <em>manage</em> <em>users</em>",
        "sections": "Give <em>users</em> access to <em>accounts</em> <em>and</em> roles (access grants)",
        "tags": "<em>New</em> <em>Relic</em> <em>One</em> <em>user</em> <em>management</em>",
        "body": "For users on our <em>New</em> <em>Relic</em> <em>One</em> <em>user</em> model, we provide various <em>user</em> <em>management</em> features, including the ability to: Use role based access control (RBAC) to assign default or custom roles to <em>user</em> groups Create custom <em>user</em> groups Grant <em>user</em> groups access to specific roles and <em>accounts</em> Important"
      },
      "id": "603e7bce28ccbc415beba74c"
    },
    {
      "sections": [
        "Introduction to user management",
        "New pricing model",
        "User management docs"
      ],
      "title": "Introduction to user management",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "New Relic One user management"
      ],
      "external_id": "6c2f39333fa3c6931fe616669244cb44f183a167",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/new-relic-one-user-management/introduction-managing-users/",
      "published_at": "2021-12-19T15:02:53Z",
      "updated_at": "2021-11-24T13:09:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New pricing model This doc is for users on our New Relic One user model. Some important things to note before starting: The docs in this section are for managing users on the New Relic One user model. Learn about user models. Note that this is a separate topic from our two different pricing models. For managing users on our original user model, see Original users. User management docs Here are our main docs for managing these users: User model/structure: learn some basic aspects of our user model, such as what basic users and full platform users are, how groups (like Admin and User) work, and how roles and capabilities work. How to manage users: an overview of user management concepts, where to manage users in the UI, and some common user management tasks. Authentication domain settings: configure an authentication domain, which governs how your users are added to New Relic (manually versus SCIM provisioning), the authentication method they use (manual login versus SAML SSO), managing how basic users become full platform users, and user session settings. For an overview of SAML SSO and SCIM options, see Introduction to SAML and SCIM. A tutorial on how to create access grants, which is how you give users access to roles and accounts. Want to understand how user count affects billing? See User-related billing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 431.77594,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>user</em> <em>management</em>",
        "sections": "Introduction to <em>user</em> <em>management</em>",
        "tags": "<em>New</em> <em>Relic</em> <em>One</em> <em>user</em> <em>management</em>",
        "body": "<em>New</em> pricing model This doc is for users on our <em>New</em> <em>Relic</em> <em>One</em> <em>user</em> model. Some important things to note before starting: The docs in this section are for managing users on the <em>New</em> <em>Relic</em> <em>One</em> <em>user</em> model. Learn about <em>user</em> models. Note that this is a separate topic from our two different pricing models"
      },
      "id": "6043f3c4196a67a215960f3c"
    },
    {
      "sections": [
        "SCIM API tutorial",
        "Requirements",
        "Overview",
        "Configure your authentication domain for SCIM",
        "Tip",
        "Create users and user groups in your system",
        "Connect to the SCIM API",
        "401 Unauthorized",
        "Create users in your authentication domain",
        "Important",
        "Example responses",
        "201 Created",
        "400 Bad Request",
        "409 Conflict",
        "Create groups in your authentication domain",
        "View users and groups in your authentication domain",
        "200 OK",
        "Update a user's attributes",
        "Update a group's members",
        "Delete users and groups",
        "Next steps",
        "Optional: Manage user type"
      ],
      "title": "SCIM API tutorial",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Automated user management",
        "New Relic One user management"
      ],
      "external_id": "ef29f5444770fc05d762fd27c4872a92660ec681",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/automated-user-management/tutorial-manage-users-groups-scim/",
      "published_at": "2021-12-19T15:40:05Z",
      "updated_at": "2021-11-24T11:08:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This tutorial will walk you through some common procedures for managing users on the New Relic One user model using the SCIM API. The SCIM API allows you to view, create, update, and delete users and groups programmatically, outside of the User management UI. Requirements Before using this tutorial, we recommend you read: The requirements for automated user management and using the SCIM API. Important user management concepts The primary SCIM API reference Other related resources: Some SCIM 2.0 RFC documents from the Internet Engineering Task Force that are most relevant: RFC 7643 - SCIM Core Resources and Extensions, RFC 7643 - JSON Representation, and RFC 7644 - SCIM Protocol. Overview This tutorial shows you how to accomplish some of the most common tasks needed for adding users to New Relic from an identity provider service and managing them from there. It is meant to supplement our primary SCIM API resource. Note that using automated user management means that your groups of users are imported into New Relic. This means that you can't use our user management UI to add users to groups. The groups are created and managed from your identity provider side. Once you're done with getting your user groups into New Relic, you must use our Organization and access UI to create access grants, which give your groups access to specific roles and/or accounts. For more information, see user management concepts. Configure your authentication domain for SCIM Before you can use the SCIM API, you must first enable SCIM for your authentication domain. Note that the API access token is displayed only once after you save the configuration, so save it somewhere safe for later user. Tip If you need to view a bearer token later, the only way to do this is to generate a new one, and that will invalidate the old one and any integrations using the old token. Create users and user groups in your system The SCIM API is typically used by scripts for importing users and groups into New Relic from a database or a third-party identity provider that doesn't have pre-configured configs for New Relic. If you want to use the SCIM API custom application or for ad-hoc requests, proceed to learn how to connect to the SCIM API. Connect to the SCIM API The SCIM API is available at https://scim-provisioning.service.newrelic.com/scim/v2 and this URL is viewable in the authentication domain settings page. To access the SCIM API, your client must include a bearer token with each request. The token is displayed after saving your Authentication Domain configuration. If you're using a third-party identity provider, configure it to use Bearer token authorization and plug in your API access token. Refer to your identity provider's documentation for help configuring this. Once configured, you're all set to import users and groups. Rather than reading the entire SCIM protocol RFCs, there are three specific sections you may find valuable: See RFC 7643 - SCIM Core Resources and Extensions and RFC 7643 - JSON Representation for the specifics. Refer to RFC 7644 - SCIM Protocol for more information about the protocol used in this tutorial. For all requests to the SCIM API, you must provide the bearer token in an Authorization header. Here's an example with curl: curl -X 'GET' -H 'Accept: application/json' -H \"Authorization: Bearer $TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Users' Copy Any request in the rest of this tutorial will receive a 401 Unauthorized response if the API access token is missing or invalid. Example response: 401 Unauthorized { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"detail\": \"invalid authentication token\", \"status\": \"401\" } Copy Create users in your authentication domain You can use the SCIM API to send a POST request to /scim/v2/Users to create a user. The following user attributes are required: userName This identifier must be unique within an authentication domain. Use the user's email address. emails Same as userName. The email address of the user. (Despite it being called emails, for this procedure enter only one.) active Boolean indicating whether or not the user should be active or inactive within New Relic. We recommend providing the following attributes for the best user experience: name.givenName The user's first or given name. name.familyName The user's last or family name. timezone The user's timezone in IANA Time Zone database format. curl -X 'POST' -H 'Content-Type: application/json' -H \"Authorization: Bearer $TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Users' --data-binary @- <<EOF { \"schemas\": [\"urn:ietf:params:scim:schemas:core:2.0:User\"], \"userName\": \"bjensen@example.com\", \"name\": { \"familyName\": \"Jensen\", \"givenName\": \"Barbara\" }, \"emails\": [ { \"primary\": true, \"value\": \"bjensen@example.com\" } ], \"active\": true, \"timezone\": \"America/Los_Angeles\" } EOF Copy Important Take note of the returned user id. To update a user in the future you'll need to supply the same ID with the request. Example responses 201 Created { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\", ], \"id\": \"5a1d580f-323c-450c-8c62-479b5c9085d6\", \"externalId\": null, \"userName\": \"bjensen@example.com\", \"name\": { \"familyName\": \"Jensen\", \"givenName\": \"Barbara\" }, \"emails\": [ { \"value\": \"bjensen@example.com\", \"primary\": true } ], \"timezone\": \"America/Los_Angeles\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2021-07-20T21:32:58.167Z\", \"lastModified\": \"2021-07-20T21:32:58.167Z\" }, \"groups\": [] } Copy 400 Bad Request { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"scimType\": \"invalidValue\", \"detail\": \"Validation failed: Username can't be blank\", \"status\": \"400\" } Copy 409 Conflict { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"scimType\": \"uniqueness\", \"detail\": \"Resource already exists\", \"status\": \"409\" } Copy Create groups in your authentication domain You can use the SCIM API to send a POST request to /scim/v2/Groups to create a group. The only group attribute required is: displayName The group name. curl -X 'POST' -H 'Content-Type: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Groups' --data-binary @- <<EOF { \"schemas\": [\"urn:ietf:params:scim:schemas:core:2.0:Group\"], \"displayName\": \"Example Group\" } EOF Copy Important Take note of the returned group id. To update a group or its members in the future you will need to supply the same ID with the request. Example responses 201 Created { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"df2b9a04-0426-4a3e-bf5f-54d5341f4e5b\", \"displayName\": \"Example Group\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2021-07-20T22:41:50.414Z\", \"lastModified\": \"2021-07-20T22:41:50.414Z\" }, \"members\": [] } Copy 400 Bad Request { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"scimType\": \"invalidValue\", \"detail\": \"Validation failed: Display name can't be blank\", \"status\": \"400\" } Copy 409 Conflict { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"scimType\": \"uniqueness\", \"detail\": \"Resource already exists\", \"status\": \"409\" } Copy View users and groups in your authentication domain After you've created some users and groups you'll see them in the User management UI. You can also retrieve them from the SCIM API. In this tutorial, you'll be searching for specific users and groups, but that's not the only way to view users and groups. Refer to the SCIM API reference and RFC 7644 for all the available query options. To retrieve a user by email, send a GET request to /scim/v2/Users with a filter query parameter. The filter parameter must be URL encoded. curl -X 'GET' -H 'Accept: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Users' --get --data-urlencode 'filter=userName eq \"bjensen@example.com\"' Copy Example response: 200 OK { \"totalResults\": 1, \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:ListResponse\" ], \"Resources\": [ { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\", ], \"id\": \"5a1d580f-323c-450c-8c62-479b5c9085d6\", \"externalId\": null, \"userName\": \"bjensen@example.com\", \"name\": { \"familyName\": \"Jensen\", \"givenName\": \"Barbara\" }, \"emails\": [ { \"value\": \"bjensen@example.com\", \"primary\": true } ], \"timezone\": \"America/Los_Angeles\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2021-07-20T21:32:58.167Z\", \"lastModified\": \"2021-07-20T21:32:58.167Z\" }, \"groups\": [] } ] } Copy Similarly, send a GET request to /scim/v2/Groups with a filter query parameter to retrieve a group by name. curl -X 'GET' -H 'Accept: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Groups' --get --data-urlencode 'filter=displayName eq \"Example Group\"' Copy Example response: 200 OK { \"totalResults\": 1, \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:ListResponse\" ], \"Resources\": [ { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"df2b9a04-0426-4a3e-bf5f-54d5341f4e5b\", \"displayName\": \"Example Group\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2021-07-20T22:41:50.414Z\", \"lastModified\": \"2021-07-20T22:41:50.414Z\" }, \"members\": [] } ] } Copy Update a user's attributes The SCIM API supports both PUT and PATCH methods for updating users. Refer to the SCIM API supported actions and RFC 7644 for details on using PATCH. This tutorial demonstrates updating a user's attributes with the PUT method. New Relic does not require all user attributes be included in the request body, only the attributes you want to update are necessary. Send a PUT request to /scim/v2/Users/${ID} to update the user. curl -X 'PUT' -H 'Content-Type: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Users/5a1d580f-323c-450c-8c62-479b5c9085d6' --data-binary @- <<EOF { \"schemas\": [\"urn:ietf:params:scim:schemas:core:2.0:User\"], \"timezone\": \"America/Chicago\" } EOF Copy Example responses 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\", ], \"id\": \"5a1d580f-323c-450c-8c62-479b5c9085d6\", \"externalId\": null, \"userName\": \"bjensen@example.com\", \"name\": { \"familyName\": \"Jensen\", \"givenName\": \"Barbara\" }, \"emails\": [ { \"value\": \"bjensen@example.com\", \"primary\": true } ], \"timezone\": \"America/Chicago\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2021-07-20T21:32:58.167Z\", \"lastModified\": \"2021-07-21T02:12:05.348Z\" }, \"groups\": [] } Copy 409 Conflict { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"scimType\": \"uniqueness\", \"detail\": \"Resource already exists\", \"status\": \"409\" } Copy Update a group's members The SCIM API supports both PUT and PATCH methods for updating groups. This tutorial will show how to update a group's members with the PATCH method. Refer to the SCIM API supported actions and RFC 7644 for details on using PUT. PATCH is convenient for adding or removing group members without needing to specify the full member list in the request. To add a user to a group, use the following operation parameters: op set to Add path set to members value set to a list of {\"value\": \"${USER_ID}\"} with each user ID to add to the group Send a PATCH request to /scim/v2/Groups/${ID} to update group members. curl -X 'PATCH' -H 'Content-Type: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Groups/df2b9a04-0426-4a3e-bf5f-54d5341f4e5b' --data-binary @- <<EOF { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:PatchOp\" ], \"Operations\": [{ \"op\": \"Add\", \"path\": \"members\", \"value\": [{ \"value\": \"5a1d580f-323c-450c-8c62-479b5c9085d6\" }] }] } EOF Copy Example response: 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"df2b9a04-0426-4a3e-bf5f-54d5341f4e5b\", \"displayName\": \"Example Group\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2021-07-20T22:41:50.414Z\", \"lastModified\": \"2021-07-20T22:41:50.414Z\" }, \"members\": [ { \"type\": \"User\", \"value\": \"5a1d580f-323c-450c-8c62-479b5c9085d6\" } ] } Copy To remove a user from a group, use the following operation parameters: op set to Remove path set to members value set to a list of {\"value\": \"${USER_ID}\"} with each user ID to remove from the group curl -X 'PATCH' -H 'Accept: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Groups/df2b9a04-0426-4a3e-bf5f-54d5341f4e5b' --data-binary @- <<EOF { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:PatchOp\" ], \"Operations\": [{ \"op\": \"Remove\", \"path\": \"members\", \"value\": [{ \"value\": \"5a1d580f-323c-450c-8c62-479b5c9085d6\" }] }] } EOF Copy Example response: 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"df2b9a04-0426-4a3e-bf5f-54d5341f4e5b\", \"displayName\": \"Example Group\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2021-07-20T22:41:50.414Z\", \"lastModified\": \"2021-07-20T22:41:50.414Z\" }, \"members\": [] } Copy Delete users and groups To remove a user from an authentication domain, send a DELETE request to /scim/v2/Users/${ID}. curl -X 'DELETE' -H 'Accept: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Users/d0f4d8e3-5413-4894-a8f9-de709994e18c' Copy Example response: 204 No Content Copy Similarly, to remove a group from your authentication domain, send a DELETE request to /scim/v2/Groups/${ID}. curl -X 'DELETE' -H 'Accept: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Groups/df2b9a04-0426-4a3e-bf5f-54d5341f4e5b' Copy Example response: 204 No Content Copy Next steps Once your integration is complete, potential next steps include: Your New Relic users will by default start out as basic users and you have the option to change some of them to full platform users. To do this, you can use the User management UI or use the SCIM API. Set up SAML SSO. Once your user groups are in New Relic, you'll need to assign access grants, which are what give your users access to specific roles and specific accounts. Learn more about access grants. Optional: Manage user type Once your SCIM API integration is complete, all users brought into New Relic start out as basic users. You can use our default method for managing user type, which is using the User management UI. Optionally, you can use our SCIM API instead. To do this, you can set update your authentication domain configuration to Delegate control of user type to your identity provider or custom application. The user's type attribute is defined in the custom schema urn:ietf:params:scim:schemas:extension:newrelic:2.0:User. Include this schema and the nrUserType string attribute in your create or update request to set a user's type. Valid values for nrUserType include: Full User (for full platform user) Basic User To create a new Basic User send a POST request /scim/v2/Users and include the custom New Relic schema extension: curl -X 'POST' -H 'Content-Type: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Users' --data-binary @- <<EOF { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\", \"urn:ietf:params:scim:schemas:extension:newrelic:2.0:User\" ], \"userName\": \"jbenson@example.com\", \"name\": { \"givenName\": \"James\", \"familyName\": \"Benson\" }, \"emails\": [{ \"primary\": true, \"value\": \"jbenson@example.com\", \"type\": \"work\" }], \"active\": true, \"timezone\": \"America/Chicago\", \"urn:ietf:params:scim:schemas:extension:newrelic:2.0:User\": { \"nrUserType\": \"Basic User\" } } EOF Copy Example responses 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\", \"urn:ietf:params:scim:schemas:extension:newrelic:2.0:User\" ], \"id\": \"8ab6c464-983c-4bb4-9507-720f28763a43\", \"externalId\": null, \"userName\": \"jbenson@example.com\", \"name\": { \"familyName\": \"Benson\", \"givenName\": \"James\" }, \"emails\": [ { \"value\": \"jbenson@example.com\", \"primary\": true } ], \"timezone\": \"America/Chicago\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2021-07-21T19:53:33.470Z\", \"lastModified\": \"2021-07-21T19:53:33.470Z\" }, \"groups\": [], \"urn:ietf:params:scim:schemas:extension:newrelic:2.0:User\": { \"nrUserType\": \"Basic User\" } } Copy 409 Conflict { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"scimType\": \"uniqueness\", \"detail\": \"Resource already exists\", \"status\": \"409\" } Copy 400 Bad Request { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"scimType\": \"invalidValue\", \"detail\": \"Validation failed: Username can't be blank\", \"status\": \"400\" } Copy To update a user's type, send a PUT request scim/v2/Users/${ID} and include the custom New Relic schema extension: curl -X 'PUT' -H 'Content-Type: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Users' --data-binary @- <<EOF { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\", \"urn:ietf:params:scim:schemas:extension:newrelic:2.0:User\" ], \"urn:ietf:params:scim:schemas:extension:newrelic:2.0:User\": { \"nrUserType\": \"Full User\" } } EOF Copy Example responses 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\", \"urn:ietf:params:scim:schemas:extension:newrelic:2.0:User\" ], \"id\": \"8ab6c464-983c-4bb4-9507-720f28763a43\", \"externalId\": null, \"userName\": \"jbenson@example.com\", \"name\": { \"familyName\": \"Benson\", \"givenName\": \"James\" }, \"emails\": [ { \"value\": \"jbenson@example.com\", \"primary\": true } ], \"timezone\": \"America/Chicago\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2021-07-21T19:53:33.470Z\", \"lastModified\": \"2021-07-21T20:15:56.718Z\" }, \"groups\": [], \"urn:ietf:params:scim:schemas:extension:newrelic:2.0:User\": { \"nrUserType\": \"Full User\" } } Copy 400 Bad Request { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"scimType\": \"invalidValue\", \"detail\": \"Validation failed: invalid nrUserType value provided\", \"status\": \"400\" } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 362.112,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Create <em>users</em> <em>and</em> <em>user</em> groups in your system",
        "tags": "<em>New</em> <em>Relic</em> <em>One</em> <em>user</em> <em>management</em>",
        "body": "This tutorial will walk you through some common procedures for managing users on the <em>New</em> <em>Relic</em> <em>One</em> <em>user</em> model using the SCIM API. The SCIM API allows you to view, create, update, and delete users and groups programmatically, outside of the <em>User</em> <em>management</em> UI. Requirements Before using this tutorial"
      },
      "id": "611fd560196a6791717ab5f9"
    }
  ],
  "/docs/accounts/accounts-billing/new-relic-one-user-management/user-mgmt-videos": [
    {
      "sections": [
        "How to manage users",
        "Important",
        "Requirements",
        "Manage users in the UI",
        "Overview of user management concepts",
        "Common user management tasks",
        "Add, edit, and delete users",
        "Edit user type (basic and full platform)",
        "Give users access to accounts and roles (access grants)",
        "Create new custom groups and roles",
        "Set up SAML SSO and/or SCIM provisioning",
        "Grant users ability to manage other users",
        "Control how basic users upgrade to full platform users",
        "Track changes",
        "User management terms and definitions"
      ],
      "title": "How to manage users",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "New Relic One user management"
      ],
      "external_id": "9e37836740ce56302734d4af636bdbe087b4dbc3",
      "image": "https://docs.newrelic.com/static/a78dad5ff794da5deaaf3b514e334ea7/c1b63/new-relic-one-user-mgmt.png",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/new-relic-one-user-management/add-manage-users-groups-roles/",
      "published_at": "2021-12-19T15:22:19Z",
      "updated_at": "2021-11-24T14:23:16Z",
      "document_type": "page",
      "popularity": 1,
      "body": "For users on our New Relic One user model, we provide various user management features, including the ability to: Use role based access control (RBAC) to assign default or custom roles to user groups Create custom user groups Grant user groups access to specific roles and accounts Important This doc applies to users on the New Relic One user model. For managing users on our original user model, see Original users. Not sure which user model you're on? See User models. Requirements To see if you can access these user management features, go to the user management UI and see what you have access to. Access requirements: These features allow managing of users on the New Relic One user model. To learn more, see User models. To avoid configuration conflicts, ensure that only one user is managing users at a time. Simultaneous configuration by more than one user may result in errors and unexpected results. Most capabilities require the Authentication domain manager role and some require the Organization manager role. For details, see Standard roles. Pricing edition requirements: To manage user groups, roles, and access grants: Pro or Enterprise edition is required. To import users from an identity provider: Enterprise is required. A New Relic user can have a maximum of either three concurrent active sessions, or three unique IP addresses in use at any given time. Manage users in the UI For users on the New Relic One user model, to find your user management options: From the account dropdown, select Administration. There are two user management UI pages there: User management: Use this to add users, update user type (basic versus full platform) and other information, and approve user type upgrade requests. Organization and access: Use this page to create and manage groups, roles, and access grants, and to set up SAML SSO and SCIM provisioning. Want to see videos of the user management UI in action? See our user management videos. Important If you can't see these UI pages, it may be because you're on our original user model or because you don't have the required user management role. Overview of user management concepts If your organization has Pro or Enterprise edition, you can create and configure access grants. An access grant gives a group of users access to a specific role on a specific account. Here's a diagram showing how access grants work and how they relate to the broader organization: A diagram explaining the concept of how access grants give a user group access to a) a role on b) a specific account. (Note that this applies to users on our New Relic One user model (and not our original user model).) When your New Relic organization is created, it starts out with some default access grants for our default User or Admin groups. Each of these groups is assigned one or more roles and granted access to the primary (first created) account: A view of the Organization and access UI, showing the default access grants associated with our default groups. If you have a relatively flat organizational structure, and are okay with all or many of your users having wide administrative access and access to all accounts, you'll probably only need at most a few access grants. For example, you might decide to add new access grants to the existing default Admin or User groups, giving those roles access to other accounts. Or, if you need more granular definition over roles and permissions, you'd create access grants that define new groups that have access to specific roles (either our standard roles or custom-defined roles). For a tutorial on how to create access grants and create custom roles, see the User management tutorial. For other examples of some common user management tasks, see Example tasks. To see the UI in action, see our user management videos. Some tips on setting up access grants: It may help to first plan out how your access grants will be organized. How many accounts will you have? What user groups will get access to which roles and which accounts? Will you use our default groups and roles or create your own custom groups and roles? If you've used automated user management to provision users via SCIM, you will have to create access grants to give those users access. A common configuration for organizations with many accounts (roughly 20 or more) is setting up groups with the more organization-scoped roles (Organization manager, Authentication domain manager, and Billing user) on the primary account, and then on other accounts, setting up groups with the more product-scoped roles (like All product admin, Standard user, or custom roles). Common user management tasks In the Organization and access UI, you can create access grants, custom groups, custom roles, and configure an authentication domain. Here are some example user management procedures: Add, edit, and delete users See the user management tutorial. Edit user type (basic and full platform) Note that there are limits around how many times full platform users can be changed to basic users. Important If you're using automated user management to provision and manage your users, you have other options for managing user type. To change the user type of one or more users: On the User management page, click the checkboxes for the users whose user type you want to edit. Once you start selecting users, an option will appear for Edit type. You can also edit the user type and group of a specific user by clicking on that user. Give users access to accounts and roles (access grants) See our user management tutorial. Create new custom groups and roles See the user management tutorial. Set up SAML SSO and/or SCIM provisioning See Get started with SAML SSO or SCIM. Grant users ability to manage other users To grant your users the ability to manage other users, you'll need to add users to a group that has the Authentication domain manager and Organization manager role. You have two options: From the User management UI, you can add a user to the default Admin group, which includes both of those roles. OR You can create a custom group and assign it these roles. For a tutorial on creating new access grants, groups, and roles, see the user management tutorial. Control how basic users upgrade to full platform users See the authentication domain settings. Track changes To see an audit log of changes to your account, including user management actions, you can query the NrAuditEvent. User management terms and definitions For an explanation of how user access to accounts and roles works, see User management concepts explained. Here are some definitions for the terms we use there: A New Relic organization is the representation of your organization, containing all your accounts, users, and data. For more information, see Organization and account structure. A capability is an ability to use or edit a specific, granular New Relic feature. Examples of capabilities: The ability to modify APM settings The ability to delete alert conditions A role is a set of capabilities. It is what gives a user their permissions. Our default standard roles have various capability sets, and you can create custom roles that have a custom set of capabilities. See some specific New Relic capabilities. A user group has one or more roles associated with it. You assign your users to a group. We have default user groups (Admin and User), and you can make your own groups. An access grant is what grants a user group access to roles and to specific New Relic accounts. An access grant essentially states, \"This group is assigned this role on this New Relic account.\" Adding a user to a group doesn’t do anything unless that group is included in an access grant. An authentication domain contains a set of users who are added to New Relic and who log in to New Relic in the same way. For example, you may have one authentication domain for users who log in via username/password and another authentication domain for users who log in via SAML. If a user is a basic user, this takes precedence over any role-related limitations. For more on this, see Basic user and roles.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 136.05707,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "How to <em>manage</em> <em>users</em>",
        "sections": "<em>Manage</em> <em>users</em> in the <em>UI</em>",
        "tags": "New Relic One <em>user</em> <em>management</em>",
        "body": ", roles, and access grants, and to set up SAML SSO and SCIM provisioning. Want to see <em>videos</em> of the <em>user</em> <em>management</em> <em>UI</em> in action? See our <em>user</em> <em>management</em> <em>videos</em>. Important If you can&#x27;t see these <em>UI</em> pages, it may be because you&#x27;re on our original <em>user</em> model or because you don&#x27;t have the required <em>user</em>"
      },
      "id": "603e7bce28ccbc415beba74c"
    },
    {
      "sections": [
        "Tutorials on user management tasks: access grants, custom roles, and adding users (New Relic One user model)",
        "Requirements",
        "Overview",
        "Add users",
        "View available accounts",
        "Grant access to accounts and roles",
        "Create custom role",
        "Add users to groups"
      ],
      "title": "Tutorials on user management tasks: access grants, custom roles, and adding users (New Relic One user model)",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "New Relic One user management"
      ],
      "external_id": "b1e5f303b6446f264c9d8c5020871a6990e052fd",
      "image": "https://docs.newrelic.com/static/a78dad5ff794da5deaaf3b514e334ea7/c1b63/new-relic-one-user-mgmt.png",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/new-relic-one-user-management/tutorial-add-new-user-groups-roles-new-relic-one-user-model/",
      "published_at": "2021-12-19T16:26:00Z",
      "updated_at": "2021-11-24T14:27:52Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This tutorial will walk you through some common procedures for managing users on the New Relic One user model. Here's a quick overview of the process (3:24 minutes): Requirements Some notes on requirements: This tutorial is for users on our New Relic One user model. Creating access grants requires Pro or Enterprise edition. For details, see user management requirements. Role requirements: Using the Organization and access UI to manage group access requires Authentication domain manager and Organization manager roles. Using the User management UI to add/edit users requires Authentication domain manager role. Overview Before using this tutorial, we recommend you check out: Understand what an access grant is Videos that show the user management UI in action This tutorial will walk you through how to: Add users View available accounts Grant groups access to roles and accounts Create custom roles Add users to groups Add users Adding users to New Relic is a separate process from creating access grants. In other words, you can do either procedure first. The key point is that if you're trying to give a user access to a role or an account that they don't yet have, they won't have that access until they're both a) in a group and b) that group has the correct access grant (access to a specific role on a specific account). There are two ways to add users: With the user management UI (described below) With automated user management, also known as SCIM provisioning: use of this method means that groups are defined in your identity provider and so you can't edit a user's group from the New Relic UI. To add users from the UI: From the top right of the New Relic UI, click the account dropdown, click Administration, and click User management. If you have multiple authentication domains, choose one from the authentication domain dropdown. Click Add user. Complete the process, including choosing user type and user group. Relevant tips: For how to bulk edit users' user type, see Edit user type. When choosing a group, you can choose either one of our default groups or a custom group that you've defined an access grant for. For more on creating access grants, keep reading. View available accounts When thinking about creating access grants for granting access to different accounts, it can help you to understand what accounts there are in your organization. To view the accounts in your organization: go to the account dropdown, click Administration, click Organizations and access, and then click Accounts. Grant access to accounts and roles Groups are used to group your users and manage what your users are able to do in New Relic: by creating an access grant, you assign a group access to a) a specific role on b) a specific account. By default, organizations on the New Relic One user model have two available groups: Admin and User. These default groups automatically have access to specific standard roles and are assigned to the account in which they were initially added. To view existing access grants: from the account dropdown, click Administration, and then click Organization and access. Even if you haven't created any custom access grants, you will see the default-created grants there that are present for our default groups. This is what you might see when you go to the Organization and access UI and view groups. The default available groups of Admin and User have access grants automatically created that grant users in those groups access to the roles associated with those groups and to the initial account those users were added in. To create a new access grant that gives a user group access to a role and an account: From the Organization and access UI, click Group access. If you don't see that UI, it may be because you're logged in with an original user model record, or because you don't have the ability to manage users. For more on such factors, see Factors affecting access. Choose one of the following: Existing: If there is already a group you want to add an access grant to, you can use this. For example, if you want to gives users in the default Admin or User group access to new accounts, you might choose this and then select the Admin Default or Admin User role. New: If you need to create a new group, choose this. You'll also need to choose the authentication domain that group is inside of (for an explanation of what \"Default\" means, see Default groups. If your organization strategy requires a good amount of restrictions over access to accounts and access to roles, you'll probably need to create a good amount of access grants. Next, under Access grant, you'll choose one of following: Account: Choose this to be able to select from the roles that are account-scoped. These are the roles that have to do with using and configuring our platform features (and not about organization and user management). Organization: Choose this to be able to select from the roles that are organization-scoped. These are the roles that govern organization- and user management. (Note that these users must also already belong to an account-scoped role. This is true for most users but if it's not, you may see a message that the user doesn't belong to an organization.) Select the Role you want to assign. Roles are organization-wide, so regardless of the authentication domain you're in, you have access to our standard roles and any custom roles you've created. For tips on selecting roles, see the tips after these instructions. Select the Account you want to add access to from the dropdown. If you don't see an account that you'd expect to see, this may be for a few reasons. One is that you yourself don't have the proper permissions for that account. Another is that that account is not actually in your organization. For more information, see Factors affecting access. If you are still having problems, talk to your account representative. If you want to continue adding more grants for that same group, select Add another at the bottom before clicking Add access. When you're done, if your users are already in the group you've added the grant to, they should have access within a few minutes (although for EU region New Relic accounts, this can take up to twenty minutes or so). If your users are not yet in that group (which would be true if you just created an access grant with a new group), you'll need to go to the User management UI and add one or more users to that group. Some tips for using this UI: If your users are managed via automated user management, you can't use the User management UI to add users to groups because your groups are imported from your identity provider. You will need to create access grants for those groups once they are in New Relic, though, to give those groups access. Note that if a user has the organization-scoped Organization manager and/or Authentication domain manager roles (which is true of users in our default Admin group) those users will always have those capabilities because those are organization-scoped abilities. This means that when you go to add those users to another account, you only have to add an account-scoped role, and not an organization-scoped role. In other words, once the users in a group have those organization-scoped roles, they will always have them for that organization unless removed. When selecting from amongst our standard roles, it's important to understand the difference between All product admin and Standard user. In short, All product admin is more popular a choice because it gives the ability to configure platform features. If you wanted to have your users be able to use platform features but not configure them, you'd choose Standard user. Create custom role When creating an access grant, you can use our standard roles, or you can create your own roles with unique sets of capabilities. To view existing roles: from the account dropdown, click Administration, then click Organization and access, and then click Roles. To create a custom role, click Add new custom role. Review the list of available capabilities and decide which ones your custom role needs. For more information about how roles and capabilities work, see Capabilities. Or check out this NerdByte video (4:07 minutes). Once you're finished creating a custom role, you will probably want to use it in a new access grant. Add users to groups In the user management UI, you can see your users and the groups they've been assigned to. Your users only have access to the access grants associated with the groups they've been added to. To view users and see their groups: from the account dropdown, click Administration, and click User management. If you don’t see that option, review the requirements. Groups reside within the boundaries of an authentication domain. If your organization has more than one authentication domain, the domain switcher in the top left will show which one you’re in. To add a user, click Add user. Complete the prompts in the UI, including choosing the user type and group. Any custom groups you’ve added should be available from the group dropdown. If the custom group you choose has been granted access to a role and an account, once you add the user to that group, that user will now have access. To edit a user’s group or other details: click on the user you want to edit and make changes.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 108.51696,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Tutorials on <em>user</em> <em>management</em> tasks: access grants, custom roles, and adding <em>users</em> (New Relic One <em>user</em> model)",
        "sections": "Tutorials on <em>user</em> <em>management</em> tasks: access grants, custom roles, and adding <em>users</em> (New Relic One <em>user</em> model)",
        "tags": "New Relic One <em>user</em> <em>management</em>",
        "body": " domain manager role. Overview Before using this tutorial, we recommend you check out: Understand what an access grant is <em>Videos</em> that show the <em>user</em> <em>management</em> <em>UI</em> in action This tutorial will walk you through how to: Add users View available accounts Grant groups access to roles and accounts Create custom"
      },
      "id": "603e7d67196a671e26a83dc5"
    },
    {
      "sections": [
        "SCIM API tutorial",
        "Requirements",
        "Overview",
        "Configure your authentication domain for SCIM",
        "Tip",
        "Create users and user groups in your system",
        "Connect to the SCIM API",
        "401 Unauthorized",
        "Create users in your authentication domain",
        "Important",
        "Example responses",
        "201 Created",
        "400 Bad Request",
        "409 Conflict",
        "Create groups in your authentication domain",
        "View users and groups in your authentication domain",
        "200 OK",
        "Update a user's attributes",
        "Update a group's members",
        "Delete users and groups",
        "Next steps",
        "Optional: Manage user type"
      ],
      "title": "SCIM API tutorial",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Automated user management",
        "New Relic One user management"
      ],
      "external_id": "ef29f5444770fc05d762fd27c4872a92660ec681",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/automated-user-management/tutorial-manage-users-groups-scim/",
      "published_at": "2021-12-19T15:40:05Z",
      "updated_at": "2021-11-24T11:08:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This tutorial will walk you through some common procedures for managing users on the New Relic One user model using the SCIM API. The SCIM API allows you to view, create, update, and delete users and groups programmatically, outside of the User management UI. Requirements Before using this tutorial, we recommend you read: The requirements for automated user management and using the SCIM API. Important user management concepts The primary SCIM API reference Other related resources: Some SCIM 2.0 RFC documents from the Internet Engineering Task Force that are most relevant: RFC 7643 - SCIM Core Resources and Extensions, RFC 7643 - JSON Representation, and RFC 7644 - SCIM Protocol. Overview This tutorial shows you how to accomplish some of the most common tasks needed for adding users to New Relic from an identity provider service and managing them from there. It is meant to supplement our primary SCIM API resource. Note that using automated user management means that your groups of users are imported into New Relic. This means that you can't use our user management UI to add users to groups. The groups are created and managed from your identity provider side. Once you're done with getting your user groups into New Relic, you must use our Organization and access UI to create access grants, which give your groups access to specific roles and/or accounts. For more information, see user management concepts. Configure your authentication domain for SCIM Before you can use the SCIM API, you must first enable SCIM for your authentication domain. Note that the API access token is displayed only once after you save the configuration, so save it somewhere safe for later user. Tip If you need to view a bearer token later, the only way to do this is to generate a new one, and that will invalidate the old one and any integrations using the old token. Create users and user groups in your system The SCIM API is typically used by scripts for importing users and groups into New Relic from a database or a third-party identity provider that doesn't have pre-configured configs for New Relic. If you want to use the SCIM API custom application or for ad-hoc requests, proceed to learn how to connect to the SCIM API. Connect to the SCIM API The SCIM API is available at https://scim-provisioning.service.newrelic.com/scim/v2 and this URL is viewable in the authentication domain settings page. To access the SCIM API, your client must include a bearer token with each request. The token is displayed after saving your Authentication Domain configuration. If you're using a third-party identity provider, configure it to use Bearer token authorization and plug in your API access token. Refer to your identity provider's documentation for help configuring this. Once configured, you're all set to import users and groups. Rather than reading the entire SCIM protocol RFCs, there are three specific sections you may find valuable: See RFC 7643 - SCIM Core Resources and Extensions and RFC 7643 - JSON Representation for the specifics. Refer to RFC 7644 - SCIM Protocol for more information about the protocol used in this tutorial. For all requests to the SCIM API, you must provide the bearer token in an Authorization header. Here's an example with curl: curl -X 'GET' -H 'Accept: application/json' -H \"Authorization: Bearer $TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Users' Copy Any request in the rest of this tutorial will receive a 401 Unauthorized response if the API access token is missing or invalid. Example response: 401 Unauthorized { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"detail\": \"invalid authentication token\", \"status\": \"401\" } Copy Create users in your authentication domain You can use the SCIM API to send a POST request to /scim/v2/Users to create a user. The following user attributes are required: userName This identifier must be unique within an authentication domain. Use the user's email address. emails Same as userName. The email address of the user. (Despite it being called emails, for this procedure enter only one.) active Boolean indicating whether or not the user should be active or inactive within New Relic. We recommend providing the following attributes for the best user experience: name.givenName The user's first or given name. name.familyName The user's last or family name. timezone The user's timezone in IANA Time Zone database format. curl -X 'POST' -H 'Content-Type: application/json' -H \"Authorization: Bearer $TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Users' --data-binary @- <<EOF { \"schemas\": [\"urn:ietf:params:scim:schemas:core:2.0:User\"], \"userName\": \"bjensen@example.com\", \"name\": { \"familyName\": \"Jensen\", \"givenName\": \"Barbara\" }, \"emails\": [ { \"primary\": true, \"value\": \"bjensen@example.com\" } ], \"active\": true, \"timezone\": \"America/Los_Angeles\" } EOF Copy Important Take note of the returned user id. To update a user in the future you'll need to supply the same ID with the request. Example responses 201 Created { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\", ], \"id\": \"5a1d580f-323c-450c-8c62-479b5c9085d6\", \"externalId\": null, \"userName\": \"bjensen@example.com\", \"name\": { \"familyName\": \"Jensen\", \"givenName\": \"Barbara\" }, \"emails\": [ { \"value\": \"bjensen@example.com\", \"primary\": true } ], \"timezone\": \"America/Los_Angeles\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2021-07-20T21:32:58.167Z\", \"lastModified\": \"2021-07-20T21:32:58.167Z\" }, \"groups\": [] } Copy 400 Bad Request { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"scimType\": \"invalidValue\", \"detail\": \"Validation failed: Username can't be blank\", \"status\": \"400\" } Copy 409 Conflict { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"scimType\": \"uniqueness\", \"detail\": \"Resource already exists\", \"status\": \"409\" } Copy Create groups in your authentication domain You can use the SCIM API to send a POST request to /scim/v2/Groups to create a group. The only group attribute required is: displayName The group name. curl -X 'POST' -H 'Content-Type: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Groups' --data-binary @- <<EOF { \"schemas\": [\"urn:ietf:params:scim:schemas:core:2.0:Group\"], \"displayName\": \"Example Group\" } EOF Copy Important Take note of the returned group id. To update a group or its members in the future you will need to supply the same ID with the request. Example responses 201 Created { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"df2b9a04-0426-4a3e-bf5f-54d5341f4e5b\", \"displayName\": \"Example Group\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2021-07-20T22:41:50.414Z\", \"lastModified\": \"2021-07-20T22:41:50.414Z\" }, \"members\": [] } Copy 400 Bad Request { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"scimType\": \"invalidValue\", \"detail\": \"Validation failed: Display name can't be blank\", \"status\": \"400\" } Copy 409 Conflict { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"scimType\": \"uniqueness\", \"detail\": \"Resource already exists\", \"status\": \"409\" } Copy View users and groups in your authentication domain After you've created some users and groups you'll see them in the User management UI. You can also retrieve them from the SCIM API. In this tutorial, you'll be searching for specific users and groups, but that's not the only way to view users and groups. Refer to the SCIM API reference and RFC 7644 for all the available query options. To retrieve a user by email, send a GET request to /scim/v2/Users with a filter query parameter. The filter parameter must be URL encoded. curl -X 'GET' -H 'Accept: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Users' --get --data-urlencode 'filter=userName eq \"bjensen@example.com\"' Copy Example response: 200 OK { \"totalResults\": 1, \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:ListResponse\" ], \"Resources\": [ { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\", ], \"id\": \"5a1d580f-323c-450c-8c62-479b5c9085d6\", \"externalId\": null, \"userName\": \"bjensen@example.com\", \"name\": { \"familyName\": \"Jensen\", \"givenName\": \"Barbara\" }, \"emails\": [ { \"value\": \"bjensen@example.com\", \"primary\": true } ], \"timezone\": \"America/Los_Angeles\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2021-07-20T21:32:58.167Z\", \"lastModified\": \"2021-07-20T21:32:58.167Z\" }, \"groups\": [] } ] } Copy Similarly, send a GET request to /scim/v2/Groups with a filter query parameter to retrieve a group by name. curl -X 'GET' -H 'Accept: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Groups' --get --data-urlencode 'filter=displayName eq \"Example Group\"' Copy Example response: 200 OK { \"totalResults\": 1, \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:ListResponse\" ], \"Resources\": [ { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"df2b9a04-0426-4a3e-bf5f-54d5341f4e5b\", \"displayName\": \"Example Group\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2021-07-20T22:41:50.414Z\", \"lastModified\": \"2021-07-20T22:41:50.414Z\" }, \"members\": [] } ] } Copy Update a user's attributes The SCIM API supports both PUT and PATCH methods for updating users. Refer to the SCIM API supported actions and RFC 7644 for details on using PATCH. This tutorial demonstrates updating a user's attributes with the PUT method. New Relic does not require all user attributes be included in the request body, only the attributes you want to update are necessary. Send a PUT request to /scim/v2/Users/${ID} to update the user. curl -X 'PUT' -H 'Content-Type: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Users/5a1d580f-323c-450c-8c62-479b5c9085d6' --data-binary @- <<EOF { \"schemas\": [\"urn:ietf:params:scim:schemas:core:2.0:User\"], \"timezone\": \"America/Chicago\" } EOF Copy Example responses 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\", ], \"id\": \"5a1d580f-323c-450c-8c62-479b5c9085d6\", \"externalId\": null, \"userName\": \"bjensen@example.com\", \"name\": { \"familyName\": \"Jensen\", \"givenName\": \"Barbara\" }, \"emails\": [ { \"value\": \"bjensen@example.com\", \"primary\": true } ], \"timezone\": \"America/Chicago\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2021-07-20T21:32:58.167Z\", \"lastModified\": \"2021-07-21T02:12:05.348Z\" }, \"groups\": [] } Copy 409 Conflict { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"scimType\": \"uniqueness\", \"detail\": \"Resource already exists\", \"status\": \"409\" } Copy Update a group's members The SCIM API supports both PUT and PATCH methods for updating groups. This tutorial will show how to update a group's members with the PATCH method. Refer to the SCIM API supported actions and RFC 7644 for details on using PUT. PATCH is convenient for adding or removing group members without needing to specify the full member list in the request. To add a user to a group, use the following operation parameters: op set to Add path set to members value set to a list of {\"value\": \"${USER_ID}\"} with each user ID to add to the group Send a PATCH request to /scim/v2/Groups/${ID} to update group members. curl -X 'PATCH' -H 'Content-Type: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Groups/df2b9a04-0426-4a3e-bf5f-54d5341f4e5b' --data-binary @- <<EOF { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:PatchOp\" ], \"Operations\": [{ \"op\": \"Add\", \"path\": \"members\", \"value\": [{ \"value\": \"5a1d580f-323c-450c-8c62-479b5c9085d6\" }] }] } EOF Copy Example response: 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"df2b9a04-0426-4a3e-bf5f-54d5341f4e5b\", \"displayName\": \"Example Group\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2021-07-20T22:41:50.414Z\", \"lastModified\": \"2021-07-20T22:41:50.414Z\" }, \"members\": [ { \"type\": \"User\", \"value\": \"5a1d580f-323c-450c-8c62-479b5c9085d6\" } ] } Copy To remove a user from a group, use the following operation parameters: op set to Remove path set to members value set to a list of {\"value\": \"${USER_ID}\"} with each user ID to remove from the group curl -X 'PATCH' -H 'Accept: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Groups/df2b9a04-0426-4a3e-bf5f-54d5341f4e5b' --data-binary @- <<EOF { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:PatchOp\" ], \"Operations\": [{ \"op\": \"Remove\", \"path\": \"members\", \"value\": [{ \"value\": \"5a1d580f-323c-450c-8c62-479b5c9085d6\" }] }] } EOF Copy Example response: 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:Group\" ], \"id\": \"df2b9a04-0426-4a3e-bf5f-54d5341f4e5b\", \"displayName\": \"Example Group\", \"meta\": { \"resourceType\": \"Group\", \"created\": \"2021-07-20T22:41:50.414Z\", \"lastModified\": \"2021-07-20T22:41:50.414Z\" }, \"members\": [] } Copy Delete users and groups To remove a user from an authentication domain, send a DELETE request to /scim/v2/Users/${ID}. curl -X 'DELETE' -H 'Accept: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Users/d0f4d8e3-5413-4894-a8f9-de709994e18c' Copy Example response: 204 No Content Copy Similarly, to remove a group from your authentication domain, send a DELETE request to /scim/v2/Groups/${ID}. curl -X 'DELETE' -H 'Accept: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Groups/df2b9a04-0426-4a3e-bf5f-54d5341f4e5b' Copy Example response: 204 No Content Copy Next steps Once your integration is complete, potential next steps include: Your New Relic users will by default start out as basic users and you have the option to change some of them to full platform users. To do this, you can use the User management UI or use the SCIM API. Set up SAML SSO. Once your user groups are in New Relic, you'll need to assign access grants, which are what give your users access to specific roles and specific accounts. Learn more about access grants. Optional: Manage user type Once your SCIM API integration is complete, all users brought into New Relic start out as basic users. You can use our default method for managing user type, which is using the User management UI. Optionally, you can use our SCIM API instead. To do this, you can set update your authentication domain configuration to Delegate control of user type to your identity provider or custom application. The user's type attribute is defined in the custom schema urn:ietf:params:scim:schemas:extension:newrelic:2.0:User. Include this schema and the nrUserType string attribute in your create or update request to set a user's type. Valid values for nrUserType include: Full User (for full platform user) Basic User To create a new Basic User send a POST request /scim/v2/Users and include the custom New Relic schema extension: curl -X 'POST' -H 'Content-Type: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Users' --data-binary @- <<EOF { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\", \"urn:ietf:params:scim:schemas:extension:newrelic:2.0:User\" ], \"userName\": \"jbenson@example.com\", \"name\": { \"givenName\": \"James\", \"familyName\": \"Benson\" }, \"emails\": [{ \"primary\": true, \"value\": \"jbenson@example.com\", \"type\": \"work\" }], \"active\": true, \"timezone\": \"America/Chicago\", \"urn:ietf:params:scim:schemas:extension:newrelic:2.0:User\": { \"nrUserType\": \"Basic User\" } } EOF Copy Example responses 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\", \"urn:ietf:params:scim:schemas:extension:newrelic:2.0:User\" ], \"id\": \"8ab6c464-983c-4bb4-9507-720f28763a43\", \"externalId\": null, \"userName\": \"jbenson@example.com\", \"name\": { \"familyName\": \"Benson\", \"givenName\": \"James\" }, \"emails\": [ { \"value\": \"jbenson@example.com\", \"primary\": true } ], \"timezone\": \"America/Chicago\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2021-07-21T19:53:33.470Z\", \"lastModified\": \"2021-07-21T19:53:33.470Z\" }, \"groups\": [], \"urn:ietf:params:scim:schemas:extension:newrelic:2.0:User\": { \"nrUserType\": \"Basic User\" } } Copy 409 Conflict { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"scimType\": \"uniqueness\", \"detail\": \"Resource already exists\", \"status\": \"409\" } Copy 400 Bad Request { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"scimType\": \"invalidValue\", \"detail\": \"Validation failed: Username can't be blank\", \"status\": \"400\" } Copy To update a user's type, send a PUT request scim/v2/Users/${ID} and include the custom New Relic schema extension: curl -X 'PUT' -H 'Content-Type: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\" 'https://scim-provisioning.service.newrelic.com/scim/v2/Users' --data-binary @- <<EOF { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\", \"urn:ietf:params:scim:schemas:extension:newrelic:2.0:User\" ], \"urn:ietf:params:scim:schemas:extension:newrelic:2.0:User\": { \"nrUserType\": \"Full User\" } } EOF Copy Example responses 200 OK { \"schemas\": [ \"urn:ietf:params:scim:schemas:core:2.0:User\", \"urn:ietf:params:scim:schemas:extension:newrelic:2.0:User\" ], \"id\": \"8ab6c464-983c-4bb4-9507-720f28763a43\", \"externalId\": null, \"userName\": \"jbenson@example.com\", \"name\": { \"familyName\": \"Benson\", \"givenName\": \"James\" }, \"emails\": [ { \"value\": \"jbenson@example.com\", \"primary\": true } ], \"timezone\": \"America/Chicago\", \"active\": true, \"meta\": { \"resourceType\": \"User\", \"created\": \"2021-07-21T19:53:33.470Z\", \"lastModified\": \"2021-07-21T20:15:56.718Z\" }, \"groups\": [], \"urn:ietf:params:scim:schemas:extension:newrelic:2.0:User\": { \"nrUserType\": \"Full User\" } } Copy 400 Bad Request { \"schemas\": [ \"urn:ietf:params:scim:api:messages:2.0:Error\" ], \"scimType\": \"invalidValue\", \"detail\": \"Validation failed: invalid nrUserType value provided\", \"status\": \"400\" } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 103.4445,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Optional: <em>Manage</em> <em>user</em> type",
        "tags": "Automated <em>user</em> <em>management</em>",
        "body": "This tutorial will walk you through some common procedures for managing users on the New Relic One <em>user</em> model using the SCIM API. The SCIM API allows you to view, create, update, and delete users and groups programmatically, outside of the <em>User</em> <em>management</em> <em>UI</em>. Requirements Before using this tutorial"
      },
      "id": "611fd560196a6791717ab5f9"
    }
  ],
  "/docs/accounts/install-new-relic/partner-based-installation/google-app-engine-environment": [
    {
      "sections": [
        "Log in to and install New Relic via partners",
        "Create a New Relic account (all partners)",
        "Sign into and deploy New Relic (most partners)",
        "Sign into and deploy New Relic (special partners)",
        "Tip",
        "A - M partners",
        "Amazon Web Services (AWS)",
        "Google Cloud Platform (GCP)",
        "Heroku",
        "Magento",
        "N - Z partners",
        "Pantheon",
        "Rackspace Cloud Tools",
        "W3 Edge and WordPress",
        "Windows Azure"
      ],
      "title": "Log in to and install New Relic via partners",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Partner installation"
      ],
      "external_id": "d6c766c0328d09f6208426396d5443d115029fcc",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/install-new-relic/partner-based-installation/log-install-new-relic-partners/",
      "published_at": "2021-12-19T14:45:50Z",
      "updated_at": "2021-09-14T05:47:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you signed up for New Relic through one of New Relic's partners, you may need to follow additional single sign on (SSO) procedures to log in to start using New Relic. There also may be special steps needed to complete the deployment process. This document provides basic login procedures for accounts created through New Relic partners, with links to additional resources as applicable. Create a New Relic account (all partners) If you have received a special offer from a New Relic partner, follow the partner's link directly to create a New Relic account. Sign into and deploy New Relic (most partners) You will receive an email confirmation message with a link to confirm your account, sign into New Relic, and complete the deployment process. Sign into and deploy New Relic (special partners) These partners require specific login and deployment procedures for you to use your New Relic account. Tip If you signed up for a New Relic account through a partner that is not listed here, follow standard procedures to sign into and deploy New Relic. A - M partners Amazon Web Services (AWS) Different installation and page viewing procedures apply, depending on whether you want to install AWS for your app or your host. For more information, see Amazon Web Services (AWS) users. Google Cloud Platform (GCP) As a Google Cloud Platform (GCP) partner, New Relic supports the App Engine flexible environment. Each APM agent has its own requirements for the GAE flex environment. After you install the New Relic agent, you can view your app performance with APM. Heroku Heroku is a Platform as a Service (PaaS) solution capable of hosting web applications. With the New Relic Heroku add-on, you can extend Heroku with metrics from APM and browser monitoring. The New Relic add-on supports Java, Node.js, PHP, Python, and Ruby. For more information about installing and using New Relic with Heroku, see: Heroku dev center documentation New Relic's documentation for installing Heroku Magento Magento has partnered with New Relic to give merchants faster access to data and out-of-the-box tools. The New Relic Reporting Extension helps e-commerce businesses to make data-driven decisions. To create a New Relic account with Magento for your PHP app: Sign up via your Magento sales solution provider. After you sign up for your New Relic account, deploy the PHP agent to your Magento installation. Wait a few minutes, then check your app's performance in the APM Overview page. N - Z partners Pantheon To create a New Relic account through Pantheon and sign into New Relic: From Pantheon's partner page with New Relic, select the sign up link. Follow the online instructions to complete the installation process. Wait for New Relic to begin collecting data for your app. After a few minutes, you can check your app's performance in the APM Overview page. For more information, see Pantheon's documentation. Rackspace Cloud Tools To create an account through Rackspace Cloud Tools and sign into New Relic, follow these basic procedures. For more information, see Rackspace Cloud Load Balancer plugin. Log in to your CloudTools account at cloudtools.rackspace.com/myapps. From Use Your Applications, select New Relic. Select your agent language (Java, .NET, PHP, Python, Ruby), then follow the online instructions to complete the installation process. Wait for New Relic to begin collecting data for your app. Wait a few minutes, then check your app's performance in the APM Overview page. W3 Edge and WordPress Tip To create an account through W3 Total Cache and sign into New Relic, you must install and run the W3 PHP agent, not just get an API key. Deploying the agent requires administrative access, including: SSH/command line access Root or sudo access to run the New Relic installer and restart your web server If you are running an economy, shared hosting, or blogger package, or if you are not sure what are your account permissions, contact your hosting provider. For example, if your WordPress site is on shared servers, you may not be able to install New Relic. Your provider should be able to give you information about your account permissions, or upgrade your account if necessary. To install the New Relic plugin for W3 Edge/Wordpress users: Log in to your WordPress account at wordpress.org/support/bb-login.php. Follow New Relic's instructions to complete the installation process, or contact W3TC support for assistance. Wait for New Relic to begin collecting data. Within a few minutes after deploying the agent, performance data about your app will appear on APM's Applications Overview page. Tip WordPress also offers a plugin for our browser monitoring. Windows Azure Windows Azure is a Platform as a Service (PaaS) solution capable of hosting web applications using various languages, including .NET and Node.js. For more information about using our .NET agent with Azure, see: Installing the .NET agent on Azure Azure Preview Portal Azure Web Apps Azure Cloud Services For more information about installing and using our Node.js agent with Azure, see Node.js agent on Microsoft Azure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.00789,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Log in to <em>and</em> <em>install</em> New Relic via <em>partners</em>",
        "sections": "Log in to <em>and</em> <em>install</em> New Relic via <em>partners</em>",
        "tags": "<em>Accounts</em> <em>and</em> <em>billing</em>",
        "body": " for <em>accounts</em> created through New Relic partners, with links to additional resources as applicable. Create a New Relic <em>account</em> (all partners) If you have received a special offer from a New Relic <em>partner</em>, follow the <em>partner</em>&#x27;s link directly to create a New Relic <em>account</em>. Sign into and deploy New Relic (most"
      },
      "id": "6043f447e7b9d2f5295799e5"
    },
    {
      "sections": [
        "RightScale users and New Relic",
        "Add New Relic RightScripts",
        "Example PHP RightScript installation",
        "View APM monitoring data",
        "Upgrade your New Relic subscription"
      ],
      "title": "RightScale users and New Relic",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Partner installation"
      ],
      "external_id": "53775c11e82931aa08ef2d107aaa3f023368559b",
      "image": "https://docs.newrelic.com/static/84cd6a6ee1fe0ce671be3d3d710ab5a4/8c557/screen-nr-rightscale-applist.png",
      "url": "https://docs.newrelic.com/docs/accounts/install-new-relic/partner-based-installation/rightscale-users-new-relic/",
      "published_at": "2021-12-19T14:15:22Z",
      "updated_at": "2021-09-14T05:47:35Z",
      "document_type": "page",
      "popularity": 1,
      "body": "RightScale is a Software as a Service (SaaS) solution capable of hosting web applications. Supported languages for both New Relic and RightScale include Java, Ruby, .NET, PHP, and Python. In order to use New Relic through the RightScale partnership, you need a New Relic account. Before getting started, be sure to enable API access. Add New Relic RightScripts Before you can view data from New Relic in RightScale, add the appropriate New Relic RightScript to your ServerTemplates: From RightCloud, select Design > MultiCloud Marketplace > ServerTemplates. Import the New Relic Performance Monitoring Toolbox ServerTemplate into your Local view. After you import the ServerTemplate, you can use the associated New Relic RightScripts with your account and add them to your ServerTemplates as appropriate for your agent language. New Relic RightScript Notes Java Tomcat For use with Tomcat5 and Tomcat6 ServerTemplates. Installs our Java agent and configures it for use with Tomcat with APM. .NET Installs and configures the New Relic agent to monitor .NET applications. Using New Relic for .NET requires .NET 3.5 or higher on the target host. (If you have only 4.0 or higher installed, you must have .NET Version 3.5 installed on the target host.) With the 3.5 framework, New Relic instruments applications targeting .NET frameworks 2.0 or higher running on Windows Server 2003, Windows Server 2008, Windows Vista, Windows 7, and Windows Azure. PHP Installs and configures the New Relic PHP agent and a local proxy daemon. New Relic supports PHP versions 5.2 or higher on Apache 2.2 or 2.4. Python To install and configure the New Relic Python agent, do not use RightScripts. Instead, send an email to rightscale@newrelic.com. Ruby To use the New Relic agent for your Ruby on Rails application, see New Relic for Ruby. Example PHP RightScript installation Here is an example of adding the APP New Relic RPM for PHP RightScripts to a PHP application server. Add the APP New Relic RPM for PHP RightScript as a boot script to the ServerTemplate that is being used to run your PHP application servers. This will automatically deploy the New Relic agent when a new server is launched with that ServerTemplate. If your PHP application is already running on a server, you can add it as an operational script under the Scripts tab, and run the action on the running server. Be sure to define the following input value for the new script: NR_License_Key: Enter your New Relic license key. Copy You can either manually enter the license key or create a credential for it. View APM monitoring data When viewing the data that New Relic monitors, you will only see the names of the applications that are on servers where you have run the New Relic RightScript. A host is a server and an instance is an instance of an application. RightScale users: Example of apps you can view in New Relic. To view your New Relic data: Go to Reports > New Relic. From the list of application(s) in your New Relic RightScript deployment, select an application name. On your first login, be sure to change your password: From the (account dropdown in the New Relic UI, select User preferences. Use the New Relic menu options to view different performance data and charts. Upgrade your New Relic subscription For RightScale customers, all new accounts start with a free 14-day trial of New Relic's paid subscription features. If you want to upgrade your free account to a paid subscription, send an email to rightscale@newrelic.com. You cannot upgrade your New Relic subscription within RightScale.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.00787,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "RightScale users <em>and</em> New Relic",
        "sections": "Example PHP RightScript <em>installation</em>",
        "tags": "<em>Accounts</em> <em>and</em> <em>billing</em>",
        "body": " customers, all new <em>accounts</em> start with a free 14-day trial of New Relic&#x27;s paid subscription features. If you want to upgrade your free <em>account</em> to a paid subscription, send an email to rightscale@newrelic.com. You cannot upgrade your New Relic subscription within RightScale."
      },
      "id": "603eb08b28ccbc289aeba7a2"
    },
    {
      "sections": [
        "Heroku: Install the New Relic add-on",
        "Install the New Relic add-on",
        "Configure the agent",
        "Java",
        "Node.js",
        "PHP",
        "Python",
        "Ruby",
        "Manage your Heroku add-on accounts",
        "License key",
        "Account URL",
        "Account ID",
        "Log on to New Relic",
        "Sign in to New Relic add-on account through Heroku",
        "Sign in to regular New Relic accounts",
        "Set up deployment notifications",
        "Important"
      ],
      "title": "Heroku: Install the New Relic add-on",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Partner installation"
      ],
      "external_id": "45ca4960759150545d4f9e586555d6e15daef3ec",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/install-new-relic/partner-based-installation/heroku-install-new-relic-add/",
      "published_at": "2021-12-19T15:03:36Z",
      "updated_at": "2021-09-14T05:46:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Heroku is a Platform as a Service (PaaS) solution for hosting web applications in various agent languages. With New Relic, you can extend Heroku with metrics from our monitoring solutions, like APM and browser monitoring. The New Relic add-on supports Java, Node.js, PHP, Python, and Ruby. Install the New Relic add-on To set up the New Relic add-on, see Heroku's documentation, including available plan levels and Toolbelt procedures. If you have problems, use the Heroku support channels. Configure the agent After you install the New Relic add-on for Heroku, follow the configuration procedures for your APM agent. Java The minimum agent version for Java is 3.17.1 or higher. To install and configure New Relic's Java agent for your add-on, see our Java agent and Heroku documentation. Node.js To install and configure our Node.js agent for your add-on, see: New Relic's Node.js and Heroku documentation Blog post (2013) with a \"real world\" example of installing our Node.js agent for a Heroku app PHP To install and configure our PHP agent for your add-on, see our PHP agent and Heroku documentation. Python To install and configure our Python agent for your add-on, see our Python agent and Heroku documentation. Ruby To install and configure our Ruby agent for your add-on, see our Ruby agent and Heroku documentation. If you are using our Ruby agent with a non-Rails application, Heroku users need to install the plugin in your repository manually. For example, in a Sinatra app, add the newrelic gem to your Gemfile, and then add the following code to your app: configure :production do require 'newrelic_rpm' end Copy Manage your Heroku add-on accounts Every time you install the New Relic add-on for Heroku, New Relic will automatically create a new account, complete with a unique license key, account URL, and account ID. These three types of information are important for managing each of your Heroku add-on accounts. License key The license key identifies the account where your application reports. To check the license key your app is using: From a command line, run: heroku config | grep -i relic Copy Look for the value of NEW_RELIC_LICENSE_KEY. This license key environment variable will override any other license key that you hard-code into your New Relic config file. Account URL If you install the New Relic add-on multiple times and need to verify the URL your Heroku app uses for reporting to New Relic, look in your agent logs for a line indicating reporting to following by a URL using this format: rpm.newrelic.com/accounts/###/applications/###### Copy Account ID You cannot change your add-on's account ID directly. If you need to change the New Relic account your Heroku app uses for reporting to New Relic, change the current license key environment variable in its config file so that it points to the license key of the New Relic account you want to use: heroku config:set NEW_RELIC_LICENSE_KEY=changed_account_license_key Copy Log on to New Relic Heroku customers may have two different types of accounts with New Relic: Add-on accounts: New Relic accounts that customers installed on their Heroku application by using Heroku's New Relic add-on \"Regular\" accounts: Other New Relic accounts that customers did not install with Heroku's add-on Regular accounts provide a wider range of features than do add-on accounts, and are installed and managed differently. Different procedures apply, depending on which type of account you want to sign into. In accordance with the terms of New Relic's partnership with Heroku, customers who install New Relic via the Heroku add-on can only access their New Relic add-on accounts by signing in through Heroku. For this reason, if you have both add-on accounts and regular New Relic accounts, you cannot switch directly between them. Sign in to New Relic add-on account through Heroku To sign in to your New Relic add-on accounts: Sign in through Heroku's login page at id.heroku.com/login. Select the application that has the New Relic add-on installed. Select New Relic from your list of add-ons. If you sign in through Heroku, you will not see any of your regular New Relic accounts when you select account dropdown > Switch account. Sign in to regular New Relic accounts To sign into or switch between your regular New Relic accounts: Sign in to New Relic at one.newrelic.com. To switch from one regular New Relic account to another: Go to: **account dropdown and select an account. If you sign in directly through New Relic, you will not see any of your New Relic add-on accounts from Heroku when you select account dropdown > Switch account. Set up deployment notifications The Heroku add-on automatically sends deployment notifications to New Relic for one application per account. If you add multiple applications to your add-on account, you must use the New Relic REST API to manually send deployment notifications for your additional applications. You cannot use a post-deploy hook, because the New Relic REST API call requires a header, and Heroku's post-deploy hook does not allow headers. However, you can write a script that generates this API call whenever you deploy on Heroku. For instructions on recording deployments via the REST API, see Recording deployments. Important When you add a user to the Heroku add-on, this creates a user record for the user at New Relic. However, if you remove the user from the Heroku add-on, the user record is not automatically removed from New Relic. Instead, you must also manually remove the New Relic user record after removing the user from the Heroku add-on. You can do this by going to the User Management page.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.00778,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Heroku: <em>Install</em> the New Relic add-on",
        "sections": "Manage your Heroku add-on <em>accounts</em>",
        "tags": "<em>Accounts</em> <em>and</em> <em>billing</em>",
        "body": " the newrelic gem to your Gemfile, and then add the following code to your app: configure :production do require &#x27;newrelic_rpm&#x27; end Copy Manage your Heroku add-on <em>accounts</em> Every time you install the New Relic add-on for Heroku, New Relic will automatically create a new <em>account</em>, complete with a unique license"
      },
      "id": "603ebc9ae7b9d272c32a0810"
    }
  ],
  "/docs/accounts/install-new-relic/partner-based-installation/heroku-install-new-relic-add": [
    {
      "sections": [
        "RightScale users and New Relic",
        "Add New Relic RightScripts",
        "Example PHP RightScript installation",
        "View APM monitoring data",
        "Upgrade your New Relic subscription"
      ],
      "title": "RightScale users and New Relic",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Partner installation"
      ],
      "external_id": "53775c11e82931aa08ef2d107aaa3f023368559b",
      "image": "https://docs.newrelic.com/static/84cd6a6ee1fe0ce671be3d3d710ab5a4/8c557/screen-nr-rightscale-applist.png",
      "url": "https://docs.newrelic.com/docs/accounts/install-new-relic/partner-based-installation/rightscale-users-new-relic/",
      "published_at": "2021-12-19T14:15:22Z",
      "updated_at": "2021-09-14T05:47:35Z",
      "document_type": "page",
      "popularity": 1,
      "body": "RightScale is a Software as a Service (SaaS) solution capable of hosting web applications. Supported languages for both New Relic and RightScale include Java, Ruby, .NET, PHP, and Python. In order to use New Relic through the RightScale partnership, you need a New Relic account. Before getting started, be sure to enable API access. Add New Relic RightScripts Before you can view data from New Relic in RightScale, add the appropriate New Relic RightScript to your ServerTemplates: From RightCloud, select Design > MultiCloud Marketplace > ServerTemplates. Import the New Relic Performance Monitoring Toolbox ServerTemplate into your Local view. After you import the ServerTemplate, you can use the associated New Relic RightScripts with your account and add them to your ServerTemplates as appropriate for your agent language. New Relic RightScript Notes Java Tomcat For use with Tomcat5 and Tomcat6 ServerTemplates. Installs our Java agent and configures it for use with Tomcat with APM. .NET Installs and configures the New Relic agent to monitor .NET applications. Using New Relic for .NET requires .NET 3.5 or higher on the target host. (If you have only 4.0 or higher installed, you must have .NET Version 3.5 installed on the target host.) With the 3.5 framework, New Relic instruments applications targeting .NET frameworks 2.0 or higher running on Windows Server 2003, Windows Server 2008, Windows Vista, Windows 7, and Windows Azure. PHP Installs and configures the New Relic PHP agent and a local proxy daemon. New Relic supports PHP versions 5.2 or higher on Apache 2.2 or 2.4. Python To install and configure the New Relic Python agent, do not use RightScripts. Instead, send an email to rightscale@newrelic.com. Ruby To use the New Relic agent for your Ruby on Rails application, see New Relic for Ruby. Example PHP RightScript installation Here is an example of adding the APP New Relic RPM for PHP RightScripts to a PHP application server. Add the APP New Relic RPM for PHP RightScript as a boot script to the ServerTemplate that is being used to run your PHP application servers. This will automatically deploy the New Relic agent when a new server is launched with that ServerTemplate. If your PHP application is already running on a server, you can add it as an operational script under the Scripts tab, and run the action on the running server. Be sure to define the following input value for the new script: NR_License_Key: Enter your New Relic license key. Copy You can either manually enter the license key or create a credential for it. View APM monitoring data When viewing the data that New Relic monitors, you will only see the names of the applications that are on servers where you have run the New Relic RightScript. A host is a server and an instance is an instance of an application. RightScale users: Example of apps you can view in New Relic. To view your New Relic data: Go to Reports > New Relic. From the list of application(s) in your New Relic RightScript deployment, select an application name. On your first login, be sure to change your password: From the (account dropdown in the New Relic UI, select User preferences. Use the New Relic menu options to view different performance data and charts. Upgrade your New Relic subscription For RightScale customers, all new accounts start with a free 14-day trial of New Relic's paid subscription features. If you want to upgrade your free account to a paid subscription, send an email to rightscale@newrelic.com. You cannot upgrade your New Relic subscription within RightScale.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.00787,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "RightScale users <em>and</em> New Relic",
        "sections": "Example PHP RightScript <em>installation</em>",
        "tags": "<em>Accounts</em> <em>and</em> <em>billing</em>",
        "body": " customers, all new <em>accounts</em> start with a free 14-day trial of New Relic&#x27;s paid subscription features. If you want to upgrade your free <em>account</em> to a paid subscription, send an email to rightscale@newrelic.com. You cannot upgrade your New Relic subscription within RightScale."
      },
      "id": "603eb08b28ccbc289aeba7a2"
    },
    {
      "sections": [
        "Log in to and install New Relic via partners",
        "Create a New Relic account (all partners)",
        "Sign into and deploy New Relic (most partners)",
        "Sign into and deploy New Relic (special partners)",
        "Tip",
        "A - M partners",
        "Amazon Web Services (AWS)",
        "Google Cloud Platform (GCP)",
        "Heroku",
        "Magento",
        "N - Z partners",
        "Pantheon",
        "Rackspace Cloud Tools",
        "W3 Edge and WordPress",
        "Windows Azure"
      ],
      "title": "Log in to and install New Relic via partners",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Partner installation"
      ],
      "external_id": "d6c766c0328d09f6208426396d5443d115029fcc",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/install-new-relic/partner-based-installation/log-install-new-relic-partners/",
      "published_at": "2021-12-19T14:45:50Z",
      "updated_at": "2021-09-14T05:47:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you signed up for New Relic through one of New Relic's partners, you may need to follow additional single sign on (SSO) procedures to log in to start using New Relic. There also may be special steps needed to complete the deployment process. This document provides basic login procedures for accounts created through New Relic partners, with links to additional resources as applicable. Create a New Relic account (all partners) If you have received a special offer from a New Relic partner, follow the partner's link directly to create a New Relic account. Sign into and deploy New Relic (most partners) You will receive an email confirmation message with a link to confirm your account, sign into New Relic, and complete the deployment process. Sign into and deploy New Relic (special partners) These partners require specific login and deployment procedures for you to use your New Relic account. Tip If you signed up for a New Relic account through a partner that is not listed here, follow standard procedures to sign into and deploy New Relic. A - M partners Amazon Web Services (AWS) Different installation and page viewing procedures apply, depending on whether you want to install AWS for your app or your host. For more information, see Amazon Web Services (AWS) users. Google Cloud Platform (GCP) As a Google Cloud Platform (GCP) partner, New Relic supports the App Engine flexible environment. Each APM agent has its own requirements for the GAE flex environment. After you install the New Relic agent, you can view your app performance with APM. Heroku Heroku is a Platform as a Service (PaaS) solution capable of hosting web applications. With the New Relic Heroku add-on, you can extend Heroku with metrics from APM and browser monitoring. The New Relic add-on supports Java, Node.js, PHP, Python, and Ruby. For more information about installing and using New Relic with Heroku, see: Heroku dev center documentation New Relic's documentation for installing Heroku Magento Magento has partnered with New Relic to give merchants faster access to data and out-of-the-box tools. The New Relic Reporting Extension helps e-commerce businesses to make data-driven decisions. To create a New Relic account with Magento for your PHP app: Sign up via your Magento sales solution provider. After you sign up for your New Relic account, deploy the PHP agent to your Magento installation. Wait a few minutes, then check your app's performance in the APM Overview page. N - Z partners Pantheon To create a New Relic account through Pantheon and sign into New Relic: From Pantheon's partner page with New Relic, select the sign up link. Follow the online instructions to complete the installation process. Wait for New Relic to begin collecting data for your app. After a few minutes, you can check your app's performance in the APM Overview page. For more information, see Pantheon's documentation. Rackspace Cloud Tools To create an account through Rackspace Cloud Tools and sign into New Relic, follow these basic procedures. For more information, see Rackspace Cloud Load Balancer plugin. Log in to your CloudTools account at cloudtools.rackspace.com/myapps. From Use Your Applications, select New Relic. Select your agent language (Java, .NET, PHP, Python, Ruby), then follow the online instructions to complete the installation process. Wait for New Relic to begin collecting data for your app. Wait a few minutes, then check your app's performance in the APM Overview page. W3 Edge and WordPress Tip To create an account through W3 Total Cache and sign into New Relic, you must install and run the W3 PHP agent, not just get an API key. Deploying the agent requires administrative access, including: SSH/command line access Root or sudo access to run the New Relic installer and restart your web server If you are running an economy, shared hosting, or blogger package, or if you are not sure what are your account permissions, contact your hosting provider. For example, if your WordPress site is on shared servers, you may not be able to install New Relic. Your provider should be able to give you information about your account permissions, or upgrade your account if necessary. To install the New Relic plugin for W3 Edge/Wordpress users: Log in to your WordPress account at wordpress.org/support/bb-login.php. Follow New Relic's instructions to complete the installation process, or contact W3TC support for assistance. Wait for New Relic to begin collecting data. Within a few minutes after deploying the agent, performance data about your app will appear on APM's Applications Overview page. Tip WordPress also offers a plugin for our browser monitoring. Windows Azure Windows Azure is a Platform as a Service (PaaS) solution capable of hosting web applications using various languages, including .NET and Node.js. For more information about using our .NET agent with Azure, see: Installing the .NET agent on Azure Azure Preview Portal Azure Web Apps Azure Cloud Services For more information about installing and using our Node.js agent with Azure, see Node.js agent on Microsoft Azure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.00787,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Log in to <em>and</em> <em>install</em> New Relic via <em>partners</em>",
        "sections": "Log in to <em>and</em> <em>install</em> New Relic via <em>partners</em>",
        "tags": "<em>Accounts</em> <em>and</em> <em>billing</em>",
        "body": " for <em>accounts</em> created through New Relic partners, with links to additional resources as applicable. Create a New Relic <em>account</em> (all partners) If you have received a special offer from a New Relic <em>partner</em>, follow the <em>partner</em>&#x27;s link directly to create a New Relic <em>account</em>. Sign into and deploy New Relic (most"
      },
      "id": "6043f447e7b9d2f5295799e5"
    },
    {
      "sections": [
        "Rackspace Cloud Load Balancer plugin",
        "Contents",
        "Prerequisites",
        "Installing the plugin",
        "Viewing the Load Balancer"
      ],
      "title": "Rackspace Cloud Load Balancer plugin",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Partner installation"
      ],
      "external_id": "8b0090fd5eb0201e4818992500b53b0295ccbdbc",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/install-new-relic/partner-based-installation/rackspace-cloud-load-balancer-plugin/",
      "published_at": "2021-12-19T14:45:49Z",
      "updated_at": "2021-05-15T22:47:28Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Rackspace offers a plugin for the Cloud Load Balancer product. The plugin allows you to see HTTP vs. HTTPS traffic, easily set alerts to your predefined thresholds, and periodically check the health of the nodes associated with your load balancer to ensure they are responding correctly. Contents Prerequisites Make sure your system has these components installed: Ruby (version 1.8.7 or higher) Rubygems (version 1.3.7 or higher) Ruby bundler gem A New Relic account Rackspace Load Balancer with logging enabled Installing the plugin If you have load balancers in multiple regions, configure a separate plugin for each region. Always use lower case characters for the region's name in your yml file. For more information about YAML parsing, see http://yaml-online-parser.appspot.com/. wget https://github.com/newrelic-platform/newrelic_rackspace_load_balancers_plugin/archive/1.0.0.zip Copy bundle install -binstubs ./bin/newrelic_rs --sample-config Copy Download the zip file from GitHub: Extract the contents into the directory you want to use. Run the following commands from within this directory to get the gem setup: Modify the config/newrelic_plugin.yml with your New Relic license key, Rackspace username and API key, and the region where your load balancer sits. Be sure to use lower case characters for the region name. Start the agent: ./bin/newrelic_rs. Verify that the output says gathering xx statistics. Viewing the Load Balancer Rackspace Cloud Load Balancer After the plugin installs successfully, you can select it in New Relic: Sign in to your New Relic account at rpm.newrelic.com/login. From the New Relic UI, select the Rackspace Cloud Load Balancer icon. From here you can select load balancer(s), view traffic, configure alerts, and more.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 149.88806,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Installing</em> the plugin",
        "tags": "<em>Accounts</em> <em>and</em> <em>billing</em>",
        "body": " Prerequisites Make sure your system has these components installed: Ruby (version 1.8.7 or higher) Rubygems (version 1.3.7 or higher) Ruby bundler gem A New Relic <em>account</em> Rackspace Load Balancer with logging enabled Installing the plugin If you have load balancers in multiple regions, configure"
      },
      "id": "603eb08b28ccbc6215eba77a"
    }
  ],
  "/docs/accounts/install-new-relic/partner-based-installation/log-install-new-relic-partners": [
    {
      "sections": [
        "RightScale users and New Relic",
        "Add New Relic RightScripts",
        "Example PHP RightScript installation",
        "View APM monitoring data",
        "Upgrade your New Relic subscription"
      ],
      "title": "RightScale users and New Relic",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Partner installation"
      ],
      "external_id": "53775c11e82931aa08ef2d107aaa3f023368559b",
      "image": "https://docs.newrelic.com/static/84cd6a6ee1fe0ce671be3d3d710ab5a4/8c557/screen-nr-rightscale-applist.png",
      "url": "https://docs.newrelic.com/docs/accounts/install-new-relic/partner-based-installation/rightscale-users-new-relic/",
      "published_at": "2021-12-19T14:15:22Z",
      "updated_at": "2021-09-14T05:47:35Z",
      "document_type": "page",
      "popularity": 1,
      "body": "RightScale is a Software as a Service (SaaS) solution capable of hosting web applications. Supported languages for both New Relic and RightScale include Java, Ruby, .NET, PHP, and Python. In order to use New Relic through the RightScale partnership, you need a New Relic account. Before getting started, be sure to enable API access. Add New Relic RightScripts Before you can view data from New Relic in RightScale, add the appropriate New Relic RightScript to your ServerTemplates: From RightCloud, select Design > MultiCloud Marketplace > ServerTemplates. Import the New Relic Performance Monitoring Toolbox ServerTemplate into your Local view. After you import the ServerTemplate, you can use the associated New Relic RightScripts with your account and add them to your ServerTemplates as appropriate for your agent language. New Relic RightScript Notes Java Tomcat For use with Tomcat5 and Tomcat6 ServerTemplates. Installs our Java agent and configures it for use with Tomcat with APM. .NET Installs and configures the New Relic agent to monitor .NET applications. Using New Relic for .NET requires .NET 3.5 or higher on the target host. (If you have only 4.0 or higher installed, you must have .NET Version 3.5 installed on the target host.) With the 3.5 framework, New Relic instruments applications targeting .NET frameworks 2.0 or higher running on Windows Server 2003, Windows Server 2008, Windows Vista, Windows 7, and Windows Azure. PHP Installs and configures the New Relic PHP agent and a local proxy daemon. New Relic supports PHP versions 5.2 or higher on Apache 2.2 or 2.4. Python To install and configure the New Relic Python agent, do not use RightScripts. Instead, send an email to rightscale@newrelic.com. Ruby To use the New Relic agent for your Ruby on Rails application, see New Relic for Ruby. Example PHP RightScript installation Here is an example of adding the APP New Relic RPM for PHP RightScripts to a PHP application server. Add the APP New Relic RPM for PHP RightScript as a boot script to the ServerTemplate that is being used to run your PHP application servers. This will automatically deploy the New Relic agent when a new server is launched with that ServerTemplate. If your PHP application is already running on a server, you can add it as an operational script under the Scripts tab, and run the action on the running server. Be sure to define the following input value for the new script: NR_License_Key: Enter your New Relic license key. Copy You can either manually enter the license key or create a credential for it. View APM monitoring data When viewing the data that New Relic monitors, you will only see the names of the applications that are on servers where you have run the New Relic RightScript. A host is a server and an instance is an instance of an application. RightScale users: Example of apps you can view in New Relic. To view your New Relic data: Go to Reports > New Relic. From the list of application(s) in your New Relic RightScript deployment, select an application name. On your first login, be sure to change your password: From the (account dropdown in the New Relic UI, select User preferences. Use the New Relic menu options to view different performance data and charts. Upgrade your New Relic subscription For RightScale customers, all new accounts start with a free 14-day trial of New Relic's paid subscription features. If you want to upgrade your free account to a paid subscription, send an email to rightscale@newrelic.com. You cannot upgrade your New Relic subscription within RightScale.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.00787,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "RightScale users <em>and</em> New Relic",
        "sections": "Example PHP RightScript <em>installation</em>",
        "tags": "<em>Accounts</em> <em>and</em> <em>billing</em>",
        "body": " customers, all new <em>accounts</em> start with a free 14-day trial of New Relic&#x27;s paid subscription features. If you want to upgrade your free <em>account</em> to a paid subscription, send an email to rightscale@newrelic.com. You cannot upgrade your New Relic subscription within RightScale."
      },
      "id": "603eb08b28ccbc289aeba7a2"
    },
    {
      "sections": [
        "Heroku: Install the New Relic add-on",
        "Install the New Relic add-on",
        "Configure the agent",
        "Java",
        "Node.js",
        "PHP",
        "Python",
        "Ruby",
        "Manage your Heroku add-on accounts",
        "License key",
        "Account URL",
        "Account ID",
        "Log on to New Relic",
        "Sign in to New Relic add-on account through Heroku",
        "Sign in to regular New Relic accounts",
        "Set up deployment notifications",
        "Important"
      ],
      "title": "Heroku: Install the New Relic add-on",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Partner installation"
      ],
      "external_id": "45ca4960759150545d4f9e586555d6e15daef3ec",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/install-new-relic/partner-based-installation/heroku-install-new-relic-add/",
      "published_at": "2021-12-19T15:03:36Z",
      "updated_at": "2021-09-14T05:46:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Heroku is a Platform as a Service (PaaS) solution for hosting web applications in various agent languages. With New Relic, you can extend Heroku with metrics from our monitoring solutions, like APM and browser monitoring. The New Relic add-on supports Java, Node.js, PHP, Python, and Ruby. Install the New Relic add-on To set up the New Relic add-on, see Heroku's documentation, including available plan levels and Toolbelt procedures. If you have problems, use the Heroku support channels. Configure the agent After you install the New Relic add-on for Heroku, follow the configuration procedures for your APM agent. Java The minimum agent version for Java is 3.17.1 or higher. To install and configure New Relic's Java agent for your add-on, see our Java agent and Heroku documentation. Node.js To install and configure our Node.js agent for your add-on, see: New Relic's Node.js and Heroku documentation Blog post (2013) with a \"real world\" example of installing our Node.js agent for a Heroku app PHP To install and configure our PHP agent for your add-on, see our PHP agent and Heroku documentation. Python To install and configure our Python agent for your add-on, see our Python agent and Heroku documentation. Ruby To install and configure our Ruby agent for your add-on, see our Ruby agent and Heroku documentation. If you are using our Ruby agent with a non-Rails application, Heroku users need to install the plugin in your repository manually. For example, in a Sinatra app, add the newrelic gem to your Gemfile, and then add the following code to your app: configure :production do require 'newrelic_rpm' end Copy Manage your Heroku add-on accounts Every time you install the New Relic add-on for Heroku, New Relic will automatically create a new account, complete with a unique license key, account URL, and account ID. These three types of information are important for managing each of your Heroku add-on accounts. License key The license key identifies the account where your application reports. To check the license key your app is using: From a command line, run: heroku config | grep -i relic Copy Look for the value of NEW_RELIC_LICENSE_KEY. This license key environment variable will override any other license key that you hard-code into your New Relic config file. Account URL If you install the New Relic add-on multiple times and need to verify the URL your Heroku app uses for reporting to New Relic, look in your agent logs for a line indicating reporting to following by a URL using this format: rpm.newrelic.com/accounts/###/applications/###### Copy Account ID You cannot change your add-on's account ID directly. If you need to change the New Relic account your Heroku app uses for reporting to New Relic, change the current license key environment variable in its config file so that it points to the license key of the New Relic account you want to use: heroku config:set NEW_RELIC_LICENSE_KEY=changed_account_license_key Copy Log on to New Relic Heroku customers may have two different types of accounts with New Relic: Add-on accounts: New Relic accounts that customers installed on their Heroku application by using Heroku's New Relic add-on \"Regular\" accounts: Other New Relic accounts that customers did not install with Heroku's add-on Regular accounts provide a wider range of features than do add-on accounts, and are installed and managed differently. Different procedures apply, depending on which type of account you want to sign into. In accordance with the terms of New Relic's partnership with Heroku, customers who install New Relic via the Heroku add-on can only access their New Relic add-on accounts by signing in through Heroku. For this reason, if you have both add-on accounts and regular New Relic accounts, you cannot switch directly between them. Sign in to New Relic add-on account through Heroku To sign in to your New Relic add-on accounts: Sign in through Heroku's login page at id.heroku.com/login. Select the application that has the New Relic add-on installed. Select New Relic from your list of add-ons. If you sign in through Heroku, you will not see any of your regular New Relic accounts when you select account dropdown > Switch account. Sign in to regular New Relic accounts To sign into or switch between your regular New Relic accounts: Sign in to New Relic at one.newrelic.com. To switch from one regular New Relic account to another: Go to: **account dropdown and select an account. If you sign in directly through New Relic, you will not see any of your New Relic add-on accounts from Heroku when you select account dropdown > Switch account. Set up deployment notifications The Heroku add-on automatically sends deployment notifications to New Relic for one application per account. If you add multiple applications to your add-on account, you must use the New Relic REST API to manually send deployment notifications for your additional applications. You cannot use a post-deploy hook, because the New Relic REST API call requires a header, and Heroku's post-deploy hook does not allow headers. However, you can write a script that generates this API call whenever you deploy on Heroku. For instructions on recording deployments via the REST API, see Recording deployments. Important When you add a user to the Heroku add-on, this creates a user record for the user at New Relic. However, if you remove the user from the Heroku add-on, the user record is not automatically removed from New Relic. Instead, you must also manually remove the New Relic user record after removing the user from the Heroku add-on. You can do this by going to the User Management page.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.00778,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Heroku: <em>Install</em> the New Relic add-on",
        "sections": "Manage your Heroku add-on <em>accounts</em>",
        "tags": "<em>Accounts</em> <em>and</em> <em>billing</em>",
        "body": " the newrelic gem to your Gemfile, and then add the following code to your app: configure :production do require &#x27;newrelic_rpm&#x27; end Copy Manage your Heroku add-on <em>accounts</em> Every time you install the New Relic add-on for Heroku, New Relic will automatically create a new <em>account</em>, complete with a unique license"
      },
      "id": "603ebc9ae7b9d272c32a0810"
    },
    {
      "sections": [
        "Rackspace Cloud Load Balancer plugin",
        "Contents",
        "Prerequisites",
        "Installing the plugin",
        "Viewing the Load Balancer"
      ],
      "title": "Rackspace Cloud Load Balancer plugin",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Partner installation"
      ],
      "external_id": "8b0090fd5eb0201e4818992500b53b0295ccbdbc",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/install-new-relic/partner-based-installation/rackspace-cloud-load-balancer-plugin/",
      "published_at": "2021-12-19T14:45:49Z",
      "updated_at": "2021-05-15T22:47:28Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Rackspace offers a plugin for the Cloud Load Balancer product. The plugin allows you to see HTTP vs. HTTPS traffic, easily set alerts to your predefined thresholds, and periodically check the health of the nodes associated with your load balancer to ensure they are responding correctly. Contents Prerequisites Make sure your system has these components installed: Ruby (version 1.8.7 or higher) Rubygems (version 1.3.7 or higher) Ruby bundler gem A New Relic account Rackspace Load Balancer with logging enabled Installing the plugin If you have load balancers in multiple regions, configure a separate plugin for each region. Always use lower case characters for the region's name in your yml file. For more information about YAML parsing, see http://yaml-online-parser.appspot.com/. wget https://github.com/newrelic-platform/newrelic_rackspace_load_balancers_plugin/archive/1.0.0.zip Copy bundle install -binstubs ./bin/newrelic_rs --sample-config Copy Download the zip file from GitHub: Extract the contents into the directory you want to use. Run the following commands from within this directory to get the gem setup: Modify the config/newrelic_plugin.yml with your New Relic license key, Rackspace username and API key, and the region where your load balancer sits. Be sure to use lower case characters for the region name. Start the agent: ./bin/newrelic_rs. Verify that the output says gathering xx statistics. Viewing the Load Balancer Rackspace Cloud Load Balancer After the plugin installs successfully, you can select it in New Relic: Sign in to your New Relic account at rpm.newrelic.com/login. From the New Relic UI, select the Rackspace Cloud Load Balancer icon. From here you can select load balancer(s), view traffic, configure alerts, and more.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 149.88806,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Installing</em> the plugin",
        "tags": "<em>Accounts</em> <em>and</em> <em>billing</em>",
        "body": " Prerequisites Make sure your system has these components installed: Ruby (version 1.8.7 or higher) Rubygems (version 1.3.7 or higher) Ruby bundler gem A New Relic <em>account</em> Rackspace Load Balancer with logging enabled Installing the plugin If you have load balancers in multiple regions, configure"
      },
      "id": "603eb08b28ccbc6215eba77a"
    }
  ],
  "/docs/accounts/install-new-relic/partner-based-installation/rackspace-cloud-load-balancer-plugin": [
    {
      "sections": [
        "RightScale users and New Relic",
        "Add New Relic RightScripts",
        "Example PHP RightScript installation",
        "View APM monitoring data",
        "Upgrade your New Relic subscription"
      ],
      "title": "RightScale users and New Relic",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Partner installation"
      ],
      "external_id": "53775c11e82931aa08ef2d107aaa3f023368559b",
      "image": "https://docs.newrelic.com/static/84cd6a6ee1fe0ce671be3d3d710ab5a4/8c557/screen-nr-rightscale-applist.png",
      "url": "https://docs.newrelic.com/docs/accounts/install-new-relic/partner-based-installation/rightscale-users-new-relic/",
      "published_at": "2021-12-19T14:15:22Z",
      "updated_at": "2021-09-14T05:47:35Z",
      "document_type": "page",
      "popularity": 1,
      "body": "RightScale is a Software as a Service (SaaS) solution capable of hosting web applications. Supported languages for both New Relic and RightScale include Java, Ruby, .NET, PHP, and Python. In order to use New Relic through the RightScale partnership, you need a New Relic account. Before getting started, be sure to enable API access. Add New Relic RightScripts Before you can view data from New Relic in RightScale, add the appropriate New Relic RightScript to your ServerTemplates: From RightCloud, select Design > MultiCloud Marketplace > ServerTemplates. Import the New Relic Performance Monitoring Toolbox ServerTemplate into your Local view. After you import the ServerTemplate, you can use the associated New Relic RightScripts with your account and add them to your ServerTemplates as appropriate for your agent language. New Relic RightScript Notes Java Tomcat For use with Tomcat5 and Tomcat6 ServerTemplates. Installs our Java agent and configures it for use with Tomcat with APM. .NET Installs and configures the New Relic agent to monitor .NET applications. Using New Relic for .NET requires .NET 3.5 or higher on the target host. (If you have only 4.0 or higher installed, you must have .NET Version 3.5 installed on the target host.) With the 3.5 framework, New Relic instruments applications targeting .NET frameworks 2.0 or higher running on Windows Server 2003, Windows Server 2008, Windows Vista, Windows 7, and Windows Azure. PHP Installs and configures the New Relic PHP agent and a local proxy daemon. New Relic supports PHP versions 5.2 or higher on Apache 2.2 or 2.4. Python To install and configure the New Relic Python agent, do not use RightScripts. Instead, send an email to rightscale@newrelic.com. Ruby To use the New Relic agent for your Ruby on Rails application, see New Relic for Ruby. Example PHP RightScript installation Here is an example of adding the APP New Relic RPM for PHP RightScripts to a PHP application server. Add the APP New Relic RPM for PHP RightScript as a boot script to the ServerTemplate that is being used to run your PHP application servers. This will automatically deploy the New Relic agent when a new server is launched with that ServerTemplate. If your PHP application is already running on a server, you can add it as an operational script under the Scripts tab, and run the action on the running server. Be sure to define the following input value for the new script: NR_License_Key: Enter your New Relic license key. Copy You can either manually enter the license key or create a credential for it. View APM monitoring data When viewing the data that New Relic monitors, you will only see the names of the applications that are on servers where you have run the New Relic RightScript. A host is a server and an instance is an instance of an application. RightScale users: Example of apps you can view in New Relic. To view your New Relic data: Go to Reports > New Relic. From the list of application(s) in your New Relic RightScript deployment, select an application name. On your first login, be sure to change your password: From the (account dropdown in the New Relic UI, select User preferences. Use the New Relic menu options to view different performance data and charts. Upgrade your New Relic subscription For RightScale customers, all new accounts start with a free 14-day trial of New Relic's paid subscription features. If you want to upgrade your free account to a paid subscription, send an email to rightscale@newrelic.com. You cannot upgrade your New Relic subscription within RightScale.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.00787,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "RightScale users <em>and</em> New Relic",
        "sections": "Example PHP RightScript <em>installation</em>",
        "tags": "<em>Accounts</em> <em>and</em> <em>billing</em>",
        "body": " customers, all new <em>accounts</em> start with a free 14-day trial of New Relic&#x27;s paid subscription features. If you want to upgrade your free <em>account</em> to a paid subscription, send an email to rightscale@newrelic.com. You cannot upgrade your New Relic subscription within RightScale."
      },
      "id": "603eb08b28ccbc289aeba7a2"
    },
    {
      "sections": [
        "Log in to and install New Relic via partners",
        "Create a New Relic account (all partners)",
        "Sign into and deploy New Relic (most partners)",
        "Sign into and deploy New Relic (special partners)",
        "Tip",
        "A - M partners",
        "Amazon Web Services (AWS)",
        "Google Cloud Platform (GCP)",
        "Heroku",
        "Magento",
        "N - Z partners",
        "Pantheon",
        "Rackspace Cloud Tools",
        "W3 Edge and WordPress",
        "Windows Azure"
      ],
      "title": "Log in to and install New Relic via partners",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Partner installation"
      ],
      "external_id": "d6c766c0328d09f6208426396d5443d115029fcc",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/install-new-relic/partner-based-installation/log-install-new-relic-partners/",
      "published_at": "2021-12-19T14:45:50Z",
      "updated_at": "2021-09-14T05:47:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you signed up for New Relic through one of New Relic's partners, you may need to follow additional single sign on (SSO) procedures to log in to start using New Relic. There also may be special steps needed to complete the deployment process. This document provides basic login procedures for accounts created through New Relic partners, with links to additional resources as applicable. Create a New Relic account (all partners) If you have received a special offer from a New Relic partner, follow the partner's link directly to create a New Relic account. Sign into and deploy New Relic (most partners) You will receive an email confirmation message with a link to confirm your account, sign into New Relic, and complete the deployment process. Sign into and deploy New Relic (special partners) These partners require specific login and deployment procedures for you to use your New Relic account. Tip If you signed up for a New Relic account through a partner that is not listed here, follow standard procedures to sign into and deploy New Relic. A - M partners Amazon Web Services (AWS) Different installation and page viewing procedures apply, depending on whether you want to install AWS for your app or your host. For more information, see Amazon Web Services (AWS) users. Google Cloud Platform (GCP) As a Google Cloud Platform (GCP) partner, New Relic supports the App Engine flexible environment. Each APM agent has its own requirements for the GAE flex environment. After you install the New Relic agent, you can view your app performance with APM. Heroku Heroku is a Platform as a Service (PaaS) solution capable of hosting web applications. With the New Relic Heroku add-on, you can extend Heroku with metrics from APM and browser monitoring. The New Relic add-on supports Java, Node.js, PHP, Python, and Ruby. For more information about installing and using New Relic with Heroku, see: Heroku dev center documentation New Relic's documentation for installing Heroku Magento Magento has partnered with New Relic to give merchants faster access to data and out-of-the-box tools. The New Relic Reporting Extension helps e-commerce businesses to make data-driven decisions. To create a New Relic account with Magento for your PHP app: Sign up via your Magento sales solution provider. After you sign up for your New Relic account, deploy the PHP agent to your Magento installation. Wait a few minutes, then check your app's performance in the APM Overview page. N - Z partners Pantheon To create a New Relic account through Pantheon and sign into New Relic: From Pantheon's partner page with New Relic, select the sign up link. Follow the online instructions to complete the installation process. Wait for New Relic to begin collecting data for your app. After a few minutes, you can check your app's performance in the APM Overview page. For more information, see Pantheon's documentation. Rackspace Cloud Tools To create an account through Rackspace Cloud Tools and sign into New Relic, follow these basic procedures. For more information, see Rackspace Cloud Load Balancer plugin. Log in to your CloudTools account at cloudtools.rackspace.com/myapps. From Use Your Applications, select New Relic. Select your agent language (Java, .NET, PHP, Python, Ruby), then follow the online instructions to complete the installation process. Wait for New Relic to begin collecting data for your app. Wait a few minutes, then check your app's performance in the APM Overview page. W3 Edge and WordPress Tip To create an account through W3 Total Cache and sign into New Relic, you must install and run the W3 PHP agent, not just get an API key. Deploying the agent requires administrative access, including: SSH/command line access Root or sudo access to run the New Relic installer and restart your web server If you are running an economy, shared hosting, or blogger package, or if you are not sure what are your account permissions, contact your hosting provider. For example, if your WordPress site is on shared servers, you may not be able to install New Relic. Your provider should be able to give you information about your account permissions, or upgrade your account if necessary. To install the New Relic plugin for W3 Edge/Wordpress users: Log in to your WordPress account at wordpress.org/support/bb-login.php. Follow New Relic's instructions to complete the installation process, or contact W3TC support for assistance. Wait for New Relic to begin collecting data. Within a few minutes after deploying the agent, performance data about your app will appear on APM's Applications Overview page. Tip WordPress also offers a plugin for our browser monitoring. Windows Azure Windows Azure is a Platform as a Service (PaaS) solution capable of hosting web applications using various languages, including .NET and Node.js. For more information about using our .NET agent with Azure, see: Installing the .NET agent on Azure Azure Preview Portal Azure Web Apps Azure Cloud Services For more information about installing and using our Node.js agent with Azure, see Node.js agent on Microsoft Azure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.00787,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Log in to <em>and</em> <em>install</em> New Relic via <em>partners</em>",
        "sections": "Log in to <em>and</em> <em>install</em> New Relic via <em>partners</em>",
        "tags": "<em>Accounts</em> <em>and</em> <em>billing</em>",
        "body": " for <em>accounts</em> created through New Relic partners, with links to additional resources as applicable. Create a New Relic <em>account</em> (all partners) If you have received a special offer from a New Relic <em>partner</em>, follow the <em>partner</em>&#x27;s link directly to create a New Relic <em>account</em>. Sign into and deploy New Relic (most"
      },
      "id": "6043f447e7b9d2f5295799e5"
    },
    {
      "sections": [
        "Heroku: Install the New Relic add-on",
        "Install the New Relic add-on",
        "Configure the agent",
        "Java",
        "Node.js",
        "PHP",
        "Python",
        "Ruby",
        "Manage your Heroku add-on accounts",
        "License key",
        "Account URL",
        "Account ID",
        "Log on to New Relic",
        "Sign in to New Relic add-on account through Heroku",
        "Sign in to regular New Relic accounts",
        "Set up deployment notifications",
        "Important"
      ],
      "title": "Heroku: Install the New Relic add-on",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Partner installation"
      ],
      "external_id": "45ca4960759150545d4f9e586555d6e15daef3ec",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/install-new-relic/partner-based-installation/heroku-install-new-relic-add/",
      "published_at": "2021-12-19T15:03:36Z",
      "updated_at": "2021-09-14T05:46:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Heroku is a Platform as a Service (PaaS) solution for hosting web applications in various agent languages. With New Relic, you can extend Heroku with metrics from our monitoring solutions, like APM and browser monitoring. The New Relic add-on supports Java, Node.js, PHP, Python, and Ruby. Install the New Relic add-on To set up the New Relic add-on, see Heroku's documentation, including available plan levels and Toolbelt procedures. If you have problems, use the Heroku support channels. Configure the agent After you install the New Relic add-on for Heroku, follow the configuration procedures for your APM agent. Java The minimum agent version for Java is 3.17.1 or higher. To install and configure New Relic's Java agent for your add-on, see our Java agent and Heroku documentation. Node.js To install and configure our Node.js agent for your add-on, see: New Relic's Node.js and Heroku documentation Blog post (2013) with a \"real world\" example of installing our Node.js agent for a Heroku app PHP To install and configure our PHP agent for your add-on, see our PHP agent and Heroku documentation. Python To install and configure our Python agent for your add-on, see our Python agent and Heroku documentation. Ruby To install and configure our Ruby agent for your add-on, see our Ruby agent and Heroku documentation. If you are using our Ruby agent with a non-Rails application, Heroku users need to install the plugin in your repository manually. For example, in a Sinatra app, add the newrelic gem to your Gemfile, and then add the following code to your app: configure :production do require 'newrelic_rpm' end Copy Manage your Heroku add-on accounts Every time you install the New Relic add-on for Heroku, New Relic will automatically create a new account, complete with a unique license key, account URL, and account ID. These three types of information are important for managing each of your Heroku add-on accounts. License key The license key identifies the account where your application reports. To check the license key your app is using: From a command line, run: heroku config | grep -i relic Copy Look for the value of NEW_RELIC_LICENSE_KEY. This license key environment variable will override any other license key that you hard-code into your New Relic config file. Account URL If you install the New Relic add-on multiple times and need to verify the URL your Heroku app uses for reporting to New Relic, look in your agent logs for a line indicating reporting to following by a URL using this format: rpm.newrelic.com/accounts/###/applications/###### Copy Account ID You cannot change your add-on's account ID directly. If you need to change the New Relic account your Heroku app uses for reporting to New Relic, change the current license key environment variable in its config file so that it points to the license key of the New Relic account you want to use: heroku config:set NEW_RELIC_LICENSE_KEY=changed_account_license_key Copy Log on to New Relic Heroku customers may have two different types of accounts with New Relic: Add-on accounts: New Relic accounts that customers installed on their Heroku application by using Heroku's New Relic add-on \"Regular\" accounts: Other New Relic accounts that customers did not install with Heroku's add-on Regular accounts provide a wider range of features than do add-on accounts, and are installed and managed differently. Different procedures apply, depending on which type of account you want to sign into. In accordance with the terms of New Relic's partnership with Heroku, customers who install New Relic via the Heroku add-on can only access their New Relic add-on accounts by signing in through Heroku. For this reason, if you have both add-on accounts and regular New Relic accounts, you cannot switch directly between them. Sign in to New Relic add-on account through Heroku To sign in to your New Relic add-on accounts: Sign in through Heroku's login page at id.heroku.com/login. Select the application that has the New Relic add-on installed. Select New Relic from your list of add-ons. If you sign in through Heroku, you will not see any of your regular New Relic accounts when you select account dropdown > Switch account. Sign in to regular New Relic accounts To sign into or switch between your regular New Relic accounts: Sign in to New Relic at one.newrelic.com. To switch from one regular New Relic account to another: Go to: **account dropdown and select an account. If you sign in directly through New Relic, you will not see any of your New Relic add-on accounts from Heroku when you select account dropdown > Switch account. Set up deployment notifications The Heroku add-on automatically sends deployment notifications to New Relic for one application per account. If you add multiple applications to your add-on account, you must use the New Relic REST API to manually send deployment notifications for your additional applications. You cannot use a post-deploy hook, because the New Relic REST API call requires a header, and Heroku's post-deploy hook does not allow headers. However, you can write a script that generates this API call whenever you deploy on Heroku. For instructions on recording deployments via the REST API, see Recording deployments. Important When you add a user to the Heroku add-on, this creates a user record for the user at New Relic. However, if you remove the user from the Heroku add-on, the user record is not automatically removed from New Relic. Instead, you must also manually remove the New Relic user record after removing the user from the Heroku add-on. You can do this by going to the User Management page.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.00778,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Heroku: <em>Install</em> the New Relic add-on",
        "sections": "Manage your Heroku add-on <em>accounts</em>",
        "tags": "<em>Accounts</em> <em>and</em> <em>billing</em>",
        "body": " the newrelic gem to your Gemfile, and then add the following code to your app: configure :production do require &#x27;newrelic_rpm&#x27; end Copy Manage your Heroku add-on <em>accounts</em> Every time you install the New Relic add-on for Heroku, New Relic will automatically create a new <em>account</em>, complete with a unique license"
      },
      "id": "603ebc9ae7b9d272c32a0810"
    }
  ],
  "/docs/accounts/install-new-relic/partner-based-installation/rightscale-users-new-relic": [
    {
      "sections": [
        "Log in to and install New Relic via partners",
        "Create a New Relic account (all partners)",
        "Sign into and deploy New Relic (most partners)",
        "Sign into and deploy New Relic (special partners)",
        "Tip",
        "A - M partners",
        "Amazon Web Services (AWS)",
        "Google Cloud Platform (GCP)",
        "Heroku",
        "Magento",
        "N - Z partners",
        "Pantheon",
        "Rackspace Cloud Tools",
        "W3 Edge and WordPress",
        "Windows Azure"
      ],
      "title": "Log in to and install New Relic via partners",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Partner installation"
      ],
      "external_id": "d6c766c0328d09f6208426396d5443d115029fcc",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/install-new-relic/partner-based-installation/log-install-new-relic-partners/",
      "published_at": "2021-12-19T14:45:50Z",
      "updated_at": "2021-09-14T05:47:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you signed up for New Relic through one of New Relic's partners, you may need to follow additional single sign on (SSO) procedures to log in to start using New Relic. There also may be special steps needed to complete the deployment process. This document provides basic login procedures for accounts created through New Relic partners, with links to additional resources as applicable. Create a New Relic account (all partners) If you have received a special offer from a New Relic partner, follow the partner's link directly to create a New Relic account. Sign into and deploy New Relic (most partners) You will receive an email confirmation message with a link to confirm your account, sign into New Relic, and complete the deployment process. Sign into and deploy New Relic (special partners) These partners require specific login and deployment procedures for you to use your New Relic account. Tip If you signed up for a New Relic account through a partner that is not listed here, follow standard procedures to sign into and deploy New Relic. A - M partners Amazon Web Services (AWS) Different installation and page viewing procedures apply, depending on whether you want to install AWS for your app or your host. For more information, see Amazon Web Services (AWS) users. Google Cloud Platform (GCP) As a Google Cloud Platform (GCP) partner, New Relic supports the App Engine flexible environment. Each APM agent has its own requirements for the GAE flex environment. After you install the New Relic agent, you can view your app performance with APM. Heroku Heroku is a Platform as a Service (PaaS) solution capable of hosting web applications. With the New Relic Heroku add-on, you can extend Heroku with metrics from APM and browser monitoring. The New Relic add-on supports Java, Node.js, PHP, Python, and Ruby. For more information about installing and using New Relic with Heroku, see: Heroku dev center documentation New Relic's documentation for installing Heroku Magento Magento has partnered with New Relic to give merchants faster access to data and out-of-the-box tools. The New Relic Reporting Extension helps e-commerce businesses to make data-driven decisions. To create a New Relic account with Magento for your PHP app: Sign up via your Magento sales solution provider. After you sign up for your New Relic account, deploy the PHP agent to your Magento installation. Wait a few minutes, then check your app's performance in the APM Overview page. N - Z partners Pantheon To create a New Relic account through Pantheon and sign into New Relic: From Pantheon's partner page with New Relic, select the sign up link. Follow the online instructions to complete the installation process. Wait for New Relic to begin collecting data for your app. After a few minutes, you can check your app's performance in the APM Overview page. For more information, see Pantheon's documentation. Rackspace Cloud Tools To create an account through Rackspace Cloud Tools and sign into New Relic, follow these basic procedures. For more information, see Rackspace Cloud Load Balancer plugin. Log in to your CloudTools account at cloudtools.rackspace.com/myapps. From Use Your Applications, select New Relic. Select your agent language (Java, .NET, PHP, Python, Ruby), then follow the online instructions to complete the installation process. Wait for New Relic to begin collecting data for your app. Wait a few minutes, then check your app's performance in the APM Overview page. W3 Edge and WordPress Tip To create an account through W3 Total Cache and sign into New Relic, you must install and run the W3 PHP agent, not just get an API key. Deploying the agent requires administrative access, including: SSH/command line access Root or sudo access to run the New Relic installer and restart your web server If you are running an economy, shared hosting, or blogger package, or if you are not sure what are your account permissions, contact your hosting provider. For example, if your WordPress site is on shared servers, you may not be able to install New Relic. Your provider should be able to give you information about your account permissions, or upgrade your account if necessary. To install the New Relic plugin for W3 Edge/Wordpress users: Log in to your WordPress account at wordpress.org/support/bb-login.php. Follow New Relic's instructions to complete the installation process, or contact W3TC support for assistance. Wait for New Relic to begin collecting data. Within a few minutes after deploying the agent, performance data about your app will appear on APM's Applications Overview page. Tip WordPress also offers a plugin for our browser monitoring. Windows Azure Windows Azure is a Platform as a Service (PaaS) solution capable of hosting web applications using various languages, including .NET and Node.js. For more information about using our .NET agent with Azure, see: Installing the .NET agent on Azure Azure Preview Portal Azure Web Apps Azure Cloud Services For more information about installing and using our Node.js agent with Azure, see Node.js agent on Microsoft Azure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.00787,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Log in to <em>and</em> <em>install</em> New Relic via <em>partners</em>",
        "sections": "Log in to <em>and</em> <em>install</em> New Relic via <em>partners</em>",
        "tags": "<em>Accounts</em> <em>and</em> <em>billing</em>",
        "body": " for <em>accounts</em> created through New Relic partners, with links to additional resources as applicable. Create a New Relic <em>account</em> (all partners) If you have received a special offer from a New Relic <em>partner</em>, follow the <em>partner</em>&#x27;s link directly to create a New Relic <em>account</em>. Sign into and deploy New Relic (most"
      },
      "id": "6043f447e7b9d2f5295799e5"
    },
    {
      "sections": [
        "Heroku: Install the New Relic add-on",
        "Install the New Relic add-on",
        "Configure the agent",
        "Java",
        "Node.js",
        "PHP",
        "Python",
        "Ruby",
        "Manage your Heroku add-on accounts",
        "License key",
        "Account URL",
        "Account ID",
        "Log on to New Relic",
        "Sign in to New Relic add-on account through Heroku",
        "Sign in to regular New Relic accounts",
        "Set up deployment notifications",
        "Important"
      ],
      "title": "Heroku: Install the New Relic add-on",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Partner installation"
      ],
      "external_id": "45ca4960759150545d4f9e586555d6e15daef3ec",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/install-new-relic/partner-based-installation/heroku-install-new-relic-add/",
      "published_at": "2021-12-19T15:03:36Z",
      "updated_at": "2021-09-14T05:46:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Heroku is a Platform as a Service (PaaS) solution for hosting web applications in various agent languages. With New Relic, you can extend Heroku with metrics from our monitoring solutions, like APM and browser monitoring. The New Relic add-on supports Java, Node.js, PHP, Python, and Ruby. Install the New Relic add-on To set up the New Relic add-on, see Heroku's documentation, including available plan levels and Toolbelt procedures. If you have problems, use the Heroku support channels. Configure the agent After you install the New Relic add-on for Heroku, follow the configuration procedures for your APM agent. Java The minimum agent version for Java is 3.17.1 or higher. To install and configure New Relic's Java agent for your add-on, see our Java agent and Heroku documentation. Node.js To install and configure our Node.js agent for your add-on, see: New Relic's Node.js and Heroku documentation Blog post (2013) with a \"real world\" example of installing our Node.js agent for a Heroku app PHP To install and configure our PHP agent for your add-on, see our PHP agent and Heroku documentation. Python To install and configure our Python agent for your add-on, see our Python agent and Heroku documentation. Ruby To install and configure our Ruby agent for your add-on, see our Ruby agent and Heroku documentation. If you are using our Ruby agent with a non-Rails application, Heroku users need to install the plugin in your repository manually. For example, in a Sinatra app, add the newrelic gem to your Gemfile, and then add the following code to your app: configure :production do require 'newrelic_rpm' end Copy Manage your Heroku add-on accounts Every time you install the New Relic add-on for Heroku, New Relic will automatically create a new account, complete with a unique license key, account URL, and account ID. These three types of information are important for managing each of your Heroku add-on accounts. License key The license key identifies the account where your application reports. To check the license key your app is using: From a command line, run: heroku config | grep -i relic Copy Look for the value of NEW_RELIC_LICENSE_KEY. This license key environment variable will override any other license key that you hard-code into your New Relic config file. Account URL If you install the New Relic add-on multiple times and need to verify the URL your Heroku app uses for reporting to New Relic, look in your agent logs for a line indicating reporting to following by a URL using this format: rpm.newrelic.com/accounts/###/applications/###### Copy Account ID You cannot change your add-on's account ID directly. If you need to change the New Relic account your Heroku app uses for reporting to New Relic, change the current license key environment variable in its config file so that it points to the license key of the New Relic account you want to use: heroku config:set NEW_RELIC_LICENSE_KEY=changed_account_license_key Copy Log on to New Relic Heroku customers may have two different types of accounts with New Relic: Add-on accounts: New Relic accounts that customers installed on their Heroku application by using Heroku's New Relic add-on \"Regular\" accounts: Other New Relic accounts that customers did not install with Heroku's add-on Regular accounts provide a wider range of features than do add-on accounts, and are installed and managed differently. Different procedures apply, depending on which type of account you want to sign into. In accordance with the terms of New Relic's partnership with Heroku, customers who install New Relic via the Heroku add-on can only access their New Relic add-on accounts by signing in through Heroku. For this reason, if you have both add-on accounts and regular New Relic accounts, you cannot switch directly between them. Sign in to New Relic add-on account through Heroku To sign in to your New Relic add-on accounts: Sign in through Heroku's login page at id.heroku.com/login. Select the application that has the New Relic add-on installed. Select New Relic from your list of add-ons. If you sign in through Heroku, you will not see any of your regular New Relic accounts when you select account dropdown > Switch account. Sign in to regular New Relic accounts To sign into or switch between your regular New Relic accounts: Sign in to New Relic at one.newrelic.com. To switch from one regular New Relic account to another: Go to: **account dropdown and select an account. If you sign in directly through New Relic, you will not see any of your New Relic add-on accounts from Heroku when you select account dropdown > Switch account. Set up deployment notifications The Heroku add-on automatically sends deployment notifications to New Relic for one application per account. If you add multiple applications to your add-on account, you must use the New Relic REST API to manually send deployment notifications for your additional applications. You cannot use a post-deploy hook, because the New Relic REST API call requires a header, and Heroku's post-deploy hook does not allow headers. However, you can write a script that generates this API call whenever you deploy on Heroku. For instructions on recording deployments via the REST API, see Recording deployments. Important When you add a user to the Heroku add-on, this creates a user record for the user at New Relic. However, if you remove the user from the Heroku add-on, the user record is not automatically removed from New Relic. Instead, you must also manually remove the New Relic user record after removing the user from the Heroku add-on. You can do this by going to the User Management page.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.00778,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Heroku: <em>Install</em> the New Relic add-on",
        "sections": "Manage your Heroku add-on <em>accounts</em>",
        "tags": "<em>Accounts</em> <em>and</em> <em>billing</em>",
        "body": " the newrelic gem to your Gemfile, and then add the following code to your app: configure :production do require &#x27;newrelic_rpm&#x27; end Copy Manage your Heroku add-on <em>accounts</em> Every time you install the New Relic add-on for Heroku, New Relic will automatically create a new <em>account</em>, complete with a unique license"
      },
      "id": "603ebc9ae7b9d272c32a0810"
    },
    {
      "sections": [
        "Rackspace Cloud Load Balancer plugin",
        "Contents",
        "Prerequisites",
        "Installing the plugin",
        "Viewing the Load Balancer"
      ],
      "title": "Rackspace Cloud Load Balancer plugin",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Partner installation"
      ],
      "external_id": "8b0090fd5eb0201e4818992500b53b0295ccbdbc",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/install-new-relic/partner-based-installation/rackspace-cloud-load-balancer-plugin/",
      "published_at": "2021-12-19T14:45:49Z",
      "updated_at": "2021-05-15T22:47:28Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Rackspace offers a plugin for the Cloud Load Balancer product. The plugin allows you to see HTTP vs. HTTPS traffic, easily set alerts to your predefined thresholds, and periodically check the health of the nodes associated with your load balancer to ensure they are responding correctly. Contents Prerequisites Make sure your system has these components installed: Ruby (version 1.8.7 or higher) Rubygems (version 1.3.7 or higher) Ruby bundler gem A New Relic account Rackspace Load Balancer with logging enabled Installing the plugin If you have load balancers in multiple regions, configure a separate plugin for each region. Always use lower case characters for the region's name in your yml file. For more information about YAML parsing, see http://yaml-online-parser.appspot.com/. wget https://github.com/newrelic-platform/newrelic_rackspace_load_balancers_plugin/archive/1.0.0.zip Copy bundle install -binstubs ./bin/newrelic_rs --sample-config Copy Download the zip file from GitHub: Extract the contents into the directory you want to use. Run the following commands from within this directory to get the gem setup: Modify the config/newrelic_plugin.yml with your New Relic license key, Rackspace username and API key, and the region where your load balancer sits. Be sure to use lower case characters for the region name. Start the agent: ./bin/newrelic_rs. Verify that the output says gathering xx statistics. Viewing the Load Balancer Rackspace Cloud Load Balancer After the plugin installs successfully, you can select it in New Relic: Sign in to your New Relic account at rpm.newrelic.com/login. From the New Relic UI, select the Rackspace Cloud Load Balancer icon. From here you can select load balancer(s), view traffic, configure alerts, and more.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 149.88806,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Installing</em> the plugin",
        "tags": "<em>Accounts</em> <em>and</em> <em>billing</em>",
        "body": " Prerequisites Make sure your system has these components installed: Ruby (version 1.8.7 or higher) Rubygems (version 1.3.7 or higher) Ruby bundler gem A New Relic <em>account</em> Rackspace Load Balancer with logging enabled Installing the plugin If you have load balancers in multiple regions, configure"
      },
      "id": "603eb08b28ccbc6215eba77a"
    }
  ],
  "/docs/accounts/install-new-relic/partner-based-installation/windows-azure-users-new-relic": [
    {
      "sections": [
        "RightScale users and New Relic",
        "Add New Relic RightScripts",
        "Example PHP RightScript installation",
        "View APM monitoring data",
        "Upgrade your New Relic subscription"
      ],
      "title": "RightScale users and New Relic",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Partner installation"
      ],
      "external_id": "53775c11e82931aa08ef2d107aaa3f023368559b",
      "image": "https://docs.newrelic.com/static/84cd6a6ee1fe0ce671be3d3d710ab5a4/8c557/screen-nr-rightscale-applist.png",
      "url": "https://docs.newrelic.com/docs/accounts/install-new-relic/partner-based-installation/rightscale-users-new-relic/",
      "published_at": "2021-12-19T14:15:22Z",
      "updated_at": "2021-09-14T05:47:35Z",
      "document_type": "page",
      "popularity": 1,
      "body": "RightScale is a Software as a Service (SaaS) solution capable of hosting web applications. Supported languages for both New Relic and RightScale include Java, Ruby, .NET, PHP, and Python. In order to use New Relic through the RightScale partnership, you need a New Relic account. Before getting started, be sure to enable API access. Add New Relic RightScripts Before you can view data from New Relic in RightScale, add the appropriate New Relic RightScript to your ServerTemplates: From RightCloud, select Design > MultiCloud Marketplace > ServerTemplates. Import the New Relic Performance Monitoring Toolbox ServerTemplate into your Local view. After you import the ServerTemplate, you can use the associated New Relic RightScripts with your account and add them to your ServerTemplates as appropriate for your agent language. New Relic RightScript Notes Java Tomcat For use with Tomcat5 and Tomcat6 ServerTemplates. Installs our Java agent and configures it for use with Tomcat with APM. .NET Installs and configures the New Relic agent to monitor .NET applications. Using New Relic for .NET requires .NET 3.5 or higher on the target host. (If you have only 4.0 or higher installed, you must have .NET Version 3.5 installed on the target host.) With the 3.5 framework, New Relic instruments applications targeting .NET frameworks 2.0 or higher running on Windows Server 2003, Windows Server 2008, Windows Vista, Windows 7, and Windows Azure. PHP Installs and configures the New Relic PHP agent and a local proxy daemon. New Relic supports PHP versions 5.2 or higher on Apache 2.2 or 2.4. Python To install and configure the New Relic Python agent, do not use RightScripts. Instead, send an email to rightscale@newrelic.com. Ruby To use the New Relic agent for your Ruby on Rails application, see New Relic for Ruby. Example PHP RightScript installation Here is an example of adding the APP New Relic RPM for PHP RightScripts to a PHP application server. Add the APP New Relic RPM for PHP RightScript as a boot script to the ServerTemplate that is being used to run your PHP application servers. This will automatically deploy the New Relic agent when a new server is launched with that ServerTemplate. If your PHP application is already running on a server, you can add it as an operational script under the Scripts tab, and run the action on the running server. Be sure to define the following input value for the new script: NR_License_Key: Enter your New Relic license key. Copy You can either manually enter the license key or create a credential for it. View APM monitoring data When viewing the data that New Relic monitors, you will only see the names of the applications that are on servers where you have run the New Relic RightScript. A host is a server and an instance is an instance of an application. RightScale users: Example of apps you can view in New Relic. To view your New Relic data: Go to Reports > New Relic. From the list of application(s) in your New Relic RightScript deployment, select an application name. On your first login, be sure to change your password: From the (account dropdown in the New Relic UI, select User preferences. Use the New Relic menu options to view different performance data and charts. Upgrade your New Relic subscription For RightScale customers, all new accounts start with a free 14-day trial of New Relic's paid subscription features. If you want to upgrade your free account to a paid subscription, send an email to rightscale@newrelic.com. You cannot upgrade your New Relic subscription within RightScale.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.00787,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "RightScale users <em>and</em> New Relic",
        "sections": "Example PHP RightScript <em>installation</em>",
        "tags": "<em>Accounts</em> <em>and</em> <em>billing</em>",
        "body": " customers, all new <em>accounts</em> start with a free 14-day trial of New Relic&#x27;s paid subscription features. If you want to upgrade your free <em>account</em> to a paid subscription, send an email to rightscale@newrelic.com. You cannot upgrade your New Relic subscription within RightScale."
      },
      "id": "603eb08b28ccbc289aeba7a2"
    },
    {
      "sections": [
        "Log in to and install New Relic via partners",
        "Create a New Relic account (all partners)",
        "Sign into and deploy New Relic (most partners)",
        "Sign into and deploy New Relic (special partners)",
        "Tip",
        "A - M partners",
        "Amazon Web Services (AWS)",
        "Google Cloud Platform (GCP)",
        "Heroku",
        "Magento",
        "N - Z partners",
        "Pantheon",
        "Rackspace Cloud Tools",
        "W3 Edge and WordPress",
        "Windows Azure"
      ],
      "title": "Log in to and install New Relic via partners",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Partner installation"
      ],
      "external_id": "d6c766c0328d09f6208426396d5443d115029fcc",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/install-new-relic/partner-based-installation/log-install-new-relic-partners/",
      "published_at": "2021-12-19T14:45:50Z",
      "updated_at": "2021-09-14T05:47:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you signed up for New Relic through one of New Relic's partners, you may need to follow additional single sign on (SSO) procedures to log in to start using New Relic. There also may be special steps needed to complete the deployment process. This document provides basic login procedures for accounts created through New Relic partners, with links to additional resources as applicable. Create a New Relic account (all partners) If you have received a special offer from a New Relic partner, follow the partner's link directly to create a New Relic account. Sign into and deploy New Relic (most partners) You will receive an email confirmation message with a link to confirm your account, sign into New Relic, and complete the deployment process. Sign into and deploy New Relic (special partners) These partners require specific login and deployment procedures for you to use your New Relic account. Tip If you signed up for a New Relic account through a partner that is not listed here, follow standard procedures to sign into and deploy New Relic. A - M partners Amazon Web Services (AWS) Different installation and page viewing procedures apply, depending on whether you want to install AWS for your app or your host. For more information, see Amazon Web Services (AWS) users. Google Cloud Platform (GCP) As a Google Cloud Platform (GCP) partner, New Relic supports the App Engine flexible environment. Each APM agent has its own requirements for the GAE flex environment. After you install the New Relic agent, you can view your app performance with APM. Heroku Heroku is a Platform as a Service (PaaS) solution capable of hosting web applications. With the New Relic Heroku add-on, you can extend Heroku with metrics from APM and browser monitoring. The New Relic add-on supports Java, Node.js, PHP, Python, and Ruby. For more information about installing and using New Relic with Heroku, see: Heroku dev center documentation New Relic's documentation for installing Heroku Magento Magento has partnered with New Relic to give merchants faster access to data and out-of-the-box tools. The New Relic Reporting Extension helps e-commerce businesses to make data-driven decisions. To create a New Relic account with Magento for your PHP app: Sign up via your Magento sales solution provider. After you sign up for your New Relic account, deploy the PHP agent to your Magento installation. Wait a few minutes, then check your app's performance in the APM Overview page. N - Z partners Pantheon To create a New Relic account through Pantheon and sign into New Relic: From Pantheon's partner page with New Relic, select the sign up link. Follow the online instructions to complete the installation process. Wait for New Relic to begin collecting data for your app. After a few minutes, you can check your app's performance in the APM Overview page. For more information, see Pantheon's documentation. Rackspace Cloud Tools To create an account through Rackspace Cloud Tools and sign into New Relic, follow these basic procedures. For more information, see Rackspace Cloud Load Balancer plugin. Log in to your CloudTools account at cloudtools.rackspace.com/myapps. From Use Your Applications, select New Relic. Select your agent language (Java, .NET, PHP, Python, Ruby), then follow the online instructions to complete the installation process. Wait for New Relic to begin collecting data for your app. Wait a few minutes, then check your app's performance in the APM Overview page. W3 Edge and WordPress Tip To create an account through W3 Total Cache and sign into New Relic, you must install and run the W3 PHP agent, not just get an API key. Deploying the agent requires administrative access, including: SSH/command line access Root or sudo access to run the New Relic installer and restart your web server If you are running an economy, shared hosting, or blogger package, or if you are not sure what are your account permissions, contact your hosting provider. For example, if your WordPress site is on shared servers, you may not be able to install New Relic. Your provider should be able to give you information about your account permissions, or upgrade your account if necessary. To install the New Relic plugin for W3 Edge/Wordpress users: Log in to your WordPress account at wordpress.org/support/bb-login.php. Follow New Relic's instructions to complete the installation process, or contact W3TC support for assistance. Wait for New Relic to begin collecting data. Within a few minutes after deploying the agent, performance data about your app will appear on APM's Applications Overview page. Tip WordPress also offers a plugin for our browser monitoring. Windows Azure Windows Azure is a Platform as a Service (PaaS) solution capable of hosting web applications using various languages, including .NET and Node.js. For more information about using our .NET agent with Azure, see: Installing the .NET agent on Azure Azure Preview Portal Azure Web Apps Azure Cloud Services For more information about installing and using our Node.js agent with Azure, see Node.js agent on Microsoft Azure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.00787,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Log in to <em>and</em> <em>install</em> New Relic via <em>partners</em>",
        "sections": "Log in to <em>and</em> <em>install</em> New Relic via <em>partners</em>",
        "tags": "<em>Accounts</em> <em>and</em> <em>billing</em>",
        "body": " for <em>accounts</em> created through New Relic partners, with links to additional resources as applicable. Create a New Relic <em>account</em> (all partners) If you have received a special offer from a New Relic <em>partner</em>, follow the <em>partner</em>&#x27;s link directly to create a New Relic <em>account</em>. Sign into and deploy New Relic (most"
      },
      "id": "6043f447e7b9d2f5295799e5"
    },
    {
      "sections": [
        "Heroku: Install the New Relic add-on",
        "Install the New Relic add-on",
        "Configure the agent",
        "Java",
        "Node.js",
        "PHP",
        "Python",
        "Ruby",
        "Manage your Heroku add-on accounts",
        "License key",
        "Account URL",
        "Account ID",
        "Log on to New Relic",
        "Sign in to New Relic add-on account through Heroku",
        "Sign in to regular New Relic accounts",
        "Set up deployment notifications",
        "Important"
      ],
      "title": "Heroku: Install the New Relic add-on",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Partner installation"
      ],
      "external_id": "45ca4960759150545d4f9e586555d6e15daef3ec",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/install-new-relic/partner-based-installation/heroku-install-new-relic-add/",
      "published_at": "2021-12-19T15:03:36Z",
      "updated_at": "2021-09-14T05:46:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Heroku is a Platform as a Service (PaaS) solution for hosting web applications in various agent languages. With New Relic, you can extend Heroku with metrics from our monitoring solutions, like APM and browser monitoring. The New Relic add-on supports Java, Node.js, PHP, Python, and Ruby. Install the New Relic add-on To set up the New Relic add-on, see Heroku's documentation, including available plan levels and Toolbelt procedures. If you have problems, use the Heroku support channels. Configure the agent After you install the New Relic add-on for Heroku, follow the configuration procedures for your APM agent. Java The minimum agent version for Java is 3.17.1 or higher. To install and configure New Relic's Java agent for your add-on, see our Java agent and Heroku documentation. Node.js To install and configure our Node.js agent for your add-on, see: New Relic's Node.js and Heroku documentation Blog post (2013) with a \"real world\" example of installing our Node.js agent for a Heroku app PHP To install and configure our PHP agent for your add-on, see our PHP agent and Heroku documentation. Python To install and configure our Python agent for your add-on, see our Python agent and Heroku documentation. Ruby To install and configure our Ruby agent for your add-on, see our Ruby agent and Heroku documentation. If you are using our Ruby agent with a non-Rails application, Heroku users need to install the plugin in your repository manually. For example, in a Sinatra app, add the newrelic gem to your Gemfile, and then add the following code to your app: configure :production do require 'newrelic_rpm' end Copy Manage your Heroku add-on accounts Every time you install the New Relic add-on for Heroku, New Relic will automatically create a new account, complete with a unique license key, account URL, and account ID. These three types of information are important for managing each of your Heroku add-on accounts. License key The license key identifies the account where your application reports. To check the license key your app is using: From a command line, run: heroku config | grep -i relic Copy Look for the value of NEW_RELIC_LICENSE_KEY. This license key environment variable will override any other license key that you hard-code into your New Relic config file. Account URL If you install the New Relic add-on multiple times and need to verify the URL your Heroku app uses for reporting to New Relic, look in your agent logs for a line indicating reporting to following by a URL using this format: rpm.newrelic.com/accounts/###/applications/###### Copy Account ID You cannot change your add-on's account ID directly. If you need to change the New Relic account your Heroku app uses for reporting to New Relic, change the current license key environment variable in its config file so that it points to the license key of the New Relic account you want to use: heroku config:set NEW_RELIC_LICENSE_KEY=changed_account_license_key Copy Log on to New Relic Heroku customers may have two different types of accounts with New Relic: Add-on accounts: New Relic accounts that customers installed on their Heroku application by using Heroku's New Relic add-on \"Regular\" accounts: Other New Relic accounts that customers did not install with Heroku's add-on Regular accounts provide a wider range of features than do add-on accounts, and are installed and managed differently. Different procedures apply, depending on which type of account you want to sign into. In accordance with the terms of New Relic's partnership with Heroku, customers who install New Relic via the Heroku add-on can only access their New Relic add-on accounts by signing in through Heroku. For this reason, if you have both add-on accounts and regular New Relic accounts, you cannot switch directly between them. Sign in to New Relic add-on account through Heroku To sign in to your New Relic add-on accounts: Sign in through Heroku's login page at id.heroku.com/login. Select the application that has the New Relic add-on installed. Select New Relic from your list of add-ons. If you sign in through Heroku, you will not see any of your regular New Relic accounts when you select account dropdown > Switch account. Sign in to regular New Relic accounts To sign into or switch between your regular New Relic accounts: Sign in to New Relic at one.newrelic.com. To switch from one regular New Relic account to another: Go to: **account dropdown and select an account. If you sign in directly through New Relic, you will not see any of your New Relic add-on accounts from Heroku when you select account dropdown > Switch account. Set up deployment notifications The Heroku add-on automatically sends deployment notifications to New Relic for one application per account. If you add multiple applications to your add-on account, you must use the New Relic REST API to manually send deployment notifications for your additional applications. You cannot use a post-deploy hook, because the New Relic REST API call requires a header, and Heroku's post-deploy hook does not allow headers. However, you can write a script that generates this API call whenever you deploy on Heroku. For instructions on recording deployments via the REST API, see Recording deployments. Important When you add a user to the Heroku add-on, this creates a user record for the user at New Relic. However, if you remove the user from the Heroku add-on, the user record is not automatically removed from New Relic. Instead, you must also manually remove the New Relic user record after removing the user from the Heroku add-on. You can do this by going to the User Management page.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 158.00778,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Heroku: <em>Install</em> the New Relic add-on",
        "sections": "Manage your Heroku add-on <em>accounts</em>",
        "tags": "<em>Accounts</em> <em>and</em> <em>billing</em>",
        "body": " the newrelic gem to your Gemfile, and then add the following code to your app: configure :production do require &#x27;newrelic_rpm&#x27; end Copy Manage your Heroku add-on <em>accounts</em> Every time you install the New Relic add-on for Heroku, New Relic will automatically create a new <em>account</em>, complete with a unique license"
      },
      "id": "603ebc9ae7b9d272c32a0810"
    }
  ],
  "/docs/accounts/original-accounts-billing/original-data-retention/event-data-retention-original-pricing-plan": [
    {
      "sections": [
        "Overview of data retention (original pricing model)",
        "Important",
        "Limits on editing and deleting data",
        "Product and API data retention policies",
        "APM",
        "APM data retention policies",
        "Legacy APM data retention policies",
        "Browser",
        "Infrastructure",
        "Tip",
        "Infrastructure downsampling and data retention",
        "Integration compute unit event limits",
        "Insights",
        "Logs",
        "Logs in context data retention",
        "Mobile",
        "Mobile data retention policies",
        "Standard Mobile (legacy) data retention policies",
        "Synthetics",
        "Synthetics data retention policies",
        "Metric API",
        "Trace API",
        "Data components",
        "Event data: reported by most products",
        "Metric timeslice data: reported by APM, Browser, and Mobile",
        "Aggregate metric timeslice data: reported by APM, Browser, and Mobile",
        "Key metrics: reported by APM, Browser, and Mobile",
        "Trace data: reported by APM, Browser, and Mobile"
      ],
      "title": "Overview of data retention (original pricing model)",
      "type": "docs",
      "tags": [
        "Accounts",
        "Original accounts and billing",
        "Original data retention"
      ],
      "external_id": "22d5ab9f4d623ead28ee7bb82c118d91804dee22",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/original-accounts-billing/product-based-pricing/overview-data-retention-components/",
      "published_at": "2021-12-19T15:14:48Z",
      "updated_at": "2021-11-14T09:24:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This doc is for accounts on our original Product-based pricing. If you're on our New Relic One pricing model, see Manage your data. Not sure which you're on? See Overview of pricing models. If you're on the original product-based pricing model, you retain your existing subscriptions and data retention values. You manage these existing retention settings from the Data management hub in New Relic One. To manage your retention settings, go to the Data management UI. You'll see your existing retention settings. Adjust retention values by clicking Edit retention. New Relic stores different types of data for different periods of time. The retention period for a type of data will vary depending on the product, the subscription level, and the feature. Limits on editing and deleting data Once telemetry data (events, metrics, logs, traces) is reported to New Relic and available for querying, that data cannot be edited or deleted. This is a purposeful design decision that optimizes New Relic's speed and performance. Data will expire after its data retention ends. If you sent unwanted data or sensitive data to New Relic that must be removed, contact your account representative at New Relic, or get support at support.newrelic.com. Product and API data retention policies Select a New Relic product to see details about its data retention: APM Specific retention policies apply to each APM subscription level, including Lite, Essentials, Pro, and Enterprise. This includes metric timeslice data, key metrics, trace data, and event data. In addition to retention limits, your data is subject to aggregation (averaging) after certain elapsed time periods. For more information, see the aggregate metric data description. APM data retention policies For accounts on our original product-based pricing, APM data retention policies depend on your APM product subscription level. Component Lite Essentials Pro Metric timeslice data 24 hours 3 days 90 days Key metrics none forever forever Distributed tracing and logs in context none none 8 days Other trace data * 1 day 3 days 7 days Event data * * 1 day 3 days 8 days * If you view a transaction trace in the New Relic UI, New Relic stores that trace data for up to one year. * * Learn about how to extend the retention of event data. Legacy APM data retention policies Component Standard Startup Small Business Enterprise Metric timeslice data 7 days 14 days 30 days 90 days Key metrics none none none forever Trace data 7 days 7 days 7 days 7 days Event data none 8 days 8 days 8 days Browser For accounts on our original product-based pricing, Browser data is stored depending on your subscription level: Component Lite Pro Metric data 24 hours 90 days Key metrics 24 hours forever Trace data 7 days 7 days Event data * 1 day 8 days SPA data Unavailable 8 days * Learn about how to extend the retention of event data. Infrastructure For accounts on our original product-based pricing, Infrastructure data retention policies depend on your Infrastructure subscription level and your New Relic Infrastructure compute units pricing model. Data retention rules apply the same whether that data is displayed in the UI or queried. Tip Infrastructure data retention is not governed by your Insights subscription. Infrastructure downsampling and data retention Types of data are stored depending on your subscription level: Component Essentials Pro Infrastructure data 3 months 13 months Host count Host count will stop reflecting a host whose agent is no longer reporting after three minutes. However, host data will be available, subject to other retention criteria. 3 minutes 3 minutes Inventory attributes removed Inventory attributes for a host are retained for 24 hours after the agent stops reporting. 24 hours 24 hours Integration data Not available with Essentials Limit of 2275 integration events per compute unit per month In addition, Infrastructure downsamples your data on the fly, as it's generated. All Infrastructure metric data types (including On-Host Integrations metrics) will display different granularity depending on the age of the data and the size of the time window. The following table illustrates when different downsampled buckets will be used, both in the Infrastructure UI and for queries: Bucket Size Used For Time Windows Covering... Data Retained For... Raw (5, 10 or 15 second) 0 to 59 minutes 7 days 1 minute 60 minutes to 6 hours 30 days 10 minutes 6 hours to 3 days Full account retention period 1 hour 3 days to 14 days Full account retention period 3 hours 14+ days Full account retention period Integration compute unit event limits The 2275 limit on integration events per compute unit per month is a limit on total Infrastructure integration events. It's not a limit of 2275 for each integration's events. Additional details and clarifications about this limit: This limit applies to all events from all integrations (cloud integrations and on-host integrations). The events are all handled the same. Default data received by the Infrastructure agent does not count against the 2275 event per compute unit limit. If you exceed your limit, we do not enforce this limit. If you exceed your limit, we'll review pricing options with you to ensure you get the most cost-effective pricing for your organization's integrations. Insights For accounts on our original product-based pricing, an Insights subscription extends your event data retention. An Insights Pro subscription allows you to customize the length of your event data retention. Logs For accounts on our original product-based pricing, log data can be retained for up to 30 days by New Relic. Shorter retention periods of 8 or 15 days are also available. Logs in context data retention New Relic Logs logs in context data retention policy is based on your current APM product subscription level. For more information, see APM data retention. Mobile For accounts on our original product-based pricing, Mobile data retention policies depend on your New Relic Mobile product subscription level. This includes data components such as metric data, aggregate metrics, key metrics, trace data, and event data. For information about your subscription-related data usage, see Mobile subscription usage. Mobile data retention policies Component Lite Enterprise Overview page data 24 hours 90 days Crash data and stack traces 24 hours 90 days HTTP requests (except errors) as metric data Unavailable 90 days HTTP request errors as metric data Unavailable 3 days Interaction traces Unavailable 90 days Custom events * Unavailable 1 day Mobile events * 1 day 8 days MobileBreadcrumb events * Unavailable 3 days MobileCrash events * 1 day 90 days MobileHandledException events * Unavailable 3 days MobileJSError events (React Native beta) * Unavailable 3 days MobileRequest events * Unavailable 3 days MobileRequestError events * Unavailable 3 days MobileSession events * 1 day 90 days * Learn how to extend retention of event data. Standard Mobile (legacy) data retention policies Unless otherwise noted, Insights event data is unavailable for Standard subscriptions. Component Standard Overview page data 7 days Crash data and stack traces 7 days HTTP requests (except errors) as metric data 7 days HTTP request errors as metric data 3 days Interaction traces Unavailable MobileCrash events 8 days MobileSession events 8 days Synthetics For accounts on our original product-based pricing, Synthetics data retention policies depend on your Synthetics product subscription level. This includes data components such as metric data, aggregate metrics, key metrics, trace data, and Insights events. Tip Synthetics events do not count against an Insights Pro subscription. Your current subscription level appears in the right column of the Account summary page: Go to synthetics.newrelic.com > (account dropdown) > Account settings > Account > Summary. Synthetics data retention policies Component Lite Pro Synthetics monitor data 2 weeks 13 months Event data 2 weeks 13 months Also see the data retention details for APIs, including: Metric API All raw metric data points will be retained for 30 days. All additional aggregated data derived from the raw metric data points (for example, one-minute rollups) will be retained for 13 months. Any change to the retention period beyond such periods may result in a charge to you. Trace API See Trace API general requirements and limits. Data components For accounts on our original product-based pricing, the section below provides an explanation of some types of data components governed by the data retention rules of specific products: Event data: reported by most products See Event data retention for information on the event data type. Metric timeslice data: reported by APM, Browser, and Mobile Metric timeslice data is a specific type of data that is used for most metric charts and reports in APM, mobile, and browser. Important Note that metric timeslice data differs from other metric data types. All metric timeslice data is aggregated, but New Relic deals with fresh data and old data in different ways. Fresh data has specific policies applied to the data to keep granular views of performance as aggregate metrics. As data ages and becomes less useful in a granular state, we summarize that data and only keep key metrics. Aggregate metric timeslice data: reported by APM, Browser, and Mobile Aggregate metric timeslice data summarizes calls to specific methods in your application: how many times each one was called and response times. In the New Relic UI, you see the class and method names along with their aggregate numbers. Metric data aggregation depends on your subscription level. Subscription Level Aggregate retention Enterprise Aggregates (averages) to 1-hour periods after 8 days. After 90 days, the permanent metric data continues to be stored in 1-hour periods. This means you cannot obtain data granularity of less than 1 hour after 8 days, and only a subset of metrics are available after 90 days. Pro After 8 days Essentials After 3 days. Legacy Small Business, Startup, and Standard After 3 days. Lite After 2 hours. We retain your most recent data in one-minute increments. We also aggregate permanent metric data to day-size increments after 90 days. When looking at older data in small time windows, you may notice that charts show less detail. As data ages, it is aggregated into larger segments. Key metrics: reported by APM, Browser, and Mobile New Relic retains certain \"key\" metric timeslice data aggregations forever for Enterprise and Pro customers, for any number of applications. New Relic aggregates permanent key metric data to hour-size increments after 90 days. Product Key metrics APM Apdex, for app server Throughput and page views Response time, plus breakdown into tiers or categories on your app's main Overview chart Page load time, plus breakdown into segments Error rate CPU usage Memory usage Browser Apdex, for browser Browser page load time Throughput total. After 90 days, there is no breakout by browser type, and only the combined average value is available for the time range requested. Mobile Crash rate: For iOS and for Android Network throughput Network response time Network error rates Interaction traces Trace data: reported by APM, Browser, and Mobile Depending on the product, New Relic retains different types of trace data: Product Trace data APM Types of trace data: Transaction traces Distributed tracing Slow query samples Error details See APM data retention details. Browser Types of trace data: Session traces Browser traces JavaScript errors* See Browser data retention details. Mobile Types of trace data: App crash stack traces Interaction traces See Mobile data retention details. * JavaScript errors in the stack trace UI are saved as trace data. JS errors are also saved as events.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 272.34518,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Overview of <em>data</em> <em>retention</em> (<em>original</em> pricing model)",
        "sections": "Overview of <em>data</em> <em>retention</em> (<em>original</em> pricing model)",
        "tags": "<em>Original</em> <em>accounts</em> <em>and</em> <em>billing</em>",
        "body": " timeslice <em>data</em>, key metrics, trace <em>data</em>, and event <em>data</em>. In addition to <em>retention</em> limits, your <em>data</em> is subject to aggregation (averaging) after certain elapsed time periods. For more information, see the aggregate metric <em>data</em> description. APM <em>data</em> <em>retention</em> policies For <em>accounts</em> on our <em>original</em> product"
      },
      "id": "6043f75364441f6967378ec6"
    },
    {
      "sections": [
        "Overview of changes to pricing and user model",
        "Overview of how pricing model and user model relate",
        "Pricing plans explained",
        "Determine pricing model",
        "Convert to new pricing",
        "User models explained",
        "Requirements for new user model",
        "Determine which user model you're on",
        "Feature impacts of user model",
        "Transition to new models"
      ],
      "title": "Overview of changes to pricing and user model",
      "type": "docs",
      "tags": [
        "Accounts",
        "Original accounts and billing",
        "Original product-based pricing"
      ],
      "external_id": "b19d61b5a0c65ca352e2ce0e260e2b53391b94fc",
      "image": "https://docs.newrelic.com/static/1461470cc1ef0be91120c84d76a7ccdd/c1b63/new-relic-one-pricing-plan-billing-ui.png",
      "url": "https://docs.newrelic.com/docs/accounts/original-accounts-billing/original-product-based-pricing/overview-changes-pricing-user-model/",
      "published_at": "2021-12-19T14:16:05Z",
      "updated_at": "2021-11-24T14:34:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In 2020, New Relic released both a newer pricing model and a newer user model. Keep reading to learn about: How the pricing model and the user model relate to each other Pricing plans explained User models explained How to switch to the new models Overview of how pricing model and user model relate In 2020, we released both a new, improved pricing model and a new, improved user model. These models represent the future. All new sign-ups are on these new models and eventually all organizations will be on these models. But currently, our customers from before July 2020 may have one of several combinations of these as they either switch to the new pricing or migrate their users to the new user model. This table shows how pricing and user model relate to each other: Pricing plan factors User model factors Original pricing If your organization was created before July 30 2020, you remain on our original pricing model until you transition to New Relic One pricing. For original pricing accounts, you have users on the original user model unless you've migrated your users to the new model. Note that if you've added users via automated user management, they're on the new model. This means it's possible to have some users on both models. New Relic One pricing An organization is on the New Relic One pricing model: If it was created on or after July 30 2020, or If it's an older organization but has switched to New Relic One pricing. If your organization was created on or after July 30 2020, your users are on the New Relic One user model. If you have an older organization, you have users on the original user model unless you've migrated your users to the new model. Pricing plans explained New Relic organizations are on one of two pricing models: New Relic One pricing: Our new pricing model is simpler. It bills on a) which edition you're on (Standard, Pro, or Enterprise), b) the GBs of data ingested, and c) how many billable full platform users you have. All organizations created on or after July 30 2020 are on this pricing model, as are older organizations that have switched to this pricing. There are two versions of this pricing model. Our original product-based pricing model: this is based on subscriptions to specific products (for example, APM, Browser, Mobile, Infrastructure). Some organizations have the option to transition to the newer pricing model: in that case, their users remain on our original user model. Determine pricing model To determine which pricing model you’re on: go to one.newrelic.com, select the account dropdown, and select Manage your plan. If you see billing information about data ingested and the number of billable users, you’re on the new pricing model. If you're on New Relic One pricing, you'll see a billing UI like this. Convert to new pricing Some organizations are able to switch to new pricing. Learn more about switching your pricing model. User models explained In this context, the term \"user model\" refers to the structure of a New Relic user record and how it interacts with the broader New Relic organization that it's in. Our two user models are: New Relic One user model: this newer, improved user model has these major differences from the original user model: All your accounts and users are contained under a top-level organization. This gives an organization stronger control over managing users and what they can access. One impact of this is that users who work with multiple New Relic organizations may have an email address associated with multiple logins. Users on this model have a different system for managing permissions and account access. Using access grants, you assign users access to specific roles on specific accounts. Original user model: some aspects of this older model that are different from our newer model: There wasn't as much organization-level control over users. For example, a New Relic user had the ability to access multiple accounts they'd been granted access to using a single login. Users on the original user model have a different user management system. To learn more about benefits of the new model, see our blog post about user model changes and an explanation of the new account structure. For impacts and limitations, see Feature impacts. Requirements for new user model Here's a synopsis of the factors governing what organizations use the new model: All New Relic organizations that signed up after July 30 2020 have users on this model, and also have the new pricing model. Some older New Relic organizations have had their users migrated to the new model by New Relic or by using the user migration procedure. Note that switching to the new pricing model is independent from migrating users. Partner accounts (resellers, managed service providers), and organizations using the partnership account structure, cannot yet migrate their users to the new model. Determine which user model you're on To determine what user model you're on, from the account dropdown select User preferences. If you see the UI below, you're on the New Relic One user model. If you see a different UI with more content displayed, you're on the original user model (see original user docs). To determine if you can manage users on the New Relic One user model, see Manage users. The user model is independent of your pricing model. For how user model relates to pricing, see the Pricing and user model table. Feature impacts of user model The new user model offers many benefits. Here are some feature impacts of being on the New Relic One user model that may be not obvious to users accustomed to our previous user model: More separation of organizations/accounts: If you have an email address associated with multiple New Relic accounts, this may mean you need to log out and log back in. Learn more about account access. Adding accounts: Pro and Enterprise edition can have more than one account in their organization. Currently users in these organizations cannot, on their own, add accounts. They must get help from their New Relic account representative. To learn about adding users to accounts, see Manage users. Alerts-related impacts: Policy actions: For alert policies and conditions, some UI locations display the user that made the last modification. For users on the new model, it will display their user ID and not their user name. For example, a policy might read Last modified Sep 7, '20 4:26 pm by user 1234567 instead of by John Doe. If you have user management permissions, you can determine what user this is by going to the User management UI. Notifications: Users on the new model won’t show up as an option when choosing users to receive alert notifications. The result of this is that these users will not get New Relic mobile app push notifications. As a workaround, you can instead use their email address as the notification channel. You can't use the REST API to generate a list of users (only original model users). Instead you'd use the user management UI. Changing account name: These users cannot change the name of their organization after it has been created. Transition to new models Some New Relic organizations on the old models are able to transition to the new models. To learn more about this, see Transition to new models.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 139.1628,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Overview of changes to pricing <em>and</em> user model",
        "sections": "Overview of changes to pricing <em>and</em> user model",
        "tags": "<em>Original</em> <em>accounts</em> <em>and</em> <em>billing</em>",
        "body": ", their users remain on our <em>original</em> user model. Determine pricing model To determine which pricing model you’re on: go to one.newrelic.com, select the <em>account</em> dropdown, and select Manage your plan. If you see <em>billing</em> information about <em>data</em> ingested and the number of billable users, you’re on the new"
      },
      "id": "603e97fa28ccbcb7c2eba754"
    },
    {
      "sections": [
        "Set session timeouts",
        "Original pricing model",
        "Requirements",
        "Overview",
        "Features",
        "Tip",
        "Select the session timeout value",
        "Select SAML SSO browser re-authentication",
        "Redirect after SAML timeout"
      ],
      "title": "Set session timeouts",
      "type": "docs",
      "tags": [
        "Accounts",
        "Original accounts and billing",
        "Original product-based pricing"
      ],
      "external_id": "a61d4c61f52ee18be0763a9cd526634d9d2f50f5",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/account-maintenance/set-session-timeouts/",
      "published_at": "2021-12-19T15:55:42Z",
      "updated_at": "2021-11-14T11:21:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Original pricing model This doc is for users on our original user model. New Relic's session configuration feature allows you to set limits on idle time before your users' browser sessions automatically expire. Requirements If you're on the New Relic One user model, see Session settings. Overview Session configuration allows you to set limits on idle time before your users' browser sessions automatically expire. A message appears three minutes before the system logs them out. Users then need to sign back in to continue. For accounts configured with SAML Single Sign On (SSO), an additional option is available to set how often the users' browser sessions are re-authenticated. Users and Restricted Users can view the time period for automatic timeout, but they cannot change it. To view the timeout value: Go to account dropdown > Account settings > Authentication > Session configuration. Features Tip Owner or Admins The session configuration options provide an additional level of security to ensure that unattended browsers will automatically time out. Session values are automatically stored in the session cookie. Additional features include: Feature Notes Easy setup Admins use the slide bar in New Relic's user interface to select predefined time periods. Default is two weeks. Separate options available by role Admins can choose for Restricted User sessions to never time out even if they select a session timeout setting. This is useful, for example, when you use a Restricted User login for demos. Automatic inheritance for child accounts By default, child accounts inherit the same session configuration as their parent account. Most restrictive by default If users have multiple accounts, the most restrictive setting applies, regardless of which account the user currently is using. Integration with SAML SSO logout URL If the account's SAML SSO configuration does not include a logout URL, New Relic includes a link from Session configuration for the Owner to set it up. If the Admin is not also the Owner, a message about the SAML SSO logout URL requirement appears. Additional re-authentication setting for SAML SSO In addition to the session timeout option, Admins can select the time (15 minutes to 2 weeks, or never) for how often a SAML-authenticated browser session must be re-authenticated. Select the session timeout value The process to select the session timeout value is the same for both SAML and non-SAML configurations. For additional SAML configuration options, see SAML SSO browser reauthentication. To select a predefined period for session timeouts with SAML SSO accounts, the account Owner must have previously identified the logout URL in the SAML SSO configuration settings. If this has not been set up, the account Admin can view the session timeout slide bar but not change it. If the Admin is also the account Owner, the Session configuration includes a link to go directly to New Relic's SAML SSO Configuration and identify the logout URL. For more information, see Setting up SSO. To select a predefined period for session timeouts for users on our original user model: Go to: account dropdown > Account settings > Authentication > Session configuration. Use the slide bar to select a time period for idle sessions to expire and log out automatically. Optional: Select the checkbox option if you do not want restricted users' browser sessions to expire. Select Save my changes. Changes take effect immediately. Select SAML SSO browser re-authentication To select a predefined period for SAML SSO-authenticated browser sessions to be re-authenticated: Go to: account dropdown > Account settings > Authentication > Session configuration. Use the SAML re-authentication time slide bar to select a time period for New Relic to check the browser session. Select Save my changes. Redirect after SAML timeout If you are logged out due to a session idle timeout on an account configured for SAML, you will be sent to the New Relic login page. Because your account is configured for SAML, you do not have a direct New Relic login. To be redirected to your SAML provider for authentication: Enter your email address in the Email field. Leave the Password field blank. Click the Sign In button. You will then be redirected to your SAML provider. Once reauthorized, you will then be returned to the New Relic website.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 130.88927,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Original</em> pricing model",
        "tags": "<em>Original</em> <em>accounts</em> <em>and</em> <em>billing</em>",
        "body": " login for demos. Automatic inheritance for child <em>accounts</em> By default, child <em>accounts</em> inherit the same session configuration as their parent <em>account</em>. Most restrictive by default If users have multiple <em>accounts</em>, the most restrictive setting applies, regardless of which <em>account</em> the user currently"
      },
      "id": "603e8914196a678f45a83de3"
    }
  ],
  "/docs/accounts/original-accounts-billing/original-product-based-pricing/introduction-new-relic-subscription-usage-data": [
    {
      "sections": [
        "Overview of data retention (original pricing model)",
        "Important",
        "Limits on editing and deleting data",
        "Product and API data retention policies",
        "APM",
        "APM data retention policies",
        "Legacy APM data retention policies",
        "Browser",
        "Infrastructure",
        "Tip",
        "Infrastructure downsampling and data retention",
        "Integration compute unit event limits",
        "Insights",
        "Logs",
        "Logs in context data retention",
        "Mobile",
        "Mobile data retention policies",
        "Standard Mobile (legacy) data retention policies",
        "Synthetics",
        "Synthetics data retention policies",
        "Metric API",
        "Trace API",
        "Data components",
        "Event data: reported by most products",
        "Metric timeslice data: reported by APM, Browser, and Mobile",
        "Aggregate metric timeslice data: reported by APM, Browser, and Mobile",
        "Key metrics: reported by APM, Browser, and Mobile",
        "Trace data: reported by APM, Browser, and Mobile"
      ],
      "title": "Overview of data retention (original pricing model)",
      "type": "docs",
      "tags": [
        "Accounts",
        "Original accounts and billing",
        "Original data retention"
      ],
      "external_id": "22d5ab9f4d623ead28ee7bb82c118d91804dee22",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/original-accounts-billing/product-based-pricing/overview-data-retention-components/",
      "published_at": "2021-12-19T15:14:48Z",
      "updated_at": "2021-11-14T09:24:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This doc is for accounts on our original Product-based pricing. If you're on our New Relic One pricing model, see Manage your data. Not sure which you're on? See Overview of pricing models. If you're on the original product-based pricing model, you retain your existing subscriptions and data retention values. You manage these existing retention settings from the Data management hub in New Relic One. To manage your retention settings, go to the Data management UI. You'll see your existing retention settings. Adjust retention values by clicking Edit retention. New Relic stores different types of data for different periods of time. The retention period for a type of data will vary depending on the product, the subscription level, and the feature. Limits on editing and deleting data Once telemetry data (events, metrics, logs, traces) is reported to New Relic and available for querying, that data cannot be edited or deleted. This is a purposeful design decision that optimizes New Relic's speed and performance. Data will expire after its data retention ends. If you sent unwanted data or sensitive data to New Relic that must be removed, contact your account representative at New Relic, or get support at support.newrelic.com. Product and API data retention policies Select a New Relic product to see details about its data retention: APM Specific retention policies apply to each APM subscription level, including Lite, Essentials, Pro, and Enterprise. This includes metric timeslice data, key metrics, trace data, and event data. In addition to retention limits, your data is subject to aggregation (averaging) after certain elapsed time periods. For more information, see the aggregate metric data description. APM data retention policies For accounts on our original product-based pricing, APM data retention policies depend on your APM product subscription level. Component Lite Essentials Pro Metric timeslice data 24 hours 3 days 90 days Key metrics none forever forever Distributed tracing and logs in context none none 8 days Other trace data * 1 day 3 days 7 days Event data * * 1 day 3 days 8 days * If you view a transaction trace in the New Relic UI, New Relic stores that trace data for up to one year. * * Learn about how to extend the retention of event data. Legacy APM data retention policies Component Standard Startup Small Business Enterprise Metric timeslice data 7 days 14 days 30 days 90 days Key metrics none none none forever Trace data 7 days 7 days 7 days 7 days Event data none 8 days 8 days 8 days Browser For accounts on our original product-based pricing, Browser data is stored depending on your subscription level: Component Lite Pro Metric data 24 hours 90 days Key metrics 24 hours forever Trace data 7 days 7 days Event data * 1 day 8 days SPA data Unavailable 8 days * Learn about how to extend the retention of event data. Infrastructure For accounts on our original product-based pricing, Infrastructure data retention policies depend on your Infrastructure subscription level and your New Relic Infrastructure compute units pricing model. Data retention rules apply the same whether that data is displayed in the UI or queried. Tip Infrastructure data retention is not governed by your Insights subscription. Infrastructure downsampling and data retention Types of data are stored depending on your subscription level: Component Essentials Pro Infrastructure data 3 months 13 months Host count Host count will stop reflecting a host whose agent is no longer reporting after three minutes. However, host data will be available, subject to other retention criteria. 3 minutes 3 minutes Inventory attributes removed Inventory attributes for a host are retained for 24 hours after the agent stops reporting. 24 hours 24 hours Integration data Not available with Essentials Limit of 2275 integration events per compute unit per month In addition, Infrastructure downsamples your data on the fly, as it's generated. All Infrastructure metric data types (including On-Host Integrations metrics) will display different granularity depending on the age of the data and the size of the time window. The following table illustrates when different downsampled buckets will be used, both in the Infrastructure UI and for queries: Bucket Size Used For Time Windows Covering... Data Retained For... Raw (5, 10 or 15 second) 0 to 59 minutes 7 days 1 minute 60 minutes to 6 hours 30 days 10 minutes 6 hours to 3 days Full account retention period 1 hour 3 days to 14 days Full account retention period 3 hours 14+ days Full account retention period Integration compute unit event limits The 2275 limit on integration events per compute unit per month is a limit on total Infrastructure integration events. It's not a limit of 2275 for each integration's events. Additional details and clarifications about this limit: This limit applies to all events from all integrations (cloud integrations and on-host integrations). The events are all handled the same. Default data received by the Infrastructure agent does not count against the 2275 event per compute unit limit. If you exceed your limit, we do not enforce this limit. If you exceed your limit, we'll review pricing options with you to ensure you get the most cost-effective pricing for your organization's integrations. Insights For accounts on our original product-based pricing, an Insights subscription extends your event data retention. An Insights Pro subscription allows you to customize the length of your event data retention. Logs For accounts on our original product-based pricing, log data can be retained for up to 30 days by New Relic. Shorter retention periods of 8 or 15 days are also available. Logs in context data retention New Relic Logs logs in context data retention policy is based on your current APM product subscription level. For more information, see APM data retention. Mobile For accounts on our original product-based pricing, Mobile data retention policies depend on your New Relic Mobile product subscription level. This includes data components such as metric data, aggregate metrics, key metrics, trace data, and event data. For information about your subscription-related data usage, see Mobile subscription usage. Mobile data retention policies Component Lite Enterprise Overview page data 24 hours 90 days Crash data and stack traces 24 hours 90 days HTTP requests (except errors) as metric data Unavailable 90 days HTTP request errors as metric data Unavailable 3 days Interaction traces Unavailable 90 days Custom events * Unavailable 1 day Mobile events * 1 day 8 days MobileBreadcrumb events * Unavailable 3 days MobileCrash events * 1 day 90 days MobileHandledException events * Unavailable 3 days MobileJSError events (React Native beta) * Unavailable 3 days MobileRequest events * Unavailable 3 days MobileRequestError events * Unavailable 3 days MobileSession events * 1 day 90 days * Learn how to extend retention of event data. Standard Mobile (legacy) data retention policies Unless otherwise noted, Insights event data is unavailable for Standard subscriptions. Component Standard Overview page data 7 days Crash data and stack traces 7 days HTTP requests (except errors) as metric data 7 days HTTP request errors as metric data 3 days Interaction traces Unavailable MobileCrash events 8 days MobileSession events 8 days Synthetics For accounts on our original product-based pricing, Synthetics data retention policies depend on your Synthetics product subscription level. This includes data components such as metric data, aggregate metrics, key metrics, trace data, and Insights events. Tip Synthetics events do not count against an Insights Pro subscription. Your current subscription level appears in the right column of the Account summary page: Go to synthetics.newrelic.com > (account dropdown) > Account settings > Account > Summary. Synthetics data retention policies Component Lite Pro Synthetics monitor data 2 weeks 13 months Event data 2 weeks 13 months Also see the data retention details for APIs, including: Metric API All raw metric data points will be retained for 30 days. All additional aggregated data derived from the raw metric data points (for example, one-minute rollups) will be retained for 13 months. Any change to the retention period beyond such periods may result in a charge to you. Trace API See Trace API general requirements and limits. Data components For accounts on our original product-based pricing, the section below provides an explanation of some types of data components governed by the data retention rules of specific products: Event data: reported by most products See Event data retention for information on the event data type. Metric timeslice data: reported by APM, Browser, and Mobile Metric timeslice data is a specific type of data that is used for most metric charts and reports in APM, mobile, and browser. Important Note that metric timeslice data differs from other metric data types. All metric timeslice data is aggregated, but New Relic deals with fresh data and old data in different ways. Fresh data has specific policies applied to the data to keep granular views of performance as aggregate metrics. As data ages and becomes less useful in a granular state, we summarize that data and only keep key metrics. Aggregate metric timeslice data: reported by APM, Browser, and Mobile Aggregate metric timeslice data summarizes calls to specific methods in your application: how many times each one was called and response times. In the New Relic UI, you see the class and method names along with their aggregate numbers. Metric data aggregation depends on your subscription level. Subscription Level Aggregate retention Enterprise Aggregates (averages) to 1-hour periods after 8 days. After 90 days, the permanent metric data continues to be stored in 1-hour periods. This means you cannot obtain data granularity of less than 1 hour after 8 days, and only a subset of metrics are available after 90 days. Pro After 8 days Essentials After 3 days. Legacy Small Business, Startup, and Standard After 3 days. Lite After 2 hours. We retain your most recent data in one-minute increments. We also aggregate permanent metric data to day-size increments after 90 days. When looking at older data in small time windows, you may notice that charts show less detail. As data ages, it is aggregated into larger segments. Key metrics: reported by APM, Browser, and Mobile New Relic retains certain \"key\" metric timeslice data aggregations forever for Enterprise and Pro customers, for any number of applications. New Relic aggregates permanent key metric data to hour-size increments after 90 days. Product Key metrics APM Apdex, for app server Throughput and page views Response time, plus breakdown into tiers or categories on your app's main Overview chart Page load time, plus breakdown into segments Error rate CPU usage Memory usage Browser Apdex, for browser Browser page load time Throughput total. After 90 days, there is no breakout by browser type, and only the combined average value is available for the time range requested. Mobile Crash rate: For iOS and for Android Network throughput Network response time Network error rates Interaction traces Trace data: reported by APM, Browser, and Mobile Depending on the product, New Relic retains different types of trace data: Product Trace data APM Types of trace data: Transaction traces Distributed tracing Slow query samples Error details See APM data retention details. Browser Types of trace data: Session traces Browser traces JavaScript errors* See Browser data retention details. Mobile Types of trace data: App crash stack traces Interaction traces See Mobile data retention details. * JavaScript errors in the stack trace UI are saved as trace data. JS errors are also saved as events.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 149.51334,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Overview of data retention (<em>original</em> <em>pricing</em> <em>model</em>)",
        "sections": "Overview of data retention (<em>original</em> <em>pricing</em> <em>model</em>)",
        "tags": "<em>Original</em> <em>accounts</em> <em>and</em> <em>billing</em>",
        "body": "Important This doc is for <em>accounts</em> on our <em>original</em> Product-based <em>pricing</em>. If you&#x27;re on our New Relic One <em>pricing</em> <em>model</em>, see Manage your data. Not sure which you&#x27;re on? See Overview of <em>pricing</em> models. If you&#x27;re on the <em>original</em> product-based <em>pricing</em> <em>model</em>, you retain your existing subscriptions"
      },
      "id": "6043f75364441f6967378ec6"
    },
    {
      "sections": [
        "Event data retention (original pricing model)",
        "Important",
        "Data retention UI",
        "Overview of event data retention",
        "Extend your event retention",
        "Insights Pro",
        "How number of events stored is calculated",
        "Insights Pro event overage example",
        "Disable/enable Transaction and Pageview event reporting",
        "Tip",
        "Flexible data retention",
        "How it works",
        "Manage retention via UI",
        "Glossary"
      ],
      "title": "Event data retention (original pricing model)",
      "type": "docs",
      "tags": [
        "Accounts",
        "Original accounts and billing",
        "Original data retention"
      ],
      "external_id": "76d1289aad7de08b355bb8c313f9e7a42a5779d8",
      "image": "https://docs.newrelic.com/static/e53a1e416eb6116545627d3ec880d08e/e9c9b/flex-2.png",
      "url": "https://docs.newrelic.com/docs/accounts/original-accounts-billing/original-data-retention/event-data-retention-original-pricing-plan/",
      "published_at": "2021-12-19T14:16:05Z",
      "updated_at": "2021-11-14T09:17:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This doc is for accounts on our original pricing model, not our New Relic One pricing model. Not sure which you're on? See Overview of pricing models. For organizations on New Relic One pricing, our various New Relic products report a wide range of event data. Different products have different data retention periods, and different ways to extend event data retention. You can customize the length of your event data retention through flexible event retention. Data retention UI For how to find the data retention UI, see Manage data. Overview of event data retention All New Relic product subscriptions come with a certain level of data retention that governs how long different types of data are retained. One type of data governed by data retention rules is event data. Event data is available in some UI charts and tables, and also available for querying via NRQL, our querying language. There are events reported from products by default, and there are custom events: each have their own retention rules, depending on the product and subscription level. Here are some examples of how different product subscriptions can affect event data retention: Free/Lite APM subscription: default-reported events available for 1 day. No custom events available. Pro APM subscription: default-reported events available for 8 days. Custom events available for 1 day (and able to be extended with Insight Pro). To see your subscriptions, go to the Account summary page. Extend your event retention Product Method APM, Browser, and Mobile Event data retention can be extended with a paid subscription to these products (see product data retention). To extend retention of both default-reported events and custom events further, you need an Insights Pro subscription. Infrastructure Event data retention can be extended with a paid Infrastructure subscription. See Infrastructure data retention rules. Synthetics Event data retention can be extended with a paid Synthetics subscription. See Synthetics data retention rules. Custom events Custom events reported by agent APIs or the Event API: Extension requires an Insights Pro subscription. Insights Pro Important As of April 12, 2021, we are upgrading Insights to an improved web and mobile experience! All of your Insights URLs will be redirected automatically to the corresponding dashboards in New Relic One. For more details about this migration and how you can easily plan for this transition, see our Explorers Hub post. A paid Insights subscription is what governs the extension of event data retention for: Our APM, Browser, Mobile, and Serverless products Custom events that come from an agent API or from the Event API Important Note that having an Insights Pro subscription doesn't require use of the Insights UI (insights.newrelic.com) to query your data: there are other querying options available. To see the data retention governed by your Insights subscription: go to the usage UI and select Insights usage. With an Insights Pro subscription, you can use flexible retention to customize how your event data is retained. This lets you keep only the data you need, for as long as you need it. How number of events stored is calculated This is an explanation of how the number of stored events are calculated by default for an Insights Pro subscription. (Note that with flexible retention, you have more fine-grained control over the retention period.) The events stored is calculated based on 1) total events stored over time (calculated based on the events generated per week) and 2) the weeks of data retention available. This equation can be represented like this: events stored = (events generated per week) * (weeks of retention) Copy An Insights Pro subscription provides a given number of weeks of data retention as well as a given number of events over that retention period. For example: (200M transactions per week) * (4 weeks of retention) = 800M events stored in Insights (16M transactions per week) * (50 weeks of retention) = 800M events stored in Insights For Insights Pro subscriptions, data is purged based on retention window, not volume. It is deleted from the system once it's past the retention window. For example: If your Insights license is for 800 million events with a 4 week retention period, your data would start being purged after it is older than four weeks. Temporary spikes in data exceeding your subscription level will still be recorded, but consistent overage should be solved by upgrading your subscription level or decreasing data collected. For customers without an Insights Pro subscription, New Relic may throttle or downsample events to a limit of not more than than 4,000 events per host per minute. Insights Pro event overage example In this example, you have an Insights Pro subscription with a license for 800 million events over 4 weeks, a rate of 200 million events per week. You have APM Pro, Browser Pro, and Mobile Enterprise. A fifth week of data is added via your subscriptions, bumping you to a total of 1 billion events stored within your plan: If you are using 975 million events, you are not over your retention. If you are using 1.25 billion events, you are over your retention. Disable/enable Transaction and Pageview event reporting Tip Owners or Admins The Insights Data summary UI page is used to see the types of events being reported. You can also use this page to enable and disable the reporting of PageView and Transaction events. To view Data summary: Go to insights.newrelic.com > Manage data. Select the Summary tab. Note: if you disable PageView or Transaction event reporting, this can affect some New Relic UI elements. You may see some empty charts on some UI pages that rely on this data. Go to insights.newrelic.com > Manage data > Summary. From the Summary tab, select Configure data sources. Toggle the appropriate switch on or off, then save. Toggling Transaction on or off will cause reporting agents to restart themselves. For more about configuring event reporting, see Event data retention. Flexible data retention With an Insights Pro subscription, you get access to flexible retention, which lets you define how some types of event data are retained. This lets you keep only the event data you need, for as long as you need it. You can manage your flexible retention through the UI or through our GraphQL API. Requirements to use this feature: An Insights Pro subscription or equivalent trial. Applies only for events governed by an Insights Pro subscription. To use this feature, you must be an account Owner or data retention add-on manager for your account. How it works To understand how standard event data retention works, first read Event data retention. With flexible retention, you specify the data retention for applicable event namespaces across your accounts. This gives you per-event namespace control of your data. The retention that you specify for an event namespace will be shared by all the event types under that namespace. If some namespaces are not relevant to you, you can avoid collecting their event data entirely. Your retention value can’t be lower than the included retention or higher than the default retention. You can control data retention either in our UI or by API. Manage retention via UI You can control data retention either using our GraphQL API or in the UI. To do this with the UI, go to the data retention UI. Your retention changes take effect within 24 hours after updating. Glossary To understand the terms used with flexible retention, see the following: Term Description Event namespace An event's namespace corresponds to one or more event types that share a single data retention value. For more information, see Event namespaces (types). You can also use NerdGraph to get the list of customizable event namespaces. Retention value The number (in days) that specifies how long your event data is stored. Retention rule The event namespace and retention value pair that you specify to override the current retention. Licensed retention Retention period that’s determined in weeks by your Insights Pro subscription contract. Included retention Retention period for which your data is stored but not charged under the Insights Pro subscription. For details, see the data retention details for a specific product. Paid retention Retention period for which your data is stored and is charged under the Insights Pro subscription. By default, your licensed retention determines this value but Flexible retention lets you override it. Default retention Retention period that comes out of the box. This is based on the total of included retention plus licensed retention. For information on managing retention settings with APIs, see the Manage data retention documentation.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 149.5097,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Event data retention (<em>original</em> <em>pricing</em> <em>model</em>)",
        "sections": "Event data retention (<em>original</em> <em>pricing</em> <em>model</em>)",
        "tags": "<em>Original</em> <em>accounts</em> <em>and</em> <em>billing</em>",
        "body": "Important This doc is for <em>accounts</em> on our <em>original</em> <em>pricing</em> <em>model</em>, not our New Relic One <em>pricing</em> <em>model</em>. Not sure which you&#x27;re on? See Overview of <em>pricing</em> models. For organizations on New Relic One <em>pricing</em>, our various New Relic products report a wide range of event data. Different products have"
      },
      "id": "6043f713e7b9d2ccee579a1d"
    },
    {
      "sections": [
        "Overview of changes to pricing and user model",
        "Overview of how pricing model and user model relate",
        "Pricing plans explained",
        "Determine pricing model",
        "Convert to new pricing",
        "User models explained",
        "Requirements for new user model",
        "Determine which user model you're on",
        "Feature impacts of user model",
        "Transition to new models"
      ],
      "title": "Overview of changes to pricing and user model",
      "type": "docs",
      "tags": [
        "Accounts",
        "Original accounts and billing",
        "Original product-based pricing"
      ],
      "external_id": "b19d61b5a0c65ca352e2ce0e260e2b53391b94fc",
      "image": "https://docs.newrelic.com/static/1461470cc1ef0be91120c84d76a7ccdd/c1b63/new-relic-one-pricing-plan-billing-ui.png",
      "url": "https://docs.newrelic.com/docs/accounts/original-accounts-billing/original-product-based-pricing/overview-changes-pricing-user-model/",
      "published_at": "2021-12-19T14:16:05Z",
      "updated_at": "2021-11-24T14:34:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In 2020, New Relic released both a newer pricing model and a newer user model. Keep reading to learn about: How the pricing model and the user model relate to each other Pricing plans explained User models explained How to switch to the new models Overview of how pricing model and user model relate In 2020, we released both a new, improved pricing model and a new, improved user model. These models represent the future. All new sign-ups are on these new models and eventually all organizations will be on these models. But currently, our customers from before July 2020 may have one of several combinations of these as they either switch to the new pricing or migrate their users to the new user model. This table shows how pricing and user model relate to each other: Pricing plan factors User model factors Original pricing If your organization was created before July 30 2020, you remain on our original pricing model until you transition to New Relic One pricing. For original pricing accounts, you have users on the original user model unless you've migrated your users to the new model. Note that if you've added users via automated user management, they're on the new model. This means it's possible to have some users on both models. New Relic One pricing An organization is on the New Relic One pricing model: If it was created on or after July 30 2020, or If it's an older organization but has switched to New Relic One pricing. If your organization was created on or after July 30 2020, your users are on the New Relic One user model. If you have an older organization, you have users on the original user model unless you've migrated your users to the new model. Pricing plans explained New Relic organizations are on one of two pricing models: New Relic One pricing: Our new pricing model is simpler. It bills on a) which edition you're on (Standard, Pro, or Enterprise), b) the GBs of data ingested, and c) how many billable full platform users you have. All organizations created on or after July 30 2020 are on this pricing model, as are older organizations that have switched to this pricing. There are two versions of this pricing model. Our original product-based pricing model: this is based on subscriptions to specific products (for example, APM, Browser, Mobile, Infrastructure). Some organizations have the option to transition to the newer pricing model: in that case, their users remain on our original user model. Determine pricing model To determine which pricing model you’re on: go to one.newrelic.com, select the account dropdown, and select Manage your plan. If you see billing information about data ingested and the number of billable users, you’re on the new pricing model. If you're on New Relic One pricing, you'll see a billing UI like this. Convert to new pricing Some organizations are able to switch to new pricing. Learn more about switching your pricing model. User models explained In this context, the term \"user model\" refers to the structure of a New Relic user record and how it interacts with the broader New Relic organization that it's in. Our two user models are: New Relic One user model: this newer, improved user model has these major differences from the original user model: All your accounts and users are contained under a top-level organization. This gives an organization stronger control over managing users and what they can access. One impact of this is that users who work with multiple New Relic organizations may have an email address associated with multiple logins. Users on this model have a different system for managing permissions and account access. Using access grants, you assign users access to specific roles on specific accounts. Original user model: some aspects of this older model that are different from our newer model: There wasn't as much organization-level control over users. For example, a New Relic user had the ability to access multiple accounts they'd been granted access to using a single login. Users on the original user model have a different user management system. To learn more about benefits of the new model, see our blog post about user model changes and an explanation of the new account structure. For impacts and limitations, see Feature impacts. Requirements for new user model Here's a synopsis of the factors governing what organizations use the new model: All New Relic organizations that signed up after July 30 2020 have users on this model, and also have the new pricing model. Some older New Relic organizations have had their users migrated to the new model by New Relic or by using the user migration procedure. Note that switching to the new pricing model is independent from migrating users. Partner accounts (resellers, managed service providers), and organizations using the partnership account structure, cannot yet migrate their users to the new model. Determine which user model you're on To determine what user model you're on, from the account dropdown select User preferences. If you see the UI below, you're on the New Relic One user model. If you see a different UI with more content displayed, you're on the original user model (see original user docs). To determine if you can manage users on the New Relic One user model, see Manage users. The user model is independent of your pricing model. For how user model relates to pricing, see the Pricing and user model table. Feature impacts of user model The new user model offers many benefits. Here are some feature impacts of being on the New Relic One user model that may be not obvious to users accustomed to our previous user model: More separation of organizations/accounts: If you have an email address associated with multiple New Relic accounts, this may mean you need to log out and log back in. Learn more about account access. Adding accounts: Pro and Enterprise edition can have more than one account in their organization. Currently users in these organizations cannot, on their own, add accounts. They must get help from their New Relic account representative. To learn about adding users to accounts, see Manage users. Alerts-related impacts: Policy actions: For alert policies and conditions, some UI locations display the user that made the last modification. For users on the new model, it will display their user ID and not their user name. For example, a policy might read Last modified Sep 7, '20 4:26 pm by user 1234567 instead of by John Doe. If you have user management permissions, you can determine what user this is by going to the User management UI. Notifications: Users on the new model won’t show up as an option when choosing users to receive alert notifications. The result of this is that these users will not get New Relic mobile app push notifications. As a workaround, you can instead use their email address as the notification channel. You can't use the REST API to generate a list of users (only original model users). Instead you'd use the user management UI. Changing account name: These users cannot change the name of their organization after it has been created. Transition to new models Some New Relic organizations on the old models are able to transition to the new models. To learn more about this, see Transition to new models.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 139.1628,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Overview of changes to <em>pricing</em> <em>and</em> user <em>model</em>",
        "sections": "Overview of changes to <em>pricing</em> <em>and</em> user <em>model</em>",
        "tags": "<em>Original</em> <em>accounts</em> <em>and</em> <em>billing</em>",
        "body": ", their users remain on our <em>original</em> user <em>model</em>. Determine <em>pricing</em> <em>model</em> To determine which <em>pricing</em> <em>model</em> you’re on: go to one.newrelic.com, select the <em>account</em> dropdown, and select Manage your plan. If you see <em>billing</em> information about data ingested and the number of billable users, you’re on the new"
      },
      "id": "603e97fa28ccbcb7c2eba754"
    }
  ],
  "/docs/accounts/original-accounts-billing/original-product-based-pricing/overview-changes-pricing-user-model": [
    {
      "sections": [
        "Set session timeouts",
        "Original pricing model",
        "Requirements",
        "Overview",
        "Features",
        "Tip",
        "Select the session timeout value",
        "Select SAML SSO browser re-authentication",
        "Redirect after SAML timeout"
      ],
      "title": "Set session timeouts",
      "type": "docs",
      "tags": [
        "Accounts",
        "Original accounts and billing",
        "Original product-based pricing"
      ],
      "external_id": "a61d4c61f52ee18be0763a9cd526634d9d2f50f5",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/account-maintenance/set-session-timeouts/",
      "published_at": "2021-12-19T15:55:42Z",
      "updated_at": "2021-11-14T11:21:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Original pricing model This doc is for users on our original user model. New Relic's session configuration feature allows you to set limits on idle time before your users' browser sessions automatically expire. Requirements If you're on the New Relic One user model, see Session settings. Overview Session configuration allows you to set limits on idle time before your users' browser sessions automatically expire. A message appears three minutes before the system logs them out. Users then need to sign back in to continue. For accounts configured with SAML Single Sign On (SSO), an additional option is available to set how often the users' browser sessions are re-authenticated. Users and Restricted Users can view the time period for automatic timeout, but they cannot change it. To view the timeout value: Go to account dropdown > Account settings > Authentication > Session configuration. Features Tip Owner or Admins The session configuration options provide an additional level of security to ensure that unattended browsers will automatically time out. Session values are automatically stored in the session cookie. Additional features include: Feature Notes Easy setup Admins use the slide bar in New Relic's user interface to select predefined time periods. Default is two weeks. Separate options available by role Admins can choose for Restricted User sessions to never time out even if they select a session timeout setting. This is useful, for example, when you use a Restricted User login for demos. Automatic inheritance for child accounts By default, child accounts inherit the same session configuration as their parent account. Most restrictive by default If users have multiple accounts, the most restrictive setting applies, regardless of which account the user currently is using. Integration with SAML SSO logout URL If the account's SAML SSO configuration does not include a logout URL, New Relic includes a link from Session configuration for the Owner to set it up. If the Admin is not also the Owner, a message about the SAML SSO logout URL requirement appears. Additional re-authentication setting for SAML SSO In addition to the session timeout option, Admins can select the time (15 minutes to 2 weeks, or never) for how often a SAML-authenticated browser session must be re-authenticated. Select the session timeout value The process to select the session timeout value is the same for both SAML and non-SAML configurations. For additional SAML configuration options, see SAML SSO browser reauthentication. To select a predefined period for session timeouts with SAML SSO accounts, the account Owner must have previously identified the logout URL in the SAML SSO configuration settings. If this has not been set up, the account Admin can view the session timeout slide bar but not change it. If the Admin is also the account Owner, the Session configuration includes a link to go directly to New Relic's SAML SSO Configuration and identify the logout URL. For more information, see Setting up SSO. To select a predefined period for session timeouts for users on our original user model: Go to: account dropdown > Account settings > Authentication > Session configuration. Use the slide bar to select a time period for idle sessions to expire and log out automatically. Optional: Select the checkbox option if you do not want restricted users' browser sessions to expire. Select Save my changes. Changes take effect immediately. Select SAML SSO browser re-authentication To select a predefined period for SAML SSO-authenticated browser sessions to be re-authenticated: Go to: account dropdown > Account settings > Authentication > Session configuration. Use the SAML re-authentication time slide bar to select a time period for New Relic to check the browser session. Select Save my changes. Redirect after SAML timeout If you are logged out due to a session idle timeout on an account configured for SAML, you will be sent to the New Relic login page. Because your account is configured for SAML, you do not have a direct New Relic login. To be redirected to your SAML provider for authentication: Enter your email address in the Email field. Leave the Password field blank. Click the Sign In button. You will then be redirected to your SAML provider. Once reauthorized, you will then be returned to the New Relic website.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 271.15594,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Original</em> <em>pricing</em> model",
        "tags": "<em>Original</em> <em>product</em>-<em>based</em> <em>pricing</em>",
        "body": "<em>Original</em> <em>pricing</em> model This doc is for users on our <em>original</em> user model. New Relic&#x27;s session configuration feature allows you to set limits on idle time before your users&#x27; browser sessions automatically expire. Requirements If you&#x27;re on the New Relic One user model, see Session settings. Overview"
      },
      "id": "603e8914196a678f45a83de3"
    },
    {
      "sections": [
        "Original product-based pricing and billing",
        "Important",
        "Overview of original pricing",
        "Annual vs monthly pricing models",
        "APM and infrastructure: Compute-unit vs host-based pricing",
        "Compute unit pricing",
        "Host-based pricing",
        "Tip",
        "How is a \"host\" defined?",
        "Prorated billing",
        "Manage subscription and billing settings",
        "View summary information",
        "View or change current subscription",
        "View usage",
        "View or update billing information"
      ],
      "title": "Original product-based pricing and billing",
      "type": "docs",
      "tags": [
        "Accounts",
        "Original accounts and billing",
        "Original product-based pricing"
      ],
      "external_id": "92a9a2aaacf80af45767d6f8f15283c541b2bf08",
      "image": "https://docs.newrelic.com/static/a5a6fd548a3c62e03183f13e6be6688a/77a9e/Accounts_CU-calculation_0.png",
      "url": "https://docs.newrelic.com/docs/accounts/original-accounts-billing/product-pricing/product-based-pricing/",
      "published_at": "2021-12-19T14:24:45Z",
      "updated_at": "2021-11-14T09:27:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This doc explains our original product-based pricing model. For more on pricing and user-related changes, see Overview of changes. Overview of original pricing New Relic has two pricing models: a newer one called New Relic One pricing, and our original pricing model. Our original pricing model was based on subscriptions to specific products, like APM, Mobile, and Infrastructure. If you are on this pricing model, your users are likely on our original user model and use these original user docs. To understand more about the new pricing and user changes, see Overview of changes. For accounts on original pricing, this doc includes: Explanation of how our original pricing model works How to manage subscription and billing settings Annual vs monthly pricing models Here are the differences between billed-annually and billed-monthly plans: Pricing plans Details Annual (best price) New Relic charges your credit card each month for a year for a committed number of hosts or compute units. You can increase this amount at any time, and charges will adjust with the next monthly bill. Your account will automatically renew at the end of the year unless you change your subscription. Early termination, downgrade, or decrease in service: Unless your order form states otherwise, you will be charged at the level and quantity of service ordered until the end of the then-current term if you cancel or downgrade to a lower level of service or fewer hosts during your commitment year. Monthly (no commitment) New Relic charges your credit card each month for a specified number of hosts or compute units. The account Owner can change the credit card number. To edit billing settings, use the Billing UI. Adjustments to billing settings will take effect for the next billing period. Your account automatically renews each month unless you change your subscription. You can cancel service or downgrade to a lower level of service without penalty. APM and infrastructure: Compute-unit vs host-based pricing APM offers a choice between two pricing models: compute unit (CU) based pricing and host-based pricing. New Relic Infrastructure offers only CU-based pricing. This section shows how both options are calculated, and explains what \"host\" means in these pricing contexts: Compute unit pricing CU-based pricing is available for these New Relic products: APM (choice of either CU-based pricing or host-based pricing) Infrastructure: only CU-based pricing With CU-based pricing, your monthly price is determined by the size of the host (computing power and memory) running New Relic and the number of hours it connects to New Relic during the month. If a host is connected to New Relic at any time during an hour, that hour counts towards the CU calculation. Each host is counted separately for each New Relic account the host reports data to. For example, if you have a parent account with two children accounts, each running applications on the same host for 3,000 CUs in a given month, the usage for the parent account will be 6,000 CUs. For APM, CU-based pricing is the best choice if you have many cloud-based dynamic computing resources. For this reason, CU-based pricing is sometimes referred to as cloud pricing. CUs are calculated as follows: The maximum size of a given host (CPUs + GB RAM) is capped at 16. Examples: If a host has 2 CPU cores, 2GB RAM, and connects to New Relic for one hour (or less than one hour), it consumes 4 CUs. If a host has 2 CPU cores, 2GB RAM, and connects to New Relic for an entire month (750 hours used as standard month size), it consumes 3,000 CUs. You can purchase blocks of CUs to be consumed on a monthly basis. The total number of CUs purchased monthly is calculated by adding up the estimated CU consumption for all hosts for the month. There is no month-to-month rollover of unused CUs. Also, New Relic does not charge by JVMs, containers (such as Docker or Cloud Foundry), or application instances--it charges by the hosts running those containers or application instances. Price points vary, depending on the New Relic product and subscription level. You can view CU-based account usage from the New Relic UI. Host-based pricing Tip Pricing for your APM account can be either CU-based or host-based. New Relic Infrastructure uses only CU-based pricing. With host-based pricing, New Relic charges based on the number of equivalent hosts used in a month. One equivalent host is defined as: a host connected to New Relic for 750 hours (750 hours used as standard month size). If a host is connected to New Relic at any time during an hour, that hour counts towards the host calculation. These hours can be divided across multiple hosts. For example, you might have three hosts that are each connected to New Relic for 250 hours during one month: these hours would add up to equal one equivalent host. Each host is counted separately for each New Relic account the host reports data to. For example, if you have a parent account with two child accounts, each running applications on the same single host for 750 hours in a given month, the usage for the parent account will be 2 equivalent hosts. Once connected to New Relic, hosts are distinguished by their unique hostnames. A host is connected to New Relic when the language agent is active and is deployed on the host. New Relic does not charge by containers (such as Docker or Cloud Foundry), JVMs, or application instances; it charges by the hosts running those containers or application instances. New Relic APM gives you a choice between host-based pricing and CU-based pricing. Host-based pricing is ideal if you have mainly static environments, consisting of hosts you manage in your own data center. For specifics on pricing amounts, see the New Relic APM pricing page. How is a \"host\" defined? To understand how New Relic computes both host-based pricing and CU-based pricing, it's important to understand how the word host is used. A host can be one of the following: A physical machine is a hardware-based device with dedicated physical resources, including memory, processing, and storage. Each machine has its own OS which applications run on. A virtual machine (VM) is the software implementation of a physical machine that executes programs like a physical machine. One or more virtual machines can run on a physical machine. Each virtual machine has its own OS and allocated virtual machine resources such as RAM and CPU. A cloud instance is a type of virtual machine that is run in the public cloud. In this context, virtual machines and cloud instances are different from Java Virtual Machines (JVMs) and containers. For New Relic's pricing calculation purposes, a month is defined as 750 hours. Prorated billing If you upgrade your subscription partway through your billing period, you will be subject to a prorated charge for the higher level of service over the remainder of your billing period. This will be invoiced or charged to your credit card when the upgrade is submitted. You will be notified about this charge as part of the subscription change process. If you have questions, contact your New Relic account representative. If you need to report billing issues, contact New Relic's Billing Department. Manage subscription and billing settings Important Note that as of July 30 2020, we have a newer pricing model. To learn more, see Overview of pricing. The account Owner can perform many subscription self-service functions directly from the user interface: From one.newrelic.com, select the account dropdown. Select your choice of self-service options. When making subscription changes, be sure to save any changes, agree to New Relic's Terms of Service and Supplemental Payment Terms as appropriate, and select Pay now. Optional: If you downgrade your subscription, complete New Relic's survey. Here is a summary of the available options from your account dropdown in the New Relic user interface: View summary information To view summary information about your subscription, go to the billing UI. View or change current subscription To adjust your subscription settings, use the Billing UI. If you need more help, contact your New Relic account representative, or contact New Relic's Billing Department. View usage To view your usage, use the usage UI. View or update billing information To view or update your New Relic account's billing information, see the billing UI.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 271.0536,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Original</em> <em>product</em>-<em>based</em> <em>pricing</em> <em>and</em> <em>billing</em>",
        "sections": "<em>Original</em> <em>product</em>-<em>based</em> <em>pricing</em> <em>and</em> <em>billing</em>",
        "tags": "<em>Original</em> <em>product</em>-<em>based</em> <em>pricing</em>",
        "body": ". For <em>accounts</em> on <em>original</em> <em>pricing</em>, this doc includes: Explanation of how our <em>original</em> <em>pricing</em> model works How to manage subscription and <em>billing</em> settings Annual vs monthly <em>pricing</em> models Here are the differences between billed-annually and billed-monthly plans: <em>Pricing</em> plans Details Annual (best <em>price</em>) New"
      },
      "id": "6043f753e7b9d212085799da"
    },
    {
      "sections": [
        "Trial and Lite accounts",
        "Important",
        "Trial accounts",
        "Trial lengths",
        "End of trial period",
        "Caution"
      ],
      "title": "Trial and Lite accounts",
      "type": "docs",
      "tags": [
        "Accounts",
        "Original accounts and billing",
        "Original product-based pricing"
      ],
      "external_id": "050d5fa2eea990cf75a7d4de2c15bebd612860f3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/original-accounts-billing/product-based-pricing/trial-lite-accounts-deprecated/",
      "published_at": "2021-12-19T17:31:49Z",
      "updated_at": "2021-11-14T09:23:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This document applies to our original product-based pricing, which is no longer available for new accounts. For an explanation of pricing models, see Overview of pricing. Free trials for New Relic allow you to identify which products and subscription tier best fit your business. Accounts not upgraded with a subscription before the end of the trial period become Lite accounts, losing many key features and data. Trial accounts When you start a free trial, you gain access to all the features of a Pro account including full access to support. Our products allow you to view and track trends. Pro level data retention allows you to track how changes in your business, such as marketing approaches or new technology, affect trends. Trial lengths Trial lengths depend on the product: Product Trial Length Alerts 30 days APM 14 days Browser 14 days Infrastructure 30 days Insights 30 days Mobile 30 days Synthetics 14 days End of trial period Once the trial ends, your account becomes a Lite account. Lite accounts can access all of our products except Infrastructure and Insights, but lose access to most product features and support. Caution Lite accounts retain only very recent data, which could cause the loss of valuable trend data. Avoid this by subscribing before your trial ends.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 271.05066,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Trial <em>and</em> Lite <em>accounts</em>",
        "sections": "Trial <em>and</em> Lite <em>accounts</em>",
        "tags": "<em>Original</em> <em>product</em>-<em>based</em> <em>pricing</em>",
        "body": "Important This document applies to our <em>original</em> <em>product</em>-<em>based</em> <em>pricing</em>, which is no longer available for new <em>accounts</em>. For an explanation of <em>pricing</em> models, see Overview of <em>pricing</em>. Free trials for New Relic allow you to identify which products and subscription tier best fit your business. <em>Accounts</em>"
      },
      "id": "603ec29a196a67b153a83dad"
    }
  ],
  "/docs/accounts/original-accounts-billing/original-product-based-pricing/switch-new-models": [
    {
      "sections": [
        "Subscription usage (original pricing model)",
        "Important",
        "How it works",
        "Tip",
        "View usage data in UI",
        "UI features",
        "Query usage data",
        "APM CU-based subscription",
        "Data generation",
        "Usage calculations",
        "Table definitions",
        "Query examples",
        "Usage for last month",
        "Monthly usage for last year",
        "Instance-CUs per application",
        "Legacy host usage report",
        "Legacy per-host usage report",
        "Agent version information",
        "Agents needing updates for container tracking",
        "Account hierarchy",
        "Use of Docker or other containers",
        "APM host-based subscription",
        "Usage per host per application",
        "Instance-hours per application",
        "Legacy host usage report (application listing)",
        "Use of Docker and other containers",
        "Browser subscription",
        "Page views for the last complete month",
        "Page views for the last week by account",
        "Page views for the past month, by application:",
        "SPA usage",
        "Notes on Insights subscription",
        "Mobile subscription",
        "Monthly active users (MAUs) for last complete month",
        "Monthly active users (MAUs), by app, for last complete month",
        "Unique mobile apps in the last month",
        "Infrastructure subscription",
        "Usage calculation",
        "Compute units for last month",
        "Detailed host report (reproducing old usage report)",
        "Insights subscription",
        "Event namespaces (types)",
        "Stored event usage for last month",
        "Current Insights usage for an account",
        "Current Insights usage for multiple accounts",
        "Events included with other New Relic products",
        "Retention periods by child account and event namespace",
        "Paid Insights usage by child account",
        "Synthetics subscription",
        "Billable checks for the last month"
      ],
      "title": "Subscription usage (original pricing model)",
      "type": "docs",
      "tags": [
        "Accounts",
        "Original accounts and billing",
        "Original pricing model usage"
      ],
      "external_id": "6bb67fc3c37889ff726c5a9273333e86fdbb34ca",
      "image": "https://docs.newrelic.com/static/228a26d2dd86f40ef06b3397da14bff5/c1b63/New-Relic-usage-UI.png",
      "url": "https://docs.newrelic.com/docs/accounts/original-accounts-billing/original-product-based-pricing/introduction-new-relic-subscription-usage-data/",
      "published_at": "2021-12-15T00:21:42Z",
      "updated_at": "2021-11-12T18:54:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This doc is for accounts on our original Product-based pricing. If you're on the newer pricing model, see New Relic One pricing model. Not sure which you're on? See Overview of pricing models. For accounts on our original pricing model, the usage UI lets you predictably track your data usage, and see how that may affect your pricing. This document explains how the subscription usage UI works, and how you can query and retrieve the usage data you need. To view your usage data: If you are an account owner or admin, you can access subscription usage data in the New Relic UI. If you are not the account owner or admin, you can query usage data by using the NrDailyUsage event. How it works Tip Owner and Admins The subscription usage UI shows how much a New Relic product is being used, and how that usage might affect your account's subscription level and pricing. For example, you can see: How much your organization's departments or child accounts are contributing to billable New Relic usage. This is useful, for example, if your organization uses IT chargebacks. How close you are to going above your Insights Pro subscription level. To understand your product usage, use any of these resources: View Usage UI pages to get an overview of your New Relic product usage. Download usage data CSVs and use other UI features. Run NRQL queries or use Insights query API calls to retrieve focused usage data. View usage data in UI Tip Only the account Owner and Admins can view the usage UI. However, anyone in your account can query usage data using the NrDailyUsage event. To view New Relic subscription usage data: Go to the usage UI. For an overview of account usage across all products, select Usage. To view usage for a specific product, select its Usage page. For organizations on our original pricing model, owners and admins can view usage for all products, or view it by product. Every New Relic product has its own unit of measurement to calculate usage. For example, Mobile tracks monthly active users. Aside from the unit of measurement, the UI charts and features are similar for all products. UI features Here are some of the general features available on New Relic usage UI pages. For product-specific UI and calculation details, see the product usage documentation. If you want to... Do this... Switch time range Use the Time picker to switch between the view of the data from the current calendar month and the data from the last 30 days. All time measurements in the UI use Coordinated Universal Time (UTC). Switch to different account levels To select account levels, such as customer partnerships, parent accounts, and child accounts, select Scope to. Download CSV On product-specific usage pages, select Download CSV to get a comma-separated value file. The columns and data in the CSV match the data shown in the UI. Group data On product-specific pages, the default chart view is organized by Account name. To switch to other ways of grouping your data, such as by application or host, select Group by. View query Mouse over a chart and select , then select View query. See chart segment details To display more details on charts with multiple segments, mouse over any segment. Query usage data New Relic stores your usage data as an NrDailyUsage event type. The usage charts have an associated NRQL query that is close to what you see in the UI. For product-specific attributes and example NRQL queries, keep reading. APM CU-based subscription For accounts on our original pricing model, our usage UI lets you predictably track your data usage, and see how that may affect your pricing. This document explains how New Relic calculates billable usage for APM accounts that have CU-based pricing (not host-based pricing), and also provides available subscription usage attributes and example NRQL queries to use in the New Relic UI or with API calls. Data generation Once per day, an APM account will generate an NrDailyUsage event for: Every application instance created over the last 24 hours Every unique host on which an application instance existed over the last 24 hours These two types of events allow your usage data to be queried and analyzed in many different ways. To query the application events, use a usageType attribute value of Application. To query the host events, use a usageType attribute value of Host. All APM events use a productLine attribute value of APM. For more information, see APM query examples. Usage calculations Monthly billable CUs for a host are calculated by the size of the host running APM (number of CPUs + GBs of RAM) multiplied by the number of hours the host is connected to New Relic during that month. For more on how this is calculated, see CU-based pricing. Tip Only the account Owner and Admins can view the usage UI. However, anyone in your account can query usage data using the NrDailyUsage event. To get an estimate of the current month's eventual billable CUs: Go to the usage UI. On the APM usage page, set the time picker to Current month. Multiply the Avg daily compute units by the number of days in the current month. For more information about the usage UI, see Intro to subscription usage data. Table definitions Here are definitions of the column headers displayed in the UI table and CSV files. The columns visible depend on the Group by option selected at the top of the page. Header Definition Account name The name of the account. This can be a standalone account, a parent account or a customer partnership. Host ID The unique identifier for that host. If the host is in AWS, we use the AWS-provided instance ID. For other hosts, New Relic assigns a host ID. For more about how this value is created, see hostID. Agent hostname The name of the host, as reported by the agent. For more about this value, see agentHostname. Average daily compute units The average daily compute units used. % of total usage The percentage of the total usage used. Query examples Here are some examples of NRQL queries you can use with your account usage data. Usage for last month SELECT sum(apmComputeUnits) FROM NrDailyUsage WHERE usageType='Host' AND productLine='APM' SINCE last month UNTIL this month Copy Monthly usage for last year SELECT SUM(apmComputeUnits) FROM NrDailyUsage WHERE usageType='Host' AND productLine='APM' FACET monthOf(timestamp) SINCE 12 month ago limit 13 Copy Instance-CUs per application This query measures the total number of CUs used by all instances (processes) of an application in the last 24 hours. It's useful for determining which applications are responsible for APM usage, but does not return results that match precisely how New Relic prices APM usage. Each application instance is counted separately in this query, even if it runs concurrently with another application instance on the same host, so the number of application instance-hours is likely to be greater than the number of host-hours (the billable quantity). SELECT sum(apmComputeUnits) FROM NrDailyUsage WHERE usageType='Application' AND productLine='APM' FACET consumingAccountName,consumingAccountId,apmAppName SINCE 1 day ago LIMIT 2000 Copy Legacy host usage report This query returns data once provided in the \"host usage report\": a listing of applications running on each host. Note that while APM usage is counted on an hour-by-hour basis, per-hour usage data is no longer available via the API or UI. FROM NrDailyUsage SELECT min(timestamp) AS 'Earliest reporting day',max(timestamp) AS 'Latest reporting day' FACET apmAppName,hostId,agentHostname,consumingAccountName,consumingAccountId WHERE usageType='Application' AND productLine='APM' SINCE 1 month ago LIMIT 2000 Copy Legacy per-host usage report This query produces a close approximation of the CSV report you would have gotten in the deprecated UI system: FROM NrDailyUsage SELECT consumingAccountId AS 'Account ID', hostId, cloudProvider AS 'Host provider', cloudInstanceType AS 'Instance type', apmBillingInstanceSize AS 'Instance size', apmHoursUsed AS 'Hours used', apmComputeUnits AS 'Usage (CU)', apmMemoryBytes/(1024*1024*1024) AS 'Total RAM', apmProcessorCount AS 'Logical processors', apmContainerCount AS 'Container count', apmComputeUnitRule AS 'Business rule', missingCpuData, missingRamData, instanceSizeCapped, cloudZone, cloudInstanceId WHERE productLine='APM' AND usageType='Host' SINCE 1 day ago LIMIT 2000 Copy This NRQL query is different than the legacy usage report: Per-host query Comments Time period This query includes only the last 24 hours of usage. To see the usage aggregated over a longer time period, change the SINCE clause. Results limited to a maximum number of rows NRQL limits the number of rows returned. If you have more hosts than the maximum number of rows returned and want to get the complete set of hosts, you can narrow your query with more WHERE clauses to return subsets of the data (for example, WHERE cloudInstanceId LIKE “%0”) to divide the data into up to 16 groups, modulo the last character in the AWS instance ID. Docker container IDs A single Docker container ID does not appear in this report. A Docker container ID is reported only at the application instance level of granularity (usageType='Application'). For the host (usageType='Host'), a count of unique containers is reported, since there are often very many. While choosing just one container ID to report for a host had been informative, now there is more data reported at the appropriate level of granularity. Business rule Business rule has been replaced with two attributes: apmComputeUnitRule defines how the size of the host was calculated, depending on what data was available from the agent. It no longer describes whether the agent needs to be updated to provide more accurate host naming in a container environment (update_agent_for_container_environment). missingCpuData, missingRamData, and instanceSizeCapped are boolean flags describing what the value missing_data means when it is present in the apmComputeUnitRule attribute. Agent version information Use this query to see which agent versions are running on your applications in the last 24 hours. This information can be useful in determining whether the agent needs to be upgraded to report a more accurate host name in a Docker container environment (either a Linux boot ID or a cloud provider instance ID). FROM NrDailyUsage SELECT count(*) WHERE productLine='APM' AND usageType='Application' FACET consumingAccountId, consumingAccountName, apmAppName, apmAgentVersion, apmLanguage SINCE 1 day ago LIMIT 2000 Copy Agents needing updates for container tracking This query helps identify applications where an agent is reporting a hostname that may be the same as a container ID. The result of the query is not a definitive list of such applications, but helps you determine which applications may be affected. Hosts that are inaccurately named after a Docker container will have a hostId value that is the same as the first part of the containerId value. The apmComputeUnitRule != ‘cloud_provider_data’ clause removes from consideration hosts that are named by the cloud provider and thus not named after an ephemeral container. FROM NrDailyUsage SELECT latest(hostId) WHERE containerId is not null AND apmComputeUnitRule != 'cloud_provider_data' AND productLine='APM' AND usageType='Application' FACET consumingAccountId,apmAppName,apmAgentVersion,apmLanguage,containerId SINCE 1 day ago LIMIT 2000 Copy Account hierarchy This query is useful for seeing the account hierarchy (partnership, parent, child accounts). SELECT count(*) FROM NrDailyUsage FACET partnershipName, masterAccountName, masterAccountId, consumingAccountName, consumingAccountId LIMIT 2000 SINCE 1 day ago Copy Use of Docker or other containers Some previous APM agents may miscount containers as hosts, which may lead to over-reporting of compute unit (CU) usage. To fix this calculation for Linux-based containers (including Docker), upgrade your APM agent to these versions: Go: 1.11 or higher Java: 3.42.0 or higher .NET: 6.19.330.0 or higher Node.js: 2.1.0 or higher PHP: 7.5.0.199 or higher Python: 2.90.0.75 or higher Ruby: 4.4.0.336 or higher APM host-based subscription For accounts on our original pricing model, this explains how New Relic calculates billable usage for APM accounts that have host-based pricing (not CU-based pricing). This document also provides available subscription usage attributes and example NRQL queries to use in the New Relic UI or with API calls. Data generation Once per day, an APM account will generate an NrDailyUsage event for: Every application instance that existed during the last 24 hours Every unique host on which an application instance ran during the last 24 hours These two types of events allow your usage data to be queried and analyzed in many different ways. To query the application events, use a usageType attribute value of Application. To query the host events, use a usageType attribute value of Host. All APM events use a productLine attribute value of APM. For more information, see APM query examples. Usage calculations For host-based APM pricing, monthly billable usage is determined by the number of equivalent hosts used during that month. An equivalent host is defined as: 750 hours (standardized number of monthly hours) of connection to New Relic by a host or multiple hosts. For more on this calculation, see host-based pricing. Tip Only the account Owner and Admins can view the usage UI. However, anyone in your account can query usage data using the NrDailyUsage event. To get an estimate of the current month's billable usage: Go to the usage UI. On the APM usage page, set the time picker to Current month. See the Avg daily equivalent hosts for an account or grouping of accounts. The UI is meant to estimate your host usage but, especially for cloud environments, your usage may go up or down over time. Query examples Here are some examples of NRQL queries you can use with your account usage data. You can run NRQL queries and use the resulting charts in dashboards. Usage for last month This query uses New Relic’s standard number of hours per month (750) for the purpose of calculating APM equivalent hosts over the last month. SELECT sum(apmHoursUsed)/750 AS 'Equivalent hosts' FROM NrDailyUsage WHERE usageType='Host' AND productLine='APM' SINCE last month UNTIL this month Copy Usage per host per application New Relic records usage per application instance, not strictly per application. This query gives an approximation of the usage for a given application on a given host. If unique application instances run sequentially on a host within a given day, this query could return an underestimate (this would be likely, for example, in a container environment). SELECT max(apmHoursUsed) FROM NrDailyUsage WHERE usageType='Application' AND productLine='APM' FACET consumingAccountName,consumingAccountId,apmAppName,hostId SINCE 1 day ago LIMIT 2000 Copy Instance-hours per application This query measures the total number of hours used by all instances (processes) of an application in the last 24 hours. It's useful for determining which applications are responsible for APM usage, but does not return results that match precisely how New Relic prices APM usage. Each application instance is counted separately in this query, even if it runs concurrently with another application instance on the same host, so the number of application instance-hours is likely to be greater than the number of host-hours (the billable quantity). SELECT sum(apmHoursUsed) FROM NrDailyUsage WHERE usageType='Application' AND productLine='APM' FACET consumingAccountName,consumingAccountId,apmAppName SINCE 1 day ago LIMIT 2000 Copy Agent version information Use this query to see which agent versions are running on your applications in the last 24 hours. This information can be useful in determining whether the agent needs to be upgraded to report a more accurate host name in a Docker container environment (either a Linux boot ID or a cloud provider instance ID). FROM NrDailyUsage SELECT count(*) WHERE productLine='APM' AND usageType='Application' FACET consumingAccountId, consumingAccountName, apmAppName, apmAgentVersion, apmLanguage SINCE 1 day ago LIMIT 2000 Copy Account hierarchy This query is useful for seeing the account hierarchy (partnership, parent accounts, child accounts). SELECT count(*) FROM NrDailyUsage FACET partnershipName,masterAccountName,masterAccountId,consumingAccountName,consumingAccountId LIMIT 2000 SINCE 1 day ago Copy Legacy per-host usage report This query produces a close approximation of the CSV report you would have seen in the deprecated UI system: FROM NrDailyUsage SELECT consumingAccountId AS 'Account ID', hostId, cloudProvider AS 'Host provider', cloudInstanceType AS 'Instance type', apmBillingInstanceSize AS 'Instance size', apmHoursUsed AS 'Hours used', apmComputeUnits AS 'Usage (CU)', apmMemoryBytes/(1024*1024*1024) AS 'Total RAM', apmProcessorCount AS 'Logical processors', apmContainerCount AS 'Container count', apmComputeUnitRule AS 'Business rule', missingCpuData, missingRamData, instanceSizeCapped, cloudZone, cloudInstanceId WHERE productLine='APM' AND usageType='Host' SINCE 1 day ago LIMIT 2000 Copy This NRQL query is different than the legacy usage report: Per-host query Comments Time period This query includes only the last 24 hours of usage. To see the usage aggregated over a longer time period, change the SINCE clause. Results limited to a maximum number of rows NRQL limits the number of rows returned. If you have more hosts than the maximum number of rows returned and want to get the complete set of hosts, you can narrow your query with more WHERE clauses to return subsets of the data (for example WHERE cloudInstanceId LIKE “%0”) to divide the data into up to 16 groups, modulo the last character in the AWS instance ID. Docker container IDs A single Docker container ID does not appear in this report. A Docker container ID is reported only at the application instance level of granularity (usageType='Application'). For the host (usageType='Host'), a count of unique containers is reported, since there are often very many. While choosing just one container ID to report for a host had been informative, now there is more data reported at the appropriate level of granularity. Business rule Business rule has been replaced with other attributes. It no longer is used to describe whether the agent needs to be updated. Legacy host usage report (application listing) This query returns data once provided in the \"host usage report\": a listing of applications running on each host. Note that while APM usage is counted on an hour-by-hour basis, per-hour usage data is no longer available via the API or UI. FROM NrDailyUsage SELECT min(timestamp) AS 'Earliest reporting day',max(timestamp) AS 'Latest reporting day' FACET apmAppName,hostId,agentHostname,consumingAccountName,consumingAccountId WHERE usageType='Application' AND productLine='APM' SINCE 1 month ago LIMIT 2000 Copy Use of Docker and other containers Some previous New Relic APM agents may miscount containers as hosts, which may lead to over-reporting of host-based usage. To fix this calculation for Linux-based containers (including Docker), upgrade your APM agent to these versions: Go: 1.11 or higher Java: 3.42.0 or higher .NET: 6.19.330.0 or higher Node.js: 2.1.0 or higher PHP: 7.5.0.199 or higher Python: 2.90.0.75 or higher Ruby: 4.4.0.336 or higher Browser subscription For accounts on our original pricing model, this explains how we calculate billable usage for Browser subscription usage. This document also provides available subscription usage attributes and example NRQL queries to use in the New Relic UI or with API calls. For details about single-page app (SPA) monitoring usage, see SPA usage. Data generation Once per hour, for every monitored application, a New Relic Browser account will generate a NrUsage event. Each event will summarize the usage for the last hour. When querying Browser usage data, use a productLine attribute value of Browser. For more information, see the Browser query examples. Usage calculations Monthly subscription usage equals the total number of page views that month across all end-user browsers. AJAX traffic does not count against your daily usage. If your page views are fairly steady over time, you can estimate usage for the current month. Tip Only the account Owner and Admins can view the usage UI. However, anyone in your account can query usage data using the NrUsage event. To view usage information in the UI: Go to: account dropdown > View your usage. On the Browser usage page, set the time picker to Last 30 days. Multiply the Avg daily page views by the number of days in the current month. For more information about the usage UI, see Introduction to New Relic subscription usage data. Table definitions Here are definitions of the column headers displayed in the UI table and CSV files. The columns visible depend on the Group by option selected at the top of the page. Header Definition Account name The name of the account. This can be a standalone account, a parent account, or a customer partnership. Account ID New Relic account ID. Average daily page views The average daily page views for that account or application. % of total usage The percentage of the total usage used. Query examples Here are some examples of NRQL queries you can use with your subscription usage data. You can run NRQL queries and use the resulting charts in dashboards. Page views for the last complete month This query shows a count of page views from the last complete month: SELECT sum(usage) FROM NrUsage WHERE productLine='Browser' SINCE last month UNTIL this month Copy Page views for the last week by account This query shows a count of page views from the last week by account: SELECT sum(usage) FROM NrUsage WHERE productLine='Browser' SINCE 7 days ago FACET consumingAccountName Copy Page views for the past month, by application: This query shows a count of page views from the past month by application SELECT sum(usage) FROM NrUsage WHERE productLine='Browser' SINCE 30 days ago FACET browserAppID Copy Account hierarchy This query is useful for seeing the account hierarchy (partnership, parent accounts, child accounts): SELECT count(*) FROM NrUsage FACET partnershipName,masterAccountName,masterAccountId,consumingAccountName,consumingAccountId LIMIT 1000 SINCE 1 day ago Copy SPA usage Page views are used by New Relic Browser to determine customer data usage and product pricing. This document defines a page view in the context of New Relic Browser's SPA monitoring and explains why: SPA monitoring does not affect Browser data usage SPA monitoring will increase Insights data usage In New Relic Browser, a page view is defined as a complete load or reload of a page, signaled by the firing of the window.onload event. New Relic's SPA monitoring tracks traditional page views, but it also tracks changes in the browser that do not require a page load, such as: Route and hash changes Synchronous and asynchronous JavaScript Dynamic server-side updates to a page Route changes are tracked automatically, and by setting up custom instrumentation you can capture almost any type of browser interaction. With New Relic Browser Pro, pricing is based on an account's number of page views per month. If SPA monitoring is enabled, browser interactions that do not require a page load are not counted as page views for billing purposes. With SPA monitoring, you can track an unlimited number of route changes and other custom browser interactions that don't involve page loads. Notes on Insights subscription If you switch from standard Browser monitoring to SPA monitoring, and you also pay for Insights (and don't use only your complementary Insights subscription), your Insights data usage will increase. Because SPA monitoring is a more advanced way to monitor your application, it creates more Insights events than standard monitoring for the following reasons: Page views create not only PageView events, but also BrowserInteraction, AjaxRequest, and BrowserTiming events. For the typical SPA-architecture app, there are more route changes than there are standard page loads. If you pay for Insights and your current Insights license is not sufficient for the amount of events generated with SPA monitoring, we will notify you when you have exceeded your data usage plan. To remedy this, the following options are available: Upgrade your Insights plan. Turn off the reporting of some event types. Use the Browser API to manually turn off collection of some events Switch from Browser SPA monitoring back to standard monitoring Mobile subscription For accounts on our original pricing model, this explains how we calculate billable usage for Mobile accounts. This document also provides available subscription usage attributes and example NRQL queries to use in the New Relic UI or with API calls. Data generation Once per day, for every monitored application, a New Relic Mobile account will generate an NrDailyUsage event. Each event summarizes the Mobile usage for the current calendar month, up to and including the last 24 hours. To query Mobile usage data, use a productLine attribute value of Mobile. For more information, see the Mobile query examples. Usage calculations New Relic Mobile subscription usage is based on the concept of monthly active users (MAUs). For a mobile app monitored by New Relic, MAUs will represent a cumulative count of unique users that used that app during a calendar month. Each unique end-user mobile device counts as a \"user\" for the purposes of this metric. For accounts that have multiple apps, total account usage will represent a sum of the MAUs for all the apps under that account. Tip Only the account Owner and Admins can view the usage UI. However, anyone in your account can query usage data using the NrDailyUsage event. To get an estimate of the current month's billable usage: Go to the usage UI. On the Mobile usage page, set the time picker to Current month. The Mobile usage chart and table display the maximum MAUs that have been reached over a given time period. It will usually ramp up quickly at the beginning of the month, then grow slowly throughout the month. Because it represents the maximum value found so far, it will never go down. This makes Mobile usage different from other New Relic usage measurements, which are based on summing daily usage. For more information about the usage UI, see Introduction to New Relic subscription usage data. Table definitions Here are definitions of the column headers displayed in the UI table and CSV files. The columns visible depend on the Group by option selected at the top of the page. Header Definition Account name The name of the account. This can be a standalone account, a parent account, or a customer partnership. Application name The name of the mobile application. Application ID New Relic Mobile application ID. Maximum MAUs Maximum monthly active users. For the current calendar month, the highest count of active users that has been reached. This number can be compared to the MAUs included in your New Relic subscription. % of total usage The percentage of the total usage that is attributed to the group (account or application, depending on the Group by selection). Query examples Here are some examples of NRQL queries you can use with your subscription usage data. You can run NRQL queries, and use the resulting charts in dashboards. Monthly active users (MAUs) for last complete month A chart of monthly active users for the last complete month: SELECT sum(mobileUniqueUsersPerMonth) FROM NrDailyUsage WHERE usageType='Application' AND productLine='Mobile' SINCE last month UNTIL this month TIMESERIES 1 day Copy The billable MAU number for the month is the maximum number on this chart. You can also get the billable number for a given month directly with this query, using the last day of the month in the since clause, for example: SELECT sum(mobileUniqueUsersPerMonth) FROM NrDailyUsage SINCE '2018-05-31' UNTIL '2018-06-01' Copy Monthly active users (MAUs), by app, for last complete month A count of monthly active users from the last complete month, faceted by application name: SELECT max(mobileUniqueUsersPerMonth) FROM NrDailyUsage WHERE usageType='Application' AND productLine='Mobile' SINCE last month UNTIL this month FACET mobileAppName Copy The sum of these numbers will be the account's billable MAUs for the last complete month. Unique mobile apps in the last month This query shows a count of unique mobile applications over the past month: SELECT uniqueCount(mobileAppName) FROM NrDailyUsage WHERE usageType='Application' AND productLine='Mobile' SINCE 1 month ago Copy Account hierarchy This query is useful for seeing the account hierarchy (partnership, parent accounts, child accounts): SELECT count(*) FROM NrDailyUsage FACET partnershipName,masterAccountName,masterAccountId,consumingAccountName,consumingAccountId LIMIT 1000 SINCE 1 day ago Copy Infrastructure subscription For accounts on our original pricing model, this explains how we calculate billable usage for Infrastructure accounts. This document also provides available subscription usage attributes and example NRQL queries to use in the New Relic UI or with API calls. Data generation Once per day, a New Relic Infrastructure account will generate an NrDailyUsage event for every unique host on which an application instance existed over the last 24 hours. All Infrastructure events have a productLine attribute value of Infrastructure. For more information, see query examples. Usage calculation Monthly billable CUs for a host are calculated by the size of the host running Infrastructure (number of CPUs + GBs of RAM) multiplied by the number of hours the host is connected to New Relic during that month. For more on how this is calculated, see CU-based pricing. If your usage is fairly steady over time, you can estimate usage for the current month. Tip Only the account Owner and Admins can view the usage UI. However, anyone in your account can query usage data using the NrDailyUsage event. To view usage information in the UI: Go to the usage UI. On the Infrastructure usage page, set the time picker to Last 30 days. Multiply the Avg daily compute units by the number of days in the current month. For more information about the usage UI, see Introduction to subscription usage data. Table definitions Here are definitions of the column headers displayed in the UI table and CSV files. The columns displayed depend on the Group by option selected at the top of the page. Header Definition Account name The name of the account. This can be a standalone account, a parent account or a customer partnership. Account ID New Relic account ID. Agent hostname The name of the host, as reported by the agent. For more about this value, see agentHostname. Average daily compute units The average daily compute units used. % of total usage The percentage of the total usage used. Query examples Here are some examples of NRQL queries you can use with your Infrastructure subscription usage data. You can run NRQL queries, and use the resulting charts in dashboards. For general information about how to use NRQL queries to get focused usage data, see Intro to usage data. Compute units for last month This query produces a count of the CUs used over the last month: SELECT sum(infrastructureComputeUnits) FROM NrDailyUsage WHERE productLine='Infrastructure' AND usageType='Host' SINCE last month UNTIL this month Copy Detailed host report (reproducing old usage report) This query reproduces as closely as possible the report you would have gotten by downloading the CSV from the previously available Infrastructure subscription usage UI: FROM NrDailyUsage SELECT consumingAccountId AS 'Account ID',agentHostname,cloudProvider AS 'Host provider', cloudInstanceType AS 'Instance type',infrastructureBillingInstanceSize AS 'Instance size',infrastructureHoursUsed AS 'Hours used',infrastructureComputeUnits AS 'Usage (CU)', infrastructureCloudDerivedMemoryBytes/(1024*1024*1024) AS 'Total RAM',infrastructureCloudDerivedProcessorCount AS 'Logical processors',infrastructureAgentMemoryBytes/(1024*1024*1024) AS 'Total RAM',infrastructureAgentProcessorCount AS 'Logical processors',infrastructureComputeUnitRule AS 'Business rule',missingCpuData, missingRamData, instanceSizeCapped,cloudZone,cloudInstanceId WHERE productLine='Infrastructure' AND usageType='Host' SINCE 1 day ago LIMIT 1000 Copy This NRQL query is different than the legacy usage report: Detailed host query Comments Time period This query includes only the last day of usage. To see the usage aggregated over a longer time period, change the SINCE clause. Results limited to 1000 NRQL limits the results to 1000. If you have more than 1000 hosts and want to get the complete set of hosts, you can narrow your query with more WHERE clauses to return subsets of the data (for example, WHERE agentHostname LIKE ... to divide the data into groups). Business rule Business rule has been replaced with two attributes: infrastructureComputeUnitRule defines how the size of the host was calculated, depending on what data was available from the agent. It no longer describes whether the agent needs to be updated to provide more accurate host naming in a container environment (update_agent_for_container_environment). missingCpuData, missingRamData, and instanceSizeCapped are now boolean flags describing what the value missing_data means when it is present in the infrastructureComputeUnitRule attribute. Account hierarchy This query is useful for seeing the account hierarchy (partnership, parent accounts, child accounts). SELECT count(*) FROM NrDailyUsage FACET partnershipName,masterAccountName,masterAccountId,consumingAccountName,consumingAccountId LIMIT 1000 SINCE 1 day ago Copy Insights subscription For accounts on our original pricing model, this explains: How New Relic calculates billable usage for an Insights Pro subscription. Available subscription usage attributes and example NRQL queries to use in the New Relic UI or in API calls. Data generation Once per day, an account with an Insights Pro subscription will generate an NrDailyUsage event for every event namespace. A NrDailyUsage event includes a count of the events under that namespace. To view the attributes for this event, see the NrDailyUsage entry in the data dictionary. Usage calculations Tip Only the account Owner and Admins can view the usage UI. However, anyone in your account can query usage data using the NrDailyUsage event. To get an estimate of the current month's billable usage: Go to the usage UI. On the Insights usage page, set the time picker to Current month. For more information about the usage UI, see Introduction to subscription usage data. The Insights usage UI only displays paid events: these are events that count towards an Insights Pro subscription. It does not display events that are included free as part of other New Relic product subscriptions. However, you can also query the product included events. How long specific events are retained will depend on each event type's data retention period. For a list of the applicable event types, see Event namespaces. Table definitions Here are definitions of the column headers displayed in the UI table and CSV files. The columns you see depend on the Group by option you select in the UI. Header Definition Account name The name of the account. This can be a standalone account, a parent account, or a partnership. Account ID New Relic account ID. Event namespace The category of event governed by Insights subscription. For example: APM or Mobile error. For more information, see the list of event namespaces. Stored paid events The number of events stored that count towards an Insights Pro subscription. Events included as part of other New Relic product subscriptions are not counted. % of total usage The percentage of the total usage used. Event namespaces (types) An Insights Pro subscription governs the data retention of certain types of events. An event's namespace (indicated by the insightsEventNamespace attribute) corresponds to one or more event types that share a single data retention policy. Event namespace in UI Event namespace when queried Event types APM Transactions APM Transaction APM Errors APM Errors TransactionError Browser Browser Ajax PageAction PageView Page view timing PcvPerf PageViewTiming SPA monitoring Browser:EventLog AjaxRequest BrowserInteraction BrowserTiming JS errors Browser:JSErrors JavaScriptError Custom: * Custom:* Event types stored using custom filters created by New Relic. Not applicable for most customers. Custom events Default Custom event types that you create using New Relic agents or the Event API Crash event trails Breadcrumb MobileBreadcrumb Mobile crash Mobile Crash MobileCrash Mobile error (for HTTP requests and HTTP errors features) Mobile Error MobileRequest MobileRequestError Mobile exception Mobile Exception MobileHandledException Mobile general Mobile General Mobile Mobile session Mobile Session Mobile Session Query examples To get the most out of your Insights usage data, you can: Run queries of this data and create custom charts and dashboards. Use this data programmatically, using one of our APIs. For general information about how to use NRQL queries, see Introduction to usage data. Tip Insights writes usage events once per day. That is why these queries use since 24 hours ago or timeseries 1 day. Here are some NRQL query examples about usage. Stored event usage for last month This query will tell you the \"billable\" number of Insights-governed events for all accounts (parent accounts and child accounts) in the hierarchy under the account where the query is run, for the last calendar month. This number is calculated as the daily average number of stored events. SELECT rate(sum(insightsTotalEventCount)-sum(insightsIncludedEventCount), 1 day) AS 'Paid events' FROM NrDailyUsage SINCE last month UNTIL this month Copy Current Insights usage for an account This query will tell you how many Insights-governed events are currently stored for a specific account, broken down by event namespace. Be sure to replace YOUR_ACCOUNT_ID in these queries with your account ID. SELECT latest(insightsTotalEventCount) FROM NrDailyUsage SINCE 24 hours ago WHERE consumingAccountId = YOUR_ACCOUNT_ID FACET insightsEventNamespace Copy This query will tell you the total current Insights-governed event storage for a specific account, for all event namespaces: SELECT sum(insightsTotalEventCount) FROM NrDailyUsage SINCE 24 hours ago WHERE consumingAccountId = YOUR_ACCOUNT_ID Copy Current Insights usage for multiple accounts This query will tell you how many Insights-governed events are currently stored for a list of accounts by event namespace. Note that you will need to replace LIST_OF_ACCOUNT_IDs in the query with a list of account IDs. SELECT latest(insightsTotalEventCount) FROM NrDailyUsage SINCE 24 hours ago WHERE consumingAccountId IN (LIST_OF_ACCOUNT_IDs) FACET consumingAccountId, insightsEventNamespace LIMIT 100 Copy You may need to increase the value in the limit clause to see all of the facets in this query. Events included with other New Relic products This query will tell you how many Insights-governed events that are included with other New Relic product subscriptions were stored for an account, by the product that created them, over the past seven days. Note that you will need to replace YOUR_ACCOUNT_ID in the query with your account ID. SELECT sum(insightsIncludedEventCount) FROM NrDailyUsage SINCE 7 days ago WHERE consumingAccountId = YOUR_ACCOUNT_ID FACET insightsNrProduct TIMESERIES 1 day Copy Retention periods by child account and event namespace This query will tell you the total, paid, and included retention periods for the different event categories associated with the various New Relic products (such as APM Transaction events), for each child account under the specified parent account ID. When creating your own NRQL queries, be sure to replace YOUR_ACCOUNT_ID with your specific account ID. SELECT latest(insightsTotalRetentionInHours)/24 AS 'Total retention, days', latest(insightsIncludedRetentionInHours)/24 AS 'Included retention, days', (latest(insightsTotalRetentionInHours) - latest(insightsIncludedRetentionInHours))/24 AS 'Paid retention, days' FROM NrDailyUsage where productLine='Insights' AND masterAccountId = YOUR_ACCOUNT_ID FACET consumingAccountId,consumingAccountName,insightsEventNamespace SINCE 1 day ago Copy Paid Insights usage by child account This query will tell you how many events under an Insights Pro subscription are being consumed by each child account for the specified parent account ID over the past seven days. When creating your own NRQL queries, be sure to replace YOUR_MASTER_ACCOUNT_ID in the query with your specific parent account ID. SELECT sum(insightsTotalEventCount)-sum(insightsIncludedEventCount) AS 'Paid events' from NrDailyUsage SINCE 7 days ago WHERE masterAccountId = YOUR_MASTER_ACCOUNT_ID FACET consumingAccountId, consumingAccountName TIMESERIES 1 day Copy Account hierarchy This query is useful for seeing the account hierarchy (partnership, parent accounts, child accounts). SELECT count(*) FROM NrDailyUsage FACET partnershipName,masterAccountName,masterAccountId, consumingAccountName,consumingAccountId LIMIT 1000 SINCE 1 day ago Copy Synthetics subscription For accounts on our original pricing model, this explains how we calculate billable usage for Synthetics accounts. This document also provides available subscription usage attributes and example NRQL queries to use in the New Relic UI or with API calls. Data generation A New Relic Synthetics subscription level is based on the number of non-ping monitor checks used during a calendar month. Once per day, every Synthetics monitor location will generate an NrDailyUsage event for every monitor type. This event will summarize Synthetics usage for that location and monitor type over the last 24 hours. All Synthetics usage events have a productLine attribute value of Synthetics. For more information, see the Synthetics query examples. Usage calculations A New Relic Synthetics subscription level is based on the number of non-ping monitor checks used during a calendar month. If your monitor checks are fairly steady over time, you can estimate the current month's eventual usage. Tip Only the account Owner and Admins can view the usage UI. However, anyone in your account can query usage data using the NrDailyUsage event. To view usage information in the UI: Go to the usage UI. On the Synthetics usage page, set the time picker to Last 30 days. Multiply the Avg daily paid checks by the number of days in the current month. The Synthetics usage chart displays the daily count of monitor checks. The table value Avg daily paid checks displays the total number of monitor checks for the selected time period, divided by the number of days. For more information about the usage UI, see Introduction to subscription usage data. Query examples Here are some examples of NRQL queries you can use with your Synthetics subscription usage data. You can run NRQL queries, and use the resulting charts in dashboards. Billable checks for the last month This query produces a chart of the billable monitor checks over the last month. Remember that a syntheticsType of SIMPLE refers to a ping monitor, which doesn't count towards paid usage. FROM NrDailyUsage SELECT sum(syntheticsFailedCheckCount) + sum(syntheticsSuccessCheckCount) AS 'Paid checks' where productLine='Synthetics' AND syntheticsType != 'SIMPLE' SINCE last month UNTIL this month Copy Account hierarchy This is a non-product-specific query useful for seeing your account hierarchy (partnership, parent accounts, child accounts). SELECT count(*) FROM NrDailyUsage FACET partnershipName,masterAccountName,masterAccountId,consumingAccountName,consumingAccountId LIMIT 1000 SINCE 1 day ago Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 173.17543,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Subscription usage (original <em>pricing</em> <em>model</em>)",
        "sections": "Subscription usage (original <em>pricing</em> <em>model</em>)",
        "tags": "Original <em>pricing</em> <em>model</em> usage",
        "body": "Important This doc is for accounts on our original Product-based <em>pricing</em>. If you&#x27;re on the newer <em>pricing</em> <em>model</em>, see <em>New</em> Relic One <em>pricing</em> <em>model</em>. Not sure which you&#x27;re on? See Overview of <em>pricing</em> models. For accounts on our original <em>pricing</em> <em>model</em>, the usage UI lets you predictably track your data"
      },
      "id": "6186548828ccbca80bcde5b9"
    },
    {
      "sections": [
        "Alerting with New Relic mobile apps",
        "Requirements",
        "Turn notifications on or off",
        "View alert incident details",
        "Troubleshoot alert settings",
        "Check notification settings for your mobile device.",
        "Delete the Android or iOS device from your New Relic account.",
        "Uninstall the New Relic mobile app.",
        "Reinstall the New Relic mobile app."
      ],
      "title": "Alerting with New Relic mobile apps",
      "type": "docs",
      "tags": [
        "Mobile apps",
        "New Relic mobile apps",
        "Authentication and alerts"
      ],
      "external_id": "19bd5fbedd3d386c36cadc21d9dc4c7d6b7f0bce",
      "image": "",
      "url": "https://docs.newrelic.com/docs/mobile-apps/authentication-alerts/alerting-new-relic-mobile-apps/",
      "published_at": "2021-12-19T17:30:17Z",
      "updated_at": "2021-12-14T04:23:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Account administrators can set up configuration to receive push notifications on Android and iOS devices from New Relic Alerts. You can receive alerts from any policy by attaching a user channel to the policy. Requirements This feature is available only to users on the original user model, not to users on the New Relic One user model. As a workaround, you can use the email notification channel. Turn notifications on or off When you log in to your New Relic account from an Android or iOS app, your device is automatically associated with your user channel. Be sure to add the associated user channel to the alert policy. View alert incident details The notification automatically appears on your device's lock screen. To start the New Relic app: Android devices: Tap the notification from the notification drawer. OR iOS devices: Swipe the screen. From the New Relic app's Alerts menu, select any alert to view error details for the associated application. Optional: Select Acknowledge. Optional: To view additional details, select Overview, Violations, or Event log. The main menu's Alerts list shows alerts in the following order, sorted by time: Active incidents Resolved incidents from today Resolved incidents and events from the past week, organized by day Troubleshoot alert settings If alerts are not working on your mobile device: Verify that you meet the requirements. Verify that alerts are enabled. Check your mobile device's notification settings, to ensure New Relic is permitted to send alerts. If the notification settings for your mobile device are correct, but you still do not receive notifications, delete the device from your account, then uninstall and reinstall the New Relic application. Check notification settings for your mobile device. Follow the procedure for your mobile device. Device To check notification settings: Android From your Android device's Settings, select Sound and notification. Check the settings for sound volume. Optional: Enable Also vibrate for calls. Check the settings for Interruptions. Check the settings for Notification. Check the settings for App notifications: Select the New Relic app, then check the settings for Block and Priority. iOS Ensure Do Not Disturb is off: From the iOS Settings app, select Do Not Disturb, and check that the Manual switch is off. Ensure the New Relic app is allowed to send notifications: From the iOS Settings app, select Notifications, and locate the New Relic app from the app list. Ensure that the Allow Notifications switch is on. Ensure that the alert style is set to Banners or Alerts. Optional: To enable audio alerts, set Sounds to on. Delete the Android or iOS device from your New Relic account. To delete the mobile device from your New Relic account, use the public graphql api api.newrelic.com/graphiql in a web browser: Query current devices by selecting actor -> mobilePushNotification -> devices and selecting appVersion, deviceId, and deviceName. Run this query to get the list of devices. Mutate to remove a device by selecting mutation -> mobilePushNotificationRemoveDevice, and passing in the deviceId from the list above. Or you can remove the device from the in-app Settings option from the menu -> Settings Look under Push notification devices, and remove from there. On iOS, slide from right to left to Delete a device, on Android, tap Delete Continue with the steps to reinstall the New Relic app from your device. Uninstall the New Relic mobile app. Follow the procedure to uninstall the New Relic app from your device, then reinstall it. Device To uninstall the New Relic app: Android From your Android device's Settings, select Apps, then select the New Relic app. Select Uninstall. Continue with the steps to reinstall the New Relic app. iOS From your iOS home screen, tap and hold the New Relic icon until it shakes. To delete the app, select the X icon. Continue with the steps to reinstall the New Relic app. Reinstall the New Relic mobile app. To reinstall the New Relic mobile app: From your Android device, select Google Play Store. OR From your iOS device's home screen, select App Store. Search for New Relic. Download the app. When the download finishes, sign in to your New Relic mobile app with your New Relic account.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 139.79103,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Alerting with <em>New</em> Relic mobile apps",
        "sections": "Delete the <em>Android</em> or iOS device from your <em>New</em> Relic account.",
        "tags": "<em>New</em> Relic mobile apps",
        "body": "Account administrators can set up configuration to receive push notifications on Android and iOS devices from <em>New</em> Relic Alerts. You can receive alerts from any policy by attaching a <em>user</em> channel to the policy. Requirements This feature is available only to users on the original <em>user</em> <em>model</em>"
      },
      "id": "603e9efd64441f19a14e88ab"
    },
    {
      "sections": [
        "Overview of changes to pricing and user model",
        "Overview of how pricing model and user model relate",
        "Pricing plans explained",
        "Determine pricing model",
        "Convert to new pricing",
        "User models explained",
        "Requirements for new user model",
        "Determine which user model you're on",
        "Feature impacts of user model",
        "Transition to new models"
      ],
      "title": "Overview of changes to pricing and user model",
      "type": "docs",
      "tags": [
        "Accounts",
        "Original accounts and billing",
        "Original product-based pricing"
      ],
      "external_id": "b19d61b5a0c65ca352e2ce0e260e2b53391b94fc",
      "image": "https://docs.newrelic.com/static/1461470cc1ef0be91120c84d76a7ccdd/c1b63/new-relic-one-pricing-plan-billing-ui.png",
      "url": "https://docs.newrelic.com/docs/accounts/original-accounts-billing/original-product-based-pricing/overview-changes-pricing-user-model/",
      "published_at": "2021-12-19T14:16:05Z",
      "updated_at": "2021-11-24T14:34:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In 2020, New Relic released both a newer pricing model and a newer user model. Keep reading to learn about: How the pricing model and the user model relate to each other Pricing plans explained User models explained How to switch to the new models Overview of how pricing model and user model relate In 2020, we released both a new, improved pricing model and a new, improved user model. These models represent the future. All new sign-ups are on these new models and eventually all organizations will be on these models. But currently, our customers from before July 2020 may have one of several combinations of these as they either switch to the new pricing or migrate their users to the new user model. This table shows how pricing and user model relate to each other: Pricing plan factors User model factors Original pricing If your organization was created before July 30 2020, you remain on our original pricing model until you transition to New Relic One pricing. For original pricing accounts, you have users on the original user model unless you've migrated your users to the new model. Note that if you've added users via automated user management, they're on the new model. This means it's possible to have some users on both models. New Relic One pricing An organization is on the New Relic One pricing model: If it was created on or after July 30 2020, or If it's an older organization but has switched to New Relic One pricing. If your organization was created on or after July 30 2020, your users are on the New Relic One user model. If you have an older organization, you have users on the original user model unless you've migrated your users to the new model. Pricing plans explained New Relic organizations are on one of two pricing models: New Relic One pricing: Our new pricing model is simpler. It bills on a) which edition you're on (Standard, Pro, or Enterprise), b) the GBs of data ingested, and c) how many billable full platform users you have. All organizations created on or after July 30 2020 are on this pricing model, as are older organizations that have switched to this pricing. There are two versions of this pricing model. Our original product-based pricing model: this is based on subscriptions to specific products (for example, APM, Browser, Mobile, Infrastructure). Some organizations have the option to transition to the newer pricing model: in that case, their users remain on our original user model. Determine pricing model To determine which pricing model you’re on: go to one.newrelic.com, select the account dropdown, and select Manage your plan. If you see billing information about data ingested and the number of billable users, you’re on the new pricing model. If you're on New Relic One pricing, you'll see a billing UI like this. Convert to new pricing Some organizations are able to switch to new pricing. Learn more about switching your pricing model. User models explained In this context, the term \"user model\" refers to the structure of a New Relic user record and how it interacts with the broader New Relic organization that it's in. Our two user models are: New Relic One user model: this newer, improved user model has these major differences from the original user model: All your accounts and users are contained under a top-level organization. This gives an organization stronger control over managing users and what they can access. One impact of this is that users who work with multiple New Relic organizations may have an email address associated with multiple logins. Users on this model have a different system for managing permissions and account access. Using access grants, you assign users access to specific roles on specific accounts. Original user model: some aspects of this older model that are different from our newer model: There wasn't as much organization-level control over users. For example, a New Relic user had the ability to access multiple accounts they'd been granted access to using a single login. Users on the original user model have a different user management system. To learn more about benefits of the new model, see our blog post about user model changes and an explanation of the new account structure. For impacts and limitations, see Feature impacts. Requirements for new user model Here's a synopsis of the factors governing what organizations use the new model: All New Relic organizations that signed up after July 30 2020 have users on this model, and also have the new pricing model. Some older New Relic organizations have had their users migrated to the new model by New Relic or by using the user migration procedure. Note that switching to the new pricing model is independent from migrating users. Partner accounts (resellers, managed service providers), and organizations using the partnership account structure, cannot yet migrate their users to the new model. Determine which user model you're on To determine what user model you're on, from the account dropdown select User preferences. If you see the UI below, you're on the New Relic One user model. If you see a different UI with more content displayed, you're on the original user model (see original user docs). To determine if you can manage users on the New Relic One user model, see Manage users. The user model is independent of your pricing model. For how user model relates to pricing, see the Pricing and user model table. Feature impacts of user model The new user model offers many benefits. Here are some feature impacts of being on the New Relic One user model that may be not obvious to users accustomed to our previous user model: More separation of organizations/accounts: If you have an email address associated with multiple New Relic accounts, this may mean you need to log out and log back in. Learn more about account access. Adding accounts: Pro and Enterprise edition can have more than one account in their organization. Currently users in these organizations cannot, on their own, add accounts. They must get help from their New Relic account representative. To learn about adding users to accounts, see Manage users. Alerts-related impacts: Policy actions: For alert policies and conditions, some UI locations display the user that made the last modification. For users on the new model, it will display their user ID and not their user name. For example, a policy might read Last modified Sep 7, '20 4:26 pm by user 1234567 instead of by John Doe. If you have user management permissions, you can determine what user this is by going to the User management UI. Notifications: Users on the new model won’t show up as an option when choosing users to receive alert notifications. The result of this is that these users will not get New Relic mobile app push notifications. As a workaround, you can instead use their email address as the notification channel. You can't use the REST API to generate a list of users (only original model users). Instead you'd use the user management UI. Changing account name: These users cannot change the name of their organization after it has been created. Transition to new models Some New Relic organizations on the old models are able to transition to the new models. To learn more about this, see Transition to new models.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 137.96356,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Overview of changes <em>to</em> <em>pricing</em> <em>and</em> <em>user</em> <em>model</em>",
        "sections": "Overview of changes <em>to</em> <em>pricing</em> <em>and</em> <em>user</em> <em>model</em>",
        "tags": "Original product-based <em>pricing</em>",
        "body": "In 2020, <em>New</em> Relic released both a newer <em>pricing</em> <em>model</em> and a newer <em>user</em> <em>model</em>. Keep reading to learn about: How the <em>pricing</em> <em>model</em> and the <em>user</em> <em>model</em> relate to each other <em>Pricing</em> plans explained <em>User</em> models explained How to <em>switch</em> to the <em>new</em> models Overview of how <em>pricing</em> <em>model</em> and <em>user</em> <em>model</em> relate"
      },
      "id": "603e97fa28ccbcb7c2eba754"
    }
  ],
  "/docs/accounts/original-accounts-billing/original-users-roles/original-account-structure": [
    {
      "sections": [
        "Overview of changes to pricing and user model",
        "Overview of how pricing model and user model relate",
        "Pricing plans explained",
        "Determine pricing model",
        "Convert to new pricing",
        "User models explained",
        "Requirements for new user model",
        "Determine which user model you're on",
        "Feature impacts of user model",
        "Transition to new models"
      ],
      "title": "Overview of changes to pricing and user model",
      "type": "docs",
      "tags": [
        "Accounts",
        "Original accounts and billing",
        "Original product-based pricing"
      ],
      "external_id": "b19d61b5a0c65ca352e2ce0e260e2b53391b94fc",
      "image": "https://docs.newrelic.com/static/1461470cc1ef0be91120c84d76a7ccdd/c1b63/new-relic-one-pricing-plan-billing-ui.png",
      "url": "https://docs.newrelic.com/docs/accounts/original-accounts-billing/original-product-based-pricing/overview-changes-pricing-user-model/",
      "published_at": "2021-12-19T14:16:05Z",
      "updated_at": "2021-11-24T14:34:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In 2020, New Relic released both a newer pricing model and a newer user model. Keep reading to learn about: How the pricing model and the user model relate to each other Pricing plans explained User models explained How to switch to the new models Overview of how pricing model and user model relate In 2020, we released both a new, improved pricing model and a new, improved user model. These models represent the future. All new sign-ups are on these new models and eventually all organizations will be on these models. But currently, our customers from before July 2020 may have one of several combinations of these as they either switch to the new pricing or migrate their users to the new user model. This table shows how pricing and user model relate to each other: Pricing plan factors User model factors Original pricing If your organization was created before July 30 2020, you remain on our original pricing model until you transition to New Relic One pricing. For original pricing accounts, you have users on the original user model unless you've migrated your users to the new model. Note that if you've added users via automated user management, they're on the new model. This means it's possible to have some users on both models. New Relic One pricing An organization is on the New Relic One pricing model: If it was created on or after July 30 2020, or If it's an older organization but has switched to New Relic One pricing. If your organization was created on or after July 30 2020, your users are on the New Relic One user model. If you have an older organization, you have users on the original user model unless you've migrated your users to the new model. Pricing plans explained New Relic organizations are on one of two pricing models: New Relic One pricing: Our new pricing model is simpler. It bills on a) which edition you're on (Standard, Pro, or Enterprise), b) the GBs of data ingested, and c) how many billable full platform users you have. All organizations created on or after July 30 2020 are on this pricing model, as are older organizations that have switched to this pricing. There are two versions of this pricing model. Our original product-based pricing model: this is based on subscriptions to specific products (for example, APM, Browser, Mobile, Infrastructure). Some organizations have the option to transition to the newer pricing model: in that case, their users remain on our original user model. Determine pricing model To determine which pricing model you’re on: go to one.newrelic.com, select the account dropdown, and select Manage your plan. If you see billing information about data ingested and the number of billable users, you’re on the new pricing model. If you're on New Relic One pricing, you'll see a billing UI like this. Convert to new pricing Some organizations are able to switch to new pricing. Learn more about switching your pricing model. User models explained In this context, the term \"user model\" refers to the structure of a New Relic user record and how it interacts with the broader New Relic organization that it's in. Our two user models are: New Relic One user model: this newer, improved user model has these major differences from the original user model: All your accounts and users are contained under a top-level organization. This gives an organization stronger control over managing users and what they can access. One impact of this is that users who work with multiple New Relic organizations may have an email address associated with multiple logins. Users on this model have a different system for managing permissions and account access. Using access grants, you assign users access to specific roles on specific accounts. Original user model: some aspects of this older model that are different from our newer model: There wasn't as much organization-level control over users. For example, a New Relic user had the ability to access multiple accounts they'd been granted access to using a single login. Users on the original user model have a different user management system. To learn more about benefits of the new model, see our blog post about user model changes and an explanation of the new account structure. For impacts and limitations, see Feature impacts. Requirements for new user model Here's a synopsis of the factors governing what organizations use the new model: All New Relic organizations that signed up after July 30 2020 have users on this model, and also have the new pricing model. Some older New Relic organizations have had their users migrated to the new model by New Relic or by using the user migration procedure. Note that switching to the new pricing model is independent from migrating users. Partner accounts (resellers, managed service providers), and organizations using the partnership account structure, cannot yet migrate their users to the new model. Determine which user model you're on To determine what user model you're on, from the account dropdown select User preferences. If you see the UI below, you're on the New Relic One user model. If you see a different UI with more content displayed, you're on the original user model (see original user docs). To determine if you can manage users on the New Relic One user model, see Manage users. The user model is independent of your pricing model. For how user model relates to pricing, see the Pricing and user model table. Feature impacts of user model The new user model offers many benefits. Here are some feature impacts of being on the New Relic One user model that may be not obvious to users accustomed to our previous user model: More separation of organizations/accounts: If you have an email address associated with multiple New Relic accounts, this may mean you need to log out and log back in. Learn more about account access. Adding accounts: Pro and Enterprise edition can have more than one account in their organization. Currently users in these organizations cannot, on their own, add accounts. They must get help from their New Relic account representative. To learn about adding users to accounts, see Manage users. Alerts-related impacts: Policy actions: For alert policies and conditions, some UI locations display the user that made the last modification. For users on the new model, it will display their user ID and not their user name. For example, a policy might read Last modified Sep 7, '20 4:26 pm by user 1234567 instead of by John Doe. If you have user management permissions, you can determine what user this is by going to the User management UI. Notifications: Users on the new model won’t show up as an option when choosing users to receive alert notifications. The result of this is that these users will not get New Relic mobile app push notifications. As a workaround, you can instead use their email address as the notification channel. You can't use the REST API to generate a list of users (only original model users). Instead you'd use the user management UI. Changing account name: These users cannot change the name of their organization after it has been created. Transition to new models Some New Relic organizations on the old models are able to transition to the new models. To learn more about this, see Transition to new models.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 216.49783,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Overview of changes to pricing <em>and</em> <em>user</em> <em>model</em>",
        "sections": "Overview of changes to pricing <em>and</em> <em>user</em> <em>model</em>",
        "tags": "<em>Original</em> <em>accounts</em> <em>and</em> billing",
        "body": " permissions and <em>account</em> access. Using access grants, you assign users access to specific roles on specific accounts. <em>Original</em> <em>user</em> <em>model</em>: some aspects of this older <em>model</em> that are different from our newer <em>model</em>: There wasn&#x27;t as much <em>organization</em>-level control over users. For example, a New Relic <em>user</em>"
      },
      "id": "603e97fa28ccbcb7c2eba754"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/account-structure/new-relic-account-structure/",
      "sections": [
        "Organization and account structure",
        "Important",
        "New Relic One user model",
        "How users access accounts",
        "Original user model"
      ],
      "published_at": "2021-12-19T15:58:31Z",
      "title": "Organization and account structure",
      "updated_at": "2021-11-24T11:32:06Z",
      "type": "docs",
      "external_id": "4f5a4cde293d0b599f489eff010f69c021ccb539",
      "document_type": "page",
      "popularity": 1,
      "body": "Depending on your user model, you have different options for adding and managing accounts and assigning users to them. We have two models: New Relic One user model Original user model Important Note that the user model is not directly related to our two pricing models. New Relic One user model Important This section is about organizations on the New Relic One user model, not the original model. Learn more about the difference. At New Relic, an \"organization\" represents a New Relic customer. The organization contains everything relevant to a New Relic customer: its accounts, its users, and its data. A New Relic \"account\" can be considered a workspace. For example, you might have an account for a specific app, or a set of related hosts and services for a specific initiative or project, or you might have an account for a specific team. Each account has its own account ID, and that ID is used for some account-specific tasks, like making API calls. Our Standard edition allows for a single account per organization. Pro and Enterprise editions allow for multiple accounts per organization. Currently you can't add accounts to your organization on your own. To add accounts, talk to your New Relic account representative. How users access accounts In your organization, your New Relic users are granted access to specific accounts that are relevant to their duties and responsibilities. To manage users’ access to accounts, you create access grants, which assign a group of users to a specific role on a specific account. For example, you may assign a user group the ability to manage billing on some accounts with the Billing manager role, and assign some users as non-admin full platform users on some accounts, and assign some users as basic users on some accounts. Our user management system allows you to create the user access you need, whether that’s a relatively simple setup with just a few roles across a few accounts, or a complex one with many roles across many accounts. Learn more about user management. Note that some features, like dashboards and workloads, can display data from across different accounts in an organization. This means that if a user isn’t granted access to all relevant accounts, they may experience missing data. To learn more about access issues, see Factors affecting access. Original user model See Original user model structure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 212.29198,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Organization</em> <em>and</em> <em>account</em> <em>structure</em>",
        "sections": "<em>Organization</em> <em>and</em> <em>account</em> <em>structure</em>",
        "body": " from across different accounts in an <em>organization</em>. This means that if a <em>user</em> isn’t granted access to all relevant accounts, they may experience missing data. To learn more about access issues, see Factors affecting access. <em>Original</em> <em>user</em> <em>model</em> See <em>Original</em> <em>user</em> <em>model</em> <em>structure</em>."
      },
      "id": "60bee5c028ccbc2413e667e4"
    },
    {
      "sections": [
        "Introduction to New Relic APIs",
        "APIs for data ingest",
        "NerdGraph (GraphQL)",
        "REST API",
        "APIs by feature",
        "Alerts",
        "APM",
        "Browser monitoring",
        "Infrastructure monitoring",
        "Mobile monitoring",
        "Synthetic monitoring",
        "Account management, admin, and usage APIs",
        "Other APIs",
        "Insights"
      ],
      "title": "Introduction to New Relic APIs",
      "type": "docs",
      "tags": [
        "APIs",
        "Get started",
        "Intro to APIs"
      ],
      "external_id": "01e9799a214baad5de04de6146483f6dbbc198aa",
      "image": "",
      "url": "https://docs.newrelic.com/docs/apis/intro-apis/introduction-new-relic-apis/",
      "published_at": "2021-12-19T15:29:05Z",
      "updated_at": "2021-12-19T15:29:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers a variety of APIs and SDKs you can use to: Send data to New Relic. Retrieve data from New Relic. View and configure settings. This document provides examples and reference information for our APIs. APIs for data ingest Our four primary data ingest APIs are some of the many solutions for reporting data to New Relic. These APIs can be used directly, but they're also the underlying ingest route for any of our tools that use those APIs (for example, our OpenTelemetry integration, or our Telemetry SDKs). API type Description Metric API Send dimensional metrics to New Relic from any source (including other telemetry monitoring services). Event API Send custom event data to New Relic without the use of an agent or integration. Log API Send log data to New Relic. Trace API Send distributed tracing data (also referred to as \"spans\") to New Relic without the use of an agent or integration. NerdGraph (GraphQL) NerdGraph is the API we recommend for querying New Relic data, querying account information, and making a range of feature configurations. To learn what you can do, check out the NerdGraph tutorials. NerdGraph is our newest API and is our attempt to bring together in one place some of our older APIs, like our REST API. Note that there is still some functionality you can do with REST APIs that can't yet be done with NerdGraph, and this is why some New Relic organizations still use the REST API. REST API Our REST API is our older API for querying and configuration, which NerdGraph is in the process of replacing. The REST API has some configuration abilities that NerdGraph doesn't yet have, but when possible you should use NerdGraph. The REST API can be used for a wide range of features: for detail, see APIs by feature. APIs by feature New Relic tools and features, like APM, infrastructure monitoring, browser monitoring, and alerts, are often used together, and sometimes can overlap in functionality. This is why multiple APIs may be relevant to each area. Some API functionality will depend on your access to features and data. Alerts Use the REST API for alerts and the API Explorer to: Create and manage policies, conditions, and notification channels. Create alert conditions based on NRQL queries. Create alert conditions based on data from other New Relic capabilities. APM API resources for application monitoring include: Resource Details REST API REST API features include: Retrieve APM data, including metrics, Apdex, error rates, and host data. Report deployments. Change the app name in the UI. Agent APIs Every APM language agent has an API that lets you customize the agent's default behavior, including reporting custom data. APM agent APIs include: C SDK API Go agent API Java agent API .NET agent API Node.js agent API PHP agent API Python agent API Ruby agent API Query API To query APM data, use NerdGraph. Account management APIs For APIs related to accounts and subscription usage, see the account-related APIs. Browser monitoring The browser API resources include: Resource Details Browser agent API Use the browser agent API for tasks such as: Report custom end user data to browser monitoring. Monitor asynchronous browser activity using SPA API calls. Insert custom data into New Relic dashboards . Manage source maps. REST API With the REST API you can: Retrieve page load timing data and throughput. Add or list apps monitored by browser monitoring. Manage alerts conditions for your browser data. Query API To query New Relic data, use NerdGraph. Account management APIs For APIs related to accounts and subscription usage, see the account-related APIs. Infrastructure monitoring The Infrastructure API resources include: Resource Details Query API To query New Relic data, use NerdGraph. Infrastructure alert API To manage your infrastructure alerts, use the Infrastructure alert API. Integrations SDK To make your own custom integrations for reporting data to infrastructure monitoring, use the Integrations SDK. NerdGraph You can use NerdGraph (our GraphQL API) to query your cloud integration data and make changes to cloud integration settings. Mobile monitoring Mobile API resources include: Resource Details Mobile agent APIs Mobile APIs let you custom instrument your own code and send events to New Relic. See the platform-specific documentation: iOS Android REST API Use the REST API for such tasks as: Retrieve a list of monitored apps. Get subscription usage data. Get metric names and data. Get crash count and crash rate data. Manage New Relic alerts conditions for your mobile apps. Query API To query New Relic data, use NerdGraph. Account management APIs For account-related APIs, see Account APIs. Synthetic monitoring Synthetics API resources include: Resource Details Synthetics REST API The Synthetics REST API functionality includes: Create and manage synthetics monitors. Manage synthetics alert notifications. Add labels to monitors, and retrieve monitors with specific labels. Query API To query New Relic data, use NerdGraph. Alerts API To create and manage alert conditions that target synthetics monitors, use the Alerts API. Account management, admin, and usage APIs Like any other New Relic product or service, you want to be confident that your APIs protect you and your customers' data privacy. The following are API resources related to New Relic account administration and usage. For more information about API capabilities, see the specific New Relic API. For more information about New Relic's security measures, see our security and privacy documentation, or visit the New Relic security website. Resource Details REST API REST API features include: Find your API keys, account ID, and information needed to use the REST API. Return a list of account users (original user model only). Get SLA report data for browser and application monitoring. Subscription usage (original pricing model) For organizations on our original pricing model, you can use NerdGraph to query subscription usage data. This can be helpful to see how usage compares to your current subscription level, or for doing departmental chargebacks. Partner API If you're a New Relic partnership organization, you can use the Partner API to retrieve data and make configurations. Other APIs Insights New Relic Insights was the name of our original product that governed custom event reporting and querying. The features associated with Insights have been rolled into our New Relic One platform (learn more), but there are still some APIs and original pricing models that use the term \"Insights\" for these historical reasons. Insights-related APIs include: Resource Details Event API To report custom events, use the Event API. Query API Our Insights Query API is mostly deprecated. Instead, use NerdGraph for querying your New Relic data. Dashboard API Use the Dashboards API.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 209.6594,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Account</em> management, admin, <em>and</em> usage APIs",
        "body": ", and information needed to use the REST API. Return a list of <em>account</em> users (<em>original</em> <em>user</em> <em>model</em> only). Get SLA report data for browser and application monitoring. Subscription usage (<em>original</em> pricing <em>model</em>) For organizations on our <em>original</em> pricing <em>model</em>, you can use NerdGraph to query subscription usage data"
      },
      "id": "609fa5cf196a67066022b194"
    }
  ],
  "/docs/accounts/original-accounts-billing/original-users-roles/parent-child-account-structure": [
    {
      "sections": [
        "Rules and limits for alerts",
        "Permission levels",
        "New Relic One pricing model",
        "Original pricing model",
        "Limits",
        "Tip"
      ],
      "title": "Rules and limits for alerts",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Rules, limits, and glossary"
      ],
      "external_id": "21a79b6a8acf57efc16c3fae83e5167367b82452",
      "image": "",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/learn-alerts/rules-limits-alerts/",
      "published_at": "2021-12-19T16:03:21Z",
      "updated_at": "2021-11-14T07:53:31Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document contains some technical rules and limits for alerts. Permission levels Permissions differ depending on whether you're on our original product-based pricing model or our New Relic One pricing model: New Relic One pricing model See Users and roles. Original pricing model For accounts on our original product-based pricing model, the user role determines the Alerts features available: Owner and Admins can add, change, disable, and delete alert policies, conditions, and channels. Users and Restricted Users can only view alert policies and conditions and their settings. Any user role can acknowledge an incident or close a violation. Limits If your organization has a parent/child account structure, child accounts do not inherit a parent account's alert policies: You must create policies separately for all child accounts. The following rules apply both to the New Relic One user interface and to the REST API (v2). New Relic Alerts Minimum value Maximum value Alert policies: Alert policy name 1 character 64 characters Policies per account n/a 10000 policies Products per policy any New Relic product (APM, mobile monitoring, synthetic monitoring, etc.) any New Relic product Alert conditions: Condition name 1 character 128 characters Conditions per policy 0 conditions 500 conditions Infrastructure alert conditions 0 conditions 3700 conditions NRQL query conditions OR Web app response percentiles per account 0 conditions 4000 conditions Targets (product entities) per condition 1 target 1000 targets from 1 or more products Thresholds per condition 0 Warnings, 1 Critical 1 Warning, 1 Critical Alert violations: Custom violation descriptions 4000 characters Duration for condition violation 5 minutes Exception: 1 minute for at least once conditions, because the violation could occur during the first minute (or any minute). 2 hours Violations per incident 1 violation 10,000 violations Violations beyond this limit will not be persisted. Violation Search API - Page Size 1 page (less than or equal to 25 violations) 1000 pages (25K violations) Tip Only use the only-open parameter to retrieve all open violations. If you have more than 25K open violations and need to retrieve them via the REST API, please contact New Relic Support. Notification channels: Notification channel name 1 character 64 characters Channels per account n/a 2500 channels per type Exception: No limits for user channels Channel limitations Depends on channel Depends on channel",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 840.77936,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": " can acknowledge an incident or close a violation. Limits If your organization has a <em>parent</em>&#x2F;<em>child</em> <em>account</em> <em>structure</em>, <em>child</em> accounts do not inherit a <em>parent</em> <em>account</em>&#x27;s alert policies: You must create policies separately for all <em>child</em> accounts. The following rules apply both to the New Relic One user"
      },
      "id": "60442974196a678217960f33"
    },
    {
      "sections": [
        "Choose your data center (US or EU)",
        "Requirements",
        "Regions and availability",
        "Regions and account hierarchy",
        "Hierarchy example for partnership accounts",
        "Create an EU region account",
        "API endpoints for EU region accounts",
        "Access New Relic One",
        "Billing and pricing",
        "Operational access and processing",
        "Verify your account is based in EU region"
      ],
      "title": "Choose your data center (US or EU)",
      "type": "docs",
      "tags": [
        "Using New Relic",
        "Welcome to New Relic",
        "Get started"
      ],
      "external_id": "58aede83cf1625a8a52aaeed540cebfbaa024d61",
      "image": "https://docs.newrelic.com/docs/accounts/accounts-billing/account-setup/choose-your-data-center/images/Eu_account_hierarchy.png",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/account-setup/choose-your-data-center/",
      "published_at": "2021-12-19T15:40:33Z",
      "updated_at": "2021-12-14T04:22:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's global data hosting structure consists of two regions: the European Union (EU) region and the United States (US) region. Selecting your preferred region during the account setup process allows you to specify the region where your performance monitoring data will be hosted. You can also create accounts in each region. Requirements Access to the New Relic EU region requires the latest agent version. For new customers: Install the most recent agent version. For existing customers: Update to the most recent agent version. Minimum agent version required: C SDK 1.0.0 or higher Go 2.0.0 or higher Java 4.0.0 or higher .NET 8.0.0 or higher Node.js 3.0.0 or higher PHP 8.0.0.204 or higher Python 3.0.0.89 or higher Ruby 5.0.0.342 or higher Regions and availability Your choice of data center is not limited by your geographic location. You can choose to create an account with data hosted in either the EU or US region, regardless of where you or your systems reside. You can also create accounts in each region. New Relic offers almost all the same active products, features, support offerings, and performance levels in the EU region as what is offered in the US region. Exceptions: The following are not supported with an EU region account: Infinite Tracing is not available. APM's weekly performance reports are not available. Errors inbox is not available. Log patterns Deprecated products and features are not available. New Relic's Incident Intelligence service operates solely in the US whether you store your data in New Relic’s US region data center or New Relic’s EU region data center, by using New Relic Incident Intelligence, you consent that New Relic may move and store your data in the US region. New Relic CodeStream operates solely in the US. Whether you have selected New Relic's US or EU region data center during setup of your New Relic account, when using New Relic CodeStream, you consent that your New Relic CodeStream data will get stored in the US. Regions and account hierarchy You can create accounts in each region. If your data is currently being hosted in the US region, you must create a new account to store data in the EU region. You cannot view EU data from a US account, or US data from an EU account. The data collected remains separate. The data cannot be aggregated or migrated between accounts. For organizations that have a parent/child account structure, you can only have one parent account. For more, see Manage apps or users with child accounts. For partnership accounts, no changes to the partnership owner account are required. However, data cannot be shared across regions, so a partnership requires a parent account for each region. Hierarchy example for partnership accounts With partnership accounts, a new parent account must be created for any data to be host in the EU region. This hierarchy illustrates how global accounts are structured with partnership owner accounts. Data is not aggregated beyond the parent account. Example hierarchy for partnership organizations. Because data cannot be shared across regions, a partnership will require a parent account for each region. Create an EU region account To create a New Relic account in the EU region: Go to the New Relic signup page. OR If you have a specific offer from a New Relic partner, follow that link directly. Follow the online steps to create your account. From the Select your region dropdown, select European Union. Agree to the Terms of Service. When you receive an email confirmation message, select the link to confirm your account and sign in to New Relic. Then install or update to the most recent agent version. API endpoints for EU region accounts If you have an EU region account, use the appropriate endpoints to access the following New Relic APIs: API EU endpoint Browser source maps API sourcemaps.service. eu .newrelic.com Copy Infrastructure Alert API infra-api. eu .newrelic.com Copy Dashboard API rpm. eu .newrelic.com/api/explore/dashboards/list Copy Insert API insights-collector. eu01 .nr-data.net Copy Insights Query API insights-api. eu .newrelic.com Copy Mobile apps rpm. eu .newrelic.com/mobile Copy NerdGraph GraphiQL API api. eu .newrelic.com/graphiql Copy Partner API The partner API is a global API with no regional data differences. Use this endpoint for both EU and US accounts: rpm.newrelic.com/api/v2/partners/ Copy REST API api. eu .newrelic.com Copy Synthetics API synthetics. eu .newrelic.com/synthetics/api Copy Trace API trace-api. eu .newrelic.com/trace/v1 Copy Metric API metric-api. eu .newrelic.com/metric/v1 Copy Log API log-api. eu .newrelic.com/log/v1 Copy Access New Relic One If your accounts report data to the EU data center, use the following link to go to New Relic One: one.eu.newrelic.com. Billing and pricing New Relic's account billing process and pricing options are the same for both the EU and US regions. Operational access and processing Customer Data is hosted in the region selected during account creation. Systems Operations Data is stored in the US region. All other information, including account information (such as license subscription information, billing, and internal monitoring) is hosted in the US region and replicated in the EU region. New Relic may access and process Customer Data in the United States and such other jurisdictions where New Relic has affiliates and subsidiaries, including as may be necessary to maintain, secure, or perform the services, to provide technical support, or as necessary to comply with law or a binding order of a government body. Customer Data from existing New Relic accounts cannot be transferred or shared across regions, and new data generated cannot be shared with existing accounts, even in partnership accounts. Verify your account is based in EU region Use either of these options to verify whether your account data is hosted in the EU region's data center: In APM, mouse over the application name to view the URL. If it begins with rpm.eu.newrelic.com/, it is an EU-based account. Check your New Relic license key. If it begins with EU, it is an EU-based account.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 819.35144,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Regions and <em>account</em> hierarchy",
        "body": " <em>structure</em>, you can only have one <em>parent</em> <em>account</em>. For more, see Manage apps or users with <em>child</em> accounts. For partnership accounts, no changes to the partnership owner <em>account</em> are required. However, data cannot be shared across regions, so a partnership requires a <em>parent</em> <em>account</em> for each region"
      },
      "id": "61b81bf6e7b9d2a96fef4e3a"
    },
    {
      "sections": [
        "High security mode",
        "Requirements",
        "Account level",
        "Enable high security mode (version 2)",
        "Caution",
        "Results of enabling high security mode (version 2)",
        "Results of enabling high security mode v1 (deprecated)",
        "Migrate from version 1 to version 2"
      ],
      "title": "High security mode",
      "type": "docs",
      "tags": [
        "Agents",
        "Manage APM agents",
        "Configuration"
      ],
      "external_id": "460ad5339fa585b7fcf6db77644b85ddf015c7d6",
      "image": "",
      "url": "https://docs.newrelic.com/docs/apm/agents/manage-apm-agents/configuration/high-security-mode/",
      "published_at": "2021-12-16T01:44:31Z",
      "updated_at": "2021-12-10T00:20:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's default APM agent settings provide a high level of security. However, you may need to guarantee that even if the default APM agent settings are overridden to be more permissive, no sensitive data will ever be sent to New Relic. If this is the case, then you will want to turn on APM's high security mode (also known as enterprise security mode). For more information about our security measures, see our security and privacy documentation, or visit the New Relic security website. Requirements Customers on our New Relic One pricing model require Enterprise edition. Customers on our original pricing model have access depending on their subscription level. Account level If you choose to turn on high security, you must enable high security for all applications reporting to the account. High security must be set on each individual account. For organizations that have a parent/child account structure, child accounts don't automatically inherit the high security setting when enabled on the parent account. Currently there are two versions of high security mode. Version 1 is deprecated and is only available if you already have it. If you are enabling high security mode for the first time, the only option is version 2 (v2). Agent Version 2 support C SDK n/a Go All versions Java 3.7 or higher (enabled by default) .NET 3.3 or higher Node.js 1.7.0 or higher PHP 4.9 or higher Python 2.22.0.0 or higher Ruby 3.9.1 or higher Enable high security mode (version 2) To enable high security, you must update both the local configuration on your server and the remote configuration in the UI. Caution Once you enable high security for an account, high security cannot be turned off without assistance from New Relic Support. Setting location Description Set in UI For users on our original user model: Only the account owner can configure this. Go to one.newrelic.com, click the account dropdown and select Account settings. On that page, select High security mode. For users on the New Relic One user model: Get the account ID for the account you want to enable. Then use that to go to this URL: https://rpm.newrelic.com/accounts/YOUR_ACCOUNT_ID. On that page, you can configure high security mode. If the agent is configured for high security via the UI but not locally, then the agent connections are rejected, and the agent will shut down. However, this won't shut down your application. Local, via agent Enable high security mode in your agent configuration file. High security mode is disabled by default, and the exact procedure to enable it varies by agent: C SDK: n/a Go Java .NET Node.js PHP Python Ruby If the agent is configured for high security locally but not via the UI, then the agent connections will be rejected, and the agent will shut down. This will not shut down your application. Results of enabling high security mode (version 2) Once enabled, high security mode (v2) ensures the following for your account: Feature Comments Requires agents to use a secure connection (HTTPS) High security mode requires a secure (HTTPS) connection. Non-secure connection attempts will be rejected. The latest version of all New Relic agents support HTTPS. If the configuration is not set appropriately, the agent will override the property to ensure all data in transit per the latest industry standards. Prevents HTTP param capture High security mode does not allow HTTP params, which may contain sensitive customer data, to be sent to the New Relic collector. If the agent is configured to send HTTP params locally or through server-side configuration, high security mode will override the configuration to never capture HTTP params. Prevents message queue param capture High security mode does not allow message queue params, which may contain sensitive customer data, to be sent to the New Relic collector. If the agent is configured to send message queue params locally or through server-side configuration, then high security mode will override the configuration to never capture message queue params. Prevents raw query statement capture High security mode does not allow raw database query statements, which may contain sensitive customer data, to be captured. If the agent is configured to capture raw queries locally or through server-side configuration, then high security mode will override the configuration to never capture raw queries. Prevents user attribute capture High security mode does not allow attributes set using each agent's API to be captured, as these may contain sensitive customer data. For example, in the Java agent, attributes passed in through the following NewRelic agent API calls will be blocked: NewRelic.addCustomParameter(String key, String value) Copy NewRelic.addCustomParameter(String key, Number value) Copy NewRelic.setUserName(String name) Copy NewRelic.setAccountName(String name) Copy NewRelic.setProductName(String name) Copy Prevents noticeError attribute capture High security mode does not allow attributes set using each agent's noticeError API call to be captured as these may contain sensitive customer data. For example, in the Java agent, attributes passed in through the following NewRelic agent API calls will be blocked: NewRelic.noticeError(String message, Map<String, String> params) Copy NewRelic.noticeError(Throwable throwable, Map<String, String> params) Copy Prevents custom events High security mode does not allow custom events to be created using the agent API, as these may contain sensitive customer data. For example, in the .NET agent, the API call RecordCustomEvent will be blocked. Prevents deploying Custom Instrumentation via CIE High security mode does not allow deploying custom instrumentation when using the Custom Instrumentation Editor. If you have high security mode enabled, you must export the instrumentation and manually import it to your app server. Results of enabling high security mode v1 (deprecated) High security mode version 1 is deprecated and only available if you enabled it prior to version 2 being available. High security mode version 1 ensures the following for your account: Feature Comments Requires agents to use a secure connection (HTTPS) High security mode requires an encrypted connection (HTTPS). Non-secure connection attempts will be rejected. The latest version of all New Relic agents support HTTPS. If the configuration is not set appropriately, the agent will override the property to ensure that all data in transit is encrypted as per the latest industry standards. Prevents HTTP param capture Agents configured to capture HTTP params, which may contain sensitive customer data, are not allowed to connect to New Relic. If the local configuration is set to capture request parameters, then New Relic's collector will reject the connection, and the agent will shut down. Prevents raw query statement capture Agents configured to capture raw database query statements, which may contain sensitive customer data, are not allowed to connect to New Relic. If the agent is configured to capture raw queries locally or through server-side configuration, New Relic's collector will reject the connection and the agent will shut down. Prevents deploying Custom Instrumentation via CIE High security mode does not allow deploying custom instrumentation when using the Custom Instrumentation Editor. If you have high security mode enabled, you must export the instrumentation and manually import it to your app server. Migrate from version 1 to version 2 These are the main differences between the two versions of high security: In order to make high security even more secure, high security must be enabled in the New Relic user interface and in the local New Relic configuration file. High security v1 only required high security to be set in the New Relic UI. User attributes, noticeError attributes, and message queue parameters are turned off with high security in version 2, but not in version 1. To update from v1 to v2, add high_security: true to your local agent configuration file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 693.8037,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Account</em> level",
        "body": " <em>account</em> <em>structure</em>, <em>child</em> accounts don&#x27;t automatically inherit the high security setting when enabled on the <em>parent</em> <em>account</em>. Currently there are two versions of high security mode. Version 1 is deprecated and is only available if you already have it. If you are enabling high security mode"
      },
      "id": "617e646328ccbc8b5a7ff3ac"
    }
  ],
  "/docs/accounts/original-accounts-billing/original-users-roles/user-migration": [
    {
      "sections": [
        "Metric normalization rules",
        "Metric normalization rules management"
      ],
      "title": "Metric normalization rules",
      "type": "docs",
      "tags": [
        "New Relic One",
        "Use New Relic One",
        "UI and data"
      ],
      "external_id": "3c55e4717f145ac7ae0d88e860878f4e8d18cd6b",
      "image": "https://docs.newrelic.com/static/83edfb6f5b1b68712cac34d138bb8cb8/3996e/create-new-rule-window.png",
      "url": "https://docs.newrelic.com/docs/new-relic-one/use-new-relic-one/ui-data/metric-normalization-rules/",
      "published_at": "2021-12-19T17:35:57Z",
      "updated_at": "2021-12-04T18:10:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "There may be cases where an application sends many individual metrics that could be better managed in groups. Most of these occur with web transactions metrics named from URLs. For more information on this issue, see Metric grouping issues (MGIs). To reduce high cardinality and prevent metric grouping issues, New Relic supports rules for grouping or filtering out metrics (normalization). In some cases, when MGIs with a significant high number of metrics are detected, a rule is created automatically to protect the platform from performance degradation. Metric normalization rules management You can create and manage new rules that deal with the noise produced from high cardinality metrics by using the metric normalization tool, which is accessible from each service dashboard in the New Relic Explorer. Once there, select Metric Normalization in the left sidebar. There you can see the existing rules or create new ones. Click a rule to modify it, or click Create a new rule to create a new one. A new pane to configure the rule will be displayed. Available fields are: Match expression: enter the regular expression to group all the metrics you want to include in the rule. Matches: here you will see a preview of the metrics matched by the regular expression above. Action: the action you want to perform on the metrics. Replace: replace the matched metrics by the regular expression with the value described in the Replacement field. Ignore: ignore any metric that matches the regular expression. Deny new metrics: only write metrics that have already been reported, and ignore those that match the regular expression. Replacement: only active when Replace is enabled. Matched metrics are replaced with the field's value. If the regular expression is capturing groups, you can use placeholders for them with \\1 or \\2 for the groups 1 and 2 respectively. Active: rules can’t be deleted, but can be deactivated. Click the toggle to enable or disable the rule. If you want the rule to be removed, reach out to New Relic's support. Terminate: When enabled, the rules waterfall is exited when the associated pattern is matched. Notes: internal notes on the rule. Has no effect on the rule. Once you have set up the fields, click Create (or Edit in case you are editing an existing rule), and the rule will be applied immediately as long as it's Active.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 274.1738,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>New</em> <em>Relic</em> <em>One</em>",
        "body": " grouping issues, <em>New</em> <em>Relic</em> supports rules for grouping or filtering out metrics (normalization). In some cases, when MGIs with a significant high number of metrics are detected, a rule is created automatically to protect the platform from performance degradation. Metric normalization rules management You"
      },
      "id": "603e810b64441ff3a74e8862"
    },
    {
      "sections": [
        "Introduction to New Relic One",
        "Tip",
        "Quickly understand context",
        "Query your data more easily",
        "Enhanced dashboards",
        "Build on New Relic One",
        "What’s next?"
      ],
      "title": "Introduction to New Relic One",
      "type": "docs",
      "tags": [
        "New Relic One",
        "Use New Relic One",
        "Get started"
      ],
      "external_id": "c9ba93c83a579625a4ba3364c6046f3c475cba3a",
      "image": "https://docs.newrelic.com/static/2bc08b6d64c16b39697bb43d8e66870e/c1b63/nrone20210722.png",
      "url": "https://docs.newrelic.com/docs/new-relic-one/use-new-relic-one/get-started/introduction-new-relic-one/",
      "published_at": "2021-12-14T14:43:45Z",
      "updated_at": "2021-11-24T20:42:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic One provides an integrated platform where you can explore core capabilities like querying data and building charts, dig deeper into more curated observability experiences in the UI, and use our alerting and Applied Intelligence tools. With New Relic One, you can see and act on all the data throughout your entire ecosystem. To access New Relic One, go to one.newrelic.com. Or, if you report data to our EU data center, go to one.eu.newrelic.com. Tip This doc gives you detailed information about why New Relic One's capabilities matter to your business. But if you want to skip ahead, just sign up for a New Relic account. (It's free, forever!) Then, after you install New Relic, you can start working with your data. Quickly understand context We provide multiple ways to understand your system's dependencies, so you can easily see how everything fits together and troubleshoot problems. New Relic One gives you and your teams a connected view that cuts through complexity! If you want to... Use this Have an overall view of your system, and drill down to get performance details. Use the New Relic Explorer as the front door to New Relic One. Observe, group, and filter the performance data from all your system's entities, including applications, services, hosts, and containers. Gain extensive visibility into each entity in your solution, its alert status, and how the entities are connected. Use the New Relic Navigator to give you a high density overview of all your entities so you can detect any issues at a glance. Use New Relic Lookout to spot entities recently experiencing deviations from expected behavior. Provide context for your entities. Use tags to illustrate relationships and contextual information for what you monitor. By thoughtfully tagging your entities, you can connect all the data your teams need to understand their increasingly complex and interdependent systems. For example: Add tags to all your entities. Create tags via API for teams and all the services they monitor. See how each part of your system is connected. Review service maps that illustrate your upstream and downstream dependencies. Visualize the aggregated health and activity data from all you monitor. Group and monitor any entities together into functional team-focused, project-focused groupings, or any other attribute, with workloads. Fetch and analyze specific data. Get more context while you query with the query builder, which surfaces data definitions as you craft and edit queries. Create visuals that showcase your business needs at a glance. Tailor custom dashboards for your unique needs. Find a service or dashboard in a complex environment. Search by name across all accounts in the unified search. Filter the explorer by tags or text. View everything you’re monitoring in one place, like entities or dashboards, across your organization. View a list of all the dependencies for a service. Use the dependencies tab in an entity summary to see all the dependencies of the entity you’re viewing. Track activity as it moves across your distributed system. Use our distributed tracing solution to analyze your complex environment. Understand how everything is connected via API. Use our NerdGraph GraphiQL explorer to manage all your entities, tags, and relationships. Query your data more easily On the Browse data menu on the top navigation menu you can easily access your basic telemetry data (metrics, events, logs, and traces). Wherever you go in the UI, Query your data is available. No matter your level of proficiency with our query language, you can create custom queries and charts: Browse your data in a query-less experience with our data explorer. Use your NRQL (our query language) expertise to build custom charts in the query builder. Run PromQL-style queries in the query builder. one.newrelic.com > Query your data: Build NRQL and PROMQL-like queries. Enhanced dashboards one.newrelic.com > Dashboards: Quickly create information-dense custom views into the data that matters most to you with dashboards in New Relic One. New Relic One dashboards let you build better visualizations more easily, with more options to customize. Dashboard features include: Perform NRQL queries and create charts and dashboards everywhere in the platform using the query builder. Manage your charts and dashboards easily using our quick-access CRUD menus and editing options. Explore and contextualize data with advanced tooltips and zoom in functions to monitor what your systems are doing in real time. Search your dashboards for attributes and metrics. Send data to your dashboards using our agents, integrations, and APIs. Share dashboards or charts as a .pdf, or embed a chart in an external site. If you previously used New Relic Insights to create dashboards, they are available as New Relic One dashboards. Build on New Relic One If custom charts and dashboards don't solve your current challenge, we give you a framework for building React JavaScript applications that: Live on New Relic One, alongside your other New Relic-monitored data. Feature highly tailored visualizations. Display data from any source you want, whether from a New Relic-monitored entity or data from any service or API. And you can use open source apps built by the community, and contribute your own open source apps. To learn more, see New Relic One applications. What’s next? Start exploring your data in New Relic One! Start using our basic UI features. See what data you have available with the data explorer. Browse your monitored entities with the New Relic Explorer. Use our NerdGraph API to add tags to your data. Learn about dashboards.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 246.81256,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction <em>to</em> <em>New</em> <em>Relic</em> <em>One</em>",
        "sections": "Introduction <em>to</em> <em>New</em> <em>Relic</em> <em>One</em>",
        "tags": "<em>New</em> <em>Relic</em> <em>One</em>",
        "body": " the data throughout <em>your</em> entire ecosystem. To access <em>New</em> <em>Relic</em> <em>One</em>, go to <em>one</em>.newrelic.com. Or, if you report data to our EU data center, go to <em>one</em>.eu.newrelic.com. Tip This doc gives you detailed information about why <em>New</em> <em>Relic</em> <em>One</em>&#x27;s capabilities matter to <em>your</em> business. But if you want to skip ahead"
      },
      "id": "603ec19164441f9e704e8896"
    },
    {
      "sections": [
        "Use tags to help organize and find your data",
        "Tip",
        "Tag format and limits",
        "Best practices and tips",
        "Tips on keeping tags simple:",
        "Tips on using tags consistently:",
        "Tips on cross-team implementation:",
        "Tag examples",
        "Team-related tags",
        "Environment-related tags",
        "Criticality-related tags",
        "How tags are added",
        "Add tags via UI or API",
        "APM agent tags",
        "Infrastructure data tags",
        "Automate tags with our CLI",
        "Use tags to filter the UI",
        "Custom queries, charts, and alerts",
        "Query and alert on APM tags",
        "Throughput across shards",
        "Transactions per team",
        "Alert on error rate for teams",
        "Query and alert on attributes"
      ],
      "title": "Use tags to help organize and find your data",
      "type": "docs",
      "tags": [
        "New Relic One",
        "Use New Relic One",
        "Core concepts"
      ],
      "external_id": "e2e2d53776b9109965df9de2a20660e9f60bd479",
      "image": "https://docs.newrelic.com/static/529f55ef72f3e366e6cb4a7be67229b6/c1b63/new-relic-tags-workloads_0.png",
      "url": "https://docs.newrelic.com/docs/new-relic-one/use-new-relic-one/core-concepts/use-tags-help-organize-find-your-data/",
      "published_at": "2021-12-14T14:43:44Z",
      "updated_at": "2021-11-24T20:42:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In the software industry, a tag typically refers to a piece of metadata added to a data set for the purpose of improving data organization and findability. At New Relic, our tags are key:value pairs (like team: operations) added to various sets of data, like monitored apps and hosts, agents, dashboards, and workloads. We make some important attributes available as tags (for example, app metadata like app name and language, and host metadata like host name and AWS region). You can also add your own custom tags. You can use tags in the UI to filter down to relevant data. Here is an example: Here you can see an accountId tag being used to filter workloads to a specific account. Tags help you to: Organize data coming from a large number of sources and/or to a large number of New Relic accounts. Identify teams, roles, environments, or regions to know who’s responsible for what (see example use cases). Better organize and search dashboards and workloads. Query and chart APM data. Tags are useful for organizing data at a high level. If you want to add more fine-grained detail, like capturing user names or other high-cardinality values, custom attributes or custom events are a better solution. Tip Tags were previously called labels in the New Relic UI. You may sometimes see labelin our code; for example, some of our APM agent config files use a label config option for tags. Tag format and limits Format requirements and limits for tags: The UI has limits on the total number of tags it can display per entity: Limit of 100 for tags reported by default or added via agent/integration configuration. Limit of 100 for custom tags added via UI or API. The following are maximum character length for tags: Key: 128 characters Value: 256 characters When using our API to add tags, a dash (-) in a tag key is interpreted as a minus symbol. If your tag key has a dash, use back ticks around it (like `key-name`). Best practices and tips Before adding tags, we recommend first seeing how our default tags work for you. When you're ready to add your own tags, review the following best practices tips to get the most out of them. Tips on keeping tags simple: Start by adding only tags you know you’ll definitely use. Unused tags create noise and may add confusion. Try to use short tags. Shorter tags are easier to parse, and also the UI may sometimes truncate longer tags. (See character limits.) When possible, use keys and values that are human-readable (for example, region: EMEA is better than Param8323 : 1229072). Avoid including several values like regions: EMEA | US | LATAM in a single tag. We recommend using three different tags for that, like region: emea, region: us, and region: latam. If you want to match multiple tags, you can do that using the advanced options in the filter UI. Tips on using tags consistently: Try to be consistent with tag language across your teams and entities. Be consistent with naming. For example, avoid using both region: emea and reg: emea. Be consistent with format. For example, avoid using both camelCase and kebab-case. Although tag searching is not case-sensitive in the UI and API, try to be consistent with capitalization. For example, avoid using both env: staging and env: Staging. Tips on cross-team implementation: Tags help improve observability and cost allocation. For this reason, responsibility for tag implementation is often assigned to an observability team, SREs, a group of architects, or a cross-team task force. We recommend the people responsible for implementing tags meet and create an internal policy to describe how the tags are defined and which conventions are to be used. Then: Keep this reference manual up to date. Automate the definition of tags when you deploy New Relic agents, at the cloud provider, or through New Relic automation tools, such as the API or Terraform. Create recurring reports that identify entities that are non-compliant with your tagging standards. Tag examples Here are some examples of common ways to use tags to organize data: Team-related tags Creating tags for team names can be helpful to understand which team, group, department, or region was responsible for a change that led to a performance issue. Team examples: team: backend team: frontend team: db Role examples: roles: architecture roles: devops roles: pm Region examples: region: emea region: america region: asia Environment-related tags You can create entities for the environment they belong to. Examples: env: production env: qa env: development env: staging Criticality-related tags You can create tags related to criticality levels, which lets you easier track the most critical issues. Examples: layer: level1 layer: level2 layer: level3 How tags are added Some important attributes, by default, are available for use as tags. For example, the account name, the account ID, and some basic host/server metadata are available as default tags. You can't remove these available-by-default tags. Here are some details about how tags are added for some specific data sources: Add tags via UI or API When you add tags via the UI or API, this occurs at the entity level. This means that, for example, if you have multiple APM agents monitoring a single entity and apply a tag via the UI, that tag is available for all data coming from that entity. To add or delete tags via the UI: Select a monitored entity, like a monitored app or host. Near the entity’s name at the top of the page, select the See metadata and manage tags menu. In the menu that pops up, add or delete a tag. For APM agents, restart your service. To manage tags via API: see our NerdGraph tagging docs. For a guide to using our CLI to automate tagging, see our developer site. APM agent tags You can add tags via the UI and API or you can add tags using APM agent configuration, both methods require you to restart your service. There are differences in how these apply tags: Via agent config: These tags are applied at the agent-level, not the entity/application level. This would be a good option if you have multiple agents monitoring different aspects of the same app (for example, a blue-green deployment model). Via the UI or API: These tags are applied at the entity/application level. Here are links to the agent configuration options: C SDK: not available Java Go .NET Node.js PHP Python Ruby For information on querying APM tags, see Query APM tags. And check out this short video on querying APM tags (3:20 minutes). Infrastructure data tags There are several ways tags are added to infrastructure entities and services: Use the UI or API to add tags at the entity level. Infrastructure agent: some attributes are available by default as tags. You can also add custom attributes, which are also available as tags in the UI. On-host integrations: some attributes are available by default as tags. You can also add tags by using integration-specific labels config options (for an example, see the labels config for the Apache integration). Cloud service integrations: by default we report tags and labels that are defined in the cloud service. To add tags, you'd add them on the cloud-service-side. Automate tags with our CLI For a guide to automating tags using our CLI tool, see our developer site. Use tags to filter the UI Use the filter field at the top of the New Relic Explorer to filter down to the entities you care about. You can use multiple filter conditions. To filter down to certain entities using tags: From one.newrelic.com, click Explorer. Click the Filter by... field to see a dropdown of available attributes and tags. You can filter by multiple elements and use advanced search options. To find an entity’s metadata and tags from the UI, here are two options: From a list of entities in the UI, at the far right, select an entity's icon. OR Select an entity. Near the top of the page, select the icon. To use the API to view tags, see our NerdGraph docs. Learn about how to use tags with dashboards and how to use tags with workloads. Custom queries, charts, and alerts Different features handle tags differently. Here are some details about how you can use NRQL to query tag data, or create NRQL condition alerts. Query and alert on APM tags You can query APM tags using NRQL. Minimum agent versions: C: not available Go: 2.3.0 Java: 4.9.0 .NET: 8.17 Node: v4.13.0 PHP: not available Python: v4.10.0.112 Ruby: 6.1.0.352 Here are some query examples: Throughput across shards Given a service that is distributed across nine shards, each shard assigned as an environment, we can plot the transaction throughput on all shards using a wildcard query like: SELECT rate(count(apm.service.transaction.duration), 1 minute) FROM Metric WHERE appName LIKE 'MyApp (%' AND transactionType = 'Other' FACET tags.Environment TIMESERIES Copy Transactions per team To see the number of transactions per service for a specific team, we might use a query like: FROM Transaction SELECT count(*) WHERE tags.Team = 'team-a' FACET tags.Project TIMESERIES Copy Alert on error rate for teams You can use a query for your services’ error rate for setting an alert without having to create a rule for each service. Here is the error rate for all services on a team called team-b. And this alert will automatically monitor any new app names added with the team tag. FROM Metric SELECT count(apm.service.error.count) / count(apm.service.transaction.duration) WHERE tags.Team = 'team-b' FACET appName Copy Related to this, you might create a general rule for a specific service deployed across several environments, so that we have an alarm for a single service monitoring each environment individually: From Metric SELECT count(apm.service.error.count) / count(apm.service.transaction.duration) WHERE tags.Project = 'MyProject' FACET tags.Environment Copy Query and alert on attributes As explained in the tag sources information, some important attributes are used as tags. You can query all attributes and create custom charts using NRQL, and create NRQL alert conditions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 246.81195,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Use tags <em>to</em> help organize and find <em>your</em> data",
        "sections": "Use tags <em>to</em> help organize and find <em>your</em> data",
        "tags": "<em>New</em> <em>Relic</em> <em>One</em>",
        "body": " to automating tags using our CLI tool, see our developer site. Use tags to filter the UI Use the filter field at the top of the <em>New</em> <em>Relic</em> Explorer to filter down to the entities you care about. You can use multiple filter conditions. To filter down to certain entities using tags: From <em>one</em>.newrelic.com, click"
      },
      "id": "603ebd1228ccbc6278eba754"
    }
  ],
  "/docs/accounts/original-accounts-billing/original-users-roles/users-roles-original-user-model": [
    {
      "sections": [
        "Alerts and applied intelligence notification integrations",
        "Early access",
        "Integration details",
        "Atlassian Jira",
        "Permissions",
        "Set up a Jira destination",
        "Important",
        "Two-way sync",
        "Configure the message template",
        "Send a test notification",
        "ServiceNow (Incident-Management)",
        "Roles",
        "Set up a destination",
        "Slack",
        "Prerequisites",
        "Set up a Slack destination",
        "Configure the Slack message settings",
        "Webhook",
        "Set up a webhook destination",
        "Configure the webhook event template",
        "Customize the webhook payload",
        "Email",
        "Configure the email settings"
      ],
      "title": "Alerts and applied intelligence notification integrations",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Incident Intelligence",
        "Destinations"
      ],
      "external_id": "7220c630fc187bb61784ff2cc2213e588b269b00",
      "image": "https://docs.newrelic.com/static/d4e9baecc3a76dd1a5945f8ec0aeca66/c1b63/webhook-notification-template.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/notifications/notification-integrations/",
      "published_at": "2021-12-19T15:28:40Z",
      "updated_at": "2021-12-19T15:28:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Early access The features described here are early access. You won't be able to use these features if you're not part of the early access program. For more information on related features, see our docs on Alerts notification channels, Incident Intelligence destinations, and Proactive Detection notifications. Alerts and Applied Intelligence notification integrations are specific services and platforms you can use to send notifications from New Relic. Integration details Read more about each of our specific notification integrations. Atlassian Jira Integrate New Relic with Atlassian Jira(Cloud) and automatically create and update Jira issues. Permissions The required permissions from the Jira API-Token are create, edit, and close tickets. To enable the two-way sync toggle, the provided Jira API-Key should have an Admin role. Set up a Jira destination Create Jira issues, then enable Jira and New Relic to share updates and stay synced. To create a Jira destination, enter the following information: Destination name: Custom name to identify the destination Jira account endpoint: the URL of the destination User-name: this will be the email address of the user making the connection API token: generated from your Atlassian account Important New Relic currently supports Atlassian-Jira Classic (company-managed) projects. Before saving the destination, we recommend you test the connection via the test connection button. Jira destination configuration. We recommand to test the connection before saving. Two-way sync You can enable a two-way integration with Jira to keep the issues' state synced with the corresponding state in New Relic. To enable two-way sync, turn on the ‘two-way integration’ toggle. When turned on, a Jira Webhook would be created in your Jira account at a later stage, for the selected project (see ‘customize a message template’). The webhook would contain access details to Newrelic (URL and Newrelic-API-KEY) Configure the message template To configure a template for a Jira issue, you first need to choose a destination. You will be able to create a new destination at this stage. Upon successful connection to the destination, you will need to choose a project, and then select the Jira issue type you would like to be used. Once the issue-type is selected, the configured project's fields are fetched from your account and automatically mapped to your Jira instance. To help you get started, we automatically present the required and recommended fields and values. Required fields must be set with a value. You can add or remove optional fields(use the X mark on their right side) Jira message template. Send a test notification You can see how the JIRA issue will appear by clicking a test notification with default field values. If successful, a JIRA issue will be created and a link will appear. ServiceNow (Incident-Management) Integrate New Relic with ServiceNow Incident-Management and automatically create and update incidents. Roles As part of the integrations, we fetch fields from the your serviceNow incident table and optional values. For this, the provided ServiceNow user details required read permissions for the tables: sys_dictionary, sys_choice, sys_user and task. A read/write permission to incident To be able to fetch users for the caller column, we required read permissions for the sys_users table. The above permissions can be achieved with the roles personalize_choices, personalize_dictionary, rest_service, itil. Read/Write permissions to the api_key_credentials table is required to enable two-way integration. This can be covered with the roles credentials_admin and discovery_admin. Set up a destination To create a ServiceNow destination, enter the following information: Destination Name: custom name to identify the destination Domain: the URL of the destination User-name: the name of the user Password: the user name’s password Before saving the destination, we recommend testing the connection by clicking the test connection button. Two-way sync You can configure a two-way integration with ServiceNow Incidents Management to keep the incidents' state synced with the corresponding state in New Relic. Here are some required steps to remember when configuring the two-way integration: Turn on the two-way integration toggle. Open and download this XML file, which includes the business rule triggering events back to New Relic One. In the ServiceNow sidebar menu, go to System Definition > Business Rules. Click the menu icon in one of the column headers, select Import XML and upload the XML file you downloaded. Once the Destination is saved, a New-Relic API-Key will be kept in the api_key_credentials. The key would sent in a header as part of the callback REST call to New-Relic Configure the message template Upon a successful connection, ServiceNow incident table columns are fetched from your account and automatically mapped to your ServiceNow instance. To help you get started, we automatically present the required and recommended fields and values. Required fields must be set with a value. You can add or remove optional fields(use the X mark on their right side) Select, edit or remove fields for the ServiceNow-Incident template. Send a test notification You can see how the ServiceNow incident will appear by clicking a test notification with default field values. If successful, an incident will be created and a link will appear. Slack Send notifications-messages to your Slack channels. Prerequisites Your Slack workspace needs to have the New Relic application installed. The application must be approved by a workspace admin before it can be individually installed by users Set up a Slack destination Click on the `one-click Slack authentication' will lead you to the Slack landing page to continue the OAuth2 authentication process. On the Slack landing page, if you're not signed into the required workspace, you're redirected to Slack to sign in. Add your workspace name or select the relevant workspace and click Continue. When signed in to the selected workspace, you are requested to allow New Relic to perform the specified actions. Clicking `Allow' will redirect you back to the Destination page. Configure the Slack message settings Select a Destination(Workspace) and select a Slack-channel where the messages will be sent. You can create a new destination if there is no pre-defined destination for the required workspace. Note that, for privacy reasons, users need to be authenticated to select private channels (one-time process) Send a test notification You can send a test notification with a pre-defined example payload to the channel. This creates a message in the selected Slack-channel. Webhook Use the webhook notifier to send the notification messages to any endpoint you like. Set up a webhook destination To create a webhook destination, you need the following: Destination Name: A unique destination name URL: the endpoint of the target application, authentication and custom headers if needed. Authorization mechanism (Optional):. Can be basic authentication or a bearer token Configure the webhook event template Pick a webhook destination from the list and configure the HTTP-POST request. The request configuration requires you to: Set a name for the template. Select a pre-configured destination from the destinations list or create a new one. Add custom headers (optional). Configure the request’s payload. Customize the webhook payload You can use the default payload or customize it to contain the required data. Pick Variables from the variables menu and apply handlebars syntax to enrich your webhook. Note that the request’s content-type is JSON by default. Hence, the payload needs to keep the JSON form. See Usage examples The ‘preview’ section on the right hand-side shows an expected payload after the template is rendered. If the eventual payload would not form a valid Json, an error will be shown and it won’t be possible to save the template. If the webhook payload conforms a valid Json, you can send a test notification to your defined webhook destination We recommend sending a test notification to make sure that everything's connected correctly. Email Send email notifications to users. Configure the email settings Add one or more recipients. Users with New Relic accounts can be found via autocomplete when searching for their name or email address. To add a user without a New Relic account or email distribution list, add the full email address. Every recipient will be translated into a 'destination'. You can follow the email notifications per destination in the notifications-log Send a test notification You can send a test notification to make sure the email notifications reach the inbox.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.18404,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Permissions</em>",
        "body": " of the integrations, we fetch fields from the your serviceNow incident table and optional values. For this, the provided ServiceNow <em>user</em> details required read <em>permissions</em> for the tables: sys_dictionary, sys_choice, sys_<em>user</em> and task. A read&#x2F;write permission to incident To be able to fetch <em>users</em>"
      },
      "id": "618ff71628ccbc60710321e4"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/style-guide/word-choice/user-related-language-guidelines/",
      "sections": [
        "Users/roles-related language guidelines",
        "Note about user models",
        "Styles and formatting",
        "Philosophy for user permissions/roles language",
        "Notes on documenting user type or group as requirements",
        "Document requirements for relevant role or capability",
        "Document requirements for both original and new user model"
      ],
      "published_at": "2021-12-19T14:19:29Z",
      "title": "Users/roles-related language guidelines",
      "updated_at": "2021-11-24T09:22:40Z",
      "type": "docs",
      "external_id": "c4e420beb3ca09fe8e138137b77d8e0b28f5fbfc",
      "document_type": "page",
      "popularity": 1,
      "body": "This doc explains required styles and recommended phrasings for user-related terminology on the New Relic One user model. Note about user models This style guide applies to the New Relic One user model, not the original user model. Even though both user models share certain terms (such as Admin), this is an entirely different user model with an entirely different structure, recommended phrasings, and style. To understand user model differences, see Overview of user models. The original user model will be increasingly deprecated, so we shouldn't need to think very much about that user model or update those docs very often. Styles and formatting Here are some user-related styles and formats for user-related language for the New Relic One user model: Category Example use Styles and tips User type (basic user vs. full platform user) Example use: If you’re a full platform user, you can... Lower case, with no style applied. We took this simple approach because the user type names are short names, easily understood without formatting or capitalization, and because this gives us the ability to refer to user types as plurals (e.g., \"your organization's basic users can...\") Note that it can sometimes be awkward when \"basic user\" is in front of a noun. For example: \"To learn about basic user capabilities...\" can sound like you're talking about user capabilities that are basic. To avoid this, rephrase to be more clear and/or link to the user type doc section from the use of \"basic user\" or \"full platform user.\" Roles Example use: This capability is available for the All product admin role. Preferred: sentence case, bold. Alternatively, instead of bold, you can use italics or code-formatting: as long as it's clear that we're referring to something that has a definite, exact name. When referencing roles, we recommend linking to the standard roles doc, which lists and defines the roles, and also explains how roles are related to the default groups (Admin and User). Default groups Example use: When adding a user, you can add them to either the default Admin or User groups, or a custom group if you have any. For docs site, use sentence case and bold. Alternatively, for non-docs-site styling, you can use italics or code-formatting: as long as it's clear that we're referring to something that has a definite, exact name. The words 'admin' and 'user' are generic, general words, so using formatting to indicate that we're referencing specific terms is important. When referencing the default groups, we recommend linking to the explanation of default groups. Capabilities Example use: The ability to do this is governed by the \"modify APM settings\" capability. One way to reference a capability is by using quotes as shown in the example here, but we don't have firm rules on how to reference capabilities. One reason for this is that capabilities don't have specific proper names and are more informal. To see what we mean, see the screenshot of the capabilities UI table. The rows in the capabilities table have a heading (like \"APM\"), and refer to a feature (like \"Application settings\"), and have columns referencing more granular abilities (like \"Create\" or \"Delete\"). For this reason, we don't have a firm rule but the goal is simply to try to make it clear which capability you're referencing. Philosophy for user permissions/roles language Before writing permission-related requirements for docs or similar use cases, you should read about the relationships between groups, roles, and capabilities. The following recommendations for permissions-related documentation philosophy applies primarly for documenting our New Relic One user model, not our original user model. Notes on documenting user type or group as requirements For documentation about New Relic features, we don't document restrictions related to user type (basic vs. full platform) or default groups. The only doc we explain what's available by user type is in the 'User type' documentation. Some reasoning about why that is: Basic vs. full platform users: The only place we document restrictions related to user type (basic vs. full) is in the primary 'User type' doc. This means that we don't need to put user type requirements in every doc about a feature. One reason for this is that basic users have explanations and upsell information in the UI, so this will mostly be self-evident. API docs exception: Because APIs are not as obvious and transparent as the New Relic UI, we should document any requirements that SMEs think are relevant, which may include user type. Attempt to put any requirements for using an API in a single location, and not spread them throughout the docs. Default user groups: We don't explain restrictions related to user groups (that is, our default groups Admin and User). This is because the actual mechanism causing the restriction is not due to the group, but to the role assigned to that group (and at a more granular level, due to the capabilities associated with that role). If you have questions about user type, user group, and user roles, see the User model doc. Document requirements for relevant role or capability When considered necessary, document any roles or capabilities that gate access to a feature. A few notes on this: Note that when creating a doc about a new feature, we don't document requirements related to user type or group. Note that our standard roles are just aggregations of capabilities. This means that there can be multiple standard roles that have some of same capabilities. In that case, in order to be more specific, you'd reference the capabilities, not the role. It is okay to use general, informal language when referencing capabilities, as is shown in the example below. Some standard roles (like Billing user and Organization manager and Authentication domain manager) have capabilities that aren't exposed in the UI to customers, so this means that in those cases we'd have to just reference the role and not a specific capability. Examples: Example of documenting roles Example of documenting capabilities. Document requirements for both original and new user model If you're including requirements about New Relic One user model roles or capabilities, you may also need to include details about the original user model. Here's an example of how to do that. Permissions requirements differ depending on your user model: * For users on the [original user model](/docs/accounts/original-accounts-billing/original-product-based-pricing/overview-changes-pricing-user-model/#user-models): the [**Data retention manager** role](/docs/accounts/original-accounts-billing/original-users-roles/users-roles-original-user-model/#roles). * For users on the [New Relic One user model](/docs/accounts/original-accounts-billing/original-product-based-pricing/overview-changes-pricing-user-model/#user-models): the [**Billing user** role](com/docs/accounts/accounts-billing/new-relic-one-user-management/new-relic-one-user-model-understand-user-structure/#standard-roles). Copy For more about role language guidelines, see the styles table.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 230.99362,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Users</em>&#x2F;<em>roles</em>-related language guidelines",
        "sections": "Philosophy for <em>user</em> <em>permissions</em>&#x2F;<em>roles</em> language",
        "body": "&#x27;s an example of how to do that. <em>Permissions</em> requirements differ depending on your <em>user</em> <em>model</em>: * For <em>users</em> on the [<em>original</em> <em>user</em> <em>model</em>](&#x2F;docs&#x2F;accounts&#x2F;<em>original</em>-accounts-billing&#x2F;<em>original</em>-product-based-pricing&#x2F;overview-changes-pricing-<em>user</em>-<em>model</em>&#x2F;#<em>user</em>-models): the [**Data retention manager** <em>role</em>](&#x2F;docs"
      },
      "id": "619e046028ccbc5c5cb99518"
    },
    {
      "sections": [
        "How to manage users",
        "Important",
        "Requirements",
        "Manage users in the UI",
        "Overview of user management concepts",
        "Common user management tasks",
        "Add, edit, and delete users",
        "Edit user type (basic and full platform)",
        "Give users access to accounts and roles (access grants)",
        "Create new custom groups and roles",
        "Set up SAML SSO and/or SCIM provisioning",
        "Grant users ability to manage other users",
        "Control how basic users upgrade to full platform users",
        "Track changes",
        "User management terms and definitions"
      ],
      "title": "How to manage users",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "New Relic One user management"
      ],
      "external_id": "9e37836740ce56302734d4af636bdbe087b4dbc3",
      "image": "https://docs.newrelic.com/static/a78dad5ff794da5deaaf3b514e334ea7/c1b63/new-relic-one-user-mgmt.png",
      "url": "https://docs.newrelic.com/docs/accounts/accounts-billing/new-relic-one-user-management/add-manage-users-groups-roles/",
      "published_at": "2021-12-19T15:22:19Z",
      "updated_at": "2021-11-24T14:23:16Z",
      "document_type": "page",
      "popularity": 1,
      "body": "For users on our New Relic One user model, we provide various user management features, including the ability to: Use role based access control (RBAC) to assign default or custom roles to user groups Create custom user groups Grant user groups access to specific roles and accounts Important This doc applies to users on the New Relic One user model. For managing users on our original user model, see Original users. Not sure which user model you're on? See User models. Requirements To see if you can access these user management features, go to the user management UI and see what you have access to. Access requirements: These features allow managing of users on the New Relic One user model. To learn more, see User models. To avoid configuration conflicts, ensure that only one user is managing users at a time. Simultaneous configuration by more than one user may result in errors and unexpected results. Most capabilities require the Authentication domain manager role and some require the Organization manager role. For details, see Standard roles. Pricing edition requirements: To manage user groups, roles, and access grants: Pro or Enterprise edition is required. To import users from an identity provider: Enterprise is required. A New Relic user can have a maximum of either three concurrent active sessions, or three unique IP addresses in use at any given time. Manage users in the UI For users on the New Relic One user model, to find your user management options: From the account dropdown, select Administration. There are two user management UI pages there: User management: Use this to add users, update user type (basic versus full platform) and other information, and approve user type upgrade requests. Organization and access: Use this page to create and manage groups, roles, and access grants, and to set up SAML SSO and SCIM provisioning. Want to see videos of the user management UI in action? See our user management videos. Important If you can't see these UI pages, it may be because you're on our original user model or because you don't have the required user management role. Overview of user management concepts If your organization has Pro or Enterprise edition, you can create and configure access grants. An access grant gives a group of users access to a specific role on a specific account. Here's a diagram showing how access grants work and how they relate to the broader organization: A diagram explaining the concept of how access grants give a user group access to a) a role on b) a specific account. (Note that this applies to users on our New Relic One user model (and not our original user model).) When your New Relic organization is created, it starts out with some default access grants for our default User or Admin groups. Each of these groups is assigned one or more roles and granted access to the primary (first created) account: A view of the Organization and access UI, showing the default access grants associated with our default groups. If you have a relatively flat organizational structure, and are okay with all or many of your users having wide administrative access and access to all accounts, you'll probably only need at most a few access grants. For example, you might decide to add new access grants to the existing default Admin or User groups, giving those roles access to other accounts. Or, if you need more granular definition over roles and permissions, you'd create access grants that define new groups that have access to specific roles (either our standard roles or custom-defined roles). For a tutorial on how to create access grants and create custom roles, see the User management tutorial. For other examples of some common user management tasks, see Example tasks. To see the UI in action, see our user management videos. Some tips on setting up access grants: It may help to first plan out how your access grants will be organized. How many accounts will you have? What user groups will get access to which roles and which accounts? Will you use our default groups and roles or create your own custom groups and roles? If you've used automated user management to provision users via SCIM, you will have to create access grants to give those users access. A common configuration for organizations with many accounts (roughly 20 or more) is setting up groups with the more organization-scoped roles (Organization manager, Authentication domain manager, and Billing user) on the primary account, and then on other accounts, setting up groups with the more product-scoped roles (like All product admin, Standard user, or custom roles). Common user management tasks In the Organization and access UI, you can create access grants, custom groups, custom roles, and configure an authentication domain. Here are some example user management procedures: Add, edit, and delete users See the user management tutorial. Edit user type (basic and full platform) Note that there are limits around how many times full platform users can be changed to basic users. Important If you're using automated user management to provision and manage your users, you have other options for managing user type. To change the user type of one or more users: On the User management page, click the checkboxes for the users whose user type you want to edit. Once you start selecting users, an option will appear for Edit type. You can also edit the user type and group of a specific user by clicking on that user. Give users access to accounts and roles (access grants) See our user management tutorial. Create new custom groups and roles See the user management tutorial. Set up SAML SSO and/or SCIM provisioning See Get started with SAML SSO or SCIM. Grant users ability to manage other users To grant your users the ability to manage other users, you'll need to add users to a group that has the Authentication domain manager and Organization manager role. You have two options: From the User management UI, you can add a user to the default Admin group, which includes both of those roles. OR You can create a custom group and assign it these roles. For a tutorial on creating new access grants, groups, and roles, see the user management tutorial. Control how basic users upgrade to full platform users See the authentication domain settings. Track changes To see an audit log of changes to your account, including user management actions, you can query the NrAuditEvent. User management terms and definitions For an explanation of how user access to accounts and roles works, see User management concepts explained. Here are some definitions for the terms we use there: A New Relic organization is the representation of your organization, containing all your accounts, users, and data. For more information, see Organization and account structure. A capability is an ability to use or edit a specific, granular New Relic feature. Examples of capabilities: The ability to modify APM settings The ability to delete alert conditions A role is a set of capabilities. It is what gives a user their permissions. Our default standard roles have various capability sets, and you can create custom roles that have a custom set of capabilities. See some specific New Relic capabilities. A user group has one or more roles associated with it. You assign your users to a group. We have default user groups (Admin and User), and you can make your own groups. An access grant is what grants a user group access to roles and to specific New Relic accounts. An access grant essentially states, \"This group is assigned this role on this New Relic account.\" Adding a user to a group doesn’t do anything unless that group is included in an access grant. An authentication domain contains a set of users who are added to New Relic and who log in to New Relic in the same way. For example, you may have one authentication domain for users who log in via username/password and another authentication domain for users who log in via SAML. If a user is a basic user, this takes precedence over any role-related limitations. For more on this, see Basic user and roles.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 219.65569,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "How to manage <em>users</em>",
        "sections": "Give <em>users</em> access to accounts and <em>roles</em> (access grants)",
        "tags": "New Relic One <em>user</em> management",
        "body": "For <em>users</em> on our New Relic One <em>user</em> <em>model</em>, we provide various <em>user</em> management features, including the ability to: Use <em>role</em> based access control (RBAC) to assign default or custom <em>roles</em> to <em>user</em> groups Create custom <em>user</em> groups Grant <em>user</em> groups access to specific <em>roles</em> and accounts Important"
      },
      "id": "603e7bce28ccbc415beba74c"
    }
  ],
  "/docs/accounts/original-accounts-billing/product-based-pricing/overview-data-retention-components": [
    {
      "sections": [
        "Event data retention (original pricing model)",
        "Important",
        "Data retention UI",
        "Overview of event data retention",
        "Extend your event retention",
        "Insights Pro",
        "How number of events stored is calculated",
        "Insights Pro event overage example",
        "Disable/enable Transaction and Pageview event reporting",
        "Tip",
        "Flexible data retention",
        "How it works",
        "Manage retention via UI",
        "Glossary"
      ],
      "title": "Event data retention (original pricing model)",
      "type": "docs",
      "tags": [
        "Accounts",
        "Original accounts and billing",
        "Original data retention"
      ],
      "external_id": "76d1289aad7de08b355bb8c313f9e7a42a5779d8",
      "image": "https://docs.newrelic.com/static/e53a1e416eb6116545627d3ec880d08e/e9c9b/flex-2.png",
      "url": "https://docs.newrelic.com/docs/accounts/original-accounts-billing/original-data-retention/event-data-retention-original-pricing-plan/",
      "published_at": "2021-12-19T14:16:05Z",
      "updated_at": "2021-11-14T09:17:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This doc is for accounts on our original pricing model, not our New Relic One pricing model. Not sure which you're on? See Overview of pricing models. For organizations on New Relic One pricing, our various New Relic products report a wide range of event data. Different products have different data retention periods, and different ways to extend event data retention. You can customize the length of your event data retention through flexible event retention. Data retention UI For how to find the data retention UI, see Manage data. Overview of event data retention All New Relic product subscriptions come with a certain level of data retention that governs how long different types of data are retained. One type of data governed by data retention rules is event data. Event data is available in some UI charts and tables, and also available for querying via NRQL, our querying language. There are events reported from products by default, and there are custom events: each have their own retention rules, depending on the product and subscription level. Here are some examples of how different product subscriptions can affect event data retention: Free/Lite APM subscription: default-reported events available for 1 day. No custom events available. Pro APM subscription: default-reported events available for 8 days. Custom events available for 1 day (and able to be extended with Insight Pro). To see your subscriptions, go to the Account summary page. Extend your event retention Product Method APM, Browser, and Mobile Event data retention can be extended with a paid subscription to these products (see product data retention). To extend retention of both default-reported events and custom events further, you need an Insights Pro subscription. Infrastructure Event data retention can be extended with a paid Infrastructure subscription. See Infrastructure data retention rules. Synthetics Event data retention can be extended with a paid Synthetics subscription. See Synthetics data retention rules. Custom events Custom events reported by agent APIs or the Event API: Extension requires an Insights Pro subscription. Insights Pro Important As of April 12, 2021, we are upgrading Insights to an improved web and mobile experience! All of your Insights URLs will be redirected automatically to the corresponding dashboards in New Relic One. For more details about this migration and how you can easily plan for this transition, see our Explorers Hub post. A paid Insights subscription is what governs the extension of event data retention for: Our APM, Browser, Mobile, and Serverless products Custom events that come from an agent API or from the Event API Important Note that having an Insights Pro subscription doesn't require use of the Insights UI (insights.newrelic.com) to query your data: there are other querying options available. To see the data retention governed by your Insights subscription: go to the usage UI and select Insights usage. With an Insights Pro subscription, you can use flexible retention to customize how your event data is retained. This lets you keep only the data you need, for as long as you need it. How number of events stored is calculated This is an explanation of how the number of stored events are calculated by default for an Insights Pro subscription. (Note that with flexible retention, you have more fine-grained control over the retention period.) The events stored is calculated based on 1) total events stored over time (calculated based on the events generated per week) and 2) the weeks of data retention available. This equation can be represented like this: events stored = (events generated per week) * (weeks of retention) Copy An Insights Pro subscription provides a given number of weeks of data retention as well as a given number of events over that retention period. For example: (200M transactions per week) * (4 weeks of retention) = 800M events stored in Insights (16M transactions per week) * (50 weeks of retention) = 800M events stored in Insights For Insights Pro subscriptions, data is purged based on retention window, not volume. It is deleted from the system once it's past the retention window. For example: If your Insights license is for 800 million events with a 4 week retention period, your data would start being purged after it is older than four weeks. Temporary spikes in data exceeding your subscription level will still be recorded, but consistent overage should be solved by upgrading your subscription level or decreasing data collected. For customers without an Insights Pro subscription, New Relic may throttle or downsample events to a limit of not more than than 4,000 events per host per minute. Insights Pro event overage example In this example, you have an Insights Pro subscription with a license for 800 million events over 4 weeks, a rate of 200 million events per week. You have APM Pro, Browser Pro, and Mobile Enterprise. A fifth week of data is added via your subscriptions, bumping you to a total of 1 billion events stored within your plan: If you are using 975 million events, you are not over your retention. If you are using 1.25 billion events, you are over your retention. Disable/enable Transaction and Pageview event reporting Tip Owners or Admins The Insights Data summary UI page is used to see the types of events being reported. You can also use this page to enable and disable the reporting of PageView and Transaction events. To view Data summary: Go to insights.newrelic.com > Manage data. Select the Summary tab. Note: if you disable PageView or Transaction event reporting, this can affect some New Relic UI elements. You may see some empty charts on some UI pages that rely on this data. Go to insights.newrelic.com > Manage data > Summary. From the Summary tab, select Configure data sources. Toggle the appropriate switch on or off, then save. Toggling Transaction on or off will cause reporting agents to restart themselves. For more about configuring event reporting, see Event data retention. Flexible data retention With an Insights Pro subscription, you get access to flexible retention, which lets you define how some types of event data are retained. This lets you keep only the event data you need, for as long as you need it. You can manage your flexible retention through the UI or through our GraphQL API. Requirements to use this feature: An Insights Pro subscription or equivalent trial. Applies only for events governed by an Insights Pro subscription. To use this feature, you must be an account Owner or data retention add-on manager for your account. How it works To understand how standard event data retention works, first read Event data retention. With flexible retention, you specify the data retention for applicable event namespaces across your accounts. This gives you per-event namespace control of your data. The retention that you specify for an event namespace will be shared by all the event types under that namespace. If some namespaces are not relevant to you, you can avoid collecting their event data entirely. Your retention value can’t be lower than the included retention or higher than the default retention. You can control data retention either in our UI or by API. Manage retention via UI You can control data retention either using our GraphQL API or in the UI. To do this with the UI, go to the data retention UI. Your retention changes take effect within 24 hours after updating. Glossary To understand the terms used with flexible retention, see the following: Term Description Event namespace An event's namespace corresponds to one or more event types that share a single data retention value. For more information, see Event namespaces (types). You can also use NerdGraph to get the list of customizable event namespaces. Retention value The number (in days) that specifies how long your event data is stored. Retention rule The event namespace and retention value pair that you specify to override the current retention. Licensed retention Retention period that’s determined in weeks by your Insights Pro subscription contract. Included retention Retention period for which your data is stored but not charged under the Insights Pro subscription. For details, see the data retention details for a specific product. Paid retention Retention period for which your data is stored and is charged under the Insights Pro subscription. By default, your licensed retention determines this value but Flexible retention lets you override it. Default retention Retention period that comes out of the box. This is based on the total of included retention plus licensed retention. For information on managing retention settings with APIs, see the Manage data retention documentation.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 272.3385,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Event <em>data</em> <em>retention</em> (<em>original</em> pricing model)",
        "sections": "Event <em>data</em> <em>retention</em> (<em>original</em> pricing model)",
        "tags": "<em>Original</em> <em>accounts</em> <em>and</em> <em>billing</em>",
        "body": " for your <em>account</em>. How it works To understand how standard event <em>data</em> <em>retention</em> works, first read Event <em>data</em> <em>retention</em>. With flexible <em>retention</em>, you specify the <em>data</em> <em>retention</em> for applicable event namespaces across your <em>accounts</em>. This gives you per-event namespace control of your <em>data</em>. The <em>retention</em>"
      },
      "id": "6043f713e7b9d2ccee579a1d"
    },
    {
      "sections": [
        "Overview of changes to pricing and user model",
        "Overview of how pricing model and user model relate",
        "Pricing plans explained",
        "Determine pricing model",
        "Convert to new pricing",
        "User models explained",
        "Requirements for new user model",
        "Determine which user model you're on",
        "Feature impacts of user model",
        "Transition to new models"
      ],
      "title": "Overview of changes to pricing and user model",
      "type": "docs",
      "tags": [
        "Accounts",
        "Original accounts and billing",
        "Original product-based pricing"
      ],
      "external_id": "b19d61b5a0c65ca352e2ce0e260e2b53391b94fc",
      "image": "https://docs.newrelic.com/static/1461470cc1ef0be91120c84d76a7ccdd/c1b63/new-relic-one-pricing-plan-billing-ui.png",
      "url": "https://docs.newrelic.com/docs/accounts/original-accounts-billing/original-product-based-pricing/overview-changes-pricing-user-model/",
      "published_at": "2021-12-19T14:16:05Z",
      "updated_at": "2021-11-24T14:34:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In 2020, New Relic released both a newer pricing model and a newer user model. Keep reading to learn about: How the pricing model and the user model relate to each other Pricing plans explained User models explained How to switch to the new models Overview of how pricing model and user model relate In 2020, we released both a new, improved pricing model and a new, improved user model. These models represent the future. All new sign-ups are on these new models and eventually all organizations will be on these models. But currently, our customers from before July 2020 may have one of several combinations of these as they either switch to the new pricing or migrate their users to the new user model. This table shows how pricing and user model relate to each other: Pricing plan factors User model factors Original pricing If your organization was created before July 30 2020, you remain on our original pricing model until you transition to New Relic One pricing. For original pricing accounts, you have users on the original user model unless you've migrated your users to the new model. Note that if you've added users via automated user management, they're on the new model. This means it's possible to have some users on both models. New Relic One pricing An organization is on the New Relic One pricing model: If it was created on or after July 30 2020, or If it's an older organization but has switched to New Relic One pricing. If your organization was created on or after July 30 2020, your users are on the New Relic One user model. If you have an older organization, you have users on the original user model unless you've migrated your users to the new model. Pricing plans explained New Relic organizations are on one of two pricing models: New Relic One pricing: Our new pricing model is simpler. It bills on a) which edition you're on (Standard, Pro, or Enterprise), b) the GBs of data ingested, and c) how many billable full platform users you have. All organizations created on or after July 30 2020 are on this pricing model, as are older organizations that have switched to this pricing. There are two versions of this pricing model. Our original product-based pricing model: this is based on subscriptions to specific products (for example, APM, Browser, Mobile, Infrastructure). Some organizations have the option to transition to the newer pricing model: in that case, their users remain on our original user model. Determine pricing model To determine which pricing model you’re on: go to one.newrelic.com, select the account dropdown, and select Manage your plan. If you see billing information about data ingested and the number of billable users, you’re on the new pricing model. If you're on New Relic One pricing, you'll see a billing UI like this. Convert to new pricing Some organizations are able to switch to new pricing. Learn more about switching your pricing model. User models explained In this context, the term \"user model\" refers to the structure of a New Relic user record and how it interacts with the broader New Relic organization that it's in. Our two user models are: New Relic One user model: this newer, improved user model has these major differences from the original user model: All your accounts and users are contained under a top-level organization. This gives an organization stronger control over managing users and what they can access. One impact of this is that users who work with multiple New Relic organizations may have an email address associated with multiple logins. Users on this model have a different system for managing permissions and account access. Using access grants, you assign users access to specific roles on specific accounts. Original user model: some aspects of this older model that are different from our newer model: There wasn't as much organization-level control over users. For example, a New Relic user had the ability to access multiple accounts they'd been granted access to using a single login. Users on the original user model have a different user management system. To learn more about benefits of the new model, see our blog post about user model changes and an explanation of the new account structure. For impacts and limitations, see Feature impacts. Requirements for new user model Here's a synopsis of the factors governing what organizations use the new model: All New Relic organizations that signed up after July 30 2020 have users on this model, and also have the new pricing model. Some older New Relic organizations have had their users migrated to the new model by New Relic or by using the user migration procedure. Note that switching to the new pricing model is independent from migrating users. Partner accounts (resellers, managed service providers), and organizations using the partnership account structure, cannot yet migrate their users to the new model. Determine which user model you're on To determine what user model you're on, from the account dropdown select User preferences. If you see the UI below, you're on the New Relic One user model. If you see a different UI with more content displayed, you're on the original user model (see original user docs). To determine if you can manage users on the New Relic One user model, see Manage users. The user model is independent of your pricing model. For how user model relates to pricing, see the Pricing and user model table. Feature impacts of user model The new user model offers many benefits. Here are some feature impacts of being on the New Relic One user model that may be not obvious to users accustomed to our previous user model: More separation of organizations/accounts: If you have an email address associated with multiple New Relic accounts, this may mean you need to log out and log back in. Learn more about account access. Adding accounts: Pro and Enterprise edition can have more than one account in their organization. Currently users in these organizations cannot, on their own, add accounts. They must get help from their New Relic account representative. To learn about adding users to accounts, see Manage users. Alerts-related impacts: Policy actions: For alert policies and conditions, some UI locations display the user that made the last modification. For users on the new model, it will display their user ID and not their user name. For example, a policy might read Last modified Sep 7, '20 4:26 pm by user 1234567 instead of by John Doe. If you have user management permissions, you can determine what user this is by going to the User management UI. Notifications: Users on the new model won’t show up as an option when choosing users to receive alert notifications. The result of this is that these users will not get New Relic mobile app push notifications. As a workaround, you can instead use their email address as the notification channel. You can't use the REST API to generate a list of users (only original model users). Instead you'd use the user management UI. Changing account name: These users cannot change the name of their organization after it has been created. Transition to new models Some New Relic organizations on the old models are able to transition to the new models. To learn more about this, see Transition to new models.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 139.16275,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Overview of changes to pricing <em>and</em> user model",
        "sections": "Overview of changes to pricing <em>and</em> user model",
        "tags": "<em>Original</em> <em>accounts</em> <em>and</em> <em>billing</em>",
        "body": ", their users remain on our <em>original</em> user model. Determine pricing model To determine which pricing model you’re on: go to one.newrelic.com, select the <em>account</em> dropdown, and select Manage your plan. If you see <em>billing</em> information about <em>data</em> ingested and the number of billable users, you’re on the new"
      },
      "id": "603e97fa28ccbcb7c2eba754"
    },
    {
      "sections": [
        "Set session timeouts",
        "Original pricing model",
        "Requirements",
        "Overview",
        "Features",
        "Tip",
        "Select the session timeout value",
        "Select SAML SSO browser re-authentication",
        "Redirect after SAML timeout"
      ],
      "title": "Set session timeouts",
      "type": "docs",
      "tags": [
        "Accounts",
        "Original accounts and billing",
        "Original product-based pricing"
      ],
      "external_id": "a61d4c61f52ee18be0763a9cd526634d9d2f50f5",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/account-maintenance/set-session-timeouts/",
      "published_at": "2021-12-19T15:55:42Z",
      "updated_at": "2021-11-14T11:21:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Original pricing model This doc is for users on our original user model. New Relic's session configuration feature allows you to set limits on idle time before your users' browser sessions automatically expire. Requirements If you're on the New Relic One user model, see Session settings. Overview Session configuration allows you to set limits on idle time before your users' browser sessions automatically expire. A message appears three minutes before the system logs them out. Users then need to sign back in to continue. For accounts configured with SAML Single Sign On (SSO), an additional option is available to set how often the users' browser sessions are re-authenticated. Users and Restricted Users can view the time period for automatic timeout, but they cannot change it. To view the timeout value: Go to account dropdown > Account settings > Authentication > Session configuration. Features Tip Owner or Admins The session configuration options provide an additional level of security to ensure that unattended browsers will automatically time out. Session values are automatically stored in the session cookie. Additional features include: Feature Notes Easy setup Admins use the slide bar in New Relic's user interface to select predefined time periods. Default is two weeks. Separate options available by role Admins can choose for Restricted User sessions to never time out even if they select a session timeout setting. This is useful, for example, when you use a Restricted User login for demos. Automatic inheritance for child accounts By default, child accounts inherit the same session configuration as their parent account. Most restrictive by default If users have multiple accounts, the most restrictive setting applies, regardless of which account the user currently is using. Integration with SAML SSO logout URL If the account's SAML SSO configuration does not include a logout URL, New Relic includes a link from Session configuration for the Owner to set it up. If the Admin is not also the Owner, a message about the SAML SSO logout URL requirement appears. Additional re-authentication setting for SAML SSO In addition to the session timeout option, Admins can select the time (15 minutes to 2 weeks, or never) for how often a SAML-authenticated browser session must be re-authenticated. Select the session timeout value The process to select the session timeout value is the same for both SAML and non-SAML configurations. For additional SAML configuration options, see SAML SSO browser reauthentication. To select a predefined period for session timeouts with SAML SSO accounts, the account Owner must have previously identified the logout URL in the SAML SSO configuration settings. If this has not been set up, the account Admin can view the session timeout slide bar but not change it. If the Admin is also the account Owner, the Session configuration includes a link to go directly to New Relic's SAML SSO Configuration and identify the logout URL. For more information, see Setting up SSO. To select a predefined period for session timeouts for users on our original user model: Go to: account dropdown > Account settings > Authentication > Session configuration. Use the slide bar to select a time period for idle sessions to expire and log out automatically. Optional: Select the checkbox option if you do not want restricted users' browser sessions to expire. Select Save my changes. Changes take effect immediately. Select SAML SSO browser re-authentication To select a predefined period for SAML SSO-authenticated browser sessions to be re-authenticated: Go to: account dropdown > Account settings > Authentication > Session configuration. Use the SAML re-authentication time slide bar to select a time period for New Relic to check the browser session. Select Save my changes. Redirect after SAML timeout If you are logged out due to a session idle timeout on an account configured for SAML, you will be sent to the New Relic login page. Because your account is configured for SAML, you do not have a direct New Relic login. To be redirected to your SAML provider for authentication: Enter your email address in the Email field. Leave the Password field blank. Click the Sign In button. You will then be redirected to your SAML provider. Once reauthorized, you will then be returned to the New Relic website.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 130.88925,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Original</em> pricing model",
        "tags": "<em>Original</em> <em>accounts</em> <em>and</em> <em>billing</em>",
        "body": " login for demos. Automatic inheritance for child <em>accounts</em> By default, child <em>accounts</em> inherit the same session configuration as their parent <em>account</em>. Most restrictive by default If users have multiple <em>accounts</em>, the most restrictive setting applies, regardless of which <em>account</em> the user currently"
      },
      "id": "603e8914196a678f45a83de3"
    }
  ],
  "/docs/accounts/original-accounts-billing/product-based-pricing/trial-lite-accounts-deprecated": [
    {
      "sections": [
        "Overview of changes to pricing and user model",
        "Overview of how pricing model and user model relate",
        "Pricing plans explained",
        "Determine pricing model",
        "Convert to new pricing",
        "User models explained",
        "Requirements for new user model",
        "Determine which user model you're on",
        "Feature impacts of user model",
        "Transition to new models"
      ],
      "title": "Overview of changes to pricing and user model",
      "type": "docs",
      "tags": [
        "Accounts",
        "Original accounts and billing",
        "Original product-based pricing"
      ],
      "external_id": "b19d61b5a0c65ca352e2ce0e260e2b53391b94fc",
      "image": "https://docs.newrelic.com/static/1461470cc1ef0be91120c84d76a7ccdd/c1b63/new-relic-one-pricing-plan-billing-ui.png",
      "url": "https://docs.newrelic.com/docs/accounts/original-accounts-billing/original-product-based-pricing/overview-changes-pricing-user-model/",
      "published_at": "2021-12-19T14:16:05Z",
      "updated_at": "2021-11-24T14:34:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In 2020, New Relic released both a newer pricing model and a newer user model. Keep reading to learn about: How the pricing model and the user model relate to each other Pricing plans explained User models explained How to switch to the new models Overview of how pricing model and user model relate In 2020, we released both a new, improved pricing model and a new, improved user model. These models represent the future. All new sign-ups are on these new models and eventually all organizations will be on these models. But currently, our customers from before July 2020 may have one of several combinations of these as they either switch to the new pricing or migrate their users to the new user model. This table shows how pricing and user model relate to each other: Pricing plan factors User model factors Original pricing If your organization was created before July 30 2020, you remain on our original pricing model until you transition to New Relic One pricing. For original pricing accounts, you have users on the original user model unless you've migrated your users to the new model. Note that if you've added users via automated user management, they're on the new model. This means it's possible to have some users on both models. New Relic One pricing An organization is on the New Relic One pricing model: If it was created on or after July 30 2020, or If it's an older organization but has switched to New Relic One pricing. If your organization was created on or after July 30 2020, your users are on the New Relic One user model. If you have an older organization, you have users on the original user model unless you've migrated your users to the new model. Pricing plans explained New Relic organizations are on one of two pricing models: New Relic One pricing: Our new pricing model is simpler. It bills on a) which edition you're on (Standard, Pro, or Enterprise), b) the GBs of data ingested, and c) how many billable full platform users you have. All organizations created on or after July 30 2020 are on this pricing model, as are older organizations that have switched to this pricing. There are two versions of this pricing model. Our original product-based pricing model: this is based on subscriptions to specific products (for example, APM, Browser, Mobile, Infrastructure). Some organizations have the option to transition to the newer pricing model: in that case, their users remain on our original user model. Determine pricing model To determine which pricing model you’re on: go to one.newrelic.com, select the account dropdown, and select Manage your plan. If you see billing information about data ingested and the number of billable users, you’re on the new pricing model. If you're on New Relic One pricing, you'll see a billing UI like this. Convert to new pricing Some organizations are able to switch to new pricing. Learn more about switching your pricing model. User models explained In this context, the term \"user model\" refers to the structure of a New Relic user record and how it interacts with the broader New Relic organization that it's in. Our two user models are: New Relic One user model: this newer, improved user model has these major differences from the original user model: All your accounts and users are contained under a top-level organization. This gives an organization stronger control over managing users and what they can access. One impact of this is that users who work with multiple New Relic organizations may have an email address associated with multiple logins. Users on this model have a different system for managing permissions and account access. Using access grants, you assign users access to specific roles on specific accounts. Original user model: some aspects of this older model that are different from our newer model: There wasn't as much organization-level control over users. For example, a New Relic user had the ability to access multiple accounts they'd been granted access to using a single login. Users on the original user model have a different user management system. To learn more about benefits of the new model, see our blog post about user model changes and an explanation of the new account structure. For impacts and limitations, see Feature impacts. Requirements for new user model Here's a synopsis of the factors governing what organizations use the new model: All New Relic organizations that signed up after July 30 2020 have users on this model, and also have the new pricing model. Some older New Relic organizations have had their users migrated to the new model by New Relic or by using the user migration procedure. Note that switching to the new pricing model is independent from migrating users. Partner accounts (resellers, managed service providers), and organizations using the partnership account structure, cannot yet migrate their users to the new model. Determine which user model you're on To determine what user model you're on, from the account dropdown select User preferences. If you see the UI below, you're on the New Relic One user model. If you see a different UI with more content displayed, you're on the original user model (see original user docs). To determine if you can manage users on the New Relic One user model, see Manage users. The user model is independent of your pricing model. For how user model relates to pricing, see the Pricing and user model table. Feature impacts of user model The new user model offers many benefits. Here are some feature impacts of being on the New Relic One user model that may be not obvious to users accustomed to our previous user model: More separation of organizations/accounts: If you have an email address associated with multiple New Relic accounts, this may mean you need to log out and log back in. Learn more about account access. Adding accounts: Pro and Enterprise edition can have more than one account in their organization. Currently users in these organizations cannot, on their own, add accounts. They must get help from their New Relic account representative. To learn about adding users to accounts, see Manage users. Alerts-related impacts: Policy actions: For alert policies and conditions, some UI locations display the user that made the last modification. For users on the new model, it will display their user ID and not their user name. For example, a policy might read Last modified Sep 7, '20 4:26 pm by user 1234567 instead of by John Doe. If you have user management permissions, you can determine what user this is by going to the User management UI. Notifications: Users on the new model won’t show up as an option when choosing users to receive alert notifications. The result of this is that these users will not get New Relic mobile app push notifications. As a workaround, you can instead use their email address as the notification channel. You can't use the REST API to generate a list of users (only original model users). Instead you'd use the user management UI. Changing account name: These users cannot change the name of their organization after it has been created. Transition to new models Some New Relic organizations on the old models are able to transition to the new models. To learn more about this, see Transition to new models.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 288.29565,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Overview of changes to <em>pricing</em> <em>and</em> user model",
        "sections": "Overview of changes to <em>pricing</em> <em>and</em> user model",
        "tags": "<em>Original</em> <em>product</em>-<em>based</em> <em>pricing</em>",
        "body": " to this <em>pricing</em>. There are two versions of this <em>pricing</em> model. Our <em>original</em> <em>product</em>-<em>based</em> <em>pricing</em> model: this is <em>based</em> on subscriptions to specific products (for example, APM, Browser, Mobile, Infrastructure). Some organizations have the option to transition to the newer <em>pricing</em> model: in that case"
      },
      "id": "603e97fa28ccbcb7c2eba754"
    },
    {
      "sections": [
        "Set session timeouts",
        "Original pricing model",
        "Requirements",
        "Overview",
        "Features",
        "Tip",
        "Select the session timeout value",
        "Select SAML SSO browser re-authentication",
        "Redirect after SAML timeout"
      ],
      "title": "Set session timeouts",
      "type": "docs",
      "tags": [
        "Accounts",
        "Original accounts and billing",
        "Original product-based pricing"
      ],
      "external_id": "a61d4c61f52ee18be0763a9cd526634d9d2f50f5",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/account-maintenance/set-session-timeouts/",
      "published_at": "2021-12-19T15:55:42Z",
      "updated_at": "2021-11-14T11:21:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Original pricing model This doc is for users on our original user model. New Relic's session configuration feature allows you to set limits on idle time before your users' browser sessions automatically expire. Requirements If you're on the New Relic One user model, see Session settings. Overview Session configuration allows you to set limits on idle time before your users' browser sessions automatically expire. A message appears three minutes before the system logs them out. Users then need to sign back in to continue. For accounts configured with SAML Single Sign On (SSO), an additional option is available to set how often the users' browser sessions are re-authenticated. Users and Restricted Users can view the time period for automatic timeout, but they cannot change it. To view the timeout value: Go to account dropdown > Account settings > Authentication > Session configuration. Features Tip Owner or Admins The session configuration options provide an additional level of security to ensure that unattended browsers will automatically time out. Session values are automatically stored in the session cookie. Additional features include: Feature Notes Easy setup Admins use the slide bar in New Relic's user interface to select predefined time periods. Default is two weeks. Separate options available by role Admins can choose for Restricted User sessions to never time out even if they select a session timeout setting. This is useful, for example, when you use a Restricted User login for demos. Automatic inheritance for child accounts By default, child accounts inherit the same session configuration as their parent account. Most restrictive by default If users have multiple accounts, the most restrictive setting applies, regardless of which account the user currently is using. Integration with SAML SSO logout URL If the account's SAML SSO configuration does not include a logout URL, New Relic includes a link from Session configuration for the Owner to set it up. If the Admin is not also the Owner, a message about the SAML SSO logout URL requirement appears. Additional re-authentication setting for SAML SSO In addition to the session timeout option, Admins can select the time (15 minutes to 2 weeks, or never) for how often a SAML-authenticated browser session must be re-authenticated. Select the session timeout value The process to select the session timeout value is the same for both SAML and non-SAML configurations. For additional SAML configuration options, see SAML SSO browser reauthentication. To select a predefined period for session timeouts with SAML SSO accounts, the account Owner must have previously identified the logout URL in the SAML SSO configuration settings. If this has not been set up, the account Admin can view the session timeout slide bar but not change it. If the Admin is also the account Owner, the Session configuration includes a link to go directly to New Relic's SAML SSO Configuration and identify the logout URL. For more information, see Setting up SSO. To select a predefined period for session timeouts for users on our original user model: Go to: account dropdown > Account settings > Authentication > Session configuration. Use the slide bar to select a time period for idle sessions to expire and log out automatically. Optional: Select the checkbox option if you do not want restricted users' browser sessions to expire. Select Save my changes. Changes take effect immediately. Select SAML SSO browser re-authentication To select a predefined period for SAML SSO-authenticated browser sessions to be re-authenticated: Go to: account dropdown > Account settings > Authentication > Session configuration. Use the SAML re-authentication time slide bar to select a time period for New Relic to check the browser session. Select Save my changes. Redirect after SAML timeout If you are logged out due to a session idle timeout on an account configured for SAML, you will be sent to the New Relic login page. Because your account is configured for SAML, you do not have a direct New Relic login. To be redirected to your SAML provider for authentication: Enter your email address in the Email field. Leave the Password field blank. Click the Sign In button. You will then be redirected to your SAML provider. Once reauthorized, you will then be returned to the New Relic website.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 271.1559,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Original</em> <em>pricing</em> model",
        "tags": "<em>Original</em> <em>product</em>-<em>based</em> <em>pricing</em>",
        "body": "<em>Original</em> <em>pricing</em> model This doc is for users on our <em>original</em> user model. New Relic&#x27;s session configuration feature allows you to set limits on idle time before your users&#x27; browser sessions automatically expire. Requirements If you&#x27;re on the New Relic One user model, see Session settings. Overview"
      },
      "id": "603e8914196a678f45a83de3"
    },
    {
      "sections": [
        "Original product-based pricing and billing",
        "Important",
        "Overview of original pricing",
        "Annual vs monthly pricing models",
        "APM and infrastructure: Compute-unit vs host-based pricing",
        "Compute unit pricing",
        "Host-based pricing",
        "Tip",
        "How is a \"host\" defined?",
        "Prorated billing",
        "Manage subscription and billing settings",
        "View summary information",
        "View or change current subscription",
        "View usage",
        "View or update billing information"
      ],
      "title": "Original product-based pricing and billing",
      "type": "docs",
      "tags": [
        "Accounts",
        "Original accounts and billing",
        "Original product-based pricing"
      ],
      "external_id": "92a9a2aaacf80af45767d6f8f15283c541b2bf08",
      "image": "https://docs.newrelic.com/static/a5a6fd548a3c62e03183f13e6be6688a/77a9e/Accounts_CU-calculation_0.png",
      "url": "https://docs.newrelic.com/docs/accounts/original-accounts-billing/product-pricing/product-based-pricing/",
      "published_at": "2021-12-19T14:24:45Z",
      "updated_at": "2021-11-14T09:27:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This doc explains our original product-based pricing model. For more on pricing and user-related changes, see Overview of changes. Overview of original pricing New Relic has two pricing models: a newer one called New Relic One pricing, and our original pricing model. Our original pricing model was based on subscriptions to specific products, like APM, Mobile, and Infrastructure. If you are on this pricing model, your users are likely on our original user model and use these original user docs. To understand more about the new pricing and user changes, see Overview of changes. For accounts on original pricing, this doc includes: Explanation of how our original pricing model works How to manage subscription and billing settings Annual vs monthly pricing models Here are the differences between billed-annually and billed-monthly plans: Pricing plans Details Annual (best price) New Relic charges your credit card each month for a year for a committed number of hosts or compute units. You can increase this amount at any time, and charges will adjust with the next monthly bill. Your account will automatically renew at the end of the year unless you change your subscription. Early termination, downgrade, or decrease in service: Unless your order form states otherwise, you will be charged at the level and quantity of service ordered until the end of the then-current term if you cancel or downgrade to a lower level of service or fewer hosts during your commitment year. Monthly (no commitment) New Relic charges your credit card each month for a specified number of hosts or compute units. The account Owner can change the credit card number. To edit billing settings, use the Billing UI. Adjustments to billing settings will take effect for the next billing period. Your account automatically renews each month unless you change your subscription. You can cancel service or downgrade to a lower level of service without penalty. APM and infrastructure: Compute-unit vs host-based pricing APM offers a choice between two pricing models: compute unit (CU) based pricing and host-based pricing. New Relic Infrastructure offers only CU-based pricing. This section shows how both options are calculated, and explains what \"host\" means in these pricing contexts: Compute unit pricing CU-based pricing is available for these New Relic products: APM (choice of either CU-based pricing or host-based pricing) Infrastructure: only CU-based pricing With CU-based pricing, your monthly price is determined by the size of the host (computing power and memory) running New Relic and the number of hours it connects to New Relic during the month. If a host is connected to New Relic at any time during an hour, that hour counts towards the CU calculation. Each host is counted separately for each New Relic account the host reports data to. For example, if you have a parent account with two children accounts, each running applications on the same host for 3,000 CUs in a given month, the usage for the parent account will be 6,000 CUs. For APM, CU-based pricing is the best choice if you have many cloud-based dynamic computing resources. For this reason, CU-based pricing is sometimes referred to as cloud pricing. CUs are calculated as follows: The maximum size of a given host (CPUs + GB RAM) is capped at 16. Examples: If a host has 2 CPU cores, 2GB RAM, and connects to New Relic for one hour (or less than one hour), it consumes 4 CUs. If a host has 2 CPU cores, 2GB RAM, and connects to New Relic for an entire month (750 hours used as standard month size), it consumes 3,000 CUs. You can purchase blocks of CUs to be consumed on a monthly basis. The total number of CUs purchased monthly is calculated by adding up the estimated CU consumption for all hosts for the month. There is no month-to-month rollover of unused CUs. Also, New Relic does not charge by JVMs, containers (such as Docker or Cloud Foundry), or application instances--it charges by the hosts running those containers or application instances. Price points vary, depending on the New Relic product and subscription level. You can view CU-based account usage from the New Relic UI. Host-based pricing Tip Pricing for your APM account can be either CU-based or host-based. New Relic Infrastructure uses only CU-based pricing. With host-based pricing, New Relic charges based on the number of equivalent hosts used in a month. One equivalent host is defined as: a host connected to New Relic for 750 hours (750 hours used as standard month size). If a host is connected to New Relic at any time during an hour, that hour counts towards the host calculation. These hours can be divided across multiple hosts. For example, you might have three hosts that are each connected to New Relic for 250 hours during one month: these hours would add up to equal one equivalent host. Each host is counted separately for each New Relic account the host reports data to. For example, if you have a parent account with two child accounts, each running applications on the same single host for 750 hours in a given month, the usage for the parent account will be 2 equivalent hosts. Once connected to New Relic, hosts are distinguished by their unique hostnames. A host is connected to New Relic when the language agent is active and is deployed on the host. New Relic does not charge by containers (such as Docker or Cloud Foundry), JVMs, or application instances; it charges by the hosts running those containers or application instances. New Relic APM gives you a choice between host-based pricing and CU-based pricing. Host-based pricing is ideal if you have mainly static environments, consisting of hosts you manage in your own data center. For specifics on pricing amounts, see the New Relic APM pricing page. How is a \"host\" defined? To understand how New Relic computes both host-based pricing and CU-based pricing, it's important to understand how the word host is used. A host can be one of the following: A physical machine is a hardware-based device with dedicated physical resources, including memory, processing, and storage. Each machine has its own OS which applications run on. A virtual machine (VM) is the software implementation of a physical machine that executes programs like a physical machine. One or more virtual machines can run on a physical machine. Each virtual machine has its own OS and allocated virtual machine resources such as RAM and CPU. A cloud instance is a type of virtual machine that is run in the public cloud. In this context, virtual machines and cloud instances are different from Java Virtual Machines (JVMs) and containers. For New Relic's pricing calculation purposes, a month is defined as 750 hours. Prorated billing If you upgrade your subscription partway through your billing period, you will be subject to a prorated charge for the higher level of service over the remainder of your billing period. This will be invoiced or charged to your credit card when the upgrade is submitted. You will be notified about this charge as part of the subscription change process. If you have questions, contact your New Relic account representative. If you need to report billing issues, contact New Relic's Billing Department. Manage subscription and billing settings Important Note that as of July 30 2020, we have a newer pricing model. To learn more, see Overview of pricing. The account Owner can perform many subscription self-service functions directly from the user interface: From one.newrelic.com, select the account dropdown. Select your choice of self-service options. When making subscription changes, be sure to save any changes, agree to New Relic's Terms of Service and Supplemental Payment Terms as appropriate, and select Pay now. Optional: If you downgrade your subscription, complete New Relic's survey. Here is a summary of the available options from your account dropdown in the New Relic user interface: View summary information To view summary information about your subscription, go to the billing UI. View or change current subscription To adjust your subscription settings, use the Billing UI. If you need more help, contact your New Relic account representative, or contact New Relic's Billing Department. View usage To view your usage, use the usage UI. View or update billing information To view or update your New Relic account's billing information, see the billing UI.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 271.05356,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Original</em> <em>product</em>-<em>based</em> <em>pricing</em> <em>and</em> <em>billing</em>",
        "sections": "<em>Original</em> <em>product</em>-<em>based</em> <em>pricing</em> <em>and</em> <em>billing</em>",
        "tags": "<em>Original</em> <em>product</em>-<em>based</em> <em>pricing</em>",
        "body": ". For <em>accounts</em> on <em>original</em> <em>pricing</em>, this doc includes: Explanation of how our <em>original</em> <em>pricing</em> model works How to manage subscription and <em>billing</em> settings Annual vs monthly <em>pricing</em> models Here are the differences between billed-annually and billed-monthly plans: <em>Pricing</em> plans Details Annual (best <em>price</em>) New"
      },
      "id": "6043f753e7b9d212085799da"
    }
  ],
  "/docs/accounts/original-accounts-billing/product-pricing/product-based-pricing": [
    {
      "sections": [
        "Overview of changes to pricing and user model",
        "Overview of how pricing model and user model relate",
        "Pricing plans explained",
        "Determine pricing model",
        "Convert to new pricing",
        "User models explained",
        "Requirements for new user model",
        "Determine which user model you're on",
        "Feature impacts of user model",
        "Transition to new models"
      ],
      "title": "Overview of changes to pricing and user model",
      "type": "docs",
      "tags": [
        "Accounts",
        "Original accounts and billing",
        "Original product-based pricing"
      ],
      "external_id": "b19d61b5a0c65ca352e2ce0e260e2b53391b94fc",
      "image": "https://docs.newrelic.com/static/1461470cc1ef0be91120c84d76a7ccdd/c1b63/new-relic-one-pricing-plan-billing-ui.png",
      "url": "https://docs.newrelic.com/docs/accounts/original-accounts-billing/original-product-based-pricing/overview-changes-pricing-user-model/",
      "published_at": "2021-12-19T14:16:05Z",
      "updated_at": "2021-11-24T14:34:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In 2020, New Relic released both a newer pricing model and a newer user model. Keep reading to learn about: How the pricing model and the user model relate to each other Pricing plans explained User models explained How to switch to the new models Overview of how pricing model and user model relate In 2020, we released both a new, improved pricing model and a new, improved user model. These models represent the future. All new sign-ups are on these new models and eventually all organizations will be on these models. But currently, our customers from before July 2020 may have one of several combinations of these as they either switch to the new pricing or migrate their users to the new user model. This table shows how pricing and user model relate to each other: Pricing plan factors User model factors Original pricing If your organization was created before July 30 2020, you remain on our original pricing model until you transition to New Relic One pricing. For original pricing accounts, you have users on the original user model unless you've migrated your users to the new model. Note that if you've added users via automated user management, they're on the new model. This means it's possible to have some users on both models. New Relic One pricing An organization is on the New Relic One pricing model: If it was created on or after July 30 2020, or If it's an older organization but has switched to New Relic One pricing. If your organization was created on or after July 30 2020, your users are on the New Relic One user model. If you have an older organization, you have users on the original user model unless you've migrated your users to the new model. Pricing plans explained New Relic organizations are on one of two pricing models: New Relic One pricing: Our new pricing model is simpler. It bills on a) which edition you're on (Standard, Pro, or Enterprise), b) the GBs of data ingested, and c) how many billable full platform users you have. All organizations created on or after July 30 2020 are on this pricing model, as are older organizations that have switched to this pricing. There are two versions of this pricing model. Our original product-based pricing model: this is based on subscriptions to specific products (for example, APM, Browser, Mobile, Infrastructure). Some organizations have the option to transition to the newer pricing model: in that case, their users remain on our original user model. Determine pricing model To determine which pricing model you’re on: go to one.newrelic.com, select the account dropdown, and select Manage your plan. If you see billing information about data ingested and the number of billable users, you’re on the new pricing model. If you're on New Relic One pricing, you'll see a billing UI like this. Convert to new pricing Some organizations are able to switch to new pricing. Learn more about switching your pricing model. User models explained In this context, the term \"user model\" refers to the structure of a New Relic user record and how it interacts with the broader New Relic organization that it's in. Our two user models are: New Relic One user model: this newer, improved user model has these major differences from the original user model: All your accounts and users are contained under a top-level organization. This gives an organization stronger control over managing users and what they can access. One impact of this is that users who work with multiple New Relic organizations may have an email address associated with multiple logins. Users on this model have a different system for managing permissions and account access. Using access grants, you assign users access to specific roles on specific accounts. Original user model: some aspects of this older model that are different from our newer model: There wasn't as much organization-level control over users. For example, a New Relic user had the ability to access multiple accounts they'd been granted access to using a single login. Users on the original user model have a different user management system. To learn more about benefits of the new model, see our blog post about user model changes and an explanation of the new account structure. For impacts and limitations, see Feature impacts. Requirements for new user model Here's a synopsis of the factors governing what organizations use the new model: All New Relic organizations that signed up after July 30 2020 have users on this model, and also have the new pricing model. Some older New Relic organizations have had their users migrated to the new model by New Relic or by using the user migration procedure. Note that switching to the new pricing model is independent from migrating users. Partner accounts (resellers, managed service providers), and organizations using the partnership account structure, cannot yet migrate their users to the new model. Determine which user model you're on To determine what user model you're on, from the account dropdown select User preferences. If you see the UI below, you're on the New Relic One user model. If you see a different UI with more content displayed, you're on the original user model (see original user docs). To determine if you can manage users on the New Relic One user model, see Manage users. The user model is independent of your pricing model. For how user model relates to pricing, see the Pricing and user model table. Feature impacts of user model The new user model offers many benefits. Here are some feature impacts of being on the New Relic One user model that may be not obvious to users accustomed to our previous user model: More separation of organizations/accounts: If you have an email address associated with multiple New Relic accounts, this may mean you need to log out and log back in. Learn more about account access. Adding accounts: Pro and Enterprise edition can have more than one account in their organization. Currently users in these organizations cannot, on their own, add accounts. They must get help from their New Relic account representative. To learn about adding users to accounts, see Manage users. Alerts-related impacts: Policy actions: For alert policies and conditions, some UI locations display the user that made the last modification. For users on the new model, it will display their user ID and not their user name. For example, a policy might read Last modified Sep 7, '20 4:26 pm by user 1234567 instead of by John Doe. If you have user management permissions, you can determine what user this is by going to the User management UI. Notifications: Users on the new model won’t show up as an option when choosing users to receive alert notifications. The result of this is that these users will not get New Relic mobile app push notifications. As a workaround, you can instead use their email address as the notification channel. You can't use the REST API to generate a list of users (only original model users). Instead you'd use the user management UI. Changing account name: These users cannot change the name of their organization after it has been created. Transition to new models Some New Relic organizations on the old models are able to transition to the new models. To learn more about this, see Transition to new models.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 288.29562,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Overview of changes to <em>pricing</em> <em>and</em> user model",
        "sections": "Overview of changes to <em>pricing</em> <em>and</em> user model",
        "tags": "<em>Original</em> <em>product</em>-<em>based</em> <em>pricing</em>",
        "body": " to this <em>pricing</em>. There are two versions of this <em>pricing</em> model. Our <em>original</em> <em>product</em>-<em>based</em> <em>pricing</em> model: this is <em>based</em> on subscriptions to specific products (for example, APM, Browser, Mobile, Infrastructure). Some organizations have the option to transition to the newer <em>pricing</em> model: in that case"
      },
      "id": "603e97fa28ccbcb7c2eba754"
    },
    {
      "sections": [
        "Set session timeouts",
        "Original pricing model",
        "Requirements",
        "Overview",
        "Features",
        "Tip",
        "Select the session timeout value",
        "Select SAML SSO browser re-authentication",
        "Redirect after SAML timeout"
      ],
      "title": "Set session timeouts",
      "type": "docs",
      "tags": [
        "Accounts",
        "Original accounts and billing",
        "Original product-based pricing"
      ],
      "external_id": "a61d4c61f52ee18be0763a9cd526634d9d2f50f5",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/account-maintenance/set-session-timeouts/",
      "published_at": "2021-12-19T15:55:42Z",
      "updated_at": "2021-11-14T11:21:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Original pricing model This doc is for users on our original user model. New Relic's session configuration feature allows you to set limits on idle time before your users' browser sessions automatically expire. Requirements If you're on the New Relic One user model, see Session settings. Overview Session configuration allows you to set limits on idle time before your users' browser sessions automatically expire. A message appears three minutes before the system logs them out. Users then need to sign back in to continue. For accounts configured with SAML Single Sign On (SSO), an additional option is available to set how often the users' browser sessions are re-authenticated. Users and Restricted Users can view the time period for automatic timeout, but they cannot change it. To view the timeout value: Go to account dropdown > Account settings > Authentication > Session configuration. Features Tip Owner or Admins The session configuration options provide an additional level of security to ensure that unattended browsers will automatically time out. Session values are automatically stored in the session cookie. Additional features include: Feature Notes Easy setup Admins use the slide bar in New Relic's user interface to select predefined time periods. Default is two weeks. Separate options available by role Admins can choose for Restricted User sessions to never time out even if they select a session timeout setting. This is useful, for example, when you use a Restricted User login for demos. Automatic inheritance for child accounts By default, child accounts inherit the same session configuration as their parent account. Most restrictive by default If users have multiple accounts, the most restrictive setting applies, regardless of which account the user currently is using. Integration with SAML SSO logout URL If the account's SAML SSO configuration does not include a logout URL, New Relic includes a link from Session configuration for the Owner to set it up. If the Admin is not also the Owner, a message about the SAML SSO logout URL requirement appears. Additional re-authentication setting for SAML SSO In addition to the session timeout option, Admins can select the time (15 minutes to 2 weeks, or never) for how often a SAML-authenticated browser session must be re-authenticated. Select the session timeout value The process to select the session timeout value is the same for both SAML and non-SAML configurations. For additional SAML configuration options, see SAML SSO browser reauthentication. To select a predefined period for session timeouts with SAML SSO accounts, the account Owner must have previously identified the logout URL in the SAML SSO configuration settings. If this has not been set up, the account Admin can view the session timeout slide bar but not change it. If the Admin is also the account Owner, the Session configuration includes a link to go directly to New Relic's SAML SSO Configuration and identify the logout URL. For more information, see Setting up SSO. To select a predefined period for session timeouts for users on our original user model: Go to: account dropdown > Account settings > Authentication > Session configuration. Use the slide bar to select a time period for idle sessions to expire and log out automatically. Optional: Select the checkbox option if you do not want restricted users' browser sessions to expire. Select Save my changes. Changes take effect immediately. Select SAML SSO browser re-authentication To select a predefined period for SAML SSO-authenticated browser sessions to be re-authenticated: Go to: account dropdown > Account settings > Authentication > Session configuration. Use the SAML re-authentication time slide bar to select a time period for New Relic to check the browser session. Select Save my changes. Redirect after SAML timeout If you are logged out due to a session idle timeout on an account configured for SAML, you will be sent to the New Relic login page. Because your account is configured for SAML, you do not have a direct New Relic login. To be redirected to your SAML provider for authentication: Enter your email address in the Email field. Leave the Password field blank. Click the Sign In button. You will then be redirected to your SAML provider. Once reauthorized, you will then be returned to the New Relic website.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 271.15588,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Original</em> <em>pricing</em> model",
        "tags": "<em>Original</em> <em>product</em>-<em>based</em> <em>pricing</em>",
        "body": "<em>Original</em> <em>pricing</em> model This doc is for users on our <em>original</em> user model. New Relic&#x27;s session configuration feature allows you to set limits on idle time before your users&#x27; browser sessions automatically expire. Requirements If you&#x27;re on the New Relic One user model, see Session settings. Overview"
      },
      "id": "603e8914196a678f45a83de3"
    },
    {
      "sections": [
        "Trial and Lite accounts",
        "Important",
        "Trial accounts",
        "Trial lengths",
        "End of trial period",
        "Caution"
      ],
      "title": "Trial and Lite accounts",
      "type": "docs",
      "tags": [
        "Accounts",
        "Original accounts and billing",
        "Original product-based pricing"
      ],
      "external_id": "050d5fa2eea990cf75a7d4de2c15bebd612860f3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/original-accounts-billing/product-based-pricing/trial-lite-accounts-deprecated/",
      "published_at": "2021-12-19T17:31:49Z",
      "updated_at": "2021-11-14T09:23:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This document applies to our original product-based pricing, which is no longer available for new accounts. For an explanation of pricing models, see Overview of pricing. Free trials for New Relic allow you to identify which products and subscription tier best fit your business. Accounts not upgraded with a subscription before the end of the trial period become Lite accounts, losing many key features and data. Trial accounts When you start a free trial, you gain access to all the features of a Pro account including full access to support. Our products allow you to view and track trends. Pro level data retention allows you to track how changes in your business, such as marketing approaches or new technology, affect trends. Trial lengths Trial lengths depend on the product: Product Trial Length Alerts 30 days APM 14 days Browser 14 days Infrastructure 30 days Insights 30 days Mobile 30 days Synthetics 14 days End of trial period Once the trial ends, your account becomes a Lite account. Lite accounts can access all of our products except Infrastructure and Insights, but lose access to most product features and support. Caution Lite accounts retain only very recent data, which could cause the loss of valuable trend data. Avoid this by subscribing before your trial ends.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 271.0506,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Trial <em>and</em> Lite <em>accounts</em>",
        "sections": "Trial <em>and</em> Lite <em>accounts</em>",
        "tags": "<em>Original</em> <em>product</em>-<em>based</em> <em>pricing</em>",
        "body": "Important This document applies to our <em>original</em> <em>product</em>-<em>based</em> <em>pricing</em>, which is no longer available for new <em>accounts</em>. For an explanation of <em>pricing</em> models, see Overview of <em>pricing</em>. Free trials for New Relic allow you to identify which products and subscription tier best fit your business. <em>Accounts</em>"
      },
      "id": "603ec29a196a67b153a83dad"
    }
  ],
  "/docs/agile-handbook/appendices/backlog-review": [
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/agile-handbook/appendices/project-scoping-cheatsheet/",
      "sections": [
        "Project scoping cheatsheet",
        "What is this",
        "Dates",
        "Scope",
        "Resources",
        "People",
        "Before meeting ends",
        "Tip"
      ],
      "published_at": "2021-12-19T14:38:34Z",
      "title": "Project scoping cheatsheet",
      "updated_at": "2021-10-31T02:36:50Z",
      "type": "docs",
      "external_id": "57d5de7b1eeb1ae1800d8186e1302ff677d1e278",
      "document_type": "page",
      "popularity": 1,
      "body": "We use this cheatsheet to help us scope projects in a consistent way. What is this What's the elevator pitch for the feature? What's the user value? What are the most exciting tasks/stories we can tell for this feature? Who is the primary audience? Any docs deliverables you already have in mind? Dates What are you working on right now? When does this \"release\" (private beta, public beta, GA, etc.)? If private beta, how many customers and do they need docs? Scope Who will write first drafts? Do you need any templates? Does this need a liaison? Resources Is there a test account/is this in staging? Are there mockups or other resources? Do you have any other collateral to share? People Who is the primary reviewer (and backups)? Who is product manager? Who is lead dev? Who is the designer? Who is program manager? Who is the researcher? Are we doing any user research? Who is the PMM? Who is the support point person? Before meeting ends Who is writing? When is it due? Do we need tickets? Who is following up with who? ← Appendix: Ticket best practices Appendix: Backlog review → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 519.67847,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": " is the support point person? Before meeting ends Who is writing? When is it due? Do we need tickets? Who is following up with who? ← Appendix: Ticket best practices Appendix: <em>Backlog</em> <em>review</em> → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue."
      },
      "id": "616c0dfa28ccbc90b0002530"
    },
    {
      "image": "https://docs.newrelic.com/static/aa5797a9c6aaadf52a7bac18b3ac8e83/c1b63/dealing_with_interrupts.png",
      "url": "https://docs.newrelic.com/docs/agile-handbook/sprint-mechanics/sprint-workflow-and-jira-boards/",
      "sections": [
        "Sprint workflow",
        "Planned work",
        "Unplanned work (surprises!)",
        "Jira boards: Backlog and future sprints",
        "Jira boards: Current sprint",
        "Proposed",
        "In Progress",
        "Needs Peer Editor",
        "In Peer Edit",
        "Peer Edit Done",
        "Blocked",
        "Done",
        "Incomplete (\"carry-over\") tickets",
        "Tip"
      ],
      "published_at": "2021-12-19T15:27:56Z",
      "title": "Sprint workflow",
      "updated_at": "2021-11-06T12:49:01Z",
      "type": "docs",
      "external_id": "e18dcb23b9a43a083a2c4496d6fb9a20b9efc496",
      "document_type": "page",
      "popularity": 1,
      "body": "All of our sprint work is tracked in Jira. The workflow depends on what type of work we're dealing with: Planned or unplanned (\"surprise!\") work. Planned work Planned work includes all work that is currently in our backlog or has been added to the current sprint as a result of a Sprint Planning session. This could include writing or updating documentation, research, meeting with SMEs, information architecture, incorporating peer edits, SME review, and so on. Unplanned work (surprises!) Usually, we get notified of major requests far enough in advance that we can include them in liaison project plans, backlog grooming, and sprint planning. Occasionally, something bigger surprises us that needs emergency support. Follow this process with new docs asks to assess the scope of work and ensure we address valid docs needs within a reasonable amount of time. Our goal is to treat the sprint as sacred and insulate against \"surprise\" work that is not absolutely crucial. But we also want to ensure we're providing good internal customer service, and not getting hung up on process niceties for things that are small. Jira boards: Backlog and future sprints This is where the vast majority of tickets spend their time. Most tickets (even for active projects) spend at least a little time here before moving into a sprint to be actively worked. Being in the backlog doesn't mean something isn't important---just that we haven't committed to it yet.  You can also add tickets straight to a future sprint. This is where tickets tentatively assigned to a future sprint will be found. Tickets can be assigned here to be held for backlog grooming and sprint planning.  Jira boards: Current sprint Proposed This step is for work that has been assigned to the current sprint during Sprint Planning and is available to be picked up by a tech writer. When you're ready to take on a new ticket, try to work the queue from the top-down and avoid cherry picking. It's also better to pick up Needs Peer Edit tickets before committing to a new ticket. Something that needs a peer edit is close to done, and helping things across the finish line helps get value into users hands, and frees us up to think about new problems. In Progress This step is for all of the work to be done by the assignee: Research, meeting with SMEs, information architecture, writing, incorporating peer edits, SME review, and so on. Tickets are moved to this step once work is started by the TW, and remain here until the work is either complete, ready for peer review, or it becomes blocked. If additional large edits are needed after the peer review, the ticket can be moved back to In Progress for those edits. Needs Peer Editor Work that is ready for a peer edit. Once a peer editor picks it up, they move it into In Peer Edit. In Peer Edit This step is for a peer editor to review docs before they go live. Follow the Peer editor workflow, then move the ticket into Peer Edit Done.  Peer Edit Done This step is a holding state once peer editing is complete. After completing their peer edit and delivering their feedback, the peer editor moves the ticket into Peer Edit Done. From there, the assignee on the ticket (not the peer editor) moves the ticket into the appropriate column (In Progress, Blocked, or Closed). Minor edits can be completed from this column but for major doc rework, the ticket should be moved back into the In Progress column. Blocked This step is for tickets that cannot be moved forward by the team. This could be because we're waiting for a response from a SME, or for a feature to deploy, or for final signoff. The team keeps an eye on this column for tickets that may need escalation. Putting something in Blocked rather than In Progress lets us see the status of every ticket at a glance. This column can also be used for extended time out of the office for the assigned writer, if it's work that can be safely held. (If the work cannot be held while you're out, find another writer to step in and take over.) Once you're un-blocked, move the ticket to the appropriate column. If the ticket remains blocked at the end of the current sprint, it will need to be re-reviewed during backlog grooming to determine if the ticket will carry-over into the upcoming sprint, or return to the backlog until a future sprint. Done This step is for work that is 100% finished. Work gets cleared out this column before we start a new sprint. Incomplete (\"carry-over\") tickets Ticket don't carry over automatically between sprints. Instead, any ticket that gets carried over is treated as a \"new\" ticket in the next sprint planning. Before sprint planning, review any open tickets in the board that are assigned to you and figure out what to do with them. For each open ticket assigned to you (or \"carry over\"), decide if you should: Recommended: Clone the ticket and close the old one. This is the best option for partially completed work because it makes metrics easier. If you do: Clone the ticket. Note why we closed the ticket. Add an estimate of points completed in the Points Completed field. Create a follow-up ticket if necessary. Move the ticket to the next sprint. If you do: Review the ticket's action items and description to make sure they're still current. Clear out the ticket points. Move the ticket back to the backlog. If you do: Update the action items and description to make sure they're still current. Note why we moved to the backlog rather than carry over. ← Planning poker Liaisonships → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 95.936,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Jira boards: <em>Backlog</em> and future sprints",
        "body": " ticket if necessary. Move the ticket to the next sprint. If you do: <em>Review</em> the ticket&#x27;s action items and description to make sure they&#x27;re still current. Clear out the ticket points. Move the ticket back to the <em>backlog</em>. If you do: Update the action items and description to make sure they&#x27;re still"
      },
      "id": "616c0dc0196a67e6583c8164"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/agile-handbook/sprint-mechanics/meetings-and-ceremonies/",
      "sections": [
        "Meetings and ceremonies",
        "Tip",
        "Sprint planning",
        "Backlog grooming",
        "Retro"
      ],
      "published_at": "2021-12-19T15:27:10Z",
      "title": "Meetings and ceremonies",
      "updated_at": "2021-10-31T02:31:45Z",
      "type": "docs",
      "external_id": "873983fa778f17f1a92871ed4ad7673b333d4acc",
      "document_type": "page",
      "popularity": 1,
      "body": "Monday Tuesday Wednesday Thursday Friday Week 1 Sprint retro (every other sprint) Backlog grooming (end of sprint) Sprint planning (start of new sprint) Team meeting Meeting-free day Week 2 Team meeting Meeting-free day We break our work into two-week sprints. The new sprint starts on a Tuesday with sprint planning, where we commit to a set of stories that we're confident we can complete by the end of the sprint. Near the end of the sprint, we prepare for the next sprint with backlog grooming. The sprint closes with a retro where we discuss what went right and what went wrong, and then we kick off a new cycle. Each squad does their own backlog grooming and sprint planning, and manages their sprint backlog independently. We do retros together so we can talk through issues that affect both squads and share expertise and ideas. Tip Why do we end sprints on Mondays and start Tuesdays? This funny schedule makes things easier to work across timezones. If we ended sprints on Fridays, our Barcelona-based writers would need to do retros and grooming on Friday evening, and who wants that? Sprint planning On the first Tuesday of a new sprint, we commit to a series of stories until we have filled our capacity for the sprint. This is the largest meeting in the sprint---about 60 to 90 minutes. Before the sprint planning meeting, the scrum master for each squad calculates their point budget. Then, during the meeting: We select the highest priority item in the backlog. The person who nominated the story introduces it, and the team asks questions until everyone feels ready to vote. The team plays planning poker. Everyone secretly chooses a card, then we reveal them at the same time: If everyone picks the same card, that's the point value and we move on. If there's an even split between two cards, we choose the larger and move on. If it's mixed, the highest and lowest voters briefly explain their votes. Then we vote again. Once we're sure we can fit the story in, we subtract its story points from the sprint's points budget. We repeat the above steps until we've used up our points budget. We generally avoid pre-assigning people to stories during sprint planning. Instead, we pick up the next story or peer edit in the To Do column as stories are completed. Backlog grooming The day before the sprint starts, we do two rounds of backlog grooming. The purpose of backlog grooming is to prioritize work for sprint planning, and identify stories that need to be fixed before sprint planning to ensure scope is clear and planning runs smoothly. The first round of backlog grooming is by squad, where the members and manager of the squad get together and work through the grooming checklist. Then we do a second round of grooming with the managers and scrum masters to look at the sprint backlog for both squads and ensure nothing is or has fallen through the cracks. In the grooming, we: Each writer brings their personal \"top five\" tickets. The manager works with writers to prioritize those into a single, stack-ranked list for the team. We talk through stories and ensure they look ready for sprint planning (essentially, do they fit the Checklist for story readiness?). After grooming, the scrum master sends out a list of \"homework\" for stories that need improving. Retro Every other sprint, we conduct a 60 minute retrospective meeting, where we discuss: How do we feel about the sprint? What went well? Where can we improve? Anything we should start or stop doing? The goal of the retro is to improve the way we work together. That could be related to the sprint process, to how we collaborate with SMEs, to peer edits, and so on. ← Agile roles Planning poker → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 70.53881,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Backlog</em> grooming",
        "body": "Monday Tuesday Wednesday Thursday Friday Week 1 Sprint retro (every other sprint) <em>Backlog</em> grooming (end of sprint) Sprint planning (start of new sprint) Team meeting Meeting-free day Week 2 Team meeting Meeting-free day We break our work into two-week sprints. The new sprint starts on a Tuesday"
      },
      "id": "616c0dfa196a679fd43c9791"
    }
  ],
  "/docs/agile-handbook/appendices/project-scoping-cheatsheet": [
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/agile-handbook/appendices/backlog-review/",
      "sections": [
        "Backlog review",
        "Goals of backlog review",
        "Who reviews the backlog",
        "What to look for in a backlog review",
        "Tip"
      ],
      "published_at": "2021-12-19T16:23:42Z",
      "title": "Backlog review",
      "updated_at": "2021-10-31T00:57:38Z",
      "type": "docs",
      "external_id": "6c1e047df7f4a43eaa253c0183625df4785455de",
      "document_type": "page",
      "popularity": 1,
      "body": "About once a quarter, the Docs team reviews our entire backlog in Jira and GitHub. This ensures that we actually know what's in there, and that we're bubbling up the right stories from the backlog into upcoming sprints. Goals of backlog review Fix easy issues: If you can fix something quickly, just do it. Find broken windows: What are the small (or big!) broken windows lurking in our backlog? What should we consider bubbling up into a future sprint? Identify gaps in the backlog: Discover important issues that are not at all covered in the backlog. Clear out cruft: Find duplicate issues, things we'll never fix, or issues that are just no longer relevant. Check labels and fields: Is the ticket assigned a correct priority score? Do we have the correct labels for other fields? Who reviews the backlog Sometimes we'll involve the whole team in a backlog review; other times, just the managers will handle the review. The most useful time to involve the whole team is when we're onboarding a new writer: Talking through issues promotes a lot of knowledge-sharing about the product, docs, stakeholders, and how to write a good ticket. Otherwise, we'll usually assign the review to the managers. This saves time, and it also tends to make it easier to close out issues (since our managers are also our product owners). If we don't involve the whole team, we'll prepare a spreadsheet of which issues we closed and list writers who might care so they can weigh in on whether the issue should have stayed open. What to look for in a backlog review When you review an issue, perform the following checks: Check if the issue can be closed: It's already resolved, or you can't reproduce the issue It's old, and we have little evidence anyone cares It's not important enough to fix If the issue doesn't have a clear goal/task, try to discover one (but don't feel obligated to rewrite the whole issue). Add context and update as-needed. Review fields and labels and ensure they're accurate. Add a label to the issue once you're done with your review. Use this format: year_month_backlog_review. For example: 2021_october_backlog_review. This helps keep track of which issues you've reviewed on this round of backlog review It's also a useful nudge for future round of review: If an issue has more than one or two review labels, you should probably close it. ← Appendix: Project scoping cheatsheet Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1371.2261,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": ". For example: 2021_october_backlog_review. This helps keep track of which issues you&#x27;ve reviewed on this round of backlog review It&#x27;s also a useful nudge for future round of review: If an issue has more than one or two review labels, you should probably close it. ← Appendix: <em>Project</em> <em>scoping</em> <em>cheatsheet</em> Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue."
      },
      "id": "616c0dfa64441f2cb01d33a5"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/agile-handbook/appendices/ticket-best-practices/",
      "sections": [
        "Ticket best practices: How to write a sprint-ready Jira",
        "Tip",
        "Why do we use Jira?",
        "What work needs a ticket?",
        "Keeping tickets up-to-date",
        "Add Jira context to PRs and commits",
        "Checklist for writing a good ticket"
      ],
      "published_at": "2021-12-19T14:29:16Z",
      "title": "Ticket best practices: How to write a sprint-ready Jira",
      "updated_at": "2021-10-31T02:28:56Z",
      "type": "docs",
      "external_id": "2109a54437970761f71a3940f189b7f10aef0bc1",
      "document_type": "page",
      "popularity": 1,
      "body": "Jira, a project management tool made by Atlassian, is how we manage our projects and understand the work we are doing and have done. Jira tickets may seem at first to be simple to-do lists that we use to know what things to do for a project. But they are much more important than that. Tip For Relics: Use the docs.newrelic.com/jira template when you create a ticket! It'll automatically pre-fill your ticket with a template that helps create a good ticket. Why do we use Jira? We create tickets to record work-to-be done for a project, scope new work, share information for any writer to complete a story, forecast our output and to estimate project timelines, and have a record of work done. In other words, Jira has a role at every point in a project: Before a project Scoping, syncing on expectations, giving tech writer instructions During a project Keeps team and management posted about project; allows for hand-offs and swarming After a project Understand what work we did, and helps researching on future projects What work needs a ticket? There aren't hard-and-fast rules about what work needs a Jira ticket and what doesn't. A good shorthand is that any project that takes more than a couple hours is a good candidate for a ticket. However, the goal of creating tickets is not to track writer time in detail. So many kinds of work (meetings, ongoing minor liaison tasks, hero work) generally do not need to go into Jira. Keeping tickets up-to-date In general, you should write your tickets as though you might win the lottery tomorrow (a principle known as lottery factor or bus factor). In practice, someone should be able to read your ticket and figure out within about ten minutes what the status is and what the next step is. This makes it easy for us to take vacations, pass work off to another docs writer if needed, and escalate blockers. These things help with lottery factor: Update the Action Item list as you complete tasks and add or remove scope. When you move a ticket to Blocked, include a note explaining the change in status. When you close a ticket, give a summary of the work done and any relevant thoughts you have on the work and potential related issues. Update the Timeline, People, and Resources sections as the project evolves. Add important conversations (emails or Slack convos from SMEs) that give important context for the work done. (Note: It's a good idea to ask permission before doing this, because some people might not like their informal words placed in a public place.) Add Jira context to PRs and commits When you edit the site, include the Jira issue key (DOC-1234, for example) in your pull request title and/or commit summary. That makes it easier for other writers to connect the dots later if we're trying to figure out why something changed or who knows about a particular subject. Checklist for writing a good ticket Helpful title A ticket name should be easy to find via search, understand the work at a glance, mention the product or feature, and describe the goal or issue. Examples of good ticket titles: Browser API: Update custom attribute-related docs or Distributed tracing: Add more detail about CAT relationship. Action items An action item list describing the work to be done What docs are affected Links to pull requests, Google Docs drafts, etc. How substantial the writing work is in each doc How the resulting work should be structured Whether or not a peer edit is needed Anyone who should be notified when a doc is published Proper sizing Story is scoped to the smallest reasonable size Can be completed within a 2 week sprint Delivers incremental value Dates Publication date or due date Dates for other key events (betas, limited releases, etc.) Resources and people People, including last names and roles List of related or affected docs Other internal and external resources Related issues Labels and fields Jira tickets: Component, Product Group, and Priority GitHub issues: from_, pg_, and content labels ← Managing the GitHub boards Appendix: Project scoping cheatsheet → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1101.6926,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": ": Component, Product Group, and Priority GitHub issues: from_, pg_, and content labels ← Managing the GitHub boards Appendix: <em>Project</em> <em>scoping</em> <em>cheatsheet</em> → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue."
      },
      "id": "616c0d9628ccbc919400346e"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/agile-handbook/sprint-mechanics/liaisonships/",
      "sections": [
        "Liaisonships",
        "Liaison responsibilities: Manage project flow",
        "Liaison responsibilities: Build expertise",
        "Liaison responsibilities: Define content strategy (oh, and do the writing)",
        "Tip"
      ],
      "published_at": "2021-12-19T15:26:49Z",
      "title": "Liaisonships",
      "updated_at": "2021-10-31T02:30:52Z",
      "type": "docs",
      "external_id": "bf8ec36541058fe18f8395db811344baf7f23e22",
      "document_type": "page",
      "popularity": 1,
      "body": "For large projects, we'll typically assign a particular tech writer to that project as a \"liaison.\" The liaison’s job is to ensure that we get complete, consistent, and timely docs. Not every project gets a liaisonship! For smaller projects, we'll encourage teams to edit the docs directly, and then have the hero review their changes. And a smallish project may not need a full liaison—a single ticket might be enough to manage the work. To figure out which type of support is best for a given project, one of the managers on the team will have a scoping conversation with a subject matter expert. Here's a few reasons a project might get a dedicated liaison: Project is complex and would benefit from intimate familiarity with the feature. Project requires significant information architecture work. Project will produce enough docs that consistency across those docs will be hard to achieve without a centralized editor. Project SMEs would benefit from a consistent \"face\" of the tech writing team. However, a liaison is not the only author on a project. Liaisons should structure their work to maximize swarming and knowledge sharing. Liaison responsibilities: Manage project flow Activity Who? Notes Learn new thing exists Team Ideally the Hero or a Tech Docs manager gets notified directly by a PM about a new project. But sometimes we'll find out about something unexpectedly. If you're not sure whether we have a writer working on something, ask a manager on the team and they'll reach out to the subject matter expert to scope it. Have a scoping meeting Tech Docs manager The manager is responsible for tracking the general state of major projects across the company, and is generally the first point of contact for new projects. When a large new project comes up, the manager will do a pre-scope meeting with the requestor. (Appendix: Project scoping cheatsheet has a list of common questions for this pre-scope meeting.) That conversation helps figure out timeline, complexity, key docs considerations, and which writer will be a good fit as liaison. Assign a liaison Tech Docs manager Once we know we need a liaison, a manager on our team will figure out who to assign. Some of the factors we use to decide who to assign include bandwidth, familiarity with the product or feature, career goals and writing strengths, and simple interest in the topic. Keep track of project dates Liaison The managers on the team keep track of upcoming projects that don't have a liaison assigned. Once a writer gets involved, that liaison keeps track of the specifics of dates: Betas, limited releases, GAs, fast-follows, and so on. Your manager's always here to help out if you're getting blocked or dates are shifting too rapidly to plan properly. Validate the docs plan with the project team Liaison The liaison works with their stakeholders to define the information architecture and deliverables. Create tickets Liaison Since the liaison defines the information architecture, the liaison will know what kinds of deliverables we need. The liaison also acts as an advocate for their tickets in the backlog grooming and sprint planning processes, and ensures their stories meet the story quality requirements. The liaison should also ensure that our partner teams have appropriate tickets in their backlogs for their work. Remove blockers (such as reviewer delay) Liaison + Manager While the liaison is primarily responsible for handling SME relationships and removing day-to-day blockers, your manager is here to help unstick things anytime you need help. Wrap up the liaisonship Liaison Liaisonships are not forever assignments! When the bulk of your work on a project is complete, it might be time to consider ending the liaisonship. Reach out to your manager to talk about it. When you end it, let stakeholders know and update the liaison roster. Also let your stakeholders know they can always ping the docs hero for help or if they have a new project. Liaison responsibilities: Build expertise Activity Who? Notes Develop a deep expertise on feature and audience. Liaison Become the Docs Team's local expert on the feature. Understand what it does, what problems it solves, and the implications for our content. Educate the team on the feature Liaison Part of your responsibility as liaison is to share expertise around the team. That helps with swarming, but it also makes for better hero review and a smarter team that can write more intelligently about the entire New Relic One platform. Coordinate with design and/or research and test your docs Liaison Reach out to the designer and/or researcher for the project, and periodically sync on any shared concerns, user needs, etc. And you should advocate for user testing and validation of your content. Liaison responsibilities: Define content strategy (oh, and do the writing) Activity Who? Notes Define the information architecture Liaison As liaison, you're the expert on both the feature the product team is building, and the docs content (new and existing) that will support that feature. Build an IA that will meet all project needs and scale to the future. Write content Team The liaison writes much of the content for their project, especially the conceptual content like intro docs. But the whole team is expected to swarm and contribute to large projects, with the liaison coordinating that work. Peer edit drafts Liaison When we swarm and have someone else contribute to the project, the liaison peer edits their drafts to ensure consistency with the overall vision. Coordinate publication Liaison When the time comes to release (whether that's beta, GA, limited release, or EoL), it's the liaison's job to coordinate with PM, Eng, and Product Marketing to ensure docs go out on time with other deliverables. ← Sprint workflow and Jira boards What is a hero? → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 897.5469,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Liaison responsibilities: Manage <em>project</em> flow",
        "body": "-<em>scope</em> meeting with the requestor. (Appendix: <em>Project</em> <em>scoping</em> <em>cheatsheet</em> has a list of common questions for this pre-<em>scope</em> meeting.) That conversation helps figure out timeline, complexity, key docs considerations, and which writer will be a good fit as liaison. Assign a liaison Tech Docs manager"
      },
      "id": "616c0d97e7b9d227264780c5"
    }
  ],
  "/docs/agile-handbook/appendices/ticket-best-practices": [
    {
      "sections": [
        "Alerts and applied intelligence notification integrations",
        "Early access",
        "Integration details",
        "Atlassian Jira",
        "Permissions",
        "Set up a Jira destination",
        "Important",
        "Two-way sync",
        "Configure the message template",
        "Send a test notification",
        "ServiceNow (Incident-Management)",
        "Roles",
        "Set up a destination",
        "Slack",
        "Prerequisites",
        "Set up a Slack destination",
        "Configure the Slack message settings",
        "Webhook",
        "Set up a webhook destination",
        "Configure the webhook event template",
        "Customize the webhook payload",
        "Email",
        "Configure the email settings"
      ],
      "title": "Alerts and applied intelligence notification integrations",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Incident Intelligence",
        "Destinations"
      ],
      "external_id": "7220c630fc187bb61784ff2cc2213e588b269b00",
      "image": "https://docs.newrelic.com/static/d4e9baecc3a76dd1a5945f8ec0aeca66/c1b63/webhook-notification-template.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/notifications/notification-integrations/",
      "published_at": "2021-12-19T15:28:40Z",
      "updated_at": "2021-12-19T15:28:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Early access The features described here are early access. You won't be able to use these features if you're not part of the early access program. For more information on related features, see our docs on Alerts notification channels, Incident Intelligence destinations, and Proactive Detection notifications. Alerts and Applied Intelligence notification integrations are specific services and platforms you can use to send notifications from New Relic. Integration details Read more about each of our specific notification integrations. Atlassian Jira Integrate New Relic with Atlassian Jira(Cloud) and automatically create and update Jira issues. Permissions The required permissions from the Jira API-Token are create, edit, and close tickets. To enable the two-way sync toggle, the provided Jira API-Key should have an Admin role. Set up a Jira destination Create Jira issues, then enable Jira and New Relic to share updates and stay synced. To create a Jira destination, enter the following information: Destination name: Custom name to identify the destination Jira account endpoint: the URL of the destination User-name: this will be the email address of the user making the connection API token: generated from your Atlassian account Important New Relic currently supports Atlassian-Jira Classic (company-managed) projects. Before saving the destination, we recommend you test the connection via the test connection button. Jira destination configuration. We recommand to test the connection before saving. Two-way sync You can enable a two-way integration with Jira to keep the issues' state synced with the corresponding state in New Relic. To enable two-way sync, turn on the ‘two-way integration’ toggle. When turned on, a Jira Webhook would be created in your Jira account at a later stage, for the selected project (see ‘customize a message template’). The webhook would contain access details to Newrelic (URL and Newrelic-API-KEY) Configure the message template To configure a template for a Jira issue, you first need to choose a destination. You will be able to create a new destination at this stage. Upon successful connection to the destination, you will need to choose a project, and then select the Jira issue type you would like to be used. Once the issue-type is selected, the configured project's fields are fetched from your account and automatically mapped to your Jira instance. To help you get started, we automatically present the required and recommended fields and values. Required fields must be set with a value. You can add or remove optional fields(use the X mark on their right side) Jira message template. Send a test notification You can see how the JIRA issue will appear by clicking a test notification with default field values. If successful, a JIRA issue will be created and a link will appear. ServiceNow (Incident-Management) Integrate New Relic with ServiceNow Incident-Management and automatically create and update incidents. Roles As part of the integrations, we fetch fields from the your serviceNow incident table and optional values. For this, the provided ServiceNow user details required read permissions for the tables: sys_dictionary, sys_choice, sys_user and task. A read/write permission to incident To be able to fetch users for the caller column, we required read permissions for the sys_users table. The above permissions can be achieved with the roles personalize_choices, personalize_dictionary, rest_service, itil. Read/Write permissions to the api_key_credentials table is required to enable two-way integration. This can be covered with the roles credentials_admin and discovery_admin. Set up a destination To create a ServiceNow destination, enter the following information: Destination Name: custom name to identify the destination Domain: the URL of the destination User-name: the name of the user Password: the user name’s password Before saving the destination, we recommend testing the connection by clicking the test connection button. Two-way sync You can configure a two-way integration with ServiceNow Incidents Management to keep the incidents' state synced with the corresponding state in New Relic. Here are some required steps to remember when configuring the two-way integration: Turn on the two-way integration toggle. Open and download this XML file, which includes the business rule triggering events back to New Relic One. In the ServiceNow sidebar menu, go to System Definition > Business Rules. Click the menu icon in one of the column headers, select Import XML and upload the XML file you downloaded. Once the Destination is saved, a New-Relic API-Key will be kept in the api_key_credentials. The key would sent in a header as part of the callback REST call to New-Relic Configure the message template Upon a successful connection, ServiceNow incident table columns are fetched from your account and automatically mapped to your ServiceNow instance. To help you get started, we automatically present the required and recommended fields and values. Required fields must be set with a value. You can add or remove optional fields(use the X mark on their right side) Select, edit or remove fields for the ServiceNow-Incident template. Send a test notification You can see how the ServiceNow incident will appear by clicking a test notification with default field values. If successful, an incident will be created and a link will appear. Slack Send notifications-messages to your Slack channels. Prerequisites Your Slack workspace needs to have the New Relic application installed. The application must be approved by a workspace admin before it can be individually installed by users Set up a Slack destination Click on the `one-click Slack authentication' will lead you to the Slack landing page to continue the OAuth2 authentication process. On the Slack landing page, if you're not signed into the required workspace, you're redirected to Slack to sign in. Add your workspace name or select the relevant workspace and click Continue. When signed in to the selected workspace, you are requested to allow New Relic to perform the specified actions. Clicking `Allow' will redirect you back to the Destination page. Configure the Slack message settings Select a Destination(Workspace) and select a Slack-channel where the messages will be sent. You can create a new destination if there is no pre-defined destination for the required workspace. Note that, for privacy reasons, users need to be authenticated to select private channels (one-time process) Send a test notification You can send a test notification with a pre-defined example payload to the channel. This creates a message in the selected Slack-channel. Webhook Use the webhook notifier to send the notification messages to any endpoint you like. Set up a webhook destination To create a webhook destination, you need the following: Destination Name: A unique destination name URL: the endpoint of the target application, authentication and custom headers if needed. Authorization mechanism (Optional):. Can be basic authentication or a bearer token Configure the webhook event template Pick a webhook destination from the list and configure the HTTP-POST request. The request configuration requires you to: Set a name for the template. Select a pre-configured destination from the destinations list or create a new one. Add custom headers (optional). Configure the request’s payload. Customize the webhook payload You can use the default payload or customize it to contain the required data. Pick Variables from the variables menu and apply handlebars syntax to enrich your webhook. Note that the request’s content-type is JSON by default. Hence, the payload needs to keep the JSON form. See Usage examples The ‘preview’ section on the right hand-side shows an expected payload after the template is rendered. If the eventual payload would not form a valid Json, an error will be shown and it won’t be possible to save the template. If the webhook payload conforms a valid Json, you can send a test notification to your defined webhook destination We recommend sending a test notification to make sure that everything's connected correctly. Email Send email notifications to users. Configure the email settings Add one or more recipients. Users with New Relic accounts can be found via autocomplete when searching for their name or email address. To add a user without a New Relic account or email distribution list, add the full email address. Every recipient will be translated into a 'destination'. You can follow the email notifications per destination in the notifications-log Send a test notification You can send a test notification to make sure the email notifications reach the inbox.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.63655,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Alerts</em> <em>and</em> <em>applied</em> intelligence notification integrations",
        "sections": "<em>Atlassian</em> <em>Jira</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> Intelligence",
        "body": " instance. To help you get started, we automatically present the required and recommended fields and values. Required fields must be set with a value. You can add or remove optional fields(use the X mark on their right side) <em>Jira</em> message template. Send a test notification You can see <em>how</em> the <em>JIRA</em>"
      },
      "id": "618ff71628ccbc60710321e4"
    },
    {
      "image": "https://docs.newrelic.com/static/aa5797a9c6aaadf52a7bac18b3ac8e83/c1b63/dealing_with_interrupts.png",
      "url": "https://docs.newrelic.com/docs/agile-handbook/sprint-mechanics/sprint-workflow-and-jira-boards/",
      "sections": [
        "Sprint workflow",
        "Planned work",
        "Unplanned work (surprises!)",
        "Jira boards: Backlog and future sprints",
        "Jira boards: Current sprint",
        "Proposed",
        "In Progress",
        "Needs Peer Editor",
        "In Peer Edit",
        "Peer Edit Done",
        "Blocked",
        "Done",
        "Incomplete (\"carry-over\") tickets",
        "Tip"
      ],
      "published_at": "2021-12-19T15:27:56Z",
      "title": "Sprint workflow",
      "updated_at": "2021-11-06T12:49:01Z",
      "type": "docs",
      "external_id": "e18dcb23b9a43a083a2c4496d6fb9a20b9efc496",
      "document_type": "page",
      "popularity": 1,
      "body": "All of our sprint work is tracked in Jira. The workflow depends on what type of work we're dealing with: Planned or unplanned (\"surprise!\") work. Planned work Planned work includes all work that is currently in our backlog or has been added to the current sprint as a result of a Sprint Planning session. This could include writing or updating documentation, research, meeting with SMEs, information architecture, incorporating peer edits, SME review, and so on. Unplanned work (surprises!) Usually, we get notified of major requests far enough in advance that we can include them in liaison project plans, backlog grooming, and sprint planning. Occasionally, something bigger surprises us that needs emergency support. Follow this process with new docs asks to assess the scope of work and ensure we address valid docs needs within a reasonable amount of time. Our goal is to treat the sprint as sacred and insulate against \"surprise\" work that is not absolutely crucial. But we also want to ensure we're providing good internal customer service, and not getting hung up on process niceties for things that are small. Jira boards: Backlog and future sprints This is where the vast majority of tickets spend their time. Most tickets (even for active projects) spend at least a little time here before moving into a sprint to be actively worked. Being in the backlog doesn't mean something isn't important---just that we haven't committed to it yet.  You can also add tickets straight to a future sprint. This is where tickets tentatively assigned to a future sprint will be found. Tickets can be assigned here to be held for backlog grooming and sprint planning.  Jira boards: Current sprint Proposed This step is for work that has been assigned to the current sprint during Sprint Planning and is available to be picked up by a tech writer. When you're ready to take on a new ticket, try to work the queue from the top-down and avoid cherry picking. It's also better to pick up Needs Peer Edit tickets before committing to a new ticket. Something that needs a peer edit is close to done, and helping things across the finish line helps get value into users hands, and frees us up to think about new problems. In Progress This step is for all of the work to be done by the assignee: Research, meeting with SMEs, information architecture, writing, incorporating peer edits, SME review, and so on. Tickets are moved to this step once work is started by the TW, and remain here until the work is either complete, ready for peer review, or it becomes blocked. If additional large edits are needed after the peer review, the ticket can be moved back to In Progress for those edits. Needs Peer Editor Work that is ready for a peer edit. Once a peer editor picks it up, they move it into In Peer Edit. In Peer Edit This step is for a peer editor to review docs before they go live. Follow the Peer editor workflow, then move the ticket into Peer Edit Done.  Peer Edit Done This step is a holding state once peer editing is complete. After completing their peer edit and delivering their feedback, the peer editor moves the ticket into Peer Edit Done. From there, the assignee on the ticket (not the peer editor) moves the ticket into the appropriate column (In Progress, Blocked, or Closed). Minor edits can be completed from this column but for major doc rework, the ticket should be moved back into the In Progress column. Blocked This step is for tickets that cannot be moved forward by the team. This could be because we're waiting for a response from a SME, or for a feature to deploy, or for final signoff. The team keeps an eye on this column for tickets that may need escalation. Putting something in Blocked rather than In Progress lets us see the status of every ticket at a glance. This column can also be used for extended time out of the office for the assigned writer, if it's work that can be safely held. (If the work cannot be held while you're out, find another writer to step in and take over.) Once you're un-blocked, move the ticket to the appropriate column. If the ticket remains blocked at the end of the current sprint, it will need to be re-reviewed during backlog grooming to determine if the ticket will carry-over into the upcoming sprint, or return to the backlog until a future sprint. Done This step is for work that is 100% finished. Work gets cleared out this column before we start a new sprint. Incomplete (\"carry-over\") tickets Ticket don't carry over automatically between sprints. Instead, any ticket that gets carried over is treated as a \"new\" ticket in the next sprint planning. Before sprint planning, review any open tickets in the board that are assigned to you and figure out what to do with them. For each open ticket assigned to you (or \"carry over\"), decide if you should: Recommended: Clone the ticket and close the old one. This is the best option for partially completed work because it makes metrics easier. If you do: Clone the ticket. Note why we closed the ticket. Add an estimate of points completed in the Points Completed field. Create a follow-up ticket if necessary. Move the ticket to the next sprint. If you do: Review the ticket's action items and description to make sure they're still current. Clear out the ticket points. Move the ticket back to the backlog. If you do: Update the action items and description to make sure they're still current. Note why we moved to the backlog rather than carry over. ← Planning poker Liaisonships → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.34007,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Sprint</em> workflow",
        "sections": "<em>Jira</em> boards: Backlog <em>and</em> future <em>sprints</em>",
        "body": "All of our <em>sprint</em> work is tracked in <em>Jira</em>. The workflow depends on what type of work we&#x27;re dealing with: Planned or unplanned (&quot;surprise!&quot;) work. Planned work Planned work includes all work that is currently in our backlog or has been added to the current <em>sprint</em> as a result of a <em>Sprint</em> Planning"
      },
      "id": "616c0dc0196a67e6583c8164"
    },
    {
      "image": "https://docs.newrelic.com/static/e74b3e31916f093c77d41e8beef9ecc9/c1b63/lines_of_communication.png",
      "url": "https://docs.newrelic.com/docs/agile-handbook/key-concepts/agile-sprints-profusion-of-terms/",
      "sections": [
        "Agile vs sprints (vs Jira vs GitHub): A profusion of terms",
        "Agile",
        "Sprint (or scrum)",
        "Jira and GitHub issues",
        "Teams and squads",
        "Tip"
      ],
      "published_at": "2021-12-19T15:09:06Z",
      "title": "Agile vs sprints (vs Jira vs GitHub): A profusion of terms",
      "updated_at": "2021-10-31T02:29:57Z",
      "type": "docs",
      "external_id": "7f3f97222daf52cf14afcf8fc432b96b3cebd4b1",
      "document_type": "page",
      "popularity": 1,
      "body": "Our team uses an agile Sprint workflow in Jira and GitHub to manage our work. We've further divided our team into squads to simplify planning and improve accountability. All those words are pretty inconsistent in their usage, so let's break them down further. Agile People use agile to mean everything from a specific system of work (which we call sprints), to just \"moving fast, preferably in a way that lets me bend things to my whims.\" Luckily, we don't need to define it from scratch. Wikipedia does an admirable job defining it: Agile software development is an approach to software development under which requirements and solutions evolve through the collaborative effort of self-organizing and cross-functional teams and their customer(s)/end user(s). It advocates adaptive planning, evolutionary development, empirical knowledge, and continual improvement, and it encourages rapid and flexible response to change. For our team, that means our process is optimized to ship early and often. This lets us respond swiftly to changes in the product roadmap. More importantly, it ensures we validate our solutions with stakeholders, and that we're not letting valuable work sit around and get moldy when it could be out in the world making our users' lives better. Sprint (or scrum) This is the particular flavor of agile we follow. The sprint system (often referred to as scrum) is one major approach to Agile, along with other Agile systems such as Kanban. Sprint systems are often accompanied by a lot of jargon and best practices, but for our team the most essential elements are: Working in strict timeboxes (two weeks in our case) Planning that sprint in advance, and not changing the scope of the sprint (much) once it starts Expecting all team members to contribute to making the sprint a success The video Agile Product Ownership in a Nutshell (18 minutes) is an excellent resource for learning about sprint methodology. The Kindle book Scrum: a Breathtakingly Brief and Agile Introduction is also a great read that you can get through in a short afternoon. For more on the \"why\" of Sprint as our chosen methodology, see Key agile principles. And for more on the \"how,\" see Sprint workflow. Jira and GitHub issues Jira and GitHub issues are the tools we use to manage our Agile workflow. If you remember one thing about them, it should be this: using Jira or GitHub issues is not the same as having an agile workflow. They're powerful tools for tracking work and managing a backlog, but the most important part of project management is the structure we impose on that tool. Jira is for sprint work. Sprints are where roadmap docs get written, monthly commits get delivered, and deeper research percolates. We have a backlog, board, and future sprint list in Jira that help us track what people want, what's coming up, and what we're working on now. For more on the mechanics of how we use Jira, see Sprint workflow and Jira boards and Ticket best practices. We use GitHub projects for hero work, customer-reported issues, and managing the flow of PRs and edits. The Docs PRs and Issues board contains everything we're actively working on in GitHub. We'll often connect work in GitHub back to Jira by putting a Jira issue key in the PR or issue title (DOC-1234, for example). For more on the mechanics of how we use GitHub, see Managing the GitHub boards. Teams and squads Our team is the Tech Docs team. We're collectively responsible for docs.newrelic.com and sundry writing content. Our team is further divided into two agile squads (The Odd Squad and The Amp Squad), one squad for each manager. The primary function of squads is to simplify sprint planning, backlog grooming, and liaisonships. Less people means shorter meetings. It also means better information sharing: The more people you have in a group, the more lines of communication are needed (see illustration) to maintain a shared understanding. Small squads can collaborate more easily than a large team, because not everyone needs to keep in mind everything that goes on everywhere. Each squad is responsible for its own grooming and sprint planning, but the managers and scrum masters coordinate grooming to ensure we're meeting our overall goals as a team. Key agile principles → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.19208,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Agile</em> vs <em>sprints</em> (vs <em>Jira</em> vs GitHub): <em>A</em> profusion of terms",
        "sections": "<em>Agile</em> vs <em>sprints</em> (vs <em>Jira</em> vs GitHub): <em>A</em> profusion of terms",
        "body": " research percolates. We have a backlog, board, and future <em>sprint</em> list in <em>Jira</em> that help us track what people want, what&#x27;s coming up, and what we&#x27;re working on now. For more on the mechanics of <em>how</em> we use <em>Jira</em>, see <em>Sprint</em> workflow and <em>Jira</em> boards and <em>Ticket</em> <em>best</em> <em>practices</em>. We use GitHub projects"
      },
      "id": "616c0d96196a677e623c7bd2"
    }
  ],
  "/docs/agile-handbook/heroing/managing-the-github-boards": [
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/agile-handbook/heroing/what-is-a-hero/",
      "sections": [
        "What is a hero?",
        "Goals for heroing",
        "Heroing is a full-time job",
        "GitHub hero responsibilities",
        "Triage issues and PRs",
        "Review PRs",
        "Merge develop into main",
        "Provide peer reviews if time allows",
        "Slack hero responsibilities",
        "Update the Slack alias",
        "Field questions in the #documentation channel",
        "Second-shift hero support",
        "Tip"
      ],
      "published_at": "2021-12-19T16:23:43Z",
      "title": "What is a hero?",
      "updated_at": "2021-10-31T02:28:56Z",
      "type": "docs",
      "external_id": "d861167d0bea38aa6f7efdcef760bfcdcb0610ff",
      "document_type": "page",
      "popularity": 1,
      "body": "\"Hero\" is a common term across New Relic for a dedicated, interruptible person who acts as an interface for a team. When you're a docs hero, you're the face of the team. We have two heroes at any given time: the GitHub hero (for issues and pull requests) and the Slack hero (for questions through Slack). We also have a second-shift hero to support EMEA Relics. Every hero's job is to keep things moving for Relics and users. We change GitHub and heroes once a day (we used to do weekly shifts, but we've found daily shifts reduce hero burnout). Goals for heroing A Relic once described the New Relic culture this way: In the end, everyone here is working toward the same purpose [...] That person pinging you with some random request that seems unrelated to your world has the same goals as you. Help them, be kind, be patient. Your mission as hero is to personify that attitude in Docs-land. Here's what that looks like in practice: Create a consistent interface for the team. Having a dedicated hero means the answer to how to get help is always the same: \"Ping the hero!\" Because we've made this our mantra for 7 years, we don't have to re-educate the org on how to get help or who to go to with docs questions. Even as new people join the team, our processes evolve, and our entire publication toolchain has changed, our interaction model remains consistent. Directing questions to the hero avoids having a single point of failure: Even if a liaison is on leave, on the beach, or has moved onto another project, the hero is there to help. Provide a great (internal and external) customer experience. The heroes respond quickly to questions, give timely draft reviews, and perform great customer service and problem solving. Build and share knowledge about the site and our products. The heroes end up touching all kinds of obscure areas of our site and interacting with teams they may never have worked with directly. It's a great opportunity to learn more about New Relic and to build relationships across the org. Edit new content to our standards. We depend on self-service to cover lots of products with relatively few writers. The GitHub hero gives us a single accountable person who can review new content against Tech Docs team standards and turn it around quickly. Buffer the team from interrupts. Since the heroes are our \"designated interruptibles\" for their shifts, the rest of the team is freed up for deeper focus time. When it is appropriate to bring in another team member, heroes can help in streamlining the handoff and providing helpful context so that their teammate can get started quickly with the lowest possible context-switching burden. Heroing is a full-time job As the hero, you're often pulled in a lot of directions in a given shift. Because of this, the expectation is that you do not take on sprint work during your hero shift, unless you really have nothing else to do after completing your hero duties. You're also not expected to know everything as a hero! If something comes up for which you have no easy answer, let the requestor know you're on it and then ping your fellow writers or other SMEs and helpers from across New Relic for help. GitHub hero responsibilities The GitHub hero monitors the GitHub board and the flow of work through GitHub. Triage issues and PRs The GitHub hero triages every incoming pull request and issue. You'll tag the issue and pull request, route it to the correct column or team, and also help review incoming edits. For details on handling all of this, see Managing the GitHub boards. Review PRs The bulk of your time is generally spent reviewing and approving PRs from non-writers. To review an incoming PR: Label the pull request appropriately (see Managing the GitHub boards for details). Assign yourself to the pull request, so it's clear that you're on point to review and merge. Review the pull request, depending on what type of edit it is: If it's a simple \"cosmetic\" edit, review the pull request to ensure it's formatted correctly, technically accurate, and fits New Relic style guidelines. If it's a deeper edit, or a completely new doc, give it an in-depth review. The Docs site edit checklist is a great resource here. If it's a really complex or large edit, consider creating a Jira ticket for an upcoming sprint to give it the review time it needs. If it's a What's new post, pay special attention to frontmatter, links, and image formatting. This content follows marketing style, so it doesn't need to fully match our style guidelines for things like capitalization. If it's a release note, focus your review on formatting and basic style, and ensuring the release note itself is helpful. Release notes don't need to follow all docs style guidelines religiously. Preview your change in Gatsby Cloud or locally. Merge the pull request into develop. Merge develop into main We merge the develop branch into the main branch a few times a day. This kicks off a build, and ultimately is how draft docs become published docs. Currently we do this three times a day: Around 9 am PST, noon PST, and 3 pm PST. To merge, just click this magic link and follow the prompts. Provide peer reviews if time allows During super busy shifts, you likely won't have time for many of these, but if you're having a slow shift please take some time to periodically check the Writer Needs Peer Edit swim lane for fresh peer edit asks from your teammates before you switch over to doing sprint work. Slack hero responsibilities The Slack hero monitors Slack and helps answer questions about docs and route people to the right resource on the team. Update the Slack alias Update the alias to ping your name at the start of your hero shift. To update the alias, type the following into the chat box: !hero set @YOUR_SLACK_HANDLE. For example, if it's Austin's hero shift, the thing to type would be !hero set @austin. Field questions in the #documentation channel Common questions and requests include: Questions about docs content. Answer the question if you know it, or reach out to other writers if it's an area you're not familiar with. Encourage the requestor to edit the docs or submit an issue wherever possible. Triage requests for docs support. If it's a project that already has a liaison attached, connect the requestor to the appropriate writer. If it's a project without exisitng writing support, connect them to a tech docs team manager to have a scoping conversation. Questions about status of a pull request or issue. Check in and see if you can figure out, or pull in the assignee for that pull reuqest if the status isn't clear. Questions about things we don't own (blog, API Explorer, newrelic.com, etc). Help them out by directing them to the appropriate Slack channel. (For a list of properties and their owner, see Who owns the other wesbites? in Google Docs.) If you can't figure out who owns it, try asking the writing team in Slack. Second-shift hero support Our tech writers in Barcelona cover the 2nd shift heroing during their regular working hours. Unlike the US-based heroes, our Barcelona writers hero for an entire two-week sprint. Second-shift heroes cover both GitHub and Slack, but we don't expect that second-shift heroes will field every single request that comes in during their working hours since they're also carrying standard sprint duties during their hero shift. ← Liaisonships Managing the GitHub boards → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1117.0627,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>GitHub</em> hero responsibilities",
        "body": " issues and PRs The <em>GitHub</em> hero triages every incoming pull request and issue. You&#x27;ll tag the issue and pull request, route it to the correct column or team, and also help review incoming edits. For details on handling all of this, see <em>Managing</em> the <em>GitHub</em> <em>boards</em>. Review PRs The bulk of your time"
      },
      "id": "616c0d10e7b9d259f647857b"
    },
    {
      "image": "https://docs.newrelic.com/static/e74b3e31916f093c77d41e8beef9ecc9/c1b63/lines_of_communication.png",
      "url": "https://docs.newrelic.com/docs/agile-handbook/key-concepts/agile-sprints-profusion-of-terms/",
      "sections": [
        "Agile vs sprints (vs Jira vs GitHub): A profusion of terms",
        "Agile",
        "Sprint (or scrum)",
        "Jira and GitHub issues",
        "Teams and squads",
        "Tip"
      ],
      "published_at": "2021-12-19T15:09:06Z",
      "title": "Agile vs sprints (vs Jira vs GitHub): A profusion of terms",
      "updated_at": "2021-10-31T02:29:57Z",
      "type": "docs",
      "external_id": "7f3f97222daf52cf14afcf8fc432b96b3cebd4b1",
      "document_type": "page",
      "popularity": 1,
      "body": "Our team uses an agile Sprint workflow in Jira and GitHub to manage our work. We've further divided our team into squads to simplify planning and improve accountability. All those words are pretty inconsistent in their usage, so let's break them down further. Agile People use agile to mean everything from a specific system of work (which we call sprints), to just \"moving fast, preferably in a way that lets me bend things to my whims.\" Luckily, we don't need to define it from scratch. Wikipedia does an admirable job defining it: Agile software development is an approach to software development under which requirements and solutions evolve through the collaborative effort of self-organizing and cross-functional teams and their customer(s)/end user(s). It advocates adaptive planning, evolutionary development, empirical knowledge, and continual improvement, and it encourages rapid and flexible response to change. For our team, that means our process is optimized to ship early and often. This lets us respond swiftly to changes in the product roadmap. More importantly, it ensures we validate our solutions with stakeholders, and that we're not letting valuable work sit around and get moldy when it could be out in the world making our users' lives better. Sprint (or scrum) This is the particular flavor of agile we follow. The sprint system (often referred to as scrum) is one major approach to Agile, along with other Agile systems such as Kanban. Sprint systems are often accompanied by a lot of jargon and best practices, but for our team the most essential elements are: Working in strict timeboxes (two weeks in our case) Planning that sprint in advance, and not changing the scope of the sprint (much) once it starts Expecting all team members to contribute to making the sprint a success The video Agile Product Ownership in a Nutshell (18 minutes) is an excellent resource for learning about sprint methodology. The Kindle book Scrum: a Breathtakingly Brief and Agile Introduction is also a great read that you can get through in a short afternoon. For more on the \"why\" of Sprint as our chosen methodology, see Key agile principles. And for more on the \"how,\" see Sprint workflow. Jira and GitHub issues Jira and GitHub issues are the tools we use to manage our Agile workflow. If you remember one thing about them, it should be this: using Jira or GitHub issues is not the same as having an agile workflow. They're powerful tools for tracking work and managing a backlog, but the most important part of project management is the structure we impose on that tool. Jira is for sprint work. Sprints are where roadmap docs get written, monthly commits get delivered, and deeper research percolates. We have a backlog, board, and future sprint list in Jira that help us track what people want, what's coming up, and what we're working on now. For more on the mechanics of how we use Jira, see Sprint workflow and Jira boards and Ticket best practices. We use GitHub projects for hero work, customer-reported issues, and managing the flow of PRs and edits. The Docs PRs and Issues board contains everything we're actively working on in GitHub. We'll often connect work in GitHub back to Jira by putting a Jira issue key in the PR or issue title (DOC-1234, for example). For more on the mechanics of how we use GitHub, see Managing the GitHub boards. Teams and squads Our team is the Tech Docs team. We're collectively responsible for docs.newrelic.com and sundry writing content. Our team is further divided into two agile squads (The Odd Squad and The Amp Squad), one squad for each manager. The primary function of squads is to simplify sprint planning, backlog grooming, and liaisonships. Less people means shorter meetings. It also means better information sharing: The more people you have in a group, the more lines of communication are needed (see illustration) to maintain a shared understanding. Small squads can collaborate more easily than a large team, because not everyone needs to keep in mind everything that goes on everywhere. Each squad is responsible for its own grooming and sprint planning, but the managers and scrum masters coordinate grooming to ensure we're meeting our overall goals as a team. Key agile principles → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 733.5554,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Agile vs sprints (vs Jira vs <em>GitHub</em>): A profusion of terms",
        "sections": "Agile vs sprints (vs Jira vs <em>GitHub</em>): A profusion of terms",
        "body": " research percolates. We have a backlog, <em>board</em>, and future sprint list in Jira that help us track what people want, what&#x27;s coming up, and what we&#x27;re working on now. For more on the mechanics of how we use Jira, see Sprint workflow and Jira <em>boards</em> and Ticket best practices. We use <em>GitHub</em> projects"
      },
      "id": "616c0d96196a677e623c7bd2"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/agile-handbook/appendices/ticket-best-practices/",
      "sections": [
        "Ticket best practices: How to write a sprint-ready Jira",
        "Tip",
        "Why do we use Jira?",
        "What work needs a ticket?",
        "Keeping tickets up-to-date",
        "Add Jira context to PRs and commits",
        "Checklist for writing a good ticket"
      ],
      "published_at": "2021-12-19T14:29:16Z",
      "title": "Ticket best practices: How to write a sprint-ready Jira",
      "updated_at": "2021-10-31T02:28:56Z",
      "type": "docs",
      "external_id": "2109a54437970761f71a3940f189b7f10aef0bc1",
      "document_type": "page",
      "popularity": 1,
      "body": "Jira, a project management tool made by Atlassian, is how we manage our projects and understand the work we are doing and have done. Jira tickets may seem at first to be simple to-do lists that we use to know what things to do for a project. But they are much more important than that. Tip For Relics: Use the docs.newrelic.com/jira template when you create a ticket! It'll automatically pre-fill your ticket with a template that helps create a good ticket. Why do we use Jira? We create tickets to record work-to-be done for a project, scope new work, share information for any writer to complete a story, forecast our output and to estimate project timelines, and have a record of work done. In other words, Jira has a role at every point in a project: Before a project Scoping, syncing on expectations, giving tech writer instructions During a project Keeps team and management posted about project; allows for hand-offs and swarming After a project Understand what work we did, and helps researching on future projects What work needs a ticket? There aren't hard-and-fast rules about what work needs a Jira ticket and what doesn't. A good shorthand is that any project that takes more than a couple hours is a good candidate for a ticket. However, the goal of creating tickets is not to track writer time in detail. So many kinds of work (meetings, ongoing minor liaison tasks, hero work) generally do not need to go into Jira. Keeping tickets up-to-date In general, you should write your tickets as though you might win the lottery tomorrow (a principle known as lottery factor or bus factor). In practice, someone should be able to read your ticket and figure out within about ten minutes what the status is and what the next step is. This makes it easy for us to take vacations, pass work off to another docs writer if needed, and escalate blockers. These things help with lottery factor: Update the Action Item list as you complete tasks and add or remove scope. When you move a ticket to Blocked, include a note explaining the change in status. When you close a ticket, give a summary of the work done and any relevant thoughts you have on the work and potential related issues. Update the Timeline, People, and Resources sections as the project evolves. Add important conversations (emails or Slack convos from SMEs) that give important context for the work done. (Note: It's a good idea to ask permission before doing this, because some people might not like their informal words placed in a public place.) Add Jira context to PRs and commits When you edit the site, include the Jira issue key (DOC-1234, for example) in your pull request title and/or commit summary. That makes it easier for other writers to connect the dots later if we're trying to figure out why something changed or who knows about a particular subject. Checklist for writing a good ticket Helpful title A ticket name should be easy to find via search, understand the work at a glance, mention the product or feature, and describe the goal or issue. Examples of good ticket titles: Browser API: Update custom attribute-related docs or Distributed tracing: Add more detail about CAT relationship. Action items An action item list describing the work to be done What docs are affected Links to pull requests, Google Docs drafts, etc. How substantial the writing work is in each doc How the resulting work should be structured Whether or not a peer edit is needed Anyone who should be notified when a doc is published Proper sizing Story is scoped to the smallest reasonable size Can be completed within a 2 week sprint Delivers incremental value Dates Publication date or due date Dates for other key events (betas, limited releases, etc.) Resources and people People, including last names and roles List of related or affected docs Other internal and external resources Related issues Labels and fields Jira tickets: Component, Product Group, and Priority GitHub issues: from_, pg_, and content labels ← Managing the GitHub boards Appendix: Project scoping cheatsheet → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 684.9109,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": ": Component, Product Group, and Priority <em>GitHub</em> issues: from_, pg_, and content labels ← <em>Managing</em> the <em>GitHub</em> <em>boards</em> Appendix: Project scoping cheatsheet → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a <em>GitHub</em> issue."
      },
      "id": "616c0d9628ccbc919400346e"
    }
  ],
  "/docs/agile-handbook/heroing/what-is-a-hero": [
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/style-guide/writing-docs/writer-workflow/github-intro/",
      "sections": [
        "Get around GitHub",
        "Who is who in an issue/PR?",
        "Track issues in the board",
        "Deal with references in GitHub (and the style guide)",
        "Merge releases into main work (or, when do we publish?)",
        "GitHub labels",
        "Check the edit history of a doc or file",
        "Docs site history before October 2021"
      ],
      "published_at": "2021-12-15T12:25:33Z",
      "title": "Get around GitHub",
      "updated_at": "2021-12-04T10:48:34Z",
      "type": "docs",
      "external_id": "539ae5620ae9be8f8c3752fd3eda664186fbb5c4",
      "document_type": "page",
      "popularity": 1,
      "body": "As tech doc writers (TW) we edit docs, do peer edits, or use the Docs Team GitHub board to track the status of issues and pull requests (PR). Who is who in an issue/PR? GitHub keeps track of all activity concerning an issue or PR, including, of course, the people involved. When a new issue or PR is filed, check on the filer’s username and see if they're listed as a member of the New Relic organization. If they aren't, try to find them on Slack based on their username. If you're not sure about someone's affiliation, treat them as external until you know otherwise. People in an issue/PR include: Creator: The person who opened the issue or PR. This could be a writer, a Relic, or an external user. We'll label the issue or PR differently depending on who created it. If you're not sure if a user is a Relic, a good trick is to click on their profile and see if they're a member of the New Relic GitHub org. Assignee: The person taking responsibility for a PR or issue. This will usually be used by the Hero or Sidekick to assign non-TW PRs and issues to themselves. It can also be used to take a TW’s PR or issue over from them. Reviewer: The person who reviews or peer edits the code/document and approves the changes. Not necessarily the person responsible for that area or responsible for merging the commit. You can pre-assign up to 100 reviewers to a given issue. Track issues in the board The docs board has the following columns: Column Description Needs triage The Hero or Sidekick review and label issues and PRs in this column, then drag them to the appropriate column. If a PR or issue is labeled eng, the Hero/Sidekick can go ahead and click its ellipses icon to archive it. Hero: to do PRs that the Hero needs to review, publish, and follow up with SMEs as needed. Hero: Assign yourself as Assignee. In review (Hero or any TW) Drag PRs to this column when they are being reviewed. This shows who is reviewing and what is being reviewed, so two writers don’t mistakenly work on the same PR. Any TW: Writer needs PR review PRs from Tech Docs team members that need a light edit pass to make sure everything in GitHub is correct. This should be checked by other writers every few hours so PRs don’t get stale. If you have a PR that’s been lingering here too long, ask for a reviewer in #doc_sprint_talk. Whoever takes it: assign yourself as Reviewer. Any TW: needs peer edit Like our Needs Peer Edit column in Jira: A writer has requested a review of their PR. Review their PR in GitHub and leave comments. Whoever takes it: assign yourself as Reviewer. Waiting on SME/Blocked For PRs that are blocked by need for SME info or confirmation (for example, as Hero you are waiting on an answer from the person who sent in a Hero pull request). Waiting on TW to merge All reviews are complete. The TW who created the PR (or who is assigned the issue) needs to merge this work into develop. Drafts A draft is a way to open a PR while indicating that the work is still in progress and not necessarily ready to merge immediately. You can't merge a Draft PR directly. Instead, you must move it out of draft status first. When you see a draft PR (especially from outside the team!), treat it as though it's a working draft, and reach out to the creator to discuss. Read more on GitHub's drafts. As a Hero, make sure you attend to the following throughout your day: Check in with the previous Hero at the start of your day (especially on Monday at the start of the week). Don’t forget to sync with the BCN Hero if necessary. Watch for incoming PRs in #docs_deploys, and review everything in the Needs triage column. Drag cards from that column to the appropriate column. Work through the cards in the Hero: to do column. Everyone on the team helps keep things moving: All writers should keep an eye on both Any TW columns. There's one column for PRs that need a simpler review before merging (typo fixes, drive-by edits, etc), and another column for PRs that need a peer edit. There are also two blocked columns: One for PRs blocked on a SME, and another column where we're waiting on the TW who created the PR to review feedback and/or merge. After merging, remove your ticket from the board. Deal with references in GitHub (and the style guide) Don't link to anything non-public from a public place. You can reference Jira tickets, but reference tickets by issue key (DOC-1234 is ok) rather than a link (https://newrelic.atlassian.net/browse/DOC-1234 is not). Don't mention traffic or usage numbers publicly. Don't reference internal people by name. If they have a GH account, @mention their GH handle. If they don't, talk instead about teams (\"talk to a browser team engineer\" or \"Support Engineer\") rather than people. You can mention the #documentation channel and hero. Merge releases into main work (or, when do we publish?) The Hero currently merges three times a day: At 9 AM (morning), 12 PM (noon), and 3 PM (evening) Pacific. We merge release branches into main to avoid interuptions when someone merges into develop during a release. To learn more about this workflow, see the gitflow documentation in Atlassian. To start a release: Create a branch based off develop Github Desktop by clicking Current Branch in the top header, clicking New Branch in the dropdown, and then selecting Develop. Name the branch following this pattern: daily-release/mm-dd-yy-morning/noon/evening. Here's an example: daily-release/10-27-21-morning. Push your changes by clicking Push Origin in GitHub Desktop. Create a pull request into main from your new daily release branch by clicking Create Pull Request. This will open a pull request screen on github.com. Pull requests default to merging into develop, so select main as the base branch in the left side of the page and then click Submit Pull Request. Wait until all the checks complete, and then merge the pull request. All branches that follow the daily-release/mm-dd-yy-morning pattern are protected branches. This means the branches can't be deleted or pushed to by non-admins. GitHub labels Every issue needs labels to help us triage and track the health of our backlog: content: Always add, this indicates the issue is content-related rather than a design or engineering issue. pg_*: Always add to indicate the product group. For full definitions, see the \"Doc Jira and GitHub fields\" doc in the internal team Google Drive. Indicate who created the issue: from_internal: A Relic created it. from_external: A user opened it in the repo OR it came in through #customer-feedback process. from_tw: One of us created it (unless we were passing along #customer-feedback). Optionally: docs-issues-migrate: Issues that are too large in scope for the docs team to handle without product team expertise. This label alerts the docs issues team to migrate these issues into the customer feedback channel where they will be triaged and sent to product teams. Jira’d: Issues that have a corresponding Jira ticket. Make sure you leave the Jira number in the comments of the issue (for example, DOC-1234). Every pull request needs these labels so we can see where our contributions come from: content: Always add, this indicates the PR is content-related rather than design or engineering. Indicate who created the pull request: from_internal: A Relic created it. from_external: A user opened it in the repo OR it came in through #customer-feedback process. from_tw: One of us created it (unless we were passing along #customer-feedback). If the PR fixes an external issue, label it as from_tw since the work was done by a tech writer. Check the edit history of a doc or file Use any of these options to check or \"diff\" the history of a file. Option 1: GitHub history tab Navigate to the doc on the doc site and click Edit page in the right nav. Click History in the top right corner of the doc. Option 2: githistory.xy Navigate to your specific file on GitHub.com: https://github.com/newrelic/docs-website/blob/develop/src/content/docs/browser/new-relic-browser/browser-agent-spa-api/add-page-action.mdx Copy In the url, replace github.com with github.githistory.xyz: https://github.githistory.xyz/newrelic/docs-website/blob/develop/src/content/docs/browser/new-relic-browser/browser-agent-spa-api/add-page-action.mdx Copy It will take you to a site which presents the visual history of that specific file. You can view changes by clicking through the commit history at the top of your page. Option 3: Git blame Follow GitHub's documentation. Alternatively, you can use the following command in your terminal: git log --follow \"**/file_name_here.mdx\" Copy This will output the commit history of that file. By default, it only shows the first few commits. You can scroll by pressing Return multiple times. For example, to find the commit history for vmware-vsphere-monitoring-integration.mdx, I would run: git log --follow \"**/vmware-vsphere-monitoring-integration.mdx\" Copy Docs site history before October 2021 We had a large site restructure in October 2021 which lost most of the file history for our docs. You can find an archived version of our site pre-rework in the pre-IA-2021 branch. By navigating the pre-rework version of our repo, you can find file history and more.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 90.908966,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Get <em>around</em> GitHub",
        "sections": "Who <em>is</em> who in <em>an</em> <em>issue</em>&#x2F;PR?",
        "body": " yourself as Assignee. In review (<em>Hero</em> or any TW) Drag PRs to this column when they are being reviewed. This shows who is reviewing and <em>what</em> is being reviewed, so two writers don’t mistakenly work on the same PR. Any TW: Writer needs PR review PRs from Tech Docs team members that need a light edit pass"
      },
      "id": "61ab4782196a672667d0efa1"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/style-guide/writing-docs/article-templates/landing-page-template/",
      "sections": [
        "Landing page template",
        "Important",
        "Front matter",
        "Tip",
        "Introduction section",
        "Tiles",
        "Button for viewing all docs in the category",
        "Code sample"
      ],
      "published_at": "2021-12-15T12:22:31Z",
      "title": "Landing page template",
      "updated_at": "2021-11-26T05:11:08Z",
      "type": "docs",
      "external_id": "7f97d929091de18beb47bff4f6dff62e9676bbb5",
      "document_type": "page",
      "popularity": 1,
      "body": "Landing pages are a specialized type of page that serve as the starting pages for various New Relic products. For example, you'll see landing pages for Application monitoring (APM) and Browser monitoring. Important This landing page information does not apply to the docs home page. If you need to create a new landing page, you can either copy an existing landing page, or you can modify the sample landing page shown at the bottom. The next sections look at what you need to include for each landing page. Front matter When you insert the front matter, be sure to designate the type as landingPage. Here's an example: --- title: APM type: landingPage --- Copy Tip In the front matter, the following are optional: tags, translate, and redirects. So, you can leave them out if they don't have any values. Introduction section Following the front matter, the first content section is a two-column introduction (also called the hero section). This includes the following: A <LandingPageHero> component wrapping all the introductory content. A <HeroContent> component wrapping the text portion of the introduction (the content in the left column). An image or video (appears in the right column). A caption (optional), which is wrapped by the <figcaption> component. Here's an example of the hero section that shows you where to insert your content: <LandingPageHero> <HeroContent> INSERT_PARAGRAPHS_FOR_YOUR_INTRODUCTION </HeroContent> ![INSERT_ALT-TEXT_HERE](./images/INSERT_IMAGE_FILE_NAME.png \"INSERT_THE_IMAGE_TITLE_HERE\") <figcaption> INSERT_OPTIONAL_CAPTION_USING_SAME_INDENTATION_AS_IMAGE </figcaption> </LandingPageHero> Copy Tiles Tiles are a series of boxes after the introduction. They contain the main subject areas for your product. You should just list these in order you want them to appear, and the cascading style sheet will render them across the page. Here's an example of a tile: <LandingPageTileGrid> <LandingPageTile title=\"INSERT_YOUR_BOX_TITLE_HERE.\" href=\"/docs/INSERT_THE_DIRECTORY_PATH_TO_THE_TARGET_LANDING_PAGE_INDEX.HTML\" icon=\"fe-INSERT_THE_ICON_NAME\" > INSERT_TILE_CONTENT_HERE... </LandingPageTile> ... Copy For each tile, do the following: Insert a value for title that explains the purpose of the category. Insert a value for href that links to the target landing page. If the target landing page is index.html, you can just include the directory path with no filename since index.html is the default (it doesn't cause any problems if you include index.html). Insert a value for icon by prefixing the icon name with fe- (Feather icons), logo- (third-party logos), or nr- (New Relic logos). For example, here is the format for a feather icon: fe-alert-triangle). Tip For more details about icons, see Embed images. Between the LandingPageTile tags, insert text, such as a bullet list with links to product documentation. Button for viewing all docs in the category After your tiles, you should have a single button that offers to take users to all the documentation for that category. The table of contents page that gets linked here is always at the same path as the landing page, but with /table-of-contents appended to it. These table of contents pages get built automatically for every landing page. For example, if this landing page was located at /docs/apm, this link should be /docs/apm/table-of-contents. Here's an example: <ButtonLink role=\"button\" to=\"INSERT_LINK_TO_DIRECTORY_WITH_ALL_THESE_DOCS/table-of-contents\" variant=\"normal\" > View all INSERT_YOUR_CATEGORY_HERE docs </ButtonLink> Copy Code sample Here's a sample landing page you could modify to suit your needs: --- title: INSERT_YOUR_TITLE_HERE type: landingPage --- <LandingPageHero> <HeroContent> INSERT_PARAGRAPHS_FOR_YOUR_INTRODUCTION </HeroContent> ![INSERT_ALT-TEXT_HERE](./images/INSERT_IMAGE_FILE_NAME.png \"INSERT_THE_IMAGE_TITLE_HERE\") <figcaption> INSERT_OPTIONAL_CAPTION_USING_SAME_INDENTATION_AS_IMAGE </figcaption> </LandingPageHero> <LandingPageTileGrid> <LandingPageTile title=\"INSERT_YOUR_BOX_TITLE_HERE\" icon=\"fe-INSERT_ICON_NAME_HERE\" > * [INSERT_LINK_NAME](INSERT_LINK_URL) Aliquam auctor mattis nisl ut iaculis. * [INSERT_LINK_NAME](INSERT_LINK_URL) Suspendisse pharetra elit sit amet risus euismod, a consectetur tortor vulputate. </LandingPageTile> <LandingPageTile title=\"INSERT_YOUR_BOX_TITLE_HERE\" icon=\"fe-INSERT_ICON_NAME_HERE\" > * Use [INSERT_LINK_NAME](INSERT_LINK_URL) to lectus diam, ornare vitae dui suscipit, laoreet ultrices lacus. * Mauris tempor massa ac augue mattis, nec pharetra quam mollis [INSERT_LINK_NAME](INSERT_LINK_URL). </LandingPageTile> <LandingPageTile title=\"INSERT_YOUR_BOX_TITLE_HERE\" icon=\"fe-INSERT_ICON_NAME_HERE\" > * Use [INSERT_LINK_NAME](INSERT_LINK_URL) rhoncus tortor vitae libero laoreet feugiat. * Donec dui elit, fermentum vel faucibus sed, rhoncus in felis [INSERT_LINK_NAME](INSERT_LINK_URL). </LandingPageTile> <LandingPageTile title=\"INSERT_YOUR_BOX_TITLE_HERE\" icon=\"fe-INSERT_ICON_NAME_HERE\" > * Use [INSERT_LINK_NAME](INSERT_LINK_URL) uspendisse pharetra elit sit amet risus euismod. * Pellentesque finibus magna vitae hendrerit gravida [INSERT_LINK_NAME](INSERT_LINK_URL). </LandingPageTile> <LandingPageTile title=\"INSERT_YOUR_BOX_TITLE_HERE\" icon=\"fe-INSERT_ICON_NAME_HERE\" > * Use [INSERT_LINK_NAME](INSERT_LINK_URL) Etiam imperdiet felis eu ipsum consequat tristique. * Etiam imperdiet felis eu ipsum consequat tristique [INSERT_LINK_NAME](INSERT_LINK_URL). </LandingPageTile> <LandingPageTile title=\"INSERT_YOUR_BOX_TITLE_HERE\" icon=\"fe-INSERT_ICON_NAME_HERE\" > * Use [INSERT_LINK_NAME](INSERT_LINK_URL) Quisque hendrerit, dolor sed sodales aliquet. * Vestibulum varius lectus ac velit euismod [INSERT_LINK_NAME](INSERT_LINK_URL). </LandingPageTile> </LandingPageTileGrid> <ButtonLink role=\"button\" to=\"INSERT_LINK_TO_DIRECTORY_WITH_ALL_THESE_DOCS/table-of-contents\" variant=\"normal\" > View all INSERT_YOUR_CATEGORY_HERE docs </ButtonLink> Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 84.0931,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Button for viewing <em>all</em> docs in the category",
        "body": " to create a new landing page, you can either copy an existing landing page, or you can modify the sample landing page shown at the bottom. The next sections look at <em>what</em> you need to include for each landing page. Front matter When you insert the front matter, be sure to designate the type as landingPage"
      },
      "id": "61ab487d28ccbcd127c24e10"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/agile-handbook/heroing/managing-the-github-boards/",
      "sections": [
        "Managing the GitHub boards",
        "A note on Assignee vs Reviewer",
        "Drafts column",
        "Hero to triage column",
        "Hero: To do column",
        "In progress/being reviewed column",
        "Writer needs PR review column",
        "Writer needs peer edit column",
        "Waiting on SME/blocked column",
        "Waiting on TW to merge column",
        "Tip"
      ],
      "published_at": "2021-12-19T17:23:23Z",
      "title": "Managing the GitHub boards",
      "updated_at": "2021-10-31T02:28:56Z",
      "type": "docs",
      "external_id": "8552cb5f3cec74364831b7d0d85a5d3bdd734e09",
      "document_type": "page",
      "popularity": 1,
      "body": "The Docs pull requests and Issues board is our source of truth for what's going on in our project. The board is divided into a series of columns so we can see visually what the status of each issue and pull request is. A note on Assignee vs Reviewer Assignee and Reviewer have different meanings: Assignee means you own the pull request or issue and are getting it into a merge-ready state. If you are no longer owning a given pull request or issue, take your name off as assignee. Reviewer means you are actively reviewing a pull request. If it's a pull request from outside the docs team, the reviewer is also responsible for merging the pull request into develop. If you're reviewing a pull request from a fellow docs writer, add your comments and mark the pull request as Approved, then move it to Waiting on TW to merge. Drafts column These issues and pull requests are in a draft state. Do not merge until their owner moves them out of the column. This column should only be for draft pull requests. Do not \"hold\" pull requests or issues here. The Hero should look at this column multiple times per day in case a pull request has been marked ready for review. Move any ready-for-review pull requests into the correct column. Hero to triage column New issues and pull requests flow into this column automatically. As hero, you need to triage each one: Determine if the pull request or issue is content-related. If it's an eng issue or pull request, you can just Archive it to remove it from the board. Assign mandatory labels: Label type Required on Description content Issues and pull requests Use this label to indicate an issue or pull request relates to content (versus the code of the site). from_ Issues and pull requests Use this label to indicate who created the issue or pull request. Use from_tw when it's created by a docs writer, from_internal when it's created by a Relic, and from_external when it's from outside the company. pg_ Issues Indicates which New Relic product group is associated with this issue. Give the ticket an assignee (most likely you). Move the ticket to the appropriate column. Hero: To do column Work that the GitHub has triaged, but hasn't started working on yet. Tickets in this column need to have an assignee. In progress/being reviewed column Work is underway on this issue or pull request. For example, reviewing pull requests from outside the team, doing a peer edit, investigating a GitHub issue. The person doing the work should make themselves the assignee as soon as they move the pull request or issue into this column to prevent others from duplicating work. Writer needs PR review column Exactly what it says. Typically, the writer who submitted the pull request will move it to this column. A pull request review means reviewing for basic stuff like is it rendering correctly, are there typos or wording issues, and are there any obvious errors in the .mdx content shown in the diff. Once you've reviewed the pull request, mark it approved in the GitHub review UI, and move it to the Waiting on TW to merge column. Writer needs peer edit column Also exactly what it says. As with pull request review column, the writer who submitted the pull request will drag to this column. This includes all the stuff in a pull request review plus an actual peer edit. Once you've reviewed the pull request and left your feedback in the GitHub review UI, mark it Approved and move it to the Waiting on TW to merge column. From there, the author of pull request is responsible for reviewing the feedback and updating it before merging. If you find significant issues (inaccuracies, bad formatting, build issues), don't mark it Approved. Waiting on SME/blocked column Blocked until something else happens. Usually this means it's waiting on answers or approval from the SME or the person who submitted the pull request. Waiting on TW to merge column When a docs writer creates a pull request, it's their responsibility to merge it into develop at the appropriate time. After a reviewer is done with their pull request review or peer edit, they move it into this column so the original writer can merge when ready. ← What is a hero? Appendix: Ticket best practices → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 75.29534,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>A</em> note on <em>Assignee</em> vs Reviewer",
        "body": " a docs writer creates a pull request, it&#x27;s their responsibility to merge it into develop at the appropriate time. After a reviewer is done with their pull request review or peer edit, they move it into this column so the original writer can merge when ready. ← <em>What</em> is a <em>hero</em>? Appendix: Ticket best practices → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue."
      },
      "id": "616c0d6628ccbcbb0b003a72"
    }
  ],
  "/docs/agile-handbook/key-concepts/agile-roles": [
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/agile-handbook/key-concepts/key-agile-principles/",
      "sections": [
        "Key agile principles for our team",
        "Focus on the team's work",
        "Maximize swarming",
        "Enforce our points budgets",
        "Work incrementally",
        "Encourage self-service edits",
        "Tip"
      ],
      "published_at": "2021-12-19T16:25:15Z",
      "title": "Key agile principles for our team",
      "updated_at": "2021-10-31T02:30:53Z",
      "type": "docs",
      "external_id": "749e499964c577501e9ae64249513d23f3d97cdb",
      "document_type": "page",
      "popularity": 1,
      "body": "Our team uses a fairly \"by the book\" agile-scrum implementation, with the understanding that we can tweak the workflow as needed to work better for our flow. This handbook doesn't describe every aspect of how a sprint system should work, but focuses on the specific choices our team made as we evolved our agile process. As we evolve the system, it's helpful to know what our goals are. That can help illuminate whether a problem with the system is worth solving, and how to solve it without compromising on the essential things that make the team run smoothly. Focus on the team's work Our goal is for our team to deliver the most valuable work for the business and our users, every sprint. This description from Scrum: A Breathtakingly Brief and Agile Introduction nails what that means in practice: The role of each and every team member is to help the team deliver potentially shippable product in each sprint. Often, the best way to do this is by contributing work in their area of specialty. Other times, however, the team will need them to work outside of their area of specialty in order to best move backlog items from \"in progress\" to \"done.\" What we are describing is a mindset change from \"doing my job\" to \"doing the job.\" We aim to complete 100% of our work every sprint. In practice this is rarely attainable: We know that dates will move, scopes will expand, or our estimates will be wrong. But we've found that this 100% benchmark ensures we prioritize our shared team goal (the sprint), rather than our individual deliverables. Maximize swarming We want writers to build expertise, but we don't want to isolate ourselves. An alerts team at New Relic summed it up nicely: Our philosophy is, \"The team is the unit of work.\" This means that teams contribute to projects, teams solve problems, etc. We don't assign projects to individuals, and no one should ever be a single point of failure in the organization. If your team struggles to function effectively without you, your flexibility to take time off will be very limited. In such a case, we need to improve the skills and overall health of your team, ensure the team is setup for success, and ensure we have an appropriate team structure and charter in place. And an ops team demonstrates the most important practical implication: Our intention for tickets is that anyone should be able to select a task and have the information needed to understand and start on the work. Sometimes, this means we work a little slower in order to learn or teach something---and that's okay! Ultimately, working this way makes our team more resilient and makes it easier for New Relic to get consistent, high-quality docs. The Ticket best practices doc describes in detail the rules and best practices we've discovered help us achieve this. Enforce our points budgets Our team votes on all stories brought into a sprint, and we cap the number of stories based on our points budget. We generally vote as though the least-experienced person on the team will take the ticket. Having a strict points budget allows us to protect the team from overwork, predict our velocity over time, and ensure we actually have enough time to finish our work. Work incrementally Our goal is to deliver value as often as possible. Work that sits in a draft state for a long time can easily become wasted work: SMEs can become unavailable, priorities can change, and our knowledge can become stale. In practice, this means we work in fairly short two week sprints, publish early and often, and plan our projects so we can easily deliver 70% and then pivot to different priorities if something more important comes along. Encourage self-service edits Anyone can edit our open-sourced docs. With hundreds of Relics and users editing the docs each year, we can spend more of our writing time on high-impact work rather than simple maintenance edits. In order to reward teams that help us work this way, we prioritize this work in our queue. For work the writers need to do themselves, we ask for at least one full sprint of lead time. If someone comes to us the Tuesday after a sprint starts, that means they could be waiting up to two weeks for us to kick off work! But if someone edits the docs themselves, we promise to get their edit live within a 1 to 3 business day SLA. This lets us create win-wins: Rather than a simple \"no,\" a requestor can decide whether they truly need that content out now (in which case they can create that first draft) or whether they're okay waiting a week or two. ← Agile vs sprints: A profusion of terms Agile roles → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 401.57922,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Key <em>agile</em> principles for our team",
        "sections": "Key <em>agile</em> principles for our team",
        "body": " is for our team to deliver the most valuable work for the business and our users, every sprint. This description from Scrum: A Breathtakingly Brief and <em>Agile</em> Introduction nails what that means in practice: The <em>role</em> of each and every team member is to help the team deliver potentially shippable product"
      },
      "id": "616c0d96196a6768873c845b"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/agile-handbook/sprint-mechanics/meetings-and-ceremonies/",
      "sections": [
        "Meetings and ceremonies",
        "Tip",
        "Sprint planning",
        "Backlog grooming",
        "Retro"
      ],
      "published_at": "2021-12-19T15:27:10Z",
      "title": "Meetings and ceremonies",
      "updated_at": "2021-10-31T02:31:45Z",
      "type": "docs",
      "external_id": "873983fa778f17f1a92871ed4ad7673b333d4acc",
      "document_type": "page",
      "popularity": 1,
      "body": "Monday Tuesday Wednesday Thursday Friday Week 1 Sprint retro (every other sprint) Backlog grooming (end of sprint) Sprint planning (start of new sprint) Team meeting Meeting-free day Week 2 Team meeting Meeting-free day We break our work into two-week sprints. The new sprint starts on a Tuesday with sprint planning, where we commit to a set of stories that we're confident we can complete by the end of the sprint. Near the end of the sprint, we prepare for the next sprint with backlog grooming. The sprint closes with a retro where we discuss what went right and what went wrong, and then we kick off a new cycle. Each squad does their own backlog grooming and sprint planning, and manages their sprint backlog independently. We do retros together so we can talk through issues that affect both squads and share expertise and ideas. Tip Why do we end sprints on Mondays and start Tuesdays? This funny schedule makes things easier to work across timezones. If we ended sprints on Fridays, our Barcelona-based writers would need to do retros and grooming on Friday evening, and who wants that? Sprint planning On the first Tuesday of a new sprint, we commit to a series of stories until we have filled our capacity for the sprint. This is the largest meeting in the sprint---about 60 to 90 minutes. Before the sprint planning meeting, the scrum master for each squad calculates their point budget. Then, during the meeting: We select the highest priority item in the backlog. The person who nominated the story introduces it, and the team asks questions until everyone feels ready to vote. The team plays planning poker. Everyone secretly chooses a card, then we reveal them at the same time: If everyone picks the same card, that's the point value and we move on. If there's an even split between two cards, we choose the larger and move on. If it's mixed, the highest and lowest voters briefly explain their votes. Then we vote again. Once we're sure we can fit the story in, we subtract its story points from the sprint's points budget. We repeat the above steps until we've used up our points budget. We generally avoid pre-assigning people to stories during sprint planning. Instead, we pick up the next story or peer edit in the To Do column as stories are completed. Backlog grooming The day before the sprint starts, we do two rounds of backlog grooming. The purpose of backlog grooming is to prioritize work for sprint planning, and identify stories that need to be fixed before sprint planning to ensure scope is clear and planning runs smoothly. The first round of backlog grooming is by squad, where the members and manager of the squad get together and work through the grooming checklist. Then we do a second round of grooming with the managers and scrum masters to look at the sprint backlog for both squads and ensure nothing is or has fallen through the cracks. In the grooming, we: Each writer brings their personal \"top five\" tickets. The manager works with writers to prioritize those into a single, stack-ranked list for the team. We talk through stories and ensure they look ready for sprint planning (essentially, do they fit the Checklist for story readiness?). After grooming, the scrum master sends out a list of \"homework\" for stories that need improving. Retro Every other sprint, we conduct a 60 minute retrospective meeting, where we discuss: How do we feel about the sprint? What went well? Where can we improve? Anything we should start or stop doing? The goal of the retro is to improve the way we work together. That could be related to the sprint process, to how we collaborate with SMEs, to peer edits, and so on. ← Agile roles Planning poker → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 375.94266,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": " we collaborate with SMEs, to peer edits, and so on. ← <em>Agile</em> <em>roles</em> Planning poker → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue."
      },
      "id": "616c0dfa196a679fd43c9791"
    },
    {
      "sections": [
        "Alerts and applied intelligence notification integrations",
        "Early access",
        "Integration details",
        "Atlassian Jira",
        "Permissions",
        "Set up a Jira destination",
        "Important",
        "Two-way sync",
        "Configure the message template",
        "Send a test notification",
        "ServiceNow (Incident-Management)",
        "Roles",
        "Set up a destination",
        "Slack",
        "Prerequisites",
        "Set up a Slack destination",
        "Configure the Slack message settings",
        "Webhook",
        "Set up a webhook destination",
        "Configure the webhook event template",
        "Customize the webhook payload",
        "Email",
        "Configure the email settings"
      ],
      "title": "Alerts and applied intelligence notification integrations",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Incident Intelligence",
        "Destinations"
      ],
      "external_id": "7220c630fc187bb61784ff2cc2213e588b269b00",
      "image": "https://docs.newrelic.com/static/d4e9baecc3a76dd1a5945f8ec0aeca66/c1b63/webhook-notification-template.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/notifications/notification-integrations/",
      "published_at": "2021-12-19T15:28:40Z",
      "updated_at": "2021-12-19T15:28:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Early access The features described here are early access. You won't be able to use these features if you're not part of the early access program. For more information on related features, see our docs on Alerts notification channels, Incident Intelligence destinations, and Proactive Detection notifications. Alerts and Applied Intelligence notification integrations are specific services and platforms you can use to send notifications from New Relic. Integration details Read more about each of our specific notification integrations. Atlassian Jira Integrate New Relic with Atlassian Jira(Cloud) and automatically create and update Jira issues. Permissions The required permissions from the Jira API-Token are create, edit, and close tickets. To enable the two-way sync toggle, the provided Jira API-Key should have an Admin role. Set up a Jira destination Create Jira issues, then enable Jira and New Relic to share updates and stay synced. To create a Jira destination, enter the following information: Destination name: Custom name to identify the destination Jira account endpoint: the URL of the destination User-name: this will be the email address of the user making the connection API token: generated from your Atlassian account Important New Relic currently supports Atlassian-Jira Classic (company-managed) projects. Before saving the destination, we recommend you test the connection via the test connection button. Jira destination configuration. We recommand to test the connection before saving. Two-way sync You can enable a two-way integration with Jira to keep the issues' state synced with the corresponding state in New Relic. To enable two-way sync, turn on the ‘two-way integration’ toggle. When turned on, a Jira Webhook would be created in your Jira account at a later stage, for the selected project (see ‘customize a message template’). The webhook would contain access details to Newrelic (URL and Newrelic-API-KEY) Configure the message template To configure a template for a Jira issue, you first need to choose a destination. You will be able to create a new destination at this stage. Upon successful connection to the destination, you will need to choose a project, and then select the Jira issue type you would like to be used. Once the issue-type is selected, the configured project's fields are fetched from your account and automatically mapped to your Jira instance. To help you get started, we automatically present the required and recommended fields and values. Required fields must be set with a value. You can add or remove optional fields(use the X mark on their right side) Jira message template. Send a test notification You can see how the JIRA issue will appear by clicking a test notification with default field values. If successful, a JIRA issue will be created and a link will appear. ServiceNow (Incident-Management) Integrate New Relic with ServiceNow Incident-Management and automatically create and update incidents. Roles As part of the integrations, we fetch fields from the your serviceNow incident table and optional values. For this, the provided ServiceNow user details required read permissions for the tables: sys_dictionary, sys_choice, sys_user and task. A read/write permission to incident To be able to fetch users for the caller column, we required read permissions for the sys_users table. The above permissions can be achieved with the roles personalize_choices, personalize_dictionary, rest_service, itil. Read/Write permissions to the api_key_credentials table is required to enable two-way integration. This can be covered with the roles credentials_admin and discovery_admin. Set up a destination To create a ServiceNow destination, enter the following information: Destination Name: custom name to identify the destination Domain: the URL of the destination User-name: the name of the user Password: the user name’s password Before saving the destination, we recommend testing the connection by clicking the test connection button. Two-way sync You can configure a two-way integration with ServiceNow Incidents Management to keep the incidents' state synced with the corresponding state in New Relic. Here are some required steps to remember when configuring the two-way integration: Turn on the two-way integration toggle. Open and download this XML file, which includes the business rule triggering events back to New Relic One. In the ServiceNow sidebar menu, go to System Definition > Business Rules. Click the menu icon in one of the column headers, select Import XML and upload the XML file you downloaded. Once the Destination is saved, a New-Relic API-Key will be kept in the api_key_credentials. The key would sent in a header as part of the callback REST call to New-Relic Configure the message template Upon a successful connection, ServiceNow incident table columns are fetched from your account and automatically mapped to your ServiceNow instance. To help you get started, we automatically present the required and recommended fields and values. Required fields must be set with a value. You can add or remove optional fields(use the X mark on their right side) Select, edit or remove fields for the ServiceNow-Incident template. Send a test notification You can see how the ServiceNow incident will appear by clicking a test notification with default field values. If successful, an incident will be created and a link will appear. Slack Send notifications-messages to your Slack channels. Prerequisites Your Slack workspace needs to have the New Relic application installed. The application must be approved by a workspace admin before it can be individually installed by users Set up a Slack destination Click on the `one-click Slack authentication' will lead you to the Slack landing page to continue the OAuth2 authentication process. On the Slack landing page, if you're not signed into the required workspace, you're redirected to Slack to sign in. Add your workspace name or select the relevant workspace and click Continue. When signed in to the selected workspace, you are requested to allow New Relic to perform the specified actions. Clicking `Allow' will redirect you back to the Destination page. Configure the Slack message settings Select a Destination(Workspace) and select a Slack-channel where the messages will be sent. You can create a new destination if there is no pre-defined destination for the required workspace. Note that, for privacy reasons, users need to be authenticated to select private channels (one-time process) Send a test notification You can send a test notification with a pre-defined example payload to the channel. This creates a message in the selected Slack-channel. Webhook Use the webhook notifier to send the notification messages to any endpoint you like. Set up a webhook destination To create a webhook destination, you need the following: Destination Name: A unique destination name URL: the endpoint of the target application, authentication and custom headers if needed. Authorization mechanism (Optional):. Can be basic authentication or a bearer token Configure the webhook event template Pick a webhook destination from the list and configure the HTTP-POST request. The request configuration requires you to: Set a name for the template. Select a pre-configured destination from the destinations list or create a new one. Add custom headers (optional). Configure the request’s payload. Customize the webhook payload You can use the default payload or customize it to contain the required data. Pick Variables from the variables menu and apply handlebars syntax to enrich your webhook. Note that the request’s content-type is JSON by default. Hence, the payload needs to keep the JSON form. See Usage examples The ‘preview’ section on the right hand-side shows an expected payload after the template is rendered. If the eventual payload would not form a valid Json, an error will be shown and it won’t be possible to save the template. If the webhook payload conforms a valid Json, you can send a test notification to your defined webhook destination We recommend sending a test notification to make sure that everything's connected correctly. Email Send email notifications to users. Configure the email settings Add one or more recipients. Users with New Relic accounts can be found via autocomplete when searching for their name or email address. To add a user without a New Relic account or email distribution list, add the full email address. Every recipient will be translated into a 'destination'. You can follow the email notifications per destination in the notifications-log Send a test notification You can send a test notification to make sure the email notifications reach the inbox.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 71.407524,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Roles</em>",
        "body": ") and automatically create and update Jira issues. Permissions The required permissions from the Jira API-Token are create, edit, and close tickets. To enable the two-way sync toggle, the provided Jira API-Key should have an Admin <em>role</em>. Set up a Jira destination Create Jira issues, then enable Jira"
      },
      "id": "618ff71628ccbc60710321e4"
    }
  ],
  "/docs/agile-handbook/key-concepts/agile-sprints-profusion-of-terms": [
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/agile-handbook/key-concepts/key-agile-principles/",
      "sections": [
        "Key agile principles for our team",
        "Focus on the team's work",
        "Maximize swarming",
        "Enforce our points budgets",
        "Work incrementally",
        "Encourage self-service edits",
        "Tip"
      ],
      "published_at": "2021-12-19T16:25:15Z",
      "title": "Key agile principles for our team",
      "updated_at": "2021-10-31T02:30:53Z",
      "type": "docs",
      "external_id": "749e499964c577501e9ae64249513d23f3d97cdb",
      "document_type": "page",
      "popularity": 1,
      "body": "Our team uses a fairly \"by the book\" agile-scrum implementation, with the understanding that we can tweak the workflow as needed to work better for our flow. This handbook doesn't describe every aspect of how a sprint system should work, but focuses on the specific choices our team made as we evolved our agile process. As we evolve the system, it's helpful to know what our goals are. That can help illuminate whether a problem with the system is worth solving, and how to solve it without compromising on the essential things that make the team run smoothly. Focus on the team's work Our goal is for our team to deliver the most valuable work for the business and our users, every sprint. This description from Scrum: A Breathtakingly Brief and Agile Introduction nails what that means in practice: The role of each and every team member is to help the team deliver potentially shippable product in each sprint. Often, the best way to do this is by contributing work in their area of specialty. Other times, however, the team will need them to work outside of their area of specialty in order to best move backlog items from \"in progress\" to \"done.\" What we are describing is a mindset change from \"doing my job\" to \"doing the job.\" We aim to complete 100% of our work every sprint. In practice this is rarely attainable: We know that dates will move, scopes will expand, or our estimates will be wrong. But we've found that this 100% benchmark ensures we prioritize our shared team goal (the sprint), rather than our individual deliverables. Maximize swarming We want writers to build expertise, but we don't want to isolate ourselves. An alerts team at New Relic summed it up nicely: Our philosophy is, \"The team is the unit of work.\" This means that teams contribute to projects, teams solve problems, etc. We don't assign projects to individuals, and no one should ever be a single point of failure in the organization. If your team struggles to function effectively without you, your flexibility to take time off will be very limited. In such a case, we need to improve the skills and overall health of your team, ensure the team is setup for success, and ensure we have an appropriate team structure and charter in place. And an ops team demonstrates the most important practical implication: Our intention for tickets is that anyone should be able to select a task and have the information needed to understand and start on the work. Sometimes, this means we work a little slower in order to learn or teach something---and that's okay! Ultimately, working this way makes our team more resilient and makes it easier for New Relic to get consistent, high-quality docs. The Ticket best practices doc describes in detail the rules and best practices we've discovered help us achieve this. Enforce our points budgets Our team votes on all stories brought into a sprint, and we cap the number of stories based on our points budget. We generally vote as though the least-experienced person on the team will take the ticket. Having a strict points budget allows us to protect the team from overwork, predict our velocity over time, and ensure we actually have enough time to finish our work. Work incrementally Our goal is to deliver value as often as possible. Work that sits in a draft state for a long time can easily become wasted work: SMEs can become unavailable, priorities can change, and our knowledge can become stale. In practice, this means we work in fairly short two week sprints, publish early and often, and plan our projects so we can easily deliver 70% and then pivot to different priorities if something more important comes along. Encourage self-service edits Anyone can edit our open-sourced docs. With hundreds of Relics and users editing the docs each year, we can spend more of our writing time on high-impact work rather than simple maintenance edits. In order to reward teams that help us work this way, we prioritize this work in our queue. For work the writers need to do themselves, we ask for at least one full sprint of lead time. If someone comes to us the Tuesday after a sprint starts, that means they could be waiting up to two weeks for us to kick off work! But if someone edits the docs themselves, we promise to get their edit live within a 1 to 3 business day SLA. This lets us create win-wins: Rather than a simple \"no,\" a requestor can decide whether they truly need that content out now (in which case they can create that first draft) or whether they're okay waiting a week or two. ← Agile vs sprints: A profusion of terms Agile roles → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 281.76566,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Key <em>agile</em> principles for our team",
        "sections": "Key <em>agile</em> principles for our team",
        "body": " okay waiting a week or two. ← <em>Agile</em> <em>vs</em> <em>sprints</em>: A <em>profusion</em> of <em>terms</em> <em>Agile</em> roles → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a <em>GitHub</em> issue."
      },
      "id": "616c0d96196a6768873c845b"
    },
    {
      "image": "https://docs.newrelic.com/static/8945e0a9c512b8638ebf8165d47aee04/69902/QS-SignUp3.png",
      "url": "https://docs.newrelic.com/docs/codestream/start-here/codestream-user-guide/",
      "sections": [
        "New Relic CodeStream user guide",
        "Jump to a topic",
        "1. Install the CodeStream extension in your IDE and sign up.",
        "2. Connect your tools",
        "3. Discuss any block of code, at any time",
        "4. Get feedback on your work in progress",
        "5. Create or review a pull request",
        "Help and feedback"
      ],
      "published_at": "2021-12-19T15:16:39Z",
      "title": "New Relic CodeStream user guide",
      "updated_at": "2021-11-24T04:44:31Z",
      "type": "docs",
      "external_id": "fa9af0118a8872fea89fda91482c44fb69913ea2",
      "document_type": "page",
      "popularity": 1,
      "body": "Jump to a topic Use the navigation on the left to jump straight to any topic. Otherwise, read on to get started with New Relic CodeStream. If you haven't already, sign up for a free New Relic account so that you can get the most out of New Relic CodeStream. 1. Install the CodeStream extension in your IDE and sign up. Install CodeStream for VS Code, Visual Studio or JetBrains. The CodeStream pane automatically appears in the sidebar for VS Code or in a tool window at the right side for JetBrains or Visual Studio. Click Sign Up and Create a team if you're the first person from your team to join CodeStream or paste in your invitation code if you were invited to a team already on CodeStream. Learn more about how to use CodeStream. 2. Connect your tools Create and review pull requests on GitHub, GitLab or Bitbucket. Create issues on Jira, Trello, and other issue trackers. Share code discussions on Slack or Microsoft Teams. CodeStream brings the tools you use every day together in your IDE. Click on your headshot at the top of the CodeStream pane and go to the Integrations page to get all of your tools connected. 3. Discuss any block of code, at any time Whether you're trying to understand someone else's code or getting help with some code you just wrote, just select the code and ask your question. Learn more about discussing code. 4. Get feedback on your work in progress Select Request Feedback from the + menu at any time in the development cycle, whether it’s a quick look over some work in progress (even uncommitted code!) or a formal review of a completed effort. Teammates can review your changes right in their IDE, with no need to switch branches or set aside their own work. Learn more about feedback requests. 5. Create or review a pull request Look for the Pull Requests section of the CodeStream sidebar to review an open pull request. Just click on a pull request (or load one from URL) to get a complete GitHub experience right in your IDE! Note that you can create a pull request in GitHub, GitLab or Bitbucket, but support for reviewing pull requests is currently only available for GitHub (cloud or Enterprise). Learn more about pull requests. Help and feedback Report a bug or suggest an improvement in GitHub issues. Contact us directly at support@codestream.com. Follow @teamcodestream for product updates and to share feedback and questions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 279.73138,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "3. Discuss <em>any</em> block <em>of</em> code, <em>at</em> <em>any</em> time",
        "body": " CodeStream or paste in your invitation code if you were invited to a team already on CodeStream. Learn more about how to use CodeStream. 2. Connect your tools Create and review pull requests on <em>GitHub</em>, <em>Git</em>Lab or Bitbucket. Create issues on <em>Jira</em>, Trello, and other issue trackers. Share code"
      },
      "id": "61744137e7b9d2428b13c6a0"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/codestream/troubleshooting/client-logs/",
      "sections": [
        "Where can I find my CodeStream client-side logs?",
        "JetBrains",
        "VS Code",
        "Visual Studio"
      ],
      "published_at": "2021-12-19T15:29:26Z",
      "title": "Where can I find my CodeStream client-side logs?",
      "updated_at": "2021-11-13T19:59:47Z",
      "type": "docs",
      "external_id": "3c9d73c62ba313ecd37ce32e87cd085aacd11bad",
      "document_type": "page",
      "popularity": 1,
      "body": "As part of submitting a bug report on our GitHub repo it would be very helpful if you'd also include your log files. Follow these instructions for your IDE to get your log files. You can attach the log files to the GitHub issue or, if you’d prefer, you can email them to support@codestream.com and reference the GitHub issue number in the subject. JetBrains Reproduce the issue. In your IDE go to Help > Collect Logs and Diagnostic Data. This will open the finder where you should see a newly created zip file. VS Code Reproduce the issue. Open the Output view in VS Code (View: Toggle Output from the command palette) and select CodeStream (Agent) from the dropdown menu at the top-right. Copy all of the output you see and save it in a text file. Visual Studio Go to Tools > Options > CodeStream and make sure your log level is set to at least Debug. If it is set to Info, Errors, or Silent, change it to Debug, then restart Visual Studio. Reproduce the issue. Grab vs-extension.log and vs-agent.log from %localappdata%\\CodeStream\\Logs as well as the ActivityLog.xml from %AppData%\\Microsoft\\VisualStudio\\16.0_<RandomText> for Visual Studio 2019 or %AppData%\\Microsoft\\VisualStudio\\15.0_<RandomText> for Visual Studio 2017.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 241.15446,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>VS</em> Code",
        "body": " and reference the <em>GitHub</em> issue number in the subject. JetBrains Reproduce the issue. In your IDE go to Help &gt; Collect Logs and Diagnostic Data. This will open the finder where you should see a newly created zip file. <em>VS</em> Code Reproduce the issue. Open the Output view in <em>VS</em> Code (View: Toggle Output from"
      },
      "id": "6174422c28ccbcb856c6b20e"
    }
  ],
  "/docs/agile-handbook/key-concepts/key-agile-principles": [
    {
      "image": "https://docs.newrelic.com/static/e74b3e31916f093c77d41e8beef9ecc9/c1b63/lines_of_communication.png",
      "url": "https://docs.newrelic.com/docs/agile-handbook/key-concepts/agile-sprints-profusion-of-terms/",
      "sections": [
        "Agile vs sprints (vs Jira vs GitHub): A profusion of terms",
        "Agile",
        "Sprint (or scrum)",
        "Jira and GitHub issues",
        "Teams and squads",
        "Tip"
      ],
      "published_at": "2021-12-19T15:09:06Z",
      "title": "Agile vs sprints (vs Jira vs GitHub): A profusion of terms",
      "updated_at": "2021-10-31T02:29:57Z",
      "type": "docs",
      "external_id": "7f3f97222daf52cf14afcf8fc432b96b3cebd4b1",
      "document_type": "page",
      "popularity": 1,
      "body": "Our team uses an agile Sprint workflow in Jira and GitHub to manage our work. We've further divided our team into squads to simplify planning and improve accountability. All those words are pretty inconsistent in their usage, so let's break them down further. Agile People use agile to mean everything from a specific system of work (which we call sprints), to just \"moving fast, preferably in a way that lets me bend things to my whims.\" Luckily, we don't need to define it from scratch. Wikipedia does an admirable job defining it: Agile software development is an approach to software development under which requirements and solutions evolve through the collaborative effort of self-organizing and cross-functional teams and their customer(s)/end user(s). It advocates adaptive planning, evolutionary development, empirical knowledge, and continual improvement, and it encourages rapid and flexible response to change. For our team, that means our process is optimized to ship early and often. This lets us respond swiftly to changes in the product roadmap. More importantly, it ensures we validate our solutions with stakeholders, and that we're not letting valuable work sit around and get moldy when it could be out in the world making our users' lives better. Sprint (or scrum) This is the particular flavor of agile we follow. The sprint system (often referred to as scrum) is one major approach to Agile, along with other Agile systems such as Kanban. Sprint systems are often accompanied by a lot of jargon and best practices, but for our team the most essential elements are: Working in strict timeboxes (two weeks in our case) Planning that sprint in advance, and not changing the scope of the sprint (much) once it starts Expecting all team members to contribute to making the sprint a success The video Agile Product Ownership in a Nutshell (18 minutes) is an excellent resource for learning about sprint methodology. The Kindle book Scrum: a Breathtakingly Brief and Agile Introduction is also a great read that you can get through in a short afternoon. For more on the \"why\" of Sprint as our chosen methodology, see Key agile principles. And for more on the \"how,\" see Sprint workflow. Jira and GitHub issues Jira and GitHub issues are the tools we use to manage our Agile workflow. If you remember one thing about them, it should be this: using Jira or GitHub issues is not the same as having an agile workflow. They're powerful tools for tracking work and managing a backlog, but the most important part of project management is the structure we impose on that tool. Jira is for sprint work. Sprints are where roadmap docs get written, monthly commits get delivered, and deeper research percolates. We have a backlog, board, and future sprint list in Jira that help us track what people want, what's coming up, and what we're working on now. For more on the mechanics of how we use Jira, see Sprint workflow and Jira boards and Ticket best practices. We use GitHub projects for hero work, customer-reported issues, and managing the flow of PRs and edits. The Docs PRs and Issues board contains everything we're actively working on in GitHub. We'll often connect work in GitHub back to Jira by putting a Jira issue key in the PR or issue title (DOC-1234, for example). For more on the mechanics of how we use GitHub, see Managing the GitHub boards. Teams and squads Our team is the Tech Docs team. We're collectively responsible for docs.newrelic.com and sundry writing content. Our team is further divided into two agile squads (The Odd Squad and The Amp Squad), one squad for each manager. The primary function of squads is to simplify sprint planning, backlog grooming, and liaisonships. Less people means shorter meetings. It also means better information sharing: The more people you have in a group, the more lines of communication are needed (see illustration) to maintain a shared understanding. Small squads can collaborate more easily than a large team, because not everyone needs to keep in mind everything that goes on everywhere. Each squad is responsible for its own grooming and sprint planning, but the managers and scrum masters coordinate grooming to ensure we're meeting our overall goals as a team. Key agile principles → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.51094,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Agile</em> vs sprints (vs Jira vs GitHub): A profusion of terms",
        "sections": "<em>Agile</em> vs sprints (vs Jira vs GitHub): A profusion of terms",
        "body": " meeting <em>our</em> overall goals as a <em>team</em>. <em>Key</em> <em>agile</em> <em>principles</em> → Tip We welcome thoughts or questions on <em>our</em> handbook! The easiest way to get in touch is to file a GitHub issue."
      },
      "id": "616c0d96196a677e623c7bd2"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/agile-handbook/key-concepts/agile-roles/",
      "sections": [
        "Agile roles",
        "Team member",
        "Scrum master",
        "Tech Docs managers (and product owner)",
        "Key stakeholders",
        "Tip"
      ],
      "published_at": "2021-12-19T15:13:39Z",
      "title": "Agile roles",
      "updated_at": "2021-10-31T02:29:56Z",
      "type": "docs",
      "external_id": "fcfd8b2a56ccc61ba57d207f49876b4788a1ca51",
      "document_type": "page",
      "popularity": 1,
      "body": "Like most agile teams, we divide up the roles on our team into team members, scrum masters, product owners, and managers. We treat some of those roles differently than a traditional engineer-centric scrum team: We combine the role of team manager and product owner into one person. Each writer is responsible for \"liaisonships\" where they track work across a particular product or feature and bring in appropriate stories. We don't expect scrum masters to clear blockers (that's a manager's job), and scrum mastering is not a full-time job. We divide the team further into two squads, each with its own team members, scrum master, and product owners. Team member Team members are responsible for: Doing writing work. Improving the team's processes and how we work together. Writing stories for sprints in accordance with their work (via laisionships, SME conversations, hero work, etc.). The person who nominates a story will also present it during sprint planning. See Ticket best practices for tips on writing a good ticket. Most team members will be assigned one or more Liaisonships (project assignments). Liaisons are responsible for nominating stories for sprints from other teams. Scrum master Each squad has a scrum master. The scrum master is not responsible for unblocking stories or communicating with stakeholders on behalf of the team (this work belongs to individual writers and their manager). We believe this lets the business have a single point of accountability (managers) for decisions, and ensures scrum masters have hands-on experience of what it's like to be a writer. Instead, for us the scrum master is a custodian of the sprint process and the MC for sprint meetings. In backlog grooming: The scrum master handles the mechanics of talking through each ticket and facilitating conversations about story quality. In sprint planning: The scrum master leads conversation, tracks discussion time, adds point values to stories, and organizes/ranks sprint candidates in real time. They don't present the stories, though—stories should be introduced by the person who nominated the story for the sprint. In retros: The scrum master facilitates the discussion, captures action items, and takes notes. Tech Docs managers (and product owner) Each squad has a manager---or perhaps you could say each manager has a squad. Either way, the manager's role is to prioritize the right work for the business, maintain a healthy workload, and help escalate when a writer needs help. The managers are responsible for understanding how our entire body of work serves the organization, and making informed decisions about our velocity and workload. The manager engages with other teams to know which features may be coming into the pipeline and has a general understanding of work that may be in future sprints. This lets them make final priority decisions for the team and be accountable to the business for those tradeoffs. The manager is also responsible for assigning liaisonships and ensuring we're covering the portfolio. They'll also work with other teams to unblock writers when needed. Key stakeholders Our key internal stakeholders include PMs, engineers, designers, and executives. Writers work with the stakeholders to know when new work is coming, and to communicate documentation needs/timelines. We should ensure our stakeholders know that we work in two week sprints, so that they can give us adequate lead time and get their documentation needs met. ← Key agile principles Meetings and ceremonies → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 127.71691,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Agile</em> roles",
        "sections": "<em>Agile</em> roles",
        "body": " needs&#x2F;timelines. We should ensure <em>our</em> stakeholders know that we work in two week sprints, so that they can give us adequate lead time and get their documentation needs met. ← <em>Key</em> <em>agile</em> <em>principles</em> Meetings and ceremonies → Tip We welcome thoughts or questions on <em>our</em> handbook! The easiest way to get in touch is to file a GitHub issue."
      },
      "id": "616c0d97e7b9d2b2f047837d"
    },
    {
      "sections": [
        "Advanced configuration for Network Performance Monitoring",
        "SNMP-base YAML sample file",
        "Devices section",
        "Trap section",
        "Discovery section",
        "Global section",
        "Optional SNMPv3 configuration",
        "Tip",
        "Optional external config files",
        "Discovery CIDRs",
        "Devices",
        "The match_attributes attribute",
        "Default 'OR' with null and empty values",
        "'AND', omit null and empty values",
        "Single match, omit null and empty values",
        "The flow_only attribute",
        "The response_time and ping_only attributes",
        "Flow data application mapping",
        "Flow data input filtering",
        "Example Filters"
      ],
      "title": "Advanced configuration for Network Performance Monitoring",
      "type": "docs",
      "tags": [
        "Integrations",
        "Network performance monitoring",
        "Advanced configuration"
      ],
      "external_id": "61f8c58056547c4a9a2c534617559472e3e62fd1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/network-performance-monitoring/advanced/advanced-config/",
      "published_at": "2021-12-19T15:15:09Z",
      "updated_at": "2021-12-19T05:14:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you want to explore all the options you can use when configuring the monitoring of your network performance, see the following sections. SNMP-base YAML sample file Here's an example of the various configuration options available in the snmp-base.yaml file used by the ktranslate docker image to poll for SNMP and flow data devices. You can also see a heavily-commented sample in the ktranslate repository on GitHub. devices: # Sample of SNMP v2c device ups_snmpv2c__10.10.0.201: device_name: ups_snmpv2c device_ip: 10.10.0.201 snmp_comm: public oid: .1.3.6.1.4.1.318.1.3.27 description: \"APC Web/SNMP Management Card (MB:v4.1.0 PF:v6.2.1 PN:apc_hw05_aos_621.bin AF1:v6.2.1 AN1:apc_hw05_sumx_621.bin MN:AP9537SUM HR:05 SN: ABC123DEF456 MD:05/21/2016) (Embedded PowerNet SNMP Agent SW v2.2 compatible)\" last_checked: 2021-11-09T18:14:59.907821489Z mib_profile: apc_ups.yml provider: kentik-ups poll_time_sec: 300 retries: 1 timeout_ms: 5000 user_tags: owning_team: dc_ops discovered_mibs: - PowerNet-MIB_UPS - TCP-MIB - UDP-MIB # Sample of SNMP v3 device router_snmpv3__10.10.0.202: device_name: router_snmpv3 device_ip: 10.10.0.202 snmp_v3: user_name: userNamev3 authentication_protocol: MD5 authentication_passphrase: authPassPrivacy privacy_protocol: AES256 privacy_passphrase: passPrivacy oid: .1.3.6.1.4.1.9.1.544 description: \"Cisco IOS Software, 3800 Software (C3845-ADVENTERPRISEK9-M), Version 15.1(3)T4, RELEASE SOFTWARE (fc1)\\r\\nTechnical Support: http://www.cisco.com/techsupport\\r\\nCopyright (c) 1986-2012 by Cisco Systems, Inc.\\r\\nCompiled Thu 24-May-12 04:27 by prod_rel_team\" last_checked: 2021-11-09T18:14:59.907821489Z mib_profile: cisco-asr.yml provider: kentik-router user_tags: owning_team: core-networking discovered_mibs: - BGP4-MIB - CISCO-MEMORY-POOL-MIB - CISCO-PROCESS-MIB - IF-MIB - OSPF-MIB engine_id: \"80:00:01:01:0a:14:1e:28\" match_attributes: if_interface_name: \"^Ten.*|^Gig.*\" \"!if_Alias\": \"[Uu]plink\" # Sample of SNMP v1 device netbotz_snmpv1__10.10.0.203: device_name: netbotz_snmpv1 device_ip: 10.10.0.201 snmp_comm: public use_snmp_v1: true oid: .1.3.6.1.4.1.5528.100.20.10.2013 description: \"Linux netbotz930A7A 2.6.12 #307 Wed Dec 29 15:25:32 EST 2010 ppc\" last_checked: 2021-11-09T18:14:59.907821489Z mib_profile: apc-netbotz.yml provider: kentik-netbotz user_tags: owning_team: sys_ops discovered_mibs: - IF-MIB - IP-MIB - TCP-MIB - UDP-MIB no_use_bulkwalkall: true # Sample of \"flow only\" device flow_only__10.10.0.210: device_name: flow_only device_ip: 10.10.0.210 user_tags: owning_team: net_eng flow_only: true # Sample of \"ping only\" device ping_only__10.10.0.220: device_name: ping_only device_ip: 10.10.0.220 user_tags: owning_team: load_balancing ping_only: true trap: listen: 127.0.0.1:1162 community: public version: \"\" transport: \"\" v3_config: null discovery: cidrs: - 10.0.0.0/24 - 10.0.0.202/32 debug: false ports: - 161 - 1161 default_communities: - public - public123 - Publ!cABC use_snmp_v1: false default_v3: null add_mibs: true threads: 4 add_devices: true replace_devices: true no_dedupe_engine_id: false global: poll_time_sec: 60 drop_if_outside_poll: false mib_profile_dir: /etc/ktranslate/profiles mibs_db: /etc/ktranslate/mibs.db mibs_enabled: - BGP4-MIB - CISCO-MEMORY-POOL-MIB - CISCO-PROCESS-MIB - IF-MIB - IP-MIB - OSPF-MIB - PowerNet-MIB_UPS - TCP-MIB - UDP-MIB timeout_ms: 3000 retries: 0 global_v3: null response_time: false user_tags: environment: production match_attributes: if_Description: \".*WAN.*\" Copy Devices section Key name Required Description device_name ✓ Name of the device. This is the unique identifier for the device in New Relic One. device_ip ✓ Target IP of the device. snmp_comm ✓ (Required for SNMPv1/2c) SNMPv1/2c community string to use. use_snmp_v1 ✓ (Required for SNMPv1) Indicates whether to use SNMPv1. By default, it's set to false. snmp_v3 ✓ (Required for SNMPv3) SNMP v3 config debug Indicates whether to enable debug level logging during SNMP polling. By default, it's set to false. port Port to send SNMP queries to. By default, it's set to port 161. oid ✓ (Required for SNMP polling) The discovered systemObjectID | sysObjectID | sysOID for the device. This is used to match the device to a known SNMP profile and set the provider attribute. If no match is found, this sets the provider as a kentik-default device. description The discovered sysDescr of the device. This field is informational. last_checked Timestamp when this device was last discovered by the ktranslate docker image. This field is informational. mib_profile ✓ (Required for SNMP polling) SNMP Profile file that was associated with this device during the discovery run based on its sysOID. If this starts with a bang (!) token, it will override the automatic matching from the sysOID and use a manual override. Ex: \"!cisco-asa.yml\" (quotes are required). provider ✓ (Required for New Relic One) Value used during entity synthesis for New Relic One. This is automatically created based on the matched mib_profile. poll_time_sec Indicates the SNMP polling frequency in seconds. This setting is used to override the global.poll_time_sec attribute. retries Indicates the number of attempts to retry polling SNMP OIDs. This setting is used to override the global.retries attribute. timeout_ms Indicates the SNMP polling timeout in milliseconds. This setting is used to override the global.timeout_ms attribute. user_tags key:value pair attributes to give more context to the device. Tags at this level will be appended to any tags applied in the global.user_tags attribute. discovered_mibs List of MIBs pulled from matched mib_profile that this device can respond to. This field is informational. engine_id The unique engine ID discovered for this device's SNMP agent. Generally found during SNMP v3 discovery. This field is informational. match_attributes attribute:regex pairs to add metrics to allowlist. Pairs at this level will be appended to any pairs applied in the global.match_attributes attribute. Uses the RE2 syntax and has a default OR operator. Prefix key with ! to force to AND operators. monitor_admin_shut Indicates whether to monitor interfaces in Administratively Shutdown status. By default, it's set to false. no_use_bulkwalkall Disables the SNMP GETBULK request action when true. By default, it's set to false. ping_only Disables all SNMP polling and enables response time polling when true. This setting is used to override the global.response_time attribute. By default, it's set to false. flow_only Disables all SNMP polling when true. By default, it's set to false. Trap section Key name Required Description listen ✓ Listening IP port for receiving SNMP traps. By default it's set to 127.0.0.1:1162, using the SNMP Trap default of 162 requires running Docker as root. community SNMPv1/v2c community string for receiving SNMP traps. version SNMP version to use. Options are v1, v2c, and v3. By default, it's set to v2c. transport SNMP transport protocol to use. Options are TCP and UDP. By default, it's set to UDP v3_config SNMP v3 config to use. Only used if version: v3. Discovery section Key name Required Description cidrs ✓ Array of target IP ranges in CIDR notation. debug Indicates whether to enable debug level logging during discovery. By default, it's set to false ports ✓ Array of target ports to scan during SNMP polling. default_communities ✓ (Required for SNMPv1/2c) Array of SNMPv1/v2c community strings to scan during SNMP polling. This array is evaluated in order and discovery accepts the first passing community. use_snmp_v1 ✓ (Required for SNMPv1) Indicates whether to use SNMPv1 during discovery. By default, it's set to false default_v3 ✓ (Required for SNMPv3) SNMPv3 configuration to scan during SNMP polling. add_devices ✓ Indicates whether to add discovered devices to the devices section of the snmp-base.yaml file. By default, it's set to true. add_mibs ✓ Indicates whether to add discovered MIBs to the global.mibs_enabled section of the snmp-base.yaml file. By default, it's set to true. threads ✓ Integer limit of threads to use during discovery. It should be less than the number of cores available to the container. By default it's set to 4. replace_devices ✓ Indicates whether to replace discovered devices if they already exist in the devices section of the snmp-base.yaml file. By default, it's set to false. no_dedup_engine_id When set to true, disables deduplication of discovered devices if it appears that they are the same device, based on their reported SNMP engine ID. By default, it's set to false Global section Key name Required Description poll_time_sec ✓ Time in seconds to poll devices. This can be overridden per device using the devices.<deviceName>.poll_time_sec attribute. By default, it's set to 60. drop_if_outside_poll Indicates whether to drop all values from this cycle if polling takes longer than the value set in poll_time_sec. By default, it's set to false mib_profile_dir Directory to find curated MIB profiles. These are pulled into the ktranslate image automatically from Kentik's snmp-profiles repository and can be overridden at Docker runtime by creating a volume mount of your own local directory of profiles. mibs_db mibs_enabled ✓ Array of all active MIBs the ktranslate docker image will poll. This list is automatically generated during discovery if the discovery_add_mibs attribute is true. MIBs not listed here will not be polled on any device in the configuration file. You can specify a SNMP table directly in a MIB file using MIB-NAME.tableName syntax. Ex: HOST-RESOURCES-MIB.hrProcessorTable. timeout_ms ✓ Time in milliseconds SNMP queries timeout. This can be overridden per device using the devices.<deviceName>.timeout_ms attribute. By default, it's set to 5000 retries ✓ Number of attempts to retry failed SNMP polls. This can be overridden per device using the devices.<deviceName>.retries attribute. By default, it's set to 0 user_tags key:value pair attributes to give more context to the device. Tags at this level will be applied to all devices in the configuration file. match_attributes attribute:regex pairs to add metrics to allowlist. Pairs at this level will matched against all devices in the configuration file. Uses the RE2 syntax and has a default OR operator. Prefix key with ! to force to AND operators. response_time Indicates whether response time polling is enabled for all devices in the configuration file. By default, it's set to false. Optional SNMPv3 configuration Key name Required Description user_name ✓ User name for SNMPv3 authentication authentication_protocol ✓ SNMPv3 authentication protocol. The possible values are NoAuth, MD5, or SHA authentication_passphrase SNMPv3 authentication passphrase privacy_protocol ✓ SNMPv3 privacy protocol. The possible values are AuthNoPriv, DES, AES, AES192, AES256, AES192C, or AES256C privacy_passphrase SNMPv3 privacy passphrase context_engine_id SNMPv3 context engine ID context_name SNMPv3 context name Tip You can use AWS Secrets Manager natively in your SNMP v3 config using the aws.sm.$SECRET_NAME syntax, replacing $SECRET_NAME as necessary to have ktranslate pull in your credentials during Docker runtime. Optional external config files To support a wide variety of configuration and automation needs, you can use external files that you volume mount into your Docker container to decouple certain elements of the standard configuration file. The syntax for these files is \"@fileName.extension\", including the double quotes. Discovery CIDRs Example: discovery: cidrs: \"@cidrs.yaml\" Copy The CIDRs file should use a YAML list syntax like this: - 10.10.0.0/24 - 10.20.0.0/24 - 192.168.0.21/32 Copy Devices Example: devices: - \"@neteng-devices.yaml\" - \"@dc-ops.yaml\" Copy The device files should use the same syntax as the standard devices section of the main config file, omitting the optional fields that are generated during discovery: devices: # Sample of SNMP v2c device ups_snmpv2c__10.10.0.201: device_name: ups_snmpv2c device_ip: 10.10.0.201 snmp_comm: public oid: .1.3.6.1.4.1.318.1.3.27 mib_profile: apc_ups.yml provider: kentik-ups poll_time_sec: 300 retries: 1 timeout_ms: 5000 user_tags: owning_team: dc_ops Copy The match_attributes attribute To support filtering of data that does not create value for your observability needs, you can set the global.match_attributes.{} and/or devices.<deviceName>.match_attributes.{} attribute map. This will provide filtering at the ktranslate level, before shipping data to New Relic, giving you granular control over monitoring of things like interfaces. The default behavior of this map is an OR condition, but you can override this and force an AND operator by prefixing your key name with !. This is also useful to return only matched items and omit all null and \"\" (empty) results. Default 'OR' with null and empty values Match when if_Alias begins with Uplink OR when if_interface_name begins with Gig, keep all null and \"\" values: devices: deviceName: ... match_attributes: if_Alias: \"^Uplink.*\" if_interface_name: \"^Gig.*\" Copy 'AND', omit null and empty values Match when if_Alias begins with Uplink AND when if_interface_name begins with Gig, drop all null and \"\" values: devices: deviceName: ... match_attributes: if_Alias: \"^Uplink.*\" \"!if_interface_name\": \"^Gig.*\" Copy Single match, omit null and empty values Match when if_Alias begins with Uplink, drop all null and \"\" values: devices: deviceName: ... match_attributes: \"!if_Alias\": \"^Uplink.*\" Copy The flow_only attribute To support monitoring of devices where performance statistics are nor accessible, available, or desired, you can set the devices.<deviceName>.flow_only attribute to true. This will generate a Flow Device entity in New Relic One which will only have telemetry in the KFlow event namespace. Alternatively, collecting flow telemetry from a device that is in your configuration file as an SNMP device will add decoration of the KFlow data to the pre-existing entity, such as a Router or Firewall. In New Relic One, you can see the results of this polling by investigating the following events: FROM KFlow SELECT count(*) FACET device_name WHERE instrumentation.name = 'netflow-events' TIMESERIES Copy The response_time and ping_only attributes To support monitoring of devices where performance statistics are not accessible or available, or in simple cases where basic round-trip time (RTT) monitoring is required, you can either set the global.response_time or devices.<deviceName>.ping_only attributes to true. This feature uses the go-ping package to send unprivileged UDP packets to devices in order to collect the average, min, and max RTT for the endpoint based on sending 1 packet/sec from ktranslate to the device IP address. Setting the global.response_time attribute to true will add RTT monitoring on top of existing SNMP polling. devices.<deviceName>.ping_only: true will allow you to monitor devices with only the UDP packets for RTT and no SNMP polling. In New Relic One, you can see the results of this polling by investigating the following metrics: FROM Metric SELECT average(kentik.ping.AvgRttMs) AS 'Average', max(kentik.ping.MaxRttMs) AS 'Max', min(kentik.ping.MinRttMs) AS 'Min' FACET device_name TIMESERIES Copy Tip You can use the ping_only attribute in replacement of the flow_only attribute if you would like to collect RTT metrics from a flow device. If both ping_only and flow_only are true, the device will be treated as a flow_only device. Flow data application mapping By default, flow telemetry is mapped to known applications based on evaluation of the layer 4 port in use on a specific flow conversation. If needed, you can override the default mapping by providing a YAML file during Docker runtime to the -application_map flag. This will allow you to specify application names based on ports you identify. Example syntax: applications: - ports: [9092, 9093] name: kafka - ports: [80, 8080] name: http - ports: [443, 8443] name: https Copy Flow data input filtering By default, flow data containers will collect and process every flow packet they receive. If needed, you can add an inclusion filter to the -nf.source flag that will ignore all traffic not matching the filter you provide. Syntax: --filters $TYPE,$FIELD,$FUNCTION,$MATCH Argument Name Required Description $TYPE ✓ The type of filter to apply. Possible values are string, int, and addr. $FIELD ✓ The name of the field to evaluate the match pattern against. $FUNCTION ✓ The type of function to use during evaluation. Possible values are Equal: ==, NotEqual: !=, LessThan: <, GreaterThan: >, Contains: % $MATCH ✓ The value to be used as a match pattern. Example Filters Only collect flow data from source addresses in the 10.0.0.0/24 CIDR range -nf.source sflow --filters addr,src_addr,%,10.10.0.0/24 Copy Only collect flow data where the destination port is not equal to 8531 -nf.source netflow5 --filters int,l4_dst_port,!=,8531 Copy You can also add multiple filters together with an inherited AND operator Only collect flow data from source addresses in the 10.0.0.0/24 CIDR range AND where the destination port is not equal to 8531 --filters addr,src_addr,%,10.0.0.0/24 --filters int,l4_dst_port,!=,8531 Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 65.382904,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Advanced configuration <em>for</em> Network Performance Monitoring",
        "sections": "Advanced configuration <em>for</em> Network Performance Monitoring",
        "body": " poll_time_sec: 300 retries: 1 timeout_ms: 5000 user_tags: owning_<em>team</em>: dc_ops discovered_mibs: - PowerNet-MIB_UPS - TCP-MIB - UDP-MIB # Sample of SNMP v3 device router_snmpv3__10.10.0.202: device_name: router_snmpv3 device_ip: 10.10.0.202 snmp_v3: user_name: userNamev3 authentication_protocol: MD5"
      },
      "id": "61b9389664441f8fc3d7182e"
    }
  ],
  "/docs/agile-handbook/sprint-mechanics/liaisonships": [
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/agile-handbook/key-concepts/agile-roles/",
      "sections": [
        "Agile roles",
        "Team member",
        "Scrum master",
        "Tech Docs managers (and product owner)",
        "Key stakeholders",
        "Tip"
      ],
      "published_at": "2021-12-19T15:13:39Z",
      "title": "Agile roles",
      "updated_at": "2021-10-31T02:29:56Z",
      "type": "docs",
      "external_id": "fcfd8b2a56ccc61ba57d207f49876b4788a1ca51",
      "document_type": "page",
      "popularity": 1,
      "body": "Like most agile teams, we divide up the roles on our team into team members, scrum masters, product owners, and managers. We treat some of those roles differently than a traditional engineer-centric scrum team: We combine the role of team manager and product owner into one person. Each writer is responsible for \"liaisonships\" where they track work across a particular product or feature and bring in appropriate stories. We don't expect scrum masters to clear blockers (that's a manager's job), and scrum mastering is not a full-time job. We divide the team further into two squads, each with its own team members, scrum master, and product owners. Team member Team members are responsible for: Doing writing work. Improving the team's processes and how we work together. Writing stories for sprints in accordance with their work (via laisionships, SME conversations, hero work, etc.). The person who nominates a story will also present it during sprint planning. See Ticket best practices for tips on writing a good ticket. Most team members will be assigned one or more Liaisonships (project assignments). Liaisons are responsible for nominating stories for sprints from other teams. Scrum master Each squad has a scrum master. The scrum master is not responsible for unblocking stories or communicating with stakeholders on behalf of the team (this work belongs to individual writers and their manager). We believe this lets the business have a single point of accountability (managers) for decisions, and ensures scrum masters have hands-on experience of what it's like to be a writer. Instead, for us the scrum master is a custodian of the sprint process and the MC for sprint meetings. In backlog grooming: The scrum master handles the mechanics of talking through each ticket and facilitating conversations about story quality. In sprint planning: The scrum master leads conversation, tracks discussion time, adds point values to stories, and organizes/ranks sprint candidates in real time. They don't present the stories, though—stories should be introduced by the person who nominated the story for the sprint. In retros: The scrum master facilitates the discussion, captures action items, and takes notes. Tech Docs managers (and product owner) Each squad has a manager---or perhaps you could say each manager has a squad. Either way, the manager's role is to prioritize the right work for the business, maintain a healthy workload, and help escalate when a writer needs help. The managers are responsible for understanding how our entire body of work serves the organization, and making informed decisions about our velocity and workload. The manager engages with other teams to know which features may be coming into the pipeline and has a general understanding of work that may be in future sprints. This lets them make final priority decisions for the team and be accountable to the business for those tradeoffs. The manager is also responsible for assigning liaisonships and ensuring we're covering the portfolio. They'll also work with other teams to unblock writers when needed. Key stakeholders Our key internal stakeholders include PMs, engineers, designers, and executives. Writers work with the stakeholders to know when new work is coming, and to communicate documentation needs/timelines. We should ensure our stakeholders know that we work in two week sprints, so that they can give us adequate lead time and get their documentation needs met. ← Key agile principles Meetings and ceremonies → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 83.17692,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": " is responsible for &quot;<em>liaisonships</em>&quot; where they track work across a particular product or feature and bring in appropriate stories. We don&#x27;t expect scrum masters to clear blockers (that&#x27;s a manager&#x27;s job), and scrum mastering is not a full-time job. We divide the team further into two squads, each with its"
      },
      "id": "616c0d97e7b9d2b2f047837d"
    },
    {
      "image": "https://docs.newrelic.com/static/e74b3e31916f093c77d41e8beef9ecc9/c1b63/lines_of_communication.png",
      "url": "https://docs.newrelic.com/docs/agile-handbook/key-concepts/agile-sprints-profusion-of-terms/",
      "sections": [
        "Agile vs sprints (vs Jira vs GitHub): A profusion of terms",
        "Agile",
        "Sprint (or scrum)",
        "Jira and GitHub issues",
        "Teams and squads",
        "Tip"
      ],
      "published_at": "2021-12-19T15:09:06Z",
      "title": "Agile vs sprints (vs Jira vs GitHub): A profusion of terms",
      "updated_at": "2021-10-31T02:29:57Z",
      "type": "docs",
      "external_id": "7f3f97222daf52cf14afcf8fc432b96b3cebd4b1",
      "document_type": "page",
      "popularity": 1,
      "body": "Our team uses an agile Sprint workflow in Jira and GitHub to manage our work. We've further divided our team into squads to simplify planning and improve accountability. All those words are pretty inconsistent in their usage, so let's break them down further. Agile People use agile to mean everything from a specific system of work (which we call sprints), to just \"moving fast, preferably in a way that lets me bend things to my whims.\" Luckily, we don't need to define it from scratch. Wikipedia does an admirable job defining it: Agile software development is an approach to software development under which requirements and solutions evolve through the collaborative effort of self-organizing and cross-functional teams and their customer(s)/end user(s). It advocates adaptive planning, evolutionary development, empirical knowledge, and continual improvement, and it encourages rapid and flexible response to change. For our team, that means our process is optimized to ship early and often. This lets us respond swiftly to changes in the product roadmap. More importantly, it ensures we validate our solutions with stakeholders, and that we're not letting valuable work sit around and get moldy when it could be out in the world making our users' lives better. Sprint (or scrum) This is the particular flavor of agile we follow. The sprint system (often referred to as scrum) is one major approach to Agile, along with other Agile systems such as Kanban. Sprint systems are often accompanied by a lot of jargon and best practices, but for our team the most essential elements are: Working in strict timeboxes (two weeks in our case) Planning that sprint in advance, and not changing the scope of the sprint (much) once it starts Expecting all team members to contribute to making the sprint a success The video Agile Product Ownership in a Nutshell (18 minutes) is an excellent resource for learning about sprint methodology. The Kindle book Scrum: a Breathtakingly Brief and Agile Introduction is also a great read that you can get through in a short afternoon. For more on the \"why\" of Sprint as our chosen methodology, see Key agile principles. And for more on the \"how,\" see Sprint workflow. Jira and GitHub issues Jira and GitHub issues are the tools we use to manage our Agile workflow. If you remember one thing about them, it should be this: using Jira or GitHub issues is not the same as having an agile workflow. They're powerful tools for tracking work and managing a backlog, but the most important part of project management is the structure we impose on that tool. Jira is for sprint work. Sprints are where roadmap docs get written, monthly commits get delivered, and deeper research percolates. We have a backlog, board, and future sprint list in Jira that help us track what people want, what's coming up, and what we're working on now. For more on the mechanics of how we use Jira, see Sprint workflow and Jira boards and Ticket best practices. We use GitHub projects for hero work, customer-reported issues, and managing the flow of PRs and edits. The Docs PRs and Issues board contains everything we're actively working on in GitHub. We'll often connect work in GitHub back to Jira by putting a Jira issue key in the PR or issue title (DOC-1234, for example). For more on the mechanics of how we use GitHub, see Managing the GitHub boards. Teams and squads Our team is the Tech Docs team. We're collectively responsible for docs.newrelic.com and sundry writing content. Our team is further divided into two agile squads (The Odd Squad and The Amp Squad), one squad for each manager. The primary function of squads is to simplify sprint planning, backlog grooming, and liaisonships. Less people means shorter meetings. It also means better information sharing: The more people you have in a group, the more lines of communication are needed (see illustration) to maintain a shared understanding. Small squads can collaborate more easily than a large team, because not everyone needs to keep in mind everything that goes on everywhere. Each squad is responsible for its own grooming and sprint planning, but the managers and scrum masters coordinate grooming to ensure we're meeting our overall goals as a team. Key agile principles → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 46.943638,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": " for each manager. The primary function of squads is to simplify sprint planning, backlog grooming, and <em>liaisonships</em>. Less people means shorter meetings. It also means better information sharing: The more people you have in a group, the more lines of communication are needed (see illustration) to maintain"
      },
      "id": "616c0d96196a677e623c7bd2"
    },
    {
      "image": "https://docs.newrelic.com/static/aa5797a9c6aaadf52a7bac18b3ac8e83/c1b63/dealing_with_interrupts.png",
      "url": "https://docs.newrelic.com/docs/agile-handbook/sprint-mechanics/sprint-workflow-and-jira-boards/",
      "sections": [
        "Sprint workflow",
        "Planned work",
        "Unplanned work (surprises!)",
        "Jira boards: Backlog and future sprints",
        "Jira boards: Current sprint",
        "Proposed",
        "In Progress",
        "Needs Peer Editor",
        "In Peer Edit",
        "Peer Edit Done",
        "Blocked",
        "Done",
        "Incomplete (\"carry-over\") tickets",
        "Tip"
      ],
      "published_at": "2021-12-19T15:27:56Z",
      "title": "Sprint workflow",
      "updated_at": "2021-11-06T12:49:01Z",
      "type": "docs",
      "external_id": "e18dcb23b9a43a083a2c4496d6fb9a20b9efc496",
      "document_type": "page",
      "popularity": 1,
      "body": "All of our sprint work is tracked in Jira. The workflow depends on what type of work we're dealing with: Planned or unplanned (\"surprise!\") work. Planned work Planned work includes all work that is currently in our backlog or has been added to the current sprint as a result of a Sprint Planning session. This could include writing or updating documentation, research, meeting with SMEs, information architecture, incorporating peer edits, SME review, and so on. Unplanned work (surprises!) Usually, we get notified of major requests far enough in advance that we can include them in liaison project plans, backlog grooming, and sprint planning. Occasionally, something bigger surprises us that needs emergency support. Follow this process with new docs asks to assess the scope of work and ensure we address valid docs needs within a reasonable amount of time. Our goal is to treat the sprint as sacred and insulate against \"surprise\" work that is not absolutely crucial. But we also want to ensure we're providing good internal customer service, and not getting hung up on process niceties for things that are small. Jira boards: Backlog and future sprints This is where the vast majority of tickets spend their time. Most tickets (even for active projects) spend at least a little time here before moving into a sprint to be actively worked. Being in the backlog doesn't mean something isn't important---just that we haven't committed to it yet.  You can also add tickets straight to a future sprint. This is where tickets tentatively assigned to a future sprint will be found. Tickets can be assigned here to be held for backlog grooming and sprint planning.  Jira boards: Current sprint Proposed This step is for work that has been assigned to the current sprint during Sprint Planning and is available to be picked up by a tech writer. When you're ready to take on a new ticket, try to work the queue from the top-down and avoid cherry picking. It's also better to pick up Needs Peer Edit tickets before committing to a new ticket. Something that needs a peer edit is close to done, and helping things across the finish line helps get value into users hands, and frees us up to think about new problems. In Progress This step is for all of the work to be done by the assignee: Research, meeting with SMEs, information architecture, writing, incorporating peer edits, SME review, and so on. Tickets are moved to this step once work is started by the TW, and remain here until the work is either complete, ready for peer review, or it becomes blocked. If additional large edits are needed after the peer review, the ticket can be moved back to In Progress for those edits. Needs Peer Editor Work that is ready for a peer edit. Once a peer editor picks it up, they move it into In Peer Edit. In Peer Edit This step is for a peer editor to review docs before they go live. Follow the Peer editor workflow, then move the ticket into Peer Edit Done.  Peer Edit Done This step is a holding state once peer editing is complete. After completing their peer edit and delivering their feedback, the peer editor moves the ticket into Peer Edit Done. From there, the assignee on the ticket (not the peer editor) moves the ticket into the appropriate column (In Progress, Blocked, or Closed). Minor edits can be completed from this column but for major doc rework, the ticket should be moved back into the In Progress column. Blocked This step is for tickets that cannot be moved forward by the team. This could be because we're waiting for a response from a SME, or for a feature to deploy, or for final signoff. The team keeps an eye on this column for tickets that may need escalation. Putting something in Blocked rather than In Progress lets us see the status of every ticket at a glance. This column can also be used for extended time out of the office for the assigned writer, if it's work that can be safely held. (If the work cannot be held while you're out, find another writer to step in and take over.) Once you're un-blocked, move the ticket to the appropriate column. If the ticket remains blocked at the end of the current sprint, it will need to be re-reviewed during backlog grooming to determine if the ticket will carry-over into the upcoming sprint, or return to the backlog until a future sprint. Done This step is for work that is 100% finished. Work gets cleared out this column before we start a new sprint. Incomplete (\"carry-over\") tickets Ticket don't carry over automatically between sprints. Instead, any ticket that gets carried over is treated as a \"new\" ticket in the next sprint planning. Before sprint planning, review any open tickets in the board that are assigned to you and figure out what to do with them. For each open ticket assigned to you (or \"carry over\"), decide if you should: Recommended: Clone the ticket and close the old one. This is the best option for partially completed work because it makes metrics easier. If you do: Clone the ticket. Note why we closed the ticket. Add an estimate of points completed in the Points Completed field. Create a follow-up ticket if necessary. Move the ticket to the next sprint. If you do: Review the ticket's action items and description to make sure they're still current. Clear out the ticket points. Move the ticket back to the backlog. If you do: Update the action items and description to make sure they're still current. Note why we moved to the backlog rather than carry over. ← Planning poker Liaisonships → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 38.08146,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": " current. Note why we moved to the backlog rather than carry over. ← Planning poker <em>Liaisonships</em> → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue."
      },
      "id": "616c0dc0196a67e6583c8164"
    }
  ],
  "/docs/agile-handbook/sprint-mechanics/meetings-and-ceremonies": [
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/agile-handbook/key-concepts/agile-roles/",
      "sections": [
        "Agile roles",
        "Team member",
        "Scrum master",
        "Tech Docs managers (and product owner)",
        "Key stakeholders",
        "Tip"
      ],
      "published_at": "2021-12-19T15:13:39Z",
      "title": "Agile roles",
      "updated_at": "2021-10-31T02:29:56Z",
      "type": "docs",
      "external_id": "fcfd8b2a56ccc61ba57d207f49876b4788a1ca51",
      "document_type": "page",
      "popularity": 1,
      "body": "Like most agile teams, we divide up the roles on our team into team members, scrum masters, product owners, and managers. We treat some of those roles differently than a traditional engineer-centric scrum team: We combine the role of team manager and product owner into one person. Each writer is responsible for \"liaisonships\" where they track work across a particular product or feature and bring in appropriate stories. We don't expect scrum masters to clear blockers (that's a manager's job), and scrum mastering is not a full-time job. We divide the team further into two squads, each with its own team members, scrum master, and product owners. Team member Team members are responsible for: Doing writing work. Improving the team's processes and how we work together. Writing stories for sprints in accordance with their work (via laisionships, SME conversations, hero work, etc.). The person who nominates a story will also present it during sprint planning. See Ticket best practices for tips on writing a good ticket. Most team members will be assigned one or more Liaisonships (project assignments). Liaisons are responsible for nominating stories for sprints from other teams. Scrum master Each squad has a scrum master. The scrum master is not responsible for unblocking stories or communicating with stakeholders on behalf of the team (this work belongs to individual writers and their manager). We believe this lets the business have a single point of accountability (managers) for decisions, and ensures scrum masters have hands-on experience of what it's like to be a writer. Instead, for us the scrum master is a custodian of the sprint process and the MC for sprint meetings. In backlog grooming: The scrum master handles the mechanics of talking through each ticket and facilitating conversations about story quality. In sprint planning: The scrum master leads conversation, tracks discussion time, adds point values to stories, and organizes/ranks sprint candidates in real time. They don't present the stories, though—stories should be introduced by the person who nominated the story for the sprint. In retros: The scrum master facilitates the discussion, captures action items, and takes notes. Tech Docs managers (and product owner) Each squad has a manager---or perhaps you could say each manager has a squad. Either way, the manager's role is to prioritize the right work for the business, maintain a healthy workload, and help escalate when a writer needs help. The managers are responsible for understanding how our entire body of work serves the organization, and making informed decisions about our velocity and workload. The manager engages with other teams to know which features may be coming into the pipeline and has a general understanding of work that may be in future sprints. This lets them make final priority decisions for the team and be accountable to the business for those tradeoffs. The manager is also responsible for assigning liaisonships and ensuring we're covering the portfolio. They'll also work with other teams to unblock writers when needed. Key stakeholders Our key internal stakeholders include PMs, engineers, designers, and executives. Writers work with the stakeholders to know when new work is coming, and to communicate documentation needs/timelines. We should ensure our stakeholders know that we work in two week sprints, so that they can give us adequate lead time and get their documentation needs met. ← Key agile principles Meetings and ceremonies → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1011.5646,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Tech Docs managers (<em>and</em> product owner)",
        "body": " needs&#x2F;timelines. We should ensure our stakeholders know that we work in two week sprints, so that they can give us adequate lead time and get their documentation needs met. ← Key agile principles <em>Meetings</em> and <em>ceremonies</em> → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue."
      },
      "id": "616c0d97e7b9d2b2f047837d"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/agile-handbook/sprint-mechanics/planning-poker/",
      "sections": [
        "Planning poker and points budgets",
        "Poker card definitions",
        "How we calculate the points budget",
        "Tip"
      ],
      "published_at": "2021-12-19T15:27:35Z",
      "title": "Planning poker and points budgets",
      "updated_at": "2021-10-31T02:31:45Z",
      "type": "docs",
      "external_id": "d002b0635204648ebe36dbdc6c242e80fa971aa3",
      "document_type": "page",
      "popularity": 1,
      "body": "Planning poker is a consensus-based estimating and planning technique. To start a planning poker session, the story reporter or liaison presents a story to the team. Then, the team all votes (at the same time, to prevent groupthink) on how difficult they think the story is. Most agile teams that estimate with planning poker use cards that follow a Fibonacci sequence. We've found over the years that those large jumps weren't very helpful to us in estimating, and that we estimated better with smaller numbers. So we use fairly small, granular numbers that roughly break down to 1 point ≈ 1 day. Note that even though each of the possible scores have approximate time values associated with them, we vote in points, not time. Scoring with points prevents getting into weedsy debates about exactly how long something will take, and instead focuses us on the requirements and difficulty of the story. Poker card definitions These how we define our poker cards: Card Value ¼ An hour or two. Anything smaller than this isn't worth bringing into a sprint or even ticketing—just do it right now. ½ About half of an \"ideal\" day. We define an \"ideal\" day as one without meetings or interruptions. Using \"ideal\" days makes it easier to estimate a story without litigating the details of how we spend our time. 1 About one \"ideal\" day. This is one of our most common story sizes. 1½ About one and a half \"ideal\" days. This is our other most common story size. 2 About two \"ideal\" days. 3 About three \"ideal\" days. We jump from 2 straight to 3 to avoid unrealistic granularity in our estimates. 5 About a week of work. 7 About a full sprint (two weeks) of work. We very rarely use this card—a ticket that takes a full sprint on its own is almost always too large and should be chunked into smaller, incremental tickets. Break! \"I need a break.\" When someone plays a break card, we finish estimating the current story then immediately take a five minute break. Defer No strong opinion. Don't play this card on the first round of poker! Seeing wildly different estimates is a sign we don't have a shared understanding of the work. Playing an early Defer card can mask that uncertainty. How we calculate the points budget In order to ensure we're not filling up our sprint with more \"ideal days\" of time than we realistically have available, we calculate the points budget based on the actual output of the team. Over time, we've found writers average 0.6 (technically, 0.57) points per day. Tip This value is totally unique to each team! A high number doesn't indicate a more productive team, and a low number isn't a problem. Story points are only meaningful within a team. If you're starting a new team or new process, you'll need to zero in on the ideal number of points per writer. An easy way to figure out the right budget is to have the team vote in retro on whether to increase, decrease, or keep the same budget next sprint. You'll overshoot or undershoot a few times, but after a couple sprints you'll get a good sense of what constistutes a sustainable pace. Once you know your baseline, calculating the budget is straightforward: Take the number of writers, and multiply by the number of days in the sprint. To make the math easy, let's say 5 writers and a 10 business day (2 calendar week) sprint. Or, 50 person-days total. Subtract out time off days, plus 1 day for each day of hero duty. Let's say we have 1 writer on vacation in Maui, and 1 day of hero time per day. 50 person days, minus 10 days of vacation and 10 days of hero time, gives us 30 person-days. Multiply the number of days times the average velocity per writer, per day. Our average velocity is 0.6 points/writer/day. 30 person days multiplied by 0.6 would give us an 18 point budget for the sprint. Because we work in two squads, we calculate a separate points budget for each squad ahead of sprint planning. ← Meetings and ceremonies Sprint workflow and Jira boards → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 872.2223,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Planning poker <em>and</em> points budgets",
        "sections": "Planning poker <em>and</em> points budgets",
        "body": " the average velocity per writer, per day. Our average velocity is 0.6 points&#x2F;writer&#x2F;day. 30 person days multiplied by 0.6 would give us an 18 point budget for the sprint. Because we work in two squads, we calculate a separate points budget for each squad ahead of sprint planning. ← <em>Meetings</em> and <em>ceremonies</em> Sprint workflow and Jira boards → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue."
      },
      "id": "616c0e2d28ccbc6e5e00337f"
    },
    {
      "image": "https://docs.newrelic.com/static/e74b3e31916f093c77d41e8beef9ecc9/c1b63/lines_of_communication.png",
      "url": "https://docs.newrelic.com/docs/agile-handbook/key-concepts/agile-sprints-profusion-of-terms/",
      "sections": [
        "Agile vs sprints (vs Jira vs GitHub): A profusion of terms",
        "Agile",
        "Sprint (or scrum)",
        "Jira and GitHub issues",
        "Teams and squads",
        "Tip"
      ],
      "published_at": "2021-12-19T15:09:06Z",
      "title": "Agile vs sprints (vs Jira vs GitHub): A profusion of terms",
      "updated_at": "2021-10-31T02:29:57Z",
      "type": "docs",
      "external_id": "7f3f97222daf52cf14afcf8fc432b96b3cebd4b1",
      "document_type": "page",
      "popularity": 1,
      "body": "Our team uses an agile Sprint workflow in Jira and GitHub to manage our work. We've further divided our team into squads to simplify planning and improve accountability. All those words are pretty inconsistent in their usage, so let's break them down further. Agile People use agile to mean everything from a specific system of work (which we call sprints), to just \"moving fast, preferably in a way that lets me bend things to my whims.\" Luckily, we don't need to define it from scratch. Wikipedia does an admirable job defining it: Agile software development is an approach to software development under which requirements and solutions evolve through the collaborative effort of self-organizing and cross-functional teams and their customer(s)/end user(s). It advocates adaptive planning, evolutionary development, empirical knowledge, and continual improvement, and it encourages rapid and flexible response to change. For our team, that means our process is optimized to ship early and often. This lets us respond swiftly to changes in the product roadmap. More importantly, it ensures we validate our solutions with stakeholders, and that we're not letting valuable work sit around and get moldy when it could be out in the world making our users' lives better. Sprint (or scrum) This is the particular flavor of agile we follow. The sprint system (often referred to as scrum) is one major approach to Agile, along with other Agile systems such as Kanban. Sprint systems are often accompanied by a lot of jargon and best practices, but for our team the most essential elements are: Working in strict timeboxes (two weeks in our case) Planning that sprint in advance, and not changing the scope of the sprint (much) once it starts Expecting all team members to contribute to making the sprint a success The video Agile Product Ownership in a Nutshell (18 minutes) is an excellent resource for learning about sprint methodology. The Kindle book Scrum: a Breathtakingly Brief and Agile Introduction is also a great read that you can get through in a short afternoon. For more on the \"why\" of Sprint as our chosen methodology, see Key agile principles. And for more on the \"how,\" see Sprint workflow. Jira and GitHub issues Jira and GitHub issues are the tools we use to manage our Agile workflow. If you remember one thing about them, it should be this: using Jira or GitHub issues is not the same as having an agile workflow. They're powerful tools for tracking work and managing a backlog, but the most important part of project management is the structure we impose on that tool. Jira is for sprint work. Sprints are where roadmap docs get written, monthly commits get delivered, and deeper research percolates. We have a backlog, board, and future sprint list in Jira that help us track what people want, what's coming up, and what we're working on now. For more on the mechanics of how we use Jira, see Sprint workflow and Jira boards and Ticket best practices. We use GitHub projects for hero work, customer-reported issues, and managing the flow of PRs and edits. The Docs PRs and Issues board contains everything we're actively working on in GitHub. We'll often connect work in GitHub back to Jira by putting a Jira issue key in the PR or issue title (DOC-1234, for example). For more on the mechanics of how we use GitHub, see Managing the GitHub boards. Teams and squads Our team is the Tech Docs team. We're collectively responsible for docs.newrelic.com and sundry writing content. Our team is further divided into two agile squads (The Odd Squad and The Amp Squad), one squad for each manager. The primary function of squads is to simplify sprint planning, backlog grooming, and liaisonships. Less people means shorter meetings. It also means better information sharing: The more people you have in a group, the more lines of communication are needed (see illustration) to maintain a shared understanding. Small squads can collaborate more easily than a large team, because not everyone needs to keep in mind everything that goes on everywhere. Each squad is responsible for its own grooming and sprint planning, but the managers and scrum masters coordinate grooming to ensure we're meeting our overall goals as a team. Key agile principles → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 36.31375,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Jira <em>and</em> GitHub issues",
        "body": " for each manager. The primary function of squads is to simplify sprint planning, backlog grooming, and liaisonships. Less people means shorter <em>meetings</em>. It also means better information sharing: The more people you have in a group, the more lines of communication are needed (see illustration) to maintain"
      },
      "id": "616c0d96196a677e623c7bd2"
    }
  ],
  "/docs/agile-handbook/sprint-mechanics/planning-poker": [
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/agile-handbook/sprint-mechanics/meetings-and-ceremonies/",
      "sections": [
        "Meetings and ceremonies",
        "Tip",
        "Sprint planning",
        "Backlog grooming",
        "Retro"
      ],
      "published_at": "2021-12-19T15:27:10Z",
      "title": "Meetings and ceremonies",
      "updated_at": "2021-10-31T02:31:45Z",
      "type": "docs",
      "external_id": "873983fa778f17f1a92871ed4ad7673b333d4acc",
      "document_type": "page",
      "popularity": 1,
      "body": "Monday Tuesday Wednesday Thursday Friday Week 1 Sprint retro (every other sprint) Backlog grooming (end of sprint) Sprint planning (start of new sprint) Team meeting Meeting-free day Week 2 Team meeting Meeting-free day We break our work into two-week sprints. The new sprint starts on a Tuesday with sprint planning, where we commit to a set of stories that we're confident we can complete by the end of the sprint. Near the end of the sprint, we prepare for the next sprint with backlog grooming. The sprint closes with a retro where we discuss what went right and what went wrong, and then we kick off a new cycle. Each squad does their own backlog grooming and sprint planning, and manages their sprint backlog independently. We do retros together so we can talk through issues that affect both squads and share expertise and ideas. Tip Why do we end sprints on Mondays and start Tuesdays? This funny schedule makes things easier to work across timezones. If we ended sprints on Fridays, our Barcelona-based writers would need to do retros and grooming on Friday evening, and who wants that? Sprint planning On the first Tuesday of a new sprint, we commit to a series of stories until we have filled our capacity for the sprint. This is the largest meeting in the sprint---about 60 to 90 minutes. Before the sprint planning meeting, the scrum master for each squad calculates their point budget. Then, during the meeting: We select the highest priority item in the backlog. The person who nominated the story introduces it, and the team asks questions until everyone feels ready to vote. The team plays planning poker. Everyone secretly chooses a card, then we reveal them at the same time: If everyone picks the same card, that's the point value and we move on. If there's an even split between two cards, we choose the larger and move on. If it's mixed, the highest and lowest voters briefly explain their votes. Then we vote again. Once we're sure we can fit the story in, we subtract its story points from the sprint's points budget. We repeat the above steps until we've used up our points budget. We generally avoid pre-assigning people to stories during sprint planning. Instead, we pick up the next story or peer edit in the To Do column as stories are completed. Backlog grooming The day before the sprint starts, we do two rounds of backlog grooming. The purpose of backlog grooming is to prioritize work for sprint planning, and identify stories that need to be fixed before sprint planning to ensure scope is clear and planning runs smoothly. The first round of backlog grooming is by squad, where the members and manager of the squad get together and work through the grooming checklist. Then we do a second round of grooming with the managers and scrum masters to look at the sprint backlog for both squads and ensure nothing is or has fallen through the cracks. In the grooming, we: Each writer brings their personal \"top five\" tickets. The manager works with writers to prioritize those into a single, stack-ranked list for the team. We talk through stories and ensure they look ready for sprint planning (essentially, do they fit the Checklist for story readiness?). After grooming, the scrum master sends out a list of \"homework\" for stories that need improving. Retro Every other sprint, we conduct a 60 minute retrospective meeting, where we discuss: How do we feel about the sprint? What went well? Where can we improve? Anything we should start or stop doing? The goal of the retro is to improve the way we work together. That could be related to the sprint process, to how we collaborate with SMEs, to peer edits, and so on. ← Agile roles Planning poker → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.25293,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Meetings <em>and</em> ceremonies",
        "sections": "Sprint <em>planning</em>",
        "body": " filled our capacity for the sprint. This is the largest meeting in the sprint---about 60 to 90 minutes. Before the sprint <em>planning</em> meeting, the scrum master for each squad calculates their <em>point</em> <em>budget</em>. Then, during the meeting: We select the highest priority item in the backlog. The person who"
      },
      "id": "616c0dfa196a679fd43c9791"
    },
    {
      "image": "https://docs.newrelic.com/static/aa5797a9c6aaadf52a7bac18b3ac8e83/c1b63/dealing_with_interrupts.png",
      "url": "https://docs.newrelic.com/docs/agile-handbook/sprint-mechanics/sprint-workflow-and-jira-boards/",
      "sections": [
        "Sprint workflow",
        "Planned work",
        "Unplanned work (surprises!)",
        "Jira boards: Backlog and future sprints",
        "Jira boards: Current sprint",
        "Proposed",
        "In Progress",
        "Needs Peer Editor",
        "In Peer Edit",
        "Peer Edit Done",
        "Blocked",
        "Done",
        "Incomplete (\"carry-over\") tickets",
        "Tip"
      ],
      "published_at": "2021-12-19T15:27:56Z",
      "title": "Sprint workflow",
      "updated_at": "2021-11-06T12:49:01Z",
      "type": "docs",
      "external_id": "e18dcb23b9a43a083a2c4496d6fb9a20b9efc496",
      "document_type": "page",
      "popularity": 1,
      "body": "All of our sprint work is tracked in Jira. The workflow depends on what type of work we're dealing with: Planned or unplanned (\"surprise!\") work. Planned work Planned work includes all work that is currently in our backlog or has been added to the current sprint as a result of a Sprint Planning session. This could include writing or updating documentation, research, meeting with SMEs, information architecture, incorporating peer edits, SME review, and so on. Unplanned work (surprises!) Usually, we get notified of major requests far enough in advance that we can include them in liaison project plans, backlog grooming, and sprint planning. Occasionally, something bigger surprises us that needs emergency support. Follow this process with new docs asks to assess the scope of work and ensure we address valid docs needs within a reasonable amount of time. Our goal is to treat the sprint as sacred and insulate against \"surprise\" work that is not absolutely crucial. But we also want to ensure we're providing good internal customer service, and not getting hung up on process niceties for things that are small. Jira boards: Backlog and future sprints This is where the vast majority of tickets spend their time. Most tickets (even for active projects) spend at least a little time here before moving into a sprint to be actively worked. Being in the backlog doesn't mean something isn't important---just that we haven't committed to it yet.  You can also add tickets straight to a future sprint. This is where tickets tentatively assigned to a future sprint will be found. Tickets can be assigned here to be held for backlog grooming and sprint planning.  Jira boards: Current sprint Proposed This step is for work that has been assigned to the current sprint during Sprint Planning and is available to be picked up by a tech writer. When you're ready to take on a new ticket, try to work the queue from the top-down and avoid cherry picking. It's also better to pick up Needs Peer Edit tickets before committing to a new ticket. Something that needs a peer edit is close to done, and helping things across the finish line helps get value into users hands, and frees us up to think about new problems. In Progress This step is for all of the work to be done by the assignee: Research, meeting with SMEs, information architecture, writing, incorporating peer edits, SME review, and so on. Tickets are moved to this step once work is started by the TW, and remain here until the work is either complete, ready for peer review, or it becomes blocked. If additional large edits are needed after the peer review, the ticket can be moved back to In Progress for those edits. Needs Peer Editor Work that is ready for a peer edit. Once a peer editor picks it up, they move it into In Peer Edit. In Peer Edit This step is for a peer editor to review docs before they go live. Follow the Peer editor workflow, then move the ticket into Peer Edit Done.  Peer Edit Done This step is a holding state once peer editing is complete. After completing their peer edit and delivering their feedback, the peer editor moves the ticket into Peer Edit Done. From there, the assignee on the ticket (not the peer editor) moves the ticket into the appropriate column (In Progress, Blocked, or Closed). Minor edits can be completed from this column but for major doc rework, the ticket should be moved back into the In Progress column. Blocked This step is for tickets that cannot be moved forward by the team. This could be because we're waiting for a response from a SME, or for a feature to deploy, or for final signoff. The team keeps an eye on this column for tickets that may need escalation. Putting something in Blocked rather than In Progress lets us see the status of every ticket at a glance. This column can also be used for extended time out of the office for the assigned writer, if it's work that can be safely held. (If the work cannot be held while you're out, find another writer to step in and take over.) Once you're un-blocked, move the ticket to the appropriate column. If the ticket remains blocked at the end of the current sprint, it will need to be re-reviewed during backlog grooming to determine if the ticket will carry-over into the upcoming sprint, or return to the backlog until a future sprint. Done This step is for work that is 100% finished. Work gets cleared out this column before we start a new sprint. Incomplete (\"carry-over\") tickets Ticket don't carry over automatically between sprints. Instead, any ticket that gets carried over is treated as a \"new\" ticket in the next sprint planning. Before sprint planning, review any open tickets in the board that are assigned to you and figure out what to do with them. For each open ticket assigned to you (or \"carry over\"), decide if you should: Recommended: Clone the ticket and close the old one. This is the best option for partially completed work because it makes metrics easier. If you do: Clone the ticket. Note why we closed the ticket. Add an estimate of points completed in the Points Completed field. Create a follow-up ticket if necessary. Move the ticket to the next sprint. If you do: Review the ticket's action items and description to make sure they're still current. Clear out the ticket points. Move the ticket back to the backlog. If you do: Update the action items and description to make sure they're still current. Note why we moved to the backlog rather than carry over. ← Planning poker Liaisonships → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 110.51567,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Planned</em> work",
        "body": " current. Note why we moved to the backlog rather than carry over. ← <em>Planning</em> <em>poker</em> Liaisonships → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue."
      },
      "id": "616c0dc0196a67e6583c8164"
    },
    {
      "sections": [
        "AWS Billing monitoring integration",
        "Features",
        "Requirements",
        "Important",
        "Activate integration",
        "Enable AWS budgets",
        "Tip",
        "Find and use data",
        "Budgets",
        "Aggregated service costs",
        "Aggregated account/subaccount costs",
        "Unaggregated account/subaccount and AWS service costs",
        "Configuration and polling",
        "Metric data",
        "Inventory data",
        "Other system data"
      ],
      "title": "AWS Billing monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "e8c32484e34d77ca3a8fbedba6a12b60347a629e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/amazon-integrations/aws-integrations-list/aws-billing-monitoring-integration/",
      "published_at": "2021-12-15T02:06:03Z",
      "updated_at": "2021-10-23T16:40:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an integration for reporting your AWS Billing data to New Relic. This document explains how to activate this integration and describes the data that can be reported. Features New Relic's AWS Billing integration collects financial data for all your applications and AWS accounts, and then groups it to make it easier for teams to gain cost visibility. This integration collects data from different sub-accounts and services, and also evaluates your AWS budgets in terms of actual spend and forecasted spend. This financial data is split into the following dashboards: Billing (Costs) dashboard Billing (Budgets) dashboard Requirements Before you enable AWS budgets, you must: Set up an AWS budget through AWS Console for New Relic to collect AWS Billing data. Enable the Receive Billing Alerts checkbox in AWS console > Billing > Billing preferences. This is necessary for Amazon to report usage fees via CloudWatch. You are not required to actually receive alerts from Amazon. Important You only need to enable the Receive Billing Alerts checkbox to access the Billing (Costs) dashboard. Activate integration To enable this integration follow standard procedures to Connect AWS services to New Relic. Enable AWS budgets Enabling AWS budgets allows New Relic to capture service consumptions as well as usage and costs information for the budgets you configured in AWS. Tip If you have not created a role in AWS, follow the procedure to connect AWS to New Relic before configuring a budget policy. From the AWS Management console, select Services > IAM > Roles. Select the role you have configured for New Relic, then select Permissions. From the selected role's Inline policy tab, select the expand icon if one already exists, or select the option to add a new inline policy to the role. Select the option to add a custom policy. Enter a new custom policy name; for example, NewRelicBudget. Enter the following permission statement in the Policy Document: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"budgets:ViewBudget\" ], \"Resource\": \"*\" } ] } Copy Save your changes. Tip If you're configuring New Relic to fetch AWS budgets data from an AWS member account, make sure that the account owner has activated IAM user access to the Billing and Cost Management console. Find and use data To find your integration data in Infrastructure, go to one.newrelic.com > Infrastructure > AWS and select one of the AWS Billing integration links. For general information about how to find and use integration data, see Understand integration data. Data is attached to the FinanceSample event type. That event type can have four different provider values, depending on the type of billing data: Budgets AWS service expenses Aggregated account and sub-account expenses Unaggregated account/sub-account data, and unaggregated AWS service expenses Details for these types of billing data and how to query them are below: Budgets Data from the BillingBudget provider is related to AWS budgets. This data allows you to monitor: Money spent on a budget Limits you set for each budget Forecasted expenses You can configure budgets based on usage or expense, in different currencies, or for a monthly, quarterly, or annual time period. Budgets are a great tool to monitor and control costs and take advantage of the AWS predictive model to see forecasted costs. NRQL example: For a table of different budgets with actual spend, forecast, limits, and usage vs. monetary cost, run the following NRQL query: SELECT latest(`provider.budgetType`) as 'Budget Type', max(`provider.actualAmount`) as 'Actual Amount', max(`provider.limitAmount`) as 'Budget Limit', latest(`provider.forecastedAmount`) as 'Forecast' FROM FinanceSample WHERE provider='BillingBudget' AND providerAccountId = 'NEW_RELIC_ID_FOR_YOUR_CLOUD_ACCOUNT' FACET `provider.budgetName` SINCE 1 day ago Copy Tip The providerAccountId is your cloud service account ID in New Relic. You can find it in one.newrelic.com > Infrastructure > AWS, next to Provider account. Aggregated service costs The data stored in the BillingServiceCost provider is aggregated for service costs containing the latest and most recent data for all AWS service costs in all registered accounts. Tip If you have configured multiple AWS accounts, New Relic recommends filtering by each account with the attribute providerAccountName or providerAccountId. NRQL example: To group providers by service names, run the following NRQL query: SELECT latest(`provider.estimatedCharges.Maximum`) FROM FinanceSample WHERE provider='BillingServiceCost' AND providerAccountId = 'NEW_RELIC_ID_FOR_YOUR_CLOUD_ACCOUNT' SINCE 1 day ago FACET `provider.serviceName` LIMIT 20 Copy Tip The providerAccountId is your cloud service account ID in New Relic. You can find it in one.newrelic.com > Infrastructure > AWS, next to Provider account. Aggregated account/subaccount costs The BillingAccountCost provider stores aggregated data for account and sub-accounts containing the latest and most recent aggregate of financial data. Please bear in mind that if you have many AWS Accounts configured it is recommended to filter by each account with the attribute providerAccountName or providerAccountId. NRQL example: To group providers by account names, run the following NRQL query: SELECT latest(`provider.estimatedCharges.Maximum`) FROM FinanceSample WHERE provider='BillingAccountCost' AND providerAccountId = 'NEW_RELIC_ID_FOR_YOUR_CLOUD_ACCOUNT' SINCE 1 day ago FACET `provider.accountId` LIMIT 20 Copy Tip The providerAccountId is your cloud service account ID in New Relic. You can find it in one.newrelic.com > Infrastructure > AWS, next to Provider account. Unaggregated account/subaccount and AWS service costs The BillingAccountServiceCost provider contains unaggregated data filterable by AWS account ID or AWS service name. NRQL example: To understand the cost split of AWS services for a particular AWS account ID, run the following NRQL query: SELECT latest(`provider.estimatedCharges.Maximum`) FROM FinanceSample WHERE provider='BillingAccountServiceCost' AND providerAccountId = 'NEW_RELIC_ID_FOR_YOUR_CLOUD_ACCOUNT' SINCE 1 day ago FACET `provider.serviceName` LIMIT 100 Copy NRQL example: To understand the cost split of AWS accounts for your AWS EC2 service, run the following NRQL query: SELECT latest(`provider.estimatedCharges.Maximum`) FROM FinanceSample WHERE provider='BillingAccountServiceCost' AND providerAccountId = 'NEW_RELIC_ID_FOR_YOUR_CLOUD_ACCOUNT' AND `provider.serviceName`='AmazonEC2' FACET `provider.accountId` SINCE 1 day ago LIMIT 100 Copy Tip The providerAccountId is your cloud service account ID in New Relic. You can find it in one.newrelic.com > Infrastructure > AWS, next to Provider account. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS Billing integration: New Relic polling interval: 1 hour Amazon CloudWatch data interval: 4 hours Metric data The New Relic infrastructure AWS Billing integration collects the following metric data: Data Description AWS Billing metrics EstimatedCharges: The estimated charges for your AWS usage. This can either be estimated charges for one service or a rollup of estimated charges for all services. The currency is USD. AWS Budgets metrics actualAmount: The actual cost or usage being tracked by a budget. limitAmount: The total amount of cost or usage that you want to track with a budget as an upper limit. forecastedAmount: The forecasted cost or usage being tracked by a budget. Inventory data Inventory data provides information about the service's state and configuration. For more about inventory data, see Understand and use data. Name Description /budgets name: The name of a budget. Unique within accounts. startTime: Start date for the period of time covered by a budget. endTime: End date for the period of time covered by a budget. limitAmount: The total amount of cost or usage that you want to track with a budget as an upper limit. limitUnit: The unit of measurement used for the budget forecast, actual spend, or budget threshold, such as dollars or GB. timeUnit: The length of time until a budget resets the actual and forecasted spend. Valid values: monthly, quarterly, annually. type: Whether this budget tracks monetary cost or usage. /budgets/costFilters name: The name of a budget. Unique within accounts. Other system data The AWS Billing integration also collects the following attributes: Attributes Description AWS Billing attributes Service Name: The name of the AWS service. This dimension is omitted for the total of estimated charges across all services. Linked Account: The linked account number. AWS Budgets attributes budgetName: The name of a budget. Unique within accounts budgetType: Whether the budget tracks monetary cost or usage",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 72.41018,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Enable AWS <em>budgets</em>",
        "body": " into the following dashboards: Billing (Costs) dashboard Billing (<em>Budgets</em>) dashboard Requirements Before you enable AWS <em>budgets</em>, you must: Set up an AWS <em>budget</em> through AWS Console for New Relic to collect AWS Billing data. Enable the Receive Billing Alerts checkbox in AWS console &gt; Billing &gt; Billing preferences"
      },
      "id": "617da775e7b9d239d4c03c80"
    }
  ],
  "/docs/agile-handbook/sprint-mechanics/sprint-workflow-and-jira-boards": [
    {
      "image": "https://docs.newrelic.com/static/e74b3e31916f093c77d41e8beef9ecc9/c1b63/lines_of_communication.png",
      "url": "https://docs.newrelic.com/docs/agile-handbook/key-concepts/agile-sprints-profusion-of-terms/",
      "sections": [
        "Agile vs sprints (vs Jira vs GitHub): A profusion of terms",
        "Agile",
        "Sprint (or scrum)",
        "Jira and GitHub issues",
        "Teams and squads",
        "Tip"
      ],
      "published_at": "2021-12-19T15:09:06Z",
      "title": "Agile vs sprints (vs Jira vs GitHub): A profusion of terms",
      "updated_at": "2021-10-31T02:29:57Z",
      "type": "docs",
      "external_id": "7f3f97222daf52cf14afcf8fc432b96b3cebd4b1",
      "document_type": "page",
      "popularity": 1,
      "body": "Our team uses an agile Sprint workflow in Jira and GitHub to manage our work. We've further divided our team into squads to simplify planning and improve accountability. All those words are pretty inconsistent in their usage, so let's break them down further. Agile People use agile to mean everything from a specific system of work (which we call sprints), to just \"moving fast, preferably in a way that lets me bend things to my whims.\" Luckily, we don't need to define it from scratch. Wikipedia does an admirable job defining it: Agile software development is an approach to software development under which requirements and solutions evolve through the collaborative effort of self-organizing and cross-functional teams and their customer(s)/end user(s). It advocates adaptive planning, evolutionary development, empirical knowledge, and continual improvement, and it encourages rapid and flexible response to change. For our team, that means our process is optimized to ship early and often. This lets us respond swiftly to changes in the product roadmap. More importantly, it ensures we validate our solutions with stakeholders, and that we're not letting valuable work sit around and get moldy when it could be out in the world making our users' lives better. Sprint (or scrum) This is the particular flavor of agile we follow. The sprint system (often referred to as scrum) is one major approach to Agile, along with other Agile systems such as Kanban. Sprint systems are often accompanied by a lot of jargon and best practices, but for our team the most essential elements are: Working in strict timeboxes (two weeks in our case) Planning that sprint in advance, and not changing the scope of the sprint (much) once it starts Expecting all team members to contribute to making the sprint a success The video Agile Product Ownership in a Nutshell (18 minutes) is an excellent resource for learning about sprint methodology. The Kindle book Scrum: a Breathtakingly Brief and Agile Introduction is also a great read that you can get through in a short afternoon. For more on the \"why\" of Sprint as our chosen methodology, see Key agile principles. And for more on the \"how,\" see Sprint workflow. Jira and GitHub issues Jira and GitHub issues are the tools we use to manage our Agile workflow. If you remember one thing about them, it should be this: using Jira or GitHub issues is not the same as having an agile workflow. They're powerful tools for tracking work and managing a backlog, but the most important part of project management is the structure we impose on that tool. Jira is for sprint work. Sprints are where roadmap docs get written, monthly commits get delivered, and deeper research percolates. We have a backlog, board, and future sprint list in Jira that help us track what people want, what's coming up, and what we're working on now. For more on the mechanics of how we use Jira, see Sprint workflow and Jira boards and Ticket best practices. We use GitHub projects for hero work, customer-reported issues, and managing the flow of PRs and edits. The Docs PRs and Issues board contains everything we're actively working on in GitHub. We'll often connect work in GitHub back to Jira by putting a Jira issue key in the PR or issue title (DOC-1234, for example). For more on the mechanics of how we use GitHub, see Managing the GitHub boards. Teams and squads Our team is the Tech Docs team. We're collectively responsible for docs.newrelic.com and sundry writing content. Our team is further divided into two agile squads (The Odd Squad and The Amp Squad), one squad for each manager. The primary function of squads is to simplify sprint planning, backlog grooming, and liaisonships. Less people means shorter meetings. It also means better information sharing: The more people you have in a group, the more lines of communication are needed (see illustration) to maintain a shared understanding. Small squads can collaborate more easily than a large team, because not everyone needs to keep in mind everything that goes on everywhere. Each squad is responsible for its own grooming and sprint planning, but the managers and scrum masters coordinate grooming to ensure we're meeting our overall goals as a team. Key agile principles → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 747.85754,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Agile vs <em>sprints</em> (vs Jira vs GitHub): A profusion of terms",
        "sections": "Agile vs <em>sprints</em> (vs Jira vs GitHub): A profusion of terms",
        "body": "Our team uses an agile <em>Sprint</em> <em>workflow</em> in Jira and GitHub to manage our work. We&#x27;ve further divided our team into squads to simplify planning and improve accountability. All those words are pretty inconsistent in their usage, so let&#x27;s break them down further. Agile People use agile to mean"
      },
      "id": "616c0d96196a677e623c7bd2"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/agile-handbook/sprint-mechanics/planning-poker/",
      "sections": [
        "Planning poker and points budgets",
        "Poker card definitions",
        "How we calculate the points budget",
        "Tip"
      ],
      "published_at": "2021-12-19T15:27:35Z",
      "title": "Planning poker and points budgets",
      "updated_at": "2021-10-31T02:31:45Z",
      "type": "docs",
      "external_id": "d002b0635204648ebe36dbdc6c242e80fa971aa3",
      "document_type": "page",
      "popularity": 1,
      "body": "Planning poker is a consensus-based estimating and planning technique. To start a planning poker session, the story reporter or liaison presents a story to the team. Then, the team all votes (at the same time, to prevent groupthink) on how difficult they think the story is. Most agile teams that estimate with planning poker use cards that follow a Fibonacci sequence. We've found over the years that those large jumps weren't very helpful to us in estimating, and that we estimated better with smaller numbers. So we use fairly small, granular numbers that roughly break down to 1 point ≈ 1 day. Note that even though each of the possible scores have approximate time values associated with them, we vote in points, not time. Scoring with points prevents getting into weedsy debates about exactly how long something will take, and instead focuses us on the requirements and difficulty of the story. Poker card definitions These how we define our poker cards: Card Value ¼ An hour or two. Anything smaller than this isn't worth bringing into a sprint or even ticketing—just do it right now. ½ About half of an \"ideal\" day. We define an \"ideal\" day as one without meetings or interruptions. Using \"ideal\" days makes it easier to estimate a story without litigating the details of how we spend our time. 1 About one \"ideal\" day. This is one of our most common story sizes. 1½ About one and a half \"ideal\" days. This is our other most common story size. 2 About two \"ideal\" days. 3 About three \"ideal\" days. We jump from 2 straight to 3 to avoid unrealistic granularity in our estimates. 5 About a week of work. 7 About a full sprint (two weeks) of work. We very rarely use this card—a ticket that takes a full sprint on its own is almost always too large and should be chunked into smaller, incremental tickets. Break! \"I need a break.\" When someone plays a break card, we finish estimating the current story then immediately take a five minute break. Defer No strong opinion. Don't play this card on the first round of poker! Seeing wildly different estimates is a sign we don't have a shared understanding of the work. Playing an early Defer card can mask that uncertainty. How we calculate the points budget In order to ensure we're not filling up our sprint with more \"ideal days\" of time than we realistically have available, we calculate the points budget based on the actual output of the team. Over time, we've found writers average 0.6 (technically, 0.57) points per day. Tip This value is totally unique to each team! A high number doesn't indicate a more productive team, and a low number isn't a problem. Story points are only meaningful within a team. If you're starting a new team or new process, you'll need to zero in on the ideal number of points per writer. An easy way to figure out the right budget is to have the team vote in retro on whether to increase, decrease, or keep the same budget next sprint. You'll overshoot or undershoot a few times, but after a couple sprints you'll get a good sense of what constistutes a sustainable pace. Once you know your baseline, calculating the budget is straightforward: Take the number of writers, and multiply by the number of days in the sprint. To make the math easy, let's say 5 writers and a 10 business day (2 calendar week) sprint. Or, 50 person-days total. Subtract out time off days, plus 1 day for each day of hero duty. Let's say we have 1 writer on vacation in Maui, and 1 day of hero time per day. 50 person days, minus 10 days of vacation and 10 days of hero time, gives us 30 person-days. Multiply the number of days times the average velocity per writer, per day. Our average velocity is 0.6 points/writer/day. 30 person days multiplied by 0.6 would give us an 18 point budget for the sprint. Because we work in two squads, we calculate a separate points budget for each squad ahead of sprint planning. ← Meetings and ceremonies Sprint workflow and Jira boards → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 480.98376,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": " the average velocity per writer, per day. Our average velocity is 0.6 points&#x2F;writer&#x2F;day. 30 person days multiplied by 0.6 would give us an 18 point budget for the <em>sprint</em>. Because we work in two squads, we calculate a separate points budget for each squad ahead of <em>sprint</em> planning. ← Meetings and ceremonies <em>Sprint</em> <em>workflow</em> and Jira boards → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue."
      },
      "id": "616c0e2d28ccbc6e5e00337f"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/agile-handbook/sprint-mechanics/liaisonships/",
      "sections": [
        "Liaisonships",
        "Liaison responsibilities: Manage project flow",
        "Liaison responsibilities: Build expertise",
        "Liaison responsibilities: Define content strategy (oh, and do the writing)",
        "Tip"
      ],
      "published_at": "2021-12-19T15:26:49Z",
      "title": "Liaisonships",
      "updated_at": "2021-10-31T02:30:52Z",
      "type": "docs",
      "external_id": "bf8ec36541058fe18f8395db811344baf7f23e22",
      "document_type": "page",
      "popularity": 1,
      "body": "For large projects, we'll typically assign a particular tech writer to that project as a \"liaison.\" The liaison’s job is to ensure that we get complete, consistent, and timely docs. Not every project gets a liaisonship! For smaller projects, we'll encourage teams to edit the docs directly, and then have the hero review their changes. And a smallish project may not need a full liaison—a single ticket might be enough to manage the work. To figure out which type of support is best for a given project, one of the managers on the team will have a scoping conversation with a subject matter expert. Here's a few reasons a project might get a dedicated liaison: Project is complex and would benefit from intimate familiarity with the feature. Project requires significant information architecture work. Project will produce enough docs that consistency across those docs will be hard to achieve without a centralized editor. Project SMEs would benefit from a consistent \"face\" of the tech writing team. However, a liaison is not the only author on a project. Liaisons should structure their work to maximize swarming and knowledge sharing. Liaison responsibilities: Manage project flow Activity Who? Notes Learn new thing exists Team Ideally the Hero or a Tech Docs manager gets notified directly by a PM about a new project. But sometimes we'll find out about something unexpectedly. If you're not sure whether we have a writer working on something, ask a manager on the team and they'll reach out to the subject matter expert to scope it. Have a scoping meeting Tech Docs manager The manager is responsible for tracking the general state of major projects across the company, and is generally the first point of contact for new projects. When a large new project comes up, the manager will do a pre-scope meeting with the requestor. (Appendix: Project scoping cheatsheet has a list of common questions for this pre-scope meeting.) That conversation helps figure out timeline, complexity, key docs considerations, and which writer will be a good fit as liaison. Assign a liaison Tech Docs manager Once we know we need a liaison, a manager on our team will figure out who to assign. Some of the factors we use to decide who to assign include bandwidth, familiarity with the product or feature, career goals and writing strengths, and simple interest in the topic. Keep track of project dates Liaison The managers on the team keep track of upcoming projects that don't have a liaison assigned. Once a writer gets involved, that liaison keeps track of the specifics of dates: Betas, limited releases, GAs, fast-follows, and so on. Your manager's always here to help out if you're getting blocked or dates are shifting too rapidly to plan properly. Validate the docs plan with the project team Liaison The liaison works with their stakeholders to define the information architecture and deliverables. Create tickets Liaison Since the liaison defines the information architecture, the liaison will know what kinds of deliverables we need. The liaison also acts as an advocate for their tickets in the backlog grooming and sprint planning processes, and ensures their stories meet the story quality requirements. The liaison should also ensure that our partner teams have appropriate tickets in their backlogs for their work. Remove blockers (such as reviewer delay) Liaison + Manager While the liaison is primarily responsible for handling SME relationships and removing day-to-day blockers, your manager is here to help unstick things anytime you need help. Wrap up the liaisonship Liaison Liaisonships are not forever assignments! When the bulk of your work on a project is complete, it might be time to consider ending the liaisonship. Reach out to your manager to talk about it. When you end it, let stakeholders know and update the liaison roster. Also let your stakeholders know they can always ping the docs hero for help or if they have a new project. Liaison responsibilities: Build expertise Activity Who? Notes Develop a deep expertise on feature and audience. Liaison Become the Docs Team's local expert on the feature. Understand what it does, what problems it solves, and the implications for our content. Educate the team on the feature Liaison Part of your responsibility as liaison is to share expertise around the team. That helps with swarming, but it also makes for better hero review and a smarter team that can write more intelligently about the entire New Relic One platform. Coordinate with design and/or research and test your docs Liaison Reach out to the designer and/or researcher for the project, and periodically sync on any shared concerns, user needs, etc. And you should advocate for user testing and validation of your content. Liaison responsibilities: Define content strategy (oh, and do the writing) Activity Who? Notes Define the information architecture Liaison As liaison, you're the expert on both the feature the product team is building, and the docs content (new and existing) that will support that feature. Build an IA that will meet all project needs and scale to the future. Write content Team The liaison writes much of the content for their project, especially the conceptual content like intro docs. But the whole team is expected to swarm and contribute to large projects, with the liaison coordinating that work. Peer edit drafts Liaison When we swarm and have someone else contribute to the project, the liaison peer edits their drafts to ensure consistency with the overall vision. Coordinate publication Liaison When the time comes to release (whether that's beta, GA, limited release, or EoL), it's the liaison's job to coordinate with PM, Eng, and Product Marketing to ensure docs go out on time with other deliverables. ← Sprint workflow and Jira boards What is a hero? → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 365.2796,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": ", and Product Marketing to ensure docs go out on time with other deliverables. ← <em>Sprint</em> <em>workflow</em> and Jira boards What is a hero? → Tip We welcome thoughts or questions on our handbook! The easiest way to get in touch is to file a GitHub issue."
      },
      "id": "616c0d97e7b9d227264780c5"
    }
  ],
  "/docs/alerts-applied-intelligence/applied-intelligence/incident-intelligence/change-applied-intelligence-correlation-logic-decisions": [
    {
      "sections": [
        "Alerts and applied intelligence notification integrations",
        "Early access",
        "Integration details",
        "Atlassian Jira",
        "Permissions",
        "Set up a Jira destination",
        "Important",
        "Two-way sync",
        "Configure the message template",
        "Send a test notification",
        "ServiceNow (Incident-Management)",
        "Roles",
        "Set up a destination",
        "Slack",
        "Prerequisites",
        "Set up a Slack destination",
        "Configure the Slack message settings",
        "Webhook",
        "Set up a webhook destination",
        "Configure the webhook event template",
        "Customize the webhook payload",
        "Email",
        "Configure the email settings"
      ],
      "title": "Alerts and applied intelligence notification integrations",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Incident Intelligence",
        "Destinations"
      ],
      "external_id": "7220c630fc187bb61784ff2cc2213e588b269b00",
      "image": "https://docs.newrelic.com/static/d4e9baecc3a76dd1a5945f8ec0aeca66/c1b63/webhook-notification-template.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/notifications/notification-integrations/",
      "published_at": "2021-12-19T15:28:40Z",
      "updated_at": "2021-12-19T15:28:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Early access The features described here are early access. You won't be able to use these features if you're not part of the early access program. For more information on related features, see our docs on Alerts notification channels, Incident Intelligence destinations, and Proactive Detection notifications. Alerts and Applied Intelligence notification integrations are specific services and platforms you can use to send notifications from New Relic. Integration details Read more about each of our specific notification integrations. Atlassian Jira Integrate New Relic with Atlassian Jira(Cloud) and automatically create and update Jira issues. Permissions The required permissions from the Jira API-Token are create, edit, and close tickets. To enable the two-way sync toggle, the provided Jira API-Key should have an Admin role. Set up a Jira destination Create Jira issues, then enable Jira and New Relic to share updates and stay synced. To create a Jira destination, enter the following information: Destination name: Custom name to identify the destination Jira account endpoint: the URL of the destination User-name: this will be the email address of the user making the connection API token: generated from your Atlassian account Important New Relic currently supports Atlassian-Jira Classic (company-managed) projects. Before saving the destination, we recommend you test the connection via the test connection button. Jira destination configuration. We recommand to test the connection before saving. Two-way sync You can enable a two-way integration with Jira to keep the issues' state synced with the corresponding state in New Relic. To enable two-way sync, turn on the ‘two-way integration’ toggle. When turned on, a Jira Webhook would be created in your Jira account at a later stage, for the selected project (see ‘customize a message template’). The webhook would contain access details to Newrelic (URL and Newrelic-API-KEY) Configure the message template To configure a template for a Jira issue, you first need to choose a destination. You will be able to create a new destination at this stage. Upon successful connection to the destination, you will need to choose a project, and then select the Jira issue type you would like to be used. Once the issue-type is selected, the configured project's fields are fetched from your account and automatically mapped to your Jira instance. To help you get started, we automatically present the required and recommended fields and values. Required fields must be set with a value. You can add or remove optional fields(use the X mark on their right side) Jira message template. Send a test notification You can see how the JIRA issue will appear by clicking a test notification with default field values. If successful, a JIRA issue will be created and a link will appear. ServiceNow (Incident-Management) Integrate New Relic with ServiceNow Incident-Management and automatically create and update incidents. Roles As part of the integrations, we fetch fields from the your serviceNow incident table and optional values. For this, the provided ServiceNow user details required read permissions for the tables: sys_dictionary, sys_choice, sys_user and task. A read/write permission to incident To be able to fetch users for the caller column, we required read permissions for the sys_users table. The above permissions can be achieved with the roles personalize_choices, personalize_dictionary, rest_service, itil. Read/Write permissions to the api_key_credentials table is required to enable two-way integration. This can be covered with the roles credentials_admin and discovery_admin. Set up a destination To create a ServiceNow destination, enter the following information: Destination Name: custom name to identify the destination Domain: the URL of the destination User-name: the name of the user Password: the user name’s password Before saving the destination, we recommend testing the connection by clicking the test connection button. Two-way sync You can configure a two-way integration with ServiceNow Incidents Management to keep the incidents' state synced with the corresponding state in New Relic. Here are some required steps to remember when configuring the two-way integration: Turn on the two-way integration toggle. Open and download this XML file, which includes the business rule triggering events back to New Relic One. In the ServiceNow sidebar menu, go to System Definition > Business Rules. Click the menu icon in one of the column headers, select Import XML and upload the XML file you downloaded. Once the Destination is saved, a New-Relic API-Key will be kept in the api_key_credentials. The key would sent in a header as part of the callback REST call to New-Relic Configure the message template Upon a successful connection, ServiceNow incident table columns are fetched from your account and automatically mapped to your ServiceNow instance. To help you get started, we automatically present the required and recommended fields and values. Required fields must be set with a value. You can add or remove optional fields(use the X mark on their right side) Select, edit or remove fields for the ServiceNow-Incident template. Send a test notification You can see how the ServiceNow incident will appear by clicking a test notification with default field values. If successful, an incident will be created and a link will appear. Slack Send notifications-messages to your Slack channels. Prerequisites Your Slack workspace needs to have the New Relic application installed. The application must be approved by a workspace admin before it can be individually installed by users Set up a Slack destination Click on the `one-click Slack authentication' will lead you to the Slack landing page to continue the OAuth2 authentication process. On the Slack landing page, if you're not signed into the required workspace, you're redirected to Slack to sign in. Add your workspace name or select the relevant workspace and click Continue. When signed in to the selected workspace, you are requested to allow New Relic to perform the specified actions. Clicking `Allow' will redirect you back to the Destination page. Configure the Slack message settings Select a Destination(Workspace) and select a Slack-channel where the messages will be sent. You can create a new destination if there is no pre-defined destination for the required workspace. Note that, for privacy reasons, users need to be authenticated to select private channels (one-time process) Send a test notification You can send a test notification with a pre-defined example payload to the channel. This creates a message in the selected Slack-channel. Webhook Use the webhook notifier to send the notification messages to any endpoint you like. Set up a webhook destination To create a webhook destination, you need the following: Destination Name: A unique destination name URL: the endpoint of the target application, authentication and custom headers if needed. Authorization mechanism (Optional):. Can be basic authentication or a bearer token Configure the webhook event template Pick a webhook destination from the list and configure the HTTP-POST request. The request configuration requires you to: Set a name for the template. Select a pre-configured destination from the destinations list or create a new one. Add custom headers (optional). Configure the request’s payload. Customize the webhook payload You can use the default payload or customize it to contain the required data. Pick Variables from the variables menu and apply handlebars syntax to enrich your webhook. Note that the request’s content-type is JSON by default. Hence, the payload needs to keep the JSON form. See Usage examples The ‘preview’ section on the right hand-side shows an expected payload after the template is rendered. If the eventual payload would not form a valid Json, an error will be shown and it won’t be possible to save the template. If the webhook payload conforms a valid Json, you can send a test notification to your defined webhook destination We recommend sending a test notification to make sure that everything's connected correctly. Email Send email notifications to users. Configure the email settings Add one or more recipients. Users with New Relic accounts can be found via autocomplete when searching for their name or email address. To add a user without a New Relic account or email distribution list, add the full email address. Every recipient will be translated into a 'destination'. You can follow the email notifications per destination in the notifications-log Send a test notification You can send a test notification to make sure the email notifications reach the inbox.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 191.16164,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Alerts and <em>applied</em> <em>intelligence</em> notification integrations",
        "sections": "Alerts and <em>applied</em> <em>intelligence</em> notification integrations",
        "tags": "Alerts and <em>Applied</em> <em>Intelligence</em>",
        "body": " notifications. Alerts and <em>Applied</em> <em>Intelligence</em> notification integrations are specific services and platforms you can use to send notifications from New Relic. Integration details Read more about each of our specific notification integrations. Atlassian Jira Integrate New Relic with Atlassian Jira(Cloud"
      },
      "id": "618ff71628ccbc60710321e4"
    },
    {
      "sections": [
        "Workflows",
        "What is an issue",
        "Add a Workflow",
        "Tip",
        "Enrichments",
        "Important",
        "Notifier"
      ],
      "title": "Workflows",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Workflows",
        "Notifications"
      ],
      "external_id": "0993c323f76467d22136cac6982ccd48e859f722",
      "image": "",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/applied-intelligence/incident-workflows/incident-workflows/",
      "published_at": "2021-12-19T13:46:37Z",
      "updated_at": "2021-12-17T01:48:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With workflows you control when and where you want to receive notifications about issues, tunnel the right information to the relevant person or team, and enrich your issue's notifications with additional New Relic data. What is an issue Issues are groups of incidents that describe the underlying problem of your symptoms. When a new incident is created, Incident Intelligence opens an issue and evaluates other open issues for correlations. For more information see Use Incident Inteligence. Add a Workflow Tip The maximum number of workflows you can add is 100 per environment and 50 per account. The workflows feature is located under the Alerts & AI menu. Go to one.newrelic.com, click Alerts & AI, in the left navigation under Enrich and Notify click Workflow, then click Add a workflow. Name your workflow. This field is mandatory and needs to be unique. Workflows filters the issues you want to send to the relevant destination. You can send all issues by selecting Send all issues or you can build a query to filter the right issues. Build a query by selecting an attribute, operator and value/s to narrow down the right issues you want to be passed on. Optional: Enriched Data, available to full platform users. Enrichments Enrichments can give additional context on alert notifications by adding NRQL query results to them The workflows enrich tool is similar to the query builder. The NRQL query's results are included in the notification message. If you create queries for your workflows, they run in real time and the query results are the value of the enricher. You can save any valid query into your workflow, even if they return an empty result, but you must run the query before saving the enrichment. You can also query with issue-specific variables; for example {{entitiesData.ids}} For enrichment examples, see Workflow data enrichment examples. Note: The query name needs to be unique because you'll use the name to decide how to include the result in the payload of the notification that will be sent by the workflow. Important At this stage, visual representation of enrichments are sent to Slack and email. For Servicenow and JIRA destinations, we recommend to limit to NRDB queries with single value results. For example: count, min, or max. The maximum number of enrichments per workflow is 5. The maximum number of variables an enrichment can contain is 1. Notify: Choose one or more destinations and add an optional message. Notifier To save and activate a Workflow, you need to include a notifier. Setting up a notifier requires having a notification destination configured. You can either set it up through the Destination menu item, or through the Create new destination item in the destination selection drop down in each destination channel screen. A notification destination includes the credentials required to send a notification to a third party platform and an option to customize the message sent by mapping fields from our issues to fields in the destination platform. To learn how to set up destinations and configure message templates, check the documentation on notifications. Click update message once completing the notifier requirements Optionally, test your workflow. We'll use existing data from your account to test what you've configured and send a sample notification Click activate to complete the workflow. Tip In any destination channel, start typing and a variable menu will open up. You'll see the names of the variables, which will be replaced with the variable's values at runtime. To use the enrichers' results, use their name.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 169.87784,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "Alerts and <em>Applied</em> <em>Intelligence</em>",
        "body": " problem of your symptoms. When a new incident is created, Incident <em>Intelligence</em> opens an issue and evaluates other open issues for correlations. For more information see Use Incident Inteligence. Add a Workflow Tip The maximum number of workflows you can add is 100 per environment and 50 per account"
      },
      "id": "603e967664441f7e6f4e889b"
    },
    {
      "sections": [
        "Get started with Incident Intelligence",
        "Requirements",
        "Set up Incident Intelligence",
        "1. Configure your environment (one-time)",
        "Tip",
        "2. Configure sources",
        "Alerts",
        "Datarobot (formerly Algorithmia)",
        "Aporia (MLOps)",
        "Superwise (MLOps)",
        "Anomalies",
        "AWS",
        "Grafana",
        "PagerDuty",
        "EOL NOTICE",
        "Prometheus Alertmanager",
        "REST API",
        "Splunk",
        "Important",
        "Splunk metadata",
        "3. Configure destinations (ServiceNow and others)",
        "Configure ServiceNow (example)",
        "Send data to ServiceNow",
        "Custom notification message",
        "Other destinations",
        "Send data to PagerDuty",
        "Send data via webhook",
        "4. Configure pathways",
        "What's next?"
      ],
      "title": "Get started with Incident Intelligence",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Incident intelligence"
      ],
      "external_id": "8c4a5a914ca01cb42250908d2fb1a12ccc697e25",
      "image": "https://docs.newrelic.com/static/5cb28999dc618c6a5a2b8be1fa72e660/b97f6/image-%25281%2529_0.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/applied-intelligence/incident-intelligence/get-started-incident-intelligence/",
      "published_at": "2021-12-19T14:23:58Z",
      "updated_at": "2021-12-09T22:19:28Z",
      "document_type": "page",
      "popularity": 1,
      "body": "As part of Applied Intelligence, Incident Intelligence helps you correlate your incidents and reduce noise in your environment. It gives you an overview of all your incidents, their sources, and related events. Requirements If you haven't already, sign up for a New Relic account to get started. To use most Applied Intelligence features, you must be a full platform user. For more information, see user type capabilities. Set up Incident Intelligence To enable Incident Intelligence, follow these steps. Afterwards, issues should start to appear in your issue feed. 1. Configure your environment (one-time). 2. Configure sources. 3. Configure destinations. 4. Configure pathways. 1. Configure your environment (one-time) To set up an environment in Incident Intelligence, you need an administrator to select a New Relic account for it. This account should be the one your team is using. Who sets the environment? Only administrators, and only for accounts where they have admin privileges. Can administrators set more than one environment? They can set one environment per parent account and its child accounts. More than one can be set if an administrator has privileges for more than one parent account. Tip Incident Intelligence is a cross-account product. This means you can send in data from any New Relic account or external source to correlate events. 2. Configure sources After setting up your environment, determine your incident sources. These are your data inputs. You can get data from any of the following sources: Alerts By integrating Incident Intelligence with your alerts violations, you can get context and correlations from what you're monitoring. To get data from alerts: From one.newrelic.com, click Alerts. On the left under Incident Intelligence, click Sources and then click Alerts. Select the policies you want to connect to Applied Intelligence, and click Connect. You can add additional alerts policies or remove policies you've already connected in Sources > New Relic Alerts. Tip Adding alerts as a source will not affect your current configuration or notifications. Datarobot (formerly Algorithmia) By integrating Incident Intelligence with your Datarobot machine-learning models, you can monitor your machine learning model performance. To configure Datarobot for Incident Intelligence, see our integration docs. Aporia (MLOps) By integrating Incident Intelligence with your Aporia machine-learning models, you can monitor your machine learning model performance. To configure our Aporia integration, see our docs. Superwise (MLOps) By integrating Incident Intelligence with your Superwise machine-learning models, you can monitor your machine learning model performance. To configure our Superwise integration, see our docs. Anomalies By integrating Incident Intelligence with your New Relic Proactive Detection anomalies, you can get context and correlations from what you're monitoring. To get data from New Relic Proactive Detection anomalies: From one.newrelic.com, click Alerts & AI. On the left under Incident Intelligence, click Sources and then click Anomalies. Select the configurations you want to connect to Applied Intelligence by toggling to on (green). To add or remove alert policies, from Alerts & AI, click Sources, then Alerts. Tip Adding anomalies as a source won't affect your current Proactive Detection configurations or notifications. AWS You can integrate Incident Intelligence with Amazon CloudWatch to provide incident management for all of your AWS services. To integrate Amazon CloudWatch: Go to one.newrelic.com and click Alerts & AI. On the left under Incident Intelligence, click Sources and then click Amazon Web Services. Copy the URL. Create a new Amazon SNS topic. Set CloudWatch to forward all Alarms state changes to that topic: In the Amazon CloudWatch UI, click Events > Event Pattern. Select Service Name > CloudWatch. Select Event Type > CloudWatch Alarm State Change. Select Targets > SNS Topic, and select your new Amazon SNS topic. Create a new subscription: In the Amazon AWS UI, click Create a Subscription. Select your new Amazon SNS topic. Select Protocol > choose HTTPS. In Endpoint, paste the URL you previously copied from the Applied Intelligence Sources. Grafana You can integrate Incident Intelligence with Grafana's notifications for insight into events across your applications and environment. Grafana's webhook notification is a simple way to send information over HTTP to a custom endpoint. To integrate Grafana as a new webhook: Log into your Grafana portal using Admin permissions, and choose Alerting. On the Grafana Notification Channels page, click New Channel > Webhook. Go to one.newrelic.com and click Alerts & AI. On the left under Incident Intelligence, click Sources, and then click Grafana. Copy the URL, and paste it into your new Grafana webhook. PagerDuty EOL NOTICE As of October 2021, we've discontinued support for several capabilities with PagerDuty, including suggested responders, golden signals, and component enrichment. For more details, including how you can easily make this transition, see our Explorers Hub post. You can integrate Incident Intelligence directly with your PagerDuty services to ingest, process, and enhance all of your PagerDuty incidents. Connecting PagerDuty services to Applied Intelligence will not affect your current services or notifications. To get data from PagerDuty: Make sure your PagerDuty API key has write access. From one.newrelic.com, click Alerts & AI. On the left under Incident Intelligence, click Sources and then click PagerDuty. Enter your PagerDuty API key. The key should be either a personal or general access key with write access. If it's created by a user, the user should be an admin. Select the PagerDuty services you want to connect to Applied Intelligence, and click Connect. You can add additional services or remove services you've already connected in Sources > PagerDuty. Prometheus Alertmanager By integrating Incident Intelligence with Prometheus Alertmanager, you can receive and correlate your Prometheus alerts with events from other sources. To integrate Prometheus Alertmanager: Set up your Alertmanager configuration file by running: ./alertmanager -config.file=simple.yml Copy Go to one.newrelic.com and click Alerts & AI. On the left under Incident Intelligence, click Sources and then click Prometheus Alertmanager. Copy the Prometheus Alertmanager URL, and paste it into the <webhook_config>/url section of your Alertmanager config file. Reload the Prometheus Alertmanager configuration with one of the two methods: Send a SIGHUP to the process. Send an HTTP POST request to the /-/reload endpoint. REST API Incident Intelligence supports a dedicated REST API interface that lets you integrate with additional systems. The interface allows instrumentation of your code or other monitoring solutions to report any kind of metric or event. A metric can be a raw data point such as CPU, memory, disk utilization, or business KPI. An event can be a monitoring alert, deployment event, incident, exceptions or any other change in state that you want to describe. You can also send any type of data to Incident Intelligence straight from your own systems or applications. The REST API supports secure token-based authentication and accepts JSON content as input. For more information on authentication and the full API reference, see REST API for New Relic Applied Intelligence. Splunk By integrating Incident Intelligence with your Splunk log monitoring, you can: Use your environment's log data for searches and key term reports. Correlate alerts and search reports with your other metrics and incidents. Important Applied Intelligence supports Splunk Light, Splunk Cloud, and Splunk Enterprise version 6.3 and higher. To get data from Splunk: In your Splunk console, start a search for the relevant events. Save your search as an alert, configure your alert conditions, and then choose the webhook as the delivery method. Go to one.newrelic.com and click Alerts & AI. On the left under Incident Intelligence, click Sources and then click Splunk. Copy the collector URL, and paste it into the webhook endpoint in the Splunk console. Optional: Use Splunk tokens to enrich alert data with Splunk metadata. Splunk metadata To enrich alerts data with your Splunk metadata, use Splunk tokens. This helps you leverage your search data, which includes metadata and values from the first row of search results. If you want to... Do this... Access search data Use the format $<fieldname>$. For example, use $app$ for the app context for the search. Access field values To access field values from the first result row that a search returns, use the format $result.<fieldname>$. For example, use $result.host$ for the host value and $result.sourcetype$ for the source type. Use variables You can leverage any of the Selected fields in the Splunk search and add any unique fields to the Selected fields to make the data available as a variable. The following fields will automatically provide hints to the correlation engine: app: parsed as APPLICATION_NAME application:parsed as APPLICATION_NAME application_name: parsed as APPLICATION_NAME cluster: parsed as CLUSTER_NAME computer: parsed as HOST_NAME Dc: parsed as DATACENTER_NAME datacenter: parsed as DATACENTER_NAME host: parsed as HOST_NAME host_name: parsed as HOST_NAME hostname: parsed as HOST_NAME transaction: parsed as EVENT_ID Transaction_id: parsed as EVENT_ID user: parsed as USER_NAME 3. Configure destinations (ServiceNow and others) Now that you've set up your sources, you can configure your destinations. These are the data outputs where you view your incidents. Configure ServiceNow (example) Using ServiceNow as a destination enables you to push valuable violation data into new ServiceNow incident tickets. Send data to ServiceNow To configure Incident Intelligence to send data to ServiceNow: Go to one.newrelic.com, click Alerts & AI, in the left nav under Incident Intelligence click Destinations, then click ServiceNow. Required: Enter a channel name. This is used internally in Applied Intelligence to identify the destination (for example, in Pathways). Required: Enter your ServiceNow credentials: Team domain (This must be unique. No two destinations can have the same domain). Username Password Follow the two-way integration on screen instructions: Open and download this XML file. In the ServiceNow sidebar menu, go to System Definition > Business Rule. Click the menu icon in one of the column headers, select Import XML, and upload the XML file you downloaded. The two way integration will allow the ServiceNow incident to be updated with changes to the Applied Intelligence issue. Closing a ServiceNow incident will close its corresponding New Relic issue. When a New Relic issue is resolved, the corresponding ServiceNow incident will be closed. Custom notification message Applied Intelligence uses a templating framework called Jinja2 in the customization section interface. The Value field must be in valid Jinja syntax. By default, the interface populates a set of default fields in ServiceNow. When you add a custom field, enter the ServiceNow field name you want to use. When you want to skip a selected field in an issue update, add the | skip_on_update string at the end of the value you've selected. Tip By default, ServiceNow adds u_ to the beginning of its custom values. When mapping to ServiceNow attributes, use the Column name value. Please note that the name needs to be lowercase separated by underscores. Go here to see the custom notification message attribute descriptions. Go here to see Jinja2 Useful Syntax. Other destinations You can set other destinations: Send data to PagerDuty EOL NOTICE As of October 2021, we've discontinued support for several capabilities with PagerDuty, including suggested responders, golden signals, and component enrichment. For more details, including how you can easily make this transition, see our Explorers Hub post. Recommended: Create a new PagerDuty service to use as a destination. Because PagerDuty services can also be used as sources, this can help you distinguish your data input from your output. To create a PagerDuty destination: Go to one.newrelic.com, click Alerts & AI, in the left nav under Incident Intelligence click Destinations, then click PagerDuty. Enter your PagerDuty API key. The key should be either a personal or general access key with write access. If it's created by a user, the user should be an admin. If you've configured a PagerDuty source with an API key, you can use the same key. Select the PagerDuty services you want to connect to Applied Intelligence, and click Connect. When you're ready, you can add policies for one or more PagerDuty destinations. You can also transfer the policies over from your existing services or leave them as sources if needed. From the Destinations > PagerDuty page, you can also: Review the permissions for your services. Click Authorize when you're done. Add or delete existing services from the PagerDuty destination. Edit permissions for any service. To configure your PagerDuty destinations, use the following settings: Configuration setting Description Trigger new incidents Required. Trigger correlated parent incidents so you can identify issues faster. Edit incident titles Required. Alter your incident titles to help you orient and understand issues. Add new integrations Required. Add integrations to enable incident creation for selected services. Add webhook extensions Add webhook extensions to sync user actions in PagerDuty to New Relic. This lets you update the correlated issue state. Auto-resolve correlated incidents When enabled, this will resolve and automatically close correlated parent/child incidents. Select a user to take actions in PagerDuty You need to select a user before you can enable deep integration with PagerDuty. Once you do, the user can: Add notes to incidents (required): Incident notes are used to enrich incidents with context. Acknowledge triggered incidents: When enabled, Applied Intelligence will acknowledge and correlate newly triggered incidents in PagerDuty before you're notified. Use the original escalation policy: When enabled, the escalation policy of the source service will be applied to each incident. Send data via webhook Incident Intelligence will send the event body in JSON format by HTTPS POST. The system expects the endpoint to return a successful HTTP code (2xx). To configure Incident Intelligence to send data via webhook: Go to one.newrelic.com, click Alerts & AI, in the left nav under Incident Intelligence click Destinations, then click Webhook. Required: Configure the unique webhook key, used in Applied Intelligence to refer to this webhook configuration and its specific settings. Required: Configure the destination endpoint where the webhook payload will be sent. Optional steps: Configure custom headers, which are key:value pairs of headers to be sent with the request. Example: \"Authentication\" \"Bearer\" <bearer token> Configure a custom payload template that can be used to map New Relic fields to match the destination tool's expected name and format. Configure priority mapping (critical, high, medium, or low), used to map New Relic's priorities to the priorities expected at the destination. Tip There’s a retry mechanism that is triggered a few times with exponential backoff for a couple of minutes once an error occurs. If we reach the retry limit, the Webhook will get auto-disabled. For examples of destination templates, webhook formats, and JSON schema, see the Incident Intelligence destination examples. 4. Configure pathways To control when and where you want to receive notifications from your incidents, you can configure pathways. To add a pathway: Go to one.newrelic.com, click Alerts & AI, in the left nav under Incident Intelligence click Pathways, then click Add a pathway. In the query builder box, select an attribute, such as application/name. This can be from the list of all attributes available in PagerDuty incidents and New Relic alerts violations, or you can add your own attributes. Select a logical operator. For example, contains. Enter a specific value to complete the logical expression. To include all issues created by your sources, select Send everything. (Use this if you only use one PagerDuty service to manage all incidents.) To build more complex logic, use the AND/OR operators. Select one or more of your destinations. To edit or remove existing pathways, mouse over the pathway's name on the Pathways page. What's next? Now that you've set up some sources and destinations for your incidents, read about how to use Incident Intelligence.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 144.30243,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Get started <em>with</em> Incident <em>Intelligence</em>",
        "sections": "Get started <em>with</em> Incident <em>Intelligence</em>",
        "tags": "Alerts and <em>Applied</em> <em>Intelligence</em>",
        "body": "As part of <em>Applied</em> <em>Intelligence</em>, Incident <em>Intelligence</em> helps you correlate your incidents and reduce noise in your environment. It gives you an overview of all your incidents, their sources, and related events. Requirements If you haven&#x27;t already, sign up for a New Relic account to get started"
      },
      "id": "603ea62e64441f119f4e883f"
    }
  ],
  "/docs/alerts-applied-intelligence/applied-intelligence/incident-intelligence/eu-us-datacenters-incident-intelligence": [
    {
      "sections": [
        "Alerts and applied intelligence notification integrations",
        "Early access",
        "Integration details",
        "Atlassian Jira",
        "Permissions",
        "Set up a Jira destination",
        "Important",
        "Two-way sync",
        "Configure the message template",
        "Send a test notification",
        "ServiceNow (Incident-Management)",
        "Roles",
        "Set up a destination",
        "Slack",
        "Prerequisites",
        "Set up a Slack destination",
        "Configure the Slack message settings",
        "Webhook",
        "Set up a webhook destination",
        "Configure the webhook event template",
        "Customize the webhook payload",
        "Email",
        "Configure the email settings"
      ],
      "title": "Alerts and applied intelligence notification integrations",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Incident Intelligence",
        "Destinations"
      ],
      "external_id": "7220c630fc187bb61784ff2cc2213e588b269b00",
      "image": "https://docs.newrelic.com/static/d4e9baecc3a76dd1a5945f8ec0aeca66/c1b63/webhook-notification-template.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/notifications/notification-integrations/",
      "published_at": "2021-12-19T15:28:40Z",
      "updated_at": "2021-12-19T15:28:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Early access The features described here are early access. You won't be able to use these features if you're not part of the early access program. For more information on related features, see our docs on Alerts notification channels, Incident Intelligence destinations, and Proactive Detection notifications. Alerts and Applied Intelligence notification integrations are specific services and platforms you can use to send notifications from New Relic. Integration details Read more about each of our specific notification integrations. Atlassian Jira Integrate New Relic with Atlassian Jira(Cloud) and automatically create and update Jira issues. Permissions The required permissions from the Jira API-Token are create, edit, and close tickets. To enable the two-way sync toggle, the provided Jira API-Key should have an Admin role. Set up a Jira destination Create Jira issues, then enable Jira and New Relic to share updates and stay synced. To create a Jira destination, enter the following information: Destination name: Custom name to identify the destination Jira account endpoint: the URL of the destination User-name: this will be the email address of the user making the connection API token: generated from your Atlassian account Important New Relic currently supports Atlassian-Jira Classic (company-managed) projects. Before saving the destination, we recommend you test the connection via the test connection button. Jira destination configuration. We recommand to test the connection before saving. Two-way sync You can enable a two-way integration with Jira to keep the issues' state synced with the corresponding state in New Relic. To enable two-way sync, turn on the ‘two-way integration’ toggle. When turned on, a Jira Webhook would be created in your Jira account at a later stage, for the selected project (see ‘customize a message template’). The webhook would contain access details to Newrelic (URL and Newrelic-API-KEY) Configure the message template To configure a template for a Jira issue, you first need to choose a destination. You will be able to create a new destination at this stage. Upon successful connection to the destination, you will need to choose a project, and then select the Jira issue type you would like to be used. Once the issue-type is selected, the configured project's fields are fetched from your account and automatically mapped to your Jira instance. To help you get started, we automatically present the required and recommended fields and values. Required fields must be set with a value. You can add or remove optional fields(use the X mark on their right side) Jira message template. Send a test notification You can see how the JIRA issue will appear by clicking a test notification with default field values. If successful, a JIRA issue will be created and a link will appear. ServiceNow (Incident-Management) Integrate New Relic with ServiceNow Incident-Management and automatically create and update incidents. Roles As part of the integrations, we fetch fields from the your serviceNow incident table and optional values. For this, the provided ServiceNow user details required read permissions for the tables: sys_dictionary, sys_choice, sys_user and task. A read/write permission to incident To be able to fetch users for the caller column, we required read permissions for the sys_users table. The above permissions can be achieved with the roles personalize_choices, personalize_dictionary, rest_service, itil. Read/Write permissions to the api_key_credentials table is required to enable two-way integration. This can be covered with the roles credentials_admin and discovery_admin. Set up a destination To create a ServiceNow destination, enter the following information: Destination Name: custom name to identify the destination Domain: the URL of the destination User-name: the name of the user Password: the user name’s password Before saving the destination, we recommend testing the connection by clicking the test connection button. Two-way sync You can configure a two-way integration with ServiceNow Incidents Management to keep the incidents' state synced with the corresponding state in New Relic. Here are some required steps to remember when configuring the two-way integration: Turn on the two-way integration toggle. Open and download this XML file, which includes the business rule triggering events back to New Relic One. In the ServiceNow sidebar menu, go to System Definition > Business Rules. Click the menu icon in one of the column headers, select Import XML and upload the XML file you downloaded. Once the Destination is saved, a New-Relic API-Key will be kept in the api_key_credentials. The key would sent in a header as part of the callback REST call to New-Relic Configure the message template Upon a successful connection, ServiceNow incident table columns are fetched from your account and automatically mapped to your ServiceNow instance. To help you get started, we automatically present the required and recommended fields and values. Required fields must be set with a value. You can add or remove optional fields(use the X mark on their right side) Select, edit or remove fields for the ServiceNow-Incident template. Send a test notification You can see how the ServiceNow incident will appear by clicking a test notification with default field values. If successful, an incident will be created and a link will appear. Slack Send notifications-messages to your Slack channels. Prerequisites Your Slack workspace needs to have the New Relic application installed. The application must be approved by a workspace admin before it can be individually installed by users Set up a Slack destination Click on the `one-click Slack authentication' will lead you to the Slack landing page to continue the OAuth2 authentication process. On the Slack landing page, if you're not signed into the required workspace, you're redirected to Slack to sign in. Add your workspace name or select the relevant workspace and click Continue. When signed in to the selected workspace, you are requested to allow New Relic to perform the specified actions. Clicking `Allow' will redirect you back to the Destination page. Configure the Slack message settings Select a Destination(Workspace) and select a Slack-channel where the messages will be sent. You can create a new destination if there is no pre-defined destination for the required workspace. Note that, for privacy reasons, users need to be authenticated to select private channels (one-time process) Send a test notification You can send a test notification with a pre-defined example payload to the channel. This creates a message in the selected Slack-channel. Webhook Use the webhook notifier to send the notification messages to any endpoint you like. Set up a webhook destination To create a webhook destination, you need the following: Destination Name: A unique destination name URL: the endpoint of the target application, authentication and custom headers if needed. Authorization mechanism (Optional):. Can be basic authentication or a bearer token Configure the webhook event template Pick a webhook destination from the list and configure the HTTP-POST request. The request configuration requires you to: Set a name for the template. Select a pre-configured destination from the destinations list or create a new one. Add custom headers (optional). Configure the request’s payload. Customize the webhook payload You can use the default payload or customize it to contain the required data. Pick Variables from the variables menu and apply handlebars syntax to enrich your webhook. Note that the request’s content-type is JSON by default. Hence, the payload needs to keep the JSON form. See Usage examples The ‘preview’ section on the right hand-side shows an expected payload after the template is rendered. If the eventual payload would not form a valid Json, an error will be shown and it won’t be possible to save the template. If the webhook payload conforms a valid Json, you can send a test notification to your defined webhook destination We recommend sending a test notification to make sure that everything's connected correctly. Email Send email notifications to users. Configure the email settings Add one or more recipients. Users with New Relic accounts can be found via autocomplete when searching for their name or email address. To add a user without a New Relic account or email distribution list, add the full email address. Every recipient will be translated into a 'destination'. You can follow the email notifications per destination in the notifications-log Send a test notification You can send a test notification to make sure the email notifications reach the inbox.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 368.6925,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Alerts</em> <em>and</em> <em>applied</em> <em>intelligence</em> notification integrations",
        "sections": "<em>Alerts</em> <em>and</em> <em>applied</em> <em>intelligence</em> notification integrations",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "Early access The features described here are early access. You won&#x27;t be able to use these features if you&#x27;re not part of the early access program. For more information on related features, see our docs on <em>Alerts</em> notification channels, <em>Incident</em> <em>Intelligence</em> destinations, and Proactive Detection"
      },
      "id": "618ff71628ccbc60710321e4"
    },
    {
      "sections": [
        "Workflows",
        "What is an issue",
        "Add a Workflow",
        "Tip",
        "Enrichments",
        "Important",
        "Notifier"
      ],
      "title": "Workflows",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Workflows",
        "Notifications"
      ],
      "external_id": "0993c323f76467d22136cac6982ccd48e859f722",
      "image": "",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/applied-intelligence/incident-workflows/incident-workflows/",
      "published_at": "2021-12-19T13:46:37Z",
      "updated_at": "2021-12-17T01:48:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With workflows you control when and where you want to receive notifications about issues, tunnel the right information to the relevant person or team, and enrich your issue's notifications with additional New Relic data. What is an issue Issues are groups of incidents that describe the underlying problem of your symptoms. When a new incident is created, Incident Intelligence opens an issue and evaluates other open issues for correlations. For more information see Use Incident Inteligence. Add a Workflow Tip The maximum number of workflows you can add is 100 per environment and 50 per account. The workflows feature is located under the Alerts & AI menu. Go to one.newrelic.com, click Alerts & AI, in the left navigation under Enrich and Notify click Workflow, then click Add a workflow. Name your workflow. This field is mandatory and needs to be unique. Workflows filters the issues you want to send to the relevant destination. You can send all issues by selecting Send all issues or you can build a query to filter the right issues. Build a query by selecting an attribute, operator and value/s to narrow down the right issues you want to be passed on. Optional: Enriched Data, available to full platform users. Enrichments Enrichments can give additional context on alert notifications by adding NRQL query results to them The workflows enrich tool is similar to the query builder. The NRQL query's results are included in the notification message. If you create queries for your workflows, they run in real time and the query results are the value of the enricher. You can save any valid query into your workflow, even if they return an empty result, but you must run the query before saving the enrichment. You can also query with issue-specific variables; for example {{entitiesData.ids}} For enrichment examples, see Workflow data enrichment examples. Note: The query name needs to be unique because you'll use the name to decide how to include the result in the payload of the notification that will be sent by the workflow. Important At this stage, visual representation of enrichments are sent to Slack and email. For Servicenow and JIRA destinations, we recommend to limit to NRDB queries with single value results. For example: count, min, or max. The maximum number of enrichments per workflow is 5. The maximum number of variables an enrichment can contain is 1. Notify: Choose one or more destinations and add an optional message. Notifier To save and activate a Workflow, you need to include a notifier. Setting up a notifier requires having a notification destination configured. You can either set it up through the Destination menu item, or through the Create new destination item in the destination selection drop down in each destination channel screen. A notification destination includes the credentials required to send a notification to a third party platform and an option to customize the message sent by mapping fields from our issues to fields in the destination platform. To learn how to set up destinations and configure message templates, check the documentation on notifications. Click update message once completing the notifier requirements Optionally, test your workflow. We'll use existing data from your account to test what you've configured and send a sample notification Click activate to complete the workflow. Tip In any destination channel, start typing and a variable menu will open up. You'll see the names of the variables, which will be replaced with the variable's values at runtime. To use the enrichers' results, use their name.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 292.5609,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": " problem of your symptoms. When a new <em>incident</em> is created, <em>Incident</em> <em>Intelligence</em> opens an issue and evaluates other open issues for correlations. For more information see Use <em>Incident</em> Inteligence. Add a Workflow Tip The maximum number of workflows you can add is 100 per environment and 50 per account"
      },
      "id": "603e967664441f7e6f4e889b"
    },
    {
      "sections": [
        "Get started with Incident Intelligence",
        "Requirements",
        "Set up Incident Intelligence",
        "1. Configure your environment (one-time)",
        "Tip",
        "2. Configure sources",
        "Alerts",
        "Datarobot (formerly Algorithmia)",
        "Aporia (MLOps)",
        "Superwise (MLOps)",
        "Anomalies",
        "AWS",
        "Grafana",
        "PagerDuty",
        "EOL NOTICE",
        "Prometheus Alertmanager",
        "REST API",
        "Splunk",
        "Important",
        "Splunk metadata",
        "3. Configure destinations (ServiceNow and others)",
        "Configure ServiceNow (example)",
        "Send data to ServiceNow",
        "Custom notification message",
        "Other destinations",
        "Send data to PagerDuty",
        "Send data via webhook",
        "4. Configure pathways",
        "What's next?"
      ],
      "title": "Get started with Incident Intelligence",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Incident intelligence"
      ],
      "external_id": "8c4a5a914ca01cb42250908d2fb1a12ccc697e25",
      "image": "https://docs.newrelic.com/static/5cb28999dc618c6a5a2b8be1fa72e660/b97f6/image-%25281%2529_0.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/applied-intelligence/incident-intelligence/get-started-incident-intelligence/",
      "published_at": "2021-12-19T14:23:58Z",
      "updated_at": "2021-12-09T22:19:28Z",
      "document_type": "page",
      "popularity": 1,
      "body": "As part of Applied Intelligence, Incident Intelligence helps you correlate your incidents and reduce noise in your environment. It gives you an overview of all your incidents, their sources, and related events. Requirements If you haven't already, sign up for a New Relic account to get started. To use most Applied Intelligence features, you must be a full platform user. For more information, see user type capabilities. Set up Incident Intelligence To enable Incident Intelligence, follow these steps. Afterwards, issues should start to appear in your issue feed. 1. Configure your environment (one-time). 2. Configure sources. 3. Configure destinations. 4. Configure pathways. 1. Configure your environment (one-time) To set up an environment in Incident Intelligence, you need an administrator to select a New Relic account for it. This account should be the one your team is using. Who sets the environment? Only administrators, and only for accounts where they have admin privileges. Can administrators set more than one environment? They can set one environment per parent account and its child accounts. More than one can be set if an administrator has privileges for more than one parent account. Tip Incident Intelligence is a cross-account product. This means you can send in data from any New Relic account or external source to correlate events. 2. Configure sources After setting up your environment, determine your incident sources. These are your data inputs. You can get data from any of the following sources: Alerts By integrating Incident Intelligence with your alerts violations, you can get context and correlations from what you're monitoring. To get data from alerts: From one.newrelic.com, click Alerts. On the left under Incident Intelligence, click Sources and then click Alerts. Select the policies you want to connect to Applied Intelligence, and click Connect. You can add additional alerts policies or remove policies you've already connected in Sources > New Relic Alerts. Tip Adding alerts as a source will not affect your current configuration or notifications. Datarobot (formerly Algorithmia) By integrating Incident Intelligence with your Datarobot machine-learning models, you can monitor your machine learning model performance. To configure Datarobot for Incident Intelligence, see our integration docs. Aporia (MLOps) By integrating Incident Intelligence with your Aporia machine-learning models, you can monitor your machine learning model performance. To configure our Aporia integration, see our docs. Superwise (MLOps) By integrating Incident Intelligence with your Superwise machine-learning models, you can monitor your machine learning model performance. To configure our Superwise integration, see our docs. Anomalies By integrating Incident Intelligence with your New Relic Proactive Detection anomalies, you can get context and correlations from what you're monitoring. To get data from New Relic Proactive Detection anomalies: From one.newrelic.com, click Alerts & AI. On the left under Incident Intelligence, click Sources and then click Anomalies. Select the configurations you want to connect to Applied Intelligence by toggling to on (green). To add or remove alert policies, from Alerts & AI, click Sources, then Alerts. Tip Adding anomalies as a source won't affect your current Proactive Detection configurations or notifications. AWS You can integrate Incident Intelligence with Amazon CloudWatch to provide incident management for all of your AWS services. To integrate Amazon CloudWatch: Go to one.newrelic.com and click Alerts & AI. On the left under Incident Intelligence, click Sources and then click Amazon Web Services. Copy the URL. Create a new Amazon SNS topic. Set CloudWatch to forward all Alarms state changes to that topic: In the Amazon CloudWatch UI, click Events > Event Pattern. Select Service Name > CloudWatch. Select Event Type > CloudWatch Alarm State Change. Select Targets > SNS Topic, and select your new Amazon SNS topic. Create a new subscription: In the Amazon AWS UI, click Create a Subscription. Select your new Amazon SNS topic. Select Protocol > choose HTTPS. In Endpoint, paste the URL you previously copied from the Applied Intelligence Sources. Grafana You can integrate Incident Intelligence with Grafana's notifications for insight into events across your applications and environment. Grafana's webhook notification is a simple way to send information over HTTP to a custom endpoint. To integrate Grafana as a new webhook: Log into your Grafana portal using Admin permissions, and choose Alerting. On the Grafana Notification Channels page, click New Channel > Webhook. Go to one.newrelic.com and click Alerts & AI. On the left under Incident Intelligence, click Sources, and then click Grafana. Copy the URL, and paste it into your new Grafana webhook. PagerDuty EOL NOTICE As of October 2021, we've discontinued support for several capabilities with PagerDuty, including suggested responders, golden signals, and component enrichment. For more details, including how you can easily make this transition, see our Explorers Hub post. You can integrate Incident Intelligence directly with your PagerDuty services to ingest, process, and enhance all of your PagerDuty incidents. Connecting PagerDuty services to Applied Intelligence will not affect your current services or notifications. To get data from PagerDuty: Make sure your PagerDuty API key has write access. From one.newrelic.com, click Alerts & AI. On the left under Incident Intelligence, click Sources and then click PagerDuty. Enter your PagerDuty API key. The key should be either a personal or general access key with write access. If it's created by a user, the user should be an admin. Select the PagerDuty services you want to connect to Applied Intelligence, and click Connect. You can add additional services or remove services you've already connected in Sources > PagerDuty. Prometheus Alertmanager By integrating Incident Intelligence with Prometheus Alertmanager, you can receive and correlate your Prometheus alerts with events from other sources. To integrate Prometheus Alertmanager: Set up your Alertmanager configuration file by running: ./alertmanager -config.file=simple.yml Copy Go to one.newrelic.com and click Alerts & AI. On the left under Incident Intelligence, click Sources and then click Prometheus Alertmanager. Copy the Prometheus Alertmanager URL, and paste it into the <webhook_config>/url section of your Alertmanager config file. Reload the Prometheus Alertmanager configuration with one of the two methods: Send a SIGHUP to the process. Send an HTTP POST request to the /-/reload endpoint. REST API Incident Intelligence supports a dedicated REST API interface that lets you integrate with additional systems. The interface allows instrumentation of your code or other monitoring solutions to report any kind of metric or event. A metric can be a raw data point such as CPU, memory, disk utilization, or business KPI. An event can be a monitoring alert, deployment event, incident, exceptions or any other change in state that you want to describe. You can also send any type of data to Incident Intelligence straight from your own systems or applications. The REST API supports secure token-based authentication and accepts JSON content as input. For more information on authentication and the full API reference, see REST API for New Relic Applied Intelligence. Splunk By integrating Incident Intelligence with your Splunk log monitoring, you can: Use your environment's log data for searches and key term reports. Correlate alerts and search reports with your other metrics and incidents. Important Applied Intelligence supports Splunk Light, Splunk Cloud, and Splunk Enterprise version 6.3 and higher. To get data from Splunk: In your Splunk console, start a search for the relevant events. Save your search as an alert, configure your alert conditions, and then choose the webhook as the delivery method. Go to one.newrelic.com and click Alerts & AI. On the left under Incident Intelligence, click Sources and then click Splunk. Copy the collector URL, and paste it into the webhook endpoint in the Splunk console. Optional: Use Splunk tokens to enrich alert data with Splunk metadata. Splunk metadata To enrich alerts data with your Splunk metadata, use Splunk tokens. This helps you leverage your search data, which includes metadata and values from the first row of search results. If you want to... Do this... Access search data Use the format $<fieldname>$. For example, use $app$ for the app context for the search. Access field values To access field values from the first result row that a search returns, use the format $result.<fieldname>$. For example, use $result.host$ for the host value and $result.sourcetype$ for the source type. Use variables You can leverage any of the Selected fields in the Splunk search and add any unique fields to the Selected fields to make the data available as a variable. The following fields will automatically provide hints to the correlation engine: app: parsed as APPLICATION_NAME application:parsed as APPLICATION_NAME application_name: parsed as APPLICATION_NAME cluster: parsed as CLUSTER_NAME computer: parsed as HOST_NAME Dc: parsed as DATACENTER_NAME datacenter: parsed as DATACENTER_NAME host: parsed as HOST_NAME host_name: parsed as HOST_NAME hostname: parsed as HOST_NAME transaction: parsed as EVENT_ID Transaction_id: parsed as EVENT_ID user: parsed as USER_NAME 3. Configure destinations (ServiceNow and others) Now that you've set up your sources, you can configure your destinations. These are the data outputs where you view your incidents. Configure ServiceNow (example) Using ServiceNow as a destination enables you to push valuable violation data into new ServiceNow incident tickets. Send data to ServiceNow To configure Incident Intelligence to send data to ServiceNow: Go to one.newrelic.com, click Alerts & AI, in the left nav under Incident Intelligence click Destinations, then click ServiceNow. Required: Enter a channel name. This is used internally in Applied Intelligence to identify the destination (for example, in Pathways). Required: Enter your ServiceNow credentials: Team domain (This must be unique. No two destinations can have the same domain). Username Password Follow the two-way integration on screen instructions: Open and download this XML file. In the ServiceNow sidebar menu, go to System Definition > Business Rule. Click the menu icon in one of the column headers, select Import XML, and upload the XML file you downloaded. The two way integration will allow the ServiceNow incident to be updated with changes to the Applied Intelligence issue. Closing a ServiceNow incident will close its corresponding New Relic issue. When a New Relic issue is resolved, the corresponding ServiceNow incident will be closed. Custom notification message Applied Intelligence uses a templating framework called Jinja2 in the customization section interface. The Value field must be in valid Jinja syntax. By default, the interface populates a set of default fields in ServiceNow. When you add a custom field, enter the ServiceNow field name you want to use. When you want to skip a selected field in an issue update, add the | skip_on_update string at the end of the value you've selected. Tip By default, ServiceNow adds u_ to the beginning of its custom values. When mapping to ServiceNow attributes, use the Column name value. Please note that the name needs to be lowercase separated by underscores. Go here to see the custom notification message attribute descriptions. Go here to see Jinja2 Useful Syntax. Other destinations You can set other destinations: Send data to PagerDuty EOL NOTICE As of October 2021, we've discontinued support for several capabilities with PagerDuty, including suggested responders, golden signals, and component enrichment. For more details, including how you can easily make this transition, see our Explorers Hub post. Recommended: Create a new PagerDuty service to use as a destination. Because PagerDuty services can also be used as sources, this can help you distinguish your data input from your output. To create a PagerDuty destination: Go to one.newrelic.com, click Alerts & AI, in the left nav under Incident Intelligence click Destinations, then click PagerDuty. Enter your PagerDuty API key. The key should be either a personal or general access key with write access. If it's created by a user, the user should be an admin. If you've configured a PagerDuty source with an API key, you can use the same key. Select the PagerDuty services you want to connect to Applied Intelligence, and click Connect. When you're ready, you can add policies for one or more PagerDuty destinations. You can also transfer the policies over from your existing services or leave them as sources if needed. From the Destinations > PagerDuty page, you can also: Review the permissions for your services. Click Authorize when you're done. Add or delete existing services from the PagerDuty destination. Edit permissions for any service. To configure your PagerDuty destinations, use the following settings: Configuration setting Description Trigger new incidents Required. Trigger correlated parent incidents so you can identify issues faster. Edit incident titles Required. Alter your incident titles to help you orient and understand issues. Add new integrations Required. Add integrations to enable incident creation for selected services. Add webhook extensions Add webhook extensions to sync user actions in PagerDuty to New Relic. This lets you update the correlated issue state. Auto-resolve correlated incidents When enabled, this will resolve and automatically close correlated parent/child incidents. Select a user to take actions in PagerDuty You need to select a user before you can enable deep integration with PagerDuty. Once you do, the user can: Add notes to incidents (required): Incident notes are used to enrich incidents with context. Acknowledge triggered incidents: When enabled, Applied Intelligence will acknowledge and correlate newly triggered incidents in PagerDuty before you're notified. Use the original escalation policy: When enabled, the escalation policy of the source service will be applied to each incident. Send data via webhook Incident Intelligence will send the event body in JSON format by HTTPS POST. The system expects the endpoint to return a successful HTTP code (2xx). To configure Incident Intelligence to send data via webhook: Go to one.newrelic.com, click Alerts & AI, in the left nav under Incident Intelligence click Destinations, then click Webhook. Required: Configure the unique webhook key, used in Applied Intelligence to refer to this webhook configuration and its specific settings. Required: Configure the destination endpoint where the webhook payload will be sent. Optional steps: Configure custom headers, which are key:value pairs of headers to be sent with the request. Example: \"Authentication\" \"Bearer\" <bearer token> Configure a custom payload template that can be used to map New Relic fields to match the destination tool's expected name and format. Configure priority mapping (critical, high, medium, or low), used to map New Relic's priorities to the priorities expected at the destination. Tip There’s a retry mechanism that is triggered a few times with exponential backoff for a couple of minutes once an error occurs. If we reach the retry limit, the Webhook will get auto-disabled. For examples of destination templates, webhook formats, and JSON schema, see the Incident Intelligence destination examples. 4. Configure pathways To control when and where you want to receive notifications from your incidents, you can configure pathways. To add a pathway: Go to one.newrelic.com, click Alerts & AI, in the left nav under Incident Intelligence click Pathways, then click Add a pathway. In the query builder box, select an attribute, such as application/name. This can be from the list of all attributes available in PagerDuty incidents and New Relic alerts violations, or you can add your own attributes. Select a logical operator. For example, contains. Enter a specific value to complete the logical expression. To include all issues created by your sources, select Send everything. (Use this if you only use one PagerDuty service to manage all incidents.) To build more complex logic, use the AND/OR operators. Select one or more of your destinations. To edit or remove existing pathways, mouse over the pathway's name on the Pathways page. What's next? Now that you've set up some sources and destinations for your incidents, read about how to use Incident Intelligence.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.1733,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Get started with <em>Incident</em> <em>Intelligence</em>",
        "sections": "Get started with <em>Incident</em> <em>Intelligence</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": " one.newrelic.com, click <em>Alerts</em> &amp; AI. On the left under <em>Incident</em> <em>Intelligence</em>, click Sources and then click Anomalies. Select the configurations you want to connect to <em>Applied</em> <em>Intelligence</em> by toggling to on (green). To add or remove <em>alert</em> policies, from <em>Alerts</em> &amp; AI, click Sources, then <em>Alerts</em>. Tip Adding anomalies"
      },
      "id": "603ea62e64441f119f4e883f"
    }
  ],
  "/docs/alerts-applied-intelligence/applied-intelligence/incident-intelligence/get-started-incident-intelligence": [
    {
      "sections": [
        "Alerts and applied intelligence notification integrations",
        "Early access",
        "Integration details",
        "Atlassian Jira",
        "Permissions",
        "Set up a Jira destination",
        "Important",
        "Two-way sync",
        "Configure the message template",
        "Send a test notification",
        "ServiceNow (Incident-Management)",
        "Roles",
        "Set up a destination",
        "Slack",
        "Prerequisites",
        "Set up a Slack destination",
        "Configure the Slack message settings",
        "Webhook",
        "Set up a webhook destination",
        "Configure the webhook event template",
        "Customize the webhook payload",
        "Email",
        "Configure the email settings"
      ],
      "title": "Alerts and applied intelligence notification integrations",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Incident Intelligence",
        "Destinations"
      ],
      "external_id": "7220c630fc187bb61784ff2cc2213e588b269b00",
      "image": "https://docs.newrelic.com/static/d4e9baecc3a76dd1a5945f8ec0aeca66/c1b63/webhook-notification-template.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/notifications/notification-integrations/",
      "published_at": "2021-12-19T15:28:40Z",
      "updated_at": "2021-12-19T15:28:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Early access The features described here are early access. You won't be able to use these features if you're not part of the early access program. For more information on related features, see our docs on Alerts notification channels, Incident Intelligence destinations, and Proactive Detection notifications. Alerts and Applied Intelligence notification integrations are specific services and platforms you can use to send notifications from New Relic. Integration details Read more about each of our specific notification integrations. Atlassian Jira Integrate New Relic with Atlassian Jira(Cloud) and automatically create and update Jira issues. Permissions The required permissions from the Jira API-Token are create, edit, and close tickets. To enable the two-way sync toggle, the provided Jira API-Key should have an Admin role. Set up a Jira destination Create Jira issues, then enable Jira and New Relic to share updates and stay synced. To create a Jira destination, enter the following information: Destination name: Custom name to identify the destination Jira account endpoint: the URL of the destination User-name: this will be the email address of the user making the connection API token: generated from your Atlassian account Important New Relic currently supports Atlassian-Jira Classic (company-managed) projects. Before saving the destination, we recommend you test the connection via the test connection button. Jira destination configuration. We recommand to test the connection before saving. Two-way sync You can enable a two-way integration with Jira to keep the issues' state synced with the corresponding state in New Relic. To enable two-way sync, turn on the ‘two-way integration’ toggle. When turned on, a Jira Webhook would be created in your Jira account at a later stage, for the selected project (see ‘customize a message template’). The webhook would contain access details to Newrelic (URL and Newrelic-API-KEY) Configure the message template To configure a template for a Jira issue, you first need to choose a destination. You will be able to create a new destination at this stage. Upon successful connection to the destination, you will need to choose a project, and then select the Jira issue type you would like to be used. Once the issue-type is selected, the configured project's fields are fetched from your account and automatically mapped to your Jira instance. To help you get started, we automatically present the required and recommended fields and values. Required fields must be set with a value. You can add or remove optional fields(use the X mark on their right side) Jira message template. Send a test notification You can see how the JIRA issue will appear by clicking a test notification with default field values. If successful, a JIRA issue will be created and a link will appear. ServiceNow (Incident-Management) Integrate New Relic with ServiceNow Incident-Management and automatically create and update incidents. Roles As part of the integrations, we fetch fields from the your serviceNow incident table and optional values. For this, the provided ServiceNow user details required read permissions for the tables: sys_dictionary, sys_choice, sys_user and task. A read/write permission to incident To be able to fetch users for the caller column, we required read permissions for the sys_users table. The above permissions can be achieved with the roles personalize_choices, personalize_dictionary, rest_service, itil. Read/Write permissions to the api_key_credentials table is required to enable two-way integration. This can be covered with the roles credentials_admin and discovery_admin. Set up a destination To create a ServiceNow destination, enter the following information: Destination Name: custom name to identify the destination Domain: the URL of the destination User-name: the name of the user Password: the user name’s password Before saving the destination, we recommend testing the connection by clicking the test connection button. Two-way sync You can configure a two-way integration with ServiceNow Incidents Management to keep the incidents' state synced with the corresponding state in New Relic. Here are some required steps to remember when configuring the two-way integration: Turn on the two-way integration toggle. Open and download this XML file, which includes the business rule triggering events back to New Relic One. In the ServiceNow sidebar menu, go to System Definition > Business Rules. Click the menu icon in one of the column headers, select Import XML and upload the XML file you downloaded. Once the Destination is saved, a New-Relic API-Key will be kept in the api_key_credentials. The key would sent in a header as part of the callback REST call to New-Relic Configure the message template Upon a successful connection, ServiceNow incident table columns are fetched from your account and automatically mapped to your ServiceNow instance. To help you get started, we automatically present the required and recommended fields and values. Required fields must be set with a value. You can add or remove optional fields(use the X mark on their right side) Select, edit or remove fields for the ServiceNow-Incident template. Send a test notification You can see how the ServiceNow incident will appear by clicking a test notification with default field values. If successful, an incident will be created and a link will appear. Slack Send notifications-messages to your Slack channels. Prerequisites Your Slack workspace needs to have the New Relic application installed. The application must be approved by a workspace admin before it can be individually installed by users Set up a Slack destination Click on the `one-click Slack authentication' will lead you to the Slack landing page to continue the OAuth2 authentication process. On the Slack landing page, if you're not signed into the required workspace, you're redirected to Slack to sign in. Add your workspace name or select the relevant workspace and click Continue. When signed in to the selected workspace, you are requested to allow New Relic to perform the specified actions. Clicking `Allow' will redirect you back to the Destination page. Configure the Slack message settings Select a Destination(Workspace) and select a Slack-channel where the messages will be sent. You can create a new destination if there is no pre-defined destination for the required workspace. Note that, for privacy reasons, users need to be authenticated to select private channels (one-time process) Send a test notification You can send a test notification with a pre-defined example payload to the channel. This creates a message in the selected Slack-channel. Webhook Use the webhook notifier to send the notification messages to any endpoint you like. Set up a webhook destination To create a webhook destination, you need the following: Destination Name: A unique destination name URL: the endpoint of the target application, authentication and custom headers if needed. Authorization mechanism (Optional):. Can be basic authentication or a bearer token Configure the webhook event template Pick a webhook destination from the list and configure the HTTP-POST request. The request configuration requires you to: Set a name for the template. Select a pre-configured destination from the destinations list or create a new one. Add custom headers (optional). Configure the request’s payload. Customize the webhook payload You can use the default payload or customize it to contain the required data. Pick Variables from the variables menu and apply handlebars syntax to enrich your webhook. Note that the request’s content-type is JSON by default. Hence, the payload needs to keep the JSON form. See Usage examples The ‘preview’ section on the right hand-side shows an expected payload after the template is rendered. If the eventual payload would not form a valid Json, an error will be shown and it won’t be possible to save the template. If the webhook payload conforms a valid Json, you can send a test notification to your defined webhook destination We recommend sending a test notification to make sure that everything's connected correctly. Email Send email notifications to users. Configure the email settings Add one or more recipients. Users with New Relic accounts can be found via autocomplete when searching for their name or email address. To add a user without a New Relic account or email distribution list, add the full email address. Every recipient will be translated into a 'destination'. You can follow the email notifications per destination in the notifications-log Send a test notification You can send a test notification to make sure the email notifications reach the inbox.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 368.6925,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Alerts</em> <em>and</em> <em>applied</em> <em>intelligence</em> notification integrations",
        "sections": "<em>Alerts</em> <em>and</em> <em>applied</em> <em>intelligence</em> notification integrations",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "Early access The features described here are early access. You won&#x27;t be able to use these features if you&#x27;re not part of the early access program. For more information on related features, see our docs on <em>Alerts</em> notification channels, <em>Incident</em> <em>Intelligence</em> destinations, and Proactive Detection"
      },
      "id": "618ff71628ccbc60710321e4"
    },
    {
      "sections": [
        "Workflows",
        "What is an issue",
        "Add a Workflow",
        "Tip",
        "Enrichments",
        "Important",
        "Notifier"
      ],
      "title": "Workflows",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Workflows",
        "Notifications"
      ],
      "external_id": "0993c323f76467d22136cac6982ccd48e859f722",
      "image": "",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/applied-intelligence/incident-workflows/incident-workflows/",
      "published_at": "2021-12-19T13:46:37Z",
      "updated_at": "2021-12-17T01:48:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With workflows you control when and where you want to receive notifications about issues, tunnel the right information to the relevant person or team, and enrich your issue's notifications with additional New Relic data. What is an issue Issues are groups of incidents that describe the underlying problem of your symptoms. When a new incident is created, Incident Intelligence opens an issue and evaluates other open issues for correlations. For more information see Use Incident Inteligence. Add a Workflow Tip The maximum number of workflows you can add is 100 per environment and 50 per account. The workflows feature is located under the Alerts & AI menu. Go to one.newrelic.com, click Alerts & AI, in the left navigation under Enrich and Notify click Workflow, then click Add a workflow. Name your workflow. This field is mandatory and needs to be unique. Workflows filters the issues you want to send to the relevant destination. You can send all issues by selecting Send all issues or you can build a query to filter the right issues. Build a query by selecting an attribute, operator and value/s to narrow down the right issues you want to be passed on. Optional: Enriched Data, available to full platform users. Enrichments Enrichments can give additional context on alert notifications by adding NRQL query results to them The workflows enrich tool is similar to the query builder. The NRQL query's results are included in the notification message. If you create queries for your workflows, they run in real time and the query results are the value of the enricher. You can save any valid query into your workflow, even if they return an empty result, but you must run the query before saving the enrichment. You can also query with issue-specific variables; for example {{entitiesData.ids}} For enrichment examples, see Workflow data enrichment examples. Note: The query name needs to be unique because you'll use the name to decide how to include the result in the payload of the notification that will be sent by the workflow. Important At this stage, visual representation of enrichments are sent to Slack and email. For Servicenow and JIRA destinations, we recommend to limit to NRDB queries with single value results. For example: count, min, or max. The maximum number of enrichments per workflow is 5. The maximum number of variables an enrichment can contain is 1. Notify: Choose one or more destinations and add an optional message. Notifier To save and activate a Workflow, you need to include a notifier. Setting up a notifier requires having a notification destination configured. You can either set it up through the Destination menu item, or through the Create new destination item in the destination selection drop down in each destination channel screen. A notification destination includes the credentials required to send a notification to a third party platform and an option to customize the message sent by mapping fields from our issues to fields in the destination platform. To learn how to set up destinations and configure message templates, check the documentation on notifications. Click update message once completing the notifier requirements Optionally, test your workflow. We'll use existing data from your account to test what you've configured and send a sample notification Click activate to complete the workflow. Tip In any destination channel, start typing and a variable menu will open up. You'll see the names of the variables, which will be replaced with the variable's values at runtime. To use the enrichers' results, use their name.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 292.5609,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": " problem of your symptoms. When a new <em>incident</em> is created, <em>Incident</em> <em>Intelligence</em> opens an issue and evaluates other open issues for correlations. For more information see Use <em>Incident</em> Inteligence. Add a Workflow Tip The maximum number of workflows you can add is 100 per environment and 50 per account"
      },
      "id": "603e967664441f7e6f4e889b"
    },
    {
      "sections": [
        "Notification message templates",
        "Early access",
        "Message template variables",
        "The variables menu",
        "Use the Handlebars syntax",
        "Helper functions",
        "JSON",
        "Equality",
        "Replace",
        "Usage examples",
        "Validate data",
        "Return JSON",
        "Get values from an array",
        "Iterate through an array",
        "Handle missing attributes"
      ],
      "title": "Notification message templates",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Incident Intelligence",
        "Notification templates"
      ],
      "external_id": "17c0cb0905ad9d7fad7c31c814704279312f55b5",
      "image": "https://docs.newrelic.com/static/7b1203c718220cb4a25d8d52dbbbbe98/c1b63/notification-payload-template.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/notifications/message-templates/",
      "published_at": "2021-12-19T15:28:20Z",
      "updated_at": "2021-12-10T06:26:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Early access The features described here are early access. You won't be able to use these features if you're not part of the early access program. For more information on related features, see our docs on Alerts notification channels, Incident Intelligence destinations, and Proactive Detection notifications. Notification message templates enable you to customize your notification event data before it's sent to your third-party destination. The templates map your custom values to the values used by your third-party destination. This gives you full control over what data will be sent and where, as well as being able to fully engage with the services you use. Message template variables A message template is what you use to convert New Relic event data to data that's consumable by your third-party service. Variables are specific attributes that are mapped to data fields in your third-party service. Message templates are written in a simple templating language called Handlebars. Variables in the message templates are written as expressions inside double curly braces {{ }}. Use the notification message template to map your New Relic notifications to the fields in your external services. The variables menu The New Relic variable names are listed in the message template variables menu. The variables are grouped into subcategories. In the variables menu, type {{ to select from a list of variables. As you type, variable names appear via autocomplete. The variable type is written on the right-hand side. You can add enriched data to these variables. The variables menu shows the options you have when mapping New Relic notification fields onto the fields in your external service. Use the Handlebars syntax When an event generates a notification, the message template uses the Handlebar variables to map the notification data to the fields used by your third-party service. The Handlebars language provides many features in addition to basic variable replacement, including evaluating nested input objects and functions, such as iterations (loops), conditional statements, and more. In Handlebars, these functions are called helpers. Helper functions Our message templates support the Handlebars built-in helpers. In addition, we've added other helpers that might be useful to you. JSON The {{json}} helper converts text to a JSON element. Use this when you're configuring a Webhook’s payload, which uses a JSON syntax, and any other situation you might want to pass JSON formatted data. For example, with a variable called data. { \"data\": { \"tags\": [\"infra, team-a\"] } } Copy To get the names array as a JSON element, use the {{json}} helper: {{json data.tags}} Copy to get: [\"infra\", \"team-a\"] Copy Equality Use the equality {{#eq}} helper to compare variables. Compares variables a and b, renders 'yes' or 'no': {{#eq a b}} yes {{else}} no {{/eq}} Compares string value \"a\" to variable b, renders 'yes' or 'no': {{#eq \"a\" b}} yes {{else}} no {{/eq}} Renders 'true' or 'false': {{eq a b}} Render 'y' or 'n': {{eq a b yes='y' no='n'}} Copy Replace The replace helper replaces instances of the first parameter in the second parameter with the child block. Use else clause to specify what happens when no instance of the first parameter is found. If it is omitted an empty string will be generated. Example #1: replace the word dog with cat in the sentence The dog likes to eat: {{#replace \"dog\" \"The dog likes to eat\"}}cat{{/replace}} Copy to get: The cat likes to eat Copy Example #2: replace the word cat with mouse in the sentence The dog likes to eat: {{#replace \"cat\" \"The dog likes to eat\"}}mouse{{/replace}} Copy to get an empty string: Copy Example #3: replace the word cat with mouse in the sentence The dog likes to eat, using the else clause: {{#replace \"cat\" \"The dog likes to eat\"}}mouse{{else}}There is no cat to replace{{/replace}} Copy to get: There is no cat to replace Copy Example #4: replace the word dog with cat in the sentence The DOG likes to eat while ignoring case: {{#replace \"/dog/i\" \"The DOG likes to eat\"}}cat{{/replace}} Copy to get: The cat likes to eat Copy Example #5: replace the variable {{needle}} with the variable {{replacement}} in the variable {{haystack}}: {{#replace needle haystack }}{{replacement}}{{/replace}} Copy using this data: { \"needle\": \"/dog/i\", \"haystack\": \"The DOG likes to eat\", \"replacement\": \"cat\" } Copy to get: The cat likes to eat Copy Usage examples The examples are based on a variable called data: \"data\": { \"tags\":[\"infra, team-a\"], \"id\":123456789, \"name\": \"Alice\", } Copy The data value has an equivalent, dot-notated format: \"data.tags\": [\"infra, team-a\"] \"data.id\": 123456789 \"data.name\": \"Alice\" Copy Validate data If id equals 123456789, then the output is valid. If not, the output is not valid. {{eq data.name \"Alice\" yes='valid' no='not valid'}} Copy If name equals Alice, then the output is valid. Return JSON Get the tags and object’s properties in a JSON form: {{json data.tags}} Copy This would return the following JSON: [\"infra\", \"team-a\"] Copy Get values from an array Get the first tag from the tags array: {{json data.tags.[0]}} Copy This would return the first value from the array: bash Copy $ \"infra\" Iterate through an array Iterate a variable of type array and aggregate the values into a string: {{#each tags}}{{this}}{{#unless @last}}, {{/unless}}{{/each}} Copy The result contains the tags, seperated by commas (the trailing comma is omitted): bash Copy $ infra, team Similarly, iterate the data variable, aggregate the object’s values, and output a JSON element: {{#each (json data)}}{{this}}{{/each}} Copy This would return a JSON such as: { \"tags\": [\"infra, team-a\"], \"name\": \"Alice\", \"id\": \"123456789\" } Copy Iterate the data variable, then aggregate the object’s entries to a string: {{#each data}}{{@key}}: {{this}}{{#unless @last}}, {{/unless}}{{/each}} Copy This would return a string such as: bash Copy $ tags: infra,team-a, name: Alice, id: 123456789 Handle missing attributes In some cases, an attribute may be missing from the variables menu, or not exist whatsoever. We can use the #if statement to set a fallback, such as: {{#if data.type}} {{ json data.type }} {{else}}\"N/A\"{{/if}} Copy would return the string \"N/A\".",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 249.4175,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "Early access The features described here are early access. You won&#x27;t be able to use these features if you&#x27;re not part of the early access program. For more information on related features, see our docs on <em>Alerts</em> notification channels, <em>Incident</em> <em>Intelligence</em> destinations, and Proactive Detection"
      },
      "id": "618f3a6c28ccbc60e70317f1"
    }
  ],
  "/docs/alerts-applied-intelligence/applied-intelligence/incident-intelligence/incident-intelligence-destination-examples": [
    {
      "sections": [
        "Alerts and applied intelligence notification integrations",
        "Early access",
        "Integration details",
        "Atlassian Jira",
        "Permissions",
        "Set up a Jira destination",
        "Important",
        "Two-way sync",
        "Configure the message template",
        "Send a test notification",
        "ServiceNow (Incident-Management)",
        "Roles",
        "Set up a destination",
        "Slack",
        "Prerequisites",
        "Set up a Slack destination",
        "Configure the Slack message settings",
        "Webhook",
        "Set up a webhook destination",
        "Configure the webhook event template",
        "Customize the webhook payload",
        "Email",
        "Configure the email settings"
      ],
      "title": "Alerts and applied intelligence notification integrations",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Incident Intelligence",
        "Destinations"
      ],
      "external_id": "7220c630fc187bb61784ff2cc2213e588b269b00",
      "image": "https://docs.newrelic.com/static/d4e9baecc3a76dd1a5945f8ec0aeca66/c1b63/webhook-notification-template.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/notifications/notification-integrations/",
      "published_at": "2021-12-19T15:28:40Z",
      "updated_at": "2021-12-19T15:28:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Early access The features described here are early access. You won't be able to use these features if you're not part of the early access program. For more information on related features, see our docs on Alerts notification channels, Incident Intelligence destinations, and Proactive Detection notifications. Alerts and Applied Intelligence notification integrations are specific services and platforms you can use to send notifications from New Relic. Integration details Read more about each of our specific notification integrations. Atlassian Jira Integrate New Relic with Atlassian Jira(Cloud) and automatically create and update Jira issues. Permissions The required permissions from the Jira API-Token are create, edit, and close tickets. To enable the two-way sync toggle, the provided Jira API-Key should have an Admin role. Set up a Jira destination Create Jira issues, then enable Jira and New Relic to share updates and stay synced. To create a Jira destination, enter the following information: Destination name: Custom name to identify the destination Jira account endpoint: the URL of the destination User-name: this will be the email address of the user making the connection API token: generated from your Atlassian account Important New Relic currently supports Atlassian-Jira Classic (company-managed) projects. Before saving the destination, we recommend you test the connection via the test connection button. Jira destination configuration. We recommand to test the connection before saving. Two-way sync You can enable a two-way integration with Jira to keep the issues' state synced with the corresponding state in New Relic. To enable two-way sync, turn on the ‘two-way integration’ toggle. When turned on, a Jira Webhook would be created in your Jira account at a later stage, for the selected project (see ‘customize a message template’). The webhook would contain access details to Newrelic (URL and Newrelic-API-KEY) Configure the message template To configure a template for a Jira issue, you first need to choose a destination. You will be able to create a new destination at this stage. Upon successful connection to the destination, you will need to choose a project, and then select the Jira issue type you would like to be used. Once the issue-type is selected, the configured project's fields are fetched from your account and automatically mapped to your Jira instance. To help you get started, we automatically present the required and recommended fields and values. Required fields must be set with a value. You can add or remove optional fields(use the X mark on their right side) Jira message template. Send a test notification You can see how the JIRA issue will appear by clicking a test notification with default field values. If successful, a JIRA issue will be created and a link will appear. ServiceNow (Incident-Management) Integrate New Relic with ServiceNow Incident-Management and automatically create and update incidents. Roles As part of the integrations, we fetch fields from the your serviceNow incident table and optional values. For this, the provided ServiceNow user details required read permissions for the tables: sys_dictionary, sys_choice, sys_user and task. A read/write permission to incident To be able to fetch users for the caller column, we required read permissions for the sys_users table. The above permissions can be achieved with the roles personalize_choices, personalize_dictionary, rest_service, itil. Read/Write permissions to the api_key_credentials table is required to enable two-way integration. This can be covered with the roles credentials_admin and discovery_admin. Set up a destination To create a ServiceNow destination, enter the following information: Destination Name: custom name to identify the destination Domain: the URL of the destination User-name: the name of the user Password: the user name’s password Before saving the destination, we recommend testing the connection by clicking the test connection button. Two-way sync You can configure a two-way integration with ServiceNow Incidents Management to keep the incidents' state synced with the corresponding state in New Relic. Here are some required steps to remember when configuring the two-way integration: Turn on the two-way integration toggle. Open and download this XML file, which includes the business rule triggering events back to New Relic One. In the ServiceNow sidebar menu, go to System Definition > Business Rules. Click the menu icon in one of the column headers, select Import XML and upload the XML file you downloaded. Once the Destination is saved, a New-Relic API-Key will be kept in the api_key_credentials. The key would sent in a header as part of the callback REST call to New-Relic Configure the message template Upon a successful connection, ServiceNow incident table columns are fetched from your account and automatically mapped to your ServiceNow instance. To help you get started, we automatically present the required and recommended fields and values. Required fields must be set with a value. You can add or remove optional fields(use the X mark on their right side) Select, edit or remove fields for the ServiceNow-Incident template. Send a test notification You can see how the ServiceNow incident will appear by clicking a test notification with default field values. If successful, an incident will be created and a link will appear. Slack Send notifications-messages to your Slack channels. Prerequisites Your Slack workspace needs to have the New Relic application installed. The application must be approved by a workspace admin before it can be individually installed by users Set up a Slack destination Click on the `one-click Slack authentication' will lead you to the Slack landing page to continue the OAuth2 authentication process. On the Slack landing page, if you're not signed into the required workspace, you're redirected to Slack to sign in. Add your workspace name or select the relevant workspace and click Continue. When signed in to the selected workspace, you are requested to allow New Relic to perform the specified actions. Clicking `Allow' will redirect you back to the Destination page. Configure the Slack message settings Select a Destination(Workspace) and select a Slack-channel where the messages will be sent. You can create a new destination if there is no pre-defined destination for the required workspace. Note that, for privacy reasons, users need to be authenticated to select private channels (one-time process) Send a test notification You can send a test notification with a pre-defined example payload to the channel. This creates a message in the selected Slack-channel. Webhook Use the webhook notifier to send the notification messages to any endpoint you like. Set up a webhook destination To create a webhook destination, you need the following: Destination Name: A unique destination name URL: the endpoint of the target application, authentication and custom headers if needed. Authorization mechanism (Optional):. Can be basic authentication or a bearer token Configure the webhook event template Pick a webhook destination from the list and configure the HTTP-POST request. The request configuration requires you to: Set a name for the template. Select a pre-configured destination from the destinations list or create a new one. Add custom headers (optional). Configure the request’s payload. Customize the webhook payload You can use the default payload or customize it to contain the required data. Pick Variables from the variables menu and apply handlebars syntax to enrich your webhook. Note that the request’s content-type is JSON by default. Hence, the payload needs to keep the JSON form. See Usage examples The ‘preview’ section on the right hand-side shows an expected payload after the template is rendered. If the eventual payload would not form a valid Json, an error will be shown and it won’t be possible to save the template. If the webhook payload conforms a valid Json, you can send a test notification to your defined webhook destination We recommend sending a test notification to make sure that everything's connected correctly. Email Send email notifications to users. Configure the email settings Add one or more recipients. Users with New Relic accounts can be found via autocomplete when searching for their name or email address. To add a user without a New Relic account or email distribution list, add the full email address. Every recipient will be translated into a 'destination'. You can follow the email notifications per destination in the notifications-log Send a test notification You can send a test notification to make sure the email notifications reach the inbox.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 368.69217,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Alerts</em> <em>and</em> <em>applied</em> <em>intelligence</em> notification integrations",
        "sections": "<em>Alerts</em> <em>and</em> <em>applied</em> <em>intelligence</em> notification integrations",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "Early access The features described here are early access. You won&#x27;t be able to use these features if you&#x27;re not part of the early access program. For more information on related features, see our docs on <em>Alerts</em> notification channels, <em>Incident</em> <em>Intelligence</em> destinations, and Proactive Detection"
      },
      "id": "618ff71628ccbc60710321e4"
    },
    {
      "sections": [
        "Workflows",
        "What is an issue",
        "Add a Workflow",
        "Tip",
        "Enrichments",
        "Important",
        "Notifier"
      ],
      "title": "Workflows",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Workflows",
        "Notifications"
      ],
      "external_id": "0993c323f76467d22136cac6982ccd48e859f722",
      "image": "",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/applied-intelligence/incident-workflows/incident-workflows/",
      "published_at": "2021-12-19T13:46:37Z",
      "updated_at": "2021-12-17T01:48:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With workflows you control when and where you want to receive notifications about issues, tunnel the right information to the relevant person or team, and enrich your issue's notifications with additional New Relic data. What is an issue Issues are groups of incidents that describe the underlying problem of your symptoms. When a new incident is created, Incident Intelligence opens an issue and evaluates other open issues for correlations. For more information see Use Incident Inteligence. Add a Workflow Tip The maximum number of workflows you can add is 100 per environment and 50 per account. The workflows feature is located under the Alerts & AI menu. Go to one.newrelic.com, click Alerts & AI, in the left navigation under Enrich and Notify click Workflow, then click Add a workflow. Name your workflow. This field is mandatory and needs to be unique. Workflows filters the issues you want to send to the relevant destination. You can send all issues by selecting Send all issues or you can build a query to filter the right issues. Build a query by selecting an attribute, operator and value/s to narrow down the right issues you want to be passed on. Optional: Enriched Data, available to full platform users. Enrichments Enrichments can give additional context on alert notifications by adding NRQL query results to them The workflows enrich tool is similar to the query builder. The NRQL query's results are included in the notification message. If you create queries for your workflows, they run in real time and the query results are the value of the enricher. You can save any valid query into your workflow, even if they return an empty result, but you must run the query before saving the enrichment. You can also query with issue-specific variables; for example {{entitiesData.ids}} For enrichment examples, see Workflow data enrichment examples. Note: The query name needs to be unique because you'll use the name to decide how to include the result in the payload of the notification that will be sent by the workflow. Important At this stage, visual representation of enrichments are sent to Slack and email. For Servicenow and JIRA destinations, we recommend to limit to NRDB queries with single value results. For example: count, min, or max. The maximum number of enrichments per workflow is 5. The maximum number of variables an enrichment can contain is 1. Notify: Choose one or more destinations and add an optional message. Notifier To save and activate a Workflow, you need to include a notifier. Setting up a notifier requires having a notification destination configured. You can either set it up through the Destination menu item, or through the Create new destination item in the destination selection drop down in each destination channel screen. A notification destination includes the credentials required to send a notification to a third party platform and an option to customize the message sent by mapping fields from our issues to fields in the destination platform. To learn how to set up destinations and configure message templates, check the documentation on notifications. Click update message once completing the notifier requirements Optionally, test your workflow. We'll use existing data from your account to test what you've configured and send a sample notification Click activate to complete the workflow. Tip In any destination channel, start typing and a variable menu will open up. You'll see the names of the variables, which will be replaced with the variable's values at runtime. To use the enrichers' results, use their name.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 292.56073,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": " problem of your symptoms. When a new <em>incident</em> is created, <em>Incident</em> <em>Intelligence</em> opens an issue and evaluates other open issues for correlations. For more information see Use <em>Incident</em> Inteligence. Add a Workflow Tip The maximum number of workflows you can add is 100 per environment and 50 per account"
      },
      "id": "603e967664441f7e6f4e889b"
    },
    {
      "sections": [
        "Get started with Incident Intelligence",
        "Requirements",
        "Set up Incident Intelligence",
        "1. Configure your environment (one-time)",
        "Tip",
        "2. Configure sources",
        "Alerts",
        "Datarobot (formerly Algorithmia)",
        "Aporia (MLOps)",
        "Superwise (MLOps)",
        "Anomalies",
        "AWS",
        "Grafana",
        "PagerDuty",
        "EOL NOTICE",
        "Prometheus Alertmanager",
        "REST API",
        "Splunk",
        "Important",
        "Splunk metadata",
        "3. Configure destinations (ServiceNow and others)",
        "Configure ServiceNow (example)",
        "Send data to ServiceNow",
        "Custom notification message",
        "Other destinations",
        "Send data to PagerDuty",
        "Send data via webhook",
        "4. Configure pathways",
        "What's next?"
      ],
      "title": "Get started with Incident Intelligence",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Incident intelligence"
      ],
      "external_id": "8c4a5a914ca01cb42250908d2fb1a12ccc697e25",
      "image": "https://docs.newrelic.com/static/5cb28999dc618c6a5a2b8be1fa72e660/b97f6/image-%25281%2529_0.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/applied-intelligence/incident-intelligence/get-started-incident-intelligence/",
      "published_at": "2021-12-19T14:23:58Z",
      "updated_at": "2021-12-09T22:19:28Z",
      "document_type": "page",
      "popularity": 1,
      "body": "As part of Applied Intelligence, Incident Intelligence helps you correlate your incidents and reduce noise in your environment. It gives you an overview of all your incidents, their sources, and related events. Requirements If you haven't already, sign up for a New Relic account to get started. To use most Applied Intelligence features, you must be a full platform user. For more information, see user type capabilities. Set up Incident Intelligence To enable Incident Intelligence, follow these steps. Afterwards, issues should start to appear in your issue feed. 1. Configure your environment (one-time). 2. Configure sources. 3. Configure destinations. 4. Configure pathways. 1. Configure your environment (one-time) To set up an environment in Incident Intelligence, you need an administrator to select a New Relic account for it. This account should be the one your team is using. Who sets the environment? Only administrators, and only for accounts where they have admin privileges. Can administrators set more than one environment? They can set one environment per parent account and its child accounts. More than one can be set if an administrator has privileges for more than one parent account. Tip Incident Intelligence is a cross-account product. This means you can send in data from any New Relic account or external source to correlate events. 2. Configure sources After setting up your environment, determine your incident sources. These are your data inputs. You can get data from any of the following sources: Alerts By integrating Incident Intelligence with your alerts violations, you can get context and correlations from what you're monitoring. To get data from alerts: From one.newrelic.com, click Alerts. On the left under Incident Intelligence, click Sources and then click Alerts. Select the policies you want to connect to Applied Intelligence, and click Connect. You can add additional alerts policies or remove policies you've already connected in Sources > New Relic Alerts. Tip Adding alerts as a source will not affect your current configuration or notifications. Datarobot (formerly Algorithmia) By integrating Incident Intelligence with your Datarobot machine-learning models, you can monitor your machine learning model performance. To configure Datarobot for Incident Intelligence, see our integration docs. Aporia (MLOps) By integrating Incident Intelligence with your Aporia machine-learning models, you can monitor your machine learning model performance. To configure our Aporia integration, see our docs. Superwise (MLOps) By integrating Incident Intelligence with your Superwise machine-learning models, you can monitor your machine learning model performance. To configure our Superwise integration, see our docs. Anomalies By integrating Incident Intelligence with your New Relic Proactive Detection anomalies, you can get context and correlations from what you're monitoring. To get data from New Relic Proactive Detection anomalies: From one.newrelic.com, click Alerts & AI. On the left under Incident Intelligence, click Sources and then click Anomalies. Select the configurations you want to connect to Applied Intelligence by toggling to on (green). To add or remove alert policies, from Alerts & AI, click Sources, then Alerts. Tip Adding anomalies as a source won't affect your current Proactive Detection configurations or notifications. AWS You can integrate Incident Intelligence with Amazon CloudWatch to provide incident management for all of your AWS services. To integrate Amazon CloudWatch: Go to one.newrelic.com and click Alerts & AI. On the left under Incident Intelligence, click Sources and then click Amazon Web Services. Copy the URL. Create a new Amazon SNS topic. Set CloudWatch to forward all Alarms state changes to that topic: In the Amazon CloudWatch UI, click Events > Event Pattern. Select Service Name > CloudWatch. Select Event Type > CloudWatch Alarm State Change. Select Targets > SNS Topic, and select your new Amazon SNS topic. Create a new subscription: In the Amazon AWS UI, click Create a Subscription. Select your new Amazon SNS topic. Select Protocol > choose HTTPS. In Endpoint, paste the URL you previously copied from the Applied Intelligence Sources. Grafana You can integrate Incident Intelligence with Grafana's notifications for insight into events across your applications and environment. Grafana's webhook notification is a simple way to send information over HTTP to a custom endpoint. To integrate Grafana as a new webhook: Log into your Grafana portal using Admin permissions, and choose Alerting. On the Grafana Notification Channels page, click New Channel > Webhook. Go to one.newrelic.com and click Alerts & AI. On the left under Incident Intelligence, click Sources, and then click Grafana. Copy the URL, and paste it into your new Grafana webhook. PagerDuty EOL NOTICE As of October 2021, we've discontinued support for several capabilities with PagerDuty, including suggested responders, golden signals, and component enrichment. For more details, including how you can easily make this transition, see our Explorers Hub post. You can integrate Incident Intelligence directly with your PagerDuty services to ingest, process, and enhance all of your PagerDuty incidents. Connecting PagerDuty services to Applied Intelligence will not affect your current services or notifications. To get data from PagerDuty: Make sure your PagerDuty API key has write access. From one.newrelic.com, click Alerts & AI. On the left under Incident Intelligence, click Sources and then click PagerDuty. Enter your PagerDuty API key. The key should be either a personal or general access key with write access. If it's created by a user, the user should be an admin. Select the PagerDuty services you want to connect to Applied Intelligence, and click Connect. You can add additional services or remove services you've already connected in Sources > PagerDuty. Prometheus Alertmanager By integrating Incident Intelligence with Prometheus Alertmanager, you can receive and correlate your Prometheus alerts with events from other sources. To integrate Prometheus Alertmanager: Set up your Alertmanager configuration file by running: ./alertmanager -config.file=simple.yml Copy Go to one.newrelic.com and click Alerts & AI. On the left under Incident Intelligence, click Sources and then click Prometheus Alertmanager. Copy the Prometheus Alertmanager URL, and paste it into the <webhook_config>/url section of your Alertmanager config file. Reload the Prometheus Alertmanager configuration with one of the two methods: Send a SIGHUP to the process. Send an HTTP POST request to the /-/reload endpoint. REST API Incident Intelligence supports a dedicated REST API interface that lets you integrate with additional systems. The interface allows instrumentation of your code or other monitoring solutions to report any kind of metric or event. A metric can be a raw data point such as CPU, memory, disk utilization, or business KPI. An event can be a monitoring alert, deployment event, incident, exceptions or any other change in state that you want to describe. You can also send any type of data to Incident Intelligence straight from your own systems or applications. The REST API supports secure token-based authentication and accepts JSON content as input. For more information on authentication and the full API reference, see REST API for New Relic Applied Intelligence. Splunk By integrating Incident Intelligence with your Splunk log monitoring, you can: Use your environment's log data for searches and key term reports. Correlate alerts and search reports with your other metrics and incidents. Important Applied Intelligence supports Splunk Light, Splunk Cloud, and Splunk Enterprise version 6.3 and higher. To get data from Splunk: In your Splunk console, start a search for the relevant events. Save your search as an alert, configure your alert conditions, and then choose the webhook as the delivery method. Go to one.newrelic.com and click Alerts & AI. On the left under Incident Intelligence, click Sources and then click Splunk. Copy the collector URL, and paste it into the webhook endpoint in the Splunk console. Optional: Use Splunk tokens to enrich alert data with Splunk metadata. Splunk metadata To enrich alerts data with your Splunk metadata, use Splunk tokens. This helps you leverage your search data, which includes metadata and values from the first row of search results. If you want to... Do this... Access search data Use the format $<fieldname>$. For example, use $app$ for the app context for the search. Access field values To access field values from the first result row that a search returns, use the format $result.<fieldname>$. For example, use $result.host$ for the host value and $result.sourcetype$ for the source type. Use variables You can leverage any of the Selected fields in the Splunk search and add any unique fields to the Selected fields to make the data available as a variable. The following fields will automatically provide hints to the correlation engine: app: parsed as APPLICATION_NAME application:parsed as APPLICATION_NAME application_name: parsed as APPLICATION_NAME cluster: parsed as CLUSTER_NAME computer: parsed as HOST_NAME Dc: parsed as DATACENTER_NAME datacenter: parsed as DATACENTER_NAME host: parsed as HOST_NAME host_name: parsed as HOST_NAME hostname: parsed as HOST_NAME transaction: parsed as EVENT_ID Transaction_id: parsed as EVENT_ID user: parsed as USER_NAME 3. Configure destinations (ServiceNow and others) Now that you've set up your sources, you can configure your destinations. These are the data outputs where you view your incidents. Configure ServiceNow (example) Using ServiceNow as a destination enables you to push valuable violation data into new ServiceNow incident tickets. Send data to ServiceNow To configure Incident Intelligence to send data to ServiceNow: Go to one.newrelic.com, click Alerts & AI, in the left nav under Incident Intelligence click Destinations, then click ServiceNow. Required: Enter a channel name. This is used internally in Applied Intelligence to identify the destination (for example, in Pathways). Required: Enter your ServiceNow credentials: Team domain (This must be unique. No two destinations can have the same domain). Username Password Follow the two-way integration on screen instructions: Open and download this XML file. In the ServiceNow sidebar menu, go to System Definition > Business Rule. Click the menu icon in one of the column headers, select Import XML, and upload the XML file you downloaded. The two way integration will allow the ServiceNow incident to be updated with changes to the Applied Intelligence issue. Closing a ServiceNow incident will close its corresponding New Relic issue. When a New Relic issue is resolved, the corresponding ServiceNow incident will be closed. Custom notification message Applied Intelligence uses a templating framework called Jinja2 in the customization section interface. The Value field must be in valid Jinja syntax. By default, the interface populates a set of default fields in ServiceNow. When you add a custom field, enter the ServiceNow field name you want to use. When you want to skip a selected field in an issue update, add the | skip_on_update string at the end of the value you've selected. Tip By default, ServiceNow adds u_ to the beginning of its custom values. When mapping to ServiceNow attributes, use the Column name value. Please note that the name needs to be lowercase separated by underscores. Go here to see the custom notification message attribute descriptions. Go here to see Jinja2 Useful Syntax. Other destinations You can set other destinations: Send data to PagerDuty EOL NOTICE As of October 2021, we've discontinued support for several capabilities with PagerDuty, including suggested responders, golden signals, and component enrichment. For more details, including how you can easily make this transition, see our Explorers Hub post. Recommended: Create a new PagerDuty service to use as a destination. Because PagerDuty services can also be used as sources, this can help you distinguish your data input from your output. To create a PagerDuty destination: Go to one.newrelic.com, click Alerts & AI, in the left nav under Incident Intelligence click Destinations, then click PagerDuty. Enter your PagerDuty API key. The key should be either a personal or general access key with write access. If it's created by a user, the user should be an admin. If you've configured a PagerDuty source with an API key, you can use the same key. Select the PagerDuty services you want to connect to Applied Intelligence, and click Connect. When you're ready, you can add policies for one or more PagerDuty destinations. You can also transfer the policies over from your existing services or leave them as sources if needed. From the Destinations > PagerDuty page, you can also: Review the permissions for your services. Click Authorize when you're done. Add or delete existing services from the PagerDuty destination. Edit permissions for any service. To configure your PagerDuty destinations, use the following settings: Configuration setting Description Trigger new incidents Required. Trigger correlated parent incidents so you can identify issues faster. Edit incident titles Required. Alter your incident titles to help you orient and understand issues. Add new integrations Required. Add integrations to enable incident creation for selected services. Add webhook extensions Add webhook extensions to sync user actions in PagerDuty to New Relic. This lets you update the correlated issue state. Auto-resolve correlated incidents When enabled, this will resolve and automatically close correlated parent/child incidents. Select a user to take actions in PagerDuty You need to select a user before you can enable deep integration with PagerDuty. Once you do, the user can: Add notes to incidents (required): Incident notes are used to enrich incidents with context. Acknowledge triggered incidents: When enabled, Applied Intelligence will acknowledge and correlate newly triggered incidents in PagerDuty before you're notified. Use the original escalation policy: When enabled, the escalation policy of the source service will be applied to each incident. Send data via webhook Incident Intelligence will send the event body in JSON format by HTTPS POST. The system expects the endpoint to return a successful HTTP code (2xx). To configure Incident Intelligence to send data via webhook: Go to one.newrelic.com, click Alerts & AI, in the left nav under Incident Intelligence click Destinations, then click Webhook. Required: Configure the unique webhook key, used in Applied Intelligence to refer to this webhook configuration and its specific settings. Required: Configure the destination endpoint where the webhook payload will be sent. Optional steps: Configure custom headers, which are key:value pairs of headers to be sent with the request. Example: \"Authentication\" \"Bearer\" <bearer token> Configure a custom payload template that can be used to map New Relic fields to match the destination tool's expected name and format. Configure priority mapping (critical, high, medium, or low), used to map New Relic's priorities to the priorities expected at the destination. Tip There’s a retry mechanism that is triggered a few times with exponential backoff for a couple of minutes once an error occurs. If we reach the retry limit, the Webhook will get auto-disabled. For examples of destination templates, webhook formats, and JSON schema, see the Incident Intelligence destination examples. 4. Configure pathways To control when and where you want to receive notifications from your incidents, you can configure pathways. To add a pathway: Go to one.newrelic.com, click Alerts & AI, in the left nav under Incident Intelligence click Pathways, then click Add a pathway. In the query builder box, select an attribute, such as application/name. This can be from the list of all attributes available in PagerDuty incidents and New Relic alerts violations, or you can add your own attributes. Select a logical operator. For example, contains. Enter a specific value to complete the logical expression. To include all issues created by your sources, select Send everything. (Use this if you only use one PagerDuty service to manage all incidents.) To build more complex logic, use the AND/OR operators. Select one or more of your destinations. To edit or remove existing pathways, mouse over the pathway's name on the Pathways page. What's next? Now that you've set up some sources and destinations for your incidents, read about how to use Incident Intelligence.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.17322,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Get started with <em>Incident</em> <em>Intelligence</em>",
        "sections": "Get started with <em>Incident</em> <em>Intelligence</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": " one.newrelic.com, click <em>Alerts</em> &amp; AI. On the left under <em>Incident</em> <em>Intelligence</em>, click Sources and then click Anomalies. Select the configurations you want to connect to <em>Applied</em> <em>Intelligence</em> by toggling to on (green). To add or remove <em>alert</em> policies, from <em>Alerts</em> &amp; AI, click Sources, then <em>Alerts</em>. Tip Adding anomalies"
      },
      "id": "603ea62e64441f119f4e883f"
    }
  ],
  "/docs/alerts-applied-intelligence/applied-intelligence/incident-intelligence/rest-api-applied-intelligence": [
    {
      "sections": [
        "Alerts and applied intelligence notification integrations",
        "Early access",
        "Integration details",
        "Atlassian Jira",
        "Permissions",
        "Set up a Jira destination",
        "Important",
        "Two-way sync",
        "Configure the message template",
        "Send a test notification",
        "ServiceNow (Incident-Management)",
        "Roles",
        "Set up a destination",
        "Slack",
        "Prerequisites",
        "Set up a Slack destination",
        "Configure the Slack message settings",
        "Webhook",
        "Set up a webhook destination",
        "Configure the webhook event template",
        "Customize the webhook payload",
        "Email",
        "Configure the email settings"
      ],
      "title": "Alerts and applied intelligence notification integrations",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Incident Intelligence",
        "Destinations"
      ],
      "external_id": "7220c630fc187bb61784ff2cc2213e588b269b00",
      "image": "https://docs.newrelic.com/static/d4e9baecc3a76dd1a5945f8ec0aeca66/c1b63/webhook-notification-template.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/notifications/notification-integrations/",
      "published_at": "2021-12-19T15:28:40Z",
      "updated_at": "2021-12-19T15:28:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Early access The features described here are early access. You won't be able to use these features if you're not part of the early access program. For more information on related features, see our docs on Alerts notification channels, Incident Intelligence destinations, and Proactive Detection notifications. Alerts and Applied Intelligence notification integrations are specific services and platforms you can use to send notifications from New Relic. Integration details Read more about each of our specific notification integrations. Atlassian Jira Integrate New Relic with Atlassian Jira(Cloud) and automatically create and update Jira issues. Permissions The required permissions from the Jira API-Token are create, edit, and close tickets. To enable the two-way sync toggle, the provided Jira API-Key should have an Admin role. Set up a Jira destination Create Jira issues, then enable Jira and New Relic to share updates and stay synced. To create a Jira destination, enter the following information: Destination name: Custom name to identify the destination Jira account endpoint: the URL of the destination User-name: this will be the email address of the user making the connection API token: generated from your Atlassian account Important New Relic currently supports Atlassian-Jira Classic (company-managed) projects. Before saving the destination, we recommend you test the connection via the test connection button. Jira destination configuration. We recommand to test the connection before saving. Two-way sync You can enable a two-way integration with Jira to keep the issues' state synced with the corresponding state in New Relic. To enable two-way sync, turn on the ‘two-way integration’ toggle. When turned on, a Jira Webhook would be created in your Jira account at a later stage, for the selected project (see ‘customize a message template’). The webhook would contain access details to Newrelic (URL and Newrelic-API-KEY) Configure the message template To configure a template for a Jira issue, you first need to choose a destination. You will be able to create a new destination at this stage. Upon successful connection to the destination, you will need to choose a project, and then select the Jira issue type you would like to be used. Once the issue-type is selected, the configured project's fields are fetched from your account and automatically mapped to your Jira instance. To help you get started, we automatically present the required and recommended fields and values. Required fields must be set with a value. You can add or remove optional fields(use the X mark on their right side) Jira message template. Send a test notification You can see how the JIRA issue will appear by clicking a test notification with default field values. If successful, a JIRA issue will be created and a link will appear. ServiceNow (Incident-Management) Integrate New Relic with ServiceNow Incident-Management and automatically create and update incidents. Roles As part of the integrations, we fetch fields from the your serviceNow incident table and optional values. For this, the provided ServiceNow user details required read permissions for the tables: sys_dictionary, sys_choice, sys_user and task. A read/write permission to incident To be able to fetch users for the caller column, we required read permissions for the sys_users table. The above permissions can be achieved with the roles personalize_choices, personalize_dictionary, rest_service, itil. Read/Write permissions to the api_key_credentials table is required to enable two-way integration. This can be covered with the roles credentials_admin and discovery_admin. Set up a destination To create a ServiceNow destination, enter the following information: Destination Name: custom name to identify the destination Domain: the URL of the destination User-name: the name of the user Password: the user name’s password Before saving the destination, we recommend testing the connection by clicking the test connection button. Two-way sync You can configure a two-way integration with ServiceNow Incidents Management to keep the incidents' state synced with the corresponding state in New Relic. Here are some required steps to remember when configuring the two-way integration: Turn on the two-way integration toggle. Open and download this XML file, which includes the business rule triggering events back to New Relic One. In the ServiceNow sidebar menu, go to System Definition > Business Rules. Click the menu icon in one of the column headers, select Import XML and upload the XML file you downloaded. Once the Destination is saved, a New-Relic API-Key will be kept in the api_key_credentials. The key would sent in a header as part of the callback REST call to New-Relic Configure the message template Upon a successful connection, ServiceNow incident table columns are fetched from your account and automatically mapped to your ServiceNow instance. To help you get started, we automatically present the required and recommended fields and values. Required fields must be set with a value. You can add or remove optional fields(use the X mark on their right side) Select, edit or remove fields for the ServiceNow-Incident template. Send a test notification You can see how the ServiceNow incident will appear by clicking a test notification with default field values. If successful, an incident will be created and a link will appear. Slack Send notifications-messages to your Slack channels. Prerequisites Your Slack workspace needs to have the New Relic application installed. The application must be approved by a workspace admin before it can be individually installed by users Set up a Slack destination Click on the `one-click Slack authentication' will lead you to the Slack landing page to continue the OAuth2 authentication process. On the Slack landing page, if you're not signed into the required workspace, you're redirected to Slack to sign in. Add your workspace name or select the relevant workspace and click Continue. When signed in to the selected workspace, you are requested to allow New Relic to perform the specified actions. Clicking `Allow' will redirect you back to the Destination page. Configure the Slack message settings Select a Destination(Workspace) and select a Slack-channel where the messages will be sent. You can create a new destination if there is no pre-defined destination for the required workspace. Note that, for privacy reasons, users need to be authenticated to select private channels (one-time process) Send a test notification You can send a test notification with a pre-defined example payload to the channel. This creates a message in the selected Slack-channel. Webhook Use the webhook notifier to send the notification messages to any endpoint you like. Set up a webhook destination To create a webhook destination, you need the following: Destination Name: A unique destination name URL: the endpoint of the target application, authentication and custom headers if needed. Authorization mechanism (Optional):. Can be basic authentication or a bearer token Configure the webhook event template Pick a webhook destination from the list and configure the HTTP-POST request. The request configuration requires you to: Set a name for the template. Select a pre-configured destination from the destinations list or create a new one. Add custom headers (optional). Configure the request’s payload. Customize the webhook payload You can use the default payload or customize it to contain the required data. Pick Variables from the variables menu and apply handlebars syntax to enrich your webhook. Note that the request’s content-type is JSON by default. Hence, the payload needs to keep the JSON form. See Usage examples The ‘preview’ section on the right hand-side shows an expected payload after the template is rendered. If the eventual payload would not form a valid Json, an error will be shown and it won’t be possible to save the template. If the webhook payload conforms a valid Json, you can send a test notification to your defined webhook destination We recommend sending a test notification to make sure that everything's connected correctly. Email Send email notifications to users. Configure the email settings Add one or more recipients. Users with New Relic accounts can be found via autocomplete when searching for their name or email address. To add a user without a New Relic account or email distribution list, add the full email address. Every recipient will be translated into a 'destination'. You can follow the email notifications per destination in the notifications-log Send a test notification You can send a test notification to make sure the email notifications reach the inbox.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 368.69217,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Alerts</em> <em>and</em> <em>applied</em> <em>intelligence</em> notification integrations",
        "sections": "<em>Alerts</em> <em>and</em> <em>applied</em> <em>intelligence</em> notification integrations",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "Early access The features described here are early access. You won&#x27;t be able to use these features if you&#x27;re not part of the early access program. For more information on related features, see our docs on <em>Alerts</em> notification channels, <em>Incident</em> <em>Intelligence</em> destinations, and Proactive Detection"
      },
      "id": "618ff71628ccbc60710321e4"
    },
    {
      "sections": [
        "Workflows",
        "What is an issue",
        "Add a Workflow",
        "Tip",
        "Enrichments",
        "Important",
        "Notifier"
      ],
      "title": "Workflows",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Workflows",
        "Notifications"
      ],
      "external_id": "0993c323f76467d22136cac6982ccd48e859f722",
      "image": "",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/applied-intelligence/incident-workflows/incident-workflows/",
      "published_at": "2021-12-19T13:46:37Z",
      "updated_at": "2021-12-17T01:48:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With workflows you control when and where you want to receive notifications about issues, tunnel the right information to the relevant person or team, and enrich your issue's notifications with additional New Relic data. What is an issue Issues are groups of incidents that describe the underlying problem of your symptoms. When a new incident is created, Incident Intelligence opens an issue and evaluates other open issues for correlations. For more information see Use Incident Inteligence. Add a Workflow Tip The maximum number of workflows you can add is 100 per environment and 50 per account. The workflows feature is located under the Alerts & AI menu. Go to one.newrelic.com, click Alerts & AI, in the left navigation under Enrich and Notify click Workflow, then click Add a workflow. Name your workflow. This field is mandatory and needs to be unique. Workflows filters the issues you want to send to the relevant destination. You can send all issues by selecting Send all issues or you can build a query to filter the right issues. Build a query by selecting an attribute, operator and value/s to narrow down the right issues you want to be passed on. Optional: Enriched Data, available to full platform users. Enrichments Enrichments can give additional context on alert notifications by adding NRQL query results to them The workflows enrich tool is similar to the query builder. The NRQL query's results are included in the notification message. If you create queries for your workflows, they run in real time and the query results are the value of the enricher. You can save any valid query into your workflow, even if they return an empty result, but you must run the query before saving the enrichment. You can also query with issue-specific variables; for example {{entitiesData.ids}} For enrichment examples, see Workflow data enrichment examples. Note: The query name needs to be unique because you'll use the name to decide how to include the result in the payload of the notification that will be sent by the workflow. Important At this stage, visual representation of enrichments are sent to Slack and email. For Servicenow and JIRA destinations, we recommend to limit to NRDB queries with single value results. For example: count, min, or max. The maximum number of enrichments per workflow is 5. The maximum number of variables an enrichment can contain is 1. Notify: Choose one or more destinations and add an optional message. Notifier To save and activate a Workflow, you need to include a notifier. Setting up a notifier requires having a notification destination configured. You can either set it up through the Destination menu item, or through the Create new destination item in the destination selection drop down in each destination channel screen. A notification destination includes the credentials required to send a notification to a third party platform and an option to customize the message sent by mapping fields from our issues to fields in the destination platform. To learn how to set up destinations and configure message templates, check the documentation on notifications. Click update message once completing the notifier requirements Optionally, test your workflow. We'll use existing data from your account to test what you've configured and send a sample notification Click activate to complete the workflow. Tip In any destination channel, start typing and a variable menu will open up. You'll see the names of the variables, which will be replaced with the variable's values at runtime. To use the enrichers' results, use their name.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 292.56073,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": " problem of your symptoms. When a new <em>incident</em> is created, <em>Incident</em> <em>Intelligence</em> opens an issue and evaluates other open issues for correlations. For more information see Use <em>Incident</em> Inteligence. Add a Workflow Tip The maximum number of workflows you can add is 100 per environment and 50 per account"
      },
      "id": "603e967664441f7e6f4e889b"
    },
    {
      "sections": [
        "Get started with Incident Intelligence",
        "Requirements",
        "Set up Incident Intelligence",
        "1. Configure your environment (one-time)",
        "Tip",
        "2. Configure sources",
        "Alerts",
        "Datarobot (formerly Algorithmia)",
        "Aporia (MLOps)",
        "Superwise (MLOps)",
        "Anomalies",
        "AWS",
        "Grafana",
        "PagerDuty",
        "EOL NOTICE",
        "Prometheus Alertmanager",
        "REST API",
        "Splunk",
        "Important",
        "Splunk metadata",
        "3. Configure destinations (ServiceNow and others)",
        "Configure ServiceNow (example)",
        "Send data to ServiceNow",
        "Custom notification message",
        "Other destinations",
        "Send data to PagerDuty",
        "Send data via webhook",
        "4. Configure pathways",
        "What's next?"
      ],
      "title": "Get started with Incident Intelligence",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Incident intelligence"
      ],
      "external_id": "8c4a5a914ca01cb42250908d2fb1a12ccc697e25",
      "image": "https://docs.newrelic.com/static/5cb28999dc618c6a5a2b8be1fa72e660/b97f6/image-%25281%2529_0.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/applied-intelligence/incident-intelligence/get-started-incident-intelligence/",
      "published_at": "2021-12-19T14:23:58Z",
      "updated_at": "2021-12-09T22:19:28Z",
      "document_type": "page",
      "popularity": 1,
      "body": "As part of Applied Intelligence, Incident Intelligence helps you correlate your incidents and reduce noise in your environment. It gives you an overview of all your incidents, their sources, and related events. Requirements If you haven't already, sign up for a New Relic account to get started. To use most Applied Intelligence features, you must be a full platform user. For more information, see user type capabilities. Set up Incident Intelligence To enable Incident Intelligence, follow these steps. Afterwards, issues should start to appear in your issue feed. 1. Configure your environment (one-time). 2. Configure sources. 3. Configure destinations. 4. Configure pathways. 1. Configure your environment (one-time) To set up an environment in Incident Intelligence, you need an administrator to select a New Relic account for it. This account should be the one your team is using. Who sets the environment? Only administrators, and only for accounts where they have admin privileges. Can administrators set more than one environment? They can set one environment per parent account and its child accounts. More than one can be set if an administrator has privileges for more than one parent account. Tip Incident Intelligence is a cross-account product. This means you can send in data from any New Relic account or external source to correlate events. 2. Configure sources After setting up your environment, determine your incident sources. These are your data inputs. You can get data from any of the following sources: Alerts By integrating Incident Intelligence with your alerts violations, you can get context and correlations from what you're monitoring. To get data from alerts: From one.newrelic.com, click Alerts. On the left under Incident Intelligence, click Sources and then click Alerts. Select the policies you want to connect to Applied Intelligence, and click Connect. You can add additional alerts policies or remove policies you've already connected in Sources > New Relic Alerts. Tip Adding alerts as a source will not affect your current configuration or notifications. Datarobot (formerly Algorithmia) By integrating Incident Intelligence with your Datarobot machine-learning models, you can monitor your machine learning model performance. To configure Datarobot for Incident Intelligence, see our integration docs. Aporia (MLOps) By integrating Incident Intelligence with your Aporia machine-learning models, you can monitor your machine learning model performance. To configure our Aporia integration, see our docs. Superwise (MLOps) By integrating Incident Intelligence with your Superwise machine-learning models, you can monitor your machine learning model performance. To configure our Superwise integration, see our docs. Anomalies By integrating Incident Intelligence with your New Relic Proactive Detection anomalies, you can get context and correlations from what you're monitoring. To get data from New Relic Proactive Detection anomalies: From one.newrelic.com, click Alerts & AI. On the left under Incident Intelligence, click Sources and then click Anomalies. Select the configurations you want to connect to Applied Intelligence by toggling to on (green). To add or remove alert policies, from Alerts & AI, click Sources, then Alerts. Tip Adding anomalies as a source won't affect your current Proactive Detection configurations or notifications. AWS You can integrate Incident Intelligence with Amazon CloudWatch to provide incident management for all of your AWS services. To integrate Amazon CloudWatch: Go to one.newrelic.com and click Alerts & AI. On the left under Incident Intelligence, click Sources and then click Amazon Web Services. Copy the URL. Create a new Amazon SNS topic. Set CloudWatch to forward all Alarms state changes to that topic: In the Amazon CloudWatch UI, click Events > Event Pattern. Select Service Name > CloudWatch. Select Event Type > CloudWatch Alarm State Change. Select Targets > SNS Topic, and select your new Amazon SNS topic. Create a new subscription: In the Amazon AWS UI, click Create a Subscription. Select your new Amazon SNS topic. Select Protocol > choose HTTPS. In Endpoint, paste the URL you previously copied from the Applied Intelligence Sources. Grafana You can integrate Incident Intelligence with Grafana's notifications for insight into events across your applications and environment. Grafana's webhook notification is a simple way to send information over HTTP to a custom endpoint. To integrate Grafana as a new webhook: Log into your Grafana portal using Admin permissions, and choose Alerting. On the Grafana Notification Channels page, click New Channel > Webhook. Go to one.newrelic.com and click Alerts & AI. On the left under Incident Intelligence, click Sources, and then click Grafana. Copy the URL, and paste it into your new Grafana webhook. PagerDuty EOL NOTICE As of October 2021, we've discontinued support for several capabilities with PagerDuty, including suggested responders, golden signals, and component enrichment. For more details, including how you can easily make this transition, see our Explorers Hub post. You can integrate Incident Intelligence directly with your PagerDuty services to ingest, process, and enhance all of your PagerDuty incidents. Connecting PagerDuty services to Applied Intelligence will not affect your current services or notifications. To get data from PagerDuty: Make sure your PagerDuty API key has write access. From one.newrelic.com, click Alerts & AI. On the left under Incident Intelligence, click Sources and then click PagerDuty. Enter your PagerDuty API key. The key should be either a personal or general access key with write access. If it's created by a user, the user should be an admin. Select the PagerDuty services you want to connect to Applied Intelligence, and click Connect. You can add additional services or remove services you've already connected in Sources > PagerDuty. Prometheus Alertmanager By integrating Incident Intelligence with Prometheus Alertmanager, you can receive and correlate your Prometheus alerts with events from other sources. To integrate Prometheus Alertmanager: Set up your Alertmanager configuration file by running: ./alertmanager -config.file=simple.yml Copy Go to one.newrelic.com and click Alerts & AI. On the left under Incident Intelligence, click Sources and then click Prometheus Alertmanager. Copy the Prometheus Alertmanager URL, and paste it into the <webhook_config>/url section of your Alertmanager config file. Reload the Prometheus Alertmanager configuration with one of the two methods: Send a SIGHUP to the process. Send an HTTP POST request to the /-/reload endpoint. REST API Incident Intelligence supports a dedicated REST API interface that lets you integrate with additional systems. The interface allows instrumentation of your code or other monitoring solutions to report any kind of metric or event. A metric can be a raw data point such as CPU, memory, disk utilization, or business KPI. An event can be a monitoring alert, deployment event, incident, exceptions or any other change in state that you want to describe. You can also send any type of data to Incident Intelligence straight from your own systems or applications. The REST API supports secure token-based authentication and accepts JSON content as input. For more information on authentication and the full API reference, see REST API for New Relic Applied Intelligence. Splunk By integrating Incident Intelligence with your Splunk log monitoring, you can: Use your environment's log data for searches and key term reports. Correlate alerts and search reports with your other metrics and incidents. Important Applied Intelligence supports Splunk Light, Splunk Cloud, and Splunk Enterprise version 6.3 and higher. To get data from Splunk: In your Splunk console, start a search for the relevant events. Save your search as an alert, configure your alert conditions, and then choose the webhook as the delivery method. Go to one.newrelic.com and click Alerts & AI. On the left under Incident Intelligence, click Sources and then click Splunk. Copy the collector URL, and paste it into the webhook endpoint in the Splunk console. Optional: Use Splunk tokens to enrich alert data with Splunk metadata. Splunk metadata To enrich alerts data with your Splunk metadata, use Splunk tokens. This helps you leverage your search data, which includes metadata and values from the first row of search results. If you want to... Do this... Access search data Use the format $<fieldname>$. For example, use $app$ for the app context for the search. Access field values To access field values from the first result row that a search returns, use the format $result.<fieldname>$. For example, use $result.host$ for the host value and $result.sourcetype$ for the source type. Use variables You can leverage any of the Selected fields in the Splunk search and add any unique fields to the Selected fields to make the data available as a variable. The following fields will automatically provide hints to the correlation engine: app: parsed as APPLICATION_NAME application:parsed as APPLICATION_NAME application_name: parsed as APPLICATION_NAME cluster: parsed as CLUSTER_NAME computer: parsed as HOST_NAME Dc: parsed as DATACENTER_NAME datacenter: parsed as DATACENTER_NAME host: parsed as HOST_NAME host_name: parsed as HOST_NAME hostname: parsed as HOST_NAME transaction: parsed as EVENT_ID Transaction_id: parsed as EVENT_ID user: parsed as USER_NAME 3. Configure destinations (ServiceNow and others) Now that you've set up your sources, you can configure your destinations. These are the data outputs where you view your incidents. Configure ServiceNow (example) Using ServiceNow as a destination enables you to push valuable violation data into new ServiceNow incident tickets. Send data to ServiceNow To configure Incident Intelligence to send data to ServiceNow: Go to one.newrelic.com, click Alerts & AI, in the left nav under Incident Intelligence click Destinations, then click ServiceNow. Required: Enter a channel name. This is used internally in Applied Intelligence to identify the destination (for example, in Pathways). Required: Enter your ServiceNow credentials: Team domain (This must be unique. No two destinations can have the same domain). Username Password Follow the two-way integration on screen instructions: Open and download this XML file. In the ServiceNow sidebar menu, go to System Definition > Business Rule. Click the menu icon in one of the column headers, select Import XML, and upload the XML file you downloaded. The two way integration will allow the ServiceNow incident to be updated with changes to the Applied Intelligence issue. Closing a ServiceNow incident will close its corresponding New Relic issue. When a New Relic issue is resolved, the corresponding ServiceNow incident will be closed. Custom notification message Applied Intelligence uses a templating framework called Jinja2 in the customization section interface. The Value field must be in valid Jinja syntax. By default, the interface populates a set of default fields in ServiceNow. When you add a custom field, enter the ServiceNow field name you want to use. When you want to skip a selected field in an issue update, add the | skip_on_update string at the end of the value you've selected. Tip By default, ServiceNow adds u_ to the beginning of its custom values. When mapping to ServiceNow attributes, use the Column name value. Please note that the name needs to be lowercase separated by underscores. Go here to see the custom notification message attribute descriptions. Go here to see Jinja2 Useful Syntax. Other destinations You can set other destinations: Send data to PagerDuty EOL NOTICE As of October 2021, we've discontinued support for several capabilities with PagerDuty, including suggested responders, golden signals, and component enrichment. For more details, including how you can easily make this transition, see our Explorers Hub post. Recommended: Create a new PagerDuty service to use as a destination. Because PagerDuty services can also be used as sources, this can help you distinguish your data input from your output. To create a PagerDuty destination: Go to one.newrelic.com, click Alerts & AI, in the left nav under Incident Intelligence click Destinations, then click PagerDuty. Enter your PagerDuty API key. The key should be either a personal or general access key with write access. If it's created by a user, the user should be an admin. If you've configured a PagerDuty source with an API key, you can use the same key. Select the PagerDuty services you want to connect to Applied Intelligence, and click Connect. When you're ready, you can add policies for one or more PagerDuty destinations. You can also transfer the policies over from your existing services or leave them as sources if needed. From the Destinations > PagerDuty page, you can also: Review the permissions for your services. Click Authorize when you're done. Add or delete existing services from the PagerDuty destination. Edit permissions for any service. To configure your PagerDuty destinations, use the following settings: Configuration setting Description Trigger new incidents Required. Trigger correlated parent incidents so you can identify issues faster. Edit incident titles Required. Alter your incident titles to help you orient and understand issues. Add new integrations Required. Add integrations to enable incident creation for selected services. Add webhook extensions Add webhook extensions to sync user actions in PagerDuty to New Relic. This lets you update the correlated issue state. Auto-resolve correlated incidents When enabled, this will resolve and automatically close correlated parent/child incidents. Select a user to take actions in PagerDuty You need to select a user before you can enable deep integration with PagerDuty. Once you do, the user can: Add notes to incidents (required): Incident notes are used to enrich incidents with context. Acknowledge triggered incidents: When enabled, Applied Intelligence will acknowledge and correlate newly triggered incidents in PagerDuty before you're notified. Use the original escalation policy: When enabled, the escalation policy of the source service will be applied to each incident. Send data via webhook Incident Intelligence will send the event body in JSON format by HTTPS POST. The system expects the endpoint to return a successful HTTP code (2xx). To configure Incident Intelligence to send data via webhook: Go to one.newrelic.com, click Alerts & AI, in the left nav under Incident Intelligence click Destinations, then click Webhook. Required: Configure the unique webhook key, used in Applied Intelligence to refer to this webhook configuration and its specific settings. Required: Configure the destination endpoint where the webhook payload will be sent. Optional steps: Configure custom headers, which are key:value pairs of headers to be sent with the request. Example: \"Authentication\" \"Bearer\" <bearer token> Configure a custom payload template that can be used to map New Relic fields to match the destination tool's expected name and format. Configure priority mapping (critical, high, medium, or low), used to map New Relic's priorities to the priorities expected at the destination. Tip There’s a retry mechanism that is triggered a few times with exponential backoff for a couple of minutes once an error occurs. If we reach the retry limit, the Webhook will get auto-disabled. For examples of destination templates, webhook formats, and JSON schema, see the Incident Intelligence destination examples. 4. Configure pathways To control when and where you want to receive notifications from your incidents, you can configure pathways. To add a pathway: Go to one.newrelic.com, click Alerts & AI, in the left nav under Incident Intelligence click Pathways, then click Add a pathway. In the query builder box, select an attribute, such as application/name. This can be from the list of all attributes available in PagerDuty incidents and New Relic alerts violations, or you can add your own attributes. Select a logical operator. For example, contains. Enter a specific value to complete the logical expression. To include all issues created by your sources, select Send everything. (Use this if you only use one PagerDuty service to manage all incidents.) To build more complex logic, use the AND/OR operators. Select one or more of your destinations. To edit or remove existing pathways, mouse over the pathway's name on the Pathways page. What's next? Now that you've set up some sources and destinations for your incidents, read about how to use Incident Intelligence.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.17322,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Get started with <em>Incident</em> <em>Intelligence</em>",
        "sections": "Get started with <em>Incident</em> <em>Intelligence</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": " one.newrelic.com, click <em>Alerts</em> &amp; AI. On the left under <em>Incident</em> <em>Intelligence</em>, click Sources and then click Anomalies. Select the configurations you want to connect to <em>Applied</em> <em>Intelligence</em> by toggling to on (green). To add or remove <em>alert</em> policies, from <em>Alerts</em> &amp; AI, click Sources, then <em>Alerts</em>. Tip Adding anomalies"
      },
      "id": "603ea62e64441f119f4e883f"
    }
  ],
  "/docs/alerts-applied-intelligence/applied-intelligence/incident-intelligence/use-incident-intelligence": [
    {
      "sections": [
        "Alerts and applied intelligence notification integrations",
        "Early access",
        "Integration details",
        "Atlassian Jira",
        "Permissions",
        "Set up a Jira destination",
        "Important",
        "Two-way sync",
        "Configure the message template",
        "Send a test notification",
        "ServiceNow (Incident-Management)",
        "Roles",
        "Set up a destination",
        "Slack",
        "Prerequisites",
        "Set up a Slack destination",
        "Configure the Slack message settings",
        "Webhook",
        "Set up a webhook destination",
        "Configure the webhook event template",
        "Customize the webhook payload",
        "Email",
        "Configure the email settings"
      ],
      "title": "Alerts and applied intelligence notification integrations",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Incident Intelligence",
        "Destinations"
      ],
      "external_id": "7220c630fc187bb61784ff2cc2213e588b269b00",
      "image": "https://docs.newrelic.com/static/d4e9baecc3a76dd1a5945f8ec0aeca66/c1b63/webhook-notification-template.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/notifications/notification-integrations/",
      "published_at": "2021-12-19T15:28:40Z",
      "updated_at": "2021-12-19T15:28:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Early access The features described here are early access. You won't be able to use these features if you're not part of the early access program. For more information on related features, see our docs on Alerts notification channels, Incident Intelligence destinations, and Proactive Detection notifications. Alerts and Applied Intelligence notification integrations are specific services and platforms you can use to send notifications from New Relic. Integration details Read more about each of our specific notification integrations. Atlassian Jira Integrate New Relic with Atlassian Jira(Cloud) and automatically create and update Jira issues. Permissions The required permissions from the Jira API-Token are create, edit, and close tickets. To enable the two-way sync toggle, the provided Jira API-Key should have an Admin role. Set up a Jira destination Create Jira issues, then enable Jira and New Relic to share updates and stay synced. To create a Jira destination, enter the following information: Destination name: Custom name to identify the destination Jira account endpoint: the URL of the destination User-name: this will be the email address of the user making the connection API token: generated from your Atlassian account Important New Relic currently supports Atlassian-Jira Classic (company-managed) projects. Before saving the destination, we recommend you test the connection via the test connection button. Jira destination configuration. We recommand to test the connection before saving. Two-way sync You can enable a two-way integration with Jira to keep the issues' state synced with the corresponding state in New Relic. To enable two-way sync, turn on the ‘two-way integration’ toggle. When turned on, a Jira Webhook would be created in your Jira account at a later stage, for the selected project (see ‘customize a message template’). The webhook would contain access details to Newrelic (URL and Newrelic-API-KEY) Configure the message template To configure a template for a Jira issue, you first need to choose a destination. You will be able to create a new destination at this stage. Upon successful connection to the destination, you will need to choose a project, and then select the Jira issue type you would like to be used. Once the issue-type is selected, the configured project's fields are fetched from your account and automatically mapped to your Jira instance. To help you get started, we automatically present the required and recommended fields and values. Required fields must be set with a value. You can add or remove optional fields(use the X mark on their right side) Jira message template. Send a test notification You can see how the JIRA issue will appear by clicking a test notification with default field values. If successful, a JIRA issue will be created and a link will appear. ServiceNow (Incident-Management) Integrate New Relic with ServiceNow Incident-Management and automatically create and update incidents. Roles As part of the integrations, we fetch fields from the your serviceNow incident table and optional values. For this, the provided ServiceNow user details required read permissions for the tables: sys_dictionary, sys_choice, sys_user and task. A read/write permission to incident To be able to fetch users for the caller column, we required read permissions for the sys_users table. The above permissions can be achieved with the roles personalize_choices, personalize_dictionary, rest_service, itil. Read/Write permissions to the api_key_credentials table is required to enable two-way integration. This can be covered with the roles credentials_admin and discovery_admin. Set up a destination To create a ServiceNow destination, enter the following information: Destination Name: custom name to identify the destination Domain: the URL of the destination User-name: the name of the user Password: the user name’s password Before saving the destination, we recommend testing the connection by clicking the test connection button. Two-way sync You can configure a two-way integration with ServiceNow Incidents Management to keep the incidents' state synced with the corresponding state in New Relic. Here are some required steps to remember when configuring the two-way integration: Turn on the two-way integration toggle. Open and download this XML file, which includes the business rule triggering events back to New Relic One. In the ServiceNow sidebar menu, go to System Definition > Business Rules. Click the menu icon in one of the column headers, select Import XML and upload the XML file you downloaded. Once the Destination is saved, a New-Relic API-Key will be kept in the api_key_credentials. The key would sent in a header as part of the callback REST call to New-Relic Configure the message template Upon a successful connection, ServiceNow incident table columns are fetched from your account and automatically mapped to your ServiceNow instance. To help you get started, we automatically present the required and recommended fields and values. Required fields must be set with a value. You can add or remove optional fields(use the X mark on their right side) Select, edit or remove fields for the ServiceNow-Incident template. Send a test notification You can see how the ServiceNow incident will appear by clicking a test notification with default field values. If successful, an incident will be created and a link will appear. Slack Send notifications-messages to your Slack channels. Prerequisites Your Slack workspace needs to have the New Relic application installed. The application must be approved by a workspace admin before it can be individually installed by users Set up a Slack destination Click on the `one-click Slack authentication' will lead you to the Slack landing page to continue the OAuth2 authentication process. On the Slack landing page, if you're not signed into the required workspace, you're redirected to Slack to sign in. Add your workspace name or select the relevant workspace and click Continue. When signed in to the selected workspace, you are requested to allow New Relic to perform the specified actions. Clicking `Allow' will redirect you back to the Destination page. Configure the Slack message settings Select a Destination(Workspace) and select a Slack-channel where the messages will be sent. You can create a new destination if there is no pre-defined destination for the required workspace. Note that, for privacy reasons, users need to be authenticated to select private channels (one-time process) Send a test notification You can send a test notification with a pre-defined example payload to the channel. This creates a message in the selected Slack-channel. Webhook Use the webhook notifier to send the notification messages to any endpoint you like. Set up a webhook destination To create a webhook destination, you need the following: Destination Name: A unique destination name URL: the endpoint of the target application, authentication and custom headers if needed. Authorization mechanism (Optional):. Can be basic authentication or a bearer token Configure the webhook event template Pick a webhook destination from the list and configure the HTTP-POST request. The request configuration requires you to: Set a name for the template. Select a pre-configured destination from the destinations list or create a new one. Add custom headers (optional). Configure the request’s payload. Customize the webhook payload You can use the default payload or customize it to contain the required data. Pick Variables from the variables menu and apply handlebars syntax to enrich your webhook. Note that the request’s content-type is JSON by default. Hence, the payload needs to keep the JSON form. See Usage examples The ‘preview’ section on the right hand-side shows an expected payload after the template is rendered. If the eventual payload would not form a valid Json, an error will be shown and it won’t be possible to save the template. If the webhook payload conforms a valid Json, you can send a test notification to your defined webhook destination We recommend sending a test notification to make sure that everything's connected correctly. Email Send email notifications to users. Configure the email settings Add one or more recipients. Users with New Relic accounts can be found via autocomplete when searching for their name or email address. To add a user without a New Relic account or email distribution list, add the full email address. Every recipient will be translated into a 'destination'. You can follow the email notifications per destination in the notifications-log Send a test notification You can send a test notification to make sure the email notifications reach the inbox.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 368.69183,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Alerts</em> <em>and</em> <em>applied</em> <em>intelligence</em> notification integrations",
        "sections": "<em>Alerts</em> <em>and</em> <em>applied</em> <em>intelligence</em> notification integrations",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "Early access The features described here are early access. You won&#x27;t be able to use these features if you&#x27;re not part of the early access program. For more information on related features, see our docs on <em>Alerts</em> notification channels, <em>Incident</em> <em>Intelligence</em> destinations, and Proactive Detection"
      },
      "id": "618ff71628ccbc60710321e4"
    },
    {
      "sections": [
        "Workflows",
        "What is an issue",
        "Add a Workflow",
        "Tip",
        "Enrichments",
        "Important",
        "Notifier"
      ],
      "title": "Workflows",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Workflows",
        "Notifications"
      ],
      "external_id": "0993c323f76467d22136cac6982ccd48e859f722",
      "image": "",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/applied-intelligence/incident-workflows/incident-workflows/",
      "published_at": "2021-12-19T13:46:37Z",
      "updated_at": "2021-12-17T01:48:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With workflows you control when and where you want to receive notifications about issues, tunnel the right information to the relevant person or team, and enrich your issue's notifications with additional New Relic data. What is an issue Issues are groups of incidents that describe the underlying problem of your symptoms. When a new incident is created, Incident Intelligence opens an issue and evaluates other open issues for correlations. For more information see Use Incident Inteligence. Add a Workflow Tip The maximum number of workflows you can add is 100 per environment and 50 per account. The workflows feature is located under the Alerts & AI menu. Go to one.newrelic.com, click Alerts & AI, in the left navigation under Enrich and Notify click Workflow, then click Add a workflow. Name your workflow. This field is mandatory and needs to be unique. Workflows filters the issues you want to send to the relevant destination. You can send all issues by selecting Send all issues or you can build a query to filter the right issues. Build a query by selecting an attribute, operator and value/s to narrow down the right issues you want to be passed on. Optional: Enriched Data, available to full platform users. Enrichments Enrichments can give additional context on alert notifications by adding NRQL query results to them The workflows enrich tool is similar to the query builder. The NRQL query's results are included in the notification message. If you create queries for your workflows, they run in real time and the query results are the value of the enricher. You can save any valid query into your workflow, even if they return an empty result, but you must run the query before saving the enrichment. You can also query with issue-specific variables; for example {{entitiesData.ids}} For enrichment examples, see Workflow data enrichment examples. Note: The query name needs to be unique because you'll use the name to decide how to include the result in the payload of the notification that will be sent by the workflow. Important At this stage, visual representation of enrichments are sent to Slack and email. For Servicenow and JIRA destinations, we recommend to limit to NRDB queries with single value results. For example: count, min, or max. The maximum number of enrichments per workflow is 5. The maximum number of variables an enrichment can contain is 1. Notify: Choose one or more destinations and add an optional message. Notifier To save and activate a Workflow, you need to include a notifier. Setting up a notifier requires having a notification destination configured. You can either set it up through the Destination menu item, or through the Create new destination item in the destination selection drop down in each destination channel screen. A notification destination includes the credentials required to send a notification to a third party platform and an option to customize the message sent by mapping fields from our issues to fields in the destination platform. To learn how to set up destinations and configure message templates, check the documentation on notifications. Click update message once completing the notifier requirements Optionally, test your workflow. We'll use existing data from your account to test what you've configured and send a sample notification Click activate to complete the workflow. Tip In any destination channel, start typing and a variable menu will open up. You'll see the names of the variables, which will be replaced with the variable's values at runtime. To use the enrichers' results, use their name.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 292.56055,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": " problem of your symptoms. When a new <em>incident</em> is created, <em>Incident</em> <em>Intelligence</em> opens an issue and evaluates other open issues for correlations. For more information see Use <em>Incident</em> Inteligence. Add a Workflow Tip The maximum number of workflows you can add is 100 per environment and 50 per account"
      },
      "id": "603e967664441f7e6f4e889b"
    },
    {
      "sections": [
        "Get started with Incident Intelligence",
        "Requirements",
        "Set up Incident Intelligence",
        "1. Configure your environment (one-time)",
        "Tip",
        "2. Configure sources",
        "Alerts",
        "Datarobot (formerly Algorithmia)",
        "Aporia (MLOps)",
        "Superwise (MLOps)",
        "Anomalies",
        "AWS",
        "Grafana",
        "PagerDuty",
        "EOL NOTICE",
        "Prometheus Alertmanager",
        "REST API",
        "Splunk",
        "Important",
        "Splunk metadata",
        "3. Configure destinations (ServiceNow and others)",
        "Configure ServiceNow (example)",
        "Send data to ServiceNow",
        "Custom notification message",
        "Other destinations",
        "Send data to PagerDuty",
        "Send data via webhook",
        "4. Configure pathways",
        "What's next?"
      ],
      "title": "Get started with Incident Intelligence",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Incident intelligence"
      ],
      "external_id": "8c4a5a914ca01cb42250908d2fb1a12ccc697e25",
      "image": "https://docs.newrelic.com/static/5cb28999dc618c6a5a2b8be1fa72e660/b97f6/image-%25281%2529_0.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/applied-intelligence/incident-intelligence/get-started-incident-intelligence/",
      "published_at": "2021-12-19T14:23:58Z",
      "updated_at": "2021-12-09T22:19:28Z",
      "document_type": "page",
      "popularity": 1,
      "body": "As part of Applied Intelligence, Incident Intelligence helps you correlate your incidents and reduce noise in your environment. It gives you an overview of all your incidents, their sources, and related events. Requirements If you haven't already, sign up for a New Relic account to get started. To use most Applied Intelligence features, you must be a full platform user. For more information, see user type capabilities. Set up Incident Intelligence To enable Incident Intelligence, follow these steps. Afterwards, issues should start to appear in your issue feed. 1. Configure your environment (one-time). 2. Configure sources. 3. Configure destinations. 4. Configure pathways. 1. Configure your environment (one-time) To set up an environment in Incident Intelligence, you need an administrator to select a New Relic account for it. This account should be the one your team is using. Who sets the environment? Only administrators, and only for accounts where they have admin privileges. Can administrators set more than one environment? They can set one environment per parent account and its child accounts. More than one can be set if an administrator has privileges for more than one parent account. Tip Incident Intelligence is a cross-account product. This means you can send in data from any New Relic account or external source to correlate events. 2. Configure sources After setting up your environment, determine your incident sources. These are your data inputs. You can get data from any of the following sources: Alerts By integrating Incident Intelligence with your alerts violations, you can get context and correlations from what you're monitoring. To get data from alerts: From one.newrelic.com, click Alerts. On the left under Incident Intelligence, click Sources and then click Alerts. Select the policies you want to connect to Applied Intelligence, and click Connect. You can add additional alerts policies or remove policies you've already connected in Sources > New Relic Alerts. Tip Adding alerts as a source will not affect your current configuration or notifications. Datarobot (formerly Algorithmia) By integrating Incident Intelligence with your Datarobot machine-learning models, you can monitor your machine learning model performance. To configure Datarobot for Incident Intelligence, see our integration docs. Aporia (MLOps) By integrating Incident Intelligence with your Aporia machine-learning models, you can monitor your machine learning model performance. To configure our Aporia integration, see our docs. Superwise (MLOps) By integrating Incident Intelligence with your Superwise machine-learning models, you can monitor your machine learning model performance. To configure our Superwise integration, see our docs. Anomalies By integrating Incident Intelligence with your New Relic Proactive Detection anomalies, you can get context and correlations from what you're monitoring. To get data from New Relic Proactive Detection anomalies: From one.newrelic.com, click Alerts & AI. On the left under Incident Intelligence, click Sources and then click Anomalies. Select the configurations you want to connect to Applied Intelligence by toggling to on (green). To add or remove alert policies, from Alerts & AI, click Sources, then Alerts. Tip Adding anomalies as a source won't affect your current Proactive Detection configurations or notifications. AWS You can integrate Incident Intelligence with Amazon CloudWatch to provide incident management for all of your AWS services. To integrate Amazon CloudWatch: Go to one.newrelic.com and click Alerts & AI. On the left under Incident Intelligence, click Sources and then click Amazon Web Services. Copy the URL. Create a new Amazon SNS topic. Set CloudWatch to forward all Alarms state changes to that topic: In the Amazon CloudWatch UI, click Events > Event Pattern. Select Service Name > CloudWatch. Select Event Type > CloudWatch Alarm State Change. Select Targets > SNS Topic, and select your new Amazon SNS topic. Create a new subscription: In the Amazon AWS UI, click Create a Subscription. Select your new Amazon SNS topic. Select Protocol > choose HTTPS. In Endpoint, paste the URL you previously copied from the Applied Intelligence Sources. Grafana You can integrate Incident Intelligence with Grafana's notifications for insight into events across your applications and environment. Grafana's webhook notification is a simple way to send information over HTTP to a custom endpoint. To integrate Grafana as a new webhook: Log into your Grafana portal using Admin permissions, and choose Alerting. On the Grafana Notification Channels page, click New Channel > Webhook. Go to one.newrelic.com and click Alerts & AI. On the left under Incident Intelligence, click Sources, and then click Grafana. Copy the URL, and paste it into your new Grafana webhook. PagerDuty EOL NOTICE As of October 2021, we've discontinued support for several capabilities with PagerDuty, including suggested responders, golden signals, and component enrichment. For more details, including how you can easily make this transition, see our Explorers Hub post. You can integrate Incident Intelligence directly with your PagerDuty services to ingest, process, and enhance all of your PagerDuty incidents. Connecting PagerDuty services to Applied Intelligence will not affect your current services or notifications. To get data from PagerDuty: Make sure your PagerDuty API key has write access. From one.newrelic.com, click Alerts & AI. On the left under Incident Intelligence, click Sources and then click PagerDuty. Enter your PagerDuty API key. The key should be either a personal or general access key with write access. If it's created by a user, the user should be an admin. Select the PagerDuty services you want to connect to Applied Intelligence, and click Connect. You can add additional services or remove services you've already connected in Sources > PagerDuty. Prometheus Alertmanager By integrating Incident Intelligence with Prometheus Alertmanager, you can receive and correlate your Prometheus alerts with events from other sources. To integrate Prometheus Alertmanager: Set up your Alertmanager configuration file by running: ./alertmanager -config.file=simple.yml Copy Go to one.newrelic.com and click Alerts & AI. On the left under Incident Intelligence, click Sources and then click Prometheus Alertmanager. Copy the Prometheus Alertmanager URL, and paste it into the <webhook_config>/url section of your Alertmanager config file. Reload the Prometheus Alertmanager configuration with one of the two methods: Send a SIGHUP to the process. Send an HTTP POST request to the /-/reload endpoint. REST API Incident Intelligence supports a dedicated REST API interface that lets you integrate with additional systems. The interface allows instrumentation of your code or other monitoring solutions to report any kind of metric or event. A metric can be a raw data point such as CPU, memory, disk utilization, or business KPI. An event can be a monitoring alert, deployment event, incident, exceptions or any other change in state that you want to describe. You can also send any type of data to Incident Intelligence straight from your own systems or applications. The REST API supports secure token-based authentication and accepts JSON content as input. For more information on authentication and the full API reference, see REST API for New Relic Applied Intelligence. Splunk By integrating Incident Intelligence with your Splunk log monitoring, you can: Use your environment's log data for searches and key term reports. Correlate alerts and search reports with your other metrics and incidents. Important Applied Intelligence supports Splunk Light, Splunk Cloud, and Splunk Enterprise version 6.3 and higher. To get data from Splunk: In your Splunk console, start a search for the relevant events. Save your search as an alert, configure your alert conditions, and then choose the webhook as the delivery method. Go to one.newrelic.com and click Alerts & AI. On the left under Incident Intelligence, click Sources and then click Splunk. Copy the collector URL, and paste it into the webhook endpoint in the Splunk console. Optional: Use Splunk tokens to enrich alert data with Splunk metadata. Splunk metadata To enrich alerts data with your Splunk metadata, use Splunk tokens. This helps you leverage your search data, which includes metadata and values from the first row of search results. If you want to... Do this... Access search data Use the format $<fieldname>$. For example, use $app$ for the app context for the search. Access field values To access field values from the first result row that a search returns, use the format $result.<fieldname>$. For example, use $result.host$ for the host value and $result.sourcetype$ for the source type. Use variables You can leverage any of the Selected fields in the Splunk search and add any unique fields to the Selected fields to make the data available as a variable. The following fields will automatically provide hints to the correlation engine: app: parsed as APPLICATION_NAME application:parsed as APPLICATION_NAME application_name: parsed as APPLICATION_NAME cluster: parsed as CLUSTER_NAME computer: parsed as HOST_NAME Dc: parsed as DATACENTER_NAME datacenter: parsed as DATACENTER_NAME host: parsed as HOST_NAME host_name: parsed as HOST_NAME hostname: parsed as HOST_NAME transaction: parsed as EVENT_ID Transaction_id: parsed as EVENT_ID user: parsed as USER_NAME 3. Configure destinations (ServiceNow and others) Now that you've set up your sources, you can configure your destinations. These are the data outputs where you view your incidents. Configure ServiceNow (example) Using ServiceNow as a destination enables you to push valuable violation data into new ServiceNow incident tickets. Send data to ServiceNow To configure Incident Intelligence to send data to ServiceNow: Go to one.newrelic.com, click Alerts & AI, in the left nav under Incident Intelligence click Destinations, then click ServiceNow. Required: Enter a channel name. This is used internally in Applied Intelligence to identify the destination (for example, in Pathways). Required: Enter your ServiceNow credentials: Team domain (This must be unique. No two destinations can have the same domain). Username Password Follow the two-way integration on screen instructions: Open and download this XML file. In the ServiceNow sidebar menu, go to System Definition > Business Rule. Click the menu icon in one of the column headers, select Import XML, and upload the XML file you downloaded. The two way integration will allow the ServiceNow incident to be updated with changes to the Applied Intelligence issue. Closing a ServiceNow incident will close its corresponding New Relic issue. When a New Relic issue is resolved, the corresponding ServiceNow incident will be closed. Custom notification message Applied Intelligence uses a templating framework called Jinja2 in the customization section interface. The Value field must be in valid Jinja syntax. By default, the interface populates a set of default fields in ServiceNow. When you add a custom field, enter the ServiceNow field name you want to use. When you want to skip a selected field in an issue update, add the | skip_on_update string at the end of the value you've selected. Tip By default, ServiceNow adds u_ to the beginning of its custom values. When mapping to ServiceNow attributes, use the Column name value. Please note that the name needs to be lowercase separated by underscores. Go here to see the custom notification message attribute descriptions. Go here to see Jinja2 Useful Syntax. Other destinations You can set other destinations: Send data to PagerDuty EOL NOTICE As of October 2021, we've discontinued support for several capabilities with PagerDuty, including suggested responders, golden signals, and component enrichment. For more details, including how you can easily make this transition, see our Explorers Hub post. Recommended: Create a new PagerDuty service to use as a destination. Because PagerDuty services can also be used as sources, this can help you distinguish your data input from your output. To create a PagerDuty destination: Go to one.newrelic.com, click Alerts & AI, in the left nav under Incident Intelligence click Destinations, then click PagerDuty. Enter your PagerDuty API key. The key should be either a personal or general access key with write access. If it's created by a user, the user should be an admin. If you've configured a PagerDuty source with an API key, you can use the same key. Select the PagerDuty services you want to connect to Applied Intelligence, and click Connect. When you're ready, you can add policies for one or more PagerDuty destinations. You can also transfer the policies over from your existing services or leave them as sources if needed. From the Destinations > PagerDuty page, you can also: Review the permissions for your services. Click Authorize when you're done. Add or delete existing services from the PagerDuty destination. Edit permissions for any service. To configure your PagerDuty destinations, use the following settings: Configuration setting Description Trigger new incidents Required. Trigger correlated parent incidents so you can identify issues faster. Edit incident titles Required. Alter your incident titles to help you orient and understand issues. Add new integrations Required. Add integrations to enable incident creation for selected services. Add webhook extensions Add webhook extensions to sync user actions in PagerDuty to New Relic. This lets you update the correlated issue state. Auto-resolve correlated incidents When enabled, this will resolve and automatically close correlated parent/child incidents. Select a user to take actions in PagerDuty You need to select a user before you can enable deep integration with PagerDuty. Once you do, the user can: Add notes to incidents (required): Incident notes are used to enrich incidents with context. Acknowledge triggered incidents: When enabled, Applied Intelligence will acknowledge and correlate newly triggered incidents in PagerDuty before you're notified. Use the original escalation policy: When enabled, the escalation policy of the source service will be applied to each incident. Send data via webhook Incident Intelligence will send the event body in JSON format by HTTPS POST. The system expects the endpoint to return a successful HTTP code (2xx). To configure Incident Intelligence to send data via webhook: Go to one.newrelic.com, click Alerts & AI, in the left nav under Incident Intelligence click Destinations, then click Webhook. Required: Configure the unique webhook key, used in Applied Intelligence to refer to this webhook configuration and its specific settings. Required: Configure the destination endpoint where the webhook payload will be sent. Optional steps: Configure custom headers, which are key:value pairs of headers to be sent with the request. Example: \"Authentication\" \"Bearer\" <bearer token> Configure a custom payload template that can be used to map New Relic fields to match the destination tool's expected name and format. Configure priority mapping (critical, high, medium, or low), used to map New Relic's priorities to the priorities expected at the destination. Tip There’s a retry mechanism that is triggered a few times with exponential backoff for a couple of minutes once an error occurs. If we reach the retry limit, the Webhook will get auto-disabled. For examples of destination templates, webhook formats, and JSON schema, see the Incident Intelligence destination examples. 4. Configure pathways To control when and where you want to receive notifications from your incidents, you can configure pathways. To add a pathway: Go to one.newrelic.com, click Alerts & AI, in the left nav under Incident Intelligence click Pathways, then click Add a pathway. In the query builder box, select an attribute, such as application/name. This can be from the list of all attributes available in PagerDuty incidents and New Relic alerts violations, or you can add your own attributes. Select a logical operator. For example, contains. Enter a specific value to complete the logical expression. To include all issues created by your sources, select Send everything. (Use this if you only use one PagerDuty service to manage all incidents.) To build more complex logic, use the AND/OR operators. Select one or more of your destinations. To edit or remove existing pathways, mouse over the pathway's name on the Pathways page. What's next? Now that you've set up some sources and destinations for your incidents, read about how to use Incident Intelligence.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.17316,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Get started with <em>Incident</em> <em>Intelligence</em>",
        "sections": "Get started with <em>Incident</em> <em>Intelligence</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": " one.newrelic.com, click <em>Alerts</em> &amp; AI. On the left under <em>Incident</em> <em>Intelligence</em>, click Sources and then click Anomalies. Select the configurations you want to connect to <em>Applied</em> <em>Intelligence</em> by toggling to on (green). To add or remove <em>alert</em> policies, from <em>Alerts</em> &amp; AI, click Sources, then <em>Alerts</em>. Tip Adding anomalies"
      },
      "id": "603ea62e64441f119f4e883f"
    }
  ],
  "/docs/alerts-applied-intelligence/applied-intelligence/incident-workflows/custom-variables-incident-workflows": [
    {
      "sections": [
        "Workflows",
        "What is an issue",
        "Add a Workflow",
        "Tip",
        "Enrichments",
        "Important",
        "Notifier"
      ],
      "title": "Workflows",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Workflows",
        "Notifications"
      ],
      "external_id": "0993c323f76467d22136cac6982ccd48e859f722",
      "image": "",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/applied-intelligence/incident-workflows/incident-workflows/",
      "published_at": "2021-12-19T13:46:37Z",
      "updated_at": "2021-12-17T01:48:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With workflows you control when and where you want to receive notifications about issues, tunnel the right information to the relevant person or team, and enrich your issue's notifications with additional New Relic data. What is an issue Issues are groups of incidents that describe the underlying problem of your symptoms. When a new incident is created, Incident Intelligence opens an issue and evaluates other open issues for correlations. For more information see Use Incident Inteligence. Add a Workflow Tip The maximum number of workflows you can add is 100 per environment and 50 per account. The workflows feature is located under the Alerts & AI menu. Go to one.newrelic.com, click Alerts & AI, in the left navigation under Enrich and Notify click Workflow, then click Add a workflow. Name your workflow. This field is mandatory and needs to be unique. Workflows filters the issues you want to send to the relevant destination. You can send all issues by selecting Send all issues or you can build a query to filter the right issues. Build a query by selecting an attribute, operator and value/s to narrow down the right issues you want to be passed on. Optional: Enriched Data, available to full platform users. Enrichments Enrichments can give additional context on alert notifications by adding NRQL query results to them The workflows enrich tool is similar to the query builder. The NRQL query's results are included in the notification message. If you create queries for your workflows, they run in real time and the query results are the value of the enricher. You can save any valid query into your workflow, even if they return an empty result, but you must run the query before saving the enrichment. You can also query with issue-specific variables; for example {{entitiesData.ids}} For enrichment examples, see Workflow data enrichment examples. Note: The query name needs to be unique because you'll use the name to decide how to include the result in the payload of the notification that will be sent by the workflow. Important At this stage, visual representation of enrichments are sent to Slack and email. For Servicenow and JIRA destinations, we recommend to limit to NRDB queries with single value results. For example: count, min, or max. The maximum number of enrichments per workflow is 5. The maximum number of variables an enrichment can contain is 1. Notify: Choose one or more destinations and add an optional message. Notifier To save and activate a Workflow, you need to include a notifier. Setting up a notifier requires having a notification destination configured. You can either set it up through the Destination menu item, or through the Create new destination item in the destination selection drop down in each destination channel screen. A notification destination includes the credentials required to send a notification to a third party platform and an option to customize the message sent by mapping fields from our issues to fields in the destination platform. To learn how to set up destinations and configure message templates, check the documentation on notifications. Click update message once completing the notifier requirements Optionally, test your workflow. We'll use existing data from your account to test what you've configured and send a sample notification Click activate to complete the workflow. Tip In any destination channel, start typing and a variable menu will open up. You'll see the names of the variables, which will be replaced with the variable's values at runtime. To use the enrichers' results, use their name.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 573.5155,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Workflows</em>",
        "sections": "<em>Workflows</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": ". The <em>workflows</em> feature is located under the <em>Alerts</em> &amp; AI menu. Go to one.newrelic.com, click <em>Alerts</em> &amp; AI, in the left navigation under <em>Enrich</em> and Notify click <em>Workflow</em>, then click Add a <em>workflow</em>. Name your <em>workflow</em>. This field is mandatory and needs to be unique. <em>Workflows</em> filters the <em>issues</em> you want"
      },
      "id": "603e967664441f7e6f4e889b"
    },
    {
      "sections": [
        "Alerts and applied intelligence notification integrations",
        "Early access",
        "Integration details",
        "Atlassian Jira",
        "Permissions",
        "Set up a Jira destination",
        "Important",
        "Two-way sync",
        "Configure the message template",
        "Send a test notification",
        "ServiceNow (Incident-Management)",
        "Roles",
        "Set up a destination",
        "Slack",
        "Prerequisites",
        "Set up a Slack destination",
        "Configure the Slack message settings",
        "Webhook",
        "Set up a webhook destination",
        "Configure the webhook event template",
        "Customize the webhook payload",
        "Email",
        "Configure the email settings"
      ],
      "title": "Alerts and applied intelligence notification integrations",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Incident Intelligence",
        "Destinations"
      ],
      "external_id": "7220c630fc187bb61784ff2cc2213e588b269b00",
      "image": "https://docs.newrelic.com/static/d4e9baecc3a76dd1a5945f8ec0aeca66/c1b63/webhook-notification-template.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/notifications/notification-integrations/",
      "published_at": "2021-12-19T15:28:40Z",
      "updated_at": "2021-12-19T15:28:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Early access The features described here are early access. You won't be able to use these features if you're not part of the early access program. For more information on related features, see our docs on Alerts notification channels, Incident Intelligence destinations, and Proactive Detection notifications. Alerts and Applied Intelligence notification integrations are specific services and platforms you can use to send notifications from New Relic. Integration details Read more about each of our specific notification integrations. Atlassian Jira Integrate New Relic with Atlassian Jira(Cloud) and automatically create and update Jira issues. Permissions The required permissions from the Jira API-Token are create, edit, and close tickets. To enable the two-way sync toggle, the provided Jira API-Key should have an Admin role. Set up a Jira destination Create Jira issues, then enable Jira and New Relic to share updates and stay synced. To create a Jira destination, enter the following information: Destination name: Custom name to identify the destination Jira account endpoint: the URL of the destination User-name: this will be the email address of the user making the connection API token: generated from your Atlassian account Important New Relic currently supports Atlassian-Jira Classic (company-managed) projects. Before saving the destination, we recommend you test the connection via the test connection button. Jira destination configuration. We recommand to test the connection before saving. Two-way sync You can enable a two-way integration with Jira to keep the issues' state synced with the corresponding state in New Relic. To enable two-way sync, turn on the ‘two-way integration’ toggle. When turned on, a Jira Webhook would be created in your Jira account at a later stage, for the selected project (see ‘customize a message template’). The webhook would contain access details to Newrelic (URL and Newrelic-API-KEY) Configure the message template To configure a template for a Jira issue, you first need to choose a destination. You will be able to create a new destination at this stage. Upon successful connection to the destination, you will need to choose a project, and then select the Jira issue type you would like to be used. Once the issue-type is selected, the configured project's fields are fetched from your account and automatically mapped to your Jira instance. To help you get started, we automatically present the required and recommended fields and values. Required fields must be set with a value. You can add or remove optional fields(use the X mark on their right side) Jira message template. Send a test notification You can see how the JIRA issue will appear by clicking a test notification with default field values. If successful, a JIRA issue will be created and a link will appear. ServiceNow (Incident-Management) Integrate New Relic with ServiceNow Incident-Management and automatically create and update incidents. Roles As part of the integrations, we fetch fields from the your serviceNow incident table and optional values. For this, the provided ServiceNow user details required read permissions for the tables: sys_dictionary, sys_choice, sys_user and task. A read/write permission to incident To be able to fetch users for the caller column, we required read permissions for the sys_users table. The above permissions can be achieved with the roles personalize_choices, personalize_dictionary, rest_service, itil. Read/Write permissions to the api_key_credentials table is required to enable two-way integration. This can be covered with the roles credentials_admin and discovery_admin. Set up a destination To create a ServiceNow destination, enter the following information: Destination Name: custom name to identify the destination Domain: the URL of the destination User-name: the name of the user Password: the user name’s password Before saving the destination, we recommend testing the connection by clicking the test connection button. Two-way sync You can configure a two-way integration with ServiceNow Incidents Management to keep the incidents' state synced with the corresponding state in New Relic. Here are some required steps to remember when configuring the two-way integration: Turn on the two-way integration toggle. Open and download this XML file, which includes the business rule triggering events back to New Relic One. In the ServiceNow sidebar menu, go to System Definition > Business Rules. Click the menu icon in one of the column headers, select Import XML and upload the XML file you downloaded. Once the Destination is saved, a New-Relic API-Key will be kept in the api_key_credentials. The key would sent in a header as part of the callback REST call to New-Relic Configure the message template Upon a successful connection, ServiceNow incident table columns are fetched from your account and automatically mapped to your ServiceNow instance. To help you get started, we automatically present the required and recommended fields and values. Required fields must be set with a value. You can add or remove optional fields(use the X mark on their right side) Select, edit or remove fields for the ServiceNow-Incident template. Send a test notification You can see how the ServiceNow incident will appear by clicking a test notification with default field values. If successful, an incident will be created and a link will appear. Slack Send notifications-messages to your Slack channels. Prerequisites Your Slack workspace needs to have the New Relic application installed. The application must be approved by a workspace admin before it can be individually installed by users Set up a Slack destination Click on the `one-click Slack authentication' will lead you to the Slack landing page to continue the OAuth2 authentication process. On the Slack landing page, if you're not signed into the required workspace, you're redirected to Slack to sign in. Add your workspace name or select the relevant workspace and click Continue. When signed in to the selected workspace, you are requested to allow New Relic to perform the specified actions. Clicking `Allow' will redirect you back to the Destination page. Configure the Slack message settings Select a Destination(Workspace) and select a Slack-channel where the messages will be sent. You can create a new destination if there is no pre-defined destination for the required workspace. Note that, for privacy reasons, users need to be authenticated to select private channels (one-time process) Send a test notification You can send a test notification with a pre-defined example payload to the channel. This creates a message in the selected Slack-channel. Webhook Use the webhook notifier to send the notification messages to any endpoint you like. Set up a webhook destination To create a webhook destination, you need the following: Destination Name: A unique destination name URL: the endpoint of the target application, authentication and custom headers if needed. Authorization mechanism (Optional):. Can be basic authentication or a bearer token Configure the webhook event template Pick a webhook destination from the list and configure the HTTP-POST request. The request configuration requires you to: Set a name for the template. Select a pre-configured destination from the destinations list or create a new one. Add custom headers (optional). Configure the request’s payload. Customize the webhook payload You can use the default payload or customize it to contain the required data. Pick Variables from the variables menu and apply handlebars syntax to enrich your webhook. Note that the request’s content-type is JSON by default. Hence, the payload needs to keep the JSON form. See Usage examples The ‘preview’ section on the right hand-side shows an expected payload after the template is rendered. If the eventual payload would not form a valid Json, an error will be shown and it won’t be possible to save the template. If the webhook payload conforms a valid Json, you can send a test notification to your defined webhook destination We recommend sending a test notification to make sure that everything's connected correctly. Email Send email notifications to users. Configure the email settings Add one or more recipients. Users with New Relic accounts can be found via autocomplete when searching for their name or email address. To add a user without a New Relic account or email distribution list, add the full email address. Every recipient will be translated into a 'destination'. You can follow the email notifications per destination in the notifications-log Send a test notification You can send a test notification to make sure the email notifications reach the inbox.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 297.8594,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Alerts</em> <em>and</em> <em>applied</em> <em>intelligence</em> notification integrations",
        "sections": "<em>Alerts</em> <em>and</em> <em>applied</em> <em>intelligence</em> notification integrations",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": " notifications. <em>Alerts</em> and <em>Applied</em> <em>Intelligence</em> notification integrations are specific services and platforms you can use to send notifications from New Relic. Integration details Read more about each of our specific notification integrations. Atlassian Jira Integrate New Relic with Atlassian Jira(Cloud"
      },
      "id": "618ff71628ccbc60710321e4"
    },
    {
      "sections": [
        "Get started with Incident Intelligence",
        "Requirements",
        "Set up Incident Intelligence",
        "1. Configure your environment (one-time)",
        "Tip",
        "2. Configure sources",
        "Alerts",
        "Datarobot (formerly Algorithmia)",
        "Aporia (MLOps)",
        "Superwise (MLOps)",
        "Anomalies",
        "AWS",
        "Grafana",
        "PagerDuty",
        "EOL NOTICE",
        "Prometheus Alertmanager",
        "REST API",
        "Splunk",
        "Important",
        "Splunk metadata",
        "3. Configure destinations (ServiceNow and others)",
        "Configure ServiceNow (example)",
        "Send data to ServiceNow",
        "Custom notification message",
        "Other destinations",
        "Send data to PagerDuty",
        "Send data via webhook",
        "4. Configure pathways",
        "What's next?"
      ],
      "title": "Get started with Incident Intelligence",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Incident intelligence"
      ],
      "external_id": "8c4a5a914ca01cb42250908d2fb1a12ccc697e25",
      "image": "https://docs.newrelic.com/static/5cb28999dc618c6a5a2b8be1fa72e660/b97f6/image-%25281%2529_0.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/applied-intelligence/incident-intelligence/get-started-incident-intelligence/",
      "published_at": "2021-12-19T14:23:58Z",
      "updated_at": "2021-12-09T22:19:28Z",
      "document_type": "page",
      "popularity": 1,
      "body": "As part of Applied Intelligence, Incident Intelligence helps you correlate your incidents and reduce noise in your environment. It gives you an overview of all your incidents, their sources, and related events. Requirements If you haven't already, sign up for a New Relic account to get started. To use most Applied Intelligence features, you must be a full platform user. For more information, see user type capabilities. Set up Incident Intelligence To enable Incident Intelligence, follow these steps. Afterwards, issues should start to appear in your issue feed. 1. Configure your environment (one-time). 2. Configure sources. 3. Configure destinations. 4. Configure pathways. 1. Configure your environment (one-time) To set up an environment in Incident Intelligence, you need an administrator to select a New Relic account for it. This account should be the one your team is using. Who sets the environment? Only administrators, and only for accounts where they have admin privileges. Can administrators set more than one environment? They can set one environment per parent account and its child accounts. More than one can be set if an administrator has privileges for more than one parent account. Tip Incident Intelligence is a cross-account product. This means you can send in data from any New Relic account or external source to correlate events. 2. Configure sources After setting up your environment, determine your incident sources. These are your data inputs. You can get data from any of the following sources: Alerts By integrating Incident Intelligence with your alerts violations, you can get context and correlations from what you're monitoring. To get data from alerts: From one.newrelic.com, click Alerts. On the left under Incident Intelligence, click Sources and then click Alerts. Select the policies you want to connect to Applied Intelligence, and click Connect. You can add additional alerts policies or remove policies you've already connected in Sources > New Relic Alerts. Tip Adding alerts as a source will not affect your current configuration or notifications. Datarobot (formerly Algorithmia) By integrating Incident Intelligence with your Datarobot machine-learning models, you can monitor your machine learning model performance. To configure Datarobot for Incident Intelligence, see our integration docs. Aporia (MLOps) By integrating Incident Intelligence with your Aporia machine-learning models, you can monitor your machine learning model performance. To configure our Aporia integration, see our docs. Superwise (MLOps) By integrating Incident Intelligence with your Superwise machine-learning models, you can monitor your machine learning model performance. To configure our Superwise integration, see our docs. Anomalies By integrating Incident Intelligence with your New Relic Proactive Detection anomalies, you can get context and correlations from what you're monitoring. To get data from New Relic Proactive Detection anomalies: From one.newrelic.com, click Alerts & AI. On the left under Incident Intelligence, click Sources and then click Anomalies. Select the configurations you want to connect to Applied Intelligence by toggling to on (green). To add or remove alert policies, from Alerts & AI, click Sources, then Alerts. Tip Adding anomalies as a source won't affect your current Proactive Detection configurations or notifications. AWS You can integrate Incident Intelligence with Amazon CloudWatch to provide incident management for all of your AWS services. To integrate Amazon CloudWatch: Go to one.newrelic.com and click Alerts & AI. On the left under Incident Intelligence, click Sources and then click Amazon Web Services. Copy the URL. Create a new Amazon SNS topic. Set CloudWatch to forward all Alarms state changes to that topic: In the Amazon CloudWatch UI, click Events > Event Pattern. Select Service Name > CloudWatch. Select Event Type > CloudWatch Alarm State Change. Select Targets > SNS Topic, and select your new Amazon SNS topic. Create a new subscription: In the Amazon AWS UI, click Create a Subscription. Select your new Amazon SNS topic. Select Protocol > choose HTTPS. In Endpoint, paste the URL you previously copied from the Applied Intelligence Sources. Grafana You can integrate Incident Intelligence with Grafana's notifications for insight into events across your applications and environment. Grafana's webhook notification is a simple way to send information over HTTP to a custom endpoint. To integrate Grafana as a new webhook: Log into your Grafana portal using Admin permissions, and choose Alerting. On the Grafana Notification Channels page, click New Channel > Webhook. Go to one.newrelic.com and click Alerts & AI. On the left under Incident Intelligence, click Sources, and then click Grafana. Copy the URL, and paste it into your new Grafana webhook. PagerDuty EOL NOTICE As of October 2021, we've discontinued support for several capabilities with PagerDuty, including suggested responders, golden signals, and component enrichment. For more details, including how you can easily make this transition, see our Explorers Hub post. You can integrate Incident Intelligence directly with your PagerDuty services to ingest, process, and enhance all of your PagerDuty incidents. Connecting PagerDuty services to Applied Intelligence will not affect your current services or notifications. To get data from PagerDuty: Make sure your PagerDuty API key has write access. From one.newrelic.com, click Alerts & AI. On the left under Incident Intelligence, click Sources and then click PagerDuty. Enter your PagerDuty API key. The key should be either a personal or general access key with write access. If it's created by a user, the user should be an admin. Select the PagerDuty services you want to connect to Applied Intelligence, and click Connect. You can add additional services or remove services you've already connected in Sources > PagerDuty. Prometheus Alertmanager By integrating Incident Intelligence with Prometheus Alertmanager, you can receive and correlate your Prometheus alerts with events from other sources. To integrate Prometheus Alertmanager: Set up your Alertmanager configuration file by running: ./alertmanager -config.file=simple.yml Copy Go to one.newrelic.com and click Alerts & AI. On the left under Incident Intelligence, click Sources and then click Prometheus Alertmanager. Copy the Prometheus Alertmanager URL, and paste it into the <webhook_config>/url section of your Alertmanager config file. Reload the Prometheus Alertmanager configuration with one of the two methods: Send a SIGHUP to the process. Send an HTTP POST request to the /-/reload endpoint. REST API Incident Intelligence supports a dedicated REST API interface that lets you integrate with additional systems. The interface allows instrumentation of your code or other monitoring solutions to report any kind of metric or event. A metric can be a raw data point such as CPU, memory, disk utilization, or business KPI. An event can be a monitoring alert, deployment event, incident, exceptions or any other change in state that you want to describe. You can also send any type of data to Incident Intelligence straight from your own systems or applications. The REST API supports secure token-based authentication and accepts JSON content as input. For more information on authentication and the full API reference, see REST API for New Relic Applied Intelligence. Splunk By integrating Incident Intelligence with your Splunk log monitoring, you can: Use your environment's log data for searches and key term reports. Correlate alerts and search reports with your other metrics and incidents. Important Applied Intelligence supports Splunk Light, Splunk Cloud, and Splunk Enterprise version 6.3 and higher. To get data from Splunk: In your Splunk console, start a search for the relevant events. Save your search as an alert, configure your alert conditions, and then choose the webhook as the delivery method. Go to one.newrelic.com and click Alerts & AI. On the left under Incident Intelligence, click Sources and then click Splunk. Copy the collector URL, and paste it into the webhook endpoint in the Splunk console. Optional: Use Splunk tokens to enrich alert data with Splunk metadata. Splunk metadata To enrich alerts data with your Splunk metadata, use Splunk tokens. This helps you leverage your search data, which includes metadata and values from the first row of search results. If you want to... Do this... Access search data Use the format $<fieldname>$. For example, use $app$ for the app context for the search. Access field values To access field values from the first result row that a search returns, use the format $result.<fieldname>$. For example, use $result.host$ for the host value and $result.sourcetype$ for the source type. Use variables You can leverage any of the Selected fields in the Splunk search and add any unique fields to the Selected fields to make the data available as a variable. The following fields will automatically provide hints to the correlation engine: app: parsed as APPLICATION_NAME application:parsed as APPLICATION_NAME application_name: parsed as APPLICATION_NAME cluster: parsed as CLUSTER_NAME computer: parsed as HOST_NAME Dc: parsed as DATACENTER_NAME datacenter: parsed as DATACENTER_NAME host: parsed as HOST_NAME host_name: parsed as HOST_NAME hostname: parsed as HOST_NAME transaction: parsed as EVENT_ID Transaction_id: parsed as EVENT_ID user: parsed as USER_NAME 3. Configure destinations (ServiceNow and others) Now that you've set up your sources, you can configure your destinations. These are the data outputs where you view your incidents. Configure ServiceNow (example) Using ServiceNow as a destination enables you to push valuable violation data into new ServiceNow incident tickets. Send data to ServiceNow To configure Incident Intelligence to send data to ServiceNow: Go to one.newrelic.com, click Alerts & AI, in the left nav under Incident Intelligence click Destinations, then click ServiceNow. Required: Enter a channel name. This is used internally in Applied Intelligence to identify the destination (for example, in Pathways). Required: Enter your ServiceNow credentials: Team domain (This must be unique. No two destinations can have the same domain). Username Password Follow the two-way integration on screen instructions: Open and download this XML file. In the ServiceNow sidebar menu, go to System Definition > Business Rule. Click the menu icon in one of the column headers, select Import XML, and upload the XML file you downloaded. The two way integration will allow the ServiceNow incident to be updated with changes to the Applied Intelligence issue. Closing a ServiceNow incident will close its corresponding New Relic issue. When a New Relic issue is resolved, the corresponding ServiceNow incident will be closed. Custom notification message Applied Intelligence uses a templating framework called Jinja2 in the customization section interface. The Value field must be in valid Jinja syntax. By default, the interface populates a set of default fields in ServiceNow. When you add a custom field, enter the ServiceNow field name you want to use. When you want to skip a selected field in an issue update, add the | skip_on_update string at the end of the value you've selected. Tip By default, ServiceNow adds u_ to the beginning of its custom values. When mapping to ServiceNow attributes, use the Column name value. Please note that the name needs to be lowercase separated by underscores. Go here to see the custom notification message attribute descriptions. Go here to see Jinja2 Useful Syntax. Other destinations You can set other destinations: Send data to PagerDuty EOL NOTICE As of October 2021, we've discontinued support for several capabilities with PagerDuty, including suggested responders, golden signals, and component enrichment. For more details, including how you can easily make this transition, see our Explorers Hub post. Recommended: Create a new PagerDuty service to use as a destination. Because PagerDuty services can also be used as sources, this can help you distinguish your data input from your output. To create a PagerDuty destination: Go to one.newrelic.com, click Alerts & AI, in the left nav under Incident Intelligence click Destinations, then click PagerDuty. Enter your PagerDuty API key. The key should be either a personal or general access key with write access. If it's created by a user, the user should be an admin. If you've configured a PagerDuty source with an API key, you can use the same key. Select the PagerDuty services you want to connect to Applied Intelligence, and click Connect. When you're ready, you can add policies for one or more PagerDuty destinations. You can also transfer the policies over from your existing services or leave them as sources if needed. From the Destinations > PagerDuty page, you can also: Review the permissions for your services. Click Authorize when you're done. Add or delete existing services from the PagerDuty destination. Edit permissions for any service. To configure your PagerDuty destinations, use the following settings: Configuration setting Description Trigger new incidents Required. Trigger correlated parent incidents so you can identify issues faster. Edit incident titles Required. Alter your incident titles to help you orient and understand issues. Add new integrations Required. Add integrations to enable incident creation for selected services. Add webhook extensions Add webhook extensions to sync user actions in PagerDuty to New Relic. This lets you update the correlated issue state. Auto-resolve correlated incidents When enabled, this will resolve and automatically close correlated parent/child incidents. Select a user to take actions in PagerDuty You need to select a user before you can enable deep integration with PagerDuty. Once you do, the user can: Add notes to incidents (required): Incident notes are used to enrich incidents with context. Acknowledge triggered incidents: When enabled, Applied Intelligence will acknowledge and correlate newly triggered incidents in PagerDuty before you're notified. Use the original escalation policy: When enabled, the escalation policy of the source service will be applied to each incident. Send data via webhook Incident Intelligence will send the event body in JSON format by HTTPS POST. The system expects the endpoint to return a successful HTTP code (2xx). To configure Incident Intelligence to send data via webhook: Go to one.newrelic.com, click Alerts & AI, in the left nav under Incident Intelligence click Destinations, then click Webhook. Required: Configure the unique webhook key, used in Applied Intelligence to refer to this webhook configuration and its specific settings. Required: Configure the destination endpoint where the webhook payload will be sent. Optional steps: Configure custom headers, which are key:value pairs of headers to be sent with the request. Example: \"Authentication\" \"Bearer\" <bearer token> Configure a custom payload template that can be used to map New Relic fields to match the destination tool's expected name and format. Configure priority mapping (critical, high, medium, or low), used to map New Relic's priorities to the priorities expected at the destination. Tip There’s a retry mechanism that is triggered a few times with exponential backoff for a couple of minutes once an error occurs. If we reach the retry limit, the Webhook will get auto-disabled. For examples of destination templates, webhook formats, and JSON schema, see the Incident Intelligence destination examples. 4. Configure pathways To control when and where you want to receive notifications from your incidents, you can configure pathways. To add a pathway: Go to one.newrelic.com, click Alerts & AI, in the left nav under Incident Intelligence click Pathways, then click Add a pathway. In the query builder box, select an attribute, such as application/name. This can be from the list of all attributes available in PagerDuty incidents and New Relic alerts violations, or you can add your own attributes. Select a logical operator. For example, contains. Enter a specific value to complete the logical expression. To include all issues created by your sources, select Send everything. (Use this if you only use one PagerDuty service to manage all incidents.) To build more complex logic, use the AND/OR operators. Select one or more of your destinations. To edit or remove existing pathways, mouse over the pathway's name on the Pathways page. What's next? Now that you've set up some sources and destinations for your incidents, read about how to use Incident Intelligence.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 222.04947,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Get started with Incident <em>Intelligence</em>",
        "sections": "Get started with Incident <em>Intelligence</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": " one.newrelic.com, click <em>Alerts</em> &amp; AI. On the left under Incident <em>Intelligence</em>, click Sources and then click Anomalies. Select the configurations you want to connect to <em>Applied</em> <em>Intelligence</em> by toggling to on (green). To add or remove <em>alert</em> policies, from <em>Alerts</em> &amp; AI, click Sources, then <em>Alerts</em>. Tip Adding anomalies"
      },
      "id": "603ea62e64441f119f4e883f"
    }
  ],
  "/docs/alerts-applied-intelligence/applied-intelligence/incident-workflows/incident-workflows": [
    {
      "sections": [
        "Alerts and applied intelligence notification integrations",
        "Early access",
        "Integration details",
        "Atlassian Jira",
        "Permissions",
        "Set up a Jira destination",
        "Important",
        "Two-way sync",
        "Configure the message template",
        "Send a test notification",
        "ServiceNow (Incident-Management)",
        "Roles",
        "Set up a destination",
        "Slack",
        "Prerequisites",
        "Set up a Slack destination",
        "Configure the Slack message settings",
        "Webhook",
        "Set up a webhook destination",
        "Configure the webhook event template",
        "Customize the webhook payload",
        "Email",
        "Configure the email settings"
      ],
      "title": "Alerts and applied intelligence notification integrations",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Incident Intelligence",
        "Destinations"
      ],
      "external_id": "7220c630fc187bb61784ff2cc2213e588b269b00",
      "image": "https://docs.newrelic.com/static/d4e9baecc3a76dd1a5945f8ec0aeca66/c1b63/webhook-notification-template.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/notifications/notification-integrations/",
      "published_at": "2021-12-19T15:28:40Z",
      "updated_at": "2021-12-19T15:28:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Early access The features described here are early access. You won't be able to use these features if you're not part of the early access program. For more information on related features, see our docs on Alerts notification channels, Incident Intelligence destinations, and Proactive Detection notifications. Alerts and Applied Intelligence notification integrations are specific services and platforms you can use to send notifications from New Relic. Integration details Read more about each of our specific notification integrations. Atlassian Jira Integrate New Relic with Atlassian Jira(Cloud) and automatically create and update Jira issues. Permissions The required permissions from the Jira API-Token are create, edit, and close tickets. To enable the two-way sync toggle, the provided Jira API-Key should have an Admin role. Set up a Jira destination Create Jira issues, then enable Jira and New Relic to share updates and stay synced. To create a Jira destination, enter the following information: Destination name: Custom name to identify the destination Jira account endpoint: the URL of the destination User-name: this will be the email address of the user making the connection API token: generated from your Atlassian account Important New Relic currently supports Atlassian-Jira Classic (company-managed) projects. Before saving the destination, we recommend you test the connection via the test connection button. Jira destination configuration. We recommand to test the connection before saving. Two-way sync You can enable a two-way integration with Jira to keep the issues' state synced with the corresponding state in New Relic. To enable two-way sync, turn on the ‘two-way integration’ toggle. When turned on, a Jira Webhook would be created in your Jira account at a later stage, for the selected project (see ‘customize a message template’). The webhook would contain access details to Newrelic (URL and Newrelic-API-KEY) Configure the message template To configure a template for a Jira issue, you first need to choose a destination. You will be able to create a new destination at this stage. Upon successful connection to the destination, you will need to choose a project, and then select the Jira issue type you would like to be used. Once the issue-type is selected, the configured project's fields are fetched from your account and automatically mapped to your Jira instance. To help you get started, we automatically present the required and recommended fields and values. Required fields must be set with a value. You can add or remove optional fields(use the X mark on their right side) Jira message template. Send a test notification You can see how the JIRA issue will appear by clicking a test notification with default field values. If successful, a JIRA issue will be created and a link will appear. ServiceNow (Incident-Management) Integrate New Relic with ServiceNow Incident-Management and automatically create and update incidents. Roles As part of the integrations, we fetch fields from the your serviceNow incident table and optional values. For this, the provided ServiceNow user details required read permissions for the tables: sys_dictionary, sys_choice, sys_user and task. A read/write permission to incident To be able to fetch users for the caller column, we required read permissions for the sys_users table. The above permissions can be achieved with the roles personalize_choices, personalize_dictionary, rest_service, itil. Read/Write permissions to the api_key_credentials table is required to enable two-way integration. This can be covered with the roles credentials_admin and discovery_admin. Set up a destination To create a ServiceNow destination, enter the following information: Destination Name: custom name to identify the destination Domain: the URL of the destination User-name: the name of the user Password: the user name’s password Before saving the destination, we recommend testing the connection by clicking the test connection button. Two-way sync You can configure a two-way integration with ServiceNow Incidents Management to keep the incidents' state synced with the corresponding state in New Relic. Here are some required steps to remember when configuring the two-way integration: Turn on the two-way integration toggle. Open and download this XML file, which includes the business rule triggering events back to New Relic One. In the ServiceNow sidebar menu, go to System Definition > Business Rules. Click the menu icon in one of the column headers, select Import XML and upload the XML file you downloaded. Once the Destination is saved, a New-Relic API-Key will be kept in the api_key_credentials. The key would sent in a header as part of the callback REST call to New-Relic Configure the message template Upon a successful connection, ServiceNow incident table columns are fetched from your account and automatically mapped to your ServiceNow instance. To help you get started, we automatically present the required and recommended fields and values. Required fields must be set with a value. You can add or remove optional fields(use the X mark on their right side) Select, edit or remove fields for the ServiceNow-Incident template. Send a test notification You can see how the ServiceNow incident will appear by clicking a test notification with default field values. If successful, an incident will be created and a link will appear. Slack Send notifications-messages to your Slack channels. Prerequisites Your Slack workspace needs to have the New Relic application installed. The application must be approved by a workspace admin before it can be individually installed by users Set up a Slack destination Click on the `one-click Slack authentication' will lead you to the Slack landing page to continue the OAuth2 authentication process. On the Slack landing page, if you're not signed into the required workspace, you're redirected to Slack to sign in. Add your workspace name or select the relevant workspace and click Continue. When signed in to the selected workspace, you are requested to allow New Relic to perform the specified actions. Clicking `Allow' will redirect you back to the Destination page. Configure the Slack message settings Select a Destination(Workspace) and select a Slack-channel where the messages will be sent. You can create a new destination if there is no pre-defined destination for the required workspace. Note that, for privacy reasons, users need to be authenticated to select private channels (one-time process) Send a test notification You can send a test notification with a pre-defined example payload to the channel. This creates a message in the selected Slack-channel. Webhook Use the webhook notifier to send the notification messages to any endpoint you like. Set up a webhook destination To create a webhook destination, you need the following: Destination Name: A unique destination name URL: the endpoint of the target application, authentication and custom headers if needed. Authorization mechanism (Optional):. Can be basic authentication or a bearer token Configure the webhook event template Pick a webhook destination from the list and configure the HTTP-POST request. The request configuration requires you to: Set a name for the template. Select a pre-configured destination from the destinations list or create a new one. Add custom headers (optional). Configure the request’s payload. Customize the webhook payload You can use the default payload or customize it to contain the required data. Pick Variables from the variables menu and apply handlebars syntax to enrich your webhook. Note that the request’s content-type is JSON by default. Hence, the payload needs to keep the JSON form. See Usage examples The ‘preview’ section on the right hand-side shows an expected payload after the template is rendered. If the eventual payload would not form a valid Json, an error will be shown and it won’t be possible to save the template. If the webhook payload conforms a valid Json, you can send a test notification to your defined webhook destination We recommend sending a test notification to make sure that everything's connected correctly. Email Send email notifications to users. Configure the email settings Add one or more recipients. Users with New Relic accounts can be found via autocomplete when searching for their name or email address. To add a user without a New Relic account or email distribution list, add the full email address. Every recipient will be translated into a 'destination'. You can follow the email notifications per destination in the notifications-log Send a test notification You can send a test notification to make sure the email notifications reach the inbox.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 335.71442,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Alerts</em> <em>and</em> <em>applied</em> <em>intelligence</em> <em>notification</em> integrations",
        "sections": "<em>Alerts</em> <em>and</em> <em>applied</em> <em>intelligence</em> <em>notification</em> integrations",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": " <em>notifications</em>. <em>Alerts</em> and <em>Applied</em> <em>Intelligence</em> notification integrations are specific services and platforms you can use to send <em>notifications</em> from New Relic. Integration details Read more about each of our specific notification integrations. Atlassian Jira Integrate New Relic with Atlassian Jira(Cloud"
      },
      "id": "618ff71628ccbc60710321e4"
    },
    {
      "sections": [
        "Workflows variables",
        "Variables",
        "Workflow data enrichment examples",
        "Query for when application traffic drops",
        "Non-web transactions time",
        "Throughput",
        "Error rate",
        "Query for transaction failures",
        "Query for Kubernetes consumption overview"
      ],
      "title": "Workflows variables",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "workflows",
        "Enrichments",
        "Issues"
      ],
      "external_id": "48f9db1f21750574985a1563c6b2dad8f4dcb2ce",
      "image": "",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/applied-intelligence/incident-workflows/custom-variables-incident-workflows/",
      "published_at": "2021-12-19T14:25:29Z",
      "updated_at": "2021-12-14T22:48:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "An explanation of the variables used for incident workflows. Variables Use the variables from the issue entity to select the kind of issues you would like to send as well as the message template which appear on the notifications. Here's a comprehensive list of variables: Key (First word will be used for grouping) Display Name (First word will be used for grouping) Description accumulations.conditionName Alert Condition Names New Relic violated condition accumulations.origin Issue Origin New Relic or third party source that created the issue accumulations.policyName Alert Policy Names Incident detection policy name that generated the violation accumulations.source Issue Source The target system reported by the source activatedAt Issue Activated At Timestamp of Issue activation annotations.description Issue Description List of all incident descriptions annotations.title Issue Title List of all incident titles closedAt Issue ClosedAt Timestamp of Issue closure, null if the issue is not closed createdAt Issue CreatedAt Timestamp of Issue creation entitiesData.entities Impacted Entities Data A list of objects describing the impacted entity name, id, type and kind entitiesData.ids Impacted Entities IDs A set of all impacted entity ids entitiesData.kinds Impacted Entities Kinds A set of all impacted entity kinds entitiesData.names Impacted Entities Names A set of all impacted entity names entitiesData.types Impacted Entities Types A set of all impacted entity types incidentIds Incident IDs A list of all the issue's aggregated incidents isCorrelated Issue Is Correlated Is Issue correlated issue.pageUrl Issue Page URL A direct link to the relevant issue Page issueId Issue ID The unique id of the issue labels.accountIds Issue Environment Associated Account ID New Relic AI's enviroinments associated account ID labels.aggregationKeys Labels Alerts Aggregation Key New Relic Incident detection original incident ID labels.originalAccountIds Labels Account IDs Incident detection policy's account ID labels.policyIds Labels Alert Policy IDs Incident detection policy IDs that generated the violation priority Issue Priority Issue's priority level priorityText Issue Priority text Issue priority in lower case state Issue State Issue's current state stateText Issue state text Issue's state in lower case status Issue Status Issue's current status totalIncidents Incident Count The number of incidents that are aggregated or correlated in the issue triggeredAt Issue Triggered At Timestamp of Issue notification triggered triggerEvent Issue Notification Trigger Event The notification trigger event updatedAt Issue Updated At Timestamp of Issue last updated workflowName Workflow Name The name of the workflow that was triggered Workflow data enrichment examples To get information about the entity that violated a condition, you can use custom variables as part of the where statement of the query. For example, to get the state of the EC2 instance use: SELECT latest(ec2State) FROM ComputeSample where provider = 'Ec2Instance' and entityName in {{entitiesData.names}} Copy This query returns a single value (for example, stopped), as the query only uses a single field. The variable entitiesData.names is a list of identifiers for the entities. You can use any other entity properties in the same way. You can use custom variables to enrich your workflow data queries in different ways: Query for when application traffic drops There are times when you want to know when traffic to your application drops. You can use the { { entitiesData.names}} variable in place of your application's name. SELECT count(*) FROM Transaction WHERE appName in {{entitiesData.names}} since 10 minutes ago Copy Non-web transactions time Average time the impacted entities spend processing requests/transactions, broken down by process type SELECT average(apm.service.overview.other) * 1000 FROM Metric WHERE appName IN {{entitiesData.names}} FACET `segmentName` TIMESERIES Copy Throughput Number of requests per minute the impacted enitities process SELECT rate(count(apm.service.transaction.duration), 1 minute) as 'Non-web throughput' FROM Metric WHERE (appName IN {{entitiesData.names}}) AND (transactionType = 'Other') TIMESERIES Copy Error rate Ratio of errors to the total number of requests processes by the impacted entities SELECT count(apm.service.error.count) / count(apm.service.transaction.duration) as 'Non-web errors' FROM Metric WHERE (appName IN {{entitiesData.names}}) AND (transactionType = 'Other') TIMESERIES Copy Query for transaction failures There are times when you want to know when your application transactions have failed. This query shows the latest HTTP status code responses filtered by the { { entitiesData.names}} variable that violated your alert policy threshold. From Transaction select latest(httpResponseCode), average(duration) where appName in {{entitiesData.names}} Copy Query for Kubernetes consumption overview Use a query like this to get the number of entities and their ingest times within a Kubernetes pod. By identifying what entities have large ingest times, you can begin to address that issue and find a potential remedy. SELECT uniqueCount(displayName), sum(nr.ingestTimeMs) from K8sServiceSample where entityName = {{entitiesData.names}} since 1 hour ago. Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 296.77344,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Workflows</em> variables",
        "sections": "<em>Workflows</em> variables",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "An explanation of the variables used for incident <em>workflows</em>. Variables Use the variables from the issue entity to select the kind of issues you would like to send as well as the message template which appear on the <em>notifications</em>. Here&#x27;s a comprehensive list of variables: Key (First word"
      },
      "id": "603e7a6528ccbcad47eba77f"
    },
    {
      "sections": [
        "Introduction to notifications",
        "Early access",
        "Destinations",
        "Message templates"
      ],
      "title": "Introduction to notifications",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Notifications",
        "Workflows",
        "Error Inbox"
      ],
      "external_id": "f623ca3e096307c13e6e9214cc2fa6908707d101",
      "image": "",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/notifications/intro-notifications/",
      "published_at": "2021-12-19T15:28:19Z",
      "updated_at": "2021-11-25T00:02:26Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Early access The features described here are early access. You won't be able to use these features if you're not part of the early access program. For more information on related features, see our docs on Alerts notification channels, Incident Intelligence destinations, and Proactive Detection notifications. Notifications are a consolidation of the different ways to send notification-events to third-party services, such as Slack, Jira, ServiceNow, and email. You can also use webhooks to send your data to any compatible third-party service. Integrate with your systems by configuring destinations and message templates. Destinations Destinations are unique identifiers and connection details for third-party systems. Use destinations to integrate, send notifications and share data between New Relic and your third-party systems. Message templates Configure the eventual notification events and map New Relic One data to your third-party services using message-templates.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.47871,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>notifications</em>",
        "sections": "Introduction to <em>notifications</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "Early access The features described here are early access. You won&#x27;t be able to use these features if you&#x27;re not part of the early access program. For more information on related features, see our docs on <em>Alerts</em> notification channels, Incident <em>Intelligence</em> destinations, and Proactive Detection"
      },
      "id": "6190270f64441f165fe9d12b"
    }
  ],
  "/docs/alerts-applied-intelligence/applied-intelligence/proactive-detection/expanded-anomaly-detection": [
    {
      "sections": [
        "Alerts and applied intelligence notification integrations",
        "Early access",
        "Integration details",
        "Atlassian Jira",
        "Permissions",
        "Set up a Jira destination",
        "Important",
        "Two-way sync",
        "Configure the message template",
        "Send a test notification",
        "ServiceNow (Incident-Management)",
        "Roles",
        "Set up a destination",
        "Slack",
        "Prerequisites",
        "Set up a Slack destination",
        "Configure the Slack message settings",
        "Webhook",
        "Set up a webhook destination",
        "Configure the webhook event template",
        "Customize the webhook payload",
        "Email",
        "Configure the email settings"
      ],
      "title": "Alerts and applied intelligence notification integrations",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Incident Intelligence",
        "Destinations"
      ],
      "external_id": "7220c630fc187bb61784ff2cc2213e588b269b00",
      "image": "https://docs.newrelic.com/static/d4e9baecc3a76dd1a5945f8ec0aeca66/c1b63/webhook-notification-template.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/notifications/notification-integrations/",
      "published_at": "2021-12-19T15:28:40Z",
      "updated_at": "2021-12-19T15:28:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Early access The features described here are early access. You won't be able to use these features if you're not part of the early access program. For more information on related features, see our docs on Alerts notification channels, Incident Intelligence destinations, and Proactive Detection notifications. Alerts and Applied Intelligence notification integrations are specific services and platforms you can use to send notifications from New Relic. Integration details Read more about each of our specific notification integrations. Atlassian Jira Integrate New Relic with Atlassian Jira(Cloud) and automatically create and update Jira issues. Permissions The required permissions from the Jira API-Token are create, edit, and close tickets. To enable the two-way sync toggle, the provided Jira API-Key should have an Admin role. Set up a Jira destination Create Jira issues, then enable Jira and New Relic to share updates and stay synced. To create a Jira destination, enter the following information: Destination name: Custom name to identify the destination Jira account endpoint: the URL of the destination User-name: this will be the email address of the user making the connection API token: generated from your Atlassian account Important New Relic currently supports Atlassian-Jira Classic (company-managed) projects. Before saving the destination, we recommend you test the connection via the test connection button. Jira destination configuration. We recommand to test the connection before saving. Two-way sync You can enable a two-way integration with Jira to keep the issues' state synced with the corresponding state in New Relic. To enable two-way sync, turn on the ‘two-way integration’ toggle. When turned on, a Jira Webhook would be created in your Jira account at a later stage, for the selected project (see ‘customize a message template’). The webhook would contain access details to Newrelic (URL and Newrelic-API-KEY) Configure the message template To configure a template for a Jira issue, you first need to choose a destination. You will be able to create a new destination at this stage. Upon successful connection to the destination, you will need to choose a project, and then select the Jira issue type you would like to be used. Once the issue-type is selected, the configured project's fields are fetched from your account and automatically mapped to your Jira instance. To help you get started, we automatically present the required and recommended fields and values. Required fields must be set with a value. You can add or remove optional fields(use the X mark on their right side) Jira message template. Send a test notification You can see how the JIRA issue will appear by clicking a test notification with default field values. If successful, a JIRA issue will be created and a link will appear. ServiceNow (Incident-Management) Integrate New Relic with ServiceNow Incident-Management and automatically create and update incidents. Roles As part of the integrations, we fetch fields from the your serviceNow incident table and optional values. For this, the provided ServiceNow user details required read permissions for the tables: sys_dictionary, sys_choice, sys_user and task. A read/write permission to incident To be able to fetch users for the caller column, we required read permissions for the sys_users table. The above permissions can be achieved with the roles personalize_choices, personalize_dictionary, rest_service, itil. Read/Write permissions to the api_key_credentials table is required to enable two-way integration. This can be covered with the roles credentials_admin and discovery_admin. Set up a destination To create a ServiceNow destination, enter the following information: Destination Name: custom name to identify the destination Domain: the URL of the destination User-name: the name of the user Password: the user name’s password Before saving the destination, we recommend testing the connection by clicking the test connection button. Two-way sync You can configure a two-way integration with ServiceNow Incidents Management to keep the incidents' state synced with the corresponding state in New Relic. Here are some required steps to remember when configuring the two-way integration: Turn on the two-way integration toggle. Open and download this XML file, which includes the business rule triggering events back to New Relic One. In the ServiceNow sidebar menu, go to System Definition > Business Rules. Click the menu icon in one of the column headers, select Import XML and upload the XML file you downloaded. Once the Destination is saved, a New-Relic API-Key will be kept in the api_key_credentials. The key would sent in a header as part of the callback REST call to New-Relic Configure the message template Upon a successful connection, ServiceNow incident table columns are fetched from your account and automatically mapped to your ServiceNow instance. To help you get started, we automatically present the required and recommended fields and values. Required fields must be set with a value. You can add or remove optional fields(use the X mark on their right side) Select, edit or remove fields for the ServiceNow-Incident template. Send a test notification You can see how the ServiceNow incident will appear by clicking a test notification with default field values. If successful, an incident will be created and a link will appear. Slack Send notifications-messages to your Slack channels. Prerequisites Your Slack workspace needs to have the New Relic application installed. The application must be approved by a workspace admin before it can be individually installed by users Set up a Slack destination Click on the `one-click Slack authentication' will lead you to the Slack landing page to continue the OAuth2 authentication process. On the Slack landing page, if you're not signed into the required workspace, you're redirected to Slack to sign in. Add your workspace name or select the relevant workspace and click Continue. When signed in to the selected workspace, you are requested to allow New Relic to perform the specified actions. Clicking `Allow' will redirect you back to the Destination page. Configure the Slack message settings Select a Destination(Workspace) and select a Slack-channel where the messages will be sent. You can create a new destination if there is no pre-defined destination for the required workspace. Note that, for privacy reasons, users need to be authenticated to select private channels (one-time process) Send a test notification You can send a test notification with a pre-defined example payload to the channel. This creates a message in the selected Slack-channel. Webhook Use the webhook notifier to send the notification messages to any endpoint you like. Set up a webhook destination To create a webhook destination, you need the following: Destination Name: A unique destination name URL: the endpoint of the target application, authentication and custom headers if needed. Authorization mechanism (Optional):. Can be basic authentication or a bearer token Configure the webhook event template Pick a webhook destination from the list and configure the HTTP-POST request. The request configuration requires you to: Set a name for the template. Select a pre-configured destination from the destinations list or create a new one. Add custom headers (optional). Configure the request’s payload. Customize the webhook payload You can use the default payload or customize it to contain the required data. Pick Variables from the variables menu and apply handlebars syntax to enrich your webhook. Note that the request’s content-type is JSON by default. Hence, the payload needs to keep the JSON form. See Usage examples The ‘preview’ section on the right hand-side shows an expected payload after the template is rendered. If the eventual payload would not form a valid Json, an error will be shown and it won’t be possible to save the template. If the webhook payload conforms a valid Json, you can send a test notification to your defined webhook destination We recommend sending a test notification to make sure that everything's connected correctly. Email Send email notifications to users. Configure the email settings Add one or more recipients. Users with New Relic accounts can be found via autocomplete when searching for their name or email address. To add a user without a New Relic account or email distribution list, add the full email address. Every recipient will be translated into a 'destination'. You can follow the email notifications per destination in the notifications-log Send a test notification You can send a test notification to make sure the email notifications reach the inbox.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 343.7876,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Alerts</em> <em>and</em> <em>applied</em> <em>intelligence</em> notification integrations",
        "sections": "<em>Alerts</em> <em>and</em> <em>applied</em> <em>intelligence</em> notification integrations",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "Early access The features described here are early access. You won&#x27;t be able to use these features if you&#x27;re not part of the early access program. For more information on related features, see our docs on <em>Alerts</em> notification channels, Incident <em>Intelligence</em> destinations, and <em>Proactive</em> <em>Detection</em>"
      },
      "id": "618ff71628ccbc60710321e4"
    },
    {
      "sections": [
        "Proactive Detection with Applied Intelligence",
        "Requirements",
        "Why it matters",
        "How it works",
        "Set up notifications for Proactive Detection",
        "Set up for Slack",
        "Tip",
        "Set up for webhooks",
        "Set up without notifications",
        "Mute notifications (Slack only)",
        "Use Proactive Detection Slack messages",
        "View overview of anomalies",
        "Anomaly visibility settings",
        "Query anomaly data",
        "Important",
        "Add anomalies as source in Incident Intelligence",
        "Webhook payload and examples",
        "JSON schema example",
        "Data limits"
      ],
      "title": "Proactive Detection with Applied Intelligence",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Proactive detection"
      ],
      "external_id": "53e01b53fc341ef1e89b96e7927b16de03e72358",
      "image": "",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/applied-intelligence/proactive-detection/proactive-detection-applied-intelligence/",
      "published_at": "2021-12-19T15:10:33Z",
      "updated_at": "2021-12-04T15:29:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With Applied Intelligence's Proactive Detection, anomalies from your APM-monitored applications are automatically surfaced in our activity streams and anomalies feed. Each anomaly can be clicked on to bring up an automatic analysis. You can have notifications for anomalies delivered in Slack, or you can set up a webhook to deliver messages when you need them. These events are available for querying, creating custom dashboards, and alerting. By creating a Proactive Detection configuration (a group of apps you’re interested in), you can then add this configuration as a source, and have anomalies automatically correlated with other data sources via Incident Intelligence. Requirements To use Proactive Detection, ensure you have: Full platform user access. If you haven't already, sign up for a New Relic account to get started. An APM agent installed for at least one applications. To receive notifications in Slack, you'll need to ask your IT administrator to install the New Relic application in your Slack workspace. For more on data limits, see Data limits. Why it matters With Proactive Detection, Applied Intelligence delivers insights about anomalies in your production system, along with an automatic analysis of the anomaly. It’s enabled automatically, at no additional cost. When an anomaly is detected, you can view it in the Applied Intelligence anomalies feed, or we'll send notifications directly to your Slack channel or a webhook. How it works Proactive Detection uses the following methods to detect anomalies in your app data: Proactive Detection monitors metric data reported by an APM agent, building a model of your typical application dynamics, and focuses on key golden signals: throughput, response time, and errors. If one of these golden signals shows anomalous behavior, the system flags it and tracks recovery to normal behavior. The system adapts to changes in your data, and continuously updates models based on new data. Automatically on: By default, Proactive Detection monitors all your APM applications, with no action required by you. When an anomaly is detected, it's automatically surfaced in various activity streams, the Applied Intelligence anomalies feed and is available for querying via NRQL. Receiving notifications: We send notifications when we detect anomalous changes in throughput, error rate, or response time. The notifications are sent to selected Slack channels, or sent via webhook. When the anomaly goes back to normal, a recovery message is sent. If you don't want to receive notifications, you still have access to the data via NRQL query. Anomaly analysis: For each anomaly, we provide a link in Slack to an analyze anomaly page. This page generates automatic insights into the anomaly. The page is also available from the anomalies tab, which lists recent anomalies. This page uses your existing APM and Proactive Detection data to provide explanations as to the cause of the anomaly. Activity stream: Inside various activity streams such as the New Relic One homepage, APM Summary page, Lookout and Explorer, you'll see relevant anomalies from your APM-monitored applications. Clicking on any of the anomaly events in the activity stream brings up the analysis page for that anomaly. Applications will not always generate anomalies, so it can be normal to not receive any detections. Set up notifications for Proactive Detection Proactive Detection is enabled automatically, at no additional cost. To receive notifications or to have a configuration (group of apps) that you can add as a source for Incident Intelligence, you will need to create a Proactive Detection configuration. You can create a configuration in the Proactive Detection UI: From one.newrelic.com, click Alerts & AI. Under Proactive Detection, click Settings. Click Add a configuration. Input the following information into the form: Choose a name for your configuration that helps you easily identify it from others in your account. Select an account. Select up to 1,000 applications. Note that certain applications with low throughput might not be good candidates for Proactive Detection, as they can be more sensitive to smaller amounts of data fluctuation. Optional: select the golden signals you'd like to monitor for anomalies. Optional: connect to Incident Intelligence. Set up for Slack To use Proactive Detection with Slack: Select Slack. Choose which Slack channel receives notifications. You can select any existing public or private channel. This prompts the workflow to add the Applied Intelligence Slack application to your selected channel. To create a new channel, do that directly in Slack first. Tip If you experience an error when assigning Slack channels, make sure that the New Relic AI Slack application has been added to your Slack workspace. Save the configuration. You can modify the applications for each configuration at any time by selecting the configuration in the configuration table. Set up for webhooks To use Proactive Detection with webhooks: Select Webhook. Input the following information into the form: Provide the webhook URL. Provide optional custom headers. Choose to edit the custom payload, or enable using the default payload. Save the configuration. You can modify the applications for each configuration at any time by selecting the configuration in the configuration table. Set up without notifications To use Proactive Detection when you don't need to receive notifications: Select No notifications. Save the configuration. You can modify the applications for each configuration at any time by selecting the configuration in the configuration table. Mute notifications (Slack only) In Slack, detections coming from specific applications can be muted temporarily or permanently. The entire channel can also be muted temporarily. This is useful in the case of an incident or when the channel should otherwise not be interrupted. To mute in Slack, select Mute this app’s warnings or Mute all warnings, then select the duration. We will resume sending notifications for any detections once the muting duration has completed. Muting an application permanently removes it from the configuration. To add it back in, go to one.newrelic.com, in the top nav click Alerts & AI, then click Proactive Detection, and select the configuration to edit. Muting Proactive Detection notifications does not affect alerts. Use Proactive Detection Slack messages Each anomaly message has several key pieces of information you can use to learn more about and start troubleshooting the potential issue: The application name and a link to more information about it in New Relic One. The metric experiencing an anomaly and a link to its details in New Relic One. A graph of the metric over time to provide a visual understanding of the anomaly’s behavior and degree. An Analyze button that navigates to an analysis page in Applied Intelligence that identifies key attributes that are unique to the anomaly, anomalies found upstream or downstream, and any other relevant signals. Once an anomaly has returned to normal, we send a recovery notification with the option to provide feedback. Your feedback provides our development team with input to help us improve detection quality. In the case of feedback provided on throughput anomalies, an evaluation is run each hour based on feedback to fit a more suitable model. If we helped you, you can select Yes or No. View overview of anomalies In addition to notifications for anomalies that give you information via Slack or webhook, you can view more information about the anomalies in your environment via the Anomalies tab on the Alerts & AI Overview page. That tab provides a list of all the recent anomalies from every configuration in the selected account, and you can select an anomaly for a detailed analysis. Anomaly visibility settings Anomalies are displayed in various New Relic activity streams and in the Applied Intelligence anomalies feed. You can customize what is displayed using the anomaly visibility settings (for example, hiding throughput anomalies on an activity stream but keeping them in the anomalies feed). To find these settings: from Alerts & AI, under Proactive Detection, click Settings. Notes on using these settings: These settings are applied at the user level. Changes you make won’t affect others users in your organization. Regardless of these settings, the anomalies are still reported and available for NRQL querying. Details on these UI sections: AI overview and anomalies tab: Use the AI overview and anomalies tab setting to hide anomalies from the AI overview and anomalies tab setting. Please note you also can use filters specific to these views as well. Global activity stream: Use the global activity stream section to customize what anomalies are shown in the various New Relic activity streams, including the New Relic One homepage, APM Summary, and Lookout. Anomaly types: Use the check boxes here to hide specific types of anomalies. For example, uncheck Web throughput and Non-web throughput anomalies to hide these types of anomalies from both the activity streams and the AI overview and anomalies tab. (Note they are still reported and available for querying.) Query anomaly data You can use NRQL to query and chart your Proactive Detection data using the NrAiAnomaly event. For example: FROM NrAiAnomaly SELECT * Important This data has previously been attached to the ProactiveDetection event. That event will be deprecated on April 7, 2021. If you use ProactiveDetection in your custom charts, you should convert those queries to using NrAiAnomaly. Here are important attributes attached to this event: Attribute Description closeTime timestamp The time when the anomaly ended. Example: 1615304100000. configurationType string The type of configuration monitoring the event. If at least one configuration is monitoring the entity, this is set to configuration. Otherwise, it's set to automatic. entity.accountId number The New Relic account ID to which the entity belongs. entity.domain number The domain of the entity (currently only APM but will change with future functionality). entity.guid string The GUID of the entity. This is used to identify and retrieve data about the entity via NerdGraph. Identical to entityGuid. entityGuid string The GUID of the entity. This is used to identify and retrieve data about the entity via NerdGraph. Identical to entity.guid. entity.name string The name of the entity whose data was determined to be anomalous. Identical to entityName. Example: Laura's coffee service. entityName string The name of the entity whose data was determined to be anomalous. Identical to entity.name. entity.type string The type of entity (currently only APPLICATION but will change with future functionality). evaluationType string This is always anomaly. event string Indicates whether it's the beginning (open) or end (close) of the anomalous data. openTime timestamp The time when the anomaly opened. Example: 1615303740000. signalType string The type of data that was analyzed. For example, error_rate or response_time.non_web. timestamp timestamp The time at which the event was written. title string Description of the anomaly. Example: Error rate was much higher than normal. Add anomalies as source in Incident Intelligence By integrating Incident Intelligence with your Proactive Detection anomalies, you can get context and correlations. To learn about doing this in Incident Intelligence, see Configure sources. You can also select Connect to Incident Intelligence from inside of a configuration. Webhook payload and examples Proactive Detection sends the event body in JSON format via HTTPS POST. The system expects the endpoint to return a successful HTTP code (2xx). If you use webhooks to configure Proactive Detection, use these examples of the webhook body format and JSON schema. Attribute Description category enum The category of data that was analyzed. Categories include web throughput, non-web throughput, web transactions, non-web transactions, and error class. data list The time series data leading up to the detection. data[].timestamp number The timestamp of the data point in milliseconds since the Unix epoch. Example: 1584366819000 data[].unit string The unit describing the value of the data point. Data units include count, milliseconds, and error_rate. data[].value number The value of the data point. Example: 1.52 detectionType enum The type of data that was analyzed. Types include latency, throughput, and error_rate. entity object The entity that reported the unusual data. entity.accountId number The ID for the entity's account. entity.domain enum The domain for the entity. Example: APM. entity.domainId string The id used to uniquely identify the entity within the domain. entity.guid string The guid used to uniquely identify the entity across all products. entity.name string The name of the entity. Example: Laura’s coffee service entity.link string A link to view the entity. Example: https://rpm.newrelic.com/accounts/YOUR_ACCOUNT_ID/applications/987654321” Copy severity enum A description of how unusual of a change occurred, including NORMAL, WARNING, or CRITICAL. version string Version used to describe the data being provided. Example: v1 viewChartImageUrl string Image showing a chart of the anomalous data. anomalyzerUrl string URL that can be opened to analyze the anomaly in New Relic One. JSON schema example Applied Intelligence will send the event body in JSON format via HTTPS POST. The system expects the endpoint to return a successful HTTP code (2xx). Template: { \"version\": \"{{version}}\", \"entity\": { \"type\": \"{{entity.type}}\", \"name\": \"{{entity.name}}\", \"link\": \"{{entity.link}}\", \"entityGuid\": \"{{entity.entityGuid}}\", \"domainId\": \"{{entity.domainId}}\", \"domain\": \"{{entity.domain}}\", \"accountId\": {{entity.accountId}} }, \"detectionType\": \"{{detectionType}}\", \"category\": \"{{category}}\", \"data\": [{{#each data}} { \"value\": {{value}}, \"unit\": \"{{unit}}\", \"timestamp\": {{timestamp}} } {{#unless @last}},{{/unless}} {{/each}}], \"viewChartImageUrl\": \"{{viewChartImageUrl}}\", \"anomalyzerUrl\": \"{{anomalyzerUrl}}\" } Copy Sample payload: { \"version\": \"v1\", \"entity\": { \"type\": \"APPLICATION\", \"name\": \"My Application\", \"link\": \"https://rpm.newrelic.com/accounts/ACCOUNT_ID/applications/123\", \"entityGuid\": \"foo\", \"domainId\": \"123\", \"domain\": \"APM\", \"accountId\": YOUR_ACCOUNT_ID }, \"detectionType\": \"metric\", \"category\": \"web throughput\", \"data\": [ { \"value\": \"100\", \"unit\": \"count\", \"timestamp\": 1637260259819 }, { \"value\": \"99\", \"unit\": \"count\", \"timestamp\": 1637260319819 }, { \"value\": \"0\", \"unit\": \"count\", \"timestamp\": 1637260379819 } ], \"viewChartImageUrl\": \"https://www.example.com/image/8353cf2c-945c-48e8-99de-e903f033a881?height=200&width=400&show_timezone=true\", \"anomalyzerUrl\": \"https://www.example.com/anomalyzerUrlExample\" } Copy Data limits In addition to requirements, data limits include: Monitored APM applications: limited to 1,000 per configuration Slack configurations: limited to 200 per account Webhook configurations: limited to 200 per account Configurations without notifications: limited to 200 per account",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.78433,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Proactive</em> <em>Detection</em> with <em>Applied</em> <em>Intelligence</em>",
        "sections": "<em>Proactive</em> <em>Detection</em> with <em>Applied</em> <em>Intelligence</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "With <em>Applied</em> <em>Intelligence</em>&#x27;s <em>Proactive</em> <em>Detection</em>, anomalies from your APM-monitored applications are automatically surfaced in our activity streams and anomalies feed. Each anomaly can be clicked on to bring up an automatic analysis. You can have notifications for anomalies delivered in Slack"
      },
      "id": "603e9d68196a67dc21a83dd2"
    },
    {
      "sections": [
        "Get started with Incident Intelligence",
        "Requirements",
        "Set up Incident Intelligence",
        "1. Configure your environment (one-time)",
        "Tip",
        "2. Configure sources",
        "Alerts",
        "Datarobot (formerly Algorithmia)",
        "Aporia (MLOps)",
        "Superwise (MLOps)",
        "Anomalies",
        "AWS",
        "Grafana",
        "PagerDuty",
        "EOL NOTICE",
        "Prometheus Alertmanager",
        "REST API",
        "Splunk",
        "Important",
        "Splunk metadata",
        "3. Configure destinations (ServiceNow and others)",
        "Configure ServiceNow (example)",
        "Send data to ServiceNow",
        "Custom notification message",
        "Other destinations",
        "Send data to PagerDuty",
        "Send data via webhook",
        "4. Configure pathways",
        "What's next?"
      ],
      "title": "Get started with Incident Intelligence",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Incident intelligence"
      ],
      "external_id": "8c4a5a914ca01cb42250908d2fb1a12ccc697e25",
      "image": "https://docs.newrelic.com/static/5cb28999dc618c6a5a2b8be1fa72e660/b97f6/image-%25281%2529_0.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/applied-intelligence/incident-intelligence/get-started-incident-intelligence/",
      "published_at": "2021-12-19T14:23:58Z",
      "updated_at": "2021-12-09T22:19:28Z",
      "document_type": "page",
      "popularity": 1,
      "body": "As part of Applied Intelligence, Incident Intelligence helps you correlate your incidents and reduce noise in your environment. It gives you an overview of all your incidents, their sources, and related events. Requirements If you haven't already, sign up for a New Relic account to get started. To use most Applied Intelligence features, you must be a full platform user. For more information, see user type capabilities. Set up Incident Intelligence To enable Incident Intelligence, follow these steps. Afterwards, issues should start to appear in your issue feed. 1. Configure your environment (one-time). 2. Configure sources. 3. Configure destinations. 4. Configure pathways. 1. Configure your environment (one-time) To set up an environment in Incident Intelligence, you need an administrator to select a New Relic account for it. This account should be the one your team is using. Who sets the environment? Only administrators, and only for accounts where they have admin privileges. Can administrators set more than one environment? They can set one environment per parent account and its child accounts. More than one can be set if an administrator has privileges for more than one parent account. Tip Incident Intelligence is a cross-account product. This means you can send in data from any New Relic account or external source to correlate events. 2. Configure sources After setting up your environment, determine your incident sources. These are your data inputs. You can get data from any of the following sources: Alerts By integrating Incident Intelligence with your alerts violations, you can get context and correlations from what you're monitoring. To get data from alerts: From one.newrelic.com, click Alerts. On the left under Incident Intelligence, click Sources and then click Alerts. Select the policies you want to connect to Applied Intelligence, and click Connect. You can add additional alerts policies or remove policies you've already connected in Sources > New Relic Alerts. Tip Adding alerts as a source will not affect your current configuration or notifications. Datarobot (formerly Algorithmia) By integrating Incident Intelligence with your Datarobot machine-learning models, you can monitor your machine learning model performance. To configure Datarobot for Incident Intelligence, see our integration docs. Aporia (MLOps) By integrating Incident Intelligence with your Aporia machine-learning models, you can monitor your machine learning model performance. To configure our Aporia integration, see our docs. Superwise (MLOps) By integrating Incident Intelligence with your Superwise machine-learning models, you can monitor your machine learning model performance. To configure our Superwise integration, see our docs. Anomalies By integrating Incident Intelligence with your New Relic Proactive Detection anomalies, you can get context and correlations from what you're monitoring. To get data from New Relic Proactive Detection anomalies: From one.newrelic.com, click Alerts & AI. On the left under Incident Intelligence, click Sources and then click Anomalies. Select the configurations you want to connect to Applied Intelligence by toggling to on (green). To add or remove alert policies, from Alerts & AI, click Sources, then Alerts. Tip Adding anomalies as a source won't affect your current Proactive Detection configurations or notifications. AWS You can integrate Incident Intelligence with Amazon CloudWatch to provide incident management for all of your AWS services. To integrate Amazon CloudWatch: Go to one.newrelic.com and click Alerts & AI. On the left under Incident Intelligence, click Sources and then click Amazon Web Services. Copy the URL. Create a new Amazon SNS topic. Set CloudWatch to forward all Alarms state changes to that topic: In the Amazon CloudWatch UI, click Events > Event Pattern. Select Service Name > CloudWatch. Select Event Type > CloudWatch Alarm State Change. Select Targets > SNS Topic, and select your new Amazon SNS topic. Create a new subscription: In the Amazon AWS UI, click Create a Subscription. Select your new Amazon SNS topic. Select Protocol > choose HTTPS. In Endpoint, paste the URL you previously copied from the Applied Intelligence Sources. Grafana You can integrate Incident Intelligence with Grafana's notifications for insight into events across your applications and environment. Grafana's webhook notification is a simple way to send information over HTTP to a custom endpoint. To integrate Grafana as a new webhook: Log into your Grafana portal using Admin permissions, and choose Alerting. On the Grafana Notification Channels page, click New Channel > Webhook. Go to one.newrelic.com and click Alerts & AI. On the left under Incident Intelligence, click Sources, and then click Grafana. Copy the URL, and paste it into your new Grafana webhook. PagerDuty EOL NOTICE As of October 2021, we've discontinued support for several capabilities with PagerDuty, including suggested responders, golden signals, and component enrichment. For more details, including how you can easily make this transition, see our Explorers Hub post. You can integrate Incident Intelligence directly with your PagerDuty services to ingest, process, and enhance all of your PagerDuty incidents. Connecting PagerDuty services to Applied Intelligence will not affect your current services or notifications. To get data from PagerDuty: Make sure your PagerDuty API key has write access. From one.newrelic.com, click Alerts & AI. On the left under Incident Intelligence, click Sources and then click PagerDuty. Enter your PagerDuty API key. The key should be either a personal or general access key with write access. If it's created by a user, the user should be an admin. Select the PagerDuty services you want to connect to Applied Intelligence, and click Connect. You can add additional services or remove services you've already connected in Sources > PagerDuty. Prometheus Alertmanager By integrating Incident Intelligence with Prometheus Alertmanager, you can receive and correlate your Prometheus alerts with events from other sources. To integrate Prometheus Alertmanager: Set up your Alertmanager configuration file by running: ./alertmanager -config.file=simple.yml Copy Go to one.newrelic.com and click Alerts & AI. On the left under Incident Intelligence, click Sources and then click Prometheus Alertmanager. Copy the Prometheus Alertmanager URL, and paste it into the <webhook_config>/url section of your Alertmanager config file. Reload the Prometheus Alertmanager configuration with one of the two methods: Send a SIGHUP to the process. Send an HTTP POST request to the /-/reload endpoint. REST API Incident Intelligence supports a dedicated REST API interface that lets you integrate with additional systems. The interface allows instrumentation of your code or other monitoring solutions to report any kind of metric or event. A metric can be a raw data point such as CPU, memory, disk utilization, or business KPI. An event can be a monitoring alert, deployment event, incident, exceptions or any other change in state that you want to describe. You can also send any type of data to Incident Intelligence straight from your own systems or applications. The REST API supports secure token-based authentication and accepts JSON content as input. For more information on authentication and the full API reference, see REST API for New Relic Applied Intelligence. Splunk By integrating Incident Intelligence with your Splunk log monitoring, you can: Use your environment's log data for searches and key term reports. Correlate alerts and search reports with your other metrics and incidents. Important Applied Intelligence supports Splunk Light, Splunk Cloud, and Splunk Enterprise version 6.3 and higher. To get data from Splunk: In your Splunk console, start a search for the relevant events. Save your search as an alert, configure your alert conditions, and then choose the webhook as the delivery method. Go to one.newrelic.com and click Alerts & AI. On the left under Incident Intelligence, click Sources and then click Splunk. Copy the collector URL, and paste it into the webhook endpoint in the Splunk console. Optional: Use Splunk tokens to enrich alert data with Splunk metadata. Splunk metadata To enrich alerts data with your Splunk metadata, use Splunk tokens. This helps you leverage your search data, which includes metadata and values from the first row of search results. If you want to... Do this... Access search data Use the format $<fieldname>$. For example, use $app$ for the app context for the search. Access field values To access field values from the first result row that a search returns, use the format $result.<fieldname>$. For example, use $result.host$ for the host value and $result.sourcetype$ for the source type. Use variables You can leverage any of the Selected fields in the Splunk search and add any unique fields to the Selected fields to make the data available as a variable. The following fields will automatically provide hints to the correlation engine: app: parsed as APPLICATION_NAME application:parsed as APPLICATION_NAME application_name: parsed as APPLICATION_NAME cluster: parsed as CLUSTER_NAME computer: parsed as HOST_NAME Dc: parsed as DATACENTER_NAME datacenter: parsed as DATACENTER_NAME host: parsed as HOST_NAME host_name: parsed as HOST_NAME hostname: parsed as HOST_NAME transaction: parsed as EVENT_ID Transaction_id: parsed as EVENT_ID user: parsed as USER_NAME 3. Configure destinations (ServiceNow and others) Now that you've set up your sources, you can configure your destinations. These are the data outputs where you view your incidents. Configure ServiceNow (example) Using ServiceNow as a destination enables you to push valuable violation data into new ServiceNow incident tickets. Send data to ServiceNow To configure Incident Intelligence to send data to ServiceNow: Go to one.newrelic.com, click Alerts & AI, in the left nav under Incident Intelligence click Destinations, then click ServiceNow. Required: Enter a channel name. This is used internally in Applied Intelligence to identify the destination (for example, in Pathways). Required: Enter your ServiceNow credentials: Team domain (This must be unique. No two destinations can have the same domain). Username Password Follow the two-way integration on screen instructions: Open and download this XML file. In the ServiceNow sidebar menu, go to System Definition > Business Rule. Click the menu icon in one of the column headers, select Import XML, and upload the XML file you downloaded. The two way integration will allow the ServiceNow incident to be updated with changes to the Applied Intelligence issue. Closing a ServiceNow incident will close its corresponding New Relic issue. When a New Relic issue is resolved, the corresponding ServiceNow incident will be closed. Custom notification message Applied Intelligence uses a templating framework called Jinja2 in the customization section interface. The Value field must be in valid Jinja syntax. By default, the interface populates a set of default fields in ServiceNow. When you add a custom field, enter the ServiceNow field name you want to use. When you want to skip a selected field in an issue update, add the | skip_on_update string at the end of the value you've selected. Tip By default, ServiceNow adds u_ to the beginning of its custom values. When mapping to ServiceNow attributes, use the Column name value. Please note that the name needs to be lowercase separated by underscores. Go here to see the custom notification message attribute descriptions. Go here to see Jinja2 Useful Syntax. Other destinations You can set other destinations: Send data to PagerDuty EOL NOTICE As of October 2021, we've discontinued support for several capabilities with PagerDuty, including suggested responders, golden signals, and component enrichment. For more details, including how you can easily make this transition, see our Explorers Hub post. Recommended: Create a new PagerDuty service to use as a destination. Because PagerDuty services can also be used as sources, this can help you distinguish your data input from your output. To create a PagerDuty destination: Go to one.newrelic.com, click Alerts & AI, in the left nav under Incident Intelligence click Destinations, then click PagerDuty. Enter your PagerDuty API key. The key should be either a personal or general access key with write access. If it's created by a user, the user should be an admin. If you've configured a PagerDuty source with an API key, you can use the same key. Select the PagerDuty services you want to connect to Applied Intelligence, and click Connect. When you're ready, you can add policies for one or more PagerDuty destinations. You can also transfer the policies over from your existing services or leave them as sources if needed. From the Destinations > PagerDuty page, you can also: Review the permissions for your services. Click Authorize when you're done. Add or delete existing services from the PagerDuty destination. Edit permissions for any service. To configure your PagerDuty destinations, use the following settings: Configuration setting Description Trigger new incidents Required. Trigger correlated parent incidents so you can identify issues faster. Edit incident titles Required. Alter your incident titles to help you orient and understand issues. Add new integrations Required. Add integrations to enable incident creation for selected services. Add webhook extensions Add webhook extensions to sync user actions in PagerDuty to New Relic. This lets you update the correlated issue state. Auto-resolve correlated incidents When enabled, this will resolve and automatically close correlated parent/child incidents. Select a user to take actions in PagerDuty You need to select a user before you can enable deep integration with PagerDuty. Once you do, the user can: Add notes to incidents (required): Incident notes are used to enrich incidents with context. Acknowledge triggered incidents: When enabled, Applied Intelligence will acknowledge and correlate newly triggered incidents in PagerDuty before you're notified. Use the original escalation policy: When enabled, the escalation policy of the source service will be applied to each incident. Send data via webhook Incident Intelligence will send the event body in JSON format by HTTPS POST. The system expects the endpoint to return a successful HTTP code (2xx). To configure Incident Intelligence to send data via webhook: Go to one.newrelic.com, click Alerts & AI, in the left nav under Incident Intelligence click Destinations, then click Webhook. Required: Configure the unique webhook key, used in Applied Intelligence to refer to this webhook configuration and its specific settings. Required: Configure the destination endpoint where the webhook payload will be sent. Optional steps: Configure custom headers, which are key:value pairs of headers to be sent with the request. Example: \"Authentication\" \"Bearer\" <bearer token> Configure a custom payload template that can be used to map New Relic fields to match the destination tool's expected name and format. Configure priority mapping (critical, high, medium, or low), used to map New Relic's priorities to the priorities expected at the destination. Tip There’s a retry mechanism that is triggered a few times with exponential backoff for a couple of minutes once an error occurs. If we reach the retry limit, the Webhook will get auto-disabled. For examples of destination templates, webhook formats, and JSON schema, see the Incident Intelligence destination examples. 4. Configure pathways To control when and where you want to receive notifications from your incidents, you can configure pathways. To add a pathway: Go to one.newrelic.com, click Alerts & AI, in the left nav under Incident Intelligence click Pathways, then click Add a pathway. In the query builder box, select an attribute, such as application/name. This can be from the list of all attributes available in PagerDuty incidents and New Relic alerts violations, or you can add your own attributes. Select a logical operator. For example, contains. Enter a specific value to complete the logical expression. To include all issues created by your sources, select Send everything. (Use this if you only use one PagerDuty service to manage all incidents.) To build more complex logic, use the AND/OR operators. Select one or more of your destinations. To edit or remove existing pathways, mouse over the pathway's name on the Pathways page. What's next? Now that you've set up some sources and destinations for your incidents, read about how to use Incident Intelligence.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.26758,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Get started with Incident <em>Intelligence</em>",
        "sections": "Get started with Incident <em>Intelligence</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": " one.newrelic.com, click <em>Alerts</em> &amp; AI. On the left under Incident <em>Intelligence</em>, click Sources and then click Anomalies. Select the configurations you want to connect to <em>Applied</em> <em>Intelligence</em> by toggling to on (green). To add or remove <em>alert</em> policies, from <em>Alerts</em> &amp; AI, click Sources, then <em>Alerts</em>. Tip Adding anomalies"
      },
      "id": "603ea62e64441f119f4e883f"
    }
  ],
  "/docs/alerts-applied-intelligence/applied-intelligence/proactive-detection/proactive-detection-applied-intelligence": [
    {
      "sections": [
        "Alerts and applied intelligence notification integrations",
        "Early access",
        "Integration details",
        "Atlassian Jira",
        "Permissions",
        "Set up a Jira destination",
        "Important",
        "Two-way sync",
        "Configure the message template",
        "Send a test notification",
        "ServiceNow (Incident-Management)",
        "Roles",
        "Set up a destination",
        "Slack",
        "Prerequisites",
        "Set up a Slack destination",
        "Configure the Slack message settings",
        "Webhook",
        "Set up a webhook destination",
        "Configure the webhook event template",
        "Customize the webhook payload",
        "Email",
        "Configure the email settings"
      ],
      "title": "Alerts and applied intelligence notification integrations",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Incident Intelligence",
        "Destinations"
      ],
      "external_id": "7220c630fc187bb61784ff2cc2213e588b269b00",
      "image": "https://docs.newrelic.com/static/d4e9baecc3a76dd1a5945f8ec0aeca66/c1b63/webhook-notification-template.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/notifications/notification-integrations/",
      "published_at": "2021-12-19T15:28:40Z",
      "updated_at": "2021-12-19T15:28:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Early access The features described here are early access. You won't be able to use these features if you're not part of the early access program. For more information on related features, see our docs on Alerts notification channels, Incident Intelligence destinations, and Proactive Detection notifications. Alerts and Applied Intelligence notification integrations are specific services and platforms you can use to send notifications from New Relic. Integration details Read more about each of our specific notification integrations. Atlassian Jira Integrate New Relic with Atlassian Jira(Cloud) and automatically create and update Jira issues. Permissions The required permissions from the Jira API-Token are create, edit, and close tickets. To enable the two-way sync toggle, the provided Jira API-Key should have an Admin role. Set up a Jira destination Create Jira issues, then enable Jira and New Relic to share updates and stay synced. To create a Jira destination, enter the following information: Destination name: Custom name to identify the destination Jira account endpoint: the URL of the destination User-name: this will be the email address of the user making the connection API token: generated from your Atlassian account Important New Relic currently supports Atlassian-Jira Classic (company-managed) projects. Before saving the destination, we recommend you test the connection via the test connection button. Jira destination configuration. We recommand to test the connection before saving. Two-way sync You can enable a two-way integration with Jira to keep the issues' state synced with the corresponding state in New Relic. To enable two-way sync, turn on the ‘two-way integration’ toggle. When turned on, a Jira Webhook would be created in your Jira account at a later stage, for the selected project (see ‘customize a message template’). The webhook would contain access details to Newrelic (URL and Newrelic-API-KEY) Configure the message template To configure a template for a Jira issue, you first need to choose a destination. You will be able to create a new destination at this stage. Upon successful connection to the destination, you will need to choose a project, and then select the Jira issue type you would like to be used. Once the issue-type is selected, the configured project's fields are fetched from your account and automatically mapped to your Jira instance. To help you get started, we automatically present the required and recommended fields and values. Required fields must be set with a value. You can add or remove optional fields(use the X mark on their right side) Jira message template. Send a test notification You can see how the JIRA issue will appear by clicking a test notification with default field values. If successful, a JIRA issue will be created and a link will appear. ServiceNow (Incident-Management) Integrate New Relic with ServiceNow Incident-Management and automatically create and update incidents. Roles As part of the integrations, we fetch fields from the your serviceNow incident table and optional values. For this, the provided ServiceNow user details required read permissions for the tables: sys_dictionary, sys_choice, sys_user and task. A read/write permission to incident To be able to fetch users for the caller column, we required read permissions for the sys_users table. The above permissions can be achieved with the roles personalize_choices, personalize_dictionary, rest_service, itil. Read/Write permissions to the api_key_credentials table is required to enable two-way integration. This can be covered with the roles credentials_admin and discovery_admin. Set up a destination To create a ServiceNow destination, enter the following information: Destination Name: custom name to identify the destination Domain: the URL of the destination User-name: the name of the user Password: the user name’s password Before saving the destination, we recommend testing the connection by clicking the test connection button. Two-way sync You can configure a two-way integration with ServiceNow Incidents Management to keep the incidents' state synced with the corresponding state in New Relic. Here are some required steps to remember when configuring the two-way integration: Turn on the two-way integration toggle. Open and download this XML file, which includes the business rule triggering events back to New Relic One. In the ServiceNow sidebar menu, go to System Definition > Business Rules. Click the menu icon in one of the column headers, select Import XML and upload the XML file you downloaded. Once the Destination is saved, a New-Relic API-Key will be kept in the api_key_credentials. The key would sent in a header as part of the callback REST call to New-Relic Configure the message template Upon a successful connection, ServiceNow incident table columns are fetched from your account and automatically mapped to your ServiceNow instance. To help you get started, we automatically present the required and recommended fields and values. Required fields must be set with a value. You can add or remove optional fields(use the X mark on their right side) Select, edit or remove fields for the ServiceNow-Incident template. Send a test notification You can see how the ServiceNow incident will appear by clicking a test notification with default field values. If successful, an incident will be created and a link will appear. Slack Send notifications-messages to your Slack channels. Prerequisites Your Slack workspace needs to have the New Relic application installed. The application must be approved by a workspace admin before it can be individually installed by users Set up a Slack destination Click on the `one-click Slack authentication' will lead you to the Slack landing page to continue the OAuth2 authentication process. On the Slack landing page, if you're not signed into the required workspace, you're redirected to Slack to sign in. Add your workspace name or select the relevant workspace and click Continue. When signed in to the selected workspace, you are requested to allow New Relic to perform the specified actions. Clicking `Allow' will redirect you back to the Destination page. Configure the Slack message settings Select a Destination(Workspace) and select a Slack-channel where the messages will be sent. You can create a new destination if there is no pre-defined destination for the required workspace. Note that, for privacy reasons, users need to be authenticated to select private channels (one-time process) Send a test notification You can send a test notification with a pre-defined example payload to the channel. This creates a message in the selected Slack-channel. Webhook Use the webhook notifier to send the notification messages to any endpoint you like. Set up a webhook destination To create a webhook destination, you need the following: Destination Name: A unique destination name URL: the endpoint of the target application, authentication and custom headers if needed. Authorization mechanism (Optional):. Can be basic authentication or a bearer token Configure the webhook event template Pick a webhook destination from the list and configure the HTTP-POST request. The request configuration requires you to: Set a name for the template. Select a pre-configured destination from the destinations list or create a new one. Add custom headers (optional). Configure the request’s payload. Customize the webhook payload You can use the default payload or customize it to contain the required data. Pick Variables from the variables menu and apply handlebars syntax to enrich your webhook. Note that the request’s content-type is JSON by default. Hence, the payload needs to keep the JSON form. See Usage examples The ‘preview’ section on the right hand-side shows an expected payload after the template is rendered. If the eventual payload would not form a valid Json, an error will be shown and it won’t be possible to save the template. If the webhook payload conforms a valid Json, you can send a test notification to your defined webhook destination We recommend sending a test notification to make sure that everything's connected correctly. Email Send email notifications to users. Configure the email settings Add one or more recipients. Users with New Relic accounts can be found via autocomplete when searching for their name or email address. To add a user without a New Relic account or email distribution list, add the full email address. Every recipient will be translated into a 'destination'. You can follow the email notifications per destination in the notifications-log Send a test notification You can send a test notification to make sure the email notifications reach the inbox.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 343.7873,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Alerts</em> <em>and</em> <em>applied</em> <em>intelligence</em> notification integrations",
        "sections": "<em>Alerts</em> <em>and</em> <em>applied</em> <em>intelligence</em> notification integrations",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "Early access The features described here are early access. You won&#x27;t be able to use these features if you&#x27;re not part of the early access program. For more information on related features, see our docs on <em>Alerts</em> notification channels, Incident <em>Intelligence</em> destinations, and <em>Proactive</em> <em>Detection</em>"
      },
      "id": "618ff71628ccbc60710321e4"
    },
    {
      "sections": [
        "Get started with Incident Intelligence",
        "Requirements",
        "Set up Incident Intelligence",
        "1. Configure your environment (one-time)",
        "Tip",
        "2. Configure sources",
        "Alerts",
        "Datarobot (formerly Algorithmia)",
        "Aporia (MLOps)",
        "Superwise (MLOps)",
        "Anomalies",
        "AWS",
        "Grafana",
        "PagerDuty",
        "EOL NOTICE",
        "Prometheus Alertmanager",
        "REST API",
        "Splunk",
        "Important",
        "Splunk metadata",
        "3. Configure destinations (ServiceNow and others)",
        "Configure ServiceNow (example)",
        "Send data to ServiceNow",
        "Custom notification message",
        "Other destinations",
        "Send data to PagerDuty",
        "Send data via webhook",
        "4. Configure pathways",
        "What's next?"
      ],
      "title": "Get started with Incident Intelligence",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Incident intelligence"
      ],
      "external_id": "8c4a5a914ca01cb42250908d2fb1a12ccc697e25",
      "image": "https://docs.newrelic.com/static/5cb28999dc618c6a5a2b8be1fa72e660/b97f6/image-%25281%2529_0.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/applied-intelligence/incident-intelligence/get-started-incident-intelligence/",
      "published_at": "2021-12-19T14:23:58Z",
      "updated_at": "2021-12-09T22:19:28Z",
      "document_type": "page",
      "popularity": 1,
      "body": "As part of Applied Intelligence, Incident Intelligence helps you correlate your incidents and reduce noise in your environment. It gives you an overview of all your incidents, their sources, and related events. Requirements If you haven't already, sign up for a New Relic account to get started. To use most Applied Intelligence features, you must be a full platform user. For more information, see user type capabilities. Set up Incident Intelligence To enable Incident Intelligence, follow these steps. Afterwards, issues should start to appear in your issue feed. 1. Configure your environment (one-time). 2. Configure sources. 3. Configure destinations. 4. Configure pathways. 1. Configure your environment (one-time) To set up an environment in Incident Intelligence, you need an administrator to select a New Relic account for it. This account should be the one your team is using. Who sets the environment? Only administrators, and only for accounts where they have admin privileges. Can administrators set more than one environment? They can set one environment per parent account and its child accounts. More than one can be set if an administrator has privileges for more than one parent account. Tip Incident Intelligence is a cross-account product. This means you can send in data from any New Relic account or external source to correlate events. 2. Configure sources After setting up your environment, determine your incident sources. These are your data inputs. You can get data from any of the following sources: Alerts By integrating Incident Intelligence with your alerts violations, you can get context and correlations from what you're monitoring. To get data from alerts: From one.newrelic.com, click Alerts. On the left under Incident Intelligence, click Sources and then click Alerts. Select the policies you want to connect to Applied Intelligence, and click Connect. You can add additional alerts policies or remove policies you've already connected in Sources > New Relic Alerts. Tip Adding alerts as a source will not affect your current configuration or notifications. Datarobot (formerly Algorithmia) By integrating Incident Intelligence with your Datarobot machine-learning models, you can monitor your machine learning model performance. To configure Datarobot for Incident Intelligence, see our integration docs. Aporia (MLOps) By integrating Incident Intelligence with your Aporia machine-learning models, you can monitor your machine learning model performance. To configure our Aporia integration, see our docs. Superwise (MLOps) By integrating Incident Intelligence with your Superwise machine-learning models, you can monitor your machine learning model performance. To configure our Superwise integration, see our docs. Anomalies By integrating Incident Intelligence with your New Relic Proactive Detection anomalies, you can get context and correlations from what you're monitoring. To get data from New Relic Proactive Detection anomalies: From one.newrelic.com, click Alerts & AI. On the left under Incident Intelligence, click Sources and then click Anomalies. Select the configurations you want to connect to Applied Intelligence by toggling to on (green). To add or remove alert policies, from Alerts & AI, click Sources, then Alerts. Tip Adding anomalies as a source won't affect your current Proactive Detection configurations or notifications. AWS You can integrate Incident Intelligence with Amazon CloudWatch to provide incident management for all of your AWS services. To integrate Amazon CloudWatch: Go to one.newrelic.com and click Alerts & AI. On the left under Incident Intelligence, click Sources and then click Amazon Web Services. Copy the URL. Create a new Amazon SNS topic. Set CloudWatch to forward all Alarms state changes to that topic: In the Amazon CloudWatch UI, click Events > Event Pattern. Select Service Name > CloudWatch. Select Event Type > CloudWatch Alarm State Change. Select Targets > SNS Topic, and select your new Amazon SNS topic. Create a new subscription: In the Amazon AWS UI, click Create a Subscription. Select your new Amazon SNS topic. Select Protocol > choose HTTPS. In Endpoint, paste the URL you previously copied from the Applied Intelligence Sources. Grafana You can integrate Incident Intelligence with Grafana's notifications for insight into events across your applications and environment. Grafana's webhook notification is a simple way to send information over HTTP to a custom endpoint. To integrate Grafana as a new webhook: Log into your Grafana portal using Admin permissions, and choose Alerting. On the Grafana Notification Channels page, click New Channel > Webhook. Go to one.newrelic.com and click Alerts & AI. On the left under Incident Intelligence, click Sources, and then click Grafana. Copy the URL, and paste it into your new Grafana webhook. PagerDuty EOL NOTICE As of October 2021, we've discontinued support for several capabilities with PagerDuty, including suggested responders, golden signals, and component enrichment. For more details, including how you can easily make this transition, see our Explorers Hub post. You can integrate Incident Intelligence directly with your PagerDuty services to ingest, process, and enhance all of your PagerDuty incidents. Connecting PagerDuty services to Applied Intelligence will not affect your current services or notifications. To get data from PagerDuty: Make sure your PagerDuty API key has write access. From one.newrelic.com, click Alerts & AI. On the left under Incident Intelligence, click Sources and then click PagerDuty. Enter your PagerDuty API key. The key should be either a personal or general access key with write access. If it's created by a user, the user should be an admin. Select the PagerDuty services you want to connect to Applied Intelligence, and click Connect. You can add additional services or remove services you've already connected in Sources > PagerDuty. Prometheus Alertmanager By integrating Incident Intelligence with Prometheus Alertmanager, you can receive and correlate your Prometheus alerts with events from other sources. To integrate Prometheus Alertmanager: Set up your Alertmanager configuration file by running: ./alertmanager -config.file=simple.yml Copy Go to one.newrelic.com and click Alerts & AI. On the left under Incident Intelligence, click Sources and then click Prometheus Alertmanager. Copy the Prometheus Alertmanager URL, and paste it into the <webhook_config>/url section of your Alertmanager config file. Reload the Prometheus Alertmanager configuration with one of the two methods: Send a SIGHUP to the process. Send an HTTP POST request to the /-/reload endpoint. REST API Incident Intelligence supports a dedicated REST API interface that lets you integrate with additional systems. The interface allows instrumentation of your code or other monitoring solutions to report any kind of metric or event. A metric can be a raw data point such as CPU, memory, disk utilization, or business KPI. An event can be a monitoring alert, deployment event, incident, exceptions or any other change in state that you want to describe. You can also send any type of data to Incident Intelligence straight from your own systems or applications. The REST API supports secure token-based authentication and accepts JSON content as input. For more information on authentication and the full API reference, see REST API for New Relic Applied Intelligence. Splunk By integrating Incident Intelligence with your Splunk log monitoring, you can: Use your environment's log data for searches and key term reports. Correlate alerts and search reports with your other metrics and incidents. Important Applied Intelligence supports Splunk Light, Splunk Cloud, and Splunk Enterprise version 6.3 and higher. To get data from Splunk: In your Splunk console, start a search for the relevant events. Save your search as an alert, configure your alert conditions, and then choose the webhook as the delivery method. Go to one.newrelic.com and click Alerts & AI. On the left under Incident Intelligence, click Sources and then click Splunk. Copy the collector URL, and paste it into the webhook endpoint in the Splunk console. Optional: Use Splunk tokens to enrich alert data with Splunk metadata. Splunk metadata To enrich alerts data with your Splunk metadata, use Splunk tokens. This helps you leverage your search data, which includes metadata and values from the first row of search results. If you want to... Do this... Access search data Use the format $<fieldname>$. For example, use $app$ for the app context for the search. Access field values To access field values from the first result row that a search returns, use the format $result.<fieldname>$. For example, use $result.host$ for the host value and $result.sourcetype$ for the source type. Use variables You can leverage any of the Selected fields in the Splunk search and add any unique fields to the Selected fields to make the data available as a variable. The following fields will automatically provide hints to the correlation engine: app: parsed as APPLICATION_NAME application:parsed as APPLICATION_NAME application_name: parsed as APPLICATION_NAME cluster: parsed as CLUSTER_NAME computer: parsed as HOST_NAME Dc: parsed as DATACENTER_NAME datacenter: parsed as DATACENTER_NAME host: parsed as HOST_NAME host_name: parsed as HOST_NAME hostname: parsed as HOST_NAME transaction: parsed as EVENT_ID Transaction_id: parsed as EVENT_ID user: parsed as USER_NAME 3. Configure destinations (ServiceNow and others) Now that you've set up your sources, you can configure your destinations. These are the data outputs where you view your incidents. Configure ServiceNow (example) Using ServiceNow as a destination enables you to push valuable violation data into new ServiceNow incident tickets. Send data to ServiceNow To configure Incident Intelligence to send data to ServiceNow: Go to one.newrelic.com, click Alerts & AI, in the left nav under Incident Intelligence click Destinations, then click ServiceNow. Required: Enter a channel name. This is used internally in Applied Intelligence to identify the destination (for example, in Pathways). Required: Enter your ServiceNow credentials: Team domain (This must be unique. No two destinations can have the same domain). Username Password Follow the two-way integration on screen instructions: Open and download this XML file. In the ServiceNow sidebar menu, go to System Definition > Business Rule. Click the menu icon in one of the column headers, select Import XML, and upload the XML file you downloaded. The two way integration will allow the ServiceNow incident to be updated with changes to the Applied Intelligence issue. Closing a ServiceNow incident will close its corresponding New Relic issue. When a New Relic issue is resolved, the corresponding ServiceNow incident will be closed. Custom notification message Applied Intelligence uses a templating framework called Jinja2 in the customization section interface. The Value field must be in valid Jinja syntax. By default, the interface populates a set of default fields in ServiceNow. When you add a custom field, enter the ServiceNow field name you want to use. When you want to skip a selected field in an issue update, add the | skip_on_update string at the end of the value you've selected. Tip By default, ServiceNow adds u_ to the beginning of its custom values. When mapping to ServiceNow attributes, use the Column name value. Please note that the name needs to be lowercase separated by underscores. Go here to see the custom notification message attribute descriptions. Go here to see Jinja2 Useful Syntax. Other destinations You can set other destinations: Send data to PagerDuty EOL NOTICE As of October 2021, we've discontinued support for several capabilities with PagerDuty, including suggested responders, golden signals, and component enrichment. For more details, including how you can easily make this transition, see our Explorers Hub post. Recommended: Create a new PagerDuty service to use as a destination. Because PagerDuty services can also be used as sources, this can help you distinguish your data input from your output. To create a PagerDuty destination: Go to one.newrelic.com, click Alerts & AI, in the left nav under Incident Intelligence click Destinations, then click PagerDuty. Enter your PagerDuty API key. The key should be either a personal or general access key with write access. If it's created by a user, the user should be an admin. If you've configured a PagerDuty source with an API key, you can use the same key. Select the PagerDuty services you want to connect to Applied Intelligence, and click Connect. When you're ready, you can add policies for one or more PagerDuty destinations. You can also transfer the policies over from your existing services or leave them as sources if needed. From the Destinations > PagerDuty page, you can also: Review the permissions for your services. Click Authorize when you're done. Add or delete existing services from the PagerDuty destination. Edit permissions for any service. To configure your PagerDuty destinations, use the following settings: Configuration setting Description Trigger new incidents Required. Trigger correlated parent incidents so you can identify issues faster. Edit incident titles Required. Alter your incident titles to help you orient and understand issues. Add new integrations Required. Add integrations to enable incident creation for selected services. Add webhook extensions Add webhook extensions to sync user actions in PagerDuty to New Relic. This lets you update the correlated issue state. Auto-resolve correlated incidents When enabled, this will resolve and automatically close correlated parent/child incidents. Select a user to take actions in PagerDuty You need to select a user before you can enable deep integration with PagerDuty. Once you do, the user can: Add notes to incidents (required): Incident notes are used to enrich incidents with context. Acknowledge triggered incidents: When enabled, Applied Intelligence will acknowledge and correlate newly triggered incidents in PagerDuty before you're notified. Use the original escalation policy: When enabled, the escalation policy of the source service will be applied to each incident. Send data via webhook Incident Intelligence will send the event body in JSON format by HTTPS POST. The system expects the endpoint to return a successful HTTP code (2xx). To configure Incident Intelligence to send data via webhook: Go to one.newrelic.com, click Alerts & AI, in the left nav under Incident Intelligence click Destinations, then click Webhook. Required: Configure the unique webhook key, used in Applied Intelligence to refer to this webhook configuration and its specific settings. Required: Configure the destination endpoint where the webhook payload will be sent. Optional steps: Configure custom headers, which are key:value pairs of headers to be sent with the request. Example: \"Authentication\" \"Bearer\" <bearer token> Configure a custom payload template that can be used to map New Relic fields to match the destination tool's expected name and format. Configure priority mapping (critical, high, medium, or low), used to map New Relic's priorities to the priorities expected at the destination. Tip There’s a retry mechanism that is triggered a few times with exponential backoff for a couple of minutes once an error occurs. If we reach the retry limit, the Webhook will get auto-disabled. For examples of destination templates, webhook formats, and JSON schema, see the Incident Intelligence destination examples. 4. Configure pathways To control when and where you want to receive notifications from your incidents, you can configure pathways. To add a pathway: Go to one.newrelic.com, click Alerts & AI, in the left nav under Incident Intelligence click Pathways, then click Add a pathway. In the query builder box, select an attribute, such as application/name. This can be from the list of all attributes available in PagerDuty incidents and New Relic alerts violations, or you can add your own attributes. Select a logical operator. For example, contains. Enter a specific value to complete the logical expression. To include all issues created by your sources, select Send everything. (Use this if you only use one PagerDuty service to manage all incidents.) To build more complex logic, use the AND/OR operators. Select one or more of your destinations. To edit or remove existing pathways, mouse over the pathway's name on the Pathways page. What's next? Now that you've set up some sources and destinations for your incidents, read about how to use Incident Intelligence.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 268.26752,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Get started with Incident <em>Intelligence</em>",
        "sections": "Get started with Incident <em>Intelligence</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": " one.newrelic.com, click <em>Alerts</em> &amp; AI. On the left under Incident <em>Intelligence</em>, click Sources and then click Anomalies. Select the configurations you want to connect to <em>Applied</em> <em>Intelligence</em> by toggling to on (green). To add or remove <em>alert</em> policies, from <em>Alerts</em> &amp; AI, click Sources, then <em>Alerts</em>. Tip Adding anomalies"
      },
      "id": "603ea62e64441f119f4e883f"
    },
    {
      "sections": [
        "Destinations",
        "Early access",
        "Tip",
        "Required capabilities",
        "Manage destinations",
        "Destination status",
        "Notifications log"
      ],
      "title": "Destinations",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Incident Intelligence",
        "Destinations"
      ],
      "external_id": "6a4550f053d167b43178996347fc6a51d2953e59",
      "image": "https://docs.newrelic.com/static/a4a0201ffecf01f56e314de250eeee71/c1b63/destinations-overview.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/notifications/destinations/",
      "published_at": "2021-12-19T15:28:19Z",
      "updated_at": "2021-12-10T06:26:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Early access The features described here are early access. You won't be able to use these features if you're not part of the early access program. For more information on related features, see our docs on Alerts notification channels, Incident Intelligence destinations, and Proactive Detection notifications. Destinations are where we send notifications about your New Relic One data. A destination is a unique identifier for a third-party system that you use. Destination settings contain the connection details to integrate with third-party systems and can be used across a variety of tools in New Relic One. The supported destination platforms include: Atlassian Jira ServiceNow Slack Webhook Email For more on these and other destinations, see notification integrations. Tip It's also possible to configure destinations using the aiNotifications NerdGraph API. Required capabilities Destination settings require specific capabilities: To access your settings: you need View capabilities for Applied Intelligence:Destinations or Alerts. To modify or delete your settings: you need Modify capabilities for Applied Intelligence:Destinations or Alerts. Manage destinations Go to one.newrelic.com, click Alerts & AI, and in the left nav under Enrich and Respond, click Destinations. The destinations table shows information about the existing destinations and allows users to enable, disable, and modify. To add a destination, click the appropriate platform tile. To modify destination settings, click the destination row in the destinations table. one.newrelic.com > Alerts & AI > Destinations. Destination status Destinations have a 'status' value that indicates if we encountered issues while processing and sending events to them (see the destinations table in the above image). Some errors, like Authentication or Authorization issues, require an update to the destination's connection details. After the update, the destination status value will be changed to \"Default\". Notifications log To view past notification events details, go to the Destination menu, and click the Notifications log tab. Notifications log enable you to view the history and status of all your past notifications. Here you can view the status of any notification along with related error details and destination ticket numbers. Filter your destination logs by destination type, sent by, and status.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 267.1492,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "Early access The features described here are early access. You won&#x27;t be able to use these features if you&#x27;re not part of the early access program. For more information on related features, see our docs on <em>Alerts</em> notification channels, Incident <em>Intelligence</em> destinations, and <em>Proactive</em> <em>Detection</em>"
      },
      "id": "618f3a3ee7b9d2bd07388279"
    }
  ],
  "/docs/alerts-applied-intelligence/index": [
    {
      "sections": [
        "Alerts and applied intelligence notification integrations",
        "Early access",
        "Integration details",
        "Atlassian Jira",
        "Permissions",
        "Set up a Jira destination",
        "Important",
        "Two-way sync",
        "Configure the message template",
        "Send a test notification",
        "ServiceNow (Incident-Management)",
        "Roles",
        "Set up a destination",
        "Slack",
        "Prerequisites",
        "Set up a Slack destination",
        "Configure the Slack message settings",
        "Webhook",
        "Set up a webhook destination",
        "Configure the webhook event template",
        "Customize the webhook payload",
        "Email",
        "Configure the email settings"
      ],
      "title": "Alerts and applied intelligence notification integrations",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Incident Intelligence",
        "Destinations"
      ],
      "external_id": "7220c630fc187bb61784ff2cc2213e588b269b00",
      "image": "https://docs.newrelic.com/static/d4e9baecc3a76dd1a5945f8ec0aeca66/c1b63/webhook-notification-template.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/notifications/notification-integrations/",
      "published_at": "2021-12-19T15:28:40Z",
      "updated_at": "2021-12-19T15:28:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Early access The features described here are early access. You won't be able to use these features if you're not part of the early access program. For more information on related features, see our docs on Alerts notification channels, Incident Intelligence destinations, and Proactive Detection notifications. Alerts and Applied Intelligence notification integrations are specific services and platforms you can use to send notifications from New Relic. Integration details Read more about each of our specific notification integrations. Atlassian Jira Integrate New Relic with Atlassian Jira(Cloud) and automatically create and update Jira issues. Permissions The required permissions from the Jira API-Token are create, edit, and close tickets. To enable the two-way sync toggle, the provided Jira API-Key should have an Admin role. Set up a Jira destination Create Jira issues, then enable Jira and New Relic to share updates and stay synced. To create a Jira destination, enter the following information: Destination name: Custom name to identify the destination Jira account endpoint: the URL of the destination User-name: this will be the email address of the user making the connection API token: generated from your Atlassian account Important New Relic currently supports Atlassian-Jira Classic (company-managed) projects. Before saving the destination, we recommend you test the connection via the test connection button. Jira destination configuration. We recommand to test the connection before saving. Two-way sync You can enable a two-way integration with Jira to keep the issues' state synced with the corresponding state in New Relic. To enable two-way sync, turn on the ‘two-way integration’ toggle. When turned on, a Jira Webhook would be created in your Jira account at a later stage, for the selected project (see ‘customize a message template’). The webhook would contain access details to Newrelic (URL and Newrelic-API-KEY) Configure the message template To configure a template for a Jira issue, you first need to choose a destination. You will be able to create a new destination at this stage. Upon successful connection to the destination, you will need to choose a project, and then select the Jira issue type you would like to be used. Once the issue-type is selected, the configured project's fields are fetched from your account and automatically mapped to your Jira instance. To help you get started, we automatically present the required and recommended fields and values. Required fields must be set with a value. You can add or remove optional fields(use the X mark on their right side) Jira message template. Send a test notification You can see how the JIRA issue will appear by clicking a test notification with default field values. If successful, a JIRA issue will be created and a link will appear. ServiceNow (Incident-Management) Integrate New Relic with ServiceNow Incident-Management and automatically create and update incidents. Roles As part of the integrations, we fetch fields from the your serviceNow incident table and optional values. For this, the provided ServiceNow user details required read permissions for the tables: sys_dictionary, sys_choice, sys_user and task. A read/write permission to incident To be able to fetch users for the caller column, we required read permissions for the sys_users table. The above permissions can be achieved with the roles personalize_choices, personalize_dictionary, rest_service, itil. Read/Write permissions to the api_key_credentials table is required to enable two-way integration. This can be covered with the roles credentials_admin and discovery_admin. Set up a destination To create a ServiceNow destination, enter the following information: Destination Name: custom name to identify the destination Domain: the URL of the destination User-name: the name of the user Password: the user name’s password Before saving the destination, we recommend testing the connection by clicking the test connection button. Two-way sync You can configure a two-way integration with ServiceNow Incidents Management to keep the incidents' state synced with the corresponding state in New Relic. Here are some required steps to remember when configuring the two-way integration: Turn on the two-way integration toggle. Open and download this XML file, which includes the business rule triggering events back to New Relic One. In the ServiceNow sidebar menu, go to System Definition > Business Rules. Click the menu icon in one of the column headers, select Import XML and upload the XML file you downloaded. Once the Destination is saved, a New-Relic API-Key will be kept in the api_key_credentials. The key would sent in a header as part of the callback REST call to New-Relic Configure the message template Upon a successful connection, ServiceNow incident table columns are fetched from your account and automatically mapped to your ServiceNow instance. To help you get started, we automatically present the required and recommended fields and values. Required fields must be set with a value. You can add or remove optional fields(use the X mark on their right side) Select, edit or remove fields for the ServiceNow-Incident template. Send a test notification You can see how the ServiceNow incident will appear by clicking a test notification with default field values. If successful, an incident will be created and a link will appear. Slack Send notifications-messages to your Slack channels. Prerequisites Your Slack workspace needs to have the New Relic application installed. The application must be approved by a workspace admin before it can be individually installed by users Set up a Slack destination Click on the `one-click Slack authentication' will lead you to the Slack landing page to continue the OAuth2 authentication process. On the Slack landing page, if you're not signed into the required workspace, you're redirected to Slack to sign in. Add your workspace name or select the relevant workspace and click Continue. When signed in to the selected workspace, you are requested to allow New Relic to perform the specified actions. Clicking `Allow' will redirect you back to the Destination page. Configure the Slack message settings Select a Destination(Workspace) and select a Slack-channel where the messages will be sent. You can create a new destination if there is no pre-defined destination for the required workspace. Note that, for privacy reasons, users need to be authenticated to select private channels (one-time process) Send a test notification You can send a test notification with a pre-defined example payload to the channel. This creates a message in the selected Slack-channel. Webhook Use the webhook notifier to send the notification messages to any endpoint you like. Set up a webhook destination To create a webhook destination, you need the following: Destination Name: A unique destination name URL: the endpoint of the target application, authentication and custom headers if needed. Authorization mechanism (Optional):. Can be basic authentication or a bearer token Configure the webhook event template Pick a webhook destination from the list and configure the HTTP-POST request. The request configuration requires you to: Set a name for the template. Select a pre-configured destination from the destinations list or create a new one. Add custom headers (optional). Configure the request’s payload. Customize the webhook payload You can use the default payload or customize it to contain the required data. Pick Variables from the variables menu and apply handlebars syntax to enrich your webhook. Note that the request’s content-type is JSON by default. Hence, the payload needs to keep the JSON form. See Usage examples The ‘preview’ section on the right hand-side shows an expected payload after the template is rendered. If the eventual payload would not form a valid Json, an error will be shown and it won’t be possible to save the template. If the webhook payload conforms a valid Json, you can send a test notification to your defined webhook destination We recommend sending a test notification to make sure that everything's connected correctly. Email Send email notifications to users. Configure the email settings Add one or more recipients. Users with New Relic accounts can be found via autocomplete when searching for their name or email address. To add a user without a New Relic account or email distribution list, add the full email address. Every recipient will be translated into a 'destination'. You can follow the email notifications per destination in the notifications-log Send a test notification You can send a test notification to make sure the email notifications reach the inbox.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1756.745,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Alerts</em> <em>and</em> <em>applied</em> <em>intelligence</em> notification integrations",
        "sections": "<em>Alerts</em> <em>and</em> <em>applied</em> <em>intelligence</em> notification integrations",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": " notifications. <em>Alerts</em> and <em>Applied</em> <em>Intelligence</em> notification integrations are specific services and platforms you can use to send notifications from New Relic. Integration details Read more about each of our specific notification integrations. Atlassian Jira Integrate New Relic with Atlassian Jira(Cloud"
      },
      "id": "618ff71628ccbc60710321e4"
    },
    {
      "sections": [
        "Workflows",
        "What is an issue",
        "Add a Workflow",
        "Tip",
        "Enrichments",
        "Important",
        "Notifier"
      ],
      "title": "Workflows",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Workflows",
        "Notifications"
      ],
      "external_id": "0993c323f76467d22136cac6982ccd48e859f722",
      "image": "",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/applied-intelligence/incident-workflows/incident-workflows/",
      "published_at": "2021-12-19T13:46:37Z",
      "updated_at": "2021-12-17T01:48:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With workflows you control when and where you want to receive notifications about issues, tunnel the right information to the relevant person or team, and enrich your issue's notifications with additional New Relic data. What is an issue Issues are groups of incidents that describe the underlying problem of your symptoms. When a new incident is created, Incident Intelligence opens an issue and evaluates other open issues for correlations. For more information see Use Incident Inteligence. Add a Workflow Tip The maximum number of workflows you can add is 100 per environment and 50 per account. The workflows feature is located under the Alerts & AI menu. Go to one.newrelic.com, click Alerts & AI, in the left navigation under Enrich and Notify click Workflow, then click Add a workflow. Name your workflow. This field is mandatory and needs to be unique. Workflows filters the issues you want to send to the relevant destination. You can send all issues by selecting Send all issues or you can build a query to filter the right issues. Build a query by selecting an attribute, operator and value/s to narrow down the right issues you want to be passed on. Optional: Enriched Data, available to full platform users. Enrichments Enrichments can give additional context on alert notifications by adding NRQL query results to them The workflows enrich tool is similar to the query builder. The NRQL query's results are included in the notification message. If you create queries for your workflows, they run in real time and the query results are the value of the enricher. You can save any valid query into your workflow, even if they return an empty result, but you must run the query before saving the enrichment. You can also query with issue-specific variables; for example {{entitiesData.ids}} For enrichment examples, see Workflow data enrichment examples. Note: The query name needs to be unique because you'll use the name to decide how to include the result in the payload of the notification that will be sent by the workflow. Important At this stage, visual representation of enrichments are sent to Slack and email. For Servicenow and JIRA destinations, we recommend to limit to NRDB queries with single value results. For example: count, min, or max. The maximum number of enrichments per workflow is 5. The maximum number of variables an enrichment can contain is 1. Notify: Choose one or more destinations and add an optional message. Notifier To save and activate a Workflow, you need to include a notifier. Setting up a notifier requires having a notification destination configured. You can either set it up through the Destination menu item, or through the Create new destination item in the destination selection drop down in each destination channel screen. A notification destination includes the credentials required to send a notification to a third party platform and an option to customize the message sent by mapping fields from our issues to fields in the destination platform. To learn how to set up destinations and configure message templates, check the documentation on notifications. Click update message once completing the notifier requirements Optionally, test your workflow. We'll use existing data from your account to test what you've configured and send a sample notification Click activate to complete the workflow. Tip In any destination channel, start typing and a variable menu will open up. You'll see the names of the variables, which will be replaced with the variable's values at runtime. To use the enrichers' results, use their name.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1694.3864,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": " problem of your symptoms. When a new incident is created, Incident <em>Intelligence</em> opens an issue and evaluates other open issues for correlations. For more information see Use Incident Inteligence. Add a Workflow Tip The maximum number of workflows you can add is 100 per environment and 50 per account"
      },
      "id": "603e967664441f7e6f4e889b"
    },
    {
      "sections": [
        "Streaming alerts: key terms and concepts",
        "Why it matters",
        "Streaming alerts process and descriptions",
        "Choose your aggregation method",
        "Event flow",
        "Caution",
        "Event timer",
        "Cadence",
        "Streaming alerts tools",
        "Tip",
        "Window duration",
        "Delay/timer",
        "Loss of signal detection",
        "Gap filling"
      ],
      "title": "Streaming alerts: key terms and concepts",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Get started"
      ],
      "external_id": "144c7f0721ec2acb1e8ac8aa06142911ea552aef",
      "image": "https://docs.newrelic.com/static/39158bd84483adf9516bb7ff5058c9bd/d30ee/streaming-alerts.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/streaming-alerts-key-terms-concepts/",
      "published_at": "2021-12-15T12:47:58Z",
      "updated_at": "2021-12-15T12:47:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The streaming alerts platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into New Relic. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that's processed by the streaming algorithm. There are three methods for aggregating the data filtered through your NRQL condition: Event flow (default) Event timer Cadence This short video describes the three aggregation method (5:31). Why it matters Understanding how streaming alerts works will help you fine-tune your NRQL conditions to be notified about what's important to you. Only data that matches the conditions of the NRQL WHERE clause is alerted on. For more details on each step of the process, see Streaming alerts process and descriptions. As data streams into New Relic, it's filtered by the NRQL condition. Before data is evaluated, it must meet the criteria defined by the NRQL query's WHERE clause. Instead of evaluating that data immediately for violations, the NRQL alert conditions collect the data over a period of time known as the aggregation window. An additional delay/timer allows for slower data points to arrive before the window is aggregated. Once the delay/timer time has elapsed, New Relic aggregates the data into a single data point. Alerts then evaluates the data point using the NRQL condition in order to determine whether it meets the violation threshold criteria. Even if a data point meets the criteria for a violation, a violation may not be triggered. A violation is only triggered when data points consistently meet the threshold criteria over a period of time. This is the threshold duration. If the data points are in violation for an entire threshold duration, we'll send you a notification based on your policy settings. All of these configurable delays give you more control over how you're alerted on sporadic and missing data. Streaming alerts process and descriptions Process Description Streaming data All data coming into New Relic. WHERE clause Filters all incoming streaming data. We only monitor for alerts on data that makes it through this filter. Aggregation methods One of three methods that control how data is collected before it's evaluated. They are: Event flow (Default) Event timer Cadence Aggregation window Data with timestamps that fall within this window will be aggregated and then evaluated. Delay/timer A time delay to ensure all data points have arrived in the aggregation window before aggregation occurs. Aggregated data Data in the aggregated window is collapsed to a single data point for alert evaluation. Evaluation The data point is evaluated by the NRQL condition, which is trigged by each incoming aggregated data point. Threshold duration A specific duration that determines if a violation is created. If your specified NRQL condition meets the threshold criteria over the threshold duration, a violation occurs. When a data point lacks data, a custom value is inserted to fill the gap. Choose your aggregation method You can choose between three different aggregation methods, depending on your needs. Event flow (default) works best for data that comes in frequently and mostly in order. Event timer works best for data that arrives infrequently in batches, such as cloud integration data or infrequent error logs. Cadence is our original and inferior aggregation method. It aggregates data on specific time intervals as detected by New Relic's internal wall clock, regardless of data timestamps. Event flow Event flow aggregates a window of data when the first data point arrives in a subsequent window. The custom delay defines which subsequent window data will start to populate to trigger aggregation of the current window. A custom delay provides extra time for data to arrive. These times are based on the data's timestamps and not New Relic's wall clock time. For example, suppose you're monitoring CPU usage in window durations of 1 minute and a 3 minute delay. When a CPU usage data point comes in with a timestamp between 12:00pm and 12:01pm, event flow will not aggregate that window until a data point shows up with a timestamp between 12:04pm and 12:05pm. When event flow receives the first data point with a timestamp of 12:04pm or later, it sends the 12:00 to 12:01 data to be aggregated. Caution If you expect your data points to arrive more than 30 minutes apart, please use the Event Timer method described below. Event timer Like event flow, event timer will only aggregate data for a given window when data arrives for that window. When a data point arrives for an aggregation window, a timer dedicated to that window starts to count down. If no further data arrives before the timer counts down, the data for that window is aggregated. If more data points arrive before the timer has completed counting down, the timer is reset. For example, suppose you're monitoring CloudWatch data that arrives fairly infrequently. You're using a window duration of 1 minute and a 3 minute timer. When a CloudWatch data point comes in with a timestamp between 12:00pm and 12:01pm, the timer will start to count down. If no further data points show up for that 12:00-12:01 window, the window will get aggregated 3 minutes later. If a new data point with a timestamp between 12:00 and 12:01 arrives, the timer resets. It keeps resetting every time more data points for that window arrive. The window will not be sent for aggregation until the timer reaches 0. If the timer for a later data point elapses before an earlier data point, the event timer method waits for the earlier timer to elapse before aggregating the later data point. For best results, make sure your timer is equal to or longer than your window duration time. If the timer is less than your window duration and your data flow is inconsistent, then your data may be evaluated before all of your data points arrive. This could cause you to be notified incorrectly. Cadence We recommend you use one of the other two methods. Cadence is our old streaming aggregation method. This method uses New Relic's wall clock time to determine when data is aggregated and evaluated. It doesn't take into account data point timestamps as they arrive. Streaming alerts tools Streaming alerts provide a set of tools to give you greater control over how your data is aggregated before it's evaluated to reduce incorrect notifications you receive. They are: Window duration Delay/timer Loss of signal detection Gap filling Tip This article covers these tools at a conceptual level. You'll find direct instructions on how to use these tools in Create NRQL alert conditions. Window duration In order to make loss of signal detection more effective and to reduce unnecessary notifications, you can customize aggregation windows to the duration that you need. An aggregation window is a specific block of time. We gather data points together in an aggregation window, before evaluating the data. A longer aggregation window can smooth out the data, since an outlier data point will have more data points to be aggregated with, giving it less of an influence on the aggregated data point that is sent for evaluation. When a data point arrives, its timestamp is used to put it in the proper aggregation window. You can set your aggregation window to anything between 30 seconds and 15 minutes. The default is 1 minute. Delay/timer The delay/timer setting controls how long the condition should wait before aggregating the data in the aggregation window. The event flow and cadence methods use delay. Event timer uses timer. The delay default is 2 minutes. The timer default is 1 minute and has a minimum value of 5 seconds. Loss of signal detection Loss of signal occurs when no data matches the NRQL condition over a specific period of time. A loss of signal is caused by different things. The WHERE clause in your NRQL query can filter out data before it's evaluated for violations. It could also mean a service or entity is offline or a periodic job has failed to run and no data is being sent to New Relic. In order to avoid unnecessary notifications, you can choose how long to wait before you're notified by a loss of signal violation. You can use loss of signal detection to open violations and be notified when a signal is lost. Alternately, you can use a loss of signal to close violations for ephemeral services or sporadic data, such as error counts. Gap filling Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with the last value received, a static value, or else do nothing and leave the gap there. The default is None. Gaps in streaming data can be caused by network or host issues, a signal may be sparse, or some signals, such as error counts, may only have data when something is wrong. By filling the gaps with known values, the alert evaluation process can process those gaps and determine how they should affect the loss of signal evaluation. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss and gap filling, see this Explorers Hub post.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1396.2888,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "sections": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "The streaming <em>alerts</em> platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into New Relic. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that&#x27;s processed"
      },
      "id": "6130c054e7b9d269edb6f21d"
    }
  ],
  "/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/alert-custom-violation-descriptions": [
    {
      "sections": [
        "Create NRQL alert conditions",
        "Tip",
        "Create a NRQL alert condition",
        "Create a condition from a chart",
        "Caution",
        "NRQL alert syntax",
        "Important",
        "Reformatting incompatible NRQL",
        "NRQL alert threshold examples",
        "Alert on specific segments of your data",
        "Alert on Nth percentile of your data",
        "Alert on max, min, avg of your data",
        "Alert on a percentage of your data",
        "Alert on Apdex with any T-value",
        "NRQL conditions and query order of operations",
        "Example: null value returned",
        "Example: zero value returned",
        "Nested aggregation NRQL alerts",
        "Nested queries with a non-faceted innermost query are not currently supported",
        "Queries at all levels must have the same aggregation window size",
        "Signal loss is not yet supported for nested queries",
        "Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported",
        "NRQL condition creation tips",
        "Condition edits can reset condition evaluation",
        "Alert condition types",
        "Sum of query results (limited or intermittent data)",
        "Set the loss of signal threshold",
        "Advanced signal settings",
        "Aggregation window duration",
        "Delay/timer",
        "Fill data gaps"
      ],
      "title": "Create NRQL alert conditions",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert conditions"
      ],
      "external_id": "2872f770a5fc8a802f99b9f848906f7e351ad761",
      "image": "https://docs.newrelic.com/static/eb8e1b2d826f9fd9cf46fe3dd5455217/c1b63/nr1_nrql_alert_conditions.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/",
      "published_at": "2021-12-19T16:01:17Z",
      "updated_at": "2021-12-14T22:49:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use NRQL queries to create alert conditions. Once you've defined your signal, you can further define your warning and critical threshold levels. This determines when an alerts violation is created. Read on to learn more about how to do this. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Click NRQL, and then Next, define thresholds. Tip For more information on key concepts relating to NRQL alert conditions and streaming alerts, see Streaming alerts: key terms and concepts. Create a NRQL alert condition To create a NRQL alert condition for a policy: On one.newrelic.com, in the header click Alerts & AI, then in the left sidebar click Policies. Select an existing policy or click New alert policy to create a new policy. Click Add a condition. Under Select a product click NRQL, and then click Next, define thresholds. Note that editing an existing condition can result in resetting its evaluation. Create a condition from a chart You can use a chart to create a NRQL alert condition. To create a NRQL alerts condition from a chart, click the chart menu , then click Create alert condition. Once you've named and customized your condition, you can add it to an existing policy or create a new one. Caution A small number of our older charts don't include the option to create an alert condition. NRQL alert syntax Here's the basic syntax for creating all NRQL alert conditions. The FACET clause is required for outlier condition types. It's optional for static and baseline. SELECT function(attribute) FROM Event WHERE attribute [comparison] [AND|OR ...] Copy Clause Notes SELECT function(attribute) Required Supported functions that return numbers include: apdex average count latest max min percentage percentile sum uniqueCount Tip If you use the percentile aggregator in a faceted alert condition with many facets, this may cause the following error to appear: An error occurred while fetching chart data. If you see this error, use average instead. FROM data type Required Only one data type can be targeted. Supported data types: Event Metric (RAW data points will be returned) WHERE attribute [comparison] [AND|OR ...] Use the WHERE clause to specify a series of one or more conditions. All the operators are supported. FACET attribute Required for outlier conditions Include an optional FACET clause in your NRQL syntax depending on the threshold type: static or baseline. Use the FACET clause to separate your results by attribute and alert on each attribute independently. No LIMIT clause is allowed, but all queries will receive the maximum number of facets possible. Faceted queries can return a maximum of 5000 values for static and baseline conditions and a maximum of 500 values for outlier conditions. Important If the query returns more than the maximum number of values, the alert condition can't be created. If you create the condition and the query returns more than this number later, the alert will fail. Modify your query so that it returns a fewer number of values. Reformatting incompatible NRQL Some elements of NRQL used in charts don’t make sense in the streaming context of alerts. Here’s a list of the most common incompatible elements and suggestions for reformatting a NRQL alert query to achieve the same effect. Element Notes SINCE and UNTIL Example: SELECT percentile(largestContentfulPaint, 75) FROM PageViewTiming WHERE (appId = 837807) SINCE yesterday Copy NRQL conditions produce a never-ending stream of windowed query results, so the SINCE and UNTIL keywords to scope the query to a point in time are not compatible. As a convenience, we automatically strip SINCE and UNTIL from a query when creating a condition from the context of a chart. TIMESERIES In NRQL queries, the TIMESERIES clause is used to return data as a time series broken out by a specified period of time. For NRQL conditions, the equivalent property of a signal is the aggregation duration window. histogram() The histogram() aggregation function is used to generate histograms. histogram() is not compatible with NRQL alerting: histogram aggregations can not be formatted as a time series. To create an alert from a portion of a histogram (for example, 95th percentile), use the percentile() aggregation function. Multiple aggregation functions Each condition can only target a single aggregated value. To alert on multiple values simultaneously, you’ll need to decompose them into individual conditions within the same policy. Original Query: SELECT count(foo), average(bar), max(baz) from Transaction Copy Decomposed: SELECT count(foo) from Transaction SELECT average(bar) from Transaction SELECT max(baz) from Transaction Copy COMPARE WITH The COMPARE WITH clause is used to compare the values for two different time ranges. This type of query is incompatible with NRQL alerting. We recommend using a Baseline Alert Condition to dynamically detect deviations for a particular signal. SLIDE BY The SLIDE BY clause supports a feature known as sliding windows. With sliding windows, SLIDE BY data is gathered into \"windows\" of time that overlap with each other. These windows can help to smooth out line graphs with a lot of variation in cases where the rolling aggregate (such as a rolling mean) is more important than aggregates from narrow windows of time. Sliding windows are not currently supported in NRQL alerts. LIMIT In NRQL queries, the LIMIT clause is used to control the amount of data a query returns, either the maximum number of facet values returned by FACET queries or the maximum number of items returned by SELECT * queries. LIMIT is not compatible with NRQL alerting: evaluation is always performed on the full result set. NRQL alert threshold examples Here are some common use cases for NRQL conditions. These queries will work for static and baseline condition types. The outlier condition type will require additional FACET clauses. Alert on specific segments of your data Create constrained alerts that target a specific segment of your data, such as a few key customers or a range of data. Use the WHERE clause to define those conditions. SELECT average(duration) FROM Transaction WHERE account_id in (91290, 102021, 20230) Copy SELECT percentile(duration, 95) FROM Transaction WHERE name LIKE 'Controller/checkout/%' Copy Alert on Nth percentile of your data Create alerts when an Nth percentile of your data hits a specified threshold; for example, maintaining SLA service levels. Since we evaluate the NRQL query based on the aggregation window duration, percentiles will be calculated for each duration separately. SELECT percentile(duration, 95) FROM Transaction Copy SELECT percentile(databaseDuration, 75) FROM Transaction Copy Alert on max, min, avg of your data Create alerts when your data hits a certain maximum, minimum, or average; for example, ensuring that a duration or response time does not pass a certain threshold. SELECT max(duration) FROM Transaction Copy SELECT average(duration) FROM Transaction Copy Alert on a percentage of your data Create alerts when a proportion of your data goes above or below a certain threshold. SELECT percentage(count(*), WHERE duration > 2) FROM Transaction Copy SELECT percentage(count(*), WHERE httpResponseCode = '500') FROM Transaction Copy Alert on Apdex with any T-value Create alerts on Apdex, applying your own T-value for certain transactions. For example, get an alert notification when your Apdex for a T-value of 500ms on transactions for production apps goes below 0.8. SELECT apdex(duration, t:0.5) FROM Transaction WHERE appName like '%prod%' Copy NRQL conditions and query order of operations By default, the aggregation window duration is 1 minute, but you can change the window to suit your needs. Whatever the aggregation window, New Relic will collect data for that window using the function in the NRQL condition’s query. The query is parsed and executed by our systems in the following order: FROM clause – which event type needs to be grabbed? WHERE clause – what can be filtered out? SELECT clause – what information needs to be returned from the now-filtered data set? Example: null value returned Let's say this is your alert condition query: SELECT count(*) FROM SyntheticCheck WHERE monitorName = 'My Cool Monitor' AND result = 'FAILURE' Copy If there are no failures for the aggregation window: The system will execute the FROM clause by grabbing all SyntheticCheck events on your account. Then it will execute the WHERE clause to filter through those events by looking only for the ones that match the monitor name and result specified. If there are still events left to scan through after completing the FROM and WHERE operations, the SELECT clause will be executed. If there are no remaining events, the SELECT clause will not be executed. This means that aggregators like count() and uniqueCount() will never return a zero value. When there is a count of 0, the SELECT clause is ignored and no data is returned, resulting in a value of NULL. Example: zero value returned If you have a data source delivering legitimate numeric zeroes, the query will return zero values and not null values. Let's say this is your alert condition query, and that MyCoolEvent is an attribute that can sometimes return a zero value. SELECT average(MyCoolAttribute) FROM MyCoolEvent Copy If, in the aggregation window being evaluated, there's at least one instance of MyCoolEvent and if the average value of all MyCoolAttribute attributes from that window is equal to zero, then a 0 value will be returned. If there are no MyCoolEvent events during that minute, then a NULL will be returned due to the order of operations. Tip For more information about this topic, you can check out our blog post on troubleshooting for zero versus null values. Tip You can determine how null values will be handled by adjusting loss of signal and gap filling settings in the Alert Conditions UI. Tip You can avoid NULL values entirely with a query order of operations shortcut. Do this by using a filter sub-clause, then including all filter elements within that sub-clause. The main body of the query will run and return data, at which point the SELECT clause will then run and apply the filter elements. The query will return a value of 0 if the filter elements result in no matching data. Here's an example: SELECT filter(count(*), WHERE result = 'SUCCESS' AND monitorName = 'My Favorite Monitor') FROM SyntheticCheck Copy Nested aggregation NRQL alerts Nested aggregation queries are a powerful way to query your data. However, they have a few restrictions that are important to note. Nested queries with a non-faceted innermost query are not currently supported Without a FACET, the inner query produces a single result, giving the outer query nothing to aggregate. If you're using a nested query, make sure your inner query is faceted. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu) ​​​​​ Copy Queries at all levels must have the same aggregation window size With an alert aggregation window of 1 minute, the inner query would produce two smaller windows of 30 seconds. In theory, these two windows could be aggregated by the outer query. However, this is not currently supported. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu TIMESERIES 30 seconds)​​ Copy Signal loss is not yet supported for nested queries For more information on signal loss, see NerdGraph API: Loss of signal and gap filling. Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported You can't use a nested query containing the WITH METRIC_FORMAT in the inner query to create NRQL alert conditions. NRQL condition creation tips Here are some tips for creating and using a NRQL condition: Topic Tips Condition types NRQL condition types include static, baseline, and outlier. Create a description For NRQL conditions, you can create a custom description to add to each violation. Descriptions can be enhanced with variable substitution based on metadata in the specific violation. For details, see Description Query results Queries must return a number. The condition evaluates the returned number against the thresholds you've set. Time period NRQL conditions evaluate data based on how it's aggregated, using aggregation windows from 30 seconds to 15 minutes, in increments of 15 seconds. For best results, we recommend using the event flow or event timer aggregation methods. For the cadence aggregation method, the implicit SINCE ... UNTIL clause specifying which minute to evaluate is controlled by your delay/timer setting. Since very recent data may be incomplete, you may want to query data from 3 minutes ago or longer, especially for: Applications that run on multiple hosts. SyntheticCheck data: Timeouts can take 3 minutes, so 5 minutes or more is recommended. Also, if a query will generate intermittent data, consider using the sum of query results option. Lost signal threshold (loss of signal detection) You can use loss of signal detection to alert on when your data (a telemetry signal) should be considered lost. A signal loss can indicate that a service or entity is no longer online or that a periodic job failed to run. You can also use this to make sure that violations for sporadic data, such as error counts, are closed when no signal is coming in. Advanced signal settings These settings give you options for better handling continuous, streaming data signals that may sometimes be missing. These settings include the aggregation window duration, the delay/timer, and an option for filling data gaps. For more on using these, see Advanced signal settings. Condition settings Use the Condition settings to: Create a concise, descriptive condition name. Provide a custom violation description for the condition that will be included in violations and notifications. Add the runbook URL to include your organization's procedures for handling incidents. You may also add this information to the custom violation description. Limits on conditions See the maximum values. Health status In order for a NRQL alert condition health status display to function properly, use a FACET clause to scope each signal to a single entity (for example, FACET hostname or FACET appname). Examples For more information, see: Expected NRQL syntax Examples of NRQL condition queries Condition edits can reset condition evaluation When you edit NRQL alert conditions in some specific ways (detailed below), their evaluations are reset, meaning that any evaluation up until that point is lost, and the evaluation starts over from that point. The two ways this will affect you are: For \"for at least x minutes\" thresholds: because the evaluation window has been reset, there will be a delay of at least x minutes before any violations can be reported. For baseline conditions: the condition starts over again and all baseline learning is lost. The following actions cause an evaluation reset for NRQL conditions: Changing the query Changing the aggregation window, aggregation method, or aggregation delay/timer setting Changing the \"close violations on signal loss\" setting Changing any gap fill settings Changing the baseline direction (if applicable) – higher, lower, or higher/lower Change the threshold value, threshold window, or threshold operator The following actions (along with any other actions not covered in the above list) will not reset the evaluation: Changing the loss of signal time window (expiration duration) Changing the time function (switching \"for at least\" to \"at least once in,\" or vice-versa) Toggling the \"open violation on signal loss\" setting Alert condition types When you create a NRQL alert, you can choose from different types of conditions: NRQL alert condition types Description Static This is the simplest type of NRQL condition. It allows you to create a condition based on a NRQL query that returns a numeric value. Optional: Include a FACET clause. Baseline (Dynamic) Uses a self-adjusting condition based on the past behavior of the monitored values. Uses the same NRQL query form as the static type, including the optional FACET clause. Outlier Looks for group behavior and values that are outliers from those groups. Uses the same NRQL query form as the static type, but requires a FACET clause. Sum of query results (limited or intermittent data) Important Available only for static (basic) condition types. If a query returns intermittent or limited data, it may be difficult to set a meaningful threshold. Missing or limited data will sometimes generate false positives or false negatives. You can use loss of signal, aggregation duration, and gap filling settings to minimize these false notifications. To avoid this problem when using the static threshold type, you can set the selector to sum of query results. This lets you set the alert on an aggregated sum instead of a value from a single harvest cycle. Up to two hours of one-minute data checks can be aggregated. The duration you select determines the width of the rolling sum and the preview chart will update accordingly. Set the loss of signal threshold Loss of signal occurs when no data matches the NRQL condition over a specific period of time. You can set your loss of signal threshold duration and also what happens when the threshold is crossed. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Loss of signal is only available for NRQL conditions. You may also manage these settings using the GraphQL API (recommended), or the REST API. Go here for specific GraphQL API examples. Loss of signal settings: Loss of signal settings include a time duration and two possible actions. Signal loss expiration time UI label: Signal is lost after: GraphQL Node: expiration.expirationDuration Expiration duration is a timer that starts and resets when we receive a data point in the streaming alerts pipeline. If we don't receive another data point before your 'expiration time' expires, we consider that signal to be lost. This can be because no data is being sent to New Relic or the WHERE clause of your NRQL query is filtering that data out before it is streamed to the alerts pipeline. Note that when you have a faceted query, each facet is a signal. So if any one of those signals ends during the duration specified, that will be considered a loss of signal. The loss of signal expiration time is independent of the threshold duration and triggers as soon as the timer expires. The maximum expiration duration is 48 hours. This is helpful when monitoring for the execution of infrequent jobs. The minimum is 30 seconds, but we recommend using at least 3-5 minutes. Loss of signal actions Once a signal is considered lost, you can close open violations, open new violations, or both. Close all current open violations: This closes all open violations that are related to a specific signal. It won't necessarily close all violations for a condition. If you're alerting on an ephemeral service, or on a sporadic signal, you'll want to choose this action to ensure that violations are closed properly. The GraphQL node name for this is \"closeViolationsOnExpiration\" Open new violations: This will open a new violation when the signal is considered lost. These violations will indicate that they are due to a loss of signal. Based on your incident preferences, this should trigger a notification. The graphQL node name for this is \"openViolationOnExpiration\" When you enable both actions, we'll close all open violations first, and then open a new violation for loss of signal. To create a NRQL alert configured with loss of signal detection in the UI: For a policy, when you create a condition, under Select a product, click NRQL, then click Next, define thresholds. Write a NRQL query that returns the values you want to alert on. For Threshold type, select Static or Baseline. Click + Add lost signal threshold, then set the signal expiration duration time in minutes or seconds in the Signal is lost after field. Choose what you want to happen when the signal is lost. You can check one or both of Close all current open violations and Open new \"lost signal\" violation. These control how loss of signal violations will be handled for the condition. Make sure you name your condition before you save it. Violations open due to loss of signal close when the signal comes back. Newly opened lost signal violations will close immediately when new data is evaluated. the condition they belong to expires. By default, conditions expire after 3 days. you manually close the violation with the Close all current open violations option. Tip Loss of signal detection doesn't work on NRQL queries that use nested aggregation or sub-queries. Advanced signal settings When creating a NRQL alert condition, use the advanced signal settings to control streaming alert data and avoid false alarms. When creating a NRQL condition, there are several advanced signal settings: Aggregation window duration Delay/timer Fill data gaps To read an explanation of what these settings are and how they relate to each other, see Streaming alerts concepts. Below are instructions and tips on how to configure them. Aggregation window duration You can set the aggregation window duration to choose how long data is accumulated in a streaming time window before it's aggregated. You can set it to anything between 30 seconds and 15 minutes. The default is one minute. Tip Baseline alert condition thresholds don't support editing the aggregation window. They use the 1 minute default. Delay/timer You can adjust the delay/timer to coordinate our streaming alerting algorithm with your data's behavior. If your data is sparse or inconsistent, you may want to use the event timer aggregation method. For the cadence method, the total supported latency is the sum of the aggregation window duration and the delay. If the data type comes from an APM language agent and is aggregated from many app instances (for example, Transactions, TransactionErrors, etc.), we recommend using the event flow method with the default settings. Important When creating NRQL conditions for data collected from Infrastructure Cloud Integrations such as AWS Cloudwatch or Azure, we recommend that you use the event timer method. Fill data gaps Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with one of these settings: None: (Default) Choose this if you don't want to take any action on empty aggregation windows. On evaluation, an empty aggregation window will reset the threshold duration timer. For example, if a condition says that all aggregation windows must have data points above the threshold for 5 minutes, and 1 of the 5 aggregation windows is empty, then the condition won't be in violation. Custom static value: Choose this if you'd like to insert a custom static value into the empty aggregation windows before they're evaluated. This option has an additional, required parameter of fillValue (as named in the API) that specifies what static value should be used. This defaults to 0. Last known value: This option inserts the last seen value before evaluation occurs. We maintain the state of the last seen value for 2 hours. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss, gap filling, and how to request access to these features, see this Explorers Hub post. Options for editing data gap settings: In the NRQL conditions UI, go to Condition settings > Advanced signal settings > fill data gaps with and choose an option. If using our Nerdgraph API (preferred), this node is located at: actor : account : alerts : nrqlCondition : signal : fillOption | fillValue NerdGraph is our recommended API for this but if you're using our REST API, you can find this setting in the REST API explorer under the \"signal\" section of the Alert NRQL conditions API.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 251.85663,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create NRQL <em>alert</em> <em>conditions</em>",
        "sections": "Create NRQL <em>alert</em> <em>conditions</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "You can use NRQL queries to create <em>alert</em> <em>conditions</em>. Once you&#x27;ve defined your signal, you can further define your warning and critical threshold levels. This determines when an <em>alerts</em> violation is created. Read on to learn more about how to do this. Go to one.newrelic.com, click <em>Alerts</em> &amp; AI"
      },
      "id": "603ef04864441fbc114e8883"
    },
    {
      "sections": [
        "Streaming alerts: key terms and concepts",
        "Why it matters",
        "Streaming alerts process and descriptions",
        "Choose your aggregation method",
        "Event flow",
        "Caution",
        "Event timer",
        "Cadence",
        "Streaming alerts tools",
        "Tip",
        "Window duration",
        "Delay/timer",
        "Loss of signal detection",
        "Gap filling"
      ],
      "title": "Streaming alerts: key terms and concepts",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Get started"
      ],
      "external_id": "144c7f0721ec2acb1e8ac8aa06142911ea552aef",
      "image": "https://docs.newrelic.com/static/39158bd84483adf9516bb7ff5058c9bd/d30ee/streaming-alerts.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/streaming-alerts-key-terms-concepts/",
      "published_at": "2021-12-15T12:47:58Z",
      "updated_at": "2021-12-15T12:47:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The streaming alerts platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into New Relic. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that's processed by the streaming algorithm. There are three methods for aggregating the data filtered through your NRQL condition: Event flow (default) Event timer Cadence This short video describes the three aggregation method (5:31). Why it matters Understanding how streaming alerts works will help you fine-tune your NRQL conditions to be notified about what's important to you. Only data that matches the conditions of the NRQL WHERE clause is alerted on. For more details on each step of the process, see Streaming alerts process and descriptions. As data streams into New Relic, it's filtered by the NRQL condition. Before data is evaluated, it must meet the criteria defined by the NRQL query's WHERE clause. Instead of evaluating that data immediately for violations, the NRQL alert conditions collect the data over a period of time known as the aggregation window. An additional delay/timer allows for slower data points to arrive before the window is aggregated. Once the delay/timer time has elapsed, New Relic aggregates the data into a single data point. Alerts then evaluates the data point using the NRQL condition in order to determine whether it meets the violation threshold criteria. Even if a data point meets the criteria for a violation, a violation may not be triggered. A violation is only triggered when data points consistently meet the threshold criteria over a period of time. This is the threshold duration. If the data points are in violation for an entire threshold duration, we'll send you a notification based on your policy settings. All of these configurable delays give you more control over how you're alerted on sporadic and missing data. Streaming alerts process and descriptions Process Description Streaming data All data coming into New Relic. WHERE clause Filters all incoming streaming data. We only monitor for alerts on data that makes it through this filter. Aggregation methods One of three methods that control how data is collected before it's evaluated. They are: Event flow (Default) Event timer Cadence Aggregation window Data with timestamps that fall within this window will be aggregated and then evaluated. Delay/timer A time delay to ensure all data points have arrived in the aggregation window before aggregation occurs. Aggregated data Data in the aggregated window is collapsed to a single data point for alert evaluation. Evaluation The data point is evaluated by the NRQL condition, which is trigged by each incoming aggregated data point. Threshold duration A specific duration that determines if a violation is created. If your specified NRQL condition meets the threshold criteria over the threshold duration, a violation occurs. When a data point lacks data, a custom value is inserted to fill the gap. Choose your aggregation method You can choose between three different aggregation methods, depending on your needs. Event flow (default) works best for data that comes in frequently and mostly in order. Event timer works best for data that arrives infrequently in batches, such as cloud integration data or infrequent error logs. Cadence is our original and inferior aggregation method. It aggregates data on specific time intervals as detected by New Relic's internal wall clock, regardless of data timestamps. Event flow Event flow aggregates a window of data when the first data point arrives in a subsequent window. The custom delay defines which subsequent window data will start to populate to trigger aggregation of the current window. A custom delay provides extra time for data to arrive. These times are based on the data's timestamps and not New Relic's wall clock time. For example, suppose you're monitoring CPU usage in window durations of 1 minute and a 3 minute delay. When a CPU usage data point comes in with a timestamp between 12:00pm and 12:01pm, event flow will not aggregate that window until a data point shows up with a timestamp between 12:04pm and 12:05pm. When event flow receives the first data point with a timestamp of 12:04pm or later, it sends the 12:00 to 12:01 data to be aggregated. Caution If you expect your data points to arrive more than 30 minutes apart, please use the Event Timer method described below. Event timer Like event flow, event timer will only aggregate data for a given window when data arrives for that window. When a data point arrives for an aggregation window, a timer dedicated to that window starts to count down. If no further data arrives before the timer counts down, the data for that window is aggregated. If more data points arrive before the timer has completed counting down, the timer is reset. For example, suppose you're monitoring CloudWatch data that arrives fairly infrequently. You're using a window duration of 1 minute and a 3 minute timer. When a CloudWatch data point comes in with a timestamp between 12:00pm and 12:01pm, the timer will start to count down. If no further data points show up for that 12:00-12:01 window, the window will get aggregated 3 minutes later. If a new data point with a timestamp between 12:00 and 12:01 arrives, the timer resets. It keeps resetting every time more data points for that window arrive. The window will not be sent for aggregation until the timer reaches 0. If the timer for a later data point elapses before an earlier data point, the event timer method waits for the earlier timer to elapse before aggregating the later data point. For best results, make sure your timer is equal to or longer than your window duration time. If the timer is less than your window duration and your data flow is inconsistent, then your data may be evaluated before all of your data points arrive. This could cause you to be notified incorrectly. Cadence We recommend you use one of the other two methods. Cadence is our old streaming aggregation method. This method uses New Relic's wall clock time to determine when data is aggregated and evaluated. It doesn't take into account data point timestamps as they arrive. Streaming alerts tools Streaming alerts provide a set of tools to give you greater control over how your data is aggregated before it's evaluated to reduce incorrect notifications you receive. They are: Window duration Delay/timer Loss of signal detection Gap filling Tip This article covers these tools at a conceptual level. You'll find direct instructions on how to use these tools in Create NRQL alert conditions. Window duration In order to make loss of signal detection more effective and to reduce unnecessary notifications, you can customize aggregation windows to the duration that you need. An aggregation window is a specific block of time. We gather data points together in an aggregation window, before evaluating the data. A longer aggregation window can smooth out the data, since an outlier data point will have more data points to be aggregated with, giving it less of an influence on the aggregated data point that is sent for evaluation. When a data point arrives, its timestamp is used to put it in the proper aggregation window. You can set your aggregation window to anything between 30 seconds and 15 minutes. The default is 1 minute. Delay/timer The delay/timer setting controls how long the condition should wait before aggregating the data in the aggregation window. The event flow and cadence methods use delay. Event timer uses timer. The delay default is 2 minutes. The timer default is 1 minute and has a minimum value of 5 seconds. Loss of signal detection Loss of signal occurs when no data matches the NRQL condition over a specific period of time. A loss of signal is caused by different things. The WHERE clause in your NRQL query can filter out data before it's evaluated for violations. It could also mean a service or entity is offline or a periodic job has failed to run and no data is being sent to New Relic. In order to avoid unnecessary notifications, you can choose how long to wait before you're notified by a loss of signal violation. You can use loss of signal detection to open violations and be notified when a signal is lost. Alternately, you can use a loss of signal to close violations for ephemeral services or sporadic data, such as error counts. Gap filling Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with the last value received, a static value, or else do nothing and leave the gap there. The default is None. Gaps in streaming data can be caused by network or host issues, a signal may be sparse, or some signals, such as error counts, may only have data when something is wrong. By filling the gaps with known values, the alert evaluation process can process those gaps and determine how they should affect the loss of signal evaluation. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss and gap filling, see this Explorers Hub post.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 229.45622,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "sections": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "The streaming <em>alerts</em> platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into <em>New</em> <em>Relic</em>. You can use NRQL <em>conditions</em> to control what part of the signal you want to be notified about. Your NRQL condition filters the data that&#x27;s processed"
      },
      "id": "6130c054e7b9d269edb6f21d"
    },
    {
      "sections": [
        "Outlier detection (NRQL alert)",
        "Important",
        "What is outlier detection?",
        "Tip",
        "Example use cases",
        "Notify if load-balanced servers have uneven workload",
        "Notify if load-balanced application has misbehaving instances",
        "Notify of changes in different environments",
        "Notify for timezone-related changes",
        "Create an outlier alert condition",
        "Rules and logic",
        "Details about alert condition logic",
        "NRQL query rules and limits",
        "Zero values for unreturned data"
      ],
      "title": "Outlier detection (NRQL alert)",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert conditions"
      ],
      "external_id": "499fa55abd48a0ccdd897fbdf64ccea2d9f98d11",
      "image": "https://docs.newrelic.com/static/f235d0630576bc2010ff07adc7a69621/f73a1/NRQL_outlier_violations.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/outlier-detection-nrql-alert/",
      "published_at": "2021-12-15T12:44:32Z",
      "updated_at": "2021-11-25T19:50:19Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Alerts offers NRQL conditions in three threshold types: static, baseline, and outlier. This document explains how the outlier threshold type works, gives some example use cases and NRQL queries, and explains how to create an outlier condition. Important NRQL alerts do not affect Alerts policies for a Synthetic monitor. For example, muting a NRQL alert will not mute a Synthetic monitor's alerts. What is outlier detection? In software development and operations, it is common to have a group consisting of members you expect to behave approximately the same. For example: for servers using a load balancer, the traffic to the servers may go up or down, but the traffic for all the servers should remain in a fairly tight grouping. See outlier detection in action in this NerdBytes video (2:51 minutes). The NRQL alert outlier detection feature parses the data returned by your faceted NRQL query and: Looks for the number of expected groups that you specify Looks for outliers (values deviating from a group) based on the sensitivity and time range you set Additionally, for queries that have more than one group, you can choose to be notified when groups start behaving the same. This visual aid will help you understand the types of situations that will trigger a violation and those that won't. For more on the rules and logic behind this calculation, see Outlier detection rules. Tip Note: this feature does not take into account the past behavior of the monitored values; it looks for outliers only in the currently reported data. For an alert type that takes into account past behavior, see Baseline alerting. Example use cases These use cases will help you understand when to use the outlier threshold type. Note that the outlier feature requires a NRQL query with a FACET clause. Notify if load-balanced servers have uneven workload A load balancer divides web traffic approximately evenly across five different servers. You can set a notification to be sent if any server starts getting significantly more or less traffic than the other servers. Example query: SELECT average(cpuPercent) FROM SystemSample WHERE apmApplicationNames = 'MY-APP-NAME' FACET hostname Copy Notify if load-balanced application has misbehaving instances Application instances behind a load balancer should have similar throughput, error rates, and response times. If an instance is in a bad state, or a load balancer is misconfigured, this will not be the case. Detecting one or two bad app instances using aggregate metrics may be difficult if there is not a significant rise in the overall error rate of the application. You can set a notification for when an app instance’s throughput, error rate, or response time deviates too far from the rest of the group. Example query: SELECT average(duration) FROM Transaction WHERE appName = 'MY-APP-NAME' FACET host Copy Notify of changes in different environments An application is deployed in two different environments, with ten application instances in each. One environment is experimental and gets more errors than the other. But the instances that are in the same environment should get approximately the same number of errors. You can set a notification for when an instance starts getting more errors than the other instances in the same environment. Also, you can set a notification for when the two environments start to have the same number of errors as each other. Notify for timezone-related changes The number of logged in users for a company is about the same for each of four applications, but varies significantly by each of the three time zones the company operates in. You can set a notification for when any application starts getting more or less traffic from a certain timezone than the other applications. Sometimes the traffic from the different time zones are the same, so you would set up the alert condition to not be notified if the time zone groups overlap. For more details on how this feature works, see Outlier rules and logic. Create an outlier alert condition To create a NRQL alert that uses outlier detection: When creating a condition, under Select a product, select NRQL. For Threshold type, select Outlier. Create a NRQL query with a FACET clause that returns the values you want to alert on. Depending on how the returned values group together, set the Number of expected groups. Adjust the deviation from the center of the group(s) and the duration that will trigger a violation. Optional: Add a warning threshold and set its deviation. Set any remaining available options and save. Rules and logic Here are the rules and logic behind how outlier detection works: Details about alert condition logic After the condition is created, the query is run once every harvest cycle and the condition is applied. Unlike baseline alerts, outlier detection uses no historical data in its calculation; it's calculated using the currently collected data. Alerts will attempt to divide the data returned from the query into the number of groups selected during condition creation. For each group, the approximate average value is calculated. The allowable deviation you have chosen when creating the condition is centered around that average value. If a member of the group is outside the allowed deviation, it produces a violation. If Trigger when groups overlap has been selected, Alerts detects a convergence of groups. If the condition is looking for two or more groups, and the returned values cannot be separated into that number of distinct groups, then that will produce a violation. This type of “overlap” event is represented on a chart by group bands touching. Because this feature does not take past behavior into account, data is never considered to \"belong\" to a certain group. For example, a value that switches places with another value wouldn't trigger a violation. Additionally, an entire group that moves together also wouldn't trigger a violation. NRQL query rules and limits The NRQL query must be a faceted query. The number of unique values returned must be 500 or less. If the query returns more than this number of values, the condition won't be created. If the query later returns more than this number after being created, the alert will fail. Zero values for unreturned data When a query returns a set of values, only values that are actually returned are taken into account. If a value is not available for calculation (including if it goes from being collected one harvest cycle to not being collected), it is rendered as a zero and is not considered. In other words, the behavior of unreturned zero values will never trigger violations.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.94475,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Outlier detection (NRQL <em>alert</em>)",
        "sections": "Create an outlier <em>alert</em> <em>condition</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "<em>Alerts</em> offers NRQL <em>conditions</em> in three threshold types: static, baseline, and outlier. This document explains how the outlier threshold type works, gives some example use cases and NRQL queries, and explains how to create an outlier condition. Important NRQL <em>alerts</em> do not affect <em>Alerts</em> policies"
      },
      "id": "6130be72196a6793654948e7"
    }
  ],
  "/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/customize-your-webhook-payload": [
    {
      "sections": [
        "Workflows",
        "What is an issue",
        "Add a Workflow",
        "Tip",
        "Enrichments",
        "Important",
        "Notifier"
      ],
      "title": "Workflows",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Workflows",
        "Notifications"
      ],
      "external_id": "0993c323f76467d22136cac6982ccd48e859f722",
      "image": "",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/applied-intelligence/incident-workflows/incident-workflows/",
      "published_at": "2021-12-19T13:46:37Z",
      "updated_at": "2021-12-17T01:48:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With workflows you control when and where you want to receive notifications about issues, tunnel the right information to the relevant person or team, and enrich your issue's notifications with additional New Relic data. What is an issue Issues are groups of incidents that describe the underlying problem of your symptoms. When a new incident is created, Incident Intelligence opens an issue and evaluates other open issues for correlations. For more information see Use Incident Inteligence. Add a Workflow Tip The maximum number of workflows you can add is 100 per environment and 50 per account. The workflows feature is located under the Alerts & AI menu. Go to one.newrelic.com, click Alerts & AI, in the left navigation under Enrich and Notify click Workflow, then click Add a workflow. Name your workflow. This field is mandatory and needs to be unique. Workflows filters the issues you want to send to the relevant destination. You can send all issues by selecting Send all issues or you can build a query to filter the right issues. Build a query by selecting an attribute, operator and value/s to narrow down the right issues you want to be passed on. Optional: Enriched Data, available to full platform users. Enrichments Enrichments can give additional context on alert notifications by adding NRQL query results to them The workflows enrich tool is similar to the query builder. The NRQL query's results are included in the notification message. If you create queries for your workflows, they run in real time and the query results are the value of the enricher. You can save any valid query into your workflow, even if they return an empty result, but you must run the query before saving the enrichment. You can also query with issue-specific variables; for example {{entitiesData.ids}} For enrichment examples, see Workflow data enrichment examples. Note: The query name needs to be unique because you'll use the name to decide how to include the result in the payload of the notification that will be sent by the workflow. Important At this stage, visual representation of enrichments are sent to Slack and email. For Servicenow and JIRA destinations, we recommend to limit to NRDB queries with single value results. For example: count, min, or max. The maximum number of enrichments per workflow is 5. The maximum number of variables an enrichment can contain is 1. Notify: Choose one or more destinations and add an optional message. Notifier To save and activate a Workflow, you need to include a notifier. Setting up a notifier requires having a notification destination configured. You can either set it up through the Destination menu item, or through the Create new destination item in the destination selection drop down in each destination channel screen. A notification destination includes the credentials required to send a notification to a third party platform and an option to customize the message sent by mapping fields from our issues to fields in the destination platform. To learn how to set up destinations and configure message templates, check the documentation on notifications. Click update message once completing the notifier requirements Optionally, test your workflow. We'll use existing data from your account to test what you've configured and send a sample notification Click activate to complete the workflow. Tip In any destination channel, start typing and a variable menu will open up. You'll see the names of the variables, which will be replaced with the variable's values at runtime. To use the enrichers' results, use their name.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 193.5292,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Notifier</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "With workflows you control when and where you want to receive <em>notifications</em> about issues, tunnel the right information to the relevant person or team, and enrich your issue&#x27;s <em>notifications</em> with additional <em>New</em> <em>Relic</em> data. What is an issue Issues are groups of incidents that describe the underlying"
      },
      "id": "603e967664441f7e6f4e889b"
    },
    {
      "sections": [
        "Streaming alerts: key terms and concepts",
        "Why it matters",
        "Streaming alerts process and descriptions",
        "Choose your aggregation method",
        "Event flow",
        "Caution",
        "Event timer",
        "Cadence",
        "Streaming alerts tools",
        "Tip",
        "Window duration",
        "Delay/timer",
        "Loss of signal detection",
        "Gap filling"
      ],
      "title": "Streaming alerts: key terms and concepts",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Get started"
      ],
      "external_id": "144c7f0721ec2acb1e8ac8aa06142911ea552aef",
      "image": "https://docs.newrelic.com/static/39158bd84483adf9516bb7ff5058c9bd/d30ee/streaming-alerts.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/streaming-alerts-key-terms-concepts/",
      "published_at": "2021-12-15T12:47:58Z",
      "updated_at": "2021-12-15T12:47:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The streaming alerts platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into New Relic. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that's processed by the streaming algorithm. There are three methods for aggregating the data filtered through your NRQL condition: Event flow (default) Event timer Cadence This short video describes the three aggregation method (5:31). Why it matters Understanding how streaming alerts works will help you fine-tune your NRQL conditions to be notified about what's important to you. Only data that matches the conditions of the NRQL WHERE clause is alerted on. For more details on each step of the process, see Streaming alerts process and descriptions. As data streams into New Relic, it's filtered by the NRQL condition. Before data is evaluated, it must meet the criteria defined by the NRQL query's WHERE clause. Instead of evaluating that data immediately for violations, the NRQL alert conditions collect the data over a period of time known as the aggregation window. An additional delay/timer allows for slower data points to arrive before the window is aggregated. Once the delay/timer time has elapsed, New Relic aggregates the data into a single data point. Alerts then evaluates the data point using the NRQL condition in order to determine whether it meets the violation threshold criteria. Even if a data point meets the criteria for a violation, a violation may not be triggered. A violation is only triggered when data points consistently meet the threshold criteria over a period of time. This is the threshold duration. If the data points are in violation for an entire threshold duration, we'll send you a notification based on your policy settings. All of these configurable delays give you more control over how you're alerted on sporadic and missing data. Streaming alerts process and descriptions Process Description Streaming data All data coming into New Relic. WHERE clause Filters all incoming streaming data. We only monitor for alerts on data that makes it through this filter. Aggregation methods One of three methods that control how data is collected before it's evaluated. They are: Event flow (Default) Event timer Cadence Aggregation window Data with timestamps that fall within this window will be aggregated and then evaluated. Delay/timer A time delay to ensure all data points have arrived in the aggregation window before aggregation occurs. Aggregated data Data in the aggregated window is collapsed to a single data point for alert evaluation. Evaluation The data point is evaluated by the NRQL condition, which is trigged by each incoming aggregated data point. Threshold duration A specific duration that determines if a violation is created. If your specified NRQL condition meets the threshold criteria over the threshold duration, a violation occurs. When a data point lacks data, a custom value is inserted to fill the gap. Choose your aggregation method You can choose between three different aggregation methods, depending on your needs. Event flow (default) works best for data that comes in frequently and mostly in order. Event timer works best for data that arrives infrequently in batches, such as cloud integration data or infrequent error logs. Cadence is our original and inferior aggregation method. It aggregates data on specific time intervals as detected by New Relic's internal wall clock, regardless of data timestamps. Event flow Event flow aggregates a window of data when the first data point arrives in a subsequent window. The custom delay defines which subsequent window data will start to populate to trigger aggregation of the current window. A custom delay provides extra time for data to arrive. These times are based on the data's timestamps and not New Relic's wall clock time. For example, suppose you're monitoring CPU usage in window durations of 1 minute and a 3 minute delay. When a CPU usage data point comes in with a timestamp between 12:00pm and 12:01pm, event flow will not aggregate that window until a data point shows up with a timestamp between 12:04pm and 12:05pm. When event flow receives the first data point with a timestamp of 12:04pm or later, it sends the 12:00 to 12:01 data to be aggregated. Caution If you expect your data points to arrive more than 30 minutes apart, please use the Event Timer method described below. Event timer Like event flow, event timer will only aggregate data for a given window when data arrives for that window. When a data point arrives for an aggregation window, a timer dedicated to that window starts to count down. If no further data arrives before the timer counts down, the data for that window is aggregated. If more data points arrive before the timer has completed counting down, the timer is reset. For example, suppose you're monitoring CloudWatch data that arrives fairly infrequently. You're using a window duration of 1 minute and a 3 minute timer. When a CloudWatch data point comes in with a timestamp between 12:00pm and 12:01pm, the timer will start to count down. If no further data points show up for that 12:00-12:01 window, the window will get aggregated 3 minutes later. If a new data point with a timestamp between 12:00 and 12:01 arrives, the timer resets. It keeps resetting every time more data points for that window arrive. The window will not be sent for aggregation until the timer reaches 0. If the timer for a later data point elapses before an earlier data point, the event timer method waits for the earlier timer to elapse before aggregating the later data point. For best results, make sure your timer is equal to or longer than your window duration time. If the timer is less than your window duration and your data flow is inconsistent, then your data may be evaluated before all of your data points arrive. This could cause you to be notified incorrectly. Cadence We recommend you use one of the other two methods. Cadence is our old streaming aggregation method. This method uses New Relic's wall clock time to determine when data is aggregated and evaluated. It doesn't take into account data point timestamps as they arrive. Streaming alerts tools Streaming alerts provide a set of tools to give you greater control over how your data is aggregated before it's evaluated to reduce incorrect notifications you receive. They are: Window duration Delay/timer Loss of signal detection Gap filling Tip This article covers these tools at a conceptual level. You'll find direct instructions on how to use these tools in Create NRQL alert conditions. Window duration In order to make loss of signal detection more effective and to reduce unnecessary notifications, you can customize aggregation windows to the duration that you need. An aggregation window is a specific block of time. We gather data points together in an aggregation window, before evaluating the data. A longer aggregation window can smooth out the data, since an outlier data point will have more data points to be aggregated with, giving it less of an influence on the aggregated data point that is sent for evaluation. When a data point arrives, its timestamp is used to put it in the proper aggregation window. You can set your aggregation window to anything between 30 seconds and 15 minutes. The default is 1 minute. Delay/timer The delay/timer setting controls how long the condition should wait before aggregating the data in the aggregation window. The event flow and cadence methods use delay. Event timer uses timer. The delay default is 2 minutes. The timer default is 1 minute and has a minimum value of 5 seconds. Loss of signal detection Loss of signal occurs when no data matches the NRQL condition over a specific period of time. A loss of signal is caused by different things. The WHERE clause in your NRQL query can filter out data before it's evaluated for violations. It could also mean a service or entity is offline or a periodic job has failed to run and no data is being sent to New Relic. In order to avoid unnecessary notifications, you can choose how long to wait before you're notified by a loss of signal violation. You can use loss of signal detection to open violations and be notified when a signal is lost. Alternately, you can use a loss of signal to close violations for ephemeral services or sporadic data, such as error counts. Gap filling Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with the last value received, a static value, or else do nothing and leave the gap there. The default is None. Gaps in streaming data can be caused by network or host issues, a signal may be sparse, or some signals, such as error counts, may only have data when something is wrong. By filling the gaps with known values, the alert evaluation process can process those gaps and determine how they should affect the loss of signal evaluation. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss and gap filling, see this Explorers Hub post.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 177.9,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "sections": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "The streaming <em>alerts</em> platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into <em>New</em> <em>Relic</em>. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that&#x27;s processed"
      },
      "id": "6130c054e7b9d269edb6f21d"
    },
    {
      "sections": [
        "Create NRQL alert conditions",
        "Tip",
        "Create a NRQL alert condition",
        "Create a condition from a chart",
        "Caution",
        "NRQL alert syntax",
        "Important",
        "Reformatting incompatible NRQL",
        "NRQL alert threshold examples",
        "Alert on specific segments of your data",
        "Alert on Nth percentile of your data",
        "Alert on max, min, avg of your data",
        "Alert on a percentage of your data",
        "Alert on Apdex with any T-value",
        "NRQL conditions and query order of operations",
        "Example: null value returned",
        "Example: zero value returned",
        "Nested aggregation NRQL alerts",
        "Nested queries with a non-faceted innermost query are not currently supported",
        "Queries at all levels must have the same aggregation window size",
        "Signal loss is not yet supported for nested queries",
        "Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported",
        "NRQL condition creation tips",
        "Condition edits can reset condition evaluation",
        "Alert condition types",
        "Sum of query results (limited or intermittent data)",
        "Set the loss of signal threshold",
        "Advanced signal settings",
        "Aggregation window duration",
        "Delay/timer",
        "Fill data gaps"
      ],
      "title": "Create NRQL alert conditions",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert conditions"
      ],
      "external_id": "2872f770a5fc8a802f99b9f848906f7e351ad761",
      "image": "https://docs.newrelic.com/static/eb8e1b2d826f9fd9cf46fe3dd5455217/c1b63/nr1_nrql_alert_conditions.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/",
      "published_at": "2021-12-19T16:01:17Z",
      "updated_at": "2021-12-14T22:49:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use NRQL queries to create alert conditions. Once you've defined your signal, you can further define your warning and critical threshold levels. This determines when an alerts violation is created. Read on to learn more about how to do this. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Click NRQL, and then Next, define thresholds. Tip For more information on key concepts relating to NRQL alert conditions and streaming alerts, see Streaming alerts: key terms and concepts. Create a NRQL alert condition To create a NRQL alert condition for a policy: On one.newrelic.com, in the header click Alerts & AI, then in the left sidebar click Policies. Select an existing policy or click New alert policy to create a new policy. Click Add a condition. Under Select a product click NRQL, and then click Next, define thresholds. Note that editing an existing condition can result in resetting its evaluation. Create a condition from a chart You can use a chart to create a NRQL alert condition. To create a NRQL alerts condition from a chart, click the chart menu , then click Create alert condition. Once you've named and customized your condition, you can add it to an existing policy or create a new one. Caution A small number of our older charts don't include the option to create an alert condition. NRQL alert syntax Here's the basic syntax for creating all NRQL alert conditions. The FACET clause is required for outlier condition types. It's optional for static and baseline. SELECT function(attribute) FROM Event WHERE attribute [comparison] [AND|OR ...] Copy Clause Notes SELECT function(attribute) Required Supported functions that return numbers include: apdex average count latest max min percentage percentile sum uniqueCount Tip If you use the percentile aggregator in a faceted alert condition with many facets, this may cause the following error to appear: An error occurred while fetching chart data. If you see this error, use average instead. FROM data type Required Only one data type can be targeted. Supported data types: Event Metric (RAW data points will be returned) WHERE attribute [comparison] [AND|OR ...] Use the WHERE clause to specify a series of one or more conditions. All the operators are supported. FACET attribute Required for outlier conditions Include an optional FACET clause in your NRQL syntax depending on the threshold type: static or baseline. Use the FACET clause to separate your results by attribute and alert on each attribute independently. No LIMIT clause is allowed, but all queries will receive the maximum number of facets possible. Faceted queries can return a maximum of 5000 values for static and baseline conditions and a maximum of 500 values for outlier conditions. Important If the query returns more than the maximum number of values, the alert condition can't be created. If you create the condition and the query returns more than this number later, the alert will fail. Modify your query so that it returns a fewer number of values. Reformatting incompatible NRQL Some elements of NRQL used in charts don’t make sense in the streaming context of alerts. Here’s a list of the most common incompatible elements and suggestions for reformatting a NRQL alert query to achieve the same effect. Element Notes SINCE and UNTIL Example: SELECT percentile(largestContentfulPaint, 75) FROM PageViewTiming WHERE (appId = 837807) SINCE yesterday Copy NRQL conditions produce a never-ending stream of windowed query results, so the SINCE and UNTIL keywords to scope the query to a point in time are not compatible. As a convenience, we automatically strip SINCE and UNTIL from a query when creating a condition from the context of a chart. TIMESERIES In NRQL queries, the TIMESERIES clause is used to return data as a time series broken out by a specified period of time. For NRQL conditions, the equivalent property of a signal is the aggregation duration window. histogram() The histogram() aggregation function is used to generate histograms. histogram() is not compatible with NRQL alerting: histogram aggregations can not be formatted as a time series. To create an alert from a portion of a histogram (for example, 95th percentile), use the percentile() aggregation function. Multiple aggregation functions Each condition can only target a single aggregated value. To alert on multiple values simultaneously, you’ll need to decompose them into individual conditions within the same policy. Original Query: SELECT count(foo), average(bar), max(baz) from Transaction Copy Decomposed: SELECT count(foo) from Transaction SELECT average(bar) from Transaction SELECT max(baz) from Transaction Copy COMPARE WITH The COMPARE WITH clause is used to compare the values for two different time ranges. This type of query is incompatible with NRQL alerting. We recommend using a Baseline Alert Condition to dynamically detect deviations for a particular signal. SLIDE BY The SLIDE BY clause supports a feature known as sliding windows. With sliding windows, SLIDE BY data is gathered into \"windows\" of time that overlap with each other. These windows can help to smooth out line graphs with a lot of variation in cases where the rolling aggregate (such as a rolling mean) is more important than aggregates from narrow windows of time. Sliding windows are not currently supported in NRQL alerts. LIMIT In NRQL queries, the LIMIT clause is used to control the amount of data a query returns, either the maximum number of facet values returned by FACET queries or the maximum number of items returned by SELECT * queries. LIMIT is not compatible with NRQL alerting: evaluation is always performed on the full result set. NRQL alert threshold examples Here are some common use cases for NRQL conditions. These queries will work for static and baseline condition types. The outlier condition type will require additional FACET clauses. Alert on specific segments of your data Create constrained alerts that target a specific segment of your data, such as a few key customers or a range of data. Use the WHERE clause to define those conditions. SELECT average(duration) FROM Transaction WHERE account_id in (91290, 102021, 20230) Copy SELECT percentile(duration, 95) FROM Transaction WHERE name LIKE 'Controller/checkout/%' Copy Alert on Nth percentile of your data Create alerts when an Nth percentile of your data hits a specified threshold; for example, maintaining SLA service levels. Since we evaluate the NRQL query based on the aggregation window duration, percentiles will be calculated for each duration separately. SELECT percentile(duration, 95) FROM Transaction Copy SELECT percentile(databaseDuration, 75) FROM Transaction Copy Alert on max, min, avg of your data Create alerts when your data hits a certain maximum, minimum, or average; for example, ensuring that a duration or response time does not pass a certain threshold. SELECT max(duration) FROM Transaction Copy SELECT average(duration) FROM Transaction Copy Alert on a percentage of your data Create alerts when a proportion of your data goes above or below a certain threshold. SELECT percentage(count(*), WHERE duration > 2) FROM Transaction Copy SELECT percentage(count(*), WHERE httpResponseCode = '500') FROM Transaction Copy Alert on Apdex with any T-value Create alerts on Apdex, applying your own T-value for certain transactions. For example, get an alert notification when your Apdex for a T-value of 500ms on transactions for production apps goes below 0.8. SELECT apdex(duration, t:0.5) FROM Transaction WHERE appName like '%prod%' Copy NRQL conditions and query order of operations By default, the aggregation window duration is 1 minute, but you can change the window to suit your needs. Whatever the aggregation window, New Relic will collect data for that window using the function in the NRQL condition’s query. The query is parsed and executed by our systems in the following order: FROM clause – which event type needs to be grabbed? WHERE clause – what can be filtered out? SELECT clause – what information needs to be returned from the now-filtered data set? Example: null value returned Let's say this is your alert condition query: SELECT count(*) FROM SyntheticCheck WHERE monitorName = 'My Cool Monitor' AND result = 'FAILURE' Copy If there are no failures for the aggregation window: The system will execute the FROM clause by grabbing all SyntheticCheck events on your account. Then it will execute the WHERE clause to filter through those events by looking only for the ones that match the monitor name and result specified. If there are still events left to scan through after completing the FROM and WHERE operations, the SELECT clause will be executed. If there are no remaining events, the SELECT clause will not be executed. This means that aggregators like count() and uniqueCount() will never return a zero value. When there is a count of 0, the SELECT clause is ignored and no data is returned, resulting in a value of NULL. Example: zero value returned If you have a data source delivering legitimate numeric zeroes, the query will return zero values and not null values. Let's say this is your alert condition query, and that MyCoolEvent is an attribute that can sometimes return a zero value. SELECT average(MyCoolAttribute) FROM MyCoolEvent Copy If, in the aggregation window being evaluated, there's at least one instance of MyCoolEvent and if the average value of all MyCoolAttribute attributes from that window is equal to zero, then a 0 value will be returned. If there are no MyCoolEvent events during that minute, then a NULL will be returned due to the order of operations. Tip For more information about this topic, you can check out our blog post on troubleshooting for zero versus null values. Tip You can determine how null values will be handled by adjusting loss of signal and gap filling settings in the Alert Conditions UI. Tip You can avoid NULL values entirely with a query order of operations shortcut. Do this by using a filter sub-clause, then including all filter elements within that sub-clause. The main body of the query will run and return data, at which point the SELECT clause will then run and apply the filter elements. The query will return a value of 0 if the filter elements result in no matching data. Here's an example: SELECT filter(count(*), WHERE result = 'SUCCESS' AND monitorName = 'My Favorite Monitor') FROM SyntheticCheck Copy Nested aggregation NRQL alerts Nested aggregation queries are a powerful way to query your data. However, they have a few restrictions that are important to note. Nested queries with a non-faceted innermost query are not currently supported Without a FACET, the inner query produces a single result, giving the outer query nothing to aggregate. If you're using a nested query, make sure your inner query is faceted. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu) ​​​​​ Copy Queries at all levels must have the same aggregation window size With an alert aggregation window of 1 minute, the inner query would produce two smaller windows of 30 seconds. In theory, these two windows could be aggregated by the outer query. However, this is not currently supported. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu TIMESERIES 30 seconds)​​ Copy Signal loss is not yet supported for nested queries For more information on signal loss, see NerdGraph API: Loss of signal and gap filling. Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported You can't use a nested query containing the WITH METRIC_FORMAT in the inner query to create NRQL alert conditions. NRQL condition creation tips Here are some tips for creating and using a NRQL condition: Topic Tips Condition types NRQL condition types include static, baseline, and outlier. Create a description For NRQL conditions, you can create a custom description to add to each violation. Descriptions can be enhanced with variable substitution based on metadata in the specific violation. For details, see Description Query results Queries must return a number. The condition evaluates the returned number against the thresholds you've set. Time period NRQL conditions evaluate data based on how it's aggregated, using aggregation windows from 30 seconds to 15 minutes, in increments of 15 seconds. For best results, we recommend using the event flow or event timer aggregation methods. For the cadence aggregation method, the implicit SINCE ... UNTIL clause specifying which minute to evaluate is controlled by your delay/timer setting. Since very recent data may be incomplete, you may want to query data from 3 minutes ago or longer, especially for: Applications that run on multiple hosts. SyntheticCheck data: Timeouts can take 3 minutes, so 5 minutes or more is recommended. Also, if a query will generate intermittent data, consider using the sum of query results option. Lost signal threshold (loss of signal detection) You can use loss of signal detection to alert on when your data (a telemetry signal) should be considered lost. A signal loss can indicate that a service or entity is no longer online or that a periodic job failed to run. You can also use this to make sure that violations for sporadic data, such as error counts, are closed when no signal is coming in. Advanced signal settings These settings give you options for better handling continuous, streaming data signals that may sometimes be missing. These settings include the aggregation window duration, the delay/timer, and an option for filling data gaps. For more on using these, see Advanced signal settings. Condition settings Use the Condition settings to: Create a concise, descriptive condition name. Provide a custom violation description for the condition that will be included in violations and notifications. Add the runbook URL to include your organization's procedures for handling incidents. You may also add this information to the custom violation description. Limits on conditions See the maximum values. Health status In order for a NRQL alert condition health status display to function properly, use a FACET clause to scope each signal to a single entity (for example, FACET hostname or FACET appname). Examples For more information, see: Expected NRQL syntax Examples of NRQL condition queries Condition edits can reset condition evaluation When you edit NRQL alert conditions in some specific ways (detailed below), their evaluations are reset, meaning that any evaluation up until that point is lost, and the evaluation starts over from that point. The two ways this will affect you are: For \"for at least x minutes\" thresholds: because the evaluation window has been reset, there will be a delay of at least x minutes before any violations can be reported. For baseline conditions: the condition starts over again and all baseline learning is lost. The following actions cause an evaluation reset for NRQL conditions: Changing the query Changing the aggregation window, aggregation method, or aggregation delay/timer setting Changing the \"close violations on signal loss\" setting Changing any gap fill settings Changing the baseline direction (if applicable) – higher, lower, or higher/lower Change the threshold value, threshold window, or threshold operator The following actions (along with any other actions not covered in the above list) will not reset the evaluation: Changing the loss of signal time window (expiration duration) Changing the time function (switching \"for at least\" to \"at least once in,\" or vice-versa) Toggling the \"open violation on signal loss\" setting Alert condition types When you create a NRQL alert, you can choose from different types of conditions: NRQL alert condition types Description Static This is the simplest type of NRQL condition. It allows you to create a condition based on a NRQL query that returns a numeric value. Optional: Include a FACET clause. Baseline (Dynamic) Uses a self-adjusting condition based on the past behavior of the monitored values. Uses the same NRQL query form as the static type, including the optional FACET clause. Outlier Looks for group behavior and values that are outliers from those groups. Uses the same NRQL query form as the static type, but requires a FACET clause. Sum of query results (limited or intermittent data) Important Available only for static (basic) condition types. If a query returns intermittent or limited data, it may be difficult to set a meaningful threshold. Missing or limited data will sometimes generate false positives or false negatives. You can use loss of signal, aggregation duration, and gap filling settings to minimize these false notifications. To avoid this problem when using the static threshold type, you can set the selector to sum of query results. This lets you set the alert on an aggregated sum instead of a value from a single harvest cycle. Up to two hours of one-minute data checks can be aggregated. The duration you select determines the width of the rolling sum and the preview chart will update accordingly. Set the loss of signal threshold Loss of signal occurs when no data matches the NRQL condition over a specific period of time. You can set your loss of signal threshold duration and also what happens when the threshold is crossed. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Loss of signal is only available for NRQL conditions. You may also manage these settings using the GraphQL API (recommended), or the REST API. Go here for specific GraphQL API examples. Loss of signal settings: Loss of signal settings include a time duration and two possible actions. Signal loss expiration time UI label: Signal is lost after: GraphQL Node: expiration.expirationDuration Expiration duration is a timer that starts and resets when we receive a data point in the streaming alerts pipeline. If we don't receive another data point before your 'expiration time' expires, we consider that signal to be lost. This can be because no data is being sent to New Relic or the WHERE clause of your NRQL query is filtering that data out before it is streamed to the alerts pipeline. Note that when you have a faceted query, each facet is a signal. So if any one of those signals ends during the duration specified, that will be considered a loss of signal. The loss of signal expiration time is independent of the threshold duration and triggers as soon as the timer expires. The maximum expiration duration is 48 hours. This is helpful when monitoring for the execution of infrequent jobs. The minimum is 30 seconds, but we recommend using at least 3-5 minutes. Loss of signal actions Once a signal is considered lost, you can close open violations, open new violations, or both. Close all current open violations: This closes all open violations that are related to a specific signal. It won't necessarily close all violations for a condition. If you're alerting on an ephemeral service, or on a sporadic signal, you'll want to choose this action to ensure that violations are closed properly. The GraphQL node name for this is \"closeViolationsOnExpiration\" Open new violations: This will open a new violation when the signal is considered lost. These violations will indicate that they are due to a loss of signal. Based on your incident preferences, this should trigger a notification. The graphQL node name for this is \"openViolationOnExpiration\" When you enable both actions, we'll close all open violations first, and then open a new violation for loss of signal. To create a NRQL alert configured with loss of signal detection in the UI: For a policy, when you create a condition, under Select a product, click NRQL, then click Next, define thresholds. Write a NRQL query that returns the values you want to alert on. For Threshold type, select Static or Baseline. Click + Add lost signal threshold, then set the signal expiration duration time in minutes or seconds in the Signal is lost after field. Choose what you want to happen when the signal is lost. You can check one or both of Close all current open violations and Open new \"lost signal\" violation. These control how loss of signal violations will be handled for the condition. Make sure you name your condition before you save it. Violations open due to loss of signal close when the signal comes back. Newly opened lost signal violations will close immediately when new data is evaluated. the condition they belong to expires. By default, conditions expire after 3 days. you manually close the violation with the Close all current open violations option. Tip Loss of signal detection doesn't work on NRQL queries that use nested aggregation or sub-queries. Advanced signal settings When creating a NRQL alert condition, use the advanced signal settings to control streaming alert data and avoid false alarms. When creating a NRQL condition, there are several advanced signal settings: Aggregation window duration Delay/timer Fill data gaps To read an explanation of what these settings are and how they relate to each other, see Streaming alerts concepts. Below are instructions and tips on how to configure them. Aggregation window duration You can set the aggregation window duration to choose how long data is accumulated in a streaming time window before it's aggregated. You can set it to anything between 30 seconds and 15 minutes. The default is one minute. Tip Baseline alert condition thresholds don't support editing the aggregation window. They use the 1 minute default. Delay/timer You can adjust the delay/timer to coordinate our streaming alerting algorithm with your data's behavior. If your data is sparse or inconsistent, you may want to use the event timer aggregation method. For the cadence method, the total supported latency is the sum of the aggregation window duration and the delay. If the data type comes from an APM language agent and is aggregated from many app instances (for example, Transactions, TransactionErrors, etc.), we recommend using the event flow method with the default settings. Important When creating NRQL conditions for data collected from Infrastructure Cloud Integrations such as AWS Cloudwatch or Azure, we recommend that you use the event timer method. Fill data gaps Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with one of these settings: None: (Default) Choose this if you don't want to take any action on empty aggregation windows. On evaluation, an empty aggregation window will reset the threshold duration timer. For example, if a condition says that all aggregation windows must have data points above the threshold for 5 minutes, and 1 of the 5 aggregation windows is empty, then the condition won't be in violation. Custom static value: Choose this if you'd like to insert a custom static value into the empty aggregation windows before they're evaluated. This option has an additional, required parameter of fillValue (as named in the API) that specifies what static value should be used. This defaults to 0. Last known value: This option inserts the last seen value before evaluation occurs. We maintain the state of the last seen value for 2 hours. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss, gap filling, and how to request access to these features, see this Explorers Hub post. Options for editing data gap settings: In the NRQL conditions UI, go to Condition settings > Advanced signal settings > fill data gaps with and choose an option. If using our Nerdgraph API (preferred), this node is located at: actor : account : alerts : nrqlCondition : signal : fillOption | fillValue NerdGraph is our recommended API for this but if you're using our REST API, you can find this setting in the REST API explorer under the \"signal\" section of the Alert NRQL conditions API.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 173.86227,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create NRQL <em>alert</em> conditions",
        "sections": "Nested aggregation NRQL <em>alerts</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": " a NRQL <em>alert</em> condition for a policy: On one.newrelic.com, in the header click <em>Alerts</em> &amp; AI, then in the left sidebar click Policies. Select an existing policy or click <em>New</em> <em>alert</em> policy to create a <em>new</em> policy. Click Add a condition. Under Select a product click NRQL, and then click Next, define"
      },
      "id": "603ef04864441fbc114e8883"
    }
  ],
  "/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/define-custom-metrics-alert-condition": [
    {
      "sections": [
        "Create NRQL alert conditions",
        "Tip",
        "Create a NRQL alert condition",
        "Create a condition from a chart",
        "Caution",
        "NRQL alert syntax",
        "Important",
        "Reformatting incompatible NRQL",
        "NRQL alert threshold examples",
        "Alert on specific segments of your data",
        "Alert on Nth percentile of your data",
        "Alert on max, min, avg of your data",
        "Alert on a percentage of your data",
        "Alert on Apdex with any T-value",
        "NRQL conditions and query order of operations",
        "Example: null value returned",
        "Example: zero value returned",
        "Nested aggregation NRQL alerts",
        "Nested queries with a non-faceted innermost query are not currently supported",
        "Queries at all levels must have the same aggregation window size",
        "Signal loss is not yet supported for nested queries",
        "Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported",
        "NRQL condition creation tips",
        "Condition edits can reset condition evaluation",
        "Alert condition types",
        "Sum of query results (limited or intermittent data)",
        "Set the loss of signal threshold",
        "Advanced signal settings",
        "Aggregation window duration",
        "Delay/timer",
        "Fill data gaps"
      ],
      "title": "Create NRQL alert conditions",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert conditions"
      ],
      "external_id": "2872f770a5fc8a802f99b9f848906f7e351ad761",
      "image": "https://docs.newrelic.com/static/eb8e1b2d826f9fd9cf46fe3dd5455217/c1b63/nr1_nrql_alert_conditions.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/",
      "published_at": "2021-12-19T16:01:17Z",
      "updated_at": "2021-12-14T22:49:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use NRQL queries to create alert conditions. Once you've defined your signal, you can further define your warning and critical threshold levels. This determines when an alerts violation is created. Read on to learn more about how to do this. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Click NRQL, and then Next, define thresholds. Tip For more information on key concepts relating to NRQL alert conditions and streaming alerts, see Streaming alerts: key terms and concepts. Create a NRQL alert condition To create a NRQL alert condition for a policy: On one.newrelic.com, in the header click Alerts & AI, then in the left sidebar click Policies. Select an existing policy or click New alert policy to create a new policy. Click Add a condition. Under Select a product click NRQL, and then click Next, define thresholds. Note that editing an existing condition can result in resetting its evaluation. Create a condition from a chart You can use a chart to create a NRQL alert condition. To create a NRQL alerts condition from a chart, click the chart menu , then click Create alert condition. Once you've named and customized your condition, you can add it to an existing policy or create a new one. Caution A small number of our older charts don't include the option to create an alert condition. NRQL alert syntax Here's the basic syntax for creating all NRQL alert conditions. The FACET clause is required for outlier condition types. It's optional for static and baseline. SELECT function(attribute) FROM Event WHERE attribute [comparison] [AND|OR ...] Copy Clause Notes SELECT function(attribute) Required Supported functions that return numbers include: apdex average count latest max min percentage percentile sum uniqueCount Tip If you use the percentile aggregator in a faceted alert condition with many facets, this may cause the following error to appear: An error occurred while fetching chart data. If you see this error, use average instead. FROM data type Required Only one data type can be targeted. Supported data types: Event Metric (RAW data points will be returned) WHERE attribute [comparison] [AND|OR ...] Use the WHERE clause to specify a series of one or more conditions. All the operators are supported. FACET attribute Required for outlier conditions Include an optional FACET clause in your NRQL syntax depending on the threshold type: static or baseline. Use the FACET clause to separate your results by attribute and alert on each attribute independently. No LIMIT clause is allowed, but all queries will receive the maximum number of facets possible. Faceted queries can return a maximum of 5000 values for static and baseline conditions and a maximum of 500 values for outlier conditions. Important If the query returns more than the maximum number of values, the alert condition can't be created. If you create the condition and the query returns more than this number later, the alert will fail. Modify your query so that it returns a fewer number of values. Reformatting incompatible NRQL Some elements of NRQL used in charts don’t make sense in the streaming context of alerts. Here’s a list of the most common incompatible elements and suggestions for reformatting a NRQL alert query to achieve the same effect. Element Notes SINCE and UNTIL Example: SELECT percentile(largestContentfulPaint, 75) FROM PageViewTiming WHERE (appId = 837807) SINCE yesterday Copy NRQL conditions produce a never-ending stream of windowed query results, so the SINCE and UNTIL keywords to scope the query to a point in time are not compatible. As a convenience, we automatically strip SINCE and UNTIL from a query when creating a condition from the context of a chart. TIMESERIES In NRQL queries, the TIMESERIES clause is used to return data as a time series broken out by a specified period of time. For NRQL conditions, the equivalent property of a signal is the aggregation duration window. histogram() The histogram() aggregation function is used to generate histograms. histogram() is not compatible with NRQL alerting: histogram aggregations can not be formatted as a time series. To create an alert from a portion of a histogram (for example, 95th percentile), use the percentile() aggregation function. Multiple aggregation functions Each condition can only target a single aggregated value. To alert on multiple values simultaneously, you’ll need to decompose them into individual conditions within the same policy. Original Query: SELECT count(foo), average(bar), max(baz) from Transaction Copy Decomposed: SELECT count(foo) from Transaction SELECT average(bar) from Transaction SELECT max(baz) from Transaction Copy COMPARE WITH The COMPARE WITH clause is used to compare the values for two different time ranges. This type of query is incompatible with NRQL alerting. We recommend using a Baseline Alert Condition to dynamically detect deviations for a particular signal. SLIDE BY The SLIDE BY clause supports a feature known as sliding windows. With sliding windows, SLIDE BY data is gathered into \"windows\" of time that overlap with each other. These windows can help to smooth out line graphs with a lot of variation in cases where the rolling aggregate (such as a rolling mean) is more important than aggregates from narrow windows of time. Sliding windows are not currently supported in NRQL alerts. LIMIT In NRQL queries, the LIMIT clause is used to control the amount of data a query returns, either the maximum number of facet values returned by FACET queries or the maximum number of items returned by SELECT * queries. LIMIT is not compatible with NRQL alerting: evaluation is always performed on the full result set. NRQL alert threshold examples Here are some common use cases for NRQL conditions. These queries will work for static and baseline condition types. The outlier condition type will require additional FACET clauses. Alert on specific segments of your data Create constrained alerts that target a specific segment of your data, such as a few key customers or a range of data. Use the WHERE clause to define those conditions. SELECT average(duration) FROM Transaction WHERE account_id in (91290, 102021, 20230) Copy SELECT percentile(duration, 95) FROM Transaction WHERE name LIKE 'Controller/checkout/%' Copy Alert on Nth percentile of your data Create alerts when an Nth percentile of your data hits a specified threshold; for example, maintaining SLA service levels. Since we evaluate the NRQL query based on the aggregation window duration, percentiles will be calculated for each duration separately. SELECT percentile(duration, 95) FROM Transaction Copy SELECT percentile(databaseDuration, 75) FROM Transaction Copy Alert on max, min, avg of your data Create alerts when your data hits a certain maximum, minimum, or average; for example, ensuring that a duration or response time does not pass a certain threshold. SELECT max(duration) FROM Transaction Copy SELECT average(duration) FROM Transaction Copy Alert on a percentage of your data Create alerts when a proportion of your data goes above or below a certain threshold. SELECT percentage(count(*), WHERE duration > 2) FROM Transaction Copy SELECT percentage(count(*), WHERE httpResponseCode = '500') FROM Transaction Copy Alert on Apdex with any T-value Create alerts on Apdex, applying your own T-value for certain transactions. For example, get an alert notification when your Apdex for a T-value of 500ms on transactions for production apps goes below 0.8. SELECT apdex(duration, t:0.5) FROM Transaction WHERE appName like '%prod%' Copy NRQL conditions and query order of operations By default, the aggregation window duration is 1 minute, but you can change the window to suit your needs. Whatever the aggregation window, New Relic will collect data for that window using the function in the NRQL condition’s query. The query is parsed and executed by our systems in the following order: FROM clause – which event type needs to be grabbed? WHERE clause – what can be filtered out? SELECT clause – what information needs to be returned from the now-filtered data set? Example: null value returned Let's say this is your alert condition query: SELECT count(*) FROM SyntheticCheck WHERE monitorName = 'My Cool Monitor' AND result = 'FAILURE' Copy If there are no failures for the aggregation window: The system will execute the FROM clause by grabbing all SyntheticCheck events on your account. Then it will execute the WHERE clause to filter through those events by looking only for the ones that match the monitor name and result specified. If there are still events left to scan through after completing the FROM and WHERE operations, the SELECT clause will be executed. If there are no remaining events, the SELECT clause will not be executed. This means that aggregators like count() and uniqueCount() will never return a zero value. When there is a count of 0, the SELECT clause is ignored and no data is returned, resulting in a value of NULL. Example: zero value returned If you have a data source delivering legitimate numeric zeroes, the query will return zero values and not null values. Let's say this is your alert condition query, and that MyCoolEvent is an attribute that can sometimes return a zero value. SELECT average(MyCoolAttribute) FROM MyCoolEvent Copy If, in the aggregation window being evaluated, there's at least one instance of MyCoolEvent and if the average value of all MyCoolAttribute attributes from that window is equal to zero, then a 0 value will be returned. If there are no MyCoolEvent events during that minute, then a NULL will be returned due to the order of operations. Tip For more information about this topic, you can check out our blog post on troubleshooting for zero versus null values. Tip You can determine how null values will be handled by adjusting loss of signal and gap filling settings in the Alert Conditions UI. Tip You can avoid NULL values entirely with a query order of operations shortcut. Do this by using a filter sub-clause, then including all filter elements within that sub-clause. The main body of the query will run and return data, at which point the SELECT clause will then run and apply the filter elements. The query will return a value of 0 if the filter elements result in no matching data. Here's an example: SELECT filter(count(*), WHERE result = 'SUCCESS' AND monitorName = 'My Favorite Monitor') FROM SyntheticCheck Copy Nested aggregation NRQL alerts Nested aggregation queries are a powerful way to query your data. However, they have a few restrictions that are important to note. Nested queries with a non-faceted innermost query are not currently supported Without a FACET, the inner query produces a single result, giving the outer query nothing to aggregate. If you're using a nested query, make sure your inner query is faceted. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu) ​​​​​ Copy Queries at all levels must have the same aggregation window size With an alert aggregation window of 1 minute, the inner query would produce two smaller windows of 30 seconds. In theory, these two windows could be aggregated by the outer query. However, this is not currently supported. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu TIMESERIES 30 seconds)​​ Copy Signal loss is not yet supported for nested queries For more information on signal loss, see NerdGraph API: Loss of signal and gap filling. Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported You can't use a nested query containing the WITH METRIC_FORMAT in the inner query to create NRQL alert conditions. NRQL condition creation tips Here are some tips for creating and using a NRQL condition: Topic Tips Condition types NRQL condition types include static, baseline, and outlier. Create a description For NRQL conditions, you can create a custom description to add to each violation. Descriptions can be enhanced with variable substitution based on metadata in the specific violation. For details, see Description Query results Queries must return a number. The condition evaluates the returned number against the thresholds you've set. Time period NRQL conditions evaluate data based on how it's aggregated, using aggregation windows from 30 seconds to 15 minutes, in increments of 15 seconds. For best results, we recommend using the event flow or event timer aggregation methods. For the cadence aggregation method, the implicit SINCE ... UNTIL clause specifying which minute to evaluate is controlled by your delay/timer setting. Since very recent data may be incomplete, you may want to query data from 3 minutes ago or longer, especially for: Applications that run on multiple hosts. SyntheticCheck data: Timeouts can take 3 minutes, so 5 minutes or more is recommended. Also, if a query will generate intermittent data, consider using the sum of query results option. Lost signal threshold (loss of signal detection) You can use loss of signal detection to alert on when your data (a telemetry signal) should be considered lost. A signal loss can indicate that a service or entity is no longer online or that a periodic job failed to run. You can also use this to make sure that violations for sporadic data, such as error counts, are closed when no signal is coming in. Advanced signal settings These settings give you options for better handling continuous, streaming data signals that may sometimes be missing. These settings include the aggregation window duration, the delay/timer, and an option for filling data gaps. For more on using these, see Advanced signal settings. Condition settings Use the Condition settings to: Create a concise, descriptive condition name. Provide a custom violation description for the condition that will be included in violations and notifications. Add the runbook URL to include your organization's procedures for handling incidents. You may also add this information to the custom violation description. Limits on conditions See the maximum values. Health status In order for a NRQL alert condition health status display to function properly, use a FACET clause to scope each signal to a single entity (for example, FACET hostname or FACET appname). Examples For more information, see: Expected NRQL syntax Examples of NRQL condition queries Condition edits can reset condition evaluation When you edit NRQL alert conditions in some specific ways (detailed below), their evaluations are reset, meaning that any evaluation up until that point is lost, and the evaluation starts over from that point. The two ways this will affect you are: For \"for at least x minutes\" thresholds: because the evaluation window has been reset, there will be a delay of at least x minutes before any violations can be reported. For baseline conditions: the condition starts over again and all baseline learning is lost. The following actions cause an evaluation reset for NRQL conditions: Changing the query Changing the aggregation window, aggregation method, or aggregation delay/timer setting Changing the \"close violations on signal loss\" setting Changing any gap fill settings Changing the baseline direction (if applicable) – higher, lower, or higher/lower Change the threshold value, threshold window, or threshold operator The following actions (along with any other actions not covered in the above list) will not reset the evaluation: Changing the loss of signal time window (expiration duration) Changing the time function (switching \"for at least\" to \"at least once in,\" or vice-versa) Toggling the \"open violation on signal loss\" setting Alert condition types When you create a NRQL alert, you can choose from different types of conditions: NRQL alert condition types Description Static This is the simplest type of NRQL condition. It allows you to create a condition based on a NRQL query that returns a numeric value. Optional: Include a FACET clause. Baseline (Dynamic) Uses a self-adjusting condition based on the past behavior of the monitored values. Uses the same NRQL query form as the static type, including the optional FACET clause. Outlier Looks for group behavior and values that are outliers from those groups. Uses the same NRQL query form as the static type, but requires a FACET clause. Sum of query results (limited or intermittent data) Important Available only for static (basic) condition types. If a query returns intermittent or limited data, it may be difficult to set a meaningful threshold. Missing or limited data will sometimes generate false positives or false negatives. You can use loss of signal, aggregation duration, and gap filling settings to minimize these false notifications. To avoid this problem when using the static threshold type, you can set the selector to sum of query results. This lets you set the alert on an aggregated sum instead of a value from a single harvest cycle. Up to two hours of one-minute data checks can be aggregated. The duration you select determines the width of the rolling sum and the preview chart will update accordingly. Set the loss of signal threshold Loss of signal occurs when no data matches the NRQL condition over a specific period of time. You can set your loss of signal threshold duration and also what happens when the threshold is crossed. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Loss of signal is only available for NRQL conditions. You may also manage these settings using the GraphQL API (recommended), or the REST API. Go here for specific GraphQL API examples. Loss of signal settings: Loss of signal settings include a time duration and two possible actions. Signal loss expiration time UI label: Signal is lost after: GraphQL Node: expiration.expirationDuration Expiration duration is a timer that starts and resets when we receive a data point in the streaming alerts pipeline. If we don't receive another data point before your 'expiration time' expires, we consider that signal to be lost. This can be because no data is being sent to New Relic or the WHERE clause of your NRQL query is filtering that data out before it is streamed to the alerts pipeline. Note that when you have a faceted query, each facet is a signal. So if any one of those signals ends during the duration specified, that will be considered a loss of signal. The loss of signal expiration time is independent of the threshold duration and triggers as soon as the timer expires. The maximum expiration duration is 48 hours. This is helpful when monitoring for the execution of infrequent jobs. The minimum is 30 seconds, but we recommend using at least 3-5 minutes. Loss of signal actions Once a signal is considered lost, you can close open violations, open new violations, or both. Close all current open violations: This closes all open violations that are related to a specific signal. It won't necessarily close all violations for a condition. If you're alerting on an ephemeral service, or on a sporadic signal, you'll want to choose this action to ensure that violations are closed properly. The GraphQL node name for this is \"closeViolationsOnExpiration\" Open new violations: This will open a new violation when the signal is considered lost. These violations will indicate that they are due to a loss of signal. Based on your incident preferences, this should trigger a notification. The graphQL node name for this is \"openViolationOnExpiration\" When you enable both actions, we'll close all open violations first, and then open a new violation for loss of signal. To create a NRQL alert configured with loss of signal detection in the UI: For a policy, when you create a condition, under Select a product, click NRQL, then click Next, define thresholds. Write a NRQL query that returns the values you want to alert on. For Threshold type, select Static or Baseline. Click + Add lost signal threshold, then set the signal expiration duration time in minutes or seconds in the Signal is lost after field. Choose what you want to happen when the signal is lost. You can check one or both of Close all current open violations and Open new \"lost signal\" violation. These control how loss of signal violations will be handled for the condition. Make sure you name your condition before you save it. Violations open due to loss of signal close when the signal comes back. Newly opened lost signal violations will close immediately when new data is evaluated. the condition they belong to expires. By default, conditions expire after 3 days. you manually close the violation with the Close all current open violations option. Tip Loss of signal detection doesn't work on NRQL queries that use nested aggregation or sub-queries. Advanced signal settings When creating a NRQL alert condition, use the advanced signal settings to control streaming alert data and avoid false alarms. When creating a NRQL condition, there are several advanced signal settings: Aggregation window duration Delay/timer Fill data gaps To read an explanation of what these settings are and how they relate to each other, see Streaming alerts concepts. Below are instructions and tips on how to configure them. Aggregation window duration You can set the aggregation window duration to choose how long data is accumulated in a streaming time window before it's aggregated. You can set it to anything between 30 seconds and 15 minutes. The default is one minute. Tip Baseline alert condition thresholds don't support editing the aggregation window. They use the 1 minute default. Delay/timer You can adjust the delay/timer to coordinate our streaming alerting algorithm with your data's behavior. If your data is sparse or inconsistent, you may want to use the event timer aggregation method. For the cadence method, the total supported latency is the sum of the aggregation window duration and the delay. If the data type comes from an APM language agent and is aggregated from many app instances (for example, Transactions, TransactionErrors, etc.), we recommend using the event flow method with the default settings. Important When creating NRQL conditions for data collected from Infrastructure Cloud Integrations such as AWS Cloudwatch or Azure, we recommend that you use the event timer method. Fill data gaps Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with one of these settings: None: (Default) Choose this if you don't want to take any action on empty aggregation windows. On evaluation, an empty aggregation window will reset the threshold duration timer. For example, if a condition says that all aggregation windows must have data points above the threshold for 5 minutes, and 1 of the 5 aggregation windows is empty, then the condition won't be in violation. Custom static value: Choose this if you'd like to insert a custom static value into the empty aggregation windows before they're evaluated. This option has an additional, required parameter of fillValue (as named in the API) that specifies what static value should be used. This defaults to 0. Last known value: This option inserts the last seen value before evaluation occurs. We maintain the state of the last seen value for 2 hours. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss, gap filling, and how to request access to these features, see this Explorers Hub post. Options for editing data gap settings: In the NRQL conditions UI, go to Condition settings > Advanced signal settings > fill data gaps with and choose an option. If using our Nerdgraph API (preferred), this node is located at: actor : account : alerts : nrqlCondition : signal : fillOption | fillValue NerdGraph is our recommended API for this but if you're using our REST API, you can find this setting in the REST API explorer under the \"signal\" section of the Alert NRQL conditions API.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 251.8565,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create NRQL <em>alert</em> <em>conditions</em>",
        "sections": "Create NRQL <em>alert</em> <em>conditions</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "You can use NRQL queries to create <em>alert</em> <em>conditions</em>. Once you&#x27;ve defined your signal, you can further define your warning and critical threshold levels. This determines when an <em>alerts</em> violation is created. Read on to learn more about how to do this. Go to one.newrelic.com, click <em>Alerts</em> &amp; AI"
      },
      "id": "603ef04864441fbc114e8883"
    },
    {
      "sections": [
        "Streaming alerts: key terms and concepts",
        "Why it matters",
        "Streaming alerts process and descriptions",
        "Choose your aggregation method",
        "Event flow",
        "Caution",
        "Event timer",
        "Cadence",
        "Streaming alerts tools",
        "Tip",
        "Window duration",
        "Delay/timer",
        "Loss of signal detection",
        "Gap filling"
      ],
      "title": "Streaming alerts: key terms and concepts",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Get started"
      ],
      "external_id": "144c7f0721ec2acb1e8ac8aa06142911ea552aef",
      "image": "https://docs.newrelic.com/static/39158bd84483adf9516bb7ff5058c9bd/d30ee/streaming-alerts.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/streaming-alerts-key-terms-concepts/",
      "published_at": "2021-12-15T12:47:58Z",
      "updated_at": "2021-12-15T12:47:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The streaming alerts platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into New Relic. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that's processed by the streaming algorithm. There are three methods for aggregating the data filtered through your NRQL condition: Event flow (default) Event timer Cadence This short video describes the three aggregation method (5:31). Why it matters Understanding how streaming alerts works will help you fine-tune your NRQL conditions to be notified about what's important to you. Only data that matches the conditions of the NRQL WHERE clause is alerted on. For more details on each step of the process, see Streaming alerts process and descriptions. As data streams into New Relic, it's filtered by the NRQL condition. Before data is evaluated, it must meet the criteria defined by the NRQL query's WHERE clause. Instead of evaluating that data immediately for violations, the NRQL alert conditions collect the data over a period of time known as the aggregation window. An additional delay/timer allows for slower data points to arrive before the window is aggregated. Once the delay/timer time has elapsed, New Relic aggregates the data into a single data point. Alerts then evaluates the data point using the NRQL condition in order to determine whether it meets the violation threshold criteria. Even if a data point meets the criteria for a violation, a violation may not be triggered. A violation is only triggered when data points consistently meet the threshold criteria over a period of time. This is the threshold duration. If the data points are in violation for an entire threshold duration, we'll send you a notification based on your policy settings. All of these configurable delays give you more control over how you're alerted on sporadic and missing data. Streaming alerts process and descriptions Process Description Streaming data All data coming into New Relic. WHERE clause Filters all incoming streaming data. We only monitor for alerts on data that makes it through this filter. Aggregation methods One of three methods that control how data is collected before it's evaluated. They are: Event flow (Default) Event timer Cadence Aggregation window Data with timestamps that fall within this window will be aggregated and then evaluated. Delay/timer A time delay to ensure all data points have arrived in the aggregation window before aggregation occurs. Aggregated data Data in the aggregated window is collapsed to a single data point for alert evaluation. Evaluation The data point is evaluated by the NRQL condition, which is trigged by each incoming aggregated data point. Threshold duration A specific duration that determines if a violation is created. If your specified NRQL condition meets the threshold criteria over the threshold duration, a violation occurs. When a data point lacks data, a custom value is inserted to fill the gap. Choose your aggregation method You can choose between three different aggregation methods, depending on your needs. Event flow (default) works best for data that comes in frequently and mostly in order. Event timer works best for data that arrives infrequently in batches, such as cloud integration data or infrequent error logs. Cadence is our original and inferior aggregation method. It aggregates data on specific time intervals as detected by New Relic's internal wall clock, regardless of data timestamps. Event flow Event flow aggregates a window of data when the first data point arrives in a subsequent window. The custom delay defines which subsequent window data will start to populate to trigger aggregation of the current window. A custom delay provides extra time for data to arrive. These times are based on the data's timestamps and not New Relic's wall clock time. For example, suppose you're monitoring CPU usage in window durations of 1 minute and a 3 minute delay. When a CPU usage data point comes in with a timestamp between 12:00pm and 12:01pm, event flow will not aggregate that window until a data point shows up with a timestamp between 12:04pm and 12:05pm. When event flow receives the first data point with a timestamp of 12:04pm or later, it sends the 12:00 to 12:01 data to be aggregated. Caution If you expect your data points to arrive more than 30 minutes apart, please use the Event Timer method described below. Event timer Like event flow, event timer will only aggregate data for a given window when data arrives for that window. When a data point arrives for an aggregation window, a timer dedicated to that window starts to count down. If no further data arrives before the timer counts down, the data for that window is aggregated. If more data points arrive before the timer has completed counting down, the timer is reset. For example, suppose you're monitoring CloudWatch data that arrives fairly infrequently. You're using a window duration of 1 minute and a 3 minute timer. When a CloudWatch data point comes in with a timestamp between 12:00pm and 12:01pm, the timer will start to count down. If no further data points show up for that 12:00-12:01 window, the window will get aggregated 3 minutes later. If a new data point with a timestamp between 12:00 and 12:01 arrives, the timer resets. It keeps resetting every time more data points for that window arrive. The window will not be sent for aggregation until the timer reaches 0. If the timer for a later data point elapses before an earlier data point, the event timer method waits for the earlier timer to elapse before aggregating the later data point. For best results, make sure your timer is equal to or longer than your window duration time. If the timer is less than your window duration and your data flow is inconsistent, then your data may be evaluated before all of your data points arrive. This could cause you to be notified incorrectly. Cadence We recommend you use one of the other two methods. Cadence is our old streaming aggregation method. This method uses New Relic's wall clock time to determine when data is aggregated and evaluated. It doesn't take into account data point timestamps as they arrive. Streaming alerts tools Streaming alerts provide a set of tools to give you greater control over how your data is aggregated before it's evaluated to reduce incorrect notifications you receive. They are: Window duration Delay/timer Loss of signal detection Gap filling Tip This article covers these tools at a conceptual level. You'll find direct instructions on how to use these tools in Create NRQL alert conditions. Window duration In order to make loss of signal detection more effective and to reduce unnecessary notifications, you can customize aggregation windows to the duration that you need. An aggregation window is a specific block of time. We gather data points together in an aggregation window, before evaluating the data. A longer aggregation window can smooth out the data, since an outlier data point will have more data points to be aggregated with, giving it less of an influence on the aggregated data point that is sent for evaluation. When a data point arrives, its timestamp is used to put it in the proper aggregation window. You can set your aggregation window to anything between 30 seconds and 15 minutes. The default is 1 minute. Delay/timer The delay/timer setting controls how long the condition should wait before aggregating the data in the aggregation window. The event flow and cadence methods use delay. Event timer uses timer. The delay default is 2 minutes. The timer default is 1 minute and has a minimum value of 5 seconds. Loss of signal detection Loss of signal occurs when no data matches the NRQL condition over a specific period of time. A loss of signal is caused by different things. The WHERE clause in your NRQL query can filter out data before it's evaluated for violations. It could also mean a service or entity is offline or a periodic job has failed to run and no data is being sent to New Relic. In order to avoid unnecessary notifications, you can choose how long to wait before you're notified by a loss of signal violation. You can use loss of signal detection to open violations and be notified when a signal is lost. Alternately, you can use a loss of signal to close violations for ephemeral services or sporadic data, such as error counts. Gap filling Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with the last value received, a static value, or else do nothing and leave the gap there. The default is None. Gaps in streaming data can be caused by network or host issues, a signal may be sparse, or some signals, such as error counts, may only have data when something is wrong. By filling the gaps with known values, the alert evaluation process can process those gaps and determine how they should affect the loss of signal evaluation. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss and gap filling, see this Explorers Hub post.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 229.45612,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "sections": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "The streaming <em>alerts</em> platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into <em>New</em> <em>Relic</em>. You can use NRQL <em>conditions</em> to control what part of the signal you want to be notified about. Your NRQL condition filters the data that&#x27;s processed"
      },
      "id": "6130c054e7b9d269edb6f21d"
    },
    {
      "sections": [
        "Outlier detection (NRQL alert)",
        "Important",
        "What is outlier detection?",
        "Tip",
        "Example use cases",
        "Notify if load-balanced servers have uneven workload",
        "Notify if load-balanced application has misbehaving instances",
        "Notify of changes in different environments",
        "Notify for timezone-related changes",
        "Create an outlier alert condition",
        "Rules and logic",
        "Details about alert condition logic",
        "NRQL query rules and limits",
        "Zero values for unreturned data"
      ],
      "title": "Outlier detection (NRQL alert)",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert conditions"
      ],
      "external_id": "499fa55abd48a0ccdd897fbdf64ccea2d9f98d11",
      "image": "https://docs.newrelic.com/static/f235d0630576bc2010ff07adc7a69621/f73a1/NRQL_outlier_violations.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/outlier-detection-nrql-alert/",
      "published_at": "2021-12-15T12:44:32Z",
      "updated_at": "2021-11-25T19:50:19Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Alerts offers NRQL conditions in three threshold types: static, baseline, and outlier. This document explains how the outlier threshold type works, gives some example use cases and NRQL queries, and explains how to create an outlier condition. Important NRQL alerts do not affect Alerts policies for a Synthetic monitor. For example, muting a NRQL alert will not mute a Synthetic monitor's alerts. What is outlier detection? In software development and operations, it is common to have a group consisting of members you expect to behave approximately the same. For example: for servers using a load balancer, the traffic to the servers may go up or down, but the traffic for all the servers should remain in a fairly tight grouping. See outlier detection in action in this NerdBytes video (2:51 minutes). The NRQL alert outlier detection feature parses the data returned by your faceted NRQL query and: Looks for the number of expected groups that you specify Looks for outliers (values deviating from a group) based on the sensitivity and time range you set Additionally, for queries that have more than one group, you can choose to be notified when groups start behaving the same. This visual aid will help you understand the types of situations that will trigger a violation and those that won't. For more on the rules and logic behind this calculation, see Outlier detection rules. Tip Note: this feature does not take into account the past behavior of the monitored values; it looks for outliers only in the currently reported data. For an alert type that takes into account past behavior, see Baseline alerting. Example use cases These use cases will help you understand when to use the outlier threshold type. Note that the outlier feature requires a NRQL query with a FACET clause. Notify if load-balanced servers have uneven workload A load balancer divides web traffic approximately evenly across five different servers. You can set a notification to be sent if any server starts getting significantly more or less traffic than the other servers. Example query: SELECT average(cpuPercent) FROM SystemSample WHERE apmApplicationNames = 'MY-APP-NAME' FACET hostname Copy Notify if load-balanced application has misbehaving instances Application instances behind a load balancer should have similar throughput, error rates, and response times. If an instance is in a bad state, or a load balancer is misconfigured, this will not be the case. Detecting one or two bad app instances using aggregate metrics may be difficult if there is not a significant rise in the overall error rate of the application. You can set a notification for when an app instance’s throughput, error rate, or response time deviates too far from the rest of the group. Example query: SELECT average(duration) FROM Transaction WHERE appName = 'MY-APP-NAME' FACET host Copy Notify of changes in different environments An application is deployed in two different environments, with ten application instances in each. One environment is experimental and gets more errors than the other. But the instances that are in the same environment should get approximately the same number of errors. You can set a notification for when an instance starts getting more errors than the other instances in the same environment. Also, you can set a notification for when the two environments start to have the same number of errors as each other. Notify for timezone-related changes The number of logged in users for a company is about the same for each of four applications, but varies significantly by each of the three time zones the company operates in. You can set a notification for when any application starts getting more or less traffic from a certain timezone than the other applications. Sometimes the traffic from the different time zones are the same, so you would set up the alert condition to not be notified if the time zone groups overlap. For more details on how this feature works, see Outlier rules and logic. Create an outlier alert condition To create a NRQL alert that uses outlier detection: When creating a condition, under Select a product, select NRQL. For Threshold type, select Outlier. Create a NRQL query with a FACET clause that returns the values you want to alert on. Depending on how the returned values group together, set the Number of expected groups. Adjust the deviation from the center of the group(s) and the duration that will trigger a violation. Optional: Add a warning threshold and set its deviation. Set any remaining available options and save. Rules and logic Here are the rules and logic behind how outlier detection works: Details about alert condition logic After the condition is created, the query is run once every harvest cycle and the condition is applied. Unlike baseline alerts, outlier detection uses no historical data in its calculation; it's calculated using the currently collected data. Alerts will attempt to divide the data returned from the query into the number of groups selected during condition creation. For each group, the approximate average value is calculated. The allowable deviation you have chosen when creating the condition is centered around that average value. If a member of the group is outside the allowed deviation, it produces a violation. If Trigger when groups overlap has been selected, Alerts detects a convergence of groups. If the condition is looking for two or more groups, and the returned values cannot be separated into that number of distinct groups, then that will produce a violation. This type of “overlap” event is represented on a chart by group bands touching. Because this feature does not take past behavior into account, data is never considered to \"belong\" to a certain group. For example, a value that switches places with another value wouldn't trigger a violation. Additionally, an entire group that moves together also wouldn't trigger a violation. NRQL query rules and limits The NRQL query must be a faceted query. The number of unique values returned must be 500 or less. If the query returns more than this number of values, the condition won't be created. If the query later returns more than this number after being created, the alert will fail. Zero values for unreturned data When a query returns a set of values, only values that are actually returned are taken into account. If a value is not available for calculation (including if it goes from being collected one harvest cycle to not being collected), it is rendered as a zero and is not considered. In other words, the behavior of unreturned zero values will never trigger violations.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.94473,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Outlier detection (NRQL <em>alert</em>)",
        "sections": "Create an outlier <em>alert</em> <em>condition</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "<em>Alerts</em> offers NRQL <em>conditions</em> in three threshold types: static, baseline, and outlier. This document explains how the outlier threshold type works, gives some example use cases and NRQL queries, and explains how to create an outlier condition. Important NRQL <em>alerts</em> do not affect <em>Alerts</em> policies"
      },
      "id": "6130be72196a6793654948e7"
    }
  ],
  "/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/monitor-scheduled-jobs": [
    {
      "sections": [
        "Create NRQL alert conditions",
        "Tip",
        "Create a NRQL alert condition",
        "Create a condition from a chart",
        "Caution",
        "NRQL alert syntax",
        "Important",
        "Reformatting incompatible NRQL",
        "NRQL alert threshold examples",
        "Alert on specific segments of your data",
        "Alert on Nth percentile of your data",
        "Alert on max, min, avg of your data",
        "Alert on a percentage of your data",
        "Alert on Apdex with any T-value",
        "NRQL conditions and query order of operations",
        "Example: null value returned",
        "Example: zero value returned",
        "Nested aggregation NRQL alerts",
        "Nested queries with a non-faceted innermost query are not currently supported",
        "Queries at all levels must have the same aggregation window size",
        "Signal loss is not yet supported for nested queries",
        "Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported",
        "NRQL condition creation tips",
        "Condition edits can reset condition evaluation",
        "Alert condition types",
        "Sum of query results (limited or intermittent data)",
        "Set the loss of signal threshold",
        "Advanced signal settings",
        "Aggregation window duration",
        "Delay/timer",
        "Fill data gaps"
      ],
      "title": "Create NRQL alert conditions",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert conditions"
      ],
      "external_id": "2872f770a5fc8a802f99b9f848906f7e351ad761",
      "image": "https://docs.newrelic.com/static/eb8e1b2d826f9fd9cf46fe3dd5455217/c1b63/nr1_nrql_alert_conditions.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/",
      "published_at": "2021-12-19T16:01:17Z",
      "updated_at": "2021-12-14T22:49:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use NRQL queries to create alert conditions. Once you've defined your signal, you can further define your warning and critical threshold levels. This determines when an alerts violation is created. Read on to learn more about how to do this. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Click NRQL, and then Next, define thresholds. Tip For more information on key concepts relating to NRQL alert conditions and streaming alerts, see Streaming alerts: key terms and concepts. Create a NRQL alert condition To create a NRQL alert condition for a policy: On one.newrelic.com, in the header click Alerts & AI, then in the left sidebar click Policies. Select an existing policy or click New alert policy to create a new policy. Click Add a condition. Under Select a product click NRQL, and then click Next, define thresholds. Note that editing an existing condition can result in resetting its evaluation. Create a condition from a chart You can use a chart to create a NRQL alert condition. To create a NRQL alerts condition from a chart, click the chart menu , then click Create alert condition. Once you've named and customized your condition, you can add it to an existing policy or create a new one. Caution A small number of our older charts don't include the option to create an alert condition. NRQL alert syntax Here's the basic syntax for creating all NRQL alert conditions. The FACET clause is required for outlier condition types. It's optional for static and baseline. SELECT function(attribute) FROM Event WHERE attribute [comparison] [AND|OR ...] Copy Clause Notes SELECT function(attribute) Required Supported functions that return numbers include: apdex average count latest max min percentage percentile sum uniqueCount Tip If you use the percentile aggregator in a faceted alert condition with many facets, this may cause the following error to appear: An error occurred while fetching chart data. If you see this error, use average instead. FROM data type Required Only one data type can be targeted. Supported data types: Event Metric (RAW data points will be returned) WHERE attribute [comparison] [AND|OR ...] Use the WHERE clause to specify a series of one or more conditions. All the operators are supported. FACET attribute Required for outlier conditions Include an optional FACET clause in your NRQL syntax depending on the threshold type: static or baseline. Use the FACET clause to separate your results by attribute and alert on each attribute independently. No LIMIT clause is allowed, but all queries will receive the maximum number of facets possible. Faceted queries can return a maximum of 5000 values for static and baseline conditions and a maximum of 500 values for outlier conditions. Important If the query returns more than the maximum number of values, the alert condition can't be created. If you create the condition and the query returns more than this number later, the alert will fail. Modify your query so that it returns a fewer number of values. Reformatting incompatible NRQL Some elements of NRQL used in charts don’t make sense in the streaming context of alerts. Here’s a list of the most common incompatible elements and suggestions for reformatting a NRQL alert query to achieve the same effect. Element Notes SINCE and UNTIL Example: SELECT percentile(largestContentfulPaint, 75) FROM PageViewTiming WHERE (appId = 837807) SINCE yesterday Copy NRQL conditions produce a never-ending stream of windowed query results, so the SINCE and UNTIL keywords to scope the query to a point in time are not compatible. As a convenience, we automatically strip SINCE and UNTIL from a query when creating a condition from the context of a chart. TIMESERIES In NRQL queries, the TIMESERIES clause is used to return data as a time series broken out by a specified period of time. For NRQL conditions, the equivalent property of a signal is the aggregation duration window. histogram() The histogram() aggregation function is used to generate histograms. histogram() is not compatible with NRQL alerting: histogram aggregations can not be formatted as a time series. To create an alert from a portion of a histogram (for example, 95th percentile), use the percentile() aggregation function. Multiple aggregation functions Each condition can only target a single aggregated value. To alert on multiple values simultaneously, you’ll need to decompose them into individual conditions within the same policy. Original Query: SELECT count(foo), average(bar), max(baz) from Transaction Copy Decomposed: SELECT count(foo) from Transaction SELECT average(bar) from Transaction SELECT max(baz) from Transaction Copy COMPARE WITH The COMPARE WITH clause is used to compare the values for two different time ranges. This type of query is incompatible with NRQL alerting. We recommend using a Baseline Alert Condition to dynamically detect deviations for a particular signal. SLIDE BY The SLIDE BY clause supports a feature known as sliding windows. With sliding windows, SLIDE BY data is gathered into \"windows\" of time that overlap with each other. These windows can help to smooth out line graphs with a lot of variation in cases where the rolling aggregate (such as a rolling mean) is more important than aggregates from narrow windows of time. Sliding windows are not currently supported in NRQL alerts. LIMIT In NRQL queries, the LIMIT clause is used to control the amount of data a query returns, either the maximum number of facet values returned by FACET queries or the maximum number of items returned by SELECT * queries. LIMIT is not compatible with NRQL alerting: evaluation is always performed on the full result set. NRQL alert threshold examples Here are some common use cases for NRQL conditions. These queries will work for static and baseline condition types. The outlier condition type will require additional FACET clauses. Alert on specific segments of your data Create constrained alerts that target a specific segment of your data, such as a few key customers or a range of data. Use the WHERE clause to define those conditions. SELECT average(duration) FROM Transaction WHERE account_id in (91290, 102021, 20230) Copy SELECT percentile(duration, 95) FROM Transaction WHERE name LIKE 'Controller/checkout/%' Copy Alert on Nth percentile of your data Create alerts when an Nth percentile of your data hits a specified threshold; for example, maintaining SLA service levels. Since we evaluate the NRQL query based on the aggregation window duration, percentiles will be calculated for each duration separately. SELECT percentile(duration, 95) FROM Transaction Copy SELECT percentile(databaseDuration, 75) FROM Transaction Copy Alert on max, min, avg of your data Create alerts when your data hits a certain maximum, minimum, or average; for example, ensuring that a duration or response time does not pass a certain threshold. SELECT max(duration) FROM Transaction Copy SELECT average(duration) FROM Transaction Copy Alert on a percentage of your data Create alerts when a proportion of your data goes above or below a certain threshold. SELECT percentage(count(*), WHERE duration > 2) FROM Transaction Copy SELECT percentage(count(*), WHERE httpResponseCode = '500') FROM Transaction Copy Alert on Apdex with any T-value Create alerts on Apdex, applying your own T-value for certain transactions. For example, get an alert notification when your Apdex for a T-value of 500ms on transactions for production apps goes below 0.8. SELECT apdex(duration, t:0.5) FROM Transaction WHERE appName like '%prod%' Copy NRQL conditions and query order of operations By default, the aggregation window duration is 1 minute, but you can change the window to suit your needs. Whatever the aggregation window, New Relic will collect data for that window using the function in the NRQL condition’s query. The query is parsed and executed by our systems in the following order: FROM clause – which event type needs to be grabbed? WHERE clause – what can be filtered out? SELECT clause – what information needs to be returned from the now-filtered data set? Example: null value returned Let's say this is your alert condition query: SELECT count(*) FROM SyntheticCheck WHERE monitorName = 'My Cool Monitor' AND result = 'FAILURE' Copy If there are no failures for the aggregation window: The system will execute the FROM clause by grabbing all SyntheticCheck events on your account. Then it will execute the WHERE clause to filter through those events by looking only for the ones that match the monitor name and result specified. If there are still events left to scan through after completing the FROM and WHERE operations, the SELECT clause will be executed. If there are no remaining events, the SELECT clause will not be executed. This means that aggregators like count() and uniqueCount() will never return a zero value. When there is a count of 0, the SELECT clause is ignored and no data is returned, resulting in a value of NULL. Example: zero value returned If you have a data source delivering legitimate numeric zeroes, the query will return zero values and not null values. Let's say this is your alert condition query, and that MyCoolEvent is an attribute that can sometimes return a zero value. SELECT average(MyCoolAttribute) FROM MyCoolEvent Copy If, in the aggregation window being evaluated, there's at least one instance of MyCoolEvent and if the average value of all MyCoolAttribute attributes from that window is equal to zero, then a 0 value will be returned. If there are no MyCoolEvent events during that minute, then a NULL will be returned due to the order of operations. Tip For more information about this topic, you can check out our blog post on troubleshooting for zero versus null values. Tip You can determine how null values will be handled by adjusting loss of signal and gap filling settings in the Alert Conditions UI. Tip You can avoid NULL values entirely with a query order of operations shortcut. Do this by using a filter sub-clause, then including all filter elements within that sub-clause. The main body of the query will run and return data, at which point the SELECT clause will then run and apply the filter elements. The query will return a value of 0 if the filter elements result in no matching data. Here's an example: SELECT filter(count(*), WHERE result = 'SUCCESS' AND monitorName = 'My Favorite Monitor') FROM SyntheticCheck Copy Nested aggregation NRQL alerts Nested aggregation queries are a powerful way to query your data. However, they have a few restrictions that are important to note. Nested queries with a non-faceted innermost query are not currently supported Without a FACET, the inner query produces a single result, giving the outer query nothing to aggregate. If you're using a nested query, make sure your inner query is faceted. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu) ​​​​​ Copy Queries at all levels must have the same aggregation window size With an alert aggregation window of 1 minute, the inner query would produce two smaller windows of 30 seconds. In theory, these two windows could be aggregated by the outer query. However, this is not currently supported. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu TIMESERIES 30 seconds)​​ Copy Signal loss is not yet supported for nested queries For more information on signal loss, see NerdGraph API: Loss of signal and gap filling. Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported You can't use a nested query containing the WITH METRIC_FORMAT in the inner query to create NRQL alert conditions. NRQL condition creation tips Here are some tips for creating and using a NRQL condition: Topic Tips Condition types NRQL condition types include static, baseline, and outlier. Create a description For NRQL conditions, you can create a custom description to add to each violation. Descriptions can be enhanced with variable substitution based on metadata in the specific violation. For details, see Description Query results Queries must return a number. The condition evaluates the returned number against the thresholds you've set. Time period NRQL conditions evaluate data based on how it's aggregated, using aggregation windows from 30 seconds to 15 minutes, in increments of 15 seconds. For best results, we recommend using the event flow or event timer aggregation methods. For the cadence aggregation method, the implicit SINCE ... UNTIL clause specifying which minute to evaluate is controlled by your delay/timer setting. Since very recent data may be incomplete, you may want to query data from 3 minutes ago or longer, especially for: Applications that run on multiple hosts. SyntheticCheck data: Timeouts can take 3 minutes, so 5 minutes or more is recommended. Also, if a query will generate intermittent data, consider using the sum of query results option. Lost signal threshold (loss of signal detection) You can use loss of signal detection to alert on when your data (a telemetry signal) should be considered lost. A signal loss can indicate that a service or entity is no longer online or that a periodic job failed to run. You can also use this to make sure that violations for sporadic data, such as error counts, are closed when no signal is coming in. Advanced signal settings These settings give you options for better handling continuous, streaming data signals that may sometimes be missing. These settings include the aggregation window duration, the delay/timer, and an option for filling data gaps. For more on using these, see Advanced signal settings. Condition settings Use the Condition settings to: Create a concise, descriptive condition name. Provide a custom violation description for the condition that will be included in violations and notifications. Add the runbook URL to include your organization's procedures for handling incidents. You may also add this information to the custom violation description. Limits on conditions See the maximum values. Health status In order for a NRQL alert condition health status display to function properly, use a FACET clause to scope each signal to a single entity (for example, FACET hostname or FACET appname). Examples For more information, see: Expected NRQL syntax Examples of NRQL condition queries Condition edits can reset condition evaluation When you edit NRQL alert conditions in some specific ways (detailed below), their evaluations are reset, meaning that any evaluation up until that point is lost, and the evaluation starts over from that point. The two ways this will affect you are: For \"for at least x minutes\" thresholds: because the evaluation window has been reset, there will be a delay of at least x minutes before any violations can be reported. For baseline conditions: the condition starts over again and all baseline learning is lost. The following actions cause an evaluation reset for NRQL conditions: Changing the query Changing the aggregation window, aggregation method, or aggregation delay/timer setting Changing the \"close violations on signal loss\" setting Changing any gap fill settings Changing the baseline direction (if applicable) – higher, lower, or higher/lower Change the threshold value, threshold window, or threshold operator The following actions (along with any other actions not covered in the above list) will not reset the evaluation: Changing the loss of signal time window (expiration duration) Changing the time function (switching \"for at least\" to \"at least once in,\" or vice-versa) Toggling the \"open violation on signal loss\" setting Alert condition types When you create a NRQL alert, you can choose from different types of conditions: NRQL alert condition types Description Static This is the simplest type of NRQL condition. It allows you to create a condition based on a NRQL query that returns a numeric value. Optional: Include a FACET clause. Baseline (Dynamic) Uses a self-adjusting condition based on the past behavior of the monitored values. Uses the same NRQL query form as the static type, including the optional FACET clause. Outlier Looks for group behavior and values that are outliers from those groups. Uses the same NRQL query form as the static type, but requires a FACET clause. Sum of query results (limited or intermittent data) Important Available only for static (basic) condition types. If a query returns intermittent or limited data, it may be difficult to set a meaningful threshold. Missing or limited data will sometimes generate false positives or false negatives. You can use loss of signal, aggregation duration, and gap filling settings to minimize these false notifications. To avoid this problem when using the static threshold type, you can set the selector to sum of query results. This lets you set the alert on an aggregated sum instead of a value from a single harvest cycle. Up to two hours of one-minute data checks can be aggregated. The duration you select determines the width of the rolling sum and the preview chart will update accordingly. Set the loss of signal threshold Loss of signal occurs when no data matches the NRQL condition over a specific period of time. You can set your loss of signal threshold duration and also what happens when the threshold is crossed. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Loss of signal is only available for NRQL conditions. You may also manage these settings using the GraphQL API (recommended), or the REST API. Go here for specific GraphQL API examples. Loss of signal settings: Loss of signal settings include a time duration and two possible actions. Signal loss expiration time UI label: Signal is lost after: GraphQL Node: expiration.expirationDuration Expiration duration is a timer that starts and resets when we receive a data point in the streaming alerts pipeline. If we don't receive another data point before your 'expiration time' expires, we consider that signal to be lost. This can be because no data is being sent to New Relic or the WHERE clause of your NRQL query is filtering that data out before it is streamed to the alerts pipeline. Note that when you have a faceted query, each facet is a signal. So if any one of those signals ends during the duration specified, that will be considered a loss of signal. The loss of signal expiration time is independent of the threshold duration and triggers as soon as the timer expires. The maximum expiration duration is 48 hours. This is helpful when monitoring for the execution of infrequent jobs. The minimum is 30 seconds, but we recommend using at least 3-5 minutes. Loss of signal actions Once a signal is considered lost, you can close open violations, open new violations, or both. Close all current open violations: This closes all open violations that are related to a specific signal. It won't necessarily close all violations for a condition. If you're alerting on an ephemeral service, or on a sporadic signal, you'll want to choose this action to ensure that violations are closed properly. The GraphQL node name for this is \"closeViolationsOnExpiration\" Open new violations: This will open a new violation when the signal is considered lost. These violations will indicate that they are due to a loss of signal. Based on your incident preferences, this should trigger a notification. The graphQL node name for this is \"openViolationOnExpiration\" When you enable both actions, we'll close all open violations first, and then open a new violation for loss of signal. To create a NRQL alert configured with loss of signal detection in the UI: For a policy, when you create a condition, under Select a product, click NRQL, then click Next, define thresholds. Write a NRQL query that returns the values you want to alert on. For Threshold type, select Static or Baseline. Click + Add lost signal threshold, then set the signal expiration duration time in minutes or seconds in the Signal is lost after field. Choose what you want to happen when the signal is lost. You can check one or both of Close all current open violations and Open new \"lost signal\" violation. These control how loss of signal violations will be handled for the condition. Make sure you name your condition before you save it. Violations open due to loss of signal close when the signal comes back. Newly opened lost signal violations will close immediately when new data is evaluated. the condition they belong to expires. By default, conditions expire after 3 days. you manually close the violation with the Close all current open violations option. Tip Loss of signal detection doesn't work on NRQL queries that use nested aggregation or sub-queries. Advanced signal settings When creating a NRQL alert condition, use the advanced signal settings to control streaming alert data and avoid false alarms. When creating a NRQL condition, there are several advanced signal settings: Aggregation window duration Delay/timer Fill data gaps To read an explanation of what these settings are and how they relate to each other, see Streaming alerts concepts. Below are instructions and tips on how to configure them. Aggregation window duration You can set the aggregation window duration to choose how long data is accumulated in a streaming time window before it's aggregated. You can set it to anything between 30 seconds and 15 minutes. The default is one minute. Tip Baseline alert condition thresholds don't support editing the aggregation window. They use the 1 minute default. Delay/timer You can adjust the delay/timer to coordinate our streaming alerting algorithm with your data's behavior. If your data is sparse or inconsistent, you may want to use the event timer aggregation method. For the cadence method, the total supported latency is the sum of the aggregation window duration and the delay. If the data type comes from an APM language agent and is aggregated from many app instances (for example, Transactions, TransactionErrors, etc.), we recommend using the event flow method with the default settings. Important When creating NRQL conditions for data collected from Infrastructure Cloud Integrations such as AWS Cloudwatch or Azure, we recommend that you use the event timer method. Fill data gaps Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with one of these settings: None: (Default) Choose this if you don't want to take any action on empty aggregation windows. On evaluation, an empty aggregation window will reset the threshold duration timer. For example, if a condition says that all aggregation windows must have data points above the threshold for 5 minutes, and 1 of the 5 aggregation windows is empty, then the condition won't be in violation. Custom static value: Choose this if you'd like to insert a custom static value into the empty aggregation windows before they're evaluated. This option has an additional, required parameter of fillValue (as named in the API) that specifies what static value should be used. This defaults to 0. Last known value: This option inserts the last seen value before evaluation occurs. We maintain the state of the last seen value for 2 hours. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss, gap filling, and how to request access to these features, see this Explorers Hub post. Options for editing data gap settings: In the NRQL conditions UI, go to Condition settings > Advanced signal settings > fill data gaps with and choose an option. If using our Nerdgraph API (preferred), this node is located at: actor : account : alerts : nrqlCondition : signal : fillOption | fillValue NerdGraph is our recommended API for this but if you're using our REST API, you can find this setting in the REST API explorer under the \"signal\" section of the Alert NRQL conditions API.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 251.8565,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create NRQL <em>alert</em> <em>conditions</em>",
        "sections": "Create NRQL <em>alert</em> <em>conditions</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "You can use NRQL queries to create <em>alert</em> <em>conditions</em>. Once you&#x27;ve defined your signal, you can further define your warning and critical threshold levels. This determines when an <em>alerts</em> violation is created. Read on to learn more about how to do this. Go to one.newrelic.com, click <em>Alerts</em> &amp; AI"
      },
      "id": "603ef04864441fbc114e8883"
    },
    {
      "sections": [
        "Streaming alerts: key terms and concepts",
        "Why it matters",
        "Streaming alerts process and descriptions",
        "Choose your aggregation method",
        "Event flow",
        "Caution",
        "Event timer",
        "Cadence",
        "Streaming alerts tools",
        "Tip",
        "Window duration",
        "Delay/timer",
        "Loss of signal detection",
        "Gap filling"
      ],
      "title": "Streaming alerts: key terms and concepts",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Get started"
      ],
      "external_id": "144c7f0721ec2acb1e8ac8aa06142911ea552aef",
      "image": "https://docs.newrelic.com/static/39158bd84483adf9516bb7ff5058c9bd/d30ee/streaming-alerts.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/streaming-alerts-key-terms-concepts/",
      "published_at": "2021-12-15T12:47:58Z",
      "updated_at": "2021-12-15T12:47:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The streaming alerts platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into New Relic. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that's processed by the streaming algorithm. There are three methods for aggregating the data filtered through your NRQL condition: Event flow (default) Event timer Cadence This short video describes the three aggregation method (5:31). Why it matters Understanding how streaming alerts works will help you fine-tune your NRQL conditions to be notified about what's important to you. Only data that matches the conditions of the NRQL WHERE clause is alerted on. For more details on each step of the process, see Streaming alerts process and descriptions. As data streams into New Relic, it's filtered by the NRQL condition. Before data is evaluated, it must meet the criteria defined by the NRQL query's WHERE clause. Instead of evaluating that data immediately for violations, the NRQL alert conditions collect the data over a period of time known as the aggregation window. An additional delay/timer allows for slower data points to arrive before the window is aggregated. Once the delay/timer time has elapsed, New Relic aggregates the data into a single data point. Alerts then evaluates the data point using the NRQL condition in order to determine whether it meets the violation threshold criteria. Even if a data point meets the criteria for a violation, a violation may not be triggered. A violation is only triggered when data points consistently meet the threshold criteria over a period of time. This is the threshold duration. If the data points are in violation for an entire threshold duration, we'll send you a notification based on your policy settings. All of these configurable delays give you more control over how you're alerted on sporadic and missing data. Streaming alerts process and descriptions Process Description Streaming data All data coming into New Relic. WHERE clause Filters all incoming streaming data. We only monitor for alerts on data that makes it through this filter. Aggregation methods One of three methods that control how data is collected before it's evaluated. They are: Event flow (Default) Event timer Cadence Aggregation window Data with timestamps that fall within this window will be aggregated and then evaluated. Delay/timer A time delay to ensure all data points have arrived in the aggregation window before aggregation occurs. Aggregated data Data in the aggregated window is collapsed to a single data point for alert evaluation. Evaluation The data point is evaluated by the NRQL condition, which is trigged by each incoming aggregated data point. Threshold duration A specific duration that determines if a violation is created. If your specified NRQL condition meets the threshold criteria over the threshold duration, a violation occurs. When a data point lacks data, a custom value is inserted to fill the gap. Choose your aggregation method You can choose between three different aggregation methods, depending on your needs. Event flow (default) works best for data that comes in frequently and mostly in order. Event timer works best for data that arrives infrequently in batches, such as cloud integration data or infrequent error logs. Cadence is our original and inferior aggregation method. It aggregates data on specific time intervals as detected by New Relic's internal wall clock, regardless of data timestamps. Event flow Event flow aggregates a window of data when the first data point arrives in a subsequent window. The custom delay defines which subsequent window data will start to populate to trigger aggregation of the current window. A custom delay provides extra time for data to arrive. These times are based on the data's timestamps and not New Relic's wall clock time. For example, suppose you're monitoring CPU usage in window durations of 1 minute and a 3 minute delay. When a CPU usage data point comes in with a timestamp between 12:00pm and 12:01pm, event flow will not aggregate that window until a data point shows up with a timestamp between 12:04pm and 12:05pm. When event flow receives the first data point with a timestamp of 12:04pm or later, it sends the 12:00 to 12:01 data to be aggregated. Caution If you expect your data points to arrive more than 30 minutes apart, please use the Event Timer method described below. Event timer Like event flow, event timer will only aggregate data for a given window when data arrives for that window. When a data point arrives for an aggregation window, a timer dedicated to that window starts to count down. If no further data arrives before the timer counts down, the data for that window is aggregated. If more data points arrive before the timer has completed counting down, the timer is reset. For example, suppose you're monitoring CloudWatch data that arrives fairly infrequently. You're using a window duration of 1 minute and a 3 minute timer. When a CloudWatch data point comes in with a timestamp between 12:00pm and 12:01pm, the timer will start to count down. If no further data points show up for that 12:00-12:01 window, the window will get aggregated 3 minutes later. If a new data point with a timestamp between 12:00 and 12:01 arrives, the timer resets. It keeps resetting every time more data points for that window arrive. The window will not be sent for aggregation until the timer reaches 0. If the timer for a later data point elapses before an earlier data point, the event timer method waits for the earlier timer to elapse before aggregating the later data point. For best results, make sure your timer is equal to or longer than your window duration time. If the timer is less than your window duration and your data flow is inconsistent, then your data may be evaluated before all of your data points arrive. This could cause you to be notified incorrectly. Cadence We recommend you use one of the other two methods. Cadence is our old streaming aggregation method. This method uses New Relic's wall clock time to determine when data is aggregated and evaluated. It doesn't take into account data point timestamps as they arrive. Streaming alerts tools Streaming alerts provide a set of tools to give you greater control over how your data is aggregated before it's evaluated to reduce incorrect notifications you receive. They are: Window duration Delay/timer Loss of signal detection Gap filling Tip This article covers these tools at a conceptual level. You'll find direct instructions on how to use these tools in Create NRQL alert conditions. Window duration In order to make loss of signal detection more effective and to reduce unnecessary notifications, you can customize aggregation windows to the duration that you need. An aggregation window is a specific block of time. We gather data points together in an aggregation window, before evaluating the data. A longer aggregation window can smooth out the data, since an outlier data point will have more data points to be aggregated with, giving it less of an influence on the aggregated data point that is sent for evaluation. When a data point arrives, its timestamp is used to put it in the proper aggregation window. You can set your aggregation window to anything between 30 seconds and 15 minutes. The default is 1 minute. Delay/timer The delay/timer setting controls how long the condition should wait before aggregating the data in the aggregation window. The event flow and cadence methods use delay. Event timer uses timer. The delay default is 2 minutes. The timer default is 1 minute and has a minimum value of 5 seconds. Loss of signal detection Loss of signal occurs when no data matches the NRQL condition over a specific period of time. A loss of signal is caused by different things. The WHERE clause in your NRQL query can filter out data before it's evaluated for violations. It could also mean a service or entity is offline or a periodic job has failed to run and no data is being sent to New Relic. In order to avoid unnecessary notifications, you can choose how long to wait before you're notified by a loss of signal violation. You can use loss of signal detection to open violations and be notified when a signal is lost. Alternately, you can use a loss of signal to close violations for ephemeral services or sporadic data, such as error counts. Gap filling Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with the last value received, a static value, or else do nothing and leave the gap there. The default is None. Gaps in streaming data can be caused by network or host issues, a signal may be sparse, or some signals, such as error counts, may only have data when something is wrong. By filling the gaps with known values, the alert evaluation process can process those gaps and determine how they should affect the loss of signal evaluation. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss and gap filling, see this Explorers Hub post.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 229.45612,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "sections": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "The streaming <em>alerts</em> platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into <em>New</em> <em>Relic</em>. You can use NRQL <em>conditions</em> to control what part of the signal you want to be notified about. Your NRQL condition filters the data that&#x27;s processed"
      },
      "id": "6130c054e7b9d269edb6f21d"
    },
    {
      "sections": [
        "Outlier detection (NRQL alert)",
        "Important",
        "What is outlier detection?",
        "Tip",
        "Example use cases",
        "Notify if load-balanced servers have uneven workload",
        "Notify if load-balanced application has misbehaving instances",
        "Notify of changes in different environments",
        "Notify for timezone-related changes",
        "Create an outlier alert condition",
        "Rules and logic",
        "Details about alert condition logic",
        "NRQL query rules and limits",
        "Zero values for unreturned data"
      ],
      "title": "Outlier detection (NRQL alert)",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert conditions"
      ],
      "external_id": "499fa55abd48a0ccdd897fbdf64ccea2d9f98d11",
      "image": "https://docs.newrelic.com/static/f235d0630576bc2010ff07adc7a69621/f73a1/NRQL_outlier_violations.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/outlier-detection-nrql-alert/",
      "published_at": "2021-12-15T12:44:32Z",
      "updated_at": "2021-11-25T19:50:19Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Alerts offers NRQL conditions in three threshold types: static, baseline, and outlier. This document explains how the outlier threshold type works, gives some example use cases and NRQL queries, and explains how to create an outlier condition. Important NRQL alerts do not affect Alerts policies for a Synthetic monitor. For example, muting a NRQL alert will not mute a Synthetic monitor's alerts. What is outlier detection? In software development and operations, it is common to have a group consisting of members you expect to behave approximately the same. For example: for servers using a load balancer, the traffic to the servers may go up or down, but the traffic for all the servers should remain in a fairly tight grouping. See outlier detection in action in this NerdBytes video (2:51 minutes). The NRQL alert outlier detection feature parses the data returned by your faceted NRQL query and: Looks for the number of expected groups that you specify Looks for outliers (values deviating from a group) based on the sensitivity and time range you set Additionally, for queries that have more than one group, you can choose to be notified when groups start behaving the same. This visual aid will help you understand the types of situations that will trigger a violation and those that won't. For more on the rules and logic behind this calculation, see Outlier detection rules. Tip Note: this feature does not take into account the past behavior of the monitored values; it looks for outliers only in the currently reported data. For an alert type that takes into account past behavior, see Baseline alerting. Example use cases These use cases will help you understand when to use the outlier threshold type. Note that the outlier feature requires a NRQL query with a FACET clause. Notify if load-balanced servers have uneven workload A load balancer divides web traffic approximately evenly across five different servers. You can set a notification to be sent if any server starts getting significantly more or less traffic than the other servers. Example query: SELECT average(cpuPercent) FROM SystemSample WHERE apmApplicationNames = 'MY-APP-NAME' FACET hostname Copy Notify if load-balanced application has misbehaving instances Application instances behind a load balancer should have similar throughput, error rates, and response times. If an instance is in a bad state, or a load balancer is misconfigured, this will not be the case. Detecting one or two bad app instances using aggregate metrics may be difficult if there is not a significant rise in the overall error rate of the application. You can set a notification for when an app instance’s throughput, error rate, or response time deviates too far from the rest of the group. Example query: SELECT average(duration) FROM Transaction WHERE appName = 'MY-APP-NAME' FACET host Copy Notify of changes in different environments An application is deployed in two different environments, with ten application instances in each. One environment is experimental and gets more errors than the other. But the instances that are in the same environment should get approximately the same number of errors. You can set a notification for when an instance starts getting more errors than the other instances in the same environment. Also, you can set a notification for when the two environments start to have the same number of errors as each other. Notify for timezone-related changes The number of logged in users for a company is about the same for each of four applications, but varies significantly by each of the three time zones the company operates in. You can set a notification for when any application starts getting more or less traffic from a certain timezone than the other applications. Sometimes the traffic from the different time zones are the same, so you would set up the alert condition to not be notified if the time zone groups overlap. For more details on how this feature works, see Outlier rules and logic. Create an outlier alert condition To create a NRQL alert that uses outlier detection: When creating a condition, under Select a product, select NRQL. For Threshold type, select Outlier. Create a NRQL query with a FACET clause that returns the values you want to alert on. Depending on how the returned values group together, set the Number of expected groups. Adjust the deviation from the center of the group(s) and the duration that will trigger a violation. Optional: Add a warning threshold and set its deviation. Set any remaining available options and save. Rules and logic Here are the rules and logic behind how outlier detection works: Details about alert condition logic After the condition is created, the query is run once every harvest cycle and the condition is applied. Unlike baseline alerts, outlier detection uses no historical data in its calculation; it's calculated using the currently collected data. Alerts will attempt to divide the data returned from the query into the number of groups selected during condition creation. For each group, the approximate average value is calculated. The allowable deviation you have chosen when creating the condition is centered around that average value. If a member of the group is outside the allowed deviation, it produces a violation. If Trigger when groups overlap has been selected, Alerts detects a convergence of groups. If the condition is looking for two or more groups, and the returned values cannot be separated into that number of distinct groups, then that will produce a violation. This type of “overlap” event is represented on a chart by group bands touching. Because this feature does not take past behavior into account, data is never considered to \"belong\" to a certain group. For example, a value that switches places with another value wouldn't trigger a violation. Additionally, an entire group that moves together also wouldn't trigger a violation. NRQL query rules and limits The NRQL query must be a faceted query. The number of unique values returned must be 500 or less. If the query returns more than this number of values, the condition won't be created. If the query later returns more than this number after being created, the alert will fail. Zero values for unreturned data When a query returns a set of values, only values that are actually returned are taken into account. If a value is not available for calculation (including if it goes from being collected one harvest cycle to not being collected), it is rendered as a zero and is not considered. In other words, the behavior of unreturned zero values will never trigger violations.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.94473,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Outlier detection (NRQL <em>alert</em>)",
        "sections": "Create an outlier <em>alert</em> <em>condition</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "<em>Alerts</em> offers NRQL <em>conditions</em> in three threshold types: static, baseline, and outlier. This document explains how the outlier threshold type works, gives some example use cases and NRQL queries, and explains how to create an outlier condition. Important NRQL <em>alerts</em> do not affect <em>Alerts</em> policies"
      },
      "id": "6130be72196a6793654948e7"
    }
  ],
  "/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/multi-location-synthetic-monitoring-alert-conditions": [
    {
      "sections": [
        "Create NRQL alert conditions",
        "Tip",
        "Create a NRQL alert condition",
        "Create a condition from a chart",
        "Caution",
        "NRQL alert syntax",
        "Important",
        "Reformatting incompatible NRQL",
        "NRQL alert threshold examples",
        "Alert on specific segments of your data",
        "Alert on Nth percentile of your data",
        "Alert on max, min, avg of your data",
        "Alert on a percentage of your data",
        "Alert on Apdex with any T-value",
        "NRQL conditions and query order of operations",
        "Example: null value returned",
        "Example: zero value returned",
        "Nested aggregation NRQL alerts",
        "Nested queries with a non-faceted innermost query are not currently supported",
        "Queries at all levels must have the same aggregation window size",
        "Signal loss is not yet supported for nested queries",
        "Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported",
        "NRQL condition creation tips",
        "Condition edits can reset condition evaluation",
        "Alert condition types",
        "Sum of query results (limited or intermittent data)",
        "Set the loss of signal threshold",
        "Advanced signal settings",
        "Aggregation window duration",
        "Delay/timer",
        "Fill data gaps"
      ],
      "title": "Create NRQL alert conditions",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert conditions"
      ],
      "external_id": "2872f770a5fc8a802f99b9f848906f7e351ad761",
      "image": "https://docs.newrelic.com/static/eb8e1b2d826f9fd9cf46fe3dd5455217/c1b63/nr1_nrql_alert_conditions.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/",
      "published_at": "2021-12-19T16:01:17Z",
      "updated_at": "2021-12-14T22:49:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use NRQL queries to create alert conditions. Once you've defined your signal, you can further define your warning and critical threshold levels. This determines when an alerts violation is created. Read on to learn more about how to do this. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Click NRQL, and then Next, define thresholds. Tip For more information on key concepts relating to NRQL alert conditions and streaming alerts, see Streaming alerts: key terms and concepts. Create a NRQL alert condition To create a NRQL alert condition for a policy: On one.newrelic.com, in the header click Alerts & AI, then in the left sidebar click Policies. Select an existing policy or click New alert policy to create a new policy. Click Add a condition. Under Select a product click NRQL, and then click Next, define thresholds. Note that editing an existing condition can result in resetting its evaluation. Create a condition from a chart You can use a chart to create a NRQL alert condition. To create a NRQL alerts condition from a chart, click the chart menu , then click Create alert condition. Once you've named and customized your condition, you can add it to an existing policy or create a new one. Caution A small number of our older charts don't include the option to create an alert condition. NRQL alert syntax Here's the basic syntax for creating all NRQL alert conditions. The FACET clause is required for outlier condition types. It's optional for static and baseline. SELECT function(attribute) FROM Event WHERE attribute [comparison] [AND|OR ...] Copy Clause Notes SELECT function(attribute) Required Supported functions that return numbers include: apdex average count latest max min percentage percentile sum uniqueCount Tip If you use the percentile aggregator in a faceted alert condition with many facets, this may cause the following error to appear: An error occurred while fetching chart data. If you see this error, use average instead. FROM data type Required Only one data type can be targeted. Supported data types: Event Metric (RAW data points will be returned) WHERE attribute [comparison] [AND|OR ...] Use the WHERE clause to specify a series of one or more conditions. All the operators are supported. FACET attribute Required for outlier conditions Include an optional FACET clause in your NRQL syntax depending on the threshold type: static or baseline. Use the FACET clause to separate your results by attribute and alert on each attribute independently. No LIMIT clause is allowed, but all queries will receive the maximum number of facets possible. Faceted queries can return a maximum of 5000 values for static and baseline conditions and a maximum of 500 values for outlier conditions. Important If the query returns more than the maximum number of values, the alert condition can't be created. If you create the condition and the query returns more than this number later, the alert will fail. Modify your query so that it returns a fewer number of values. Reformatting incompatible NRQL Some elements of NRQL used in charts don’t make sense in the streaming context of alerts. Here’s a list of the most common incompatible elements and suggestions for reformatting a NRQL alert query to achieve the same effect. Element Notes SINCE and UNTIL Example: SELECT percentile(largestContentfulPaint, 75) FROM PageViewTiming WHERE (appId = 837807) SINCE yesterday Copy NRQL conditions produce a never-ending stream of windowed query results, so the SINCE and UNTIL keywords to scope the query to a point in time are not compatible. As a convenience, we automatically strip SINCE and UNTIL from a query when creating a condition from the context of a chart. TIMESERIES In NRQL queries, the TIMESERIES clause is used to return data as a time series broken out by a specified period of time. For NRQL conditions, the equivalent property of a signal is the aggregation duration window. histogram() The histogram() aggregation function is used to generate histograms. histogram() is not compatible with NRQL alerting: histogram aggregations can not be formatted as a time series. To create an alert from a portion of a histogram (for example, 95th percentile), use the percentile() aggregation function. Multiple aggregation functions Each condition can only target a single aggregated value. To alert on multiple values simultaneously, you’ll need to decompose them into individual conditions within the same policy. Original Query: SELECT count(foo), average(bar), max(baz) from Transaction Copy Decomposed: SELECT count(foo) from Transaction SELECT average(bar) from Transaction SELECT max(baz) from Transaction Copy COMPARE WITH The COMPARE WITH clause is used to compare the values for two different time ranges. This type of query is incompatible with NRQL alerting. We recommend using a Baseline Alert Condition to dynamically detect deviations for a particular signal. SLIDE BY The SLIDE BY clause supports a feature known as sliding windows. With sliding windows, SLIDE BY data is gathered into \"windows\" of time that overlap with each other. These windows can help to smooth out line graphs with a lot of variation in cases where the rolling aggregate (such as a rolling mean) is more important than aggregates from narrow windows of time. Sliding windows are not currently supported in NRQL alerts. LIMIT In NRQL queries, the LIMIT clause is used to control the amount of data a query returns, either the maximum number of facet values returned by FACET queries or the maximum number of items returned by SELECT * queries. LIMIT is not compatible with NRQL alerting: evaluation is always performed on the full result set. NRQL alert threshold examples Here are some common use cases for NRQL conditions. These queries will work for static and baseline condition types. The outlier condition type will require additional FACET clauses. Alert on specific segments of your data Create constrained alerts that target a specific segment of your data, such as a few key customers or a range of data. Use the WHERE clause to define those conditions. SELECT average(duration) FROM Transaction WHERE account_id in (91290, 102021, 20230) Copy SELECT percentile(duration, 95) FROM Transaction WHERE name LIKE 'Controller/checkout/%' Copy Alert on Nth percentile of your data Create alerts when an Nth percentile of your data hits a specified threshold; for example, maintaining SLA service levels. Since we evaluate the NRQL query based on the aggregation window duration, percentiles will be calculated for each duration separately. SELECT percentile(duration, 95) FROM Transaction Copy SELECT percentile(databaseDuration, 75) FROM Transaction Copy Alert on max, min, avg of your data Create alerts when your data hits a certain maximum, minimum, or average; for example, ensuring that a duration or response time does not pass a certain threshold. SELECT max(duration) FROM Transaction Copy SELECT average(duration) FROM Transaction Copy Alert on a percentage of your data Create alerts when a proportion of your data goes above or below a certain threshold. SELECT percentage(count(*), WHERE duration > 2) FROM Transaction Copy SELECT percentage(count(*), WHERE httpResponseCode = '500') FROM Transaction Copy Alert on Apdex with any T-value Create alerts on Apdex, applying your own T-value for certain transactions. For example, get an alert notification when your Apdex for a T-value of 500ms on transactions for production apps goes below 0.8. SELECT apdex(duration, t:0.5) FROM Transaction WHERE appName like '%prod%' Copy NRQL conditions and query order of operations By default, the aggregation window duration is 1 minute, but you can change the window to suit your needs. Whatever the aggregation window, New Relic will collect data for that window using the function in the NRQL condition’s query. The query is parsed and executed by our systems in the following order: FROM clause – which event type needs to be grabbed? WHERE clause – what can be filtered out? SELECT clause – what information needs to be returned from the now-filtered data set? Example: null value returned Let's say this is your alert condition query: SELECT count(*) FROM SyntheticCheck WHERE monitorName = 'My Cool Monitor' AND result = 'FAILURE' Copy If there are no failures for the aggregation window: The system will execute the FROM clause by grabbing all SyntheticCheck events on your account. Then it will execute the WHERE clause to filter through those events by looking only for the ones that match the monitor name and result specified. If there are still events left to scan through after completing the FROM and WHERE operations, the SELECT clause will be executed. If there are no remaining events, the SELECT clause will not be executed. This means that aggregators like count() and uniqueCount() will never return a zero value. When there is a count of 0, the SELECT clause is ignored and no data is returned, resulting in a value of NULL. Example: zero value returned If you have a data source delivering legitimate numeric zeroes, the query will return zero values and not null values. Let's say this is your alert condition query, and that MyCoolEvent is an attribute that can sometimes return a zero value. SELECT average(MyCoolAttribute) FROM MyCoolEvent Copy If, in the aggregation window being evaluated, there's at least one instance of MyCoolEvent and if the average value of all MyCoolAttribute attributes from that window is equal to zero, then a 0 value will be returned. If there are no MyCoolEvent events during that minute, then a NULL will be returned due to the order of operations. Tip For more information about this topic, you can check out our blog post on troubleshooting for zero versus null values. Tip You can determine how null values will be handled by adjusting loss of signal and gap filling settings in the Alert Conditions UI. Tip You can avoid NULL values entirely with a query order of operations shortcut. Do this by using a filter sub-clause, then including all filter elements within that sub-clause. The main body of the query will run and return data, at which point the SELECT clause will then run and apply the filter elements. The query will return a value of 0 if the filter elements result in no matching data. Here's an example: SELECT filter(count(*), WHERE result = 'SUCCESS' AND monitorName = 'My Favorite Monitor') FROM SyntheticCheck Copy Nested aggregation NRQL alerts Nested aggregation queries are a powerful way to query your data. However, they have a few restrictions that are important to note. Nested queries with a non-faceted innermost query are not currently supported Without a FACET, the inner query produces a single result, giving the outer query nothing to aggregate. If you're using a nested query, make sure your inner query is faceted. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu) ​​​​​ Copy Queries at all levels must have the same aggregation window size With an alert aggregation window of 1 minute, the inner query would produce two smaller windows of 30 seconds. In theory, these two windows could be aggregated by the outer query. However, this is not currently supported. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu TIMESERIES 30 seconds)​​ Copy Signal loss is not yet supported for nested queries For more information on signal loss, see NerdGraph API: Loss of signal and gap filling. Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported You can't use a nested query containing the WITH METRIC_FORMAT in the inner query to create NRQL alert conditions. NRQL condition creation tips Here are some tips for creating and using a NRQL condition: Topic Tips Condition types NRQL condition types include static, baseline, and outlier. Create a description For NRQL conditions, you can create a custom description to add to each violation. Descriptions can be enhanced with variable substitution based on metadata in the specific violation. For details, see Description Query results Queries must return a number. The condition evaluates the returned number against the thresholds you've set. Time period NRQL conditions evaluate data based on how it's aggregated, using aggregation windows from 30 seconds to 15 minutes, in increments of 15 seconds. For best results, we recommend using the event flow or event timer aggregation methods. For the cadence aggregation method, the implicit SINCE ... UNTIL clause specifying which minute to evaluate is controlled by your delay/timer setting. Since very recent data may be incomplete, you may want to query data from 3 minutes ago or longer, especially for: Applications that run on multiple hosts. SyntheticCheck data: Timeouts can take 3 minutes, so 5 minutes or more is recommended. Also, if a query will generate intermittent data, consider using the sum of query results option. Lost signal threshold (loss of signal detection) You can use loss of signal detection to alert on when your data (a telemetry signal) should be considered lost. A signal loss can indicate that a service or entity is no longer online or that a periodic job failed to run. You can also use this to make sure that violations for sporadic data, such as error counts, are closed when no signal is coming in. Advanced signal settings These settings give you options for better handling continuous, streaming data signals that may sometimes be missing. These settings include the aggregation window duration, the delay/timer, and an option for filling data gaps. For more on using these, see Advanced signal settings. Condition settings Use the Condition settings to: Create a concise, descriptive condition name. Provide a custom violation description for the condition that will be included in violations and notifications. Add the runbook URL to include your organization's procedures for handling incidents. You may also add this information to the custom violation description. Limits on conditions See the maximum values. Health status In order for a NRQL alert condition health status display to function properly, use a FACET clause to scope each signal to a single entity (for example, FACET hostname or FACET appname). Examples For more information, see: Expected NRQL syntax Examples of NRQL condition queries Condition edits can reset condition evaluation When you edit NRQL alert conditions in some specific ways (detailed below), their evaluations are reset, meaning that any evaluation up until that point is lost, and the evaluation starts over from that point. The two ways this will affect you are: For \"for at least x minutes\" thresholds: because the evaluation window has been reset, there will be a delay of at least x minutes before any violations can be reported. For baseline conditions: the condition starts over again and all baseline learning is lost. The following actions cause an evaluation reset for NRQL conditions: Changing the query Changing the aggregation window, aggregation method, or aggregation delay/timer setting Changing the \"close violations on signal loss\" setting Changing any gap fill settings Changing the baseline direction (if applicable) – higher, lower, or higher/lower Change the threshold value, threshold window, or threshold operator The following actions (along with any other actions not covered in the above list) will not reset the evaluation: Changing the loss of signal time window (expiration duration) Changing the time function (switching \"for at least\" to \"at least once in,\" or vice-versa) Toggling the \"open violation on signal loss\" setting Alert condition types When you create a NRQL alert, you can choose from different types of conditions: NRQL alert condition types Description Static This is the simplest type of NRQL condition. It allows you to create a condition based on a NRQL query that returns a numeric value. Optional: Include a FACET clause. Baseline (Dynamic) Uses a self-adjusting condition based on the past behavior of the monitored values. Uses the same NRQL query form as the static type, including the optional FACET clause. Outlier Looks for group behavior and values that are outliers from those groups. Uses the same NRQL query form as the static type, but requires a FACET clause. Sum of query results (limited or intermittent data) Important Available only for static (basic) condition types. If a query returns intermittent or limited data, it may be difficult to set a meaningful threshold. Missing or limited data will sometimes generate false positives or false negatives. You can use loss of signal, aggregation duration, and gap filling settings to minimize these false notifications. To avoid this problem when using the static threshold type, you can set the selector to sum of query results. This lets you set the alert on an aggregated sum instead of a value from a single harvest cycle. Up to two hours of one-minute data checks can be aggregated. The duration you select determines the width of the rolling sum and the preview chart will update accordingly. Set the loss of signal threshold Loss of signal occurs when no data matches the NRQL condition over a specific period of time. You can set your loss of signal threshold duration and also what happens when the threshold is crossed. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Loss of signal is only available for NRQL conditions. You may also manage these settings using the GraphQL API (recommended), or the REST API. Go here for specific GraphQL API examples. Loss of signal settings: Loss of signal settings include a time duration and two possible actions. Signal loss expiration time UI label: Signal is lost after: GraphQL Node: expiration.expirationDuration Expiration duration is a timer that starts and resets when we receive a data point in the streaming alerts pipeline. If we don't receive another data point before your 'expiration time' expires, we consider that signal to be lost. This can be because no data is being sent to New Relic or the WHERE clause of your NRQL query is filtering that data out before it is streamed to the alerts pipeline. Note that when you have a faceted query, each facet is a signal. So if any one of those signals ends during the duration specified, that will be considered a loss of signal. The loss of signal expiration time is independent of the threshold duration and triggers as soon as the timer expires. The maximum expiration duration is 48 hours. This is helpful when monitoring for the execution of infrequent jobs. The minimum is 30 seconds, but we recommend using at least 3-5 minutes. Loss of signal actions Once a signal is considered lost, you can close open violations, open new violations, or both. Close all current open violations: This closes all open violations that are related to a specific signal. It won't necessarily close all violations for a condition. If you're alerting on an ephemeral service, or on a sporadic signal, you'll want to choose this action to ensure that violations are closed properly. The GraphQL node name for this is \"closeViolationsOnExpiration\" Open new violations: This will open a new violation when the signal is considered lost. These violations will indicate that they are due to a loss of signal. Based on your incident preferences, this should trigger a notification. The graphQL node name for this is \"openViolationOnExpiration\" When you enable both actions, we'll close all open violations first, and then open a new violation for loss of signal. To create a NRQL alert configured with loss of signal detection in the UI: For a policy, when you create a condition, under Select a product, click NRQL, then click Next, define thresholds. Write a NRQL query that returns the values you want to alert on. For Threshold type, select Static or Baseline. Click + Add lost signal threshold, then set the signal expiration duration time in minutes or seconds in the Signal is lost after field. Choose what you want to happen when the signal is lost. You can check one or both of Close all current open violations and Open new \"lost signal\" violation. These control how loss of signal violations will be handled for the condition. Make sure you name your condition before you save it. Violations open due to loss of signal close when the signal comes back. Newly opened lost signal violations will close immediately when new data is evaluated. the condition they belong to expires. By default, conditions expire after 3 days. you manually close the violation with the Close all current open violations option. Tip Loss of signal detection doesn't work on NRQL queries that use nested aggregation or sub-queries. Advanced signal settings When creating a NRQL alert condition, use the advanced signal settings to control streaming alert data and avoid false alarms. When creating a NRQL condition, there are several advanced signal settings: Aggregation window duration Delay/timer Fill data gaps To read an explanation of what these settings are and how they relate to each other, see Streaming alerts concepts. Below are instructions and tips on how to configure them. Aggregation window duration You can set the aggregation window duration to choose how long data is accumulated in a streaming time window before it's aggregated. You can set it to anything between 30 seconds and 15 minutes. The default is one minute. Tip Baseline alert condition thresholds don't support editing the aggregation window. They use the 1 minute default. Delay/timer You can adjust the delay/timer to coordinate our streaming alerting algorithm with your data's behavior. If your data is sparse or inconsistent, you may want to use the event timer aggregation method. For the cadence method, the total supported latency is the sum of the aggregation window duration and the delay. If the data type comes from an APM language agent and is aggregated from many app instances (for example, Transactions, TransactionErrors, etc.), we recommend using the event flow method with the default settings. Important When creating NRQL conditions for data collected from Infrastructure Cloud Integrations such as AWS Cloudwatch or Azure, we recommend that you use the event timer method. Fill data gaps Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with one of these settings: None: (Default) Choose this if you don't want to take any action on empty aggregation windows. On evaluation, an empty aggregation window will reset the threshold duration timer. For example, if a condition says that all aggregation windows must have data points above the threshold for 5 minutes, and 1 of the 5 aggregation windows is empty, then the condition won't be in violation. Custom static value: Choose this if you'd like to insert a custom static value into the empty aggregation windows before they're evaluated. This option has an additional, required parameter of fillValue (as named in the API) that specifies what static value should be used. This defaults to 0. Last known value: This option inserts the last seen value before evaluation occurs. We maintain the state of the last seen value for 2 hours. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss, gap filling, and how to request access to these features, see this Explorers Hub post. Options for editing data gap settings: In the NRQL conditions UI, go to Condition settings > Advanced signal settings > fill data gaps with and choose an option. If using our Nerdgraph API (preferred), this node is located at: actor : account : alerts : nrqlCondition : signal : fillOption | fillValue NerdGraph is our recommended API for this but if you're using our REST API, you can find this setting in the REST API explorer under the \"signal\" section of the Alert NRQL conditions API.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 251.8564,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create NRQL <em>alert</em> <em>conditions</em>",
        "sections": "Create NRQL <em>alert</em> <em>conditions</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "You can use NRQL queries to create <em>alert</em> <em>conditions</em>. Once you&#x27;ve defined your signal, you can further define your warning and critical threshold levels. This determines when an <em>alerts</em> violation is created. Read on to learn more about how to do this. Go to one.newrelic.com, click <em>Alerts</em> &amp; AI"
      },
      "id": "603ef04864441fbc114e8883"
    },
    {
      "sections": [
        "Streaming alerts: key terms and concepts",
        "Why it matters",
        "Streaming alerts process and descriptions",
        "Choose your aggregation method",
        "Event flow",
        "Caution",
        "Event timer",
        "Cadence",
        "Streaming alerts tools",
        "Tip",
        "Window duration",
        "Delay/timer",
        "Loss of signal detection",
        "Gap filling"
      ],
      "title": "Streaming alerts: key terms and concepts",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Get started"
      ],
      "external_id": "144c7f0721ec2acb1e8ac8aa06142911ea552aef",
      "image": "https://docs.newrelic.com/static/39158bd84483adf9516bb7ff5058c9bd/d30ee/streaming-alerts.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/streaming-alerts-key-terms-concepts/",
      "published_at": "2021-12-15T12:47:58Z",
      "updated_at": "2021-12-15T12:47:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The streaming alerts platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into New Relic. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that's processed by the streaming algorithm. There are three methods for aggregating the data filtered through your NRQL condition: Event flow (default) Event timer Cadence This short video describes the three aggregation method (5:31). Why it matters Understanding how streaming alerts works will help you fine-tune your NRQL conditions to be notified about what's important to you. Only data that matches the conditions of the NRQL WHERE clause is alerted on. For more details on each step of the process, see Streaming alerts process and descriptions. As data streams into New Relic, it's filtered by the NRQL condition. Before data is evaluated, it must meet the criteria defined by the NRQL query's WHERE clause. Instead of evaluating that data immediately for violations, the NRQL alert conditions collect the data over a period of time known as the aggregation window. An additional delay/timer allows for slower data points to arrive before the window is aggregated. Once the delay/timer time has elapsed, New Relic aggregates the data into a single data point. Alerts then evaluates the data point using the NRQL condition in order to determine whether it meets the violation threshold criteria. Even if a data point meets the criteria for a violation, a violation may not be triggered. A violation is only triggered when data points consistently meet the threshold criteria over a period of time. This is the threshold duration. If the data points are in violation for an entire threshold duration, we'll send you a notification based on your policy settings. All of these configurable delays give you more control over how you're alerted on sporadic and missing data. Streaming alerts process and descriptions Process Description Streaming data All data coming into New Relic. WHERE clause Filters all incoming streaming data. We only monitor for alerts on data that makes it through this filter. Aggregation methods One of three methods that control how data is collected before it's evaluated. They are: Event flow (Default) Event timer Cadence Aggregation window Data with timestamps that fall within this window will be aggregated and then evaluated. Delay/timer A time delay to ensure all data points have arrived in the aggregation window before aggregation occurs. Aggregated data Data in the aggregated window is collapsed to a single data point for alert evaluation. Evaluation The data point is evaluated by the NRQL condition, which is trigged by each incoming aggregated data point. Threshold duration A specific duration that determines if a violation is created. If your specified NRQL condition meets the threshold criteria over the threshold duration, a violation occurs. When a data point lacks data, a custom value is inserted to fill the gap. Choose your aggregation method You can choose between three different aggregation methods, depending on your needs. Event flow (default) works best for data that comes in frequently and mostly in order. Event timer works best for data that arrives infrequently in batches, such as cloud integration data or infrequent error logs. Cadence is our original and inferior aggregation method. It aggregates data on specific time intervals as detected by New Relic's internal wall clock, regardless of data timestamps. Event flow Event flow aggregates a window of data when the first data point arrives in a subsequent window. The custom delay defines which subsequent window data will start to populate to trigger aggregation of the current window. A custom delay provides extra time for data to arrive. These times are based on the data's timestamps and not New Relic's wall clock time. For example, suppose you're monitoring CPU usage in window durations of 1 minute and a 3 minute delay. When a CPU usage data point comes in with a timestamp between 12:00pm and 12:01pm, event flow will not aggregate that window until a data point shows up with a timestamp between 12:04pm and 12:05pm. When event flow receives the first data point with a timestamp of 12:04pm or later, it sends the 12:00 to 12:01 data to be aggregated. Caution If you expect your data points to arrive more than 30 minutes apart, please use the Event Timer method described below. Event timer Like event flow, event timer will only aggregate data for a given window when data arrives for that window. When a data point arrives for an aggregation window, a timer dedicated to that window starts to count down. If no further data arrives before the timer counts down, the data for that window is aggregated. If more data points arrive before the timer has completed counting down, the timer is reset. For example, suppose you're monitoring CloudWatch data that arrives fairly infrequently. You're using a window duration of 1 minute and a 3 minute timer. When a CloudWatch data point comes in with a timestamp between 12:00pm and 12:01pm, the timer will start to count down. If no further data points show up for that 12:00-12:01 window, the window will get aggregated 3 minutes later. If a new data point with a timestamp between 12:00 and 12:01 arrives, the timer resets. It keeps resetting every time more data points for that window arrive. The window will not be sent for aggregation until the timer reaches 0. If the timer for a later data point elapses before an earlier data point, the event timer method waits for the earlier timer to elapse before aggregating the later data point. For best results, make sure your timer is equal to or longer than your window duration time. If the timer is less than your window duration and your data flow is inconsistent, then your data may be evaluated before all of your data points arrive. This could cause you to be notified incorrectly. Cadence We recommend you use one of the other two methods. Cadence is our old streaming aggregation method. This method uses New Relic's wall clock time to determine when data is aggregated and evaluated. It doesn't take into account data point timestamps as they arrive. Streaming alerts tools Streaming alerts provide a set of tools to give you greater control over how your data is aggregated before it's evaluated to reduce incorrect notifications you receive. They are: Window duration Delay/timer Loss of signal detection Gap filling Tip This article covers these tools at a conceptual level. You'll find direct instructions on how to use these tools in Create NRQL alert conditions. Window duration In order to make loss of signal detection more effective and to reduce unnecessary notifications, you can customize aggregation windows to the duration that you need. An aggregation window is a specific block of time. We gather data points together in an aggregation window, before evaluating the data. A longer aggregation window can smooth out the data, since an outlier data point will have more data points to be aggregated with, giving it less of an influence on the aggregated data point that is sent for evaluation. When a data point arrives, its timestamp is used to put it in the proper aggregation window. You can set your aggregation window to anything between 30 seconds and 15 minutes. The default is 1 minute. Delay/timer The delay/timer setting controls how long the condition should wait before aggregating the data in the aggregation window. The event flow and cadence methods use delay. Event timer uses timer. The delay default is 2 minutes. The timer default is 1 minute and has a minimum value of 5 seconds. Loss of signal detection Loss of signal occurs when no data matches the NRQL condition over a specific period of time. A loss of signal is caused by different things. The WHERE clause in your NRQL query can filter out data before it's evaluated for violations. It could also mean a service or entity is offline or a periodic job has failed to run and no data is being sent to New Relic. In order to avoid unnecessary notifications, you can choose how long to wait before you're notified by a loss of signal violation. You can use loss of signal detection to open violations and be notified when a signal is lost. Alternately, you can use a loss of signal to close violations for ephemeral services or sporadic data, such as error counts. Gap filling Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with the last value received, a static value, or else do nothing and leave the gap there. The default is None. Gaps in streaming data can be caused by network or host issues, a signal may be sparse, or some signals, such as error counts, may only have data when something is wrong. By filling the gaps with known values, the alert evaluation process can process those gaps and determine how they should affect the loss of signal evaluation. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss and gap filling, see this Explorers Hub post.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 229.456,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "sections": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "The streaming <em>alerts</em> platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into <em>New</em> <em>Relic</em>. You can use NRQL <em>conditions</em> to control what part of the signal you want to be notified about. Your NRQL condition filters the data that&#x27;s processed"
      },
      "id": "6130c054e7b9d269edb6f21d"
    },
    {
      "sections": [
        "Outlier detection (NRQL alert)",
        "Important",
        "What is outlier detection?",
        "Tip",
        "Example use cases",
        "Notify if load-balanced servers have uneven workload",
        "Notify if load-balanced application has misbehaving instances",
        "Notify of changes in different environments",
        "Notify for timezone-related changes",
        "Create an outlier alert condition",
        "Rules and logic",
        "Details about alert condition logic",
        "NRQL query rules and limits",
        "Zero values for unreturned data"
      ],
      "title": "Outlier detection (NRQL alert)",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert conditions"
      ],
      "external_id": "499fa55abd48a0ccdd897fbdf64ccea2d9f98d11",
      "image": "https://docs.newrelic.com/static/f235d0630576bc2010ff07adc7a69621/f73a1/NRQL_outlier_violations.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/outlier-detection-nrql-alert/",
      "published_at": "2021-12-15T12:44:32Z",
      "updated_at": "2021-11-25T19:50:19Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Alerts offers NRQL conditions in three threshold types: static, baseline, and outlier. This document explains how the outlier threshold type works, gives some example use cases and NRQL queries, and explains how to create an outlier condition. Important NRQL alerts do not affect Alerts policies for a Synthetic monitor. For example, muting a NRQL alert will not mute a Synthetic monitor's alerts. What is outlier detection? In software development and operations, it is common to have a group consisting of members you expect to behave approximately the same. For example: for servers using a load balancer, the traffic to the servers may go up or down, but the traffic for all the servers should remain in a fairly tight grouping. See outlier detection in action in this NerdBytes video (2:51 minutes). The NRQL alert outlier detection feature parses the data returned by your faceted NRQL query and: Looks for the number of expected groups that you specify Looks for outliers (values deviating from a group) based on the sensitivity and time range you set Additionally, for queries that have more than one group, you can choose to be notified when groups start behaving the same. This visual aid will help you understand the types of situations that will trigger a violation and those that won't. For more on the rules and logic behind this calculation, see Outlier detection rules. Tip Note: this feature does not take into account the past behavior of the monitored values; it looks for outliers only in the currently reported data. For an alert type that takes into account past behavior, see Baseline alerting. Example use cases These use cases will help you understand when to use the outlier threshold type. Note that the outlier feature requires a NRQL query with a FACET clause. Notify if load-balanced servers have uneven workload A load balancer divides web traffic approximately evenly across five different servers. You can set a notification to be sent if any server starts getting significantly more or less traffic than the other servers. Example query: SELECT average(cpuPercent) FROM SystemSample WHERE apmApplicationNames = 'MY-APP-NAME' FACET hostname Copy Notify if load-balanced application has misbehaving instances Application instances behind a load balancer should have similar throughput, error rates, and response times. If an instance is in a bad state, or a load balancer is misconfigured, this will not be the case. Detecting one or two bad app instances using aggregate metrics may be difficult if there is not a significant rise in the overall error rate of the application. You can set a notification for when an app instance’s throughput, error rate, or response time deviates too far from the rest of the group. Example query: SELECT average(duration) FROM Transaction WHERE appName = 'MY-APP-NAME' FACET host Copy Notify of changes in different environments An application is deployed in two different environments, with ten application instances in each. One environment is experimental and gets more errors than the other. But the instances that are in the same environment should get approximately the same number of errors. You can set a notification for when an instance starts getting more errors than the other instances in the same environment. Also, you can set a notification for when the two environments start to have the same number of errors as each other. Notify for timezone-related changes The number of logged in users for a company is about the same for each of four applications, but varies significantly by each of the three time zones the company operates in. You can set a notification for when any application starts getting more or less traffic from a certain timezone than the other applications. Sometimes the traffic from the different time zones are the same, so you would set up the alert condition to not be notified if the time zone groups overlap. For more details on how this feature works, see Outlier rules and logic. Create an outlier alert condition To create a NRQL alert that uses outlier detection: When creating a condition, under Select a product, select NRQL. For Threshold type, select Outlier. Create a NRQL query with a FACET clause that returns the values you want to alert on. Depending on how the returned values group together, set the Number of expected groups. Adjust the deviation from the center of the group(s) and the duration that will trigger a violation. Optional: Add a warning threshold and set its deviation. Set any remaining available options and save. Rules and logic Here are the rules and logic behind how outlier detection works: Details about alert condition logic After the condition is created, the query is run once every harvest cycle and the condition is applied. Unlike baseline alerts, outlier detection uses no historical data in its calculation; it's calculated using the currently collected data. Alerts will attempt to divide the data returned from the query into the number of groups selected during condition creation. For each group, the approximate average value is calculated. The allowable deviation you have chosen when creating the condition is centered around that average value. If a member of the group is outside the allowed deviation, it produces a violation. If Trigger when groups overlap has been selected, Alerts detects a convergence of groups. If the condition is looking for two or more groups, and the returned values cannot be separated into that number of distinct groups, then that will produce a violation. This type of “overlap” event is represented on a chart by group bands touching. Because this feature does not take past behavior into account, data is never considered to \"belong\" to a certain group. For example, a value that switches places with another value wouldn't trigger a violation. Additionally, an entire group that moves together also wouldn't trigger a violation. NRQL query rules and limits The NRQL query must be a faceted query. The number of unique values returned must be 500 or less. If the query returns more than this number of values, the condition won't be created. If the query later returns more than this number after being created, the alert will fail. Zero values for unreturned data When a query returns a set of values, only values that are actually returned are taken into account. If a value is not available for calculation (including if it goes from being collected one harvest cycle to not being collected), it is rendered as a zero and is not considered. In other words, the behavior of unreturned zero values will never trigger violations.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.9447,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Outlier detection (NRQL <em>alert</em>)",
        "sections": "Create an outlier <em>alert</em> <em>condition</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "<em>Alerts</em> offers NRQL <em>conditions</em> in three threshold types: static, baseline, and outlier. This document explains how the outlier threshold type works, gives some example use cases and NRQL queries, and explains how to create an outlier condition. Important NRQL <em>alerts</em> do not affect <em>Alerts</em> policies"
      },
      "id": "6130be72196a6793654948e7"
    }
  ],
  "/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/outlier-detection-nrql-alert": [
    {
      "sections": [
        "Create NRQL alert conditions",
        "Tip",
        "Create a NRQL alert condition",
        "Create a condition from a chart",
        "Caution",
        "NRQL alert syntax",
        "Important",
        "Reformatting incompatible NRQL",
        "NRQL alert threshold examples",
        "Alert on specific segments of your data",
        "Alert on Nth percentile of your data",
        "Alert on max, min, avg of your data",
        "Alert on a percentage of your data",
        "Alert on Apdex with any T-value",
        "NRQL conditions and query order of operations",
        "Example: null value returned",
        "Example: zero value returned",
        "Nested aggregation NRQL alerts",
        "Nested queries with a non-faceted innermost query are not currently supported",
        "Queries at all levels must have the same aggregation window size",
        "Signal loss is not yet supported for nested queries",
        "Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported",
        "NRQL condition creation tips",
        "Condition edits can reset condition evaluation",
        "Alert condition types",
        "Sum of query results (limited or intermittent data)",
        "Set the loss of signal threshold",
        "Advanced signal settings",
        "Aggregation window duration",
        "Delay/timer",
        "Fill data gaps"
      ],
      "title": "Create NRQL alert conditions",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert conditions"
      ],
      "external_id": "2872f770a5fc8a802f99b9f848906f7e351ad761",
      "image": "https://docs.newrelic.com/static/eb8e1b2d826f9fd9cf46fe3dd5455217/c1b63/nr1_nrql_alert_conditions.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/",
      "published_at": "2021-12-19T16:01:17Z",
      "updated_at": "2021-12-14T22:49:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use NRQL queries to create alert conditions. Once you've defined your signal, you can further define your warning and critical threshold levels. This determines when an alerts violation is created. Read on to learn more about how to do this. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Click NRQL, and then Next, define thresholds. Tip For more information on key concepts relating to NRQL alert conditions and streaming alerts, see Streaming alerts: key terms and concepts. Create a NRQL alert condition To create a NRQL alert condition for a policy: On one.newrelic.com, in the header click Alerts & AI, then in the left sidebar click Policies. Select an existing policy or click New alert policy to create a new policy. Click Add a condition. Under Select a product click NRQL, and then click Next, define thresholds. Note that editing an existing condition can result in resetting its evaluation. Create a condition from a chart You can use a chart to create a NRQL alert condition. To create a NRQL alerts condition from a chart, click the chart menu , then click Create alert condition. Once you've named and customized your condition, you can add it to an existing policy or create a new one. Caution A small number of our older charts don't include the option to create an alert condition. NRQL alert syntax Here's the basic syntax for creating all NRQL alert conditions. The FACET clause is required for outlier condition types. It's optional for static and baseline. SELECT function(attribute) FROM Event WHERE attribute [comparison] [AND|OR ...] Copy Clause Notes SELECT function(attribute) Required Supported functions that return numbers include: apdex average count latest max min percentage percentile sum uniqueCount Tip If you use the percentile aggregator in a faceted alert condition with many facets, this may cause the following error to appear: An error occurred while fetching chart data. If you see this error, use average instead. FROM data type Required Only one data type can be targeted. Supported data types: Event Metric (RAW data points will be returned) WHERE attribute [comparison] [AND|OR ...] Use the WHERE clause to specify a series of one or more conditions. All the operators are supported. FACET attribute Required for outlier conditions Include an optional FACET clause in your NRQL syntax depending on the threshold type: static or baseline. Use the FACET clause to separate your results by attribute and alert on each attribute independently. No LIMIT clause is allowed, but all queries will receive the maximum number of facets possible. Faceted queries can return a maximum of 5000 values for static and baseline conditions and a maximum of 500 values for outlier conditions. Important If the query returns more than the maximum number of values, the alert condition can't be created. If you create the condition and the query returns more than this number later, the alert will fail. Modify your query so that it returns a fewer number of values. Reformatting incompatible NRQL Some elements of NRQL used in charts don’t make sense in the streaming context of alerts. Here’s a list of the most common incompatible elements and suggestions for reformatting a NRQL alert query to achieve the same effect. Element Notes SINCE and UNTIL Example: SELECT percentile(largestContentfulPaint, 75) FROM PageViewTiming WHERE (appId = 837807) SINCE yesterday Copy NRQL conditions produce a never-ending stream of windowed query results, so the SINCE and UNTIL keywords to scope the query to a point in time are not compatible. As a convenience, we automatically strip SINCE and UNTIL from a query when creating a condition from the context of a chart. TIMESERIES In NRQL queries, the TIMESERIES clause is used to return data as a time series broken out by a specified period of time. For NRQL conditions, the equivalent property of a signal is the aggregation duration window. histogram() The histogram() aggregation function is used to generate histograms. histogram() is not compatible with NRQL alerting: histogram aggregations can not be formatted as a time series. To create an alert from a portion of a histogram (for example, 95th percentile), use the percentile() aggregation function. Multiple aggregation functions Each condition can only target a single aggregated value. To alert on multiple values simultaneously, you’ll need to decompose them into individual conditions within the same policy. Original Query: SELECT count(foo), average(bar), max(baz) from Transaction Copy Decomposed: SELECT count(foo) from Transaction SELECT average(bar) from Transaction SELECT max(baz) from Transaction Copy COMPARE WITH The COMPARE WITH clause is used to compare the values for two different time ranges. This type of query is incompatible with NRQL alerting. We recommend using a Baseline Alert Condition to dynamically detect deviations for a particular signal. SLIDE BY The SLIDE BY clause supports a feature known as sliding windows. With sliding windows, SLIDE BY data is gathered into \"windows\" of time that overlap with each other. These windows can help to smooth out line graphs with a lot of variation in cases where the rolling aggregate (such as a rolling mean) is more important than aggregates from narrow windows of time. Sliding windows are not currently supported in NRQL alerts. LIMIT In NRQL queries, the LIMIT clause is used to control the amount of data a query returns, either the maximum number of facet values returned by FACET queries or the maximum number of items returned by SELECT * queries. LIMIT is not compatible with NRQL alerting: evaluation is always performed on the full result set. NRQL alert threshold examples Here are some common use cases for NRQL conditions. These queries will work for static and baseline condition types. The outlier condition type will require additional FACET clauses. Alert on specific segments of your data Create constrained alerts that target a specific segment of your data, such as a few key customers or a range of data. Use the WHERE clause to define those conditions. SELECT average(duration) FROM Transaction WHERE account_id in (91290, 102021, 20230) Copy SELECT percentile(duration, 95) FROM Transaction WHERE name LIKE 'Controller/checkout/%' Copy Alert on Nth percentile of your data Create alerts when an Nth percentile of your data hits a specified threshold; for example, maintaining SLA service levels. Since we evaluate the NRQL query based on the aggregation window duration, percentiles will be calculated for each duration separately. SELECT percentile(duration, 95) FROM Transaction Copy SELECT percentile(databaseDuration, 75) FROM Transaction Copy Alert on max, min, avg of your data Create alerts when your data hits a certain maximum, minimum, or average; for example, ensuring that a duration or response time does not pass a certain threshold. SELECT max(duration) FROM Transaction Copy SELECT average(duration) FROM Transaction Copy Alert on a percentage of your data Create alerts when a proportion of your data goes above or below a certain threshold. SELECT percentage(count(*), WHERE duration > 2) FROM Transaction Copy SELECT percentage(count(*), WHERE httpResponseCode = '500') FROM Transaction Copy Alert on Apdex with any T-value Create alerts on Apdex, applying your own T-value for certain transactions. For example, get an alert notification when your Apdex for a T-value of 500ms on transactions for production apps goes below 0.8. SELECT apdex(duration, t:0.5) FROM Transaction WHERE appName like '%prod%' Copy NRQL conditions and query order of operations By default, the aggregation window duration is 1 minute, but you can change the window to suit your needs. Whatever the aggregation window, New Relic will collect data for that window using the function in the NRQL condition’s query. The query is parsed and executed by our systems in the following order: FROM clause – which event type needs to be grabbed? WHERE clause – what can be filtered out? SELECT clause – what information needs to be returned from the now-filtered data set? Example: null value returned Let's say this is your alert condition query: SELECT count(*) FROM SyntheticCheck WHERE monitorName = 'My Cool Monitor' AND result = 'FAILURE' Copy If there are no failures for the aggregation window: The system will execute the FROM clause by grabbing all SyntheticCheck events on your account. Then it will execute the WHERE clause to filter through those events by looking only for the ones that match the monitor name and result specified. If there are still events left to scan through after completing the FROM and WHERE operations, the SELECT clause will be executed. If there are no remaining events, the SELECT clause will not be executed. This means that aggregators like count() and uniqueCount() will never return a zero value. When there is a count of 0, the SELECT clause is ignored and no data is returned, resulting in a value of NULL. Example: zero value returned If you have a data source delivering legitimate numeric zeroes, the query will return zero values and not null values. Let's say this is your alert condition query, and that MyCoolEvent is an attribute that can sometimes return a zero value. SELECT average(MyCoolAttribute) FROM MyCoolEvent Copy If, in the aggregation window being evaluated, there's at least one instance of MyCoolEvent and if the average value of all MyCoolAttribute attributes from that window is equal to zero, then a 0 value will be returned. If there are no MyCoolEvent events during that minute, then a NULL will be returned due to the order of operations. Tip For more information about this topic, you can check out our blog post on troubleshooting for zero versus null values. Tip You can determine how null values will be handled by adjusting loss of signal and gap filling settings in the Alert Conditions UI. Tip You can avoid NULL values entirely with a query order of operations shortcut. Do this by using a filter sub-clause, then including all filter elements within that sub-clause. The main body of the query will run and return data, at which point the SELECT clause will then run and apply the filter elements. The query will return a value of 0 if the filter elements result in no matching data. Here's an example: SELECT filter(count(*), WHERE result = 'SUCCESS' AND monitorName = 'My Favorite Monitor') FROM SyntheticCheck Copy Nested aggregation NRQL alerts Nested aggregation queries are a powerful way to query your data. However, they have a few restrictions that are important to note. Nested queries with a non-faceted innermost query are not currently supported Without a FACET, the inner query produces a single result, giving the outer query nothing to aggregate. If you're using a nested query, make sure your inner query is faceted. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu) ​​​​​ Copy Queries at all levels must have the same aggregation window size With an alert aggregation window of 1 minute, the inner query would produce two smaller windows of 30 seconds. In theory, these two windows could be aggregated by the outer query. However, this is not currently supported. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu TIMESERIES 30 seconds)​​ Copy Signal loss is not yet supported for nested queries For more information on signal loss, see NerdGraph API: Loss of signal and gap filling. Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported You can't use a nested query containing the WITH METRIC_FORMAT in the inner query to create NRQL alert conditions. NRQL condition creation tips Here are some tips for creating and using a NRQL condition: Topic Tips Condition types NRQL condition types include static, baseline, and outlier. Create a description For NRQL conditions, you can create a custom description to add to each violation. Descriptions can be enhanced with variable substitution based on metadata in the specific violation. For details, see Description Query results Queries must return a number. The condition evaluates the returned number against the thresholds you've set. Time period NRQL conditions evaluate data based on how it's aggregated, using aggregation windows from 30 seconds to 15 minutes, in increments of 15 seconds. For best results, we recommend using the event flow or event timer aggregation methods. For the cadence aggregation method, the implicit SINCE ... UNTIL clause specifying which minute to evaluate is controlled by your delay/timer setting. Since very recent data may be incomplete, you may want to query data from 3 minutes ago or longer, especially for: Applications that run on multiple hosts. SyntheticCheck data: Timeouts can take 3 minutes, so 5 minutes or more is recommended. Also, if a query will generate intermittent data, consider using the sum of query results option. Lost signal threshold (loss of signal detection) You can use loss of signal detection to alert on when your data (a telemetry signal) should be considered lost. A signal loss can indicate that a service or entity is no longer online or that a periodic job failed to run. You can also use this to make sure that violations for sporadic data, such as error counts, are closed when no signal is coming in. Advanced signal settings These settings give you options for better handling continuous, streaming data signals that may sometimes be missing. These settings include the aggregation window duration, the delay/timer, and an option for filling data gaps. For more on using these, see Advanced signal settings. Condition settings Use the Condition settings to: Create a concise, descriptive condition name. Provide a custom violation description for the condition that will be included in violations and notifications. Add the runbook URL to include your organization's procedures for handling incidents. You may also add this information to the custom violation description. Limits on conditions See the maximum values. Health status In order for a NRQL alert condition health status display to function properly, use a FACET clause to scope each signal to a single entity (for example, FACET hostname or FACET appname). Examples For more information, see: Expected NRQL syntax Examples of NRQL condition queries Condition edits can reset condition evaluation When you edit NRQL alert conditions in some specific ways (detailed below), their evaluations are reset, meaning that any evaluation up until that point is lost, and the evaluation starts over from that point. The two ways this will affect you are: For \"for at least x minutes\" thresholds: because the evaluation window has been reset, there will be a delay of at least x minutes before any violations can be reported. For baseline conditions: the condition starts over again and all baseline learning is lost. The following actions cause an evaluation reset for NRQL conditions: Changing the query Changing the aggregation window, aggregation method, or aggregation delay/timer setting Changing the \"close violations on signal loss\" setting Changing any gap fill settings Changing the baseline direction (if applicable) – higher, lower, or higher/lower Change the threshold value, threshold window, or threshold operator The following actions (along with any other actions not covered in the above list) will not reset the evaluation: Changing the loss of signal time window (expiration duration) Changing the time function (switching \"for at least\" to \"at least once in,\" or vice-versa) Toggling the \"open violation on signal loss\" setting Alert condition types When you create a NRQL alert, you can choose from different types of conditions: NRQL alert condition types Description Static This is the simplest type of NRQL condition. It allows you to create a condition based on a NRQL query that returns a numeric value. Optional: Include a FACET clause. Baseline (Dynamic) Uses a self-adjusting condition based on the past behavior of the monitored values. Uses the same NRQL query form as the static type, including the optional FACET clause. Outlier Looks for group behavior and values that are outliers from those groups. Uses the same NRQL query form as the static type, but requires a FACET clause. Sum of query results (limited or intermittent data) Important Available only for static (basic) condition types. If a query returns intermittent or limited data, it may be difficult to set a meaningful threshold. Missing or limited data will sometimes generate false positives or false negatives. You can use loss of signal, aggregation duration, and gap filling settings to minimize these false notifications. To avoid this problem when using the static threshold type, you can set the selector to sum of query results. This lets you set the alert on an aggregated sum instead of a value from a single harvest cycle. Up to two hours of one-minute data checks can be aggregated. The duration you select determines the width of the rolling sum and the preview chart will update accordingly. Set the loss of signal threshold Loss of signal occurs when no data matches the NRQL condition over a specific period of time. You can set your loss of signal threshold duration and also what happens when the threshold is crossed. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Loss of signal is only available for NRQL conditions. You may also manage these settings using the GraphQL API (recommended), or the REST API. Go here for specific GraphQL API examples. Loss of signal settings: Loss of signal settings include a time duration and two possible actions. Signal loss expiration time UI label: Signal is lost after: GraphQL Node: expiration.expirationDuration Expiration duration is a timer that starts and resets when we receive a data point in the streaming alerts pipeline. If we don't receive another data point before your 'expiration time' expires, we consider that signal to be lost. This can be because no data is being sent to New Relic or the WHERE clause of your NRQL query is filtering that data out before it is streamed to the alerts pipeline. Note that when you have a faceted query, each facet is a signal. So if any one of those signals ends during the duration specified, that will be considered a loss of signal. The loss of signal expiration time is independent of the threshold duration and triggers as soon as the timer expires. The maximum expiration duration is 48 hours. This is helpful when monitoring for the execution of infrequent jobs. The minimum is 30 seconds, but we recommend using at least 3-5 minutes. Loss of signal actions Once a signal is considered lost, you can close open violations, open new violations, or both. Close all current open violations: This closes all open violations that are related to a specific signal. It won't necessarily close all violations for a condition. If you're alerting on an ephemeral service, or on a sporadic signal, you'll want to choose this action to ensure that violations are closed properly. The GraphQL node name for this is \"closeViolationsOnExpiration\" Open new violations: This will open a new violation when the signal is considered lost. These violations will indicate that they are due to a loss of signal. Based on your incident preferences, this should trigger a notification. The graphQL node name for this is \"openViolationOnExpiration\" When you enable both actions, we'll close all open violations first, and then open a new violation for loss of signal. To create a NRQL alert configured with loss of signal detection in the UI: For a policy, when you create a condition, under Select a product, click NRQL, then click Next, define thresholds. Write a NRQL query that returns the values you want to alert on. For Threshold type, select Static or Baseline. Click + Add lost signal threshold, then set the signal expiration duration time in minutes or seconds in the Signal is lost after field. Choose what you want to happen when the signal is lost. You can check one or both of Close all current open violations and Open new \"lost signal\" violation. These control how loss of signal violations will be handled for the condition. Make sure you name your condition before you save it. Violations open due to loss of signal close when the signal comes back. Newly opened lost signal violations will close immediately when new data is evaluated. the condition they belong to expires. By default, conditions expire after 3 days. you manually close the violation with the Close all current open violations option. Tip Loss of signal detection doesn't work on NRQL queries that use nested aggregation or sub-queries. Advanced signal settings When creating a NRQL alert condition, use the advanced signal settings to control streaming alert data and avoid false alarms. When creating a NRQL condition, there are several advanced signal settings: Aggregation window duration Delay/timer Fill data gaps To read an explanation of what these settings are and how they relate to each other, see Streaming alerts concepts. Below are instructions and tips on how to configure them. Aggregation window duration You can set the aggregation window duration to choose how long data is accumulated in a streaming time window before it's aggregated. You can set it to anything between 30 seconds and 15 minutes. The default is one minute. Tip Baseline alert condition thresholds don't support editing the aggregation window. They use the 1 minute default. Delay/timer You can adjust the delay/timer to coordinate our streaming alerting algorithm with your data's behavior. If your data is sparse or inconsistent, you may want to use the event timer aggregation method. For the cadence method, the total supported latency is the sum of the aggregation window duration and the delay. If the data type comes from an APM language agent and is aggregated from many app instances (for example, Transactions, TransactionErrors, etc.), we recommend using the event flow method with the default settings. Important When creating NRQL conditions for data collected from Infrastructure Cloud Integrations such as AWS Cloudwatch or Azure, we recommend that you use the event timer method. Fill data gaps Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with one of these settings: None: (Default) Choose this if you don't want to take any action on empty aggregation windows. On evaluation, an empty aggregation window will reset the threshold duration timer. For example, if a condition says that all aggregation windows must have data points above the threshold for 5 minutes, and 1 of the 5 aggregation windows is empty, then the condition won't be in violation. Custom static value: Choose this if you'd like to insert a custom static value into the empty aggregation windows before they're evaluated. This option has an additional, required parameter of fillValue (as named in the API) that specifies what static value should be used. This defaults to 0. Last known value: This option inserts the last seen value before evaluation occurs. We maintain the state of the last seen value for 2 hours. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss, gap filling, and how to request access to these features, see this Explorers Hub post. Options for editing data gap settings: In the NRQL conditions UI, go to Condition settings > Advanced signal settings > fill data gaps with and choose an option. If using our Nerdgraph API (preferred), this node is located at: actor : account : alerts : nrqlCondition : signal : fillOption | fillValue NerdGraph is our recommended API for this but if you're using our REST API, you can find this setting in the REST API explorer under the \"signal\" section of the Alert NRQL conditions API.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 251.8564,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create NRQL <em>alert</em> <em>conditions</em>",
        "sections": "Create NRQL <em>alert</em> <em>conditions</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "You can use NRQL queries to create <em>alert</em> <em>conditions</em>. Once you&#x27;ve defined your signal, you can further define your warning and critical threshold levels. This determines when an <em>alerts</em> violation is created. Read on to learn more about how to do this. Go to one.newrelic.com, click <em>Alerts</em> &amp; AI"
      },
      "id": "603ef04864441fbc114e8883"
    },
    {
      "sections": [
        "Streaming alerts: key terms and concepts",
        "Why it matters",
        "Streaming alerts process and descriptions",
        "Choose your aggregation method",
        "Event flow",
        "Caution",
        "Event timer",
        "Cadence",
        "Streaming alerts tools",
        "Tip",
        "Window duration",
        "Delay/timer",
        "Loss of signal detection",
        "Gap filling"
      ],
      "title": "Streaming alerts: key terms and concepts",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Get started"
      ],
      "external_id": "144c7f0721ec2acb1e8ac8aa06142911ea552aef",
      "image": "https://docs.newrelic.com/static/39158bd84483adf9516bb7ff5058c9bd/d30ee/streaming-alerts.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/streaming-alerts-key-terms-concepts/",
      "published_at": "2021-12-15T12:47:58Z",
      "updated_at": "2021-12-15T12:47:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The streaming alerts platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into New Relic. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that's processed by the streaming algorithm. There are three methods for aggregating the data filtered through your NRQL condition: Event flow (default) Event timer Cadence This short video describes the three aggregation method (5:31). Why it matters Understanding how streaming alerts works will help you fine-tune your NRQL conditions to be notified about what's important to you. Only data that matches the conditions of the NRQL WHERE clause is alerted on. For more details on each step of the process, see Streaming alerts process and descriptions. As data streams into New Relic, it's filtered by the NRQL condition. Before data is evaluated, it must meet the criteria defined by the NRQL query's WHERE clause. Instead of evaluating that data immediately for violations, the NRQL alert conditions collect the data over a period of time known as the aggregation window. An additional delay/timer allows for slower data points to arrive before the window is aggregated. Once the delay/timer time has elapsed, New Relic aggregates the data into a single data point. Alerts then evaluates the data point using the NRQL condition in order to determine whether it meets the violation threshold criteria. Even if a data point meets the criteria for a violation, a violation may not be triggered. A violation is only triggered when data points consistently meet the threshold criteria over a period of time. This is the threshold duration. If the data points are in violation for an entire threshold duration, we'll send you a notification based on your policy settings. All of these configurable delays give you more control over how you're alerted on sporadic and missing data. Streaming alerts process and descriptions Process Description Streaming data All data coming into New Relic. WHERE clause Filters all incoming streaming data. We only monitor for alerts on data that makes it through this filter. Aggregation methods One of three methods that control how data is collected before it's evaluated. They are: Event flow (Default) Event timer Cadence Aggregation window Data with timestamps that fall within this window will be aggregated and then evaluated. Delay/timer A time delay to ensure all data points have arrived in the aggregation window before aggregation occurs. Aggregated data Data in the aggregated window is collapsed to a single data point for alert evaluation. Evaluation The data point is evaluated by the NRQL condition, which is trigged by each incoming aggregated data point. Threshold duration A specific duration that determines if a violation is created. If your specified NRQL condition meets the threshold criteria over the threshold duration, a violation occurs. When a data point lacks data, a custom value is inserted to fill the gap. Choose your aggregation method You can choose between three different aggregation methods, depending on your needs. Event flow (default) works best for data that comes in frequently and mostly in order. Event timer works best for data that arrives infrequently in batches, such as cloud integration data or infrequent error logs. Cadence is our original and inferior aggregation method. It aggregates data on specific time intervals as detected by New Relic's internal wall clock, regardless of data timestamps. Event flow Event flow aggregates a window of data when the first data point arrives in a subsequent window. The custom delay defines which subsequent window data will start to populate to trigger aggregation of the current window. A custom delay provides extra time for data to arrive. These times are based on the data's timestamps and not New Relic's wall clock time. For example, suppose you're monitoring CPU usage in window durations of 1 minute and a 3 minute delay. When a CPU usage data point comes in with a timestamp between 12:00pm and 12:01pm, event flow will not aggregate that window until a data point shows up with a timestamp between 12:04pm and 12:05pm. When event flow receives the first data point with a timestamp of 12:04pm or later, it sends the 12:00 to 12:01 data to be aggregated. Caution If you expect your data points to arrive more than 30 minutes apart, please use the Event Timer method described below. Event timer Like event flow, event timer will only aggregate data for a given window when data arrives for that window. When a data point arrives for an aggregation window, a timer dedicated to that window starts to count down. If no further data arrives before the timer counts down, the data for that window is aggregated. If more data points arrive before the timer has completed counting down, the timer is reset. For example, suppose you're monitoring CloudWatch data that arrives fairly infrequently. You're using a window duration of 1 minute and a 3 minute timer. When a CloudWatch data point comes in with a timestamp between 12:00pm and 12:01pm, the timer will start to count down. If no further data points show up for that 12:00-12:01 window, the window will get aggregated 3 minutes later. If a new data point with a timestamp between 12:00 and 12:01 arrives, the timer resets. It keeps resetting every time more data points for that window arrive. The window will not be sent for aggregation until the timer reaches 0. If the timer for a later data point elapses before an earlier data point, the event timer method waits for the earlier timer to elapse before aggregating the later data point. For best results, make sure your timer is equal to or longer than your window duration time. If the timer is less than your window duration and your data flow is inconsistent, then your data may be evaluated before all of your data points arrive. This could cause you to be notified incorrectly. Cadence We recommend you use one of the other two methods. Cadence is our old streaming aggregation method. This method uses New Relic's wall clock time to determine when data is aggregated and evaluated. It doesn't take into account data point timestamps as they arrive. Streaming alerts tools Streaming alerts provide a set of tools to give you greater control over how your data is aggregated before it's evaluated to reduce incorrect notifications you receive. They are: Window duration Delay/timer Loss of signal detection Gap filling Tip This article covers these tools at a conceptual level. You'll find direct instructions on how to use these tools in Create NRQL alert conditions. Window duration In order to make loss of signal detection more effective and to reduce unnecessary notifications, you can customize aggregation windows to the duration that you need. An aggregation window is a specific block of time. We gather data points together in an aggregation window, before evaluating the data. A longer aggregation window can smooth out the data, since an outlier data point will have more data points to be aggregated with, giving it less of an influence on the aggregated data point that is sent for evaluation. When a data point arrives, its timestamp is used to put it in the proper aggregation window. You can set your aggregation window to anything between 30 seconds and 15 minutes. The default is 1 minute. Delay/timer The delay/timer setting controls how long the condition should wait before aggregating the data in the aggregation window. The event flow and cadence methods use delay. Event timer uses timer. The delay default is 2 minutes. The timer default is 1 minute and has a minimum value of 5 seconds. Loss of signal detection Loss of signal occurs when no data matches the NRQL condition over a specific period of time. A loss of signal is caused by different things. The WHERE clause in your NRQL query can filter out data before it's evaluated for violations. It could also mean a service or entity is offline or a periodic job has failed to run and no data is being sent to New Relic. In order to avoid unnecessary notifications, you can choose how long to wait before you're notified by a loss of signal violation. You can use loss of signal detection to open violations and be notified when a signal is lost. Alternately, you can use a loss of signal to close violations for ephemeral services or sporadic data, such as error counts. Gap filling Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with the last value received, a static value, or else do nothing and leave the gap there. The default is None. Gaps in streaming data can be caused by network or host issues, a signal may be sparse, or some signals, such as error counts, may only have data when something is wrong. By filling the gaps with known values, the alert evaluation process can process those gaps and determine how they should affect the loss of signal evaluation. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss and gap filling, see this Explorers Hub post.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 229.456,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "sections": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "The streaming <em>alerts</em> platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into <em>New</em> <em>Relic</em>. You can use NRQL <em>conditions</em> to control what part of the signal you want to be notified about. Your NRQL condition filters the data that&#x27;s processed"
      },
      "id": "6130c054e7b9d269edb6f21d"
    },
    {
      "sections": [
        "Create your first alert",
        "Step 1: Write your query",
        "Step 2: Name your NRQL condition",
        "Step 3: Set your condition thresholds",
        "Step 4: Create a policy",
        "What's next?"
      ],
      "title": "Create your first alert",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert conditions",
        "NRQL"
      ],
      "external_id": "9fa48ac38d206d032db15f3b14e2db3163254bdb",
      "image": "https://docs.newrelic.com/static/de4c97b1a0e749c84cc1c7d5148fb691/22d0a/nrql-create-policy.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/get-started/your-first-nrql-condition/",
      "published_at": "2021-12-19T13:42:53Z",
      "updated_at": "2021-11-24T14:48:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use our New Relic query language (NRQL) to create alerts conditions with ease. Your condition describes what you want to be notified about, and only requires two attributes: a query and a threshold. The query defines the data that's important to you, and determines what you want to be notified about. The threshold determines when you’ll be notified. Follow these steps to write your first alerts condition using a NRQL query and a threshold. Once you're done, you'll have a working alert condition. Ready to get started? Make sure you have a New Relic account. It's free, forever! Step 1: Write your query You can use a NRQL query to return data about how your environment is performing. Then, you can create a condition from that query. In this example, a condition is created for each host's CPU utilization (query), which will trigger a notification when the CPU utilization is above the 0.7% (threshold). Generally, you won't use thresholds below 1%, but this example simply shows you how to set up a threshold. Find the NRQL query builder at the top of the UI. Click the Query your data button to open the query builder and start writing a NRQL query. Once you've written a valid query, you can create a condition from it. The condition shows each host’s CPU utilization. Use this NRQL query, or something similar that's relevant to what you're monitoring, to create a condition. This query finds the average CPU utilization in an environment and then breaks it down (facets) by individual host names. Once you've created a valid NRQL query, click Create alert. Learn more about the NRQL syntax to create other useful queries. Step 2: Name your NRQL condition Give your NRQL condition a name that's meaningful to you. Step 3: Set your condition thresholds Set critical and warning thresholds to determine when you’re notified about your environment's performance. A critical threshold is required for your alerts condition. A warning threshold is optional. These thresholds don't create an incident or notify you like the critical threshold does. If a critical threshold opens an incident and notifies you, warning threshold violations created afterwards will be included in the report. In this example from the Set your condition thresholds section, our critical threshold (red) checks for any hosts with CPU utilization over 0.7% for at least 5 minutes. A warning threshold (yellow) is also added showing when any host's CPU utilization goes over 0.67% for at least 5 minutes. After you enter threshold values, you can see the threshold lines on the graph: Step 4: Create a policy Finally, add your condition to an existing policy or create a new one. When you create a new policy, you can group your violations by policy, condition, or by violation (this creates an incident for each violation). These incident preferences settings determine how frequently you're notified when there's a violation. What's next? Now that you've created your first condition, set up your incident preferences and notification channels. For a detailed, comprehensive overview of NRQL conditions, see Create NRQL alert conditions",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.22382,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create your first <em>alert</em>",
        "sections": "Create your first <em>alert</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "Use our <em>New</em> <em>Relic</em> query language (NRQL) to create <em>alerts</em> <em>conditions</em> with ease. Your condition describes what you want to be notified about, and only requires two attributes: a query and a threshold. The query defines the data that&#x27;s important to you, and determines what you want to be notified"
      },
      "id": "61208dbb28ccbcc0bbf9abcc"
    }
  ],
  "/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/scope-alert-thresholds-specific-instances": [
    {
      "sections": [
        "Create NRQL alert conditions",
        "Tip",
        "Create a NRQL alert condition",
        "Create a condition from a chart",
        "Caution",
        "NRQL alert syntax",
        "Important",
        "Reformatting incompatible NRQL",
        "NRQL alert threshold examples",
        "Alert on specific segments of your data",
        "Alert on Nth percentile of your data",
        "Alert on max, min, avg of your data",
        "Alert on a percentage of your data",
        "Alert on Apdex with any T-value",
        "NRQL conditions and query order of operations",
        "Example: null value returned",
        "Example: zero value returned",
        "Nested aggregation NRQL alerts",
        "Nested queries with a non-faceted innermost query are not currently supported",
        "Queries at all levels must have the same aggregation window size",
        "Signal loss is not yet supported for nested queries",
        "Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported",
        "NRQL condition creation tips",
        "Condition edits can reset condition evaluation",
        "Alert condition types",
        "Sum of query results (limited or intermittent data)",
        "Set the loss of signal threshold",
        "Advanced signal settings",
        "Aggregation window duration",
        "Delay/timer",
        "Fill data gaps"
      ],
      "title": "Create NRQL alert conditions",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert conditions"
      ],
      "external_id": "2872f770a5fc8a802f99b9f848906f7e351ad761",
      "image": "https://docs.newrelic.com/static/eb8e1b2d826f9fd9cf46fe3dd5455217/c1b63/nr1_nrql_alert_conditions.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/",
      "published_at": "2021-12-19T16:01:17Z",
      "updated_at": "2021-12-14T22:49:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use NRQL queries to create alert conditions. Once you've defined your signal, you can further define your warning and critical threshold levels. This determines when an alerts violation is created. Read on to learn more about how to do this. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Click NRQL, and then Next, define thresholds. Tip For more information on key concepts relating to NRQL alert conditions and streaming alerts, see Streaming alerts: key terms and concepts. Create a NRQL alert condition To create a NRQL alert condition for a policy: On one.newrelic.com, in the header click Alerts & AI, then in the left sidebar click Policies. Select an existing policy or click New alert policy to create a new policy. Click Add a condition. Under Select a product click NRQL, and then click Next, define thresholds. Note that editing an existing condition can result in resetting its evaluation. Create a condition from a chart You can use a chart to create a NRQL alert condition. To create a NRQL alerts condition from a chart, click the chart menu , then click Create alert condition. Once you've named and customized your condition, you can add it to an existing policy or create a new one. Caution A small number of our older charts don't include the option to create an alert condition. NRQL alert syntax Here's the basic syntax for creating all NRQL alert conditions. The FACET clause is required for outlier condition types. It's optional for static and baseline. SELECT function(attribute) FROM Event WHERE attribute [comparison] [AND|OR ...] Copy Clause Notes SELECT function(attribute) Required Supported functions that return numbers include: apdex average count latest max min percentage percentile sum uniqueCount Tip If you use the percentile aggregator in a faceted alert condition with many facets, this may cause the following error to appear: An error occurred while fetching chart data. If you see this error, use average instead. FROM data type Required Only one data type can be targeted. Supported data types: Event Metric (RAW data points will be returned) WHERE attribute [comparison] [AND|OR ...] Use the WHERE clause to specify a series of one or more conditions. All the operators are supported. FACET attribute Required for outlier conditions Include an optional FACET clause in your NRQL syntax depending on the threshold type: static or baseline. Use the FACET clause to separate your results by attribute and alert on each attribute independently. No LIMIT clause is allowed, but all queries will receive the maximum number of facets possible. Faceted queries can return a maximum of 5000 values for static and baseline conditions and a maximum of 500 values for outlier conditions. Important If the query returns more than the maximum number of values, the alert condition can't be created. If you create the condition and the query returns more than this number later, the alert will fail. Modify your query so that it returns a fewer number of values. Reformatting incompatible NRQL Some elements of NRQL used in charts don’t make sense in the streaming context of alerts. Here’s a list of the most common incompatible elements and suggestions for reformatting a NRQL alert query to achieve the same effect. Element Notes SINCE and UNTIL Example: SELECT percentile(largestContentfulPaint, 75) FROM PageViewTiming WHERE (appId = 837807) SINCE yesterday Copy NRQL conditions produce a never-ending stream of windowed query results, so the SINCE and UNTIL keywords to scope the query to a point in time are not compatible. As a convenience, we automatically strip SINCE and UNTIL from a query when creating a condition from the context of a chart. TIMESERIES In NRQL queries, the TIMESERIES clause is used to return data as a time series broken out by a specified period of time. For NRQL conditions, the equivalent property of a signal is the aggregation duration window. histogram() The histogram() aggregation function is used to generate histograms. histogram() is not compatible with NRQL alerting: histogram aggregations can not be formatted as a time series. To create an alert from a portion of a histogram (for example, 95th percentile), use the percentile() aggregation function. Multiple aggregation functions Each condition can only target a single aggregated value. To alert on multiple values simultaneously, you’ll need to decompose them into individual conditions within the same policy. Original Query: SELECT count(foo), average(bar), max(baz) from Transaction Copy Decomposed: SELECT count(foo) from Transaction SELECT average(bar) from Transaction SELECT max(baz) from Transaction Copy COMPARE WITH The COMPARE WITH clause is used to compare the values for two different time ranges. This type of query is incompatible with NRQL alerting. We recommend using a Baseline Alert Condition to dynamically detect deviations for a particular signal. SLIDE BY The SLIDE BY clause supports a feature known as sliding windows. With sliding windows, SLIDE BY data is gathered into \"windows\" of time that overlap with each other. These windows can help to smooth out line graphs with a lot of variation in cases where the rolling aggregate (such as a rolling mean) is more important than aggregates from narrow windows of time. Sliding windows are not currently supported in NRQL alerts. LIMIT In NRQL queries, the LIMIT clause is used to control the amount of data a query returns, either the maximum number of facet values returned by FACET queries or the maximum number of items returned by SELECT * queries. LIMIT is not compatible with NRQL alerting: evaluation is always performed on the full result set. NRQL alert threshold examples Here are some common use cases for NRQL conditions. These queries will work for static and baseline condition types. The outlier condition type will require additional FACET clauses. Alert on specific segments of your data Create constrained alerts that target a specific segment of your data, such as a few key customers or a range of data. Use the WHERE clause to define those conditions. SELECT average(duration) FROM Transaction WHERE account_id in (91290, 102021, 20230) Copy SELECT percentile(duration, 95) FROM Transaction WHERE name LIKE 'Controller/checkout/%' Copy Alert on Nth percentile of your data Create alerts when an Nth percentile of your data hits a specified threshold; for example, maintaining SLA service levels. Since we evaluate the NRQL query based on the aggregation window duration, percentiles will be calculated for each duration separately. SELECT percentile(duration, 95) FROM Transaction Copy SELECT percentile(databaseDuration, 75) FROM Transaction Copy Alert on max, min, avg of your data Create alerts when your data hits a certain maximum, minimum, or average; for example, ensuring that a duration or response time does not pass a certain threshold. SELECT max(duration) FROM Transaction Copy SELECT average(duration) FROM Transaction Copy Alert on a percentage of your data Create alerts when a proportion of your data goes above or below a certain threshold. SELECT percentage(count(*), WHERE duration > 2) FROM Transaction Copy SELECT percentage(count(*), WHERE httpResponseCode = '500') FROM Transaction Copy Alert on Apdex with any T-value Create alerts on Apdex, applying your own T-value for certain transactions. For example, get an alert notification when your Apdex for a T-value of 500ms on transactions for production apps goes below 0.8. SELECT apdex(duration, t:0.5) FROM Transaction WHERE appName like '%prod%' Copy NRQL conditions and query order of operations By default, the aggregation window duration is 1 minute, but you can change the window to suit your needs. Whatever the aggregation window, New Relic will collect data for that window using the function in the NRQL condition’s query. The query is parsed and executed by our systems in the following order: FROM clause – which event type needs to be grabbed? WHERE clause – what can be filtered out? SELECT clause – what information needs to be returned from the now-filtered data set? Example: null value returned Let's say this is your alert condition query: SELECT count(*) FROM SyntheticCheck WHERE monitorName = 'My Cool Monitor' AND result = 'FAILURE' Copy If there are no failures for the aggregation window: The system will execute the FROM clause by grabbing all SyntheticCheck events on your account. Then it will execute the WHERE clause to filter through those events by looking only for the ones that match the monitor name and result specified. If there are still events left to scan through after completing the FROM and WHERE operations, the SELECT clause will be executed. If there are no remaining events, the SELECT clause will not be executed. This means that aggregators like count() and uniqueCount() will never return a zero value. When there is a count of 0, the SELECT clause is ignored and no data is returned, resulting in a value of NULL. Example: zero value returned If you have a data source delivering legitimate numeric zeroes, the query will return zero values and not null values. Let's say this is your alert condition query, and that MyCoolEvent is an attribute that can sometimes return a zero value. SELECT average(MyCoolAttribute) FROM MyCoolEvent Copy If, in the aggregation window being evaluated, there's at least one instance of MyCoolEvent and if the average value of all MyCoolAttribute attributes from that window is equal to zero, then a 0 value will be returned. If there are no MyCoolEvent events during that minute, then a NULL will be returned due to the order of operations. Tip For more information about this topic, you can check out our blog post on troubleshooting for zero versus null values. Tip You can determine how null values will be handled by adjusting loss of signal and gap filling settings in the Alert Conditions UI. Tip You can avoid NULL values entirely with a query order of operations shortcut. Do this by using a filter sub-clause, then including all filter elements within that sub-clause. The main body of the query will run and return data, at which point the SELECT clause will then run and apply the filter elements. The query will return a value of 0 if the filter elements result in no matching data. Here's an example: SELECT filter(count(*), WHERE result = 'SUCCESS' AND monitorName = 'My Favorite Monitor') FROM SyntheticCheck Copy Nested aggregation NRQL alerts Nested aggregation queries are a powerful way to query your data. However, they have a few restrictions that are important to note. Nested queries with a non-faceted innermost query are not currently supported Without a FACET, the inner query produces a single result, giving the outer query nothing to aggregate. If you're using a nested query, make sure your inner query is faceted. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu) ​​​​​ Copy Queries at all levels must have the same aggregation window size With an alert aggregation window of 1 minute, the inner query would produce two smaller windows of 30 seconds. In theory, these two windows could be aggregated by the outer query. However, this is not currently supported. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu TIMESERIES 30 seconds)​​ Copy Signal loss is not yet supported for nested queries For more information on signal loss, see NerdGraph API: Loss of signal and gap filling. Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported You can't use a nested query containing the WITH METRIC_FORMAT in the inner query to create NRQL alert conditions. NRQL condition creation tips Here are some tips for creating and using a NRQL condition: Topic Tips Condition types NRQL condition types include static, baseline, and outlier. Create a description For NRQL conditions, you can create a custom description to add to each violation. Descriptions can be enhanced with variable substitution based on metadata in the specific violation. For details, see Description Query results Queries must return a number. The condition evaluates the returned number against the thresholds you've set. Time period NRQL conditions evaluate data based on how it's aggregated, using aggregation windows from 30 seconds to 15 minutes, in increments of 15 seconds. For best results, we recommend using the event flow or event timer aggregation methods. For the cadence aggregation method, the implicit SINCE ... UNTIL clause specifying which minute to evaluate is controlled by your delay/timer setting. Since very recent data may be incomplete, you may want to query data from 3 minutes ago or longer, especially for: Applications that run on multiple hosts. SyntheticCheck data: Timeouts can take 3 minutes, so 5 minutes or more is recommended. Also, if a query will generate intermittent data, consider using the sum of query results option. Lost signal threshold (loss of signal detection) You can use loss of signal detection to alert on when your data (a telemetry signal) should be considered lost. A signal loss can indicate that a service or entity is no longer online or that a periodic job failed to run. You can also use this to make sure that violations for sporadic data, such as error counts, are closed when no signal is coming in. Advanced signal settings These settings give you options for better handling continuous, streaming data signals that may sometimes be missing. These settings include the aggregation window duration, the delay/timer, and an option for filling data gaps. For more on using these, see Advanced signal settings. Condition settings Use the Condition settings to: Create a concise, descriptive condition name. Provide a custom violation description for the condition that will be included in violations and notifications. Add the runbook URL to include your organization's procedures for handling incidents. You may also add this information to the custom violation description. Limits on conditions See the maximum values. Health status In order for a NRQL alert condition health status display to function properly, use a FACET clause to scope each signal to a single entity (for example, FACET hostname or FACET appname). Examples For more information, see: Expected NRQL syntax Examples of NRQL condition queries Condition edits can reset condition evaluation When you edit NRQL alert conditions in some specific ways (detailed below), their evaluations are reset, meaning that any evaluation up until that point is lost, and the evaluation starts over from that point. The two ways this will affect you are: For \"for at least x minutes\" thresholds: because the evaluation window has been reset, there will be a delay of at least x minutes before any violations can be reported. For baseline conditions: the condition starts over again and all baseline learning is lost. The following actions cause an evaluation reset for NRQL conditions: Changing the query Changing the aggregation window, aggregation method, or aggregation delay/timer setting Changing the \"close violations on signal loss\" setting Changing any gap fill settings Changing the baseline direction (if applicable) – higher, lower, or higher/lower Change the threshold value, threshold window, or threshold operator The following actions (along with any other actions not covered in the above list) will not reset the evaluation: Changing the loss of signal time window (expiration duration) Changing the time function (switching \"for at least\" to \"at least once in,\" or vice-versa) Toggling the \"open violation on signal loss\" setting Alert condition types When you create a NRQL alert, you can choose from different types of conditions: NRQL alert condition types Description Static This is the simplest type of NRQL condition. It allows you to create a condition based on a NRQL query that returns a numeric value. Optional: Include a FACET clause. Baseline (Dynamic) Uses a self-adjusting condition based on the past behavior of the monitored values. Uses the same NRQL query form as the static type, including the optional FACET clause. Outlier Looks for group behavior and values that are outliers from those groups. Uses the same NRQL query form as the static type, but requires a FACET clause. Sum of query results (limited or intermittent data) Important Available only for static (basic) condition types. If a query returns intermittent or limited data, it may be difficult to set a meaningful threshold. Missing or limited data will sometimes generate false positives or false negatives. You can use loss of signal, aggregation duration, and gap filling settings to minimize these false notifications. To avoid this problem when using the static threshold type, you can set the selector to sum of query results. This lets you set the alert on an aggregated sum instead of a value from a single harvest cycle. Up to two hours of one-minute data checks can be aggregated. The duration you select determines the width of the rolling sum and the preview chart will update accordingly. Set the loss of signal threshold Loss of signal occurs when no data matches the NRQL condition over a specific period of time. You can set your loss of signal threshold duration and also what happens when the threshold is crossed. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Loss of signal is only available for NRQL conditions. You may also manage these settings using the GraphQL API (recommended), or the REST API. Go here for specific GraphQL API examples. Loss of signal settings: Loss of signal settings include a time duration and two possible actions. Signal loss expiration time UI label: Signal is lost after: GraphQL Node: expiration.expirationDuration Expiration duration is a timer that starts and resets when we receive a data point in the streaming alerts pipeline. If we don't receive another data point before your 'expiration time' expires, we consider that signal to be lost. This can be because no data is being sent to New Relic or the WHERE clause of your NRQL query is filtering that data out before it is streamed to the alerts pipeline. Note that when you have a faceted query, each facet is a signal. So if any one of those signals ends during the duration specified, that will be considered a loss of signal. The loss of signal expiration time is independent of the threshold duration and triggers as soon as the timer expires. The maximum expiration duration is 48 hours. This is helpful when monitoring for the execution of infrequent jobs. The minimum is 30 seconds, but we recommend using at least 3-5 minutes. Loss of signal actions Once a signal is considered lost, you can close open violations, open new violations, or both. Close all current open violations: This closes all open violations that are related to a specific signal. It won't necessarily close all violations for a condition. If you're alerting on an ephemeral service, or on a sporadic signal, you'll want to choose this action to ensure that violations are closed properly. The GraphQL node name for this is \"closeViolationsOnExpiration\" Open new violations: This will open a new violation when the signal is considered lost. These violations will indicate that they are due to a loss of signal. Based on your incident preferences, this should trigger a notification. The graphQL node name for this is \"openViolationOnExpiration\" When you enable both actions, we'll close all open violations first, and then open a new violation for loss of signal. To create a NRQL alert configured with loss of signal detection in the UI: For a policy, when you create a condition, under Select a product, click NRQL, then click Next, define thresholds. Write a NRQL query that returns the values you want to alert on. For Threshold type, select Static or Baseline. Click + Add lost signal threshold, then set the signal expiration duration time in minutes or seconds in the Signal is lost after field. Choose what you want to happen when the signal is lost. You can check one or both of Close all current open violations and Open new \"lost signal\" violation. These control how loss of signal violations will be handled for the condition. Make sure you name your condition before you save it. Violations open due to loss of signal close when the signal comes back. Newly opened lost signal violations will close immediately when new data is evaluated. the condition they belong to expires. By default, conditions expire after 3 days. you manually close the violation with the Close all current open violations option. Tip Loss of signal detection doesn't work on NRQL queries that use nested aggregation or sub-queries. Advanced signal settings When creating a NRQL alert condition, use the advanced signal settings to control streaming alert data and avoid false alarms. When creating a NRQL condition, there are several advanced signal settings: Aggregation window duration Delay/timer Fill data gaps To read an explanation of what these settings are and how they relate to each other, see Streaming alerts concepts. Below are instructions and tips on how to configure them. Aggregation window duration You can set the aggregation window duration to choose how long data is accumulated in a streaming time window before it's aggregated. You can set it to anything between 30 seconds and 15 minutes. The default is one minute. Tip Baseline alert condition thresholds don't support editing the aggregation window. They use the 1 minute default. Delay/timer You can adjust the delay/timer to coordinate our streaming alerting algorithm with your data's behavior. If your data is sparse or inconsistent, you may want to use the event timer aggregation method. For the cadence method, the total supported latency is the sum of the aggregation window duration and the delay. If the data type comes from an APM language agent and is aggregated from many app instances (for example, Transactions, TransactionErrors, etc.), we recommend using the event flow method with the default settings. Important When creating NRQL conditions for data collected from Infrastructure Cloud Integrations such as AWS Cloudwatch or Azure, we recommend that you use the event timer method. Fill data gaps Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with one of these settings: None: (Default) Choose this if you don't want to take any action on empty aggregation windows. On evaluation, an empty aggregation window will reset the threshold duration timer. For example, if a condition says that all aggregation windows must have data points above the threshold for 5 minutes, and 1 of the 5 aggregation windows is empty, then the condition won't be in violation. Custom static value: Choose this if you'd like to insert a custom static value into the empty aggregation windows before they're evaluated. This option has an additional, required parameter of fillValue (as named in the API) that specifies what static value should be used. This defaults to 0. Last known value: This option inserts the last seen value before evaluation occurs. We maintain the state of the last seen value for 2 hours. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss, gap filling, and how to request access to these features, see this Explorers Hub post. Options for editing data gap settings: In the NRQL conditions UI, go to Condition settings > Advanced signal settings > fill data gaps with and choose an option. If using our Nerdgraph API (preferred), this node is located at: actor : account : alerts : nrqlCondition : signal : fillOption | fillValue NerdGraph is our recommended API for this but if you're using our REST API, you can find this setting in the REST API explorer under the \"signal\" section of the Alert NRQL conditions API.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 251.8563,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create NRQL <em>alert</em> <em>conditions</em>",
        "sections": "Create NRQL <em>alert</em> <em>conditions</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "You can use NRQL queries to create <em>alert</em> <em>conditions</em>. Once you&#x27;ve defined your signal, you can further define your warning and critical threshold levels. This determines when an <em>alerts</em> violation is created. Read on to learn more about how to do this. Go to one.newrelic.com, click <em>Alerts</em> &amp; AI"
      },
      "id": "603ef04864441fbc114e8883"
    },
    {
      "sections": [
        "Streaming alerts: key terms and concepts",
        "Why it matters",
        "Streaming alerts process and descriptions",
        "Choose your aggregation method",
        "Event flow",
        "Caution",
        "Event timer",
        "Cadence",
        "Streaming alerts tools",
        "Tip",
        "Window duration",
        "Delay/timer",
        "Loss of signal detection",
        "Gap filling"
      ],
      "title": "Streaming alerts: key terms and concepts",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Get started"
      ],
      "external_id": "144c7f0721ec2acb1e8ac8aa06142911ea552aef",
      "image": "https://docs.newrelic.com/static/39158bd84483adf9516bb7ff5058c9bd/d30ee/streaming-alerts.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/streaming-alerts-key-terms-concepts/",
      "published_at": "2021-12-15T12:47:58Z",
      "updated_at": "2021-12-15T12:47:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The streaming alerts platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into New Relic. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that's processed by the streaming algorithm. There are three methods for aggregating the data filtered through your NRQL condition: Event flow (default) Event timer Cadence This short video describes the three aggregation method (5:31). Why it matters Understanding how streaming alerts works will help you fine-tune your NRQL conditions to be notified about what's important to you. Only data that matches the conditions of the NRQL WHERE clause is alerted on. For more details on each step of the process, see Streaming alerts process and descriptions. As data streams into New Relic, it's filtered by the NRQL condition. Before data is evaluated, it must meet the criteria defined by the NRQL query's WHERE clause. Instead of evaluating that data immediately for violations, the NRQL alert conditions collect the data over a period of time known as the aggregation window. An additional delay/timer allows for slower data points to arrive before the window is aggregated. Once the delay/timer time has elapsed, New Relic aggregates the data into a single data point. Alerts then evaluates the data point using the NRQL condition in order to determine whether it meets the violation threshold criteria. Even if a data point meets the criteria for a violation, a violation may not be triggered. A violation is only triggered when data points consistently meet the threshold criteria over a period of time. This is the threshold duration. If the data points are in violation for an entire threshold duration, we'll send you a notification based on your policy settings. All of these configurable delays give you more control over how you're alerted on sporadic and missing data. Streaming alerts process and descriptions Process Description Streaming data All data coming into New Relic. WHERE clause Filters all incoming streaming data. We only monitor for alerts on data that makes it through this filter. Aggregation methods One of three methods that control how data is collected before it's evaluated. They are: Event flow (Default) Event timer Cadence Aggregation window Data with timestamps that fall within this window will be aggregated and then evaluated. Delay/timer A time delay to ensure all data points have arrived in the aggregation window before aggregation occurs. Aggregated data Data in the aggregated window is collapsed to a single data point for alert evaluation. Evaluation The data point is evaluated by the NRQL condition, which is trigged by each incoming aggregated data point. Threshold duration A specific duration that determines if a violation is created. If your specified NRQL condition meets the threshold criteria over the threshold duration, a violation occurs. When a data point lacks data, a custom value is inserted to fill the gap. Choose your aggregation method You can choose between three different aggregation methods, depending on your needs. Event flow (default) works best for data that comes in frequently and mostly in order. Event timer works best for data that arrives infrequently in batches, such as cloud integration data or infrequent error logs. Cadence is our original and inferior aggregation method. It aggregates data on specific time intervals as detected by New Relic's internal wall clock, regardless of data timestamps. Event flow Event flow aggregates a window of data when the first data point arrives in a subsequent window. The custom delay defines which subsequent window data will start to populate to trigger aggregation of the current window. A custom delay provides extra time for data to arrive. These times are based on the data's timestamps and not New Relic's wall clock time. For example, suppose you're monitoring CPU usage in window durations of 1 minute and a 3 minute delay. When a CPU usage data point comes in with a timestamp between 12:00pm and 12:01pm, event flow will not aggregate that window until a data point shows up with a timestamp between 12:04pm and 12:05pm. When event flow receives the first data point with a timestamp of 12:04pm or later, it sends the 12:00 to 12:01 data to be aggregated. Caution If you expect your data points to arrive more than 30 minutes apart, please use the Event Timer method described below. Event timer Like event flow, event timer will only aggregate data for a given window when data arrives for that window. When a data point arrives for an aggregation window, a timer dedicated to that window starts to count down. If no further data arrives before the timer counts down, the data for that window is aggregated. If more data points arrive before the timer has completed counting down, the timer is reset. For example, suppose you're monitoring CloudWatch data that arrives fairly infrequently. You're using a window duration of 1 minute and a 3 minute timer. When a CloudWatch data point comes in with a timestamp between 12:00pm and 12:01pm, the timer will start to count down. If no further data points show up for that 12:00-12:01 window, the window will get aggregated 3 minutes later. If a new data point with a timestamp between 12:00 and 12:01 arrives, the timer resets. It keeps resetting every time more data points for that window arrive. The window will not be sent for aggregation until the timer reaches 0. If the timer for a later data point elapses before an earlier data point, the event timer method waits for the earlier timer to elapse before aggregating the later data point. For best results, make sure your timer is equal to or longer than your window duration time. If the timer is less than your window duration and your data flow is inconsistent, then your data may be evaluated before all of your data points arrive. This could cause you to be notified incorrectly. Cadence We recommend you use one of the other two methods. Cadence is our old streaming aggregation method. This method uses New Relic's wall clock time to determine when data is aggregated and evaluated. It doesn't take into account data point timestamps as they arrive. Streaming alerts tools Streaming alerts provide a set of tools to give you greater control over how your data is aggregated before it's evaluated to reduce incorrect notifications you receive. They are: Window duration Delay/timer Loss of signal detection Gap filling Tip This article covers these tools at a conceptual level. You'll find direct instructions on how to use these tools in Create NRQL alert conditions. Window duration In order to make loss of signal detection more effective and to reduce unnecessary notifications, you can customize aggregation windows to the duration that you need. An aggregation window is a specific block of time. We gather data points together in an aggregation window, before evaluating the data. A longer aggregation window can smooth out the data, since an outlier data point will have more data points to be aggregated with, giving it less of an influence on the aggregated data point that is sent for evaluation. When a data point arrives, its timestamp is used to put it in the proper aggregation window. You can set your aggregation window to anything between 30 seconds and 15 minutes. The default is 1 minute. Delay/timer The delay/timer setting controls how long the condition should wait before aggregating the data in the aggregation window. The event flow and cadence methods use delay. Event timer uses timer. The delay default is 2 minutes. The timer default is 1 minute and has a minimum value of 5 seconds. Loss of signal detection Loss of signal occurs when no data matches the NRQL condition over a specific period of time. A loss of signal is caused by different things. The WHERE clause in your NRQL query can filter out data before it's evaluated for violations. It could also mean a service or entity is offline or a periodic job has failed to run and no data is being sent to New Relic. In order to avoid unnecessary notifications, you can choose how long to wait before you're notified by a loss of signal violation. You can use loss of signal detection to open violations and be notified when a signal is lost. Alternately, you can use a loss of signal to close violations for ephemeral services or sporadic data, such as error counts. Gap filling Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with the last value received, a static value, or else do nothing and leave the gap there. The default is None. Gaps in streaming data can be caused by network or host issues, a signal may be sparse, or some signals, such as error counts, may only have data when something is wrong. By filling the gaps with known values, the alert evaluation process can process those gaps and determine how they should affect the loss of signal evaluation. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss and gap filling, see this Explorers Hub post.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 229.4559,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "sections": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "The streaming <em>alerts</em> platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into <em>New</em> <em>Relic</em>. You can use NRQL <em>conditions</em> to control what part of the signal you want to be notified about. Your NRQL condition filters the data that&#x27;s processed"
      },
      "id": "6130c054e7b9d269edb6f21d"
    },
    {
      "sections": [
        "Outlier detection (NRQL alert)",
        "Important",
        "What is outlier detection?",
        "Tip",
        "Example use cases",
        "Notify if load-balanced servers have uneven workload",
        "Notify if load-balanced application has misbehaving instances",
        "Notify of changes in different environments",
        "Notify for timezone-related changes",
        "Create an outlier alert condition",
        "Rules and logic",
        "Details about alert condition logic",
        "NRQL query rules and limits",
        "Zero values for unreturned data"
      ],
      "title": "Outlier detection (NRQL alert)",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert conditions"
      ],
      "external_id": "499fa55abd48a0ccdd897fbdf64ccea2d9f98d11",
      "image": "https://docs.newrelic.com/static/f235d0630576bc2010ff07adc7a69621/f73a1/NRQL_outlier_violations.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/outlier-detection-nrql-alert/",
      "published_at": "2021-12-15T12:44:32Z",
      "updated_at": "2021-11-25T19:50:19Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Alerts offers NRQL conditions in three threshold types: static, baseline, and outlier. This document explains how the outlier threshold type works, gives some example use cases and NRQL queries, and explains how to create an outlier condition. Important NRQL alerts do not affect Alerts policies for a Synthetic monitor. For example, muting a NRQL alert will not mute a Synthetic monitor's alerts. What is outlier detection? In software development and operations, it is common to have a group consisting of members you expect to behave approximately the same. For example: for servers using a load balancer, the traffic to the servers may go up or down, but the traffic for all the servers should remain in a fairly tight grouping. See outlier detection in action in this NerdBytes video (2:51 minutes). The NRQL alert outlier detection feature parses the data returned by your faceted NRQL query and: Looks for the number of expected groups that you specify Looks for outliers (values deviating from a group) based on the sensitivity and time range you set Additionally, for queries that have more than one group, you can choose to be notified when groups start behaving the same. This visual aid will help you understand the types of situations that will trigger a violation and those that won't. For more on the rules and logic behind this calculation, see Outlier detection rules. Tip Note: this feature does not take into account the past behavior of the monitored values; it looks for outliers only in the currently reported data. For an alert type that takes into account past behavior, see Baseline alerting. Example use cases These use cases will help you understand when to use the outlier threshold type. Note that the outlier feature requires a NRQL query with a FACET clause. Notify if load-balanced servers have uneven workload A load balancer divides web traffic approximately evenly across five different servers. You can set a notification to be sent if any server starts getting significantly more or less traffic than the other servers. Example query: SELECT average(cpuPercent) FROM SystemSample WHERE apmApplicationNames = 'MY-APP-NAME' FACET hostname Copy Notify if load-balanced application has misbehaving instances Application instances behind a load balancer should have similar throughput, error rates, and response times. If an instance is in a bad state, or a load balancer is misconfigured, this will not be the case. Detecting one or two bad app instances using aggregate metrics may be difficult if there is not a significant rise in the overall error rate of the application. You can set a notification for when an app instance’s throughput, error rate, or response time deviates too far from the rest of the group. Example query: SELECT average(duration) FROM Transaction WHERE appName = 'MY-APP-NAME' FACET host Copy Notify of changes in different environments An application is deployed in two different environments, with ten application instances in each. One environment is experimental and gets more errors than the other. But the instances that are in the same environment should get approximately the same number of errors. You can set a notification for when an instance starts getting more errors than the other instances in the same environment. Also, you can set a notification for when the two environments start to have the same number of errors as each other. Notify for timezone-related changes The number of logged in users for a company is about the same for each of four applications, but varies significantly by each of the three time zones the company operates in. You can set a notification for when any application starts getting more or less traffic from a certain timezone than the other applications. Sometimes the traffic from the different time zones are the same, so you would set up the alert condition to not be notified if the time zone groups overlap. For more details on how this feature works, see Outlier rules and logic. Create an outlier alert condition To create a NRQL alert that uses outlier detection: When creating a condition, under Select a product, select NRQL. For Threshold type, select Outlier. Create a NRQL query with a FACET clause that returns the values you want to alert on. Depending on how the returned values group together, set the Number of expected groups. Adjust the deviation from the center of the group(s) and the duration that will trigger a violation. Optional: Add a warning threshold and set its deviation. Set any remaining available options and save. Rules and logic Here are the rules and logic behind how outlier detection works: Details about alert condition logic After the condition is created, the query is run once every harvest cycle and the condition is applied. Unlike baseline alerts, outlier detection uses no historical data in its calculation; it's calculated using the currently collected data. Alerts will attempt to divide the data returned from the query into the number of groups selected during condition creation. For each group, the approximate average value is calculated. The allowable deviation you have chosen when creating the condition is centered around that average value. If a member of the group is outside the allowed deviation, it produces a violation. If Trigger when groups overlap has been selected, Alerts detects a convergence of groups. If the condition is looking for two or more groups, and the returned values cannot be separated into that number of distinct groups, then that will produce a violation. This type of “overlap” event is represented on a chart by group bands touching. Because this feature does not take past behavior into account, data is never considered to \"belong\" to a certain group. For example, a value that switches places with another value wouldn't trigger a violation. Additionally, an entire group that moves together also wouldn't trigger a violation. NRQL query rules and limits The NRQL query must be a faceted query. The number of unique values returned must be 500 or less. If the query returns more than this number of values, the condition won't be created. If the query later returns more than this number after being created, the alert will fail. Zero values for unreturned data When a query returns a set of values, only values that are actually returned are taken into account. If a value is not available for calculation (including if it goes from being collected one harvest cycle to not being collected), it is rendered as a zero and is not considered. In other words, the behavior of unreturned zero values will never trigger violations.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.94469,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Outlier detection (NRQL <em>alert</em>)",
        "sections": "Create an outlier <em>alert</em> <em>condition</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "<em>Alerts</em> offers NRQL <em>conditions</em> in three threshold types: static, baseline, and outlier. This document explains how the outlier threshold type works, gives some example use cases and NRQL queries, and explains how to create an outlier condition. Important NRQL <em>alerts</em> do not affect <em>Alerts</em> policies"
      },
      "id": "6130be72196a6793654948e7"
    }
  ],
  "/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/select-product-targets-alert-condition": [
    {
      "sections": [
        "Create NRQL alert conditions",
        "Tip",
        "Create a NRQL alert condition",
        "Create a condition from a chart",
        "Caution",
        "NRQL alert syntax",
        "Important",
        "Reformatting incompatible NRQL",
        "NRQL alert threshold examples",
        "Alert on specific segments of your data",
        "Alert on Nth percentile of your data",
        "Alert on max, min, avg of your data",
        "Alert on a percentage of your data",
        "Alert on Apdex with any T-value",
        "NRQL conditions and query order of operations",
        "Example: null value returned",
        "Example: zero value returned",
        "Nested aggregation NRQL alerts",
        "Nested queries with a non-faceted innermost query are not currently supported",
        "Queries at all levels must have the same aggregation window size",
        "Signal loss is not yet supported for nested queries",
        "Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported",
        "NRQL condition creation tips",
        "Condition edits can reset condition evaluation",
        "Alert condition types",
        "Sum of query results (limited or intermittent data)",
        "Set the loss of signal threshold",
        "Advanced signal settings",
        "Aggregation window duration",
        "Delay/timer",
        "Fill data gaps"
      ],
      "title": "Create NRQL alert conditions",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert conditions"
      ],
      "external_id": "2872f770a5fc8a802f99b9f848906f7e351ad761",
      "image": "https://docs.newrelic.com/static/eb8e1b2d826f9fd9cf46fe3dd5455217/c1b63/nr1_nrql_alert_conditions.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/",
      "published_at": "2021-12-19T16:01:17Z",
      "updated_at": "2021-12-14T22:49:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use NRQL queries to create alert conditions. Once you've defined your signal, you can further define your warning and critical threshold levels. This determines when an alerts violation is created. Read on to learn more about how to do this. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Click NRQL, and then Next, define thresholds. Tip For more information on key concepts relating to NRQL alert conditions and streaming alerts, see Streaming alerts: key terms and concepts. Create a NRQL alert condition To create a NRQL alert condition for a policy: On one.newrelic.com, in the header click Alerts & AI, then in the left sidebar click Policies. Select an existing policy or click New alert policy to create a new policy. Click Add a condition. Under Select a product click NRQL, and then click Next, define thresholds. Note that editing an existing condition can result in resetting its evaluation. Create a condition from a chart You can use a chart to create a NRQL alert condition. To create a NRQL alerts condition from a chart, click the chart menu , then click Create alert condition. Once you've named and customized your condition, you can add it to an existing policy or create a new one. Caution A small number of our older charts don't include the option to create an alert condition. NRQL alert syntax Here's the basic syntax for creating all NRQL alert conditions. The FACET clause is required for outlier condition types. It's optional for static and baseline. SELECT function(attribute) FROM Event WHERE attribute [comparison] [AND|OR ...] Copy Clause Notes SELECT function(attribute) Required Supported functions that return numbers include: apdex average count latest max min percentage percentile sum uniqueCount Tip If you use the percentile aggregator in a faceted alert condition with many facets, this may cause the following error to appear: An error occurred while fetching chart data. If you see this error, use average instead. FROM data type Required Only one data type can be targeted. Supported data types: Event Metric (RAW data points will be returned) WHERE attribute [comparison] [AND|OR ...] Use the WHERE clause to specify a series of one or more conditions. All the operators are supported. FACET attribute Required for outlier conditions Include an optional FACET clause in your NRQL syntax depending on the threshold type: static or baseline. Use the FACET clause to separate your results by attribute and alert on each attribute independently. No LIMIT clause is allowed, but all queries will receive the maximum number of facets possible. Faceted queries can return a maximum of 5000 values for static and baseline conditions and a maximum of 500 values for outlier conditions. Important If the query returns more than the maximum number of values, the alert condition can't be created. If you create the condition and the query returns more than this number later, the alert will fail. Modify your query so that it returns a fewer number of values. Reformatting incompatible NRQL Some elements of NRQL used in charts don’t make sense in the streaming context of alerts. Here’s a list of the most common incompatible elements and suggestions for reformatting a NRQL alert query to achieve the same effect. Element Notes SINCE and UNTIL Example: SELECT percentile(largestContentfulPaint, 75) FROM PageViewTiming WHERE (appId = 837807) SINCE yesterday Copy NRQL conditions produce a never-ending stream of windowed query results, so the SINCE and UNTIL keywords to scope the query to a point in time are not compatible. As a convenience, we automatically strip SINCE and UNTIL from a query when creating a condition from the context of a chart. TIMESERIES In NRQL queries, the TIMESERIES clause is used to return data as a time series broken out by a specified period of time. For NRQL conditions, the equivalent property of a signal is the aggregation duration window. histogram() The histogram() aggregation function is used to generate histograms. histogram() is not compatible with NRQL alerting: histogram aggregations can not be formatted as a time series. To create an alert from a portion of a histogram (for example, 95th percentile), use the percentile() aggregation function. Multiple aggregation functions Each condition can only target a single aggregated value. To alert on multiple values simultaneously, you’ll need to decompose them into individual conditions within the same policy. Original Query: SELECT count(foo), average(bar), max(baz) from Transaction Copy Decomposed: SELECT count(foo) from Transaction SELECT average(bar) from Transaction SELECT max(baz) from Transaction Copy COMPARE WITH The COMPARE WITH clause is used to compare the values for two different time ranges. This type of query is incompatible with NRQL alerting. We recommend using a Baseline Alert Condition to dynamically detect deviations for a particular signal. SLIDE BY The SLIDE BY clause supports a feature known as sliding windows. With sliding windows, SLIDE BY data is gathered into \"windows\" of time that overlap with each other. These windows can help to smooth out line graphs with a lot of variation in cases where the rolling aggregate (such as a rolling mean) is more important than aggregates from narrow windows of time. Sliding windows are not currently supported in NRQL alerts. LIMIT In NRQL queries, the LIMIT clause is used to control the amount of data a query returns, either the maximum number of facet values returned by FACET queries or the maximum number of items returned by SELECT * queries. LIMIT is not compatible with NRQL alerting: evaluation is always performed on the full result set. NRQL alert threshold examples Here are some common use cases for NRQL conditions. These queries will work for static and baseline condition types. The outlier condition type will require additional FACET clauses. Alert on specific segments of your data Create constrained alerts that target a specific segment of your data, such as a few key customers or a range of data. Use the WHERE clause to define those conditions. SELECT average(duration) FROM Transaction WHERE account_id in (91290, 102021, 20230) Copy SELECT percentile(duration, 95) FROM Transaction WHERE name LIKE 'Controller/checkout/%' Copy Alert on Nth percentile of your data Create alerts when an Nth percentile of your data hits a specified threshold; for example, maintaining SLA service levels. Since we evaluate the NRQL query based on the aggregation window duration, percentiles will be calculated for each duration separately. SELECT percentile(duration, 95) FROM Transaction Copy SELECT percentile(databaseDuration, 75) FROM Transaction Copy Alert on max, min, avg of your data Create alerts when your data hits a certain maximum, minimum, or average; for example, ensuring that a duration or response time does not pass a certain threshold. SELECT max(duration) FROM Transaction Copy SELECT average(duration) FROM Transaction Copy Alert on a percentage of your data Create alerts when a proportion of your data goes above or below a certain threshold. SELECT percentage(count(*), WHERE duration > 2) FROM Transaction Copy SELECT percentage(count(*), WHERE httpResponseCode = '500') FROM Transaction Copy Alert on Apdex with any T-value Create alerts on Apdex, applying your own T-value for certain transactions. For example, get an alert notification when your Apdex for a T-value of 500ms on transactions for production apps goes below 0.8. SELECT apdex(duration, t:0.5) FROM Transaction WHERE appName like '%prod%' Copy NRQL conditions and query order of operations By default, the aggregation window duration is 1 minute, but you can change the window to suit your needs. Whatever the aggregation window, New Relic will collect data for that window using the function in the NRQL condition’s query. The query is parsed and executed by our systems in the following order: FROM clause – which event type needs to be grabbed? WHERE clause – what can be filtered out? SELECT clause – what information needs to be returned from the now-filtered data set? Example: null value returned Let's say this is your alert condition query: SELECT count(*) FROM SyntheticCheck WHERE monitorName = 'My Cool Monitor' AND result = 'FAILURE' Copy If there are no failures for the aggregation window: The system will execute the FROM clause by grabbing all SyntheticCheck events on your account. Then it will execute the WHERE clause to filter through those events by looking only for the ones that match the monitor name and result specified. If there are still events left to scan through after completing the FROM and WHERE operations, the SELECT clause will be executed. If there are no remaining events, the SELECT clause will not be executed. This means that aggregators like count() and uniqueCount() will never return a zero value. When there is a count of 0, the SELECT clause is ignored and no data is returned, resulting in a value of NULL. Example: zero value returned If you have a data source delivering legitimate numeric zeroes, the query will return zero values and not null values. Let's say this is your alert condition query, and that MyCoolEvent is an attribute that can sometimes return a zero value. SELECT average(MyCoolAttribute) FROM MyCoolEvent Copy If, in the aggregation window being evaluated, there's at least one instance of MyCoolEvent and if the average value of all MyCoolAttribute attributes from that window is equal to zero, then a 0 value will be returned. If there are no MyCoolEvent events during that minute, then a NULL will be returned due to the order of operations. Tip For more information about this topic, you can check out our blog post on troubleshooting for zero versus null values. Tip You can determine how null values will be handled by adjusting loss of signal and gap filling settings in the Alert Conditions UI. Tip You can avoid NULL values entirely with a query order of operations shortcut. Do this by using a filter sub-clause, then including all filter elements within that sub-clause. The main body of the query will run and return data, at which point the SELECT clause will then run and apply the filter elements. The query will return a value of 0 if the filter elements result in no matching data. Here's an example: SELECT filter(count(*), WHERE result = 'SUCCESS' AND monitorName = 'My Favorite Monitor') FROM SyntheticCheck Copy Nested aggregation NRQL alerts Nested aggregation queries are a powerful way to query your data. However, they have a few restrictions that are important to note. Nested queries with a non-faceted innermost query are not currently supported Without a FACET, the inner query produces a single result, giving the outer query nothing to aggregate. If you're using a nested query, make sure your inner query is faceted. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu) ​​​​​ Copy Queries at all levels must have the same aggregation window size With an alert aggregation window of 1 minute, the inner query would produce two smaller windows of 30 seconds. In theory, these two windows could be aggregated by the outer query. However, this is not currently supported. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu TIMESERIES 30 seconds)​​ Copy Signal loss is not yet supported for nested queries For more information on signal loss, see NerdGraph API: Loss of signal and gap filling. Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported You can't use a nested query containing the WITH METRIC_FORMAT in the inner query to create NRQL alert conditions. NRQL condition creation tips Here are some tips for creating and using a NRQL condition: Topic Tips Condition types NRQL condition types include static, baseline, and outlier. Create a description For NRQL conditions, you can create a custom description to add to each violation. Descriptions can be enhanced with variable substitution based on metadata in the specific violation. For details, see Description Query results Queries must return a number. The condition evaluates the returned number against the thresholds you've set. Time period NRQL conditions evaluate data based on how it's aggregated, using aggregation windows from 30 seconds to 15 minutes, in increments of 15 seconds. For best results, we recommend using the event flow or event timer aggregation methods. For the cadence aggregation method, the implicit SINCE ... UNTIL clause specifying which minute to evaluate is controlled by your delay/timer setting. Since very recent data may be incomplete, you may want to query data from 3 minutes ago or longer, especially for: Applications that run on multiple hosts. SyntheticCheck data: Timeouts can take 3 minutes, so 5 minutes or more is recommended. Also, if a query will generate intermittent data, consider using the sum of query results option. Lost signal threshold (loss of signal detection) You can use loss of signal detection to alert on when your data (a telemetry signal) should be considered lost. A signal loss can indicate that a service or entity is no longer online or that a periodic job failed to run. You can also use this to make sure that violations for sporadic data, such as error counts, are closed when no signal is coming in. Advanced signal settings These settings give you options for better handling continuous, streaming data signals that may sometimes be missing. These settings include the aggregation window duration, the delay/timer, and an option for filling data gaps. For more on using these, see Advanced signal settings. Condition settings Use the Condition settings to: Create a concise, descriptive condition name. Provide a custom violation description for the condition that will be included in violations and notifications. Add the runbook URL to include your organization's procedures for handling incidents. You may also add this information to the custom violation description. Limits on conditions See the maximum values. Health status In order for a NRQL alert condition health status display to function properly, use a FACET clause to scope each signal to a single entity (for example, FACET hostname or FACET appname). Examples For more information, see: Expected NRQL syntax Examples of NRQL condition queries Condition edits can reset condition evaluation When you edit NRQL alert conditions in some specific ways (detailed below), their evaluations are reset, meaning that any evaluation up until that point is lost, and the evaluation starts over from that point. The two ways this will affect you are: For \"for at least x minutes\" thresholds: because the evaluation window has been reset, there will be a delay of at least x minutes before any violations can be reported. For baseline conditions: the condition starts over again and all baseline learning is lost. The following actions cause an evaluation reset for NRQL conditions: Changing the query Changing the aggregation window, aggregation method, or aggregation delay/timer setting Changing the \"close violations on signal loss\" setting Changing any gap fill settings Changing the baseline direction (if applicable) – higher, lower, or higher/lower Change the threshold value, threshold window, or threshold operator The following actions (along with any other actions not covered in the above list) will not reset the evaluation: Changing the loss of signal time window (expiration duration) Changing the time function (switching \"for at least\" to \"at least once in,\" or vice-versa) Toggling the \"open violation on signal loss\" setting Alert condition types When you create a NRQL alert, you can choose from different types of conditions: NRQL alert condition types Description Static This is the simplest type of NRQL condition. It allows you to create a condition based on a NRQL query that returns a numeric value. Optional: Include a FACET clause. Baseline (Dynamic) Uses a self-adjusting condition based on the past behavior of the monitored values. Uses the same NRQL query form as the static type, including the optional FACET clause. Outlier Looks for group behavior and values that are outliers from those groups. Uses the same NRQL query form as the static type, but requires a FACET clause. Sum of query results (limited or intermittent data) Important Available only for static (basic) condition types. If a query returns intermittent or limited data, it may be difficult to set a meaningful threshold. Missing or limited data will sometimes generate false positives or false negatives. You can use loss of signal, aggregation duration, and gap filling settings to minimize these false notifications. To avoid this problem when using the static threshold type, you can set the selector to sum of query results. This lets you set the alert on an aggregated sum instead of a value from a single harvest cycle. Up to two hours of one-minute data checks can be aggregated. The duration you select determines the width of the rolling sum and the preview chart will update accordingly. Set the loss of signal threshold Loss of signal occurs when no data matches the NRQL condition over a specific period of time. You can set your loss of signal threshold duration and also what happens when the threshold is crossed. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Loss of signal is only available for NRQL conditions. You may also manage these settings using the GraphQL API (recommended), or the REST API. Go here for specific GraphQL API examples. Loss of signal settings: Loss of signal settings include a time duration and two possible actions. Signal loss expiration time UI label: Signal is lost after: GraphQL Node: expiration.expirationDuration Expiration duration is a timer that starts and resets when we receive a data point in the streaming alerts pipeline. If we don't receive another data point before your 'expiration time' expires, we consider that signal to be lost. This can be because no data is being sent to New Relic or the WHERE clause of your NRQL query is filtering that data out before it is streamed to the alerts pipeline. Note that when you have a faceted query, each facet is a signal. So if any one of those signals ends during the duration specified, that will be considered a loss of signal. The loss of signal expiration time is independent of the threshold duration and triggers as soon as the timer expires. The maximum expiration duration is 48 hours. This is helpful when monitoring for the execution of infrequent jobs. The minimum is 30 seconds, but we recommend using at least 3-5 minutes. Loss of signal actions Once a signal is considered lost, you can close open violations, open new violations, or both. Close all current open violations: This closes all open violations that are related to a specific signal. It won't necessarily close all violations for a condition. If you're alerting on an ephemeral service, or on a sporadic signal, you'll want to choose this action to ensure that violations are closed properly. The GraphQL node name for this is \"closeViolationsOnExpiration\" Open new violations: This will open a new violation when the signal is considered lost. These violations will indicate that they are due to a loss of signal. Based on your incident preferences, this should trigger a notification. The graphQL node name for this is \"openViolationOnExpiration\" When you enable both actions, we'll close all open violations first, and then open a new violation for loss of signal. To create a NRQL alert configured with loss of signal detection in the UI: For a policy, when you create a condition, under Select a product, click NRQL, then click Next, define thresholds. Write a NRQL query that returns the values you want to alert on. For Threshold type, select Static or Baseline. Click + Add lost signal threshold, then set the signal expiration duration time in minutes or seconds in the Signal is lost after field. Choose what you want to happen when the signal is lost. You can check one or both of Close all current open violations and Open new \"lost signal\" violation. These control how loss of signal violations will be handled for the condition. Make sure you name your condition before you save it. Violations open due to loss of signal close when the signal comes back. Newly opened lost signal violations will close immediately when new data is evaluated. the condition they belong to expires. By default, conditions expire after 3 days. you manually close the violation with the Close all current open violations option. Tip Loss of signal detection doesn't work on NRQL queries that use nested aggregation or sub-queries. Advanced signal settings When creating a NRQL alert condition, use the advanced signal settings to control streaming alert data and avoid false alarms. When creating a NRQL condition, there are several advanced signal settings: Aggregation window duration Delay/timer Fill data gaps To read an explanation of what these settings are and how they relate to each other, see Streaming alerts concepts. Below are instructions and tips on how to configure them. Aggregation window duration You can set the aggregation window duration to choose how long data is accumulated in a streaming time window before it's aggregated. You can set it to anything between 30 seconds and 15 minutes. The default is one minute. Tip Baseline alert condition thresholds don't support editing the aggregation window. They use the 1 minute default. Delay/timer You can adjust the delay/timer to coordinate our streaming alerting algorithm with your data's behavior. If your data is sparse or inconsistent, you may want to use the event timer aggregation method. For the cadence method, the total supported latency is the sum of the aggregation window duration and the delay. If the data type comes from an APM language agent and is aggregated from many app instances (for example, Transactions, TransactionErrors, etc.), we recommend using the event flow method with the default settings. Important When creating NRQL conditions for data collected from Infrastructure Cloud Integrations such as AWS Cloudwatch or Azure, we recommend that you use the event timer method. Fill data gaps Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with one of these settings: None: (Default) Choose this if you don't want to take any action on empty aggregation windows. On evaluation, an empty aggregation window will reset the threshold duration timer. For example, if a condition says that all aggregation windows must have data points above the threshold for 5 minutes, and 1 of the 5 aggregation windows is empty, then the condition won't be in violation. Custom static value: Choose this if you'd like to insert a custom static value into the empty aggregation windows before they're evaluated. This option has an additional, required parameter of fillValue (as named in the API) that specifies what static value should be used. This defaults to 0. Last known value: This option inserts the last seen value before evaluation occurs. We maintain the state of the last seen value for 2 hours. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss, gap filling, and how to request access to these features, see this Explorers Hub post. Options for editing data gap settings: In the NRQL conditions UI, go to Condition settings > Advanced signal settings > fill data gaps with and choose an option. If using our Nerdgraph API (preferred), this node is located at: actor : account : alerts : nrqlCondition : signal : fillOption | fillValue NerdGraph is our recommended API for this but if you're using our REST API, you can find this setting in the REST API explorer under the \"signal\" section of the Alert NRQL conditions API.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 251.85617,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create NRQL <em>alert</em> <em>conditions</em>",
        "sections": "Create NRQL <em>alert</em> <em>conditions</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "You can use NRQL queries to create <em>alert</em> <em>conditions</em>. Once you&#x27;ve defined your signal, you can further define your warning and critical threshold levels. This determines when an <em>alerts</em> violation is created. Read on to learn more about how to do this. Go to one.newrelic.com, click <em>Alerts</em> &amp; AI"
      },
      "id": "603ef04864441fbc114e8883"
    },
    {
      "sections": [
        "Streaming alerts: key terms and concepts",
        "Why it matters",
        "Streaming alerts process and descriptions",
        "Choose your aggregation method",
        "Event flow",
        "Caution",
        "Event timer",
        "Cadence",
        "Streaming alerts tools",
        "Tip",
        "Window duration",
        "Delay/timer",
        "Loss of signal detection",
        "Gap filling"
      ],
      "title": "Streaming alerts: key terms and concepts",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Get started"
      ],
      "external_id": "144c7f0721ec2acb1e8ac8aa06142911ea552aef",
      "image": "https://docs.newrelic.com/static/39158bd84483adf9516bb7ff5058c9bd/d30ee/streaming-alerts.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/streaming-alerts-key-terms-concepts/",
      "published_at": "2021-12-15T12:47:58Z",
      "updated_at": "2021-12-15T12:47:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The streaming alerts platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into New Relic. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that's processed by the streaming algorithm. There are three methods for aggregating the data filtered through your NRQL condition: Event flow (default) Event timer Cadence This short video describes the three aggregation method (5:31). Why it matters Understanding how streaming alerts works will help you fine-tune your NRQL conditions to be notified about what's important to you. Only data that matches the conditions of the NRQL WHERE clause is alerted on. For more details on each step of the process, see Streaming alerts process and descriptions. As data streams into New Relic, it's filtered by the NRQL condition. Before data is evaluated, it must meet the criteria defined by the NRQL query's WHERE clause. Instead of evaluating that data immediately for violations, the NRQL alert conditions collect the data over a period of time known as the aggregation window. An additional delay/timer allows for slower data points to arrive before the window is aggregated. Once the delay/timer time has elapsed, New Relic aggregates the data into a single data point. Alerts then evaluates the data point using the NRQL condition in order to determine whether it meets the violation threshold criteria. Even if a data point meets the criteria for a violation, a violation may not be triggered. A violation is only triggered when data points consistently meet the threshold criteria over a period of time. This is the threshold duration. If the data points are in violation for an entire threshold duration, we'll send you a notification based on your policy settings. All of these configurable delays give you more control over how you're alerted on sporadic and missing data. Streaming alerts process and descriptions Process Description Streaming data All data coming into New Relic. WHERE clause Filters all incoming streaming data. We only monitor for alerts on data that makes it through this filter. Aggregation methods One of three methods that control how data is collected before it's evaluated. They are: Event flow (Default) Event timer Cadence Aggregation window Data with timestamps that fall within this window will be aggregated and then evaluated. Delay/timer A time delay to ensure all data points have arrived in the aggregation window before aggregation occurs. Aggregated data Data in the aggregated window is collapsed to a single data point for alert evaluation. Evaluation The data point is evaluated by the NRQL condition, which is trigged by each incoming aggregated data point. Threshold duration A specific duration that determines if a violation is created. If your specified NRQL condition meets the threshold criteria over the threshold duration, a violation occurs. When a data point lacks data, a custom value is inserted to fill the gap. Choose your aggregation method You can choose between three different aggregation methods, depending on your needs. Event flow (default) works best for data that comes in frequently and mostly in order. Event timer works best for data that arrives infrequently in batches, such as cloud integration data or infrequent error logs. Cadence is our original and inferior aggregation method. It aggregates data on specific time intervals as detected by New Relic's internal wall clock, regardless of data timestamps. Event flow Event flow aggregates a window of data when the first data point arrives in a subsequent window. The custom delay defines which subsequent window data will start to populate to trigger aggregation of the current window. A custom delay provides extra time for data to arrive. These times are based on the data's timestamps and not New Relic's wall clock time. For example, suppose you're monitoring CPU usage in window durations of 1 minute and a 3 minute delay. When a CPU usage data point comes in with a timestamp between 12:00pm and 12:01pm, event flow will not aggregate that window until a data point shows up with a timestamp between 12:04pm and 12:05pm. When event flow receives the first data point with a timestamp of 12:04pm or later, it sends the 12:00 to 12:01 data to be aggregated. Caution If you expect your data points to arrive more than 30 minutes apart, please use the Event Timer method described below. Event timer Like event flow, event timer will only aggregate data for a given window when data arrives for that window. When a data point arrives for an aggregation window, a timer dedicated to that window starts to count down. If no further data arrives before the timer counts down, the data for that window is aggregated. If more data points arrive before the timer has completed counting down, the timer is reset. For example, suppose you're monitoring CloudWatch data that arrives fairly infrequently. You're using a window duration of 1 minute and a 3 minute timer. When a CloudWatch data point comes in with a timestamp between 12:00pm and 12:01pm, the timer will start to count down. If no further data points show up for that 12:00-12:01 window, the window will get aggregated 3 minutes later. If a new data point with a timestamp between 12:00 and 12:01 arrives, the timer resets. It keeps resetting every time more data points for that window arrive. The window will not be sent for aggregation until the timer reaches 0. If the timer for a later data point elapses before an earlier data point, the event timer method waits for the earlier timer to elapse before aggregating the later data point. For best results, make sure your timer is equal to or longer than your window duration time. If the timer is less than your window duration and your data flow is inconsistent, then your data may be evaluated before all of your data points arrive. This could cause you to be notified incorrectly. Cadence We recommend you use one of the other two methods. Cadence is our old streaming aggregation method. This method uses New Relic's wall clock time to determine when data is aggregated and evaluated. It doesn't take into account data point timestamps as they arrive. Streaming alerts tools Streaming alerts provide a set of tools to give you greater control over how your data is aggregated before it's evaluated to reduce incorrect notifications you receive. They are: Window duration Delay/timer Loss of signal detection Gap filling Tip This article covers these tools at a conceptual level. You'll find direct instructions on how to use these tools in Create NRQL alert conditions. Window duration In order to make loss of signal detection more effective and to reduce unnecessary notifications, you can customize aggregation windows to the duration that you need. An aggregation window is a specific block of time. We gather data points together in an aggregation window, before evaluating the data. A longer aggregation window can smooth out the data, since an outlier data point will have more data points to be aggregated with, giving it less of an influence on the aggregated data point that is sent for evaluation. When a data point arrives, its timestamp is used to put it in the proper aggregation window. You can set your aggregation window to anything between 30 seconds and 15 minutes. The default is 1 minute. Delay/timer The delay/timer setting controls how long the condition should wait before aggregating the data in the aggregation window. The event flow and cadence methods use delay. Event timer uses timer. The delay default is 2 minutes. The timer default is 1 minute and has a minimum value of 5 seconds. Loss of signal detection Loss of signal occurs when no data matches the NRQL condition over a specific period of time. A loss of signal is caused by different things. The WHERE clause in your NRQL query can filter out data before it's evaluated for violations. It could also mean a service or entity is offline or a periodic job has failed to run and no data is being sent to New Relic. In order to avoid unnecessary notifications, you can choose how long to wait before you're notified by a loss of signal violation. You can use loss of signal detection to open violations and be notified when a signal is lost. Alternately, you can use a loss of signal to close violations for ephemeral services or sporadic data, such as error counts. Gap filling Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with the last value received, a static value, or else do nothing and leave the gap there. The default is None. Gaps in streaming data can be caused by network or host issues, a signal may be sparse, or some signals, such as error counts, may only have data when something is wrong. By filling the gaps with known values, the alert evaluation process can process those gaps and determine how they should affect the loss of signal evaluation. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss and gap filling, see this Explorers Hub post.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 229.45578,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "sections": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "The streaming <em>alerts</em> platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into <em>New</em> <em>Relic</em>. You can use NRQL <em>conditions</em> to control what part of the signal you want to be notified about. Your NRQL condition filters the data that&#x27;s processed"
      },
      "id": "6130c054e7b9d269edb6f21d"
    },
    {
      "sections": [
        "Outlier detection (NRQL alert)",
        "Important",
        "What is outlier detection?",
        "Tip",
        "Example use cases",
        "Notify if load-balanced servers have uneven workload",
        "Notify if load-balanced application has misbehaving instances",
        "Notify of changes in different environments",
        "Notify for timezone-related changes",
        "Create an outlier alert condition",
        "Rules and logic",
        "Details about alert condition logic",
        "NRQL query rules and limits",
        "Zero values for unreturned data"
      ],
      "title": "Outlier detection (NRQL alert)",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert conditions"
      ],
      "external_id": "499fa55abd48a0ccdd897fbdf64ccea2d9f98d11",
      "image": "https://docs.newrelic.com/static/f235d0630576bc2010ff07adc7a69621/f73a1/NRQL_outlier_violations.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/outlier-detection-nrql-alert/",
      "published_at": "2021-12-15T12:44:32Z",
      "updated_at": "2021-11-25T19:50:19Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Alerts offers NRQL conditions in three threshold types: static, baseline, and outlier. This document explains how the outlier threshold type works, gives some example use cases and NRQL queries, and explains how to create an outlier condition. Important NRQL alerts do not affect Alerts policies for a Synthetic monitor. For example, muting a NRQL alert will not mute a Synthetic monitor's alerts. What is outlier detection? In software development and operations, it is common to have a group consisting of members you expect to behave approximately the same. For example: for servers using a load balancer, the traffic to the servers may go up or down, but the traffic for all the servers should remain in a fairly tight grouping. See outlier detection in action in this NerdBytes video (2:51 minutes). The NRQL alert outlier detection feature parses the data returned by your faceted NRQL query and: Looks for the number of expected groups that you specify Looks for outliers (values deviating from a group) based on the sensitivity and time range you set Additionally, for queries that have more than one group, you can choose to be notified when groups start behaving the same. This visual aid will help you understand the types of situations that will trigger a violation and those that won't. For more on the rules and logic behind this calculation, see Outlier detection rules. Tip Note: this feature does not take into account the past behavior of the monitored values; it looks for outliers only in the currently reported data. For an alert type that takes into account past behavior, see Baseline alerting. Example use cases These use cases will help you understand when to use the outlier threshold type. Note that the outlier feature requires a NRQL query with a FACET clause. Notify if load-balanced servers have uneven workload A load balancer divides web traffic approximately evenly across five different servers. You can set a notification to be sent if any server starts getting significantly more or less traffic than the other servers. Example query: SELECT average(cpuPercent) FROM SystemSample WHERE apmApplicationNames = 'MY-APP-NAME' FACET hostname Copy Notify if load-balanced application has misbehaving instances Application instances behind a load balancer should have similar throughput, error rates, and response times. If an instance is in a bad state, or a load balancer is misconfigured, this will not be the case. Detecting one or two bad app instances using aggregate metrics may be difficult if there is not a significant rise in the overall error rate of the application. You can set a notification for when an app instance’s throughput, error rate, or response time deviates too far from the rest of the group. Example query: SELECT average(duration) FROM Transaction WHERE appName = 'MY-APP-NAME' FACET host Copy Notify of changes in different environments An application is deployed in two different environments, with ten application instances in each. One environment is experimental and gets more errors than the other. But the instances that are in the same environment should get approximately the same number of errors. You can set a notification for when an instance starts getting more errors than the other instances in the same environment. Also, you can set a notification for when the two environments start to have the same number of errors as each other. Notify for timezone-related changes The number of logged in users for a company is about the same for each of four applications, but varies significantly by each of the three time zones the company operates in. You can set a notification for when any application starts getting more or less traffic from a certain timezone than the other applications. Sometimes the traffic from the different time zones are the same, so you would set up the alert condition to not be notified if the time zone groups overlap. For more details on how this feature works, see Outlier rules and logic. Create an outlier alert condition To create a NRQL alert that uses outlier detection: When creating a condition, under Select a product, select NRQL. For Threshold type, select Outlier. Create a NRQL query with a FACET clause that returns the values you want to alert on. Depending on how the returned values group together, set the Number of expected groups. Adjust the deviation from the center of the group(s) and the duration that will trigger a violation. Optional: Add a warning threshold and set its deviation. Set any remaining available options and save. Rules and logic Here are the rules and logic behind how outlier detection works: Details about alert condition logic After the condition is created, the query is run once every harvest cycle and the condition is applied. Unlike baseline alerts, outlier detection uses no historical data in its calculation; it's calculated using the currently collected data. Alerts will attempt to divide the data returned from the query into the number of groups selected during condition creation. For each group, the approximate average value is calculated. The allowable deviation you have chosen when creating the condition is centered around that average value. If a member of the group is outside the allowed deviation, it produces a violation. If Trigger when groups overlap has been selected, Alerts detects a convergence of groups. If the condition is looking for two or more groups, and the returned values cannot be separated into that number of distinct groups, then that will produce a violation. This type of “overlap” event is represented on a chart by group bands touching. Because this feature does not take past behavior into account, data is never considered to \"belong\" to a certain group. For example, a value that switches places with another value wouldn't trigger a violation. Additionally, an entire group that moves together also wouldn't trigger a violation. NRQL query rules and limits The NRQL query must be a faceted query. The number of unique values returned must be 500 or less. If the query returns more than this number of values, the condition won't be created. If the query later returns more than this number after being created, the alert will fail. Zero values for unreturned data When a query returns a set of values, only values that are actually returned are taken into account. If a value is not available for calculation (including if it goes from being collected one harvest cycle to not being collected), it is rendered as a zero and is not considered. In other words, the behavior of unreturned zero values will never trigger violations.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.94467,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Outlier detection (NRQL <em>alert</em>)",
        "sections": "Create an outlier <em>alert</em> <em>condition</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "<em>Alerts</em> offers NRQL <em>conditions</em> in three threshold types: static, baseline, and outlier. This document explains how the outlier threshold type works, gives some example use cases and NRQL queries, and explains how to create an outlier condition. Important NRQL <em>alerts</em> do not affect <em>Alerts</em> policies"
      },
      "id": "6130be72196a6793654948e7"
    }
  ],
  "/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/set-thresholds-alert-condition": [
    {
      "sections": [
        "Create NRQL alert conditions",
        "Tip",
        "Create a NRQL alert condition",
        "Create a condition from a chart",
        "Caution",
        "NRQL alert syntax",
        "Important",
        "Reformatting incompatible NRQL",
        "NRQL alert threshold examples",
        "Alert on specific segments of your data",
        "Alert on Nth percentile of your data",
        "Alert on max, min, avg of your data",
        "Alert on a percentage of your data",
        "Alert on Apdex with any T-value",
        "NRQL conditions and query order of operations",
        "Example: null value returned",
        "Example: zero value returned",
        "Nested aggregation NRQL alerts",
        "Nested queries with a non-faceted innermost query are not currently supported",
        "Queries at all levels must have the same aggregation window size",
        "Signal loss is not yet supported for nested queries",
        "Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported",
        "NRQL condition creation tips",
        "Condition edits can reset condition evaluation",
        "Alert condition types",
        "Sum of query results (limited or intermittent data)",
        "Set the loss of signal threshold",
        "Advanced signal settings",
        "Aggregation window duration",
        "Delay/timer",
        "Fill data gaps"
      ],
      "title": "Create NRQL alert conditions",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert conditions"
      ],
      "external_id": "2872f770a5fc8a802f99b9f848906f7e351ad761",
      "image": "https://docs.newrelic.com/static/eb8e1b2d826f9fd9cf46fe3dd5455217/c1b63/nr1_nrql_alert_conditions.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/",
      "published_at": "2021-12-19T16:01:17Z",
      "updated_at": "2021-12-14T22:49:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use NRQL queries to create alert conditions. Once you've defined your signal, you can further define your warning and critical threshold levels. This determines when an alerts violation is created. Read on to learn more about how to do this. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Click NRQL, and then Next, define thresholds. Tip For more information on key concepts relating to NRQL alert conditions and streaming alerts, see Streaming alerts: key terms and concepts. Create a NRQL alert condition To create a NRQL alert condition for a policy: On one.newrelic.com, in the header click Alerts & AI, then in the left sidebar click Policies. Select an existing policy or click New alert policy to create a new policy. Click Add a condition. Under Select a product click NRQL, and then click Next, define thresholds. Note that editing an existing condition can result in resetting its evaluation. Create a condition from a chart You can use a chart to create a NRQL alert condition. To create a NRQL alerts condition from a chart, click the chart menu , then click Create alert condition. Once you've named and customized your condition, you can add it to an existing policy or create a new one. Caution A small number of our older charts don't include the option to create an alert condition. NRQL alert syntax Here's the basic syntax for creating all NRQL alert conditions. The FACET clause is required for outlier condition types. It's optional for static and baseline. SELECT function(attribute) FROM Event WHERE attribute [comparison] [AND|OR ...] Copy Clause Notes SELECT function(attribute) Required Supported functions that return numbers include: apdex average count latest max min percentage percentile sum uniqueCount Tip If you use the percentile aggregator in a faceted alert condition with many facets, this may cause the following error to appear: An error occurred while fetching chart data. If you see this error, use average instead. FROM data type Required Only one data type can be targeted. Supported data types: Event Metric (RAW data points will be returned) WHERE attribute [comparison] [AND|OR ...] Use the WHERE clause to specify a series of one or more conditions. All the operators are supported. FACET attribute Required for outlier conditions Include an optional FACET clause in your NRQL syntax depending on the threshold type: static or baseline. Use the FACET clause to separate your results by attribute and alert on each attribute independently. No LIMIT clause is allowed, but all queries will receive the maximum number of facets possible. Faceted queries can return a maximum of 5000 values for static and baseline conditions and a maximum of 500 values for outlier conditions. Important If the query returns more than the maximum number of values, the alert condition can't be created. If you create the condition and the query returns more than this number later, the alert will fail. Modify your query so that it returns a fewer number of values. Reformatting incompatible NRQL Some elements of NRQL used in charts don’t make sense in the streaming context of alerts. Here’s a list of the most common incompatible elements and suggestions for reformatting a NRQL alert query to achieve the same effect. Element Notes SINCE and UNTIL Example: SELECT percentile(largestContentfulPaint, 75) FROM PageViewTiming WHERE (appId = 837807) SINCE yesterday Copy NRQL conditions produce a never-ending stream of windowed query results, so the SINCE and UNTIL keywords to scope the query to a point in time are not compatible. As a convenience, we automatically strip SINCE and UNTIL from a query when creating a condition from the context of a chart. TIMESERIES In NRQL queries, the TIMESERIES clause is used to return data as a time series broken out by a specified period of time. For NRQL conditions, the equivalent property of a signal is the aggregation duration window. histogram() The histogram() aggregation function is used to generate histograms. histogram() is not compatible with NRQL alerting: histogram aggregations can not be formatted as a time series. To create an alert from a portion of a histogram (for example, 95th percentile), use the percentile() aggregation function. Multiple aggregation functions Each condition can only target a single aggregated value. To alert on multiple values simultaneously, you’ll need to decompose them into individual conditions within the same policy. Original Query: SELECT count(foo), average(bar), max(baz) from Transaction Copy Decomposed: SELECT count(foo) from Transaction SELECT average(bar) from Transaction SELECT max(baz) from Transaction Copy COMPARE WITH The COMPARE WITH clause is used to compare the values for two different time ranges. This type of query is incompatible with NRQL alerting. We recommend using a Baseline Alert Condition to dynamically detect deviations for a particular signal. SLIDE BY The SLIDE BY clause supports a feature known as sliding windows. With sliding windows, SLIDE BY data is gathered into \"windows\" of time that overlap with each other. These windows can help to smooth out line graphs with a lot of variation in cases where the rolling aggregate (such as a rolling mean) is more important than aggregates from narrow windows of time. Sliding windows are not currently supported in NRQL alerts. LIMIT In NRQL queries, the LIMIT clause is used to control the amount of data a query returns, either the maximum number of facet values returned by FACET queries or the maximum number of items returned by SELECT * queries. LIMIT is not compatible with NRQL alerting: evaluation is always performed on the full result set. NRQL alert threshold examples Here are some common use cases for NRQL conditions. These queries will work for static and baseline condition types. The outlier condition type will require additional FACET clauses. Alert on specific segments of your data Create constrained alerts that target a specific segment of your data, such as a few key customers or a range of data. Use the WHERE clause to define those conditions. SELECT average(duration) FROM Transaction WHERE account_id in (91290, 102021, 20230) Copy SELECT percentile(duration, 95) FROM Transaction WHERE name LIKE 'Controller/checkout/%' Copy Alert on Nth percentile of your data Create alerts when an Nth percentile of your data hits a specified threshold; for example, maintaining SLA service levels. Since we evaluate the NRQL query based on the aggregation window duration, percentiles will be calculated for each duration separately. SELECT percentile(duration, 95) FROM Transaction Copy SELECT percentile(databaseDuration, 75) FROM Transaction Copy Alert on max, min, avg of your data Create alerts when your data hits a certain maximum, minimum, or average; for example, ensuring that a duration or response time does not pass a certain threshold. SELECT max(duration) FROM Transaction Copy SELECT average(duration) FROM Transaction Copy Alert on a percentage of your data Create alerts when a proportion of your data goes above or below a certain threshold. SELECT percentage(count(*), WHERE duration > 2) FROM Transaction Copy SELECT percentage(count(*), WHERE httpResponseCode = '500') FROM Transaction Copy Alert on Apdex with any T-value Create alerts on Apdex, applying your own T-value for certain transactions. For example, get an alert notification when your Apdex for a T-value of 500ms on transactions for production apps goes below 0.8. SELECT apdex(duration, t:0.5) FROM Transaction WHERE appName like '%prod%' Copy NRQL conditions and query order of operations By default, the aggregation window duration is 1 minute, but you can change the window to suit your needs. Whatever the aggregation window, New Relic will collect data for that window using the function in the NRQL condition’s query. The query is parsed and executed by our systems in the following order: FROM clause – which event type needs to be grabbed? WHERE clause – what can be filtered out? SELECT clause – what information needs to be returned from the now-filtered data set? Example: null value returned Let's say this is your alert condition query: SELECT count(*) FROM SyntheticCheck WHERE monitorName = 'My Cool Monitor' AND result = 'FAILURE' Copy If there are no failures for the aggregation window: The system will execute the FROM clause by grabbing all SyntheticCheck events on your account. Then it will execute the WHERE clause to filter through those events by looking only for the ones that match the monitor name and result specified. If there are still events left to scan through after completing the FROM and WHERE operations, the SELECT clause will be executed. If there are no remaining events, the SELECT clause will not be executed. This means that aggregators like count() and uniqueCount() will never return a zero value. When there is a count of 0, the SELECT clause is ignored and no data is returned, resulting in a value of NULL. Example: zero value returned If you have a data source delivering legitimate numeric zeroes, the query will return zero values and not null values. Let's say this is your alert condition query, and that MyCoolEvent is an attribute that can sometimes return a zero value. SELECT average(MyCoolAttribute) FROM MyCoolEvent Copy If, in the aggregation window being evaluated, there's at least one instance of MyCoolEvent and if the average value of all MyCoolAttribute attributes from that window is equal to zero, then a 0 value will be returned. If there are no MyCoolEvent events during that minute, then a NULL will be returned due to the order of operations. Tip For more information about this topic, you can check out our blog post on troubleshooting for zero versus null values. Tip You can determine how null values will be handled by adjusting loss of signal and gap filling settings in the Alert Conditions UI. Tip You can avoid NULL values entirely with a query order of operations shortcut. Do this by using a filter sub-clause, then including all filter elements within that sub-clause. The main body of the query will run and return data, at which point the SELECT clause will then run and apply the filter elements. The query will return a value of 0 if the filter elements result in no matching data. Here's an example: SELECT filter(count(*), WHERE result = 'SUCCESS' AND monitorName = 'My Favorite Monitor') FROM SyntheticCheck Copy Nested aggregation NRQL alerts Nested aggregation queries are a powerful way to query your data. However, they have a few restrictions that are important to note. Nested queries with a non-faceted innermost query are not currently supported Without a FACET, the inner query produces a single result, giving the outer query nothing to aggregate. If you're using a nested query, make sure your inner query is faceted. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu) ​​​​​ Copy Queries at all levels must have the same aggregation window size With an alert aggregation window of 1 minute, the inner query would produce two smaller windows of 30 seconds. In theory, these two windows could be aggregated by the outer query. However, this is not currently supported. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu TIMESERIES 30 seconds)​​ Copy Signal loss is not yet supported for nested queries For more information on signal loss, see NerdGraph API: Loss of signal and gap filling. Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported You can't use a nested query containing the WITH METRIC_FORMAT in the inner query to create NRQL alert conditions. NRQL condition creation tips Here are some tips for creating and using a NRQL condition: Topic Tips Condition types NRQL condition types include static, baseline, and outlier. Create a description For NRQL conditions, you can create a custom description to add to each violation. Descriptions can be enhanced with variable substitution based on metadata in the specific violation. For details, see Description Query results Queries must return a number. The condition evaluates the returned number against the thresholds you've set. Time period NRQL conditions evaluate data based on how it's aggregated, using aggregation windows from 30 seconds to 15 minutes, in increments of 15 seconds. For best results, we recommend using the event flow or event timer aggregation methods. For the cadence aggregation method, the implicit SINCE ... UNTIL clause specifying which minute to evaluate is controlled by your delay/timer setting. Since very recent data may be incomplete, you may want to query data from 3 minutes ago or longer, especially for: Applications that run on multiple hosts. SyntheticCheck data: Timeouts can take 3 minutes, so 5 minutes or more is recommended. Also, if a query will generate intermittent data, consider using the sum of query results option. Lost signal threshold (loss of signal detection) You can use loss of signal detection to alert on when your data (a telemetry signal) should be considered lost. A signal loss can indicate that a service or entity is no longer online or that a periodic job failed to run. You can also use this to make sure that violations for sporadic data, such as error counts, are closed when no signal is coming in. Advanced signal settings These settings give you options for better handling continuous, streaming data signals that may sometimes be missing. These settings include the aggregation window duration, the delay/timer, and an option for filling data gaps. For more on using these, see Advanced signal settings. Condition settings Use the Condition settings to: Create a concise, descriptive condition name. Provide a custom violation description for the condition that will be included in violations and notifications. Add the runbook URL to include your organization's procedures for handling incidents. You may also add this information to the custom violation description. Limits on conditions See the maximum values. Health status In order for a NRQL alert condition health status display to function properly, use a FACET clause to scope each signal to a single entity (for example, FACET hostname or FACET appname). Examples For more information, see: Expected NRQL syntax Examples of NRQL condition queries Condition edits can reset condition evaluation When you edit NRQL alert conditions in some specific ways (detailed below), their evaluations are reset, meaning that any evaluation up until that point is lost, and the evaluation starts over from that point. The two ways this will affect you are: For \"for at least x minutes\" thresholds: because the evaluation window has been reset, there will be a delay of at least x minutes before any violations can be reported. For baseline conditions: the condition starts over again and all baseline learning is lost. The following actions cause an evaluation reset for NRQL conditions: Changing the query Changing the aggregation window, aggregation method, or aggregation delay/timer setting Changing the \"close violations on signal loss\" setting Changing any gap fill settings Changing the baseline direction (if applicable) – higher, lower, or higher/lower Change the threshold value, threshold window, or threshold operator The following actions (along with any other actions not covered in the above list) will not reset the evaluation: Changing the loss of signal time window (expiration duration) Changing the time function (switching \"for at least\" to \"at least once in,\" or vice-versa) Toggling the \"open violation on signal loss\" setting Alert condition types When you create a NRQL alert, you can choose from different types of conditions: NRQL alert condition types Description Static This is the simplest type of NRQL condition. It allows you to create a condition based on a NRQL query that returns a numeric value. Optional: Include a FACET clause. Baseline (Dynamic) Uses a self-adjusting condition based on the past behavior of the monitored values. Uses the same NRQL query form as the static type, including the optional FACET clause. Outlier Looks for group behavior and values that are outliers from those groups. Uses the same NRQL query form as the static type, but requires a FACET clause. Sum of query results (limited or intermittent data) Important Available only for static (basic) condition types. If a query returns intermittent or limited data, it may be difficult to set a meaningful threshold. Missing or limited data will sometimes generate false positives or false negatives. You can use loss of signal, aggregation duration, and gap filling settings to minimize these false notifications. To avoid this problem when using the static threshold type, you can set the selector to sum of query results. This lets you set the alert on an aggregated sum instead of a value from a single harvest cycle. Up to two hours of one-minute data checks can be aggregated. The duration you select determines the width of the rolling sum and the preview chart will update accordingly. Set the loss of signal threshold Loss of signal occurs when no data matches the NRQL condition over a specific period of time. You can set your loss of signal threshold duration and also what happens when the threshold is crossed. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Loss of signal is only available for NRQL conditions. You may also manage these settings using the GraphQL API (recommended), or the REST API. Go here for specific GraphQL API examples. Loss of signal settings: Loss of signal settings include a time duration and two possible actions. Signal loss expiration time UI label: Signal is lost after: GraphQL Node: expiration.expirationDuration Expiration duration is a timer that starts and resets when we receive a data point in the streaming alerts pipeline. If we don't receive another data point before your 'expiration time' expires, we consider that signal to be lost. This can be because no data is being sent to New Relic or the WHERE clause of your NRQL query is filtering that data out before it is streamed to the alerts pipeline. Note that when you have a faceted query, each facet is a signal. So if any one of those signals ends during the duration specified, that will be considered a loss of signal. The loss of signal expiration time is independent of the threshold duration and triggers as soon as the timer expires. The maximum expiration duration is 48 hours. This is helpful when monitoring for the execution of infrequent jobs. The minimum is 30 seconds, but we recommend using at least 3-5 minutes. Loss of signal actions Once a signal is considered lost, you can close open violations, open new violations, or both. Close all current open violations: This closes all open violations that are related to a specific signal. It won't necessarily close all violations for a condition. If you're alerting on an ephemeral service, or on a sporadic signal, you'll want to choose this action to ensure that violations are closed properly. The GraphQL node name for this is \"closeViolationsOnExpiration\" Open new violations: This will open a new violation when the signal is considered lost. These violations will indicate that they are due to a loss of signal. Based on your incident preferences, this should trigger a notification. The graphQL node name for this is \"openViolationOnExpiration\" When you enable both actions, we'll close all open violations first, and then open a new violation for loss of signal. To create a NRQL alert configured with loss of signal detection in the UI: For a policy, when you create a condition, under Select a product, click NRQL, then click Next, define thresholds. Write a NRQL query that returns the values you want to alert on. For Threshold type, select Static or Baseline. Click + Add lost signal threshold, then set the signal expiration duration time in minutes or seconds in the Signal is lost after field. Choose what you want to happen when the signal is lost. You can check one or both of Close all current open violations and Open new \"lost signal\" violation. These control how loss of signal violations will be handled for the condition. Make sure you name your condition before you save it. Violations open due to loss of signal close when the signal comes back. Newly opened lost signal violations will close immediately when new data is evaluated. the condition they belong to expires. By default, conditions expire after 3 days. you manually close the violation with the Close all current open violations option. Tip Loss of signal detection doesn't work on NRQL queries that use nested aggregation or sub-queries. Advanced signal settings When creating a NRQL alert condition, use the advanced signal settings to control streaming alert data and avoid false alarms. When creating a NRQL condition, there are several advanced signal settings: Aggregation window duration Delay/timer Fill data gaps To read an explanation of what these settings are and how they relate to each other, see Streaming alerts concepts. Below are instructions and tips on how to configure them. Aggregation window duration You can set the aggregation window duration to choose how long data is accumulated in a streaming time window before it's aggregated. You can set it to anything between 30 seconds and 15 minutes. The default is one minute. Tip Baseline alert condition thresholds don't support editing the aggregation window. They use the 1 minute default. Delay/timer You can adjust the delay/timer to coordinate our streaming alerting algorithm with your data's behavior. If your data is sparse or inconsistent, you may want to use the event timer aggregation method. For the cadence method, the total supported latency is the sum of the aggregation window duration and the delay. If the data type comes from an APM language agent and is aggregated from many app instances (for example, Transactions, TransactionErrors, etc.), we recommend using the event flow method with the default settings. Important When creating NRQL conditions for data collected from Infrastructure Cloud Integrations such as AWS Cloudwatch or Azure, we recommend that you use the event timer method. Fill data gaps Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with one of these settings: None: (Default) Choose this if you don't want to take any action on empty aggregation windows. On evaluation, an empty aggregation window will reset the threshold duration timer. For example, if a condition says that all aggregation windows must have data points above the threshold for 5 minutes, and 1 of the 5 aggregation windows is empty, then the condition won't be in violation. Custom static value: Choose this if you'd like to insert a custom static value into the empty aggregation windows before they're evaluated. This option has an additional, required parameter of fillValue (as named in the API) that specifies what static value should be used. This defaults to 0. Last known value: This option inserts the last seen value before evaluation occurs. We maintain the state of the last seen value for 2 hours. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss, gap filling, and how to request access to these features, see this Explorers Hub post. Options for editing data gap settings: In the NRQL conditions UI, go to Condition settings > Advanced signal settings > fill data gaps with and choose an option. If using our Nerdgraph API (preferred), this node is located at: actor : account : alerts : nrqlCondition : signal : fillOption | fillValue NerdGraph is our recommended API for this but if you're using our REST API, you can find this setting in the REST API explorer under the \"signal\" section of the Alert NRQL conditions API.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 251.85617,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create NRQL <em>alert</em> <em>conditions</em>",
        "sections": "Create NRQL <em>alert</em> <em>conditions</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "You can use NRQL queries to create <em>alert</em> <em>conditions</em>. Once you&#x27;ve defined your signal, you can further define your warning and critical threshold levels. This determines when an <em>alerts</em> violation is created. Read on to learn more about how to do this. Go to one.newrelic.com, click <em>Alerts</em> &amp; AI"
      },
      "id": "603ef04864441fbc114e8883"
    },
    {
      "sections": [
        "Streaming alerts: key terms and concepts",
        "Why it matters",
        "Streaming alerts process and descriptions",
        "Choose your aggregation method",
        "Event flow",
        "Caution",
        "Event timer",
        "Cadence",
        "Streaming alerts tools",
        "Tip",
        "Window duration",
        "Delay/timer",
        "Loss of signal detection",
        "Gap filling"
      ],
      "title": "Streaming alerts: key terms and concepts",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Get started"
      ],
      "external_id": "144c7f0721ec2acb1e8ac8aa06142911ea552aef",
      "image": "https://docs.newrelic.com/static/39158bd84483adf9516bb7ff5058c9bd/d30ee/streaming-alerts.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/streaming-alerts-key-terms-concepts/",
      "published_at": "2021-12-15T12:47:58Z",
      "updated_at": "2021-12-15T12:47:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The streaming alerts platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into New Relic. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that's processed by the streaming algorithm. There are three methods for aggregating the data filtered through your NRQL condition: Event flow (default) Event timer Cadence This short video describes the three aggregation method (5:31). Why it matters Understanding how streaming alerts works will help you fine-tune your NRQL conditions to be notified about what's important to you. Only data that matches the conditions of the NRQL WHERE clause is alerted on. For more details on each step of the process, see Streaming alerts process and descriptions. As data streams into New Relic, it's filtered by the NRQL condition. Before data is evaluated, it must meet the criteria defined by the NRQL query's WHERE clause. Instead of evaluating that data immediately for violations, the NRQL alert conditions collect the data over a period of time known as the aggregation window. An additional delay/timer allows for slower data points to arrive before the window is aggregated. Once the delay/timer time has elapsed, New Relic aggregates the data into a single data point. Alerts then evaluates the data point using the NRQL condition in order to determine whether it meets the violation threshold criteria. Even if a data point meets the criteria for a violation, a violation may not be triggered. A violation is only triggered when data points consistently meet the threshold criteria over a period of time. This is the threshold duration. If the data points are in violation for an entire threshold duration, we'll send you a notification based on your policy settings. All of these configurable delays give you more control over how you're alerted on sporadic and missing data. Streaming alerts process and descriptions Process Description Streaming data All data coming into New Relic. WHERE clause Filters all incoming streaming data. We only monitor for alerts on data that makes it through this filter. Aggregation methods One of three methods that control how data is collected before it's evaluated. They are: Event flow (Default) Event timer Cadence Aggregation window Data with timestamps that fall within this window will be aggregated and then evaluated. Delay/timer A time delay to ensure all data points have arrived in the aggregation window before aggregation occurs. Aggregated data Data in the aggregated window is collapsed to a single data point for alert evaluation. Evaluation The data point is evaluated by the NRQL condition, which is trigged by each incoming aggregated data point. Threshold duration A specific duration that determines if a violation is created. If your specified NRQL condition meets the threshold criteria over the threshold duration, a violation occurs. When a data point lacks data, a custom value is inserted to fill the gap. Choose your aggregation method You can choose between three different aggregation methods, depending on your needs. Event flow (default) works best for data that comes in frequently and mostly in order. Event timer works best for data that arrives infrequently in batches, such as cloud integration data or infrequent error logs. Cadence is our original and inferior aggregation method. It aggregates data on specific time intervals as detected by New Relic's internal wall clock, regardless of data timestamps. Event flow Event flow aggregates a window of data when the first data point arrives in a subsequent window. The custom delay defines which subsequent window data will start to populate to trigger aggregation of the current window. A custom delay provides extra time for data to arrive. These times are based on the data's timestamps and not New Relic's wall clock time. For example, suppose you're monitoring CPU usage in window durations of 1 minute and a 3 minute delay. When a CPU usage data point comes in with a timestamp between 12:00pm and 12:01pm, event flow will not aggregate that window until a data point shows up with a timestamp between 12:04pm and 12:05pm. When event flow receives the first data point with a timestamp of 12:04pm or later, it sends the 12:00 to 12:01 data to be aggregated. Caution If you expect your data points to arrive more than 30 minutes apart, please use the Event Timer method described below. Event timer Like event flow, event timer will only aggregate data for a given window when data arrives for that window. When a data point arrives for an aggregation window, a timer dedicated to that window starts to count down. If no further data arrives before the timer counts down, the data for that window is aggregated. If more data points arrive before the timer has completed counting down, the timer is reset. For example, suppose you're monitoring CloudWatch data that arrives fairly infrequently. You're using a window duration of 1 minute and a 3 minute timer. When a CloudWatch data point comes in with a timestamp between 12:00pm and 12:01pm, the timer will start to count down. If no further data points show up for that 12:00-12:01 window, the window will get aggregated 3 minutes later. If a new data point with a timestamp between 12:00 and 12:01 arrives, the timer resets. It keeps resetting every time more data points for that window arrive. The window will not be sent for aggregation until the timer reaches 0. If the timer for a later data point elapses before an earlier data point, the event timer method waits for the earlier timer to elapse before aggregating the later data point. For best results, make sure your timer is equal to or longer than your window duration time. If the timer is less than your window duration and your data flow is inconsistent, then your data may be evaluated before all of your data points arrive. This could cause you to be notified incorrectly. Cadence We recommend you use one of the other two methods. Cadence is our old streaming aggregation method. This method uses New Relic's wall clock time to determine when data is aggregated and evaluated. It doesn't take into account data point timestamps as they arrive. Streaming alerts tools Streaming alerts provide a set of tools to give you greater control over how your data is aggregated before it's evaluated to reduce incorrect notifications you receive. They are: Window duration Delay/timer Loss of signal detection Gap filling Tip This article covers these tools at a conceptual level. You'll find direct instructions on how to use these tools in Create NRQL alert conditions. Window duration In order to make loss of signal detection more effective and to reduce unnecessary notifications, you can customize aggregation windows to the duration that you need. An aggregation window is a specific block of time. We gather data points together in an aggregation window, before evaluating the data. A longer aggregation window can smooth out the data, since an outlier data point will have more data points to be aggregated with, giving it less of an influence on the aggregated data point that is sent for evaluation. When a data point arrives, its timestamp is used to put it in the proper aggregation window. You can set your aggregation window to anything between 30 seconds and 15 minutes. The default is 1 minute. Delay/timer The delay/timer setting controls how long the condition should wait before aggregating the data in the aggregation window. The event flow and cadence methods use delay. Event timer uses timer. The delay default is 2 minutes. The timer default is 1 minute and has a minimum value of 5 seconds. Loss of signal detection Loss of signal occurs when no data matches the NRQL condition over a specific period of time. A loss of signal is caused by different things. The WHERE clause in your NRQL query can filter out data before it's evaluated for violations. It could also mean a service or entity is offline or a periodic job has failed to run and no data is being sent to New Relic. In order to avoid unnecessary notifications, you can choose how long to wait before you're notified by a loss of signal violation. You can use loss of signal detection to open violations and be notified when a signal is lost. Alternately, you can use a loss of signal to close violations for ephemeral services or sporadic data, such as error counts. Gap filling Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with the last value received, a static value, or else do nothing and leave the gap there. The default is None. Gaps in streaming data can be caused by network or host issues, a signal may be sparse, or some signals, such as error counts, may only have data when something is wrong. By filling the gaps with known values, the alert evaluation process can process those gaps and determine how they should affect the loss of signal evaluation. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss and gap filling, see this Explorers Hub post.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 229.45578,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "sections": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "The streaming <em>alerts</em> platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into <em>New</em> <em>Relic</em>. You can use NRQL <em>conditions</em> to control what part of the signal you want to be notified about. Your NRQL condition filters the data that&#x27;s processed"
      },
      "id": "6130c054e7b9d269edb6f21d"
    },
    {
      "sections": [
        "Outlier detection (NRQL alert)",
        "Important",
        "What is outlier detection?",
        "Tip",
        "Example use cases",
        "Notify if load-balanced servers have uneven workload",
        "Notify if load-balanced application has misbehaving instances",
        "Notify of changes in different environments",
        "Notify for timezone-related changes",
        "Create an outlier alert condition",
        "Rules and logic",
        "Details about alert condition logic",
        "NRQL query rules and limits",
        "Zero values for unreturned data"
      ],
      "title": "Outlier detection (NRQL alert)",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert conditions"
      ],
      "external_id": "499fa55abd48a0ccdd897fbdf64ccea2d9f98d11",
      "image": "https://docs.newrelic.com/static/f235d0630576bc2010ff07adc7a69621/f73a1/NRQL_outlier_violations.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/outlier-detection-nrql-alert/",
      "published_at": "2021-12-15T12:44:32Z",
      "updated_at": "2021-11-25T19:50:19Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Alerts offers NRQL conditions in three threshold types: static, baseline, and outlier. This document explains how the outlier threshold type works, gives some example use cases and NRQL queries, and explains how to create an outlier condition. Important NRQL alerts do not affect Alerts policies for a Synthetic monitor. For example, muting a NRQL alert will not mute a Synthetic monitor's alerts. What is outlier detection? In software development and operations, it is common to have a group consisting of members you expect to behave approximately the same. For example: for servers using a load balancer, the traffic to the servers may go up or down, but the traffic for all the servers should remain in a fairly tight grouping. See outlier detection in action in this NerdBytes video (2:51 minutes). The NRQL alert outlier detection feature parses the data returned by your faceted NRQL query and: Looks for the number of expected groups that you specify Looks for outliers (values deviating from a group) based on the sensitivity and time range you set Additionally, for queries that have more than one group, you can choose to be notified when groups start behaving the same. This visual aid will help you understand the types of situations that will trigger a violation and those that won't. For more on the rules and logic behind this calculation, see Outlier detection rules. Tip Note: this feature does not take into account the past behavior of the monitored values; it looks for outliers only in the currently reported data. For an alert type that takes into account past behavior, see Baseline alerting. Example use cases These use cases will help you understand when to use the outlier threshold type. Note that the outlier feature requires a NRQL query with a FACET clause. Notify if load-balanced servers have uneven workload A load balancer divides web traffic approximately evenly across five different servers. You can set a notification to be sent if any server starts getting significantly more or less traffic than the other servers. Example query: SELECT average(cpuPercent) FROM SystemSample WHERE apmApplicationNames = 'MY-APP-NAME' FACET hostname Copy Notify if load-balanced application has misbehaving instances Application instances behind a load balancer should have similar throughput, error rates, and response times. If an instance is in a bad state, or a load balancer is misconfigured, this will not be the case. Detecting one or two bad app instances using aggregate metrics may be difficult if there is not a significant rise in the overall error rate of the application. You can set a notification for when an app instance’s throughput, error rate, or response time deviates too far from the rest of the group. Example query: SELECT average(duration) FROM Transaction WHERE appName = 'MY-APP-NAME' FACET host Copy Notify of changes in different environments An application is deployed in two different environments, with ten application instances in each. One environment is experimental and gets more errors than the other. But the instances that are in the same environment should get approximately the same number of errors. You can set a notification for when an instance starts getting more errors than the other instances in the same environment. Also, you can set a notification for when the two environments start to have the same number of errors as each other. Notify for timezone-related changes The number of logged in users for a company is about the same for each of four applications, but varies significantly by each of the three time zones the company operates in. You can set a notification for when any application starts getting more or less traffic from a certain timezone than the other applications. Sometimes the traffic from the different time zones are the same, so you would set up the alert condition to not be notified if the time zone groups overlap. For more details on how this feature works, see Outlier rules and logic. Create an outlier alert condition To create a NRQL alert that uses outlier detection: When creating a condition, under Select a product, select NRQL. For Threshold type, select Outlier. Create a NRQL query with a FACET clause that returns the values you want to alert on. Depending on how the returned values group together, set the Number of expected groups. Adjust the deviation from the center of the group(s) and the duration that will trigger a violation. Optional: Add a warning threshold and set its deviation. Set any remaining available options and save. Rules and logic Here are the rules and logic behind how outlier detection works: Details about alert condition logic After the condition is created, the query is run once every harvest cycle and the condition is applied. Unlike baseline alerts, outlier detection uses no historical data in its calculation; it's calculated using the currently collected data. Alerts will attempt to divide the data returned from the query into the number of groups selected during condition creation. For each group, the approximate average value is calculated. The allowable deviation you have chosen when creating the condition is centered around that average value. If a member of the group is outside the allowed deviation, it produces a violation. If Trigger when groups overlap has been selected, Alerts detects a convergence of groups. If the condition is looking for two or more groups, and the returned values cannot be separated into that number of distinct groups, then that will produce a violation. This type of “overlap” event is represented on a chart by group bands touching. Because this feature does not take past behavior into account, data is never considered to \"belong\" to a certain group. For example, a value that switches places with another value wouldn't trigger a violation. Additionally, an entire group that moves together also wouldn't trigger a violation. NRQL query rules and limits The NRQL query must be a faceted query. The number of unique values returned must be 500 or less. If the query returns more than this number of values, the condition won't be created. If the query later returns more than this number after being created, the alert will fail. Zero values for unreturned data When a query returns a set of values, only values that are actually returned are taken into account. If a value is not available for calculation (including if it goes from being collected one harvest cycle to not being collected), it is rendered as a zero and is not considered. In other words, the behavior of unreturned zero values will never trigger violations.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.94467,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Outlier detection (NRQL <em>alert</em>)",
        "sections": "Create an outlier <em>alert</em> <em>condition</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "<em>Alerts</em> offers NRQL <em>conditions</em> in three threshold types: static, baseline, and outlier. This document explains how the outlier threshold type works, gives some example use cases and NRQL queries, and explains how to create an outlier condition. Important NRQL <em>alerts</em> do not affect <em>Alerts</em> policies"
      },
      "id": "6130be72196a6793654948e7"
    }
  ],
  "/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/view-events-their-products": [
    {
      "sections": [
        "Streaming alerts: key terms and concepts",
        "Why it matters",
        "Streaming alerts process and descriptions",
        "Choose your aggregation method",
        "Event flow",
        "Caution",
        "Event timer",
        "Cadence",
        "Streaming alerts tools",
        "Tip",
        "Window duration",
        "Delay/timer",
        "Loss of signal detection",
        "Gap filling"
      ],
      "title": "Streaming alerts: key terms and concepts",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Get started"
      ],
      "external_id": "144c7f0721ec2acb1e8ac8aa06142911ea552aef",
      "image": "https://docs.newrelic.com/static/39158bd84483adf9516bb7ff5058c9bd/d30ee/streaming-alerts.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/streaming-alerts-key-terms-concepts/",
      "published_at": "2021-12-15T12:47:58Z",
      "updated_at": "2021-12-15T12:47:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The streaming alerts platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into New Relic. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that's processed by the streaming algorithm. There are three methods for aggregating the data filtered through your NRQL condition: Event flow (default) Event timer Cadence This short video describes the three aggregation method (5:31). Why it matters Understanding how streaming alerts works will help you fine-tune your NRQL conditions to be notified about what's important to you. Only data that matches the conditions of the NRQL WHERE clause is alerted on. For more details on each step of the process, see Streaming alerts process and descriptions. As data streams into New Relic, it's filtered by the NRQL condition. Before data is evaluated, it must meet the criteria defined by the NRQL query's WHERE clause. Instead of evaluating that data immediately for violations, the NRQL alert conditions collect the data over a period of time known as the aggregation window. An additional delay/timer allows for slower data points to arrive before the window is aggregated. Once the delay/timer time has elapsed, New Relic aggregates the data into a single data point. Alerts then evaluates the data point using the NRQL condition in order to determine whether it meets the violation threshold criteria. Even if a data point meets the criteria for a violation, a violation may not be triggered. A violation is only triggered when data points consistently meet the threshold criteria over a period of time. This is the threshold duration. If the data points are in violation for an entire threshold duration, we'll send you a notification based on your policy settings. All of these configurable delays give you more control over how you're alerted on sporadic and missing data. Streaming alerts process and descriptions Process Description Streaming data All data coming into New Relic. WHERE clause Filters all incoming streaming data. We only monitor for alerts on data that makes it through this filter. Aggregation methods One of three methods that control how data is collected before it's evaluated. They are: Event flow (Default) Event timer Cadence Aggregation window Data with timestamps that fall within this window will be aggregated and then evaluated. Delay/timer A time delay to ensure all data points have arrived in the aggregation window before aggregation occurs. Aggregated data Data in the aggregated window is collapsed to a single data point for alert evaluation. Evaluation The data point is evaluated by the NRQL condition, which is trigged by each incoming aggregated data point. Threshold duration A specific duration that determines if a violation is created. If your specified NRQL condition meets the threshold criteria over the threshold duration, a violation occurs. When a data point lacks data, a custom value is inserted to fill the gap. Choose your aggregation method You can choose between three different aggregation methods, depending on your needs. Event flow (default) works best for data that comes in frequently and mostly in order. Event timer works best for data that arrives infrequently in batches, such as cloud integration data or infrequent error logs. Cadence is our original and inferior aggregation method. It aggregates data on specific time intervals as detected by New Relic's internal wall clock, regardless of data timestamps. Event flow Event flow aggregates a window of data when the first data point arrives in a subsequent window. The custom delay defines which subsequent window data will start to populate to trigger aggregation of the current window. A custom delay provides extra time for data to arrive. These times are based on the data's timestamps and not New Relic's wall clock time. For example, suppose you're monitoring CPU usage in window durations of 1 minute and a 3 minute delay. When a CPU usage data point comes in with a timestamp between 12:00pm and 12:01pm, event flow will not aggregate that window until a data point shows up with a timestamp between 12:04pm and 12:05pm. When event flow receives the first data point with a timestamp of 12:04pm or later, it sends the 12:00 to 12:01 data to be aggregated. Caution If you expect your data points to arrive more than 30 minutes apart, please use the Event Timer method described below. Event timer Like event flow, event timer will only aggregate data for a given window when data arrives for that window. When a data point arrives for an aggregation window, a timer dedicated to that window starts to count down. If no further data arrives before the timer counts down, the data for that window is aggregated. If more data points arrive before the timer has completed counting down, the timer is reset. For example, suppose you're monitoring CloudWatch data that arrives fairly infrequently. You're using a window duration of 1 minute and a 3 minute timer. When a CloudWatch data point comes in with a timestamp between 12:00pm and 12:01pm, the timer will start to count down. If no further data points show up for that 12:00-12:01 window, the window will get aggregated 3 minutes later. If a new data point with a timestamp between 12:00 and 12:01 arrives, the timer resets. It keeps resetting every time more data points for that window arrive. The window will not be sent for aggregation until the timer reaches 0. If the timer for a later data point elapses before an earlier data point, the event timer method waits for the earlier timer to elapse before aggregating the later data point. For best results, make sure your timer is equal to or longer than your window duration time. If the timer is less than your window duration and your data flow is inconsistent, then your data may be evaluated before all of your data points arrive. This could cause you to be notified incorrectly. Cadence We recommend you use one of the other two methods. Cadence is our old streaming aggregation method. This method uses New Relic's wall clock time to determine when data is aggregated and evaluated. It doesn't take into account data point timestamps as they arrive. Streaming alerts tools Streaming alerts provide a set of tools to give you greater control over how your data is aggregated before it's evaluated to reduce incorrect notifications you receive. They are: Window duration Delay/timer Loss of signal detection Gap filling Tip This article covers these tools at a conceptual level. You'll find direct instructions on how to use these tools in Create NRQL alert conditions. Window duration In order to make loss of signal detection more effective and to reduce unnecessary notifications, you can customize aggregation windows to the duration that you need. An aggregation window is a specific block of time. We gather data points together in an aggregation window, before evaluating the data. A longer aggregation window can smooth out the data, since an outlier data point will have more data points to be aggregated with, giving it less of an influence on the aggregated data point that is sent for evaluation. When a data point arrives, its timestamp is used to put it in the proper aggregation window. You can set your aggregation window to anything between 30 seconds and 15 minutes. The default is 1 minute. Delay/timer The delay/timer setting controls how long the condition should wait before aggregating the data in the aggregation window. The event flow and cadence methods use delay. Event timer uses timer. The delay default is 2 minutes. The timer default is 1 minute and has a minimum value of 5 seconds. Loss of signal detection Loss of signal occurs when no data matches the NRQL condition over a specific period of time. A loss of signal is caused by different things. The WHERE clause in your NRQL query can filter out data before it's evaluated for violations. It could also mean a service or entity is offline or a periodic job has failed to run and no data is being sent to New Relic. In order to avoid unnecessary notifications, you can choose how long to wait before you're notified by a loss of signal violation. You can use loss of signal detection to open violations and be notified when a signal is lost. Alternately, you can use a loss of signal to close violations for ephemeral services or sporadic data, such as error counts. Gap filling Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with the last value received, a static value, or else do nothing and leave the gap there. The default is None. Gaps in streaming data can be caused by network or host issues, a signal may be sparse, or some signals, such as error counts, may only have data when something is wrong. By filling the gaps with known values, the alert evaluation process can process those gaps and determine how they should affect the loss of signal evaluation. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss and gap filling, see this Explorers Hub post.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.07332,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "sections": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "The streaming <em>alerts</em> platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into <em>New</em> <em>Relic</em>. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that&#x27;s processed"
      },
      "id": "6130c054e7b9d269edb6f21d"
    },
    {
      "sections": [
        "Create NRQL alert conditions",
        "Tip",
        "Create a NRQL alert condition",
        "Create a condition from a chart",
        "Caution",
        "NRQL alert syntax",
        "Important",
        "Reformatting incompatible NRQL",
        "NRQL alert threshold examples",
        "Alert on specific segments of your data",
        "Alert on Nth percentile of your data",
        "Alert on max, min, avg of your data",
        "Alert on a percentage of your data",
        "Alert on Apdex with any T-value",
        "NRQL conditions and query order of operations",
        "Example: null value returned",
        "Example: zero value returned",
        "Nested aggregation NRQL alerts",
        "Nested queries with a non-faceted innermost query are not currently supported",
        "Queries at all levels must have the same aggregation window size",
        "Signal loss is not yet supported for nested queries",
        "Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported",
        "NRQL condition creation tips",
        "Condition edits can reset condition evaluation",
        "Alert condition types",
        "Sum of query results (limited or intermittent data)",
        "Set the loss of signal threshold",
        "Advanced signal settings",
        "Aggregation window duration",
        "Delay/timer",
        "Fill data gaps"
      ],
      "title": "Create NRQL alert conditions",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert conditions"
      ],
      "external_id": "2872f770a5fc8a802f99b9f848906f7e351ad761",
      "image": "https://docs.newrelic.com/static/eb8e1b2d826f9fd9cf46fe3dd5455217/c1b63/nr1_nrql_alert_conditions.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/",
      "published_at": "2021-12-19T16:01:17Z",
      "updated_at": "2021-12-14T22:49:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use NRQL queries to create alert conditions. Once you've defined your signal, you can further define your warning and critical threshold levels. This determines when an alerts violation is created. Read on to learn more about how to do this. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Click NRQL, and then Next, define thresholds. Tip For more information on key concepts relating to NRQL alert conditions and streaming alerts, see Streaming alerts: key terms and concepts. Create a NRQL alert condition To create a NRQL alert condition for a policy: On one.newrelic.com, in the header click Alerts & AI, then in the left sidebar click Policies. Select an existing policy or click New alert policy to create a new policy. Click Add a condition. Under Select a product click NRQL, and then click Next, define thresholds. Note that editing an existing condition can result in resetting its evaluation. Create a condition from a chart You can use a chart to create a NRQL alert condition. To create a NRQL alerts condition from a chart, click the chart menu , then click Create alert condition. Once you've named and customized your condition, you can add it to an existing policy or create a new one. Caution A small number of our older charts don't include the option to create an alert condition. NRQL alert syntax Here's the basic syntax for creating all NRQL alert conditions. The FACET clause is required for outlier condition types. It's optional for static and baseline. SELECT function(attribute) FROM Event WHERE attribute [comparison] [AND|OR ...] Copy Clause Notes SELECT function(attribute) Required Supported functions that return numbers include: apdex average count latest max min percentage percentile sum uniqueCount Tip If you use the percentile aggregator in a faceted alert condition with many facets, this may cause the following error to appear: An error occurred while fetching chart data. If you see this error, use average instead. FROM data type Required Only one data type can be targeted. Supported data types: Event Metric (RAW data points will be returned) WHERE attribute [comparison] [AND|OR ...] Use the WHERE clause to specify a series of one or more conditions. All the operators are supported. FACET attribute Required for outlier conditions Include an optional FACET clause in your NRQL syntax depending on the threshold type: static or baseline. Use the FACET clause to separate your results by attribute and alert on each attribute independently. No LIMIT clause is allowed, but all queries will receive the maximum number of facets possible. Faceted queries can return a maximum of 5000 values for static and baseline conditions and a maximum of 500 values for outlier conditions. Important If the query returns more than the maximum number of values, the alert condition can't be created. If you create the condition and the query returns more than this number later, the alert will fail. Modify your query so that it returns a fewer number of values. Reformatting incompatible NRQL Some elements of NRQL used in charts don’t make sense in the streaming context of alerts. Here’s a list of the most common incompatible elements and suggestions for reformatting a NRQL alert query to achieve the same effect. Element Notes SINCE and UNTIL Example: SELECT percentile(largestContentfulPaint, 75) FROM PageViewTiming WHERE (appId = 837807) SINCE yesterday Copy NRQL conditions produce a never-ending stream of windowed query results, so the SINCE and UNTIL keywords to scope the query to a point in time are not compatible. As a convenience, we automatically strip SINCE and UNTIL from a query when creating a condition from the context of a chart. TIMESERIES In NRQL queries, the TIMESERIES clause is used to return data as a time series broken out by a specified period of time. For NRQL conditions, the equivalent property of a signal is the aggregation duration window. histogram() The histogram() aggregation function is used to generate histograms. histogram() is not compatible with NRQL alerting: histogram aggregations can not be formatted as a time series. To create an alert from a portion of a histogram (for example, 95th percentile), use the percentile() aggregation function. Multiple aggregation functions Each condition can only target a single aggregated value. To alert on multiple values simultaneously, you’ll need to decompose them into individual conditions within the same policy. Original Query: SELECT count(foo), average(bar), max(baz) from Transaction Copy Decomposed: SELECT count(foo) from Transaction SELECT average(bar) from Transaction SELECT max(baz) from Transaction Copy COMPARE WITH The COMPARE WITH clause is used to compare the values for two different time ranges. This type of query is incompatible with NRQL alerting. We recommend using a Baseline Alert Condition to dynamically detect deviations for a particular signal. SLIDE BY The SLIDE BY clause supports a feature known as sliding windows. With sliding windows, SLIDE BY data is gathered into \"windows\" of time that overlap with each other. These windows can help to smooth out line graphs with a lot of variation in cases where the rolling aggregate (such as a rolling mean) is more important than aggregates from narrow windows of time. Sliding windows are not currently supported in NRQL alerts. LIMIT In NRQL queries, the LIMIT clause is used to control the amount of data a query returns, either the maximum number of facet values returned by FACET queries or the maximum number of items returned by SELECT * queries. LIMIT is not compatible with NRQL alerting: evaluation is always performed on the full result set. NRQL alert threshold examples Here are some common use cases for NRQL conditions. These queries will work for static and baseline condition types. The outlier condition type will require additional FACET clauses. Alert on specific segments of your data Create constrained alerts that target a specific segment of your data, such as a few key customers or a range of data. Use the WHERE clause to define those conditions. SELECT average(duration) FROM Transaction WHERE account_id in (91290, 102021, 20230) Copy SELECT percentile(duration, 95) FROM Transaction WHERE name LIKE 'Controller/checkout/%' Copy Alert on Nth percentile of your data Create alerts when an Nth percentile of your data hits a specified threshold; for example, maintaining SLA service levels. Since we evaluate the NRQL query based on the aggregation window duration, percentiles will be calculated for each duration separately. SELECT percentile(duration, 95) FROM Transaction Copy SELECT percentile(databaseDuration, 75) FROM Transaction Copy Alert on max, min, avg of your data Create alerts when your data hits a certain maximum, minimum, or average; for example, ensuring that a duration or response time does not pass a certain threshold. SELECT max(duration) FROM Transaction Copy SELECT average(duration) FROM Transaction Copy Alert on a percentage of your data Create alerts when a proportion of your data goes above or below a certain threshold. SELECT percentage(count(*), WHERE duration > 2) FROM Transaction Copy SELECT percentage(count(*), WHERE httpResponseCode = '500') FROM Transaction Copy Alert on Apdex with any T-value Create alerts on Apdex, applying your own T-value for certain transactions. For example, get an alert notification when your Apdex for a T-value of 500ms on transactions for production apps goes below 0.8. SELECT apdex(duration, t:0.5) FROM Transaction WHERE appName like '%prod%' Copy NRQL conditions and query order of operations By default, the aggregation window duration is 1 minute, but you can change the window to suit your needs. Whatever the aggregation window, New Relic will collect data for that window using the function in the NRQL condition’s query. The query is parsed and executed by our systems in the following order: FROM clause – which event type needs to be grabbed? WHERE clause – what can be filtered out? SELECT clause – what information needs to be returned from the now-filtered data set? Example: null value returned Let's say this is your alert condition query: SELECT count(*) FROM SyntheticCheck WHERE monitorName = 'My Cool Monitor' AND result = 'FAILURE' Copy If there are no failures for the aggregation window: The system will execute the FROM clause by grabbing all SyntheticCheck events on your account. Then it will execute the WHERE clause to filter through those events by looking only for the ones that match the monitor name and result specified. If there are still events left to scan through after completing the FROM and WHERE operations, the SELECT clause will be executed. If there are no remaining events, the SELECT clause will not be executed. This means that aggregators like count() and uniqueCount() will never return a zero value. When there is a count of 0, the SELECT clause is ignored and no data is returned, resulting in a value of NULL. Example: zero value returned If you have a data source delivering legitimate numeric zeroes, the query will return zero values and not null values. Let's say this is your alert condition query, and that MyCoolEvent is an attribute that can sometimes return a zero value. SELECT average(MyCoolAttribute) FROM MyCoolEvent Copy If, in the aggregation window being evaluated, there's at least one instance of MyCoolEvent and if the average value of all MyCoolAttribute attributes from that window is equal to zero, then a 0 value will be returned. If there are no MyCoolEvent events during that minute, then a NULL will be returned due to the order of operations. Tip For more information about this topic, you can check out our blog post on troubleshooting for zero versus null values. Tip You can determine how null values will be handled by adjusting loss of signal and gap filling settings in the Alert Conditions UI. Tip You can avoid NULL values entirely with a query order of operations shortcut. Do this by using a filter sub-clause, then including all filter elements within that sub-clause. The main body of the query will run and return data, at which point the SELECT clause will then run and apply the filter elements. The query will return a value of 0 if the filter elements result in no matching data. Here's an example: SELECT filter(count(*), WHERE result = 'SUCCESS' AND monitorName = 'My Favorite Monitor') FROM SyntheticCheck Copy Nested aggregation NRQL alerts Nested aggregation queries are a powerful way to query your data. However, they have a few restrictions that are important to note. Nested queries with a non-faceted innermost query are not currently supported Without a FACET, the inner query produces a single result, giving the outer query nothing to aggregate. If you're using a nested query, make sure your inner query is faceted. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu) ​​​​​ Copy Queries at all levels must have the same aggregation window size With an alert aggregation window of 1 minute, the inner query would produce two smaller windows of 30 seconds. In theory, these two windows could be aggregated by the outer query. However, this is not currently supported. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu TIMESERIES 30 seconds)​​ Copy Signal loss is not yet supported for nested queries For more information on signal loss, see NerdGraph API: Loss of signal and gap filling. Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported You can't use a nested query containing the WITH METRIC_FORMAT in the inner query to create NRQL alert conditions. NRQL condition creation tips Here are some tips for creating and using a NRQL condition: Topic Tips Condition types NRQL condition types include static, baseline, and outlier. Create a description For NRQL conditions, you can create a custom description to add to each violation. Descriptions can be enhanced with variable substitution based on metadata in the specific violation. For details, see Description Query results Queries must return a number. The condition evaluates the returned number against the thresholds you've set. Time period NRQL conditions evaluate data based on how it's aggregated, using aggregation windows from 30 seconds to 15 minutes, in increments of 15 seconds. For best results, we recommend using the event flow or event timer aggregation methods. For the cadence aggregation method, the implicit SINCE ... UNTIL clause specifying which minute to evaluate is controlled by your delay/timer setting. Since very recent data may be incomplete, you may want to query data from 3 minutes ago or longer, especially for: Applications that run on multiple hosts. SyntheticCheck data: Timeouts can take 3 minutes, so 5 minutes or more is recommended. Also, if a query will generate intermittent data, consider using the sum of query results option. Lost signal threshold (loss of signal detection) You can use loss of signal detection to alert on when your data (a telemetry signal) should be considered lost. A signal loss can indicate that a service or entity is no longer online or that a periodic job failed to run. You can also use this to make sure that violations for sporadic data, such as error counts, are closed when no signal is coming in. Advanced signal settings These settings give you options for better handling continuous, streaming data signals that may sometimes be missing. These settings include the aggregation window duration, the delay/timer, and an option for filling data gaps. For more on using these, see Advanced signal settings. Condition settings Use the Condition settings to: Create a concise, descriptive condition name. Provide a custom violation description for the condition that will be included in violations and notifications. Add the runbook URL to include your organization's procedures for handling incidents. You may also add this information to the custom violation description. Limits on conditions See the maximum values. Health status In order for a NRQL alert condition health status display to function properly, use a FACET clause to scope each signal to a single entity (for example, FACET hostname or FACET appname). Examples For more information, see: Expected NRQL syntax Examples of NRQL condition queries Condition edits can reset condition evaluation When you edit NRQL alert conditions in some specific ways (detailed below), their evaluations are reset, meaning that any evaluation up until that point is lost, and the evaluation starts over from that point. The two ways this will affect you are: For \"for at least x minutes\" thresholds: because the evaluation window has been reset, there will be a delay of at least x minutes before any violations can be reported. For baseline conditions: the condition starts over again and all baseline learning is lost. The following actions cause an evaluation reset for NRQL conditions: Changing the query Changing the aggregation window, aggregation method, or aggregation delay/timer setting Changing the \"close violations on signal loss\" setting Changing any gap fill settings Changing the baseline direction (if applicable) – higher, lower, or higher/lower Change the threshold value, threshold window, or threshold operator The following actions (along with any other actions not covered in the above list) will not reset the evaluation: Changing the loss of signal time window (expiration duration) Changing the time function (switching \"for at least\" to \"at least once in,\" or vice-versa) Toggling the \"open violation on signal loss\" setting Alert condition types When you create a NRQL alert, you can choose from different types of conditions: NRQL alert condition types Description Static This is the simplest type of NRQL condition. It allows you to create a condition based on a NRQL query that returns a numeric value. Optional: Include a FACET clause. Baseline (Dynamic) Uses a self-adjusting condition based on the past behavior of the monitored values. Uses the same NRQL query form as the static type, including the optional FACET clause. Outlier Looks for group behavior and values that are outliers from those groups. Uses the same NRQL query form as the static type, but requires a FACET clause. Sum of query results (limited or intermittent data) Important Available only for static (basic) condition types. If a query returns intermittent or limited data, it may be difficult to set a meaningful threshold. Missing or limited data will sometimes generate false positives or false negatives. You can use loss of signal, aggregation duration, and gap filling settings to minimize these false notifications. To avoid this problem when using the static threshold type, you can set the selector to sum of query results. This lets you set the alert on an aggregated sum instead of a value from a single harvest cycle. Up to two hours of one-minute data checks can be aggregated. The duration you select determines the width of the rolling sum and the preview chart will update accordingly. Set the loss of signal threshold Loss of signal occurs when no data matches the NRQL condition over a specific period of time. You can set your loss of signal threshold duration and also what happens when the threshold is crossed. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Loss of signal is only available for NRQL conditions. You may also manage these settings using the GraphQL API (recommended), or the REST API. Go here for specific GraphQL API examples. Loss of signal settings: Loss of signal settings include a time duration and two possible actions. Signal loss expiration time UI label: Signal is lost after: GraphQL Node: expiration.expirationDuration Expiration duration is a timer that starts and resets when we receive a data point in the streaming alerts pipeline. If we don't receive another data point before your 'expiration time' expires, we consider that signal to be lost. This can be because no data is being sent to New Relic or the WHERE clause of your NRQL query is filtering that data out before it is streamed to the alerts pipeline. Note that when you have a faceted query, each facet is a signal. So if any one of those signals ends during the duration specified, that will be considered a loss of signal. The loss of signal expiration time is independent of the threshold duration and triggers as soon as the timer expires. The maximum expiration duration is 48 hours. This is helpful when monitoring for the execution of infrequent jobs. The minimum is 30 seconds, but we recommend using at least 3-5 minutes. Loss of signal actions Once a signal is considered lost, you can close open violations, open new violations, or both. Close all current open violations: This closes all open violations that are related to a specific signal. It won't necessarily close all violations for a condition. If you're alerting on an ephemeral service, or on a sporadic signal, you'll want to choose this action to ensure that violations are closed properly. The GraphQL node name for this is \"closeViolationsOnExpiration\" Open new violations: This will open a new violation when the signal is considered lost. These violations will indicate that they are due to a loss of signal. Based on your incident preferences, this should trigger a notification. The graphQL node name for this is \"openViolationOnExpiration\" When you enable both actions, we'll close all open violations first, and then open a new violation for loss of signal. To create a NRQL alert configured with loss of signal detection in the UI: For a policy, when you create a condition, under Select a product, click NRQL, then click Next, define thresholds. Write a NRQL query that returns the values you want to alert on. For Threshold type, select Static or Baseline. Click + Add lost signal threshold, then set the signal expiration duration time in minutes or seconds in the Signal is lost after field. Choose what you want to happen when the signal is lost. You can check one or both of Close all current open violations and Open new \"lost signal\" violation. These control how loss of signal violations will be handled for the condition. Make sure you name your condition before you save it. Violations open due to loss of signal close when the signal comes back. Newly opened lost signal violations will close immediately when new data is evaluated. the condition they belong to expires. By default, conditions expire after 3 days. you manually close the violation with the Close all current open violations option. Tip Loss of signal detection doesn't work on NRQL queries that use nested aggregation or sub-queries. Advanced signal settings When creating a NRQL alert condition, use the advanced signal settings to control streaming alert data and avoid false alarms. When creating a NRQL condition, there are several advanced signal settings: Aggregation window duration Delay/timer Fill data gaps To read an explanation of what these settings are and how they relate to each other, see Streaming alerts concepts. Below are instructions and tips on how to configure them. Aggregation window duration You can set the aggregation window duration to choose how long data is accumulated in a streaming time window before it's aggregated. You can set it to anything between 30 seconds and 15 minutes. The default is one minute. Tip Baseline alert condition thresholds don't support editing the aggregation window. They use the 1 minute default. Delay/timer You can adjust the delay/timer to coordinate our streaming alerting algorithm with your data's behavior. If your data is sparse or inconsistent, you may want to use the event timer aggregation method. For the cadence method, the total supported latency is the sum of the aggregation window duration and the delay. If the data type comes from an APM language agent and is aggregated from many app instances (for example, Transactions, TransactionErrors, etc.), we recommend using the event flow method with the default settings. Important When creating NRQL conditions for data collected from Infrastructure Cloud Integrations such as AWS Cloudwatch or Azure, we recommend that you use the event timer method. Fill data gaps Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with one of these settings: None: (Default) Choose this if you don't want to take any action on empty aggregation windows. On evaluation, an empty aggregation window will reset the threshold duration timer. For example, if a condition says that all aggregation windows must have data points above the threshold for 5 minutes, and 1 of the 5 aggregation windows is empty, then the condition won't be in violation. Custom static value: Choose this if you'd like to insert a custom static value into the empty aggregation windows before they're evaluated. This option has an additional, required parameter of fillValue (as named in the API) that specifies what static value should be used. This defaults to 0. Last known value: This option inserts the last seen value before evaluation occurs. We maintain the state of the last seen value for 2 hours. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss, gap filling, and how to request access to these features, see this Explorers Hub post. Options for editing data gap settings: In the NRQL conditions UI, go to Condition settings > Advanced signal settings > fill data gaps with and choose an option. If using our Nerdgraph API (preferred), this node is located at: actor : account : alerts : nrqlCondition : signal : fillOption | fillValue NerdGraph is our recommended API for this but if you're using our REST API, you can find this setting in the REST API explorer under the \"signal\" section of the Alert NRQL conditions API.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.00899,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create NRQL <em>alert</em> conditions",
        "sections": "Nested aggregation NRQL <em>alerts</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": " a NRQL <em>alert</em> condition for a policy: On one.newrelic.com, in the header click <em>Alerts</em> &amp; AI, then in the left sidebar click Policies. Select an existing policy or click <em>New</em> <em>alert</em> policy to create a <em>new</em> policy. Click Add a condition. Under Select a product click NRQL, and then click Next, define"
      },
      "id": "603ef04864441fbc114e8883"
    },
    {
      "sections": [
        "View violation and event details for incidents",
        "View the incidents index and violation details",
        "Tip",
        "View the events in an incident",
        "Time between violation and notification",
        "Anomalous behavior detection",
        "Alerts notification in Slack"
      ],
      "title": "View violation and event details for incidents",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert incidents"
      ],
      "external_id": "7c01873917140e1922227598b7532b36343e308a",
      "image": "https://docs.newrelic.com/static/5878dd9ec30c9251961517a34ba88dc5/8c557/screen-alerts-incident-page_0.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/alert-incidents/view-violation-event-details-incidents/",
      "published_at": "2021-12-19T15:12:41Z",
      "updated_at": "2021-11-06T13:12:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "When violations of the threshold set in a condition occur, depending on the policy's Incident preference settings, Alerts may create an incident. You can review information about incidents in several ways: View the incidents index so you can scan for patterns in a list of incidents. View the violations included in a specific incident to examine associated performance details. View the events included in a specific incident to review the timestamps for events, such as a violation opening or closing, notifications, and acknowledgments. one.newrelic.com > Alerts & AI > Incidents > (select an incident): Notice that the condition's threshold was violated around 2pm (the blue line went over the red dotted line), but the alert wasn't triggered until the violation occurred for more than five minutes, as specified in the condition. View the incidents index and violation details Violations are grouped together into incidents. If you want to change how violations are grouped, open the associated policy and change the Incident preference setting. To view violation details: In the one.newrelic.com top nav, click Alerts & AI, click Incidents, then click Open incidents or All incidents. Select an incident row. Click Violations to view a list of the violations included in this incident. Select one of the violations to see a chart and details for it. Details for individual violation charts include: Timing information: The shaded red area on the chart shows you the time period when the violation occurred, where the preceding shaded pink area represents the degradation period. If you select a violation that lasted longer than two hours, the timeline on the bottom of the chart will be jagged. To provide context for events in the incident, the chart also shows the time frame surrounding the violation. Chart guidelines: The red dotted line marks the threshold for the condition. The blue line depicts performance information. Anomalous behavior: If Alerts detects anomalous behavior near the time of the violation, you'll see a notification in the violation details. From this page, you can take action regarding the incident: If you want to... Do this Assume responsibility for the incident Acknowledge the incident by selecting the acknowledge icon or button. View information about events Mouse over any spot on the blue line in the chart to display event information. Manually close the violation Below the chart, select the Manually close violation link. Tip Anyone in the account who can view the violation can also close it. Edit the policy or condition Select the Settings gear icon or select the name of the policy above the chart. View the events in an incident If you want to view alerting events across all products, go to one.newrelic.com, then click Explorer. To view the events for just one incident: Go to one.newrelic.com, then click Explorer. Select an entity row. In the left nav under Events, click Violations. Select one of the events to view a chart and details for it. Time between violation and notification There may be a difference of up to three minutes between the violation event time and the initial notification time due to variances in data processing time. Notification time: The time in the notification reflects the timestamp of when we received the request to deliver a notification. Violation time: The time you see on the Events page for the violation reflects the timestamp of data collection for the last data point that contributed to opening the violation. Anomalous behavior detection When we detect large changes in key signals in the alerting entity and/or upstream/downstream applications of the alerting entity, an \"anomalous behavior detected\" notification appears on the violation's page and in notification channels. You can: Expand the notification for details about the detected anomaly (web only). See upstream/downstream anomalies (Slack only). Select a link to go to the relevant product chart for further investigation. Alerts notification in Slack Example of an \"anomalous behavior detected\" notification in Slack.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.8968,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View violation <em>and</em> event details for <em>incidents</em>",
        "sections": "View violation <em>and</em> event details for <em>incidents</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "When violations of the threshold set in a condition occur, depending on the policy&#x27;s Incident preference settings, <em>Alerts</em> may create an incident. You can review information about <em>incidents</em> in several ways: View the <em>incidents</em> index so you can scan for patterns in a list of <em>incidents</em>. View"
      },
      "id": "60440c01196a67ba9c960f3c"
    }
  ],
  "/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/alerts-nerdgraph/nerdgraph-api-alerts-policies": [
    {
      "sections": [
        "NerdGraph tutorial: Notification channels",
        "Tip",
        "Get notification channels",
        "List all notification channels for an account",
        "Paginate through notification channels with cursor pagination",
        "Find a specific notification channel by id",
        "List notification channels with their associated policies",
        "Create a notification channel",
        "Caution",
        "Create an email notification channel",
        "Create an OpsGenie notification channel",
        "Create a PagerDuty notification channel",
        "Create a Slack notification channel",
        "Create a VictorOps notification channel",
        "Create a Webhook notification channel",
        "Create an xMatters notification channel",
        "Update a notification channel",
        "Update an email notification channel",
        "Update an OpsGenie notification channel",
        "Update a PagerDuty notification channel",
        "Update a Slack notification channel",
        "Update a VictorOps notification channel",
        "Update a Webhook notification channel",
        "Update an xMatters notification channel",
        "Delete a notification channel",
        "Associate channels to a policy",
        "Dissociate a channel from a policy"
      ],
      "title": "NerdGraph tutorial: Notification channels",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alerts and NerdGraph"
      ],
      "external_id": "d62b37e04d0601fec177951123d72e13f57458a0",
      "image": "",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/alerts-nerdgraph/nerdgraph-api-notification-channels/",
      "published_at": "2021-12-15T12:45:54Z",
      "updated_at": "2021-11-15T05:31:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can manage your alert notification channels using our GraphQL NerdGraph API. Here are some queries and mutations you can develop in our NerdGraph API explorer. Tip See the NerdGraph introduction for help getting started with NerdGraph API explorer. Get notification channels The notificationChannels query allows you to paginate through all of your notification channels per account. You can also use the notificationChannel query to get a specific notification channel by its ID. Tip Note that certain secret fields (for example, passwords or API keys) are obfuscated in the returned fields. List all notification channels for an account This example returns every field for every notification channel on the supplied account ID, up to the page limit of 200. Note how we use inline fragments to refer to the specific fields on the concrete types implementing the AlertsNotificationChannel interface. { actor { account(id: YOUR_ACCOUNT_ID) { alerts { notificationChannels { channels { id name type ... on AlertsXMattersNotificationChannel { config { integrationUrl } } ... on AlertsWebhookNotificationChannel { config { baseUrl basicAuth { password username } customHttpHeaders { name value } customPayloadBody customPayloadType } } ... on AlertsVictorOpsNotificationChannel { config { key routeKey } } ... on AlertsUserNotificationChannel { config { userId } } ... on AlertsSlackNotificationChannel { config { teamChannel url } } ... on AlertsPagerDutyNotificationChannel { config { apiKey } } ... on AlertsOpsGenieNotificationChannel { config { apiKey dataCenterRegion recipients tags teams } } ... on AlertsHipChatNotificationChannel { config { authToken baseUrl roomId } } ... on AlertsEmailNotificationChannel { config { emails includeJson } } ... on AlertsCampfireNotificationChannel { config { room subdomain token } } } totalCount nextCursor } } } } } Copy Paginate through notification channels with cursor pagination If a given account's list of notification channels exceeds the 200 channel page limit, you can use the pagination cursor to retrieve additional pages. With cursor pagination, you continue to request additional pages using the nextCursor until that field returns empty in the response. An empty nextCursor signals that you have reached the end of the result set. Here's an example: { actor { account(id: YOUR_ACCOUNT_ID) { alerts { notificationChannels { channels { id name type } totalCount nextCursor } } } } } Copy The code above returns a set of results like this: { \"data\": { \"actor\": { \"account\": { \"alerts\": { \"notificationChannels\": { \"channels\": [ { \"id\": \"250\", \"name\": \"Channel 1\", \"type\": \"SLACK\" }, { \"id\": \"713\", \"name\": \"Channel 2\", \"type\": \"WEBHOOK\" }, // ... +198 more notification channels in reality ], \"nextCursor\": \"Wh4LK9JYzfACVlNkyvf7Rg==:I5VbSEpgx3UWNA5AOVsUPv4=\", \"totalCount\": 268 } } } } } } Copy In your next request, provide the cursor like so, updating each subsequent request to return the updated cursor, until the cursor is empty: { actor { account(id: YOUR_ACCOUNT_ID) { alerts { notificationChannels(cursor: \"Wh4LK9JYzfACVlNkyvf7Rg==:I5VbSEpgx3UWNA5AOVsUPv4=\") { channels { id name type } totalCount nextCursor } } } } } Copy Find a specific notification channel by id If you have a specific notification channel's ID, the API allows you to look it up directly. Note that because the specific channel is a concrete type implementing the AlertsNotificationChannel interface, you may need to specify certain fields using the ... on syntax for inline fragments. In this example, we are retrieving a Slack channel: { actor { account(id: YOUR_ACCOUNT_ID) { alerts { notificationChannel(id: YOUR_CHANNEL_ID) { id name type ... on AlertsSlackNotificationChannel { config { teamChannel url } } } } } } } Copy List notification channels with their associated policies This example returns the ID, name, and type for every notification channel on the supplied account ID, as well as a list of every policy that is associated with that channel. { actor { account(id: YOUR_ACCOUNT_ID) { alerts { notificationChannels { channels { id name type associatedPolicies { policies { id name } totalCount } } nextCursor totalCount } } } } } Copy Create a notification channel In order to create an alert notification channel, you need to know the specific type of notification channel you want to create (for example email, Slack, etc.), as well as the details necessary to configure it (which will depend on the channel type). Once a notification channel has been created, it can be associated with one or more alert policies. Once associated, those channels will receive notifications from those policies when conditions are violated. Caution While you can query for any existing notification channel type, you can only create a subset of them. Specifically, the user channel type has no editable fields, and the Campfire and HipChat channel types are both deprecated. Create an email notification channel An example create mutation for an email notification channel: mutation { alertsNotificationChannelCreate(accountId: YOUR_ACCOUNT_ID, notificationChannel: { email: { emails: [\"email@example.com\"], includeJson: true, name: \"Some Name <email@example.com>\" } }) { notificationChannel { ... on AlertsEmailNotificationChannel { id name type config { emails includeJson } } } error { description errorType } } } Copy Create an OpsGenie notification channel An example create mutation for an OpsGenie notification channel: mutation { alertsNotificationChannelCreate(accountId: YOUR_ACCOUNT_ID, notificationChannel: { opsGenie: { apiKey: \"api-key-from-opsgenie\", dataCenterRegion: US, name: \"OpsGenie notification channel name\", recipients: [\"user@example.com\"], tags: [\"tag1\", \"tag2\"], teams: [\"team1\", \"team2\"] } }) { notificationChannel { ... on AlertsOpsGenieNotificationChannel { id name type config { apiKey teams tags recipients dataCenterRegion } } } error { description errorType } } } Copy Create a PagerDuty notification channel An example create mutation for a PagerDuty notification channel: mutation { alertsNotificationChannelCreate(accountId: YOUR_ACCOUNT_ID, notificationChannel: { pagerDuty: { name: \"PagerDuty notification channel name\", apiKey: \"api-key-from-pagerduty\" } }) { notificationChannel { ... on AlertsPagerDutyNotificationChannel { id name type config { apiKey } } } error { description errorType } } } Copy Create a Slack notification channel An example create mutation for a Slack notification channel: mutation { alertsNotificationChannelCreate(accountId: YOUR_ACCOUNT_ID, notificationChannel: { slack: { name: \"Slack notification channel name\", teamChannel: \"#team-channel\", url: \"https://hooks.slack.com/services/FAKE/MOREFAKE/IMAGINARYEXAMPLEURLCHUNK\" } }) { notificationChannel { ... on AlertsSlackNotificationChannel { id name type config { teamChannel url } } } error { description errorType } } } Copy Create a VictorOps notification channel An example create mutation for a VictorOps notification channel: mutation { alertsNotificationChannelCreate(accountId: YOUR_ACCOUNT_ID, notificationChannel: { victorOps: { key: \"example-api-key-from-victorops\", name: \"VictorOps notification channel name\", routeKey: \"example-route-key\" } }) { notificationChannel { ... on AlertsVictorOpsNotificationChannel { id name type config { key routeKey } } } error { description errorType } } } Copy Create a Webhook notification channel An example create mutation for a Webhook notification channel: mutation { alertsNotificationChannelCreate(accountId: YOUR_ACCOUNT_ID, notificationChannel: { webhook: { baseUrl: \"https://example.com/webhook\", basicAuth: { password: \"t0t4lly-s3cr3t-p455w0rd\", username: \"webhook-user\" }, customHttpHeaders: [ {name: \"X-Api-Key\", value: \"100%-real-api-key\"}, {name: \"X-Calling-Service\", value: \"New Relic Alerts\"} ], customPayloadBody: \"{ \\\"account_id\\\": \\\"$ACCOUNT_ID\\\", \\\"account_name\\\": \\\"$ACCOUNT_NAME\\\", \\\"closed_violations_count_critical\\\": \\\"$CLOSED_VIOLATIONS_COUNT_CRITICAL\\\", \\\"closed_violations_count_warning\\\": \\\"$CLOSED_VIOLATIONS_COUNT_WARNING\\\", \\\"condition_description\\\": \\\"$DESCRIPTION\\\", \\\"condition_family_id\\\": \\\"$CONDITION_FAMILY_ID\\\", \\\"condition_name\\\": \\\"$CONDITION_NAME\\\", \\\"current_state\\\": \\\"$EVENT_STATE\\\", \\\"details\\\": \\\"$EVENT_DETAILS\\\", \\\"duration\\\": \\\"$DURATION\\\", \\\"event_type\\\": \\\"$EVENT_TYPE\\\", \\\"incident_acknowledge_url\\\": \\\"$INCIDENT_ACKNOWLEDGE_URL\\\", \\\"incident_id\\\": \\\"$INCIDENT_ID\\\", \\\"incident_url\\\": \\\"$INCIDENT_URL\\\", \\\"metadata\\\": \\\"$METADATA\\\", \\\"open_violations_count_critical\\\": \\\"$OPEN_VIOLATIONS_COUNT_CRITICAL\\\", \\\"open_violations_count_warning\\\": \\\"$OPEN_VIOLATIONS_COUNT_WARNING\\\", \\\"owner\\\": \\\"$EVENT_OWNER\\\", \\\"policy_name\\\": \\\"$POLICY_NAME\\\", \\\"policy_url\\\": \\\"$POLICY_URL\\\", \\\"runbook_url\\\": \\\"$RUNBOOK_URL\\\", \\\"severity\\\": \\\"$SEVERITY\\\", \\\"targets\\\": \\\"$TARGETS\\\", \\\"timestamp\\\": \\\"$TIMESTAMP\\\", \\\"timestamp_utc_string\\\": \\\"$TIMESTAMP_UTC_STRING\\\", \\\"violation_callback_url\\\": \\\"$VIOLATION_CALLBACK_URL\\\", \\\"violation_chart_url\\\": \\\"$VIOLATION_CHART_URL\\\" }\", customPayloadType: JSON, name: \"Webhook notification channel name\" } }) { notificationChannel { ... on AlertsWebhookNotificationChannel { id name type config { customPayloadType customPayloadBody customHttpHeaders { value name } basicAuth { password username } baseUrl } } } error { description errorType } } } Copy Create an xMatters notification channel An example create mutation for an xMatters notification channel: mutation { alertsNotificationChannelCreate(accountId: YOUR_ACCOUNT_ID, notificationChannel: { xMatters: { integrationUrl: \"https://company.instance.xmatters.com/api/xm/v<version>/...\", name: \"xMatters notification channel name\" } }) { notificationChannel { ... on AlertsXMattersNotificationChannel { id name type config { integrationUrl } } } error { description errorType } } } Copy Update a notification channel In order to update an alert notification channel, you need to know the specific type of notification channel you want to change (for example email, Slack, etc.), as well as the details necessary to configure it (which will depend on the channel type). Consistent with other GraphQL APIs, you can update a single field on the channel without knowing anything other than the channel's ID. Caution While you can query for any existing notification channel type, you can only update a subset of them. Specifically, the user channel type has no editable fields, and the Campfire and HipChat channel types are both deprecated. Update an email notification channel An example update mutation for an email notification channel where we're updating only the name: mutation { alertsNotificationChannelUpdate(accountId: YOUR_ACCOUNT_ID, id: YOUR_CHANNEL_ID, notificationChannel: { email: { name: \"Updated Name <email@example.com>\" } }) { notificationChannel { ... on AlertsEmailNotificationChannel { id name type } } error { description errorType notificationChannelId } } } Copy Update an OpsGenie notification channel An example update mutation for an OpsGenie notification channel where we're updating only the name: mutation { alertsNotificationChannelUpdate(accountId: YOUR_ACCOUNT_ID, id: YOUR_CHANNEL_ID, notificationChannel: { opsGenie: { name: \"OpsGenie updated channel name\" } }) { notificationChannel { ... on AlertsOpsGenieNotificationChannel { id name type } } error { description errorType notificationChannelId } } } Copy Update a PagerDuty notification channel An example update mutation for a PagerDuty notification channel where we're updating only the name: mutation { alertsNotificationChannelUpdate(accountId: YOUR_ACCOUNT_ID, id: YOUR_CHANNEL_ID, notificationChannel: { pagerDuty: { name: \"PagerDuty updated channel name\" } }) { notificationChannel { ... on AlertsPagerDutyNotificationChannel { id name type } } error { description errorType notificationChannelId } } } Copy Update a Slack notification channel An example update mutation for a Slack notification channel where we're updating only the name: mutation { alertsNotificationChannelUpdate(accountId: YOUR_ACCOUNT_ID, id: YOUR_CHANNEL_ID, notificationChannel: { slack: { name: \"Slack updated channel name\" } }) { notificationChannel { ... on AlertsSlackNotificationChannel { id name type } } error { description errorType notificationChannelId } } } Copy Update a VictorOps notification channel An example update mutation for a VictorOps notification channel where we're updating only the name: mutation { alertsNotificationChannelUpdate(accountId: YOUR_ACCOUNT_ID, id: YOUR_CHANNEL_ID, id: YOUR_CHANNEL_ID, notificationChannel: { victorOps: { name: \"VictorOps updated channel name\" } }) { notificationChannel { ... on AlertsVictorOpsNotificationChannel { id name type } } error { description errorType notificationChannelId } } } Copy Update a Webhook notification channel An example update mutation for a Webhook notification channel where we're updating only the name: mutation { alertsNotificationChannelUpdate(accountId: YOUR_ACCOUNT_ID, id: YOUR_CHANNEL_ID, notificationChannel: { webhook: { name: \"Webhook updated channel name\" } }) { notificationChannel { ... on AlertsWebhookNotificationChannel { id name type } } error { description errorType notificationChannelId } } } Copy Update an xMatters notification channel An example update mutation for an xMatters notification channel where we're updating only the name: mutation { alertsNotificationChannelUpdate(accountId: YOUR_ACCOUNT_ID, id: YOUR_CHANNEL_ID, notificationChannel: { xMatters: { name: \"xMatters updated channel name\" } }) { notificationChannel { ... on AlertsXMattersNotificationChannel { id name type } } error { description errorType notificationChannelId } } } Copy Delete a notification channel You can delete a notification channel with only the account ID and the channel ID. Note that deleting a channel dissociates it from all policies, meaning that no further notifications will be sent to that channel. mutation { alertsNotificationChannelDelete(accountId: YOUR_ACCOUNT_ID, id: YOUR_CHANNEL_ID) { id error { description errorType notificationChannelId } } } Copy Associate channels to a policy Creating an alert notification channel is not enough: Once the channel has been created, it needs to be associated to one or more policies. Once associated to a policy, the channel can recieve alert notifications when conditions on that policy go into violation. In this example, we associate two channels with a policy: mutation { alertsNotificationChannelsAddToPolicy(accountId: YOUR_ACCOUNT_ID, notificationChannelIds: [FIRST_CHANNEL_ID, SECOND_CHANNEL_ID], policyId: YOUR_POLICY_ID ) { notificationChannels { id } policyId errors { description errorType notificationChannelId } } } Copy Dissociate a channel from a policy In those instances where a notification channel has outlived its usefulness (for example, an email list that has been retired), the time has come to dissociate that channel from the policy (or policies) that are sending alert notifications to it. This API call leaves the channel itself intact, but removes it from the specified policy. In this example, we are removing two channels from a policy (leaving any others in place), and getting back confirmation that those two channel IDs have been removed: mutation { alertsNotificationChannelsRemoveFromPolicy(accountId: YOUR_ACCOUNT_ID, notificationChannelIds: [FIRST_CHANNEL_ID, SECOND_CHANNEL_ID], policyId: YOUR_POLICY_ID ) { notificationChannels { id } policyId errors { description errorType notificationChannelId } } } Copy Tip Removing an alert notification channel from a policy does not delete the channel because it might be used by other policies. On the other hand, deleting a channel will cause all associated policies to stop sending alert notifications to that channel.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.80414,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>NerdGraph</em> tutorial: Notification channels",
        "sections": "<em>NerdGraph</em> tutorial: Notification channels",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": ": &quot;<em>New</em> <em>Relic</em> <em>Alerts</em>&quot;} ], customPayloadBody: &quot;{ \\&quot;account_id\\&quot;: \\&quot;$ACCOUNT_ID\\&quot;, \\&quot;account_name\\&quot;: \\&quot;$ACCOUNT_NAME\\&quot;, \\&quot;closed_violations_count_critical\\&quot;: \\&quot;$CLOSED_VIOLATIONS_COUNT_CRITICAL\\&quot;, \\&quot;closed_violations_count_warning\\&quot;: \\&quot;$CLOSED_VIOLATIONS_COUNT_WARNING\\&quot;, \\&quot;condition_description"
      },
      "id": "6130bf9c28ccbc027d56a863"
    },
    {
      "sections": [
        "Streaming alerts: key terms and concepts",
        "Why it matters",
        "Streaming alerts process and descriptions",
        "Choose your aggregation method",
        "Event flow",
        "Caution",
        "Event timer",
        "Cadence",
        "Streaming alerts tools",
        "Tip",
        "Window duration",
        "Delay/timer",
        "Loss of signal detection",
        "Gap filling"
      ],
      "title": "Streaming alerts: key terms and concepts",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Get started"
      ],
      "external_id": "144c7f0721ec2acb1e8ac8aa06142911ea552aef",
      "image": "https://docs.newrelic.com/static/39158bd84483adf9516bb7ff5058c9bd/d30ee/streaming-alerts.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/streaming-alerts-key-terms-concepts/",
      "published_at": "2021-12-15T12:47:58Z",
      "updated_at": "2021-12-15T12:47:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The streaming alerts platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into New Relic. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that's processed by the streaming algorithm. There are three methods for aggregating the data filtered through your NRQL condition: Event flow (default) Event timer Cadence This short video describes the three aggregation method (5:31). Why it matters Understanding how streaming alerts works will help you fine-tune your NRQL conditions to be notified about what's important to you. Only data that matches the conditions of the NRQL WHERE clause is alerted on. For more details on each step of the process, see Streaming alerts process and descriptions. As data streams into New Relic, it's filtered by the NRQL condition. Before data is evaluated, it must meet the criteria defined by the NRQL query's WHERE clause. Instead of evaluating that data immediately for violations, the NRQL alert conditions collect the data over a period of time known as the aggregation window. An additional delay/timer allows for slower data points to arrive before the window is aggregated. Once the delay/timer time has elapsed, New Relic aggregates the data into a single data point. Alerts then evaluates the data point using the NRQL condition in order to determine whether it meets the violation threshold criteria. Even if a data point meets the criteria for a violation, a violation may not be triggered. A violation is only triggered when data points consistently meet the threshold criteria over a period of time. This is the threshold duration. If the data points are in violation for an entire threshold duration, we'll send you a notification based on your policy settings. All of these configurable delays give you more control over how you're alerted on sporadic and missing data. Streaming alerts process and descriptions Process Description Streaming data All data coming into New Relic. WHERE clause Filters all incoming streaming data. We only monitor for alerts on data that makes it through this filter. Aggregation methods One of three methods that control how data is collected before it's evaluated. They are: Event flow (Default) Event timer Cadence Aggregation window Data with timestamps that fall within this window will be aggregated and then evaluated. Delay/timer A time delay to ensure all data points have arrived in the aggregation window before aggregation occurs. Aggregated data Data in the aggregated window is collapsed to a single data point for alert evaluation. Evaluation The data point is evaluated by the NRQL condition, which is trigged by each incoming aggregated data point. Threshold duration A specific duration that determines if a violation is created. If your specified NRQL condition meets the threshold criteria over the threshold duration, a violation occurs. When a data point lacks data, a custom value is inserted to fill the gap. Choose your aggregation method You can choose between three different aggregation methods, depending on your needs. Event flow (default) works best for data that comes in frequently and mostly in order. Event timer works best for data that arrives infrequently in batches, such as cloud integration data or infrequent error logs. Cadence is our original and inferior aggregation method. It aggregates data on specific time intervals as detected by New Relic's internal wall clock, regardless of data timestamps. Event flow Event flow aggregates a window of data when the first data point arrives in a subsequent window. The custom delay defines which subsequent window data will start to populate to trigger aggregation of the current window. A custom delay provides extra time for data to arrive. These times are based on the data's timestamps and not New Relic's wall clock time. For example, suppose you're monitoring CPU usage in window durations of 1 minute and a 3 minute delay. When a CPU usage data point comes in with a timestamp between 12:00pm and 12:01pm, event flow will not aggregate that window until a data point shows up with a timestamp between 12:04pm and 12:05pm. When event flow receives the first data point with a timestamp of 12:04pm or later, it sends the 12:00 to 12:01 data to be aggregated. Caution If you expect your data points to arrive more than 30 minutes apart, please use the Event Timer method described below. Event timer Like event flow, event timer will only aggregate data for a given window when data arrives for that window. When a data point arrives for an aggregation window, a timer dedicated to that window starts to count down. If no further data arrives before the timer counts down, the data for that window is aggregated. If more data points arrive before the timer has completed counting down, the timer is reset. For example, suppose you're monitoring CloudWatch data that arrives fairly infrequently. You're using a window duration of 1 minute and a 3 minute timer. When a CloudWatch data point comes in with a timestamp between 12:00pm and 12:01pm, the timer will start to count down. If no further data points show up for that 12:00-12:01 window, the window will get aggregated 3 minutes later. If a new data point with a timestamp between 12:00 and 12:01 arrives, the timer resets. It keeps resetting every time more data points for that window arrive. The window will not be sent for aggregation until the timer reaches 0. If the timer for a later data point elapses before an earlier data point, the event timer method waits for the earlier timer to elapse before aggregating the later data point. For best results, make sure your timer is equal to or longer than your window duration time. If the timer is less than your window duration and your data flow is inconsistent, then your data may be evaluated before all of your data points arrive. This could cause you to be notified incorrectly. Cadence We recommend you use one of the other two methods. Cadence is our old streaming aggregation method. This method uses New Relic's wall clock time to determine when data is aggregated and evaluated. It doesn't take into account data point timestamps as they arrive. Streaming alerts tools Streaming alerts provide a set of tools to give you greater control over how your data is aggregated before it's evaluated to reduce incorrect notifications you receive. They are: Window duration Delay/timer Loss of signal detection Gap filling Tip This article covers these tools at a conceptual level. You'll find direct instructions on how to use these tools in Create NRQL alert conditions. Window duration In order to make loss of signal detection more effective and to reduce unnecessary notifications, you can customize aggregation windows to the duration that you need. An aggregation window is a specific block of time. We gather data points together in an aggregation window, before evaluating the data. A longer aggregation window can smooth out the data, since an outlier data point will have more data points to be aggregated with, giving it less of an influence on the aggregated data point that is sent for evaluation. When a data point arrives, its timestamp is used to put it in the proper aggregation window. You can set your aggregation window to anything between 30 seconds and 15 minutes. The default is 1 minute. Delay/timer The delay/timer setting controls how long the condition should wait before aggregating the data in the aggregation window. The event flow and cadence methods use delay. Event timer uses timer. The delay default is 2 minutes. The timer default is 1 minute and has a minimum value of 5 seconds. Loss of signal detection Loss of signal occurs when no data matches the NRQL condition over a specific period of time. A loss of signal is caused by different things. The WHERE clause in your NRQL query can filter out data before it's evaluated for violations. It could also mean a service or entity is offline or a periodic job has failed to run and no data is being sent to New Relic. In order to avoid unnecessary notifications, you can choose how long to wait before you're notified by a loss of signal violation. You can use loss of signal detection to open violations and be notified when a signal is lost. Alternately, you can use a loss of signal to close violations for ephemeral services or sporadic data, such as error counts. Gap filling Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with the last value received, a static value, or else do nothing and leave the gap there. The default is None. Gaps in streaming data can be caused by network or host issues, a signal may be sparse, or some signals, such as error counts, may only have data when something is wrong. By filling the gaps with known values, the alert evaluation process can process those gaps and determine how they should affect the loss of signal evaluation. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss and gap filling, see this Explorers Hub post.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.07332,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "sections": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "The streaming <em>alerts</em> platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into <em>New</em> <em>Relic</em>. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that&#x27;s processed"
      },
      "id": "6130c054e7b9d269edb6f21d"
    },
    {
      "sections": [
        "Create NRQL alert conditions",
        "Tip",
        "Create a NRQL alert condition",
        "Create a condition from a chart",
        "Caution",
        "NRQL alert syntax",
        "Important",
        "Reformatting incompatible NRQL",
        "NRQL alert threshold examples",
        "Alert on specific segments of your data",
        "Alert on Nth percentile of your data",
        "Alert on max, min, avg of your data",
        "Alert on a percentage of your data",
        "Alert on Apdex with any T-value",
        "NRQL conditions and query order of operations",
        "Example: null value returned",
        "Example: zero value returned",
        "Nested aggregation NRQL alerts",
        "Nested queries with a non-faceted innermost query are not currently supported",
        "Queries at all levels must have the same aggregation window size",
        "Signal loss is not yet supported for nested queries",
        "Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported",
        "NRQL condition creation tips",
        "Condition edits can reset condition evaluation",
        "Alert condition types",
        "Sum of query results (limited or intermittent data)",
        "Set the loss of signal threshold",
        "Advanced signal settings",
        "Aggregation window duration",
        "Delay/timer",
        "Fill data gaps"
      ],
      "title": "Create NRQL alert conditions",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert conditions"
      ],
      "external_id": "2872f770a5fc8a802f99b9f848906f7e351ad761",
      "image": "https://docs.newrelic.com/static/eb8e1b2d826f9fd9cf46fe3dd5455217/c1b63/nr1_nrql_alert_conditions.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/",
      "published_at": "2021-12-19T16:01:17Z",
      "updated_at": "2021-12-14T22:49:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use NRQL queries to create alert conditions. Once you've defined your signal, you can further define your warning and critical threshold levels. This determines when an alerts violation is created. Read on to learn more about how to do this. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Click NRQL, and then Next, define thresholds. Tip For more information on key concepts relating to NRQL alert conditions and streaming alerts, see Streaming alerts: key terms and concepts. Create a NRQL alert condition To create a NRQL alert condition for a policy: On one.newrelic.com, in the header click Alerts & AI, then in the left sidebar click Policies. Select an existing policy or click New alert policy to create a new policy. Click Add a condition. Under Select a product click NRQL, and then click Next, define thresholds. Note that editing an existing condition can result in resetting its evaluation. Create a condition from a chart You can use a chart to create a NRQL alert condition. To create a NRQL alerts condition from a chart, click the chart menu , then click Create alert condition. Once you've named and customized your condition, you can add it to an existing policy or create a new one. Caution A small number of our older charts don't include the option to create an alert condition. NRQL alert syntax Here's the basic syntax for creating all NRQL alert conditions. The FACET clause is required for outlier condition types. It's optional for static and baseline. SELECT function(attribute) FROM Event WHERE attribute [comparison] [AND|OR ...] Copy Clause Notes SELECT function(attribute) Required Supported functions that return numbers include: apdex average count latest max min percentage percentile sum uniqueCount Tip If you use the percentile aggregator in a faceted alert condition with many facets, this may cause the following error to appear: An error occurred while fetching chart data. If you see this error, use average instead. FROM data type Required Only one data type can be targeted. Supported data types: Event Metric (RAW data points will be returned) WHERE attribute [comparison] [AND|OR ...] Use the WHERE clause to specify a series of one or more conditions. All the operators are supported. FACET attribute Required for outlier conditions Include an optional FACET clause in your NRQL syntax depending on the threshold type: static or baseline. Use the FACET clause to separate your results by attribute and alert on each attribute independently. No LIMIT clause is allowed, but all queries will receive the maximum number of facets possible. Faceted queries can return a maximum of 5000 values for static and baseline conditions and a maximum of 500 values for outlier conditions. Important If the query returns more than the maximum number of values, the alert condition can't be created. If you create the condition and the query returns more than this number later, the alert will fail. Modify your query so that it returns a fewer number of values. Reformatting incompatible NRQL Some elements of NRQL used in charts don’t make sense in the streaming context of alerts. Here’s a list of the most common incompatible elements and suggestions for reformatting a NRQL alert query to achieve the same effect. Element Notes SINCE and UNTIL Example: SELECT percentile(largestContentfulPaint, 75) FROM PageViewTiming WHERE (appId = 837807) SINCE yesterday Copy NRQL conditions produce a never-ending stream of windowed query results, so the SINCE and UNTIL keywords to scope the query to a point in time are not compatible. As a convenience, we automatically strip SINCE and UNTIL from a query when creating a condition from the context of a chart. TIMESERIES In NRQL queries, the TIMESERIES clause is used to return data as a time series broken out by a specified period of time. For NRQL conditions, the equivalent property of a signal is the aggregation duration window. histogram() The histogram() aggregation function is used to generate histograms. histogram() is not compatible with NRQL alerting: histogram aggregations can not be formatted as a time series. To create an alert from a portion of a histogram (for example, 95th percentile), use the percentile() aggregation function. Multiple aggregation functions Each condition can only target a single aggregated value. To alert on multiple values simultaneously, you’ll need to decompose them into individual conditions within the same policy. Original Query: SELECT count(foo), average(bar), max(baz) from Transaction Copy Decomposed: SELECT count(foo) from Transaction SELECT average(bar) from Transaction SELECT max(baz) from Transaction Copy COMPARE WITH The COMPARE WITH clause is used to compare the values for two different time ranges. This type of query is incompatible with NRQL alerting. We recommend using a Baseline Alert Condition to dynamically detect deviations for a particular signal. SLIDE BY The SLIDE BY clause supports a feature known as sliding windows. With sliding windows, SLIDE BY data is gathered into \"windows\" of time that overlap with each other. These windows can help to smooth out line graphs with a lot of variation in cases where the rolling aggregate (such as a rolling mean) is more important than aggregates from narrow windows of time. Sliding windows are not currently supported in NRQL alerts. LIMIT In NRQL queries, the LIMIT clause is used to control the amount of data a query returns, either the maximum number of facet values returned by FACET queries or the maximum number of items returned by SELECT * queries. LIMIT is not compatible with NRQL alerting: evaluation is always performed on the full result set. NRQL alert threshold examples Here are some common use cases for NRQL conditions. These queries will work for static and baseline condition types. The outlier condition type will require additional FACET clauses. Alert on specific segments of your data Create constrained alerts that target a specific segment of your data, such as a few key customers or a range of data. Use the WHERE clause to define those conditions. SELECT average(duration) FROM Transaction WHERE account_id in (91290, 102021, 20230) Copy SELECT percentile(duration, 95) FROM Transaction WHERE name LIKE 'Controller/checkout/%' Copy Alert on Nth percentile of your data Create alerts when an Nth percentile of your data hits a specified threshold; for example, maintaining SLA service levels. Since we evaluate the NRQL query based on the aggregation window duration, percentiles will be calculated for each duration separately. SELECT percentile(duration, 95) FROM Transaction Copy SELECT percentile(databaseDuration, 75) FROM Transaction Copy Alert on max, min, avg of your data Create alerts when your data hits a certain maximum, minimum, or average; for example, ensuring that a duration or response time does not pass a certain threshold. SELECT max(duration) FROM Transaction Copy SELECT average(duration) FROM Transaction Copy Alert on a percentage of your data Create alerts when a proportion of your data goes above or below a certain threshold. SELECT percentage(count(*), WHERE duration > 2) FROM Transaction Copy SELECT percentage(count(*), WHERE httpResponseCode = '500') FROM Transaction Copy Alert on Apdex with any T-value Create alerts on Apdex, applying your own T-value for certain transactions. For example, get an alert notification when your Apdex for a T-value of 500ms on transactions for production apps goes below 0.8. SELECT apdex(duration, t:0.5) FROM Transaction WHERE appName like '%prod%' Copy NRQL conditions and query order of operations By default, the aggregation window duration is 1 minute, but you can change the window to suit your needs. Whatever the aggregation window, New Relic will collect data for that window using the function in the NRQL condition’s query. The query is parsed and executed by our systems in the following order: FROM clause – which event type needs to be grabbed? WHERE clause – what can be filtered out? SELECT clause – what information needs to be returned from the now-filtered data set? Example: null value returned Let's say this is your alert condition query: SELECT count(*) FROM SyntheticCheck WHERE monitorName = 'My Cool Monitor' AND result = 'FAILURE' Copy If there are no failures for the aggregation window: The system will execute the FROM clause by grabbing all SyntheticCheck events on your account. Then it will execute the WHERE clause to filter through those events by looking only for the ones that match the monitor name and result specified. If there are still events left to scan through after completing the FROM and WHERE operations, the SELECT clause will be executed. If there are no remaining events, the SELECT clause will not be executed. This means that aggregators like count() and uniqueCount() will never return a zero value. When there is a count of 0, the SELECT clause is ignored and no data is returned, resulting in a value of NULL. Example: zero value returned If you have a data source delivering legitimate numeric zeroes, the query will return zero values and not null values. Let's say this is your alert condition query, and that MyCoolEvent is an attribute that can sometimes return a zero value. SELECT average(MyCoolAttribute) FROM MyCoolEvent Copy If, in the aggregation window being evaluated, there's at least one instance of MyCoolEvent and if the average value of all MyCoolAttribute attributes from that window is equal to zero, then a 0 value will be returned. If there are no MyCoolEvent events during that minute, then a NULL will be returned due to the order of operations. Tip For more information about this topic, you can check out our blog post on troubleshooting for zero versus null values. Tip You can determine how null values will be handled by adjusting loss of signal and gap filling settings in the Alert Conditions UI. Tip You can avoid NULL values entirely with a query order of operations shortcut. Do this by using a filter sub-clause, then including all filter elements within that sub-clause. The main body of the query will run and return data, at which point the SELECT clause will then run and apply the filter elements. The query will return a value of 0 if the filter elements result in no matching data. Here's an example: SELECT filter(count(*), WHERE result = 'SUCCESS' AND monitorName = 'My Favorite Monitor') FROM SyntheticCheck Copy Nested aggregation NRQL alerts Nested aggregation queries are a powerful way to query your data. However, they have a few restrictions that are important to note. Nested queries with a non-faceted innermost query are not currently supported Without a FACET, the inner query produces a single result, giving the outer query nothing to aggregate. If you're using a nested query, make sure your inner query is faceted. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu) ​​​​​ Copy Queries at all levels must have the same aggregation window size With an alert aggregation window of 1 minute, the inner query would produce two smaller windows of 30 seconds. In theory, these two windows could be aggregated by the outer query. However, this is not currently supported. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu TIMESERIES 30 seconds)​​ Copy Signal loss is not yet supported for nested queries For more information on signal loss, see NerdGraph API: Loss of signal and gap filling. Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported You can't use a nested query containing the WITH METRIC_FORMAT in the inner query to create NRQL alert conditions. NRQL condition creation tips Here are some tips for creating and using a NRQL condition: Topic Tips Condition types NRQL condition types include static, baseline, and outlier. Create a description For NRQL conditions, you can create a custom description to add to each violation. Descriptions can be enhanced with variable substitution based on metadata in the specific violation. For details, see Description Query results Queries must return a number. The condition evaluates the returned number against the thresholds you've set. Time period NRQL conditions evaluate data based on how it's aggregated, using aggregation windows from 30 seconds to 15 minutes, in increments of 15 seconds. For best results, we recommend using the event flow or event timer aggregation methods. For the cadence aggregation method, the implicit SINCE ... UNTIL clause specifying which minute to evaluate is controlled by your delay/timer setting. Since very recent data may be incomplete, you may want to query data from 3 minutes ago or longer, especially for: Applications that run on multiple hosts. SyntheticCheck data: Timeouts can take 3 minutes, so 5 minutes or more is recommended. Also, if a query will generate intermittent data, consider using the sum of query results option. Lost signal threshold (loss of signal detection) You can use loss of signal detection to alert on when your data (a telemetry signal) should be considered lost. A signal loss can indicate that a service or entity is no longer online or that a periodic job failed to run. You can also use this to make sure that violations for sporadic data, such as error counts, are closed when no signal is coming in. Advanced signal settings These settings give you options for better handling continuous, streaming data signals that may sometimes be missing. These settings include the aggregation window duration, the delay/timer, and an option for filling data gaps. For more on using these, see Advanced signal settings. Condition settings Use the Condition settings to: Create a concise, descriptive condition name. Provide a custom violation description for the condition that will be included in violations and notifications. Add the runbook URL to include your organization's procedures for handling incidents. You may also add this information to the custom violation description. Limits on conditions See the maximum values. Health status In order for a NRQL alert condition health status display to function properly, use a FACET clause to scope each signal to a single entity (for example, FACET hostname or FACET appname). Examples For more information, see: Expected NRQL syntax Examples of NRQL condition queries Condition edits can reset condition evaluation When you edit NRQL alert conditions in some specific ways (detailed below), their evaluations are reset, meaning that any evaluation up until that point is lost, and the evaluation starts over from that point. The two ways this will affect you are: For \"for at least x minutes\" thresholds: because the evaluation window has been reset, there will be a delay of at least x minutes before any violations can be reported. For baseline conditions: the condition starts over again and all baseline learning is lost. The following actions cause an evaluation reset for NRQL conditions: Changing the query Changing the aggregation window, aggregation method, or aggregation delay/timer setting Changing the \"close violations on signal loss\" setting Changing any gap fill settings Changing the baseline direction (if applicable) – higher, lower, or higher/lower Change the threshold value, threshold window, or threshold operator The following actions (along with any other actions not covered in the above list) will not reset the evaluation: Changing the loss of signal time window (expiration duration) Changing the time function (switching \"for at least\" to \"at least once in,\" or vice-versa) Toggling the \"open violation on signal loss\" setting Alert condition types When you create a NRQL alert, you can choose from different types of conditions: NRQL alert condition types Description Static This is the simplest type of NRQL condition. It allows you to create a condition based on a NRQL query that returns a numeric value. Optional: Include a FACET clause. Baseline (Dynamic) Uses a self-adjusting condition based on the past behavior of the monitored values. Uses the same NRQL query form as the static type, including the optional FACET clause. Outlier Looks for group behavior and values that are outliers from those groups. Uses the same NRQL query form as the static type, but requires a FACET clause. Sum of query results (limited or intermittent data) Important Available only for static (basic) condition types. If a query returns intermittent or limited data, it may be difficult to set a meaningful threshold. Missing or limited data will sometimes generate false positives or false negatives. You can use loss of signal, aggregation duration, and gap filling settings to minimize these false notifications. To avoid this problem when using the static threshold type, you can set the selector to sum of query results. This lets you set the alert on an aggregated sum instead of a value from a single harvest cycle. Up to two hours of one-minute data checks can be aggregated. The duration you select determines the width of the rolling sum and the preview chart will update accordingly. Set the loss of signal threshold Loss of signal occurs when no data matches the NRQL condition over a specific period of time. You can set your loss of signal threshold duration and also what happens when the threshold is crossed. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Loss of signal is only available for NRQL conditions. You may also manage these settings using the GraphQL API (recommended), or the REST API. Go here for specific GraphQL API examples. Loss of signal settings: Loss of signal settings include a time duration and two possible actions. Signal loss expiration time UI label: Signal is lost after: GraphQL Node: expiration.expirationDuration Expiration duration is a timer that starts and resets when we receive a data point in the streaming alerts pipeline. If we don't receive another data point before your 'expiration time' expires, we consider that signal to be lost. This can be because no data is being sent to New Relic or the WHERE clause of your NRQL query is filtering that data out before it is streamed to the alerts pipeline. Note that when you have a faceted query, each facet is a signal. So if any one of those signals ends during the duration specified, that will be considered a loss of signal. The loss of signal expiration time is independent of the threshold duration and triggers as soon as the timer expires. The maximum expiration duration is 48 hours. This is helpful when monitoring for the execution of infrequent jobs. The minimum is 30 seconds, but we recommend using at least 3-5 minutes. Loss of signal actions Once a signal is considered lost, you can close open violations, open new violations, or both. Close all current open violations: This closes all open violations that are related to a specific signal. It won't necessarily close all violations for a condition. If you're alerting on an ephemeral service, or on a sporadic signal, you'll want to choose this action to ensure that violations are closed properly. The GraphQL node name for this is \"closeViolationsOnExpiration\" Open new violations: This will open a new violation when the signal is considered lost. These violations will indicate that they are due to a loss of signal. Based on your incident preferences, this should trigger a notification. The graphQL node name for this is \"openViolationOnExpiration\" When you enable both actions, we'll close all open violations first, and then open a new violation for loss of signal. To create a NRQL alert configured with loss of signal detection in the UI: For a policy, when you create a condition, under Select a product, click NRQL, then click Next, define thresholds. Write a NRQL query that returns the values you want to alert on. For Threshold type, select Static or Baseline. Click + Add lost signal threshold, then set the signal expiration duration time in minutes or seconds in the Signal is lost after field. Choose what you want to happen when the signal is lost. You can check one or both of Close all current open violations and Open new \"lost signal\" violation. These control how loss of signal violations will be handled for the condition. Make sure you name your condition before you save it. Violations open due to loss of signal close when the signal comes back. Newly opened lost signal violations will close immediately when new data is evaluated. the condition they belong to expires. By default, conditions expire after 3 days. you manually close the violation with the Close all current open violations option. Tip Loss of signal detection doesn't work on NRQL queries that use nested aggregation or sub-queries. Advanced signal settings When creating a NRQL alert condition, use the advanced signal settings to control streaming alert data and avoid false alarms. When creating a NRQL condition, there are several advanced signal settings: Aggregation window duration Delay/timer Fill data gaps To read an explanation of what these settings are and how they relate to each other, see Streaming alerts concepts. Below are instructions and tips on how to configure them. Aggregation window duration You can set the aggregation window duration to choose how long data is accumulated in a streaming time window before it's aggregated. You can set it to anything between 30 seconds and 15 minutes. The default is one minute. Tip Baseline alert condition thresholds don't support editing the aggregation window. They use the 1 minute default. Delay/timer You can adjust the delay/timer to coordinate our streaming alerting algorithm with your data's behavior. If your data is sparse or inconsistent, you may want to use the event timer aggregation method. For the cadence method, the total supported latency is the sum of the aggregation window duration and the delay. If the data type comes from an APM language agent and is aggregated from many app instances (for example, Transactions, TransactionErrors, etc.), we recommend using the event flow method with the default settings. Important When creating NRQL conditions for data collected from Infrastructure Cloud Integrations such as AWS Cloudwatch or Azure, we recommend that you use the event timer method. Fill data gaps Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with one of these settings: None: (Default) Choose this if you don't want to take any action on empty aggregation windows. On evaluation, an empty aggregation window will reset the threshold duration timer. For example, if a condition says that all aggregation windows must have data points above the threshold for 5 minutes, and 1 of the 5 aggregation windows is empty, then the condition won't be in violation. Custom static value: Choose this if you'd like to insert a custom static value into the empty aggregation windows before they're evaluated. This option has an additional, required parameter of fillValue (as named in the API) that specifies what static value should be used. This defaults to 0. Last known value: This option inserts the last seen value before evaluation occurs. We maintain the state of the last seen value for 2 hours. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss, gap filling, and how to request access to these features, see this Explorers Hub post. Options for editing data gap settings: In the NRQL conditions UI, go to Condition settings > Advanced signal settings > fill data gaps with and choose an option. If using our Nerdgraph API (preferred), this node is located at: actor : account : alerts : nrqlCondition : signal : fillOption | fillValue NerdGraph is our recommended API for this but if you're using our REST API, you can find this setting in the REST API explorer under the \"signal\" section of the Alert NRQL conditions API.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.00899,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create NRQL <em>alert</em> conditions",
        "sections": "Nested aggregation NRQL <em>alerts</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": " a NRQL <em>alert</em> condition for a policy: On one.newrelic.com, in the header click <em>Alerts</em> &amp; AI, then in the left sidebar click Policies. Select an existing policy or click <em>New</em> <em>alert</em> policy to create a <em>new</em> policy. Click Add a condition. Under Select a product click NRQL, and then click Next, define"
      },
      "id": "603ef04864441fbc114e8883"
    }
  ],
  "/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/alerts-nerdgraph/nerdgraph-api-examples": [
    {
      "sections": [
        "NerdGraph tutorial: Notification channels",
        "Tip",
        "Get notification channels",
        "List all notification channels for an account",
        "Paginate through notification channels with cursor pagination",
        "Find a specific notification channel by id",
        "List notification channels with their associated policies",
        "Create a notification channel",
        "Caution",
        "Create an email notification channel",
        "Create an OpsGenie notification channel",
        "Create a PagerDuty notification channel",
        "Create a Slack notification channel",
        "Create a VictorOps notification channel",
        "Create a Webhook notification channel",
        "Create an xMatters notification channel",
        "Update a notification channel",
        "Update an email notification channel",
        "Update an OpsGenie notification channel",
        "Update a PagerDuty notification channel",
        "Update a Slack notification channel",
        "Update a VictorOps notification channel",
        "Update a Webhook notification channel",
        "Update an xMatters notification channel",
        "Delete a notification channel",
        "Associate channels to a policy",
        "Dissociate a channel from a policy"
      ],
      "title": "NerdGraph tutorial: Notification channels",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alerts and NerdGraph"
      ],
      "external_id": "d62b37e04d0601fec177951123d72e13f57458a0",
      "image": "",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/alerts-nerdgraph/nerdgraph-api-notification-channels/",
      "published_at": "2021-12-15T12:45:54Z",
      "updated_at": "2021-11-15T05:31:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can manage your alert notification channels using our GraphQL NerdGraph API. Here are some queries and mutations you can develop in our NerdGraph API explorer. Tip See the NerdGraph introduction for help getting started with NerdGraph API explorer. Get notification channels The notificationChannels query allows you to paginate through all of your notification channels per account. You can also use the notificationChannel query to get a specific notification channel by its ID. Tip Note that certain secret fields (for example, passwords or API keys) are obfuscated in the returned fields. List all notification channels for an account This example returns every field for every notification channel on the supplied account ID, up to the page limit of 200. Note how we use inline fragments to refer to the specific fields on the concrete types implementing the AlertsNotificationChannel interface. { actor { account(id: YOUR_ACCOUNT_ID) { alerts { notificationChannels { channels { id name type ... on AlertsXMattersNotificationChannel { config { integrationUrl } } ... on AlertsWebhookNotificationChannel { config { baseUrl basicAuth { password username } customHttpHeaders { name value } customPayloadBody customPayloadType } } ... on AlertsVictorOpsNotificationChannel { config { key routeKey } } ... on AlertsUserNotificationChannel { config { userId } } ... on AlertsSlackNotificationChannel { config { teamChannel url } } ... on AlertsPagerDutyNotificationChannel { config { apiKey } } ... on AlertsOpsGenieNotificationChannel { config { apiKey dataCenterRegion recipients tags teams } } ... on AlertsHipChatNotificationChannel { config { authToken baseUrl roomId } } ... on AlertsEmailNotificationChannel { config { emails includeJson } } ... on AlertsCampfireNotificationChannel { config { room subdomain token } } } totalCount nextCursor } } } } } Copy Paginate through notification channels with cursor pagination If a given account's list of notification channels exceeds the 200 channel page limit, you can use the pagination cursor to retrieve additional pages. With cursor pagination, you continue to request additional pages using the nextCursor until that field returns empty in the response. An empty nextCursor signals that you have reached the end of the result set. Here's an example: { actor { account(id: YOUR_ACCOUNT_ID) { alerts { notificationChannels { channels { id name type } totalCount nextCursor } } } } } Copy The code above returns a set of results like this: { \"data\": { \"actor\": { \"account\": { \"alerts\": { \"notificationChannels\": { \"channels\": [ { \"id\": \"250\", \"name\": \"Channel 1\", \"type\": \"SLACK\" }, { \"id\": \"713\", \"name\": \"Channel 2\", \"type\": \"WEBHOOK\" }, // ... +198 more notification channels in reality ], \"nextCursor\": \"Wh4LK9JYzfACVlNkyvf7Rg==:I5VbSEpgx3UWNA5AOVsUPv4=\", \"totalCount\": 268 } } } } } } Copy In your next request, provide the cursor like so, updating each subsequent request to return the updated cursor, until the cursor is empty: { actor { account(id: YOUR_ACCOUNT_ID) { alerts { notificationChannels(cursor: \"Wh4LK9JYzfACVlNkyvf7Rg==:I5VbSEpgx3UWNA5AOVsUPv4=\") { channels { id name type } totalCount nextCursor } } } } } Copy Find a specific notification channel by id If you have a specific notification channel's ID, the API allows you to look it up directly. Note that because the specific channel is a concrete type implementing the AlertsNotificationChannel interface, you may need to specify certain fields using the ... on syntax for inline fragments. In this example, we are retrieving a Slack channel: { actor { account(id: YOUR_ACCOUNT_ID) { alerts { notificationChannel(id: YOUR_CHANNEL_ID) { id name type ... on AlertsSlackNotificationChannel { config { teamChannel url } } } } } } } Copy List notification channels with their associated policies This example returns the ID, name, and type for every notification channel on the supplied account ID, as well as a list of every policy that is associated with that channel. { actor { account(id: YOUR_ACCOUNT_ID) { alerts { notificationChannels { channels { id name type associatedPolicies { policies { id name } totalCount } } nextCursor totalCount } } } } } Copy Create a notification channel In order to create an alert notification channel, you need to know the specific type of notification channel you want to create (for example email, Slack, etc.), as well as the details necessary to configure it (which will depend on the channel type). Once a notification channel has been created, it can be associated with one or more alert policies. Once associated, those channels will receive notifications from those policies when conditions are violated. Caution While you can query for any existing notification channel type, you can only create a subset of them. Specifically, the user channel type has no editable fields, and the Campfire and HipChat channel types are both deprecated. Create an email notification channel An example create mutation for an email notification channel: mutation { alertsNotificationChannelCreate(accountId: YOUR_ACCOUNT_ID, notificationChannel: { email: { emails: [\"email@example.com\"], includeJson: true, name: \"Some Name <email@example.com>\" } }) { notificationChannel { ... on AlertsEmailNotificationChannel { id name type config { emails includeJson } } } error { description errorType } } } Copy Create an OpsGenie notification channel An example create mutation for an OpsGenie notification channel: mutation { alertsNotificationChannelCreate(accountId: YOUR_ACCOUNT_ID, notificationChannel: { opsGenie: { apiKey: \"api-key-from-opsgenie\", dataCenterRegion: US, name: \"OpsGenie notification channel name\", recipients: [\"user@example.com\"], tags: [\"tag1\", \"tag2\"], teams: [\"team1\", \"team2\"] } }) { notificationChannel { ... on AlertsOpsGenieNotificationChannel { id name type config { apiKey teams tags recipients dataCenterRegion } } } error { description errorType } } } Copy Create a PagerDuty notification channel An example create mutation for a PagerDuty notification channel: mutation { alertsNotificationChannelCreate(accountId: YOUR_ACCOUNT_ID, notificationChannel: { pagerDuty: { name: \"PagerDuty notification channel name\", apiKey: \"api-key-from-pagerduty\" } }) { notificationChannel { ... on AlertsPagerDutyNotificationChannel { id name type config { apiKey } } } error { description errorType } } } Copy Create a Slack notification channel An example create mutation for a Slack notification channel: mutation { alertsNotificationChannelCreate(accountId: YOUR_ACCOUNT_ID, notificationChannel: { slack: { name: \"Slack notification channel name\", teamChannel: \"#team-channel\", url: \"https://hooks.slack.com/services/FAKE/MOREFAKE/IMAGINARYEXAMPLEURLCHUNK\" } }) { notificationChannel { ... on AlertsSlackNotificationChannel { id name type config { teamChannel url } } } error { description errorType } } } Copy Create a VictorOps notification channel An example create mutation for a VictorOps notification channel: mutation { alertsNotificationChannelCreate(accountId: YOUR_ACCOUNT_ID, notificationChannel: { victorOps: { key: \"example-api-key-from-victorops\", name: \"VictorOps notification channel name\", routeKey: \"example-route-key\" } }) { notificationChannel { ... on AlertsVictorOpsNotificationChannel { id name type config { key routeKey } } } error { description errorType } } } Copy Create a Webhook notification channel An example create mutation for a Webhook notification channel: mutation { alertsNotificationChannelCreate(accountId: YOUR_ACCOUNT_ID, notificationChannel: { webhook: { baseUrl: \"https://example.com/webhook\", basicAuth: { password: \"t0t4lly-s3cr3t-p455w0rd\", username: \"webhook-user\" }, customHttpHeaders: [ {name: \"X-Api-Key\", value: \"100%-real-api-key\"}, {name: \"X-Calling-Service\", value: \"New Relic Alerts\"} ], customPayloadBody: \"{ \\\"account_id\\\": \\\"$ACCOUNT_ID\\\", \\\"account_name\\\": \\\"$ACCOUNT_NAME\\\", \\\"closed_violations_count_critical\\\": \\\"$CLOSED_VIOLATIONS_COUNT_CRITICAL\\\", \\\"closed_violations_count_warning\\\": \\\"$CLOSED_VIOLATIONS_COUNT_WARNING\\\", \\\"condition_description\\\": \\\"$DESCRIPTION\\\", \\\"condition_family_id\\\": \\\"$CONDITION_FAMILY_ID\\\", \\\"condition_name\\\": \\\"$CONDITION_NAME\\\", \\\"current_state\\\": \\\"$EVENT_STATE\\\", \\\"details\\\": \\\"$EVENT_DETAILS\\\", \\\"duration\\\": \\\"$DURATION\\\", \\\"event_type\\\": \\\"$EVENT_TYPE\\\", \\\"incident_acknowledge_url\\\": \\\"$INCIDENT_ACKNOWLEDGE_URL\\\", \\\"incident_id\\\": \\\"$INCIDENT_ID\\\", \\\"incident_url\\\": \\\"$INCIDENT_URL\\\", \\\"metadata\\\": \\\"$METADATA\\\", \\\"open_violations_count_critical\\\": \\\"$OPEN_VIOLATIONS_COUNT_CRITICAL\\\", \\\"open_violations_count_warning\\\": \\\"$OPEN_VIOLATIONS_COUNT_WARNING\\\", \\\"owner\\\": \\\"$EVENT_OWNER\\\", \\\"policy_name\\\": \\\"$POLICY_NAME\\\", \\\"policy_url\\\": \\\"$POLICY_URL\\\", \\\"runbook_url\\\": \\\"$RUNBOOK_URL\\\", \\\"severity\\\": \\\"$SEVERITY\\\", \\\"targets\\\": \\\"$TARGETS\\\", \\\"timestamp\\\": \\\"$TIMESTAMP\\\", \\\"timestamp_utc_string\\\": \\\"$TIMESTAMP_UTC_STRING\\\", \\\"violation_callback_url\\\": \\\"$VIOLATION_CALLBACK_URL\\\", \\\"violation_chart_url\\\": \\\"$VIOLATION_CHART_URL\\\" }\", customPayloadType: JSON, name: \"Webhook notification channel name\" } }) { notificationChannel { ... on AlertsWebhookNotificationChannel { id name type config { customPayloadType customPayloadBody customHttpHeaders { value name } basicAuth { password username } baseUrl } } } error { description errorType } } } Copy Create an xMatters notification channel An example create mutation for an xMatters notification channel: mutation { alertsNotificationChannelCreate(accountId: YOUR_ACCOUNT_ID, notificationChannel: { xMatters: { integrationUrl: \"https://company.instance.xmatters.com/api/xm/v<version>/...\", name: \"xMatters notification channel name\" } }) { notificationChannel { ... on AlertsXMattersNotificationChannel { id name type config { integrationUrl } } } error { description errorType } } } Copy Update a notification channel In order to update an alert notification channel, you need to know the specific type of notification channel you want to change (for example email, Slack, etc.), as well as the details necessary to configure it (which will depend on the channel type). Consistent with other GraphQL APIs, you can update a single field on the channel without knowing anything other than the channel's ID. Caution While you can query for any existing notification channel type, you can only update a subset of them. Specifically, the user channel type has no editable fields, and the Campfire and HipChat channel types are both deprecated. Update an email notification channel An example update mutation for an email notification channel where we're updating only the name: mutation { alertsNotificationChannelUpdate(accountId: YOUR_ACCOUNT_ID, id: YOUR_CHANNEL_ID, notificationChannel: { email: { name: \"Updated Name <email@example.com>\" } }) { notificationChannel { ... on AlertsEmailNotificationChannel { id name type } } error { description errorType notificationChannelId } } } Copy Update an OpsGenie notification channel An example update mutation for an OpsGenie notification channel where we're updating only the name: mutation { alertsNotificationChannelUpdate(accountId: YOUR_ACCOUNT_ID, id: YOUR_CHANNEL_ID, notificationChannel: { opsGenie: { name: \"OpsGenie updated channel name\" } }) { notificationChannel { ... on AlertsOpsGenieNotificationChannel { id name type } } error { description errorType notificationChannelId } } } Copy Update a PagerDuty notification channel An example update mutation for a PagerDuty notification channel where we're updating only the name: mutation { alertsNotificationChannelUpdate(accountId: YOUR_ACCOUNT_ID, id: YOUR_CHANNEL_ID, notificationChannel: { pagerDuty: { name: \"PagerDuty updated channel name\" } }) { notificationChannel { ... on AlertsPagerDutyNotificationChannel { id name type } } error { description errorType notificationChannelId } } } Copy Update a Slack notification channel An example update mutation for a Slack notification channel where we're updating only the name: mutation { alertsNotificationChannelUpdate(accountId: YOUR_ACCOUNT_ID, id: YOUR_CHANNEL_ID, notificationChannel: { slack: { name: \"Slack updated channel name\" } }) { notificationChannel { ... on AlertsSlackNotificationChannel { id name type } } error { description errorType notificationChannelId } } } Copy Update a VictorOps notification channel An example update mutation for a VictorOps notification channel where we're updating only the name: mutation { alertsNotificationChannelUpdate(accountId: YOUR_ACCOUNT_ID, id: YOUR_CHANNEL_ID, id: YOUR_CHANNEL_ID, notificationChannel: { victorOps: { name: \"VictorOps updated channel name\" } }) { notificationChannel { ... on AlertsVictorOpsNotificationChannel { id name type } } error { description errorType notificationChannelId } } } Copy Update a Webhook notification channel An example update mutation for a Webhook notification channel where we're updating only the name: mutation { alertsNotificationChannelUpdate(accountId: YOUR_ACCOUNT_ID, id: YOUR_CHANNEL_ID, notificationChannel: { webhook: { name: \"Webhook updated channel name\" } }) { notificationChannel { ... on AlertsWebhookNotificationChannel { id name type } } error { description errorType notificationChannelId } } } Copy Update an xMatters notification channel An example update mutation for an xMatters notification channel where we're updating only the name: mutation { alertsNotificationChannelUpdate(accountId: YOUR_ACCOUNT_ID, id: YOUR_CHANNEL_ID, notificationChannel: { xMatters: { name: \"xMatters updated channel name\" } }) { notificationChannel { ... on AlertsXMattersNotificationChannel { id name type } } error { description errorType notificationChannelId } } } Copy Delete a notification channel You can delete a notification channel with only the account ID and the channel ID. Note that deleting a channel dissociates it from all policies, meaning that no further notifications will be sent to that channel. mutation { alertsNotificationChannelDelete(accountId: YOUR_ACCOUNT_ID, id: YOUR_CHANNEL_ID) { id error { description errorType notificationChannelId } } } Copy Associate channels to a policy Creating an alert notification channel is not enough: Once the channel has been created, it needs to be associated to one or more policies. Once associated to a policy, the channel can recieve alert notifications when conditions on that policy go into violation. In this example, we associate two channels with a policy: mutation { alertsNotificationChannelsAddToPolicy(accountId: YOUR_ACCOUNT_ID, notificationChannelIds: [FIRST_CHANNEL_ID, SECOND_CHANNEL_ID], policyId: YOUR_POLICY_ID ) { notificationChannels { id } policyId errors { description errorType notificationChannelId } } } Copy Dissociate a channel from a policy In those instances where a notification channel has outlived its usefulness (for example, an email list that has been retired), the time has come to dissociate that channel from the policy (or policies) that are sending alert notifications to it. This API call leaves the channel itself intact, but removes it from the specified policy. In this example, we are removing two channels from a policy (leaving any others in place), and getting back confirmation that those two channel IDs have been removed: mutation { alertsNotificationChannelsRemoveFromPolicy(accountId: YOUR_ACCOUNT_ID, notificationChannelIds: [FIRST_CHANNEL_ID, SECOND_CHANNEL_ID], policyId: YOUR_POLICY_ID ) { notificationChannels { id } policyId errors { description errorType notificationChannelId } } } Copy Tip Removing an alert notification channel from a policy does not delete the channel because it might be used by other policies. On the other hand, deleting a channel will cause all associated policies to stop sending alert notifications to that channel.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.80414,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>NerdGraph</em> tutorial: Notification channels",
        "sections": "<em>NerdGraph</em> tutorial: Notification channels",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": ": &quot;<em>New</em> <em>Relic</em> <em>Alerts</em>&quot;} ], customPayloadBody: &quot;{ \\&quot;account_id\\&quot;: \\&quot;$ACCOUNT_ID\\&quot;, \\&quot;account_name\\&quot;: \\&quot;$ACCOUNT_NAME\\&quot;, \\&quot;closed_violations_count_critical\\&quot;: \\&quot;$CLOSED_VIOLATIONS_COUNT_CRITICAL\\&quot;, \\&quot;closed_violations_count_warning\\&quot;: \\&quot;$CLOSED_VIOLATIONS_COUNT_WARNING\\&quot;, \\&quot;condition_description"
      },
      "id": "6130bf9c28ccbc027d56a863"
    },
    {
      "sections": [
        "Streaming alerts: key terms and concepts",
        "Why it matters",
        "Streaming alerts process and descriptions",
        "Choose your aggregation method",
        "Event flow",
        "Caution",
        "Event timer",
        "Cadence",
        "Streaming alerts tools",
        "Tip",
        "Window duration",
        "Delay/timer",
        "Loss of signal detection",
        "Gap filling"
      ],
      "title": "Streaming alerts: key terms and concepts",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Get started"
      ],
      "external_id": "144c7f0721ec2acb1e8ac8aa06142911ea552aef",
      "image": "https://docs.newrelic.com/static/39158bd84483adf9516bb7ff5058c9bd/d30ee/streaming-alerts.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/streaming-alerts-key-terms-concepts/",
      "published_at": "2021-12-15T12:47:58Z",
      "updated_at": "2021-12-15T12:47:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The streaming alerts platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into New Relic. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that's processed by the streaming algorithm. There are three methods for aggregating the data filtered through your NRQL condition: Event flow (default) Event timer Cadence This short video describes the three aggregation method (5:31). Why it matters Understanding how streaming alerts works will help you fine-tune your NRQL conditions to be notified about what's important to you. Only data that matches the conditions of the NRQL WHERE clause is alerted on. For more details on each step of the process, see Streaming alerts process and descriptions. As data streams into New Relic, it's filtered by the NRQL condition. Before data is evaluated, it must meet the criteria defined by the NRQL query's WHERE clause. Instead of evaluating that data immediately for violations, the NRQL alert conditions collect the data over a period of time known as the aggregation window. An additional delay/timer allows for slower data points to arrive before the window is aggregated. Once the delay/timer time has elapsed, New Relic aggregates the data into a single data point. Alerts then evaluates the data point using the NRQL condition in order to determine whether it meets the violation threshold criteria. Even if a data point meets the criteria for a violation, a violation may not be triggered. A violation is only triggered when data points consistently meet the threshold criteria over a period of time. This is the threshold duration. If the data points are in violation for an entire threshold duration, we'll send you a notification based on your policy settings. All of these configurable delays give you more control over how you're alerted on sporadic and missing data. Streaming alerts process and descriptions Process Description Streaming data All data coming into New Relic. WHERE clause Filters all incoming streaming data. We only monitor for alerts on data that makes it through this filter. Aggregation methods One of three methods that control how data is collected before it's evaluated. They are: Event flow (Default) Event timer Cadence Aggregation window Data with timestamps that fall within this window will be aggregated and then evaluated. Delay/timer A time delay to ensure all data points have arrived in the aggregation window before aggregation occurs. Aggregated data Data in the aggregated window is collapsed to a single data point for alert evaluation. Evaluation The data point is evaluated by the NRQL condition, which is trigged by each incoming aggregated data point. Threshold duration A specific duration that determines if a violation is created. If your specified NRQL condition meets the threshold criteria over the threshold duration, a violation occurs. When a data point lacks data, a custom value is inserted to fill the gap. Choose your aggregation method You can choose between three different aggregation methods, depending on your needs. Event flow (default) works best for data that comes in frequently and mostly in order. Event timer works best for data that arrives infrequently in batches, such as cloud integration data or infrequent error logs. Cadence is our original and inferior aggregation method. It aggregates data on specific time intervals as detected by New Relic's internal wall clock, regardless of data timestamps. Event flow Event flow aggregates a window of data when the first data point arrives in a subsequent window. The custom delay defines which subsequent window data will start to populate to trigger aggregation of the current window. A custom delay provides extra time for data to arrive. These times are based on the data's timestamps and not New Relic's wall clock time. For example, suppose you're monitoring CPU usage in window durations of 1 minute and a 3 minute delay. When a CPU usage data point comes in with a timestamp between 12:00pm and 12:01pm, event flow will not aggregate that window until a data point shows up with a timestamp between 12:04pm and 12:05pm. When event flow receives the first data point with a timestamp of 12:04pm or later, it sends the 12:00 to 12:01 data to be aggregated. Caution If you expect your data points to arrive more than 30 minutes apart, please use the Event Timer method described below. Event timer Like event flow, event timer will only aggregate data for a given window when data arrives for that window. When a data point arrives for an aggregation window, a timer dedicated to that window starts to count down. If no further data arrives before the timer counts down, the data for that window is aggregated. If more data points arrive before the timer has completed counting down, the timer is reset. For example, suppose you're monitoring CloudWatch data that arrives fairly infrequently. You're using a window duration of 1 minute and a 3 minute timer. When a CloudWatch data point comes in with a timestamp between 12:00pm and 12:01pm, the timer will start to count down. If no further data points show up for that 12:00-12:01 window, the window will get aggregated 3 minutes later. If a new data point with a timestamp between 12:00 and 12:01 arrives, the timer resets. It keeps resetting every time more data points for that window arrive. The window will not be sent for aggregation until the timer reaches 0. If the timer for a later data point elapses before an earlier data point, the event timer method waits for the earlier timer to elapse before aggregating the later data point. For best results, make sure your timer is equal to or longer than your window duration time. If the timer is less than your window duration and your data flow is inconsistent, then your data may be evaluated before all of your data points arrive. This could cause you to be notified incorrectly. Cadence We recommend you use one of the other two methods. Cadence is our old streaming aggregation method. This method uses New Relic's wall clock time to determine when data is aggregated and evaluated. It doesn't take into account data point timestamps as they arrive. Streaming alerts tools Streaming alerts provide a set of tools to give you greater control over how your data is aggregated before it's evaluated to reduce incorrect notifications you receive. They are: Window duration Delay/timer Loss of signal detection Gap filling Tip This article covers these tools at a conceptual level. You'll find direct instructions on how to use these tools in Create NRQL alert conditions. Window duration In order to make loss of signal detection more effective and to reduce unnecessary notifications, you can customize aggregation windows to the duration that you need. An aggregation window is a specific block of time. We gather data points together in an aggregation window, before evaluating the data. A longer aggregation window can smooth out the data, since an outlier data point will have more data points to be aggregated with, giving it less of an influence on the aggregated data point that is sent for evaluation. When a data point arrives, its timestamp is used to put it in the proper aggregation window. You can set your aggregation window to anything between 30 seconds and 15 minutes. The default is 1 minute. Delay/timer The delay/timer setting controls how long the condition should wait before aggregating the data in the aggregation window. The event flow and cadence methods use delay. Event timer uses timer. The delay default is 2 minutes. The timer default is 1 minute and has a minimum value of 5 seconds. Loss of signal detection Loss of signal occurs when no data matches the NRQL condition over a specific period of time. A loss of signal is caused by different things. The WHERE clause in your NRQL query can filter out data before it's evaluated for violations. It could also mean a service or entity is offline or a periodic job has failed to run and no data is being sent to New Relic. In order to avoid unnecessary notifications, you can choose how long to wait before you're notified by a loss of signal violation. You can use loss of signal detection to open violations and be notified when a signal is lost. Alternately, you can use a loss of signal to close violations for ephemeral services or sporadic data, such as error counts. Gap filling Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with the last value received, a static value, or else do nothing and leave the gap there. The default is None. Gaps in streaming data can be caused by network or host issues, a signal may be sparse, or some signals, such as error counts, may only have data when something is wrong. By filling the gaps with known values, the alert evaluation process can process those gaps and determine how they should affect the loss of signal evaluation. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss and gap filling, see this Explorers Hub post.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.07324,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "sections": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "The streaming <em>alerts</em> platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into <em>New</em> <em>Relic</em>. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that&#x27;s processed"
      },
      "id": "6130c054e7b9d269edb6f21d"
    },
    {
      "sections": [
        "Create NRQL alert conditions",
        "Tip",
        "Create a NRQL alert condition",
        "Create a condition from a chart",
        "Caution",
        "NRQL alert syntax",
        "Important",
        "Reformatting incompatible NRQL",
        "NRQL alert threshold examples",
        "Alert on specific segments of your data",
        "Alert on Nth percentile of your data",
        "Alert on max, min, avg of your data",
        "Alert on a percentage of your data",
        "Alert on Apdex with any T-value",
        "NRQL conditions and query order of operations",
        "Example: null value returned",
        "Example: zero value returned",
        "Nested aggregation NRQL alerts",
        "Nested queries with a non-faceted innermost query are not currently supported",
        "Queries at all levels must have the same aggregation window size",
        "Signal loss is not yet supported for nested queries",
        "Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported",
        "NRQL condition creation tips",
        "Condition edits can reset condition evaluation",
        "Alert condition types",
        "Sum of query results (limited or intermittent data)",
        "Set the loss of signal threshold",
        "Advanced signal settings",
        "Aggregation window duration",
        "Delay/timer",
        "Fill data gaps"
      ],
      "title": "Create NRQL alert conditions",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert conditions"
      ],
      "external_id": "2872f770a5fc8a802f99b9f848906f7e351ad761",
      "image": "https://docs.newrelic.com/static/eb8e1b2d826f9fd9cf46fe3dd5455217/c1b63/nr1_nrql_alert_conditions.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/",
      "published_at": "2021-12-19T16:01:17Z",
      "updated_at": "2021-12-14T22:49:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use NRQL queries to create alert conditions. Once you've defined your signal, you can further define your warning and critical threshold levels. This determines when an alerts violation is created. Read on to learn more about how to do this. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Click NRQL, and then Next, define thresholds. Tip For more information on key concepts relating to NRQL alert conditions and streaming alerts, see Streaming alerts: key terms and concepts. Create a NRQL alert condition To create a NRQL alert condition for a policy: On one.newrelic.com, in the header click Alerts & AI, then in the left sidebar click Policies. Select an existing policy or click New alert policy to create a new policy. Click Add a condition. Under Select a product click NRQL, and then click Next, define thresholds. Note that editing an existing condition can result in resetting its evaluation. Create a condition from a chart You can use a chart to create a NRQL alert condition. To create a NRQL alerts condition from a chart, click the chart menu , then click Create alert condition. Once you've named and customized your condition, you can add it to an existing policy or create a new one. Caution A small number of our older charts don't include the option to create an alert condition. NRQL alert syntax Here's the basic syntax for creating all NRQL alert conditions. The FACET clause is required for outlier condition types. It's optional for static and baseline. SELECT function(attribute) FROM Event WHERE attribute [comparison] [AND|OR ...] Copy Clause Notes SELECT function(attribute) Required Supported functions that return numbers include: apdex average count latest max min percentage percentile sum uniqueCount Tip If you use the percentile aggregator in a faceted alert condition with many facets, this may cause the following error to appear: An error occurred while fetching chart data. If you see this error, use average instead. FROM data type Required Only one data type can be targeted. Supported data types: Event Metric (RAW data points will be returned) WHERE attribute [comparison] [AND|OR ...] Use the WHERE clause to specify a series of one or more conditions. All the operators are supported. FACET attribute Required for outlier conditions Include an optional FACET clause in your NRQL syntax depending on the threshold type: static or baseline. Use the FACET clause to separate your results by attribute and alert on each attribute independently. No LIMIT clause is allowed, but all queries will receive the maximum number of facets possible. Faceted queries can return a maximum of 5000 values for static and baseline conditions and a maximum of 500 values for outlier conditions. Important If the query returns more than the maximum number of values, the alert condition can't be created. If you create the condition and the query returns more than this number later, the alert will fail. Modify your query so that it returns a fewer number of values. Reformatting incompatible NRQL Some elements of NRQL used in charts don’t make sense in the streaming context of alerts. Here’s a list of the most common incompatible elements and suggestions for reformatting a NRQL alert query to achieve the same effect. Element Notes SINCE and UNTIL Example: SELECT percentile(largestContentfulPaint, 75) FROM PageViewTiming WHERE (appId = 837807) SINCE yesterday Copy NRQL conditions produce a never-ending stream of windowed query results, so the SINCE and UNTIL keywords to scope the query to a point in time are not compatible. As a convenience, we automatically strip SINCE and UNTIL from a query when creating a condition from the context of a chart. TIMESERIES In NRQL queries, the TIMESERIES clause is used to return data as a time series broken out by a specified period of time. For NRQL conditions, the equivalent property of a signal is the aggregation duration window. histogram() The histogram() aggregation function is used to generate histograms. histogram() is not compatible with NRQL alerting: histogram aggregations can not be formatted as a time series. To create an alert from a portion of a histogram (for example, 95th percentile), use the percentile() aggregation function. Multiple aggregation functions Each condition can only target a single aggregated value. To alert on multiple values simultaneously, you’ll need to decompose them into individual conditions within the same policy. Original Query: SELECT count(foo), average(bar), max(baz) from Transaction Copy Decomposed: SELECT count(foo) from Transaction SELECT average(bar) from Transaction SELECT max(baz) from Transaction Copy COMPARE WITH The COMPARE WITH clause is used to compare the values for two different time ranges. This type of query is incompatible with NRQL alerting. We recommend using a Baseline Alert Condition to dynamically detect deviations for a particular signal. SLIDE BY The SLIDE BY clause supports a feature known as sliding windows. With sliding windows, SLIDE BY data is gathered into \"windows\" of time that overlap with each other. These windows can help to smooth out line graphs with a lot of variation in cases where the rolling aggregate (such as a rolling mean) is more important than aggregates from narrow windows of time. Sliding windows are not currently supported in NRQL alerts. LIMIT In NRQL queries, the LIMIT clause is used to control the amount of data a query returns, either the maximum number of facet values returned by FACET queries or the maximum number of items returned by SELECT * queries. LIMIT is not compatible with NRQL alerting: evaluation is always performed on the full result set. NRQL alert threshold examples Here are some common use cases for NRQL conditions. These queries will work for static and baseline condition types. The outlier condition type will require additional FACET clauses. Alert on specific segments of your data Create constrained alerts that target a specific segment of your data, such as a few key customers or a range of data. Use the WHERE clause to define those conditions. SELECT average(duration) FROM Transaction WHERE account_id in (91290, 102021, 20230) Copy SELECT percentile(duration, 95) FROM Transaction WHERE name LIKE 'Controller/checkout/%' Copy Alert on Nth percentile of your data Create alerts when an Nth percentile of your data hits a specified threshold; for example, maintaining SLA service levels. Since we evaluate the NRQL query based on the aggregation window duration, percentiles will be calculated for each duration separately. SELECT percentile(duration, 95) FROM Transaction Copy SELECT percentile(databaseDuration, 75) FROM Transaction Copy Alert on max, min, avg of your data Create alerts when your data hits a certain maximum, minimum, or average; for example, ensuring that a duration or response time does not pass a certain threshold. SELECT max(duration) FROM Transaction Copy SELECT average(duration) FROM Transaction Copy Alert on a percentage of your data Create alerts when a proportion of your data goes above or below a certain threshold. SELECT percentage(count(*), WHERE duration > 2) FROM Transaction Copy SELECT percentage(count(*), WHERE httpResponseCode = '500') FROM Transaction Copy Alert on Apdex with any T-value Create alerts on Apdex, applying your own T-value for certain transactions. For example, get an alert notification when your Apdex for a T-value of 500ms on transactions for production apps goes below 0.8. SELECT apdex(duration, t:0.5) FROM Transaction WHERE appName like '%prod%' Copy NRQL conditions and query order of operations By default, the aggregation window duration is 1 minute, but you can change the window to suit your needs. Whatever the aggregation window, New Relic will collect data for that window using the function in the NRQL condition’s query. The query is parsed and executed by our systems in the following order: FROM clause – which event type needs to be grabbed? WHERE clause – what can be filtered out? SELECT clause – what information needs to be returned from the now-filtered data set? Example: null value returned Let's say this is your alert condition query: SELECT count(*) FROM SyntheticCheck WHERE monitorName = 'My Cool Monitor' AND result = 'FAILURE' Copy If there are no failures for the aggregation window: The system will execute the FROM clause by grabbing all SyntheticCheck events on your account. Then it will execute the WHERE clause to filter through those events by looking only for the ones that match the monitor name and result specified. If there are still events left to scan through after completing the FROM and WHERE operations, the SELECT clause will be executed. If there are no remaining events, the SELECT clause will not be executed. This means that aggregators like count() and uniqueCount() will never return a zero value. When there is a count of 0, the SELECT clause is ignored and no data is returned, resulting in a value of NULL. Example: zero value returned If you have a data source delivering legitimate numeric zeroes, the query will return zero values and not null values. Let's say this is your alert condition query, and that MyCoolEvent is an attribute that can sometimes return a zero value. SELECT average(MyCoolAttribute) FROM MyCoolEvent Copy If, in the aggregation window being evaluated, there's at least one instance of MyCoolEvent and if the average value of all MyCoolAttribute attributes from that window is equal to zero, then a 0 value will be returned. If there are no MyCoolEvent events during that minute, then a NULL will be returned due to the order of operations. Tip For more information about this topic, you can check out our blog post on troubleshooting for zero versus null values. Tip You can determine how null values will be handled by adjusting loss of signal and gap filling settings in the Alert Conditions UI. Tip You can avoid NULL values entirely with a query order of operations shortcut. Do this by using a filter sub-clause, then including all filter elements within that sub-clause. The main body of the query will run and return data, at which point the SELECT clause will then run and apply the filter elements. The query will return a value of 0 if the filter elements result in no matching data. Here's an example: SELECT filter(count(*), WHERE result = 'SUCCESS' AND monitorName = 'My Favorite Monitor') FROM SyntheticCheck Copy Nested aggregation NRQL alerts Nested aggregation queries are a powerful way to query your data. However, they have a few restrictions that are important to note. Nested queries with a non-faceted innermost query are not currently supported Without a FACET, the inner query produces a single result, giving the outer query nothing to aggregate. If you're using a nested query, make sure your inner query is faceted. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu) ​​​​​ Copy Queries at all levels must have the same aggregation window size With an alert aggregation window of 1 minute, the inner query would produce two smaller windows of 30 seconds. In theory, these two windows could be aggregated by the outer query. However, this is not currently supported. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu TIMESERIES 30 seconds)​​ Copy Signal loss is not yet supported for nested queries For more information on signal loss, see NerdGraph API: Loss of signal and gap filling. Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported You can't use a nested query containing the WITH METRIC_FORMAT in the inner query to create NRQL alert conditions. NRQL condition creation tips Here are some tips for creating and using a NRQL condition: Topic Tips Condition types NRQL condition types include static, baseline, and outlier. Create a description For NRQL conditions, you can create a custom description to add to each violation. Descriptions can be enhanced with variable substitution based on metadata in the specific violation. For details, see Description Query results Queries must return a number. The condition evaluates the returned number against the thresholds you've set. Time period NRQL conditions evaluate data based on how it's aggregated, using aggregation windows from 30 seconds to 15 minutes, in increments of 15 seconds. For best results, we recommend using the event flow or event timer aggregation methods. For the cadence aggregation method, the implicit SINCE ... UNTIL clause specifying which minute to evaluate is controlled by your delay/timer setting. Since very recent data may be incomplete, you may want to query data from 3 minutes ago or longer, especially for: Applications that run on multiple hosts. SyntheticCheck data: Timeouts can take 3 minutes, so 5 minutes or more is recommended. Also, if a query will generate intermittent data, consider using the sum of query results option. Lost signal threshold (loss of signal detection) You can use loss of signal detection to alert on when your data (a telemetry signal) should be considered lost. A signal loss can indicate that a service or entity is no longer online or that a periodic job failed to run. You can also use this to make sure that violations for sporadic data, such as error counts, are closed when no signal is coming in. Advanced signal settings These settings give you options for better handling continuous, streaming data signals that may sometimes be missing. These settings include the aggregation window duration, the delay/timer, and an option for filling data gaps. For more on using these, see Advanced signal settings. Condition settings Use the Condition settings to: Create a concise, descriptive condition name. Provide a custom violation description for the condition that will be included in violations and notifications. Add the runbook URL to include your organization's procedures for handling incidents. You may also add this information to the custom violation description. Limits on conditions See the maximum values. Health status In order for a NRQL alert condition health status display to function properly, use a FACET clause to scope each signal to a single entity (for example, FACET hostname or FACET appname). Examples For more information, see: Expected NRQL syntax Examples of NRQL condition queries Condition edits can reset condition evaluation When you edit NRQL alert conditions in some specific ways (detailed below), their evaluations are reset, meaning that any evaluation up until that point is lost, and the evaluation starts over from that point. The two ways this will affect you are: For \"for at least x minutes\" thresholds: because the evaluation window has been reset, there will be a delay of at least x minutes before any violations can be reported. For baseline conditions: the condition starts over again and all baseline learning is lost. The following actions cause an evaluation reset for NRQL conditions: Changing the query Changing the aggregation window, aggregation method, or aggregation delay/timer setting Changing the \"close violations on signal loss\" setting Changing any gap fill settings Changing the baseline direction (if applicable) – higher, lower, or higher/lower Change the threshold value, threshold window, or threshold operator The following actions (along with any other actions not covered in the above list) will not reset the evaluation: Changing the loss of signal time window (expiration duration) Changing the time function (switching \"for at least\" to \"at least once in,\" or vice-versa) Toggling the \"open violation on signal loss\" setting Alert condition types When you create a NRQL alert, you can choose from different types of conditions: NRQL alert condition types Description Static This is the simplest type of NRQL condition. It allows you to create a condition based on a NRQL query that returns a numeric value. Optional: Include a FACET clause. Baseline (Dynamic) Uses a self-adjusting condition based on the past behavior of the monitored values. Uses the same NRQL query form as the static type, including the optional FACET clause. Outlier Looks for group behavior and values that are outliers from those groups. Uses the same NRQL query form as the static type, but requires a FACET clause. Sum of query results (limited or intermittent data) Important Available only for static (basic) condition types. If a query returns intermittent or limited data, it may be difficult to set a meaningful threshold. Missing or limited data will sometimes generate false positives or false negatives. You can use loss of signal, aggregation duration, and gap filling settings to minimize these false notifications. To avoid this problem when using the static threshold type, you can set the selector to sum of query results. This lets you set the alert on an aggregated sum instead of a value from a single harvest cycle. Up to two hours of one-minute data checks can be aggregated. The duration you select determines the width of the rolling sum and the preview chart will update accordingly. Set the loss of signal threshold Loss of signal occurs when no data matches the NRQL condition over a specific period of time. You can set your loss of signal threshold duration and also what happens when the threshold is crossed. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Loss of signal is only available for NRQL conditions. You may also manage these settings using the GraphQL API (recommended), or the REST API. Go here for specific GraphQL API examples. Loss of signal settings: Loss of signal settings include a time duration and two possible actions. Signal loss expiration time UI label: Signal is lost after: GraphQL Node: expiration.expirationDuration Expiration duration is a timer that starts and resets when we receive a data point in the streaming alerts pipeline. If we don't receive another data point before your 'expiration time' expires, we consider that signal to be lost. This can be because no data is being sent to New Relic or the WHERE clause of your NRQL query is filtering that data out before it is streamed to the alerts pipeline. Note that when you have a faceted query, each facet is a signal. So if any one of those signals ends during the duration specified, that will be considered a loss of signal. The loss of signal expiration time is independent of the threshold duration and triggers as soon as the timer expires. The maximum expiration duration is 48 hours. This is helpful when monitoring for the execution of infrequent jobs. The minimum is 30 seconds, but we recommend using at least 3-5 minutes. Loss of signal actions Once a signal is considered lost, you can close open violations, open new violations, or both. Close all current open violations: This closes all open violations that are related to a specific signal. It won't necessarily close all violations for a condition. If you're alerting on an ephemeral service, or on a sporadic signal, you'll want to choose this action to ensure that violations are closed properly. The GraphQL node name for this is \"closeViolationsOnExpiration\" Open new violations: This will open a new violation when the signal is considered lost. These violations will indicate that they are due to a loss of signal. Based on your incident preferences, this should trigger a notification. The graphQL node name for this is \"openViolationOnExpiration\" When you enable both actions, we'll close all open violations first, and then open a new violation for loss of signal. To create a NRQL alert configured with loss of signal detection in the UI: For a policy, when you create a condition, under Select a product, click NRQL, then click Next, define thresholds. Write a NRQL query that returns the values you want to alert on. For Threshold type, select Static or Baseline. Click + Add lost signal threshold, then set the signal expiration duration time in minutes or seconds in the Signal is lost after field. Choose what you want to happen when the signal is lost. You can check one or both of Close all current open violations and Open new \"lost signal\" violation. These control how loss of signal violations will be handled for the condition. Make sure you name your condition before you save it. Violations open due to loss of signal close when the signal comes back. Newly opened lost signal violations will close immediately when new data is evaluated. the condition they belong to expires. By default, conditions expire after 3 days. you manually close the violation with the Close all current open violations option. Tip Loss of signal detection doesn't work on NRQL queries that use nested aggregation or sub-queries. Advanced signal settings When creating a NRQL alert condition, use the advanced signal settings to control streaming alert data and avoid false alarms. When creating a NRQL condition, there are several advanced signal settings: Aggregation window duration Delay/timer Fill data gaps To read an explanation of what these settings are and how they relate to each other, see Streaming alerts concepts. Below are instructions and tips on how to configure them. Aggregation window duration You can set the aggregation window duration to choose how long data is accumulated in a streaming time window before it's aggregated. You can set it to anything between 30 seconds and 15 minutes. The default is one minute. Tip Baseline alert condition thresholds don't support editing the aggregation window. They use the 1 minute default. Delay/timer You can adjust the delay/timer to coordinate our streaming alerting algorithm with your data's behavior. If your data is sparse or inconsistent, you may want to use the event timer aggregation method. For the cadence method, the total supported latency is the sum of the aggregation window duration and the delay. If the data type comes from an APM language agent and is aggregated from many app instances (for example, Transactions, TransactionErrors, etc.), we recommend using the event flow method with the default settings. Important When creating NRQL conditions for data collected from Infrastructure Cloud Integrations such as AWS Cloudwatch or Azure, we recommend that you use the event timer method. Fill data gaps Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with one of these settings: None: (Default) Choose this if you don't want to take any action on empty aggregation windows. On evaluation, an empty aggregation window will reset the threshold duration timer. For example, if a condition says that all aggregation windows must have data points above the threshold for 5 minutes, and 1 of the 5 aggregation windows is empty, then the condition won't be in violation. Custom static value: Choose this if you'd like to insert a custom static value into the empty aggregation windows before they're evaluated. This option has an additional, required parameter of fillValue (as named in the API) that specifies what static value should be used. This defaults to 0. Last known value: This option inserts the last seen value before evaluation occurs. We maintain the state of the last seen value for 2 hours. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss, gap filling, and how to request access to these features, see this Explorers Hub post. Options for editing data gap settings: In the NRQL conditions UI, go to Condition settings > Advanced signal settings > fill data gaps with and choose an option. If using our Nerdgraph API (preferred), this node is located at: actor : account : alerts : nrqlCondition : signal : fillOption | fillValue NerdGraph is our recommended API for this but if you're using our REST API, you can find this setting in the REST API explorer under the \"signal\" section of the Alert NRQL conditions API.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.00891,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create NRQL <em>alert</em> conditions",
        "sections": "Nested aggregation NRQL <em>alerts</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": " a NRQL <em>alert</em> condition for a policy: On one.newrelic.com, in the header click <em>Alerts</em> &amp; AI, then in the left sidebar click Policies. Select an existing policy or click <em>New</em> <em>alert</em> policy to create a <em>new</em> policy. Click Add a condition. Under Select a product click NRQL, and then click Next, define"
      },
      "id": "603ef04864441fbc114e8883"
    }
  ],
  "/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/alerts-nerdgraph/nerdgraph-api-loss-signal-gap-filling": [
    {
      "sections": [
        "NerdGraph tutorial: Notification channels",
        "Tip",
        "Get notification channels",
        "List all notification channels for an account",
        "Paginate through notification channels with cursor pagination",
        "Find a specific notification channel by id",
        "List notification channels with their associated policies",
        "Create a notification channel",
        "Caution",
        "Create an email notification channel",
        "Create an OpsGenie notification channel",
        "Create a PagerDuty notification channel",
        "Create a Slack notification channel",
        "Create a VictorOps notification channel",
        "Create a Webhook notification channel",
        "Create an xMatters notification channel",
        "Update a notification channel",
        "Update an email notification channel",
        "Update an OpsGenie notification channel",
        "Update a PagerDuty notification channel",
        "Update a Slack notification channel",
        "Update a VictorOps notification channel",
        "Update a Webhook notification channel",
        "Update an xMatters notification channel",
        "Delete a notification channel",
        "Associate channels to a policy",
        "Dissociate a channel from a policy"
      ],
      "title": "NerdGraph tutorial: Notification channels",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alerts and NerdGraph"
      ],
      "external_id": "d62b37e04d0601fec177951123d72e13f57458a0",
      "image": "",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/alerts-nerdgraph/nerdgraph-api-notification-channels/",
      "published_at": "2021-12-15T12:45:54Z",
      "updated_at": "2021-11-15T05:31:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can manage your alert notification channels using our GraphQL NerdGraph API. Here are some queries and mutations you can develop in our NerdGraph API explorer. Tip See the NerdGraph introduction for help getting started with NerdGraph API explorer. Get notification channels The notificationChannels query allows you to paginate through all of your notification channels per account. You can also use the notificationChannel query to get a specific notification channel by its ID. Tip Note that certain secret fields (for example, passwords or API keys) are obfuscated in the returned fields. List all notification channels for an account This example returns every field for every notification channel on the supplied account ID, up to the page limit of 200. Note how we use inline fragments to refer to the specific fields on the concrete types implementing the AlertsNotificationChannel interface. { actor { account(id: YOUR_ACCOUNT_ID) { alerts { notificationChannels { channels { id name type ... on AlertsXMattersNotificationChannel { config { integrationUrl } } ... on AlertsWebhookNotificationChannel { config { baseUrl basicAuth { password username } customHttpHeaders { name value } customPayloadBody customPayloadType } } ... on AlertsVictorOpsNotificationChannel { config { key routeKey } } ... on AlertsUserNotificationChannel { config { userId } } ... on AlertsSlackNotificationChannel { config { teamChannel url } } ... on AlertsPagerDutyNotificationChannel { config { apiKey } } ... on AlertsOpsGenieNotificationChannel { config { apiKey dataCenterRegion recipients tags teams } } ... on AlertsHipChatNotificationChannel { config { authToken baseUrl roomId } } ... on AlertsEmailNotificationChannel { config { emails includeJson } } ... on AlertsCampfireNotificationChannel { config { room subdomain token } } } totalCount nextCursor } } } } } Copy Paginate through notification channels with cursor pagination If a given account's list of notification channels exceeds the 200 channel page limit, you can use the pagination cursor to retrieve additional pages. With cursor pagination, you continue to request additional pages using the nextCursor until that field returns empty in the response. An empty nextCursor signals that you have reached the end of the result set. Here's an example: { actor { account(id: YOUR_ACCOUNT_ID) { alerts { notificationChannels { channels { id name type } totalCount nextCursor } } } } } Copy The code above returns a set of results like this: { \"data\": { \"actor\": { \"account\": { \"alerts\": { \"notificationChannels\": { \"channels\": [ { \"id\": \"250\", \"name\": \"Channel 1\", \"type\": \"SLACK\" }, { \"id\": \"713\", \"name\": \"Channel 2\", \"type\": \"WEBHOOK\" }, // ... +198 more notification channels in reality ], \"nextCursor\": \"Wh4LK9JYzfACVlNkyvf7Rg==:I5VbSEpgx3UWNA5AOVsUPv4=\", \"totalCount\": 268 } } } } } } Copy In your next request, provide the cursor like so, updating each subsequent request to return the updated cursor, until the cursor is empty: { actor { account(id: YOUR_ACCOUNT_ID) { alerts { notificationChannels(cursor: \"Wh4LK9JYzfACVlNkyvf7Rg==:I5VbSEpgx3UWNA5AOVsUPv4=\") { channels { id name type } totalCount nextCursor } } } } } Copy Find a specific notification channel by id If you have a specific notification channel's ID, the API allows you to look it up directly. Note that because the specific channel is a concrete type implementing the AlertsNotificationChannel interface, you may need to specify certain fields using the ... on syntax for inline fragments. In this example, we are retrieving a Slack channel: { actor { account(id: YOUR_ACCOUNT_ID) { alerts { notificationChannel(id: YOUR_CHANNEL_ID) { id name type ... on AlertsSlackNotificationChannel { config { teamChannel url } } } } } } } Copy List notification channels with their associated policies This example returns the ID, name, and type for every notification channel on the supplied account ID, as well as a list of every policy that is associated with that channel. { actor { account(id: YOUR_ACCOUNT_ID) { alerts { notificationChannels { channels { id name type associatedPolicies { policies { id name } totalCount } } nextCursor totalCount } } } } } Copy Create a notification channel In order to create an alert notification channel, you need to know the specific type of notification channel you want to create (for example email, Slack, etc.), as well as the details necessary to configure it (which will depend on the channel type). Once a notification channel has been created, it can be associated with one or more alert policies. Once associated, those channels will receive notifications from those policies when conditions are violated. Caution While you can query for any existing notification channel type, you can only create a subset of them. Specifically, the user channel type has no editable fields, and the Campfire and HipChat channel types are both deprecated. Create an email notification channel An example create mutation for an email notification channel: mutation { alertsNotificationChannelCreate(accountId: YOUR_ACCOUNT_ID, notificationChannel: { email: { emails: [\"email@example.com\"], includeJson: true, name: \"Some Name <email@example.com>\" } }) { notificationChannel { ... on AlertsEmailNotificationChannel { id name type config { emails includeJson } } } error { description errorType } } } Copy Create an OpsGenie notification channel An example create mutation for an OpsGenie notification channel: mutation { alertsNotificationChannelCreate(accountId: YOUR_ACCOUNT_ID, notificationChannel: { opsGenie: { apiKey: \"api-key-from-opsgenie\", dataCenterRegion: US, name: \"OpsGenie notification channel name\", recipients: [\"user@example.com\"], tags: [\"tag1\", \"tag2\"], teams: [\"team1\", \"team2\"] } }) { notificationChannel { ... on AlertsOpsGenieNotificationChannel { id name type config { apiKey teams tags recipients dataCenterRegion } } } error { description errorType } } } Copy Create a PagerDuty notification channel An example create mutation for a PagerDuty notification channel: mutation { alertsNotificationChannelCreate(accountId: YOUR_ACCOUNT_ID, notificationChannel: { pagerDuty: { name: \"PagerDuty notification channel name\", apiKey: \"api-key-from-pagerduty\" } }) { notificationChannel { ... on AlertsPagerDutyNotificationChannel { id name type config { apiKey } } } error { description errorType } } } Copy Create a Slack notification channel An example create mutation for a Slack notification channel: mutation { alertsNotificationChannelCreate(accountId: YOUR_ACCOUNT_ID, notificationChannel: { slack: { name: \"Slack notification channel name\", teamChannel: \"#team-channel\", url: \"https://hooks.slack.com/services/FAKE/MOREFAKE/IMAGINARYEXAMPLEURLCHUNK\" } }) { notificationChannel { ... on AlertsSlackNotificationChannel { id name type config { teamChannel url } } } error { description errorType } } } Copy Create a VictorOps notification channel An example create mutation for a VictorOps notification channel: mutation { alertsNotificationChannelCreate(accountId: YOUR_ACCOUNT_ID, notificationChannel: { victorOps: { key: \"example-api-key-from-victorops\", name: \"VictorOps notification channel name\", routeKey: \"example-route-key\" } }) { notificationChannel { ... on AlertsVictorOpsNotificationChannel { id name type config { key routeKey } } } error { description errorType } } } Copy Create a Webhook notification channel An example create mutation for a Webhook notification channel: mutation { alertsNotificationChannelCreate(accountId: YOUR_ACCOUNT_ID, notificationChannel: { webhook: { baseUrl: \"https://example.com/webhook\", basicAuth: { password: \"t0t4lly-s3cr3t-p455w0rd\", username: \"webhook-user\" }, customHttpHeaders: [ {name: \"X-Api-Key\", value: \"100%-real-api-key\"}, {name: \"X-Calling-Service\", value: \"New Relic Alerts\"} ], customPayloadBody: \"{ \\\"account_id\\\": \\\"$ACCOUNT_ID\\\", \\\"account_name\\\": \\\"$ACCOUNT_NAME\\\", \\\"closed_violations_count_critical\\\": \\\"$CLOSED_VIOLATIONS_COUNT_CRITICAL\\\", \\\"closed_violations_count_warning\\\": \\\"$CLOSED_VIOLATIONS_COUNT_WARNING\\\", \\\"condition_description\\\": \\\"$DESCRIPTION\\\", \\\"condition_family_id\\\": \\\"$CONDITION_FAMILY_ID\\\", \\\"condition_name\\\": \\\"$CONDITION_NAME\\\", \\\"current_state\\\": \\\"$EVENT_STATE\\\", \\\"details\\\": \\\"$EVENT_DETAILS\\\", \\\"duration\\\": \\\"$DURATION\\\", \\\"event_type\\\": \\\"$EVENT_TYPE\\\", \\\"incident_acknowledge_url\\\": \\\"$INCIDENT_ACKNOWLEDGE_URL\\\", \\\"incident_id\\\": \\\"$INCIDENT_ID\\\", \\\"incident_url\\\": \\\"$INCIDENT_URL\\\", \\\"metadata\\\": \\\"$METADATA\\\", \\\"open_violations_count_critical\\\": \\\"$OPEN_VIOLATIONS_COUNT_CRITICAL\\\", \\\"open_violations_count_warning\\\": \\\"$OPEN_VIOLATIONS_COUNT_WARNING\\\", \\\"owner\\\": \\\"$EVENT_OWNER\\\", \\\"policy_name\\\": \\\"$POLICY_NAME\\\", \\\"policy_url\\\": \\\"$POLICY_URL\\\", \\\"runbook_url\\\": \\\"$RUNBOOK_URL\\\", \\\"severity\\\": \\\"$SEVERITY\\\", \\\"targets\\\": \\\"$TARGETS\\\", \\\"timestamp\\\": \\\"$TIMESTAMP\\\", \\\"timestamp_utc_string\\\": \\\"$TIMESTAMP_UTC_STRING\\\", \\\"violation_callback_url\\\": \\\"$VIOLATION_CALLBACK_URL\\\", \\\"violation_chart_url\\\": \\\"$VIOLATION_CHART_URL\\\" }\", customPayloadType: JSON, name: \"Webhook notification channel name\" } }) { notificationChannel { ... on AlertsWebhookNotificationChannel { id name type config { customPayloadType customPayloadBody customHttpHeaders { value name } basicAuth { password username } baseUrl } } } error { description errorType } } } Copy Create an xMatters notification channel An example create mutation for an xMatters notification channel: mutation { alertsNotificationChannelCreate(accountId: YOUR_ACCOUNT_ID, notificationChannel: { xMatters: { integrationUrl: \"https://company.instance.xmatters.com/api/xm/v<version>/...\", name: \"xMatters notification channel name\" } }) { notificationChannel { ... on AlertsXMattersNotificationChannel { id name type config { integrationUrl } } } error { description errorType } } } Copy Update a notification channel In order to update an alert notification channel, you need to know the specific type of notification channel you want to change (for example email, Slack, etc.), as well as the details necessary to configure it (which will depend on the channel type). Consistent with other GraphQL APIs, you can update a single field on the channel without knowing anything other than the channel's ID. Caution While you can query for any existing notification channel type, you can only update a subset of them. Specifically, the user channel type has no editable fields, and the Campfire and HipChat channel types are both deprecated. Update an email notification channel An example update mutation for an email notification channel where we're updating only the name: mutation { alertsNotificationChannelUpdate(accountId: YOUR_ACCOUNT_ID, id: YOUR_CHANNEL_ID, notificationChannel: { email: { name: \"Updated Name <email@example.com>\" } }) { notificationChannel { ... on AlertsEmailNotificationChannel { id name type } } error { description errorType notificationChannelId } } } Copy Update an OpsGenie notification channel An example update mutation for an OpsGenie notification channel where we're updating only the name: mutation { alertsNotificationChannelUpdate(accountId: YOUR_ACCOUNT_ID, id: YOUR_CHANNEL_ID, notificationChannel: { opsGenie: { name: \"OpsGenie updated channel name\" } }) { notificationChannel { ... on AlertsOpsGenieNotificationChannel { id name type } } error { description errorType notificationChannelId } } } Copy Update a PagerDuty notification channel An example update mutation for a PagerDuty notification channel where we're updating only the name: mutation { alertsNotificationChannelUpdate(accountId: YOUR_ACCOUNT_ID, id: YOUR_CHANNEL_ID, notificationChannel: { pagerDuty: { name: \"PagerDuty updated channel name\" } }) { notificationChannel { ... on AlertsPagerDutyNotificationChannel { id name type } } error { description errorType notificationChannelId } } } Copy Update a Slack notification channel An example update mutation for a Slack notification channel where we're updating only the name: mutation { alertsNotificationChannelUpdate(accountId: YOUR_ACCOUNT_ID, id: YOUR_CHANNEL_ID, notificationChannel: { slack: { name: \"Slack updated channel name\" } }) { notificationChannel { ... on AlertsSlackNotificationChannel { id name type } } error { description errorType notificationChannelId } } } Copy Update a VictorOps notification channel An example update mutation for a VictorOps notification channel where we're updating only the name: mutation { alertsNotificationChannelUpdate(accountId: YOUR_ACCOUNT_ID, id: YOUR_CHANNEL_ID, id: YOUR_CHANNEL_ID, notificationChannel: { victorOps: { name: \"VictorOps updated channel name\" } }) { notificationChannel { ... on AlertsVictorOpsNotificationChannel { id name type } } error { description errorType notificationChannelId } } } Copy Update a Webhook notification channel An example update mutation for a Webhook notification channel where we're updating only the name: mutation { alertsNotificationChannelUpdate(accountId: YOUR_ACCOUNT_ID, id: YOUR_CHANNEL_ID, notificationChannel: { webhook: { name: \"Webhook updated channel name\" } }) { notificationChannel { ... on AlertsWebhookNotificationChannel { id name type } } error { description errorType notificationChannelId } } } Copy Update an xMatters notification channel An example update mutation for an xMatters notification channel where we're updating only the name: mutation { alertsNotificationChannelUpdate(accountId: YOUR_ACCOUNT_ID, id: YOUR_CHANNEL_ID, notificationChannel: { xMatters: { name: \"xMatters updated channel name\" } }) { notificationChannel { ... on AlertsXMattersNotificationChannel { id name type } } error { description errorType notificationChannelId } } } Copy Delete a notification channel You can delete a notification channel with only the account ID and the channel ID. Note that deleting a channel dissociates it from all policies, meaning that no further notifications will be sent to that channel. mutation { alertsNotificationChannelDelete(accountId: YOUR_ACCOUNT_ID, id: YOUR_CHANNEL_ID) { id error { description errorType notificationChannelId } } } Copy Associate channels to a policy Creating an alert notification channel is not enough: Once the channel has been created, it needs to be associated to one or more policies. Once associated to a policy, the channel can recieve alert notifications when conditions on that policy go into violation. In this example, we associate two channels with a policy: mutation { alertsNotificationChannelsAddToPolicy(accountId: YOUR_ACCOUNT_ID, notificationChannelIds: [FIRST_CHANNEL_ID, SECOND_CHANNEL_ID], policyId: YOUR_POLICY_ID ) { notificationChannels { id } policyId errors { description errorType notificationChannelId } } } Copy Dissociate a channel from a policy In those instances where a notification channel has outlived its usefulness (for example, an email list that has been retired), the time has come to dissociate that channel from the policy (or policies) that are sending alert notifications to it. This API call leaves the channel itself intact, but removes it from the specified policy. In this example, we are removing two channels from a policy (leaving any others in place), and getting back confirmation that those two channel IDs have been removed: mutation { alertsNotificationChannelsRemoveFromPolicy(accountId: YOUR_ACCOUNT_ID, notificationChannelIds: [FIRST_CHANNEL_ID, SECOND_CHANNEL_ID], policyId: YOUR_POLICY_ID ) { notificationChannels { id } policyId errors { description errorType notificationChannelId } } } Copy Tip Removing an alert notification channel from a policy does not delete the channel because it might be used by other policies. On the other hand, deleting a channel will cause all associated policies to stop sending alert notifications to that channel.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.80414,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>NerdGraph</em> tutorial: Notification channels",
        "sections": "<em>NerdGraph</em> tutorial: Notification channels",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": ": &quot;<em>New</em> <em>Relic</em> <em>Alerts</em>&quot;} ], customPayloadBody: &quot;{ \\&quot;account_id\\&quot;: \\&quot;$ACCOUNT_ID\\&quot;, \\&quot;account_name\\&quot;: \\&quot;$ACCOUNT_NAME\\&quot;, \\&quot;closed_violations_count_critical\\&quot;: \\&quot;$CLOSED_VIOLATIONS_COUNT_CRITICAL\\&quot;, \\&quot;closed_violations_count_warning\\&quot;: \\&quot;$CLOSED_VIOLATIONS_COUNT_WARNING\\&quot;, \\&quot;condition_description"
      },
      "id": "6130bf9c28ccbc027d56a863"
    },
    {
      "sections": [
        "Streaming alerts: key terms and concepts",
        "Why it matters",
        "Streaming alerts process and descriptions",
        "Choose your aggregation method",
        "Event flow",
        "Caution",
        "Event timer",
        "Cadence",
        "Streaming alerts tools",
        "Tip",
        "Window duration",
        "Delay/timer",
        "Loss of signal detection",
        "Gap filling"
      ],
      "title": "Streaming alerts: key terms and concepts",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Get started"
      ],
      "external_id": "144c7f0721ec2acb1e8ac8aa06142911ea552aef",
      "image": "https://docs.newrelic.com/static/39158bd84483adf9516bb7ff5058c9bd/d30ee/streaming-alerts.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/streaming-alerts-key-terms-concepts/",
      "published_at": "2021-12-15T12:47:58Z",
      "updated_at": "2021-12-15T12:47:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The streaming alerts platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into New Relic. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that's processed by the streaming algorithm. There are three methods for aggregating the data filtered through your NRQL condition: Event flow (default) Event timer Cadence This short video describes the three aggregation method (5:31). Why it matters Understanding how streaming alerts works will help you fine-tune your NRQL conditions to be notified about what's important to you. Only data that matches the conditions of the NRQL WHERE clause is alerted on. For more details on each step of the process, see Streaming alerts process and descriptions. As data streams into New Relic, it's filtered by the NRQL condition. Before data is evaluated, it must meet the criteria defined by the NRQL query's WHERE clause. Instead of evaluating that data immediately for violations, the NRQL alert conditions collect the data over a period of time known as the aggregation window. An additional delay/timer allows for slower data points to arrive before the window is aggregated. Once the delay/timer time has elapsed, New Relic aggregates the data into a single data point. Alerts then evaluates the data point using the NRQL condition in order to determine whether it meets the violation threshold criteria. Even if a data point meets the criteria for a violation, a violation may not be triggered. A violation is only triggered when data points consistently meet the threshold criteria over a period of time. This is the threshold duration. If the data points are in violation for an entire threshold duration, we'll send you a notification based on your policy settings. All of these configurable delays give you more control over how you're alerted on sporadic and missing data. Streaming alerts process and descriptions Process Description Streaming data All data coming into New Relic. WHERE clause Filters all incoming streaming data. We only monitor for alerts on data that makes it through this filter. Aggregation methods One of three methods that control how data is collected before it's evaluated. They are: Event flow (Default) Event timer Cadence Aggregation window Data with timestamps that fall within this window will be aggregated and then evaluated. Delay/timer A time delay to ensure all data points have arrived in the aggregation window before aggregation occurs. Aggregated data Data in the aggregated window is collapsed to a single data point for alert evaluation. Evaluation The data point is evaluated by the NRQL condition, which is trigged by each incoming aggregated data point. Threshold duration A specific duration that determines if a violation is created. If your specified NRQL condition meets the threshold criteria over the threshold duration, a violation occurs. When a data point lacks data, a custom value is inserted to fill the gap. Choose your aggregation method You can choose between three different aggregation methods, depending on your needs. Event flow (default) works best for data that comes in frequently and mostly in order. Event timer works best for data that arrives infrequently in batches, such as cloud integration data or infrequent error logs. Cadence is our original and inferior aggregation method. It aggregates data on specific time intervals as detected by New Relic's internal wall clock, regardless of data timestamps. Event flow Event flow aggregates a window of data when the first data point arrives in a subsequent window. The custom delay defines which subsequent window data will start to populate to trigger aggregation of the current window. A custom delay provides extra time for data to arrive. These times are based on the data's timestamps and not New Relic's wall clock time. For example, suppose you're monitoring CPU usage in window durations of 1 minute and a 3 minute delay. When a CPU usage data point comes in with a timestamp between 12:00pm and 12:01pm, event flow will not aggregate that window until a data point shows up with a timestamp between 12:04pm and 12:05pm. When event flow receives the first data point with a timestamp of 12:04pm or later, it sends the 12:00 to 12:01 data to be aggregated. Caution If you expect your data points to arrive more than 30 minutes apart, please use the Event Timer method described below. Event timer Like event flow, event timer will only aggregate data for a given window when data arrives for that window. When a data point arrives for an aggregation window, a timer dedicated to that window starts to count down. If no further data arrives before the timer counts down, the data for that window is aggregated. If more data points arrive before the timer has completed counting down, the timer is reset. For example, suppose you're monitoring CloudWatch data that arrives fairly infrequently. You're using a window duration of 1 minute and a 3 minute timer. When a CloudWatch data point comes in with a timestamp between 12:00pm and 12:01pm, the timer will start to count down. If no further data points show up for that 12:00-12:01 window, the window will get aggregated 3 minutes later. If a new data point with a timestamp between 12:00 and 12:01 arrives, the timer resets. It keeps resetting every time more data points for that window arrive. The window will not be sent for aggregation until the timer reaches 0. If the timer for a later data point elapses before an earlier data point, the event timer method waits for the earlier timer to elapse before aggregating the later data point. For best results, make sure your timer is equal to or longer than your window duration time. If the timer is less than your window duration and your data flow is inconsistent, then your data may be evaluated before all of your data points arrive. This could cause you to be notified incorrectly. Cadence We recommend you use one of the other two methods. Cadence is our old streaming aggregation method. This method uses New Relic's wall clock time to determine when data is aggregated and evaluated. It doesn't take into account data point timestamps as they arrive. Streaming alerts tools Streaming alerts provide a set of tools to give you greater control over how your data is aggregated before it's evaluated to reduce incorrect notifications you receive. They are: Window duration Delay/timer Loss of signal detection Gap filling Tip This article covers these tools at a conceptual level. You'll find direct instructions on how to use these tools in Create NRQL alert conditions. Window duration In order to make loss of signal detection more effective and to reduce unnecessary notifications, you can customize aggregation windows to the duration that you need. An aggregation window is a specific block of time. We gather data points together in an aggregation window, before evaluating the data. A longer aggregation window can smooth out the data, since an outlier data point will have more data points to be aggregated with, giving it less of an influence on the aggregated data point that is sent for evaluation. When a data point arrives, its timestamp is used to put it in the proper aggregation window. You can set your aggregation window to anything between 30 seconds and 15 minutes. The default is 1 minute. Delay/timer The delay/timer setting controls how long the condition should wait before aggregating the data in the aggregation window. The event flow and cadence methods use delay. Event timer uses timer. The delay default is 2 minutes. The timer default is 1 minute and has a minimum value of 5 seconds. Loss of signal detection Loss of signal occurs when no data matches the NRQL condition over a specific period of time. A loss of signal is caused by different things. The WHERE clause in your NRQL query can filter out data before it's evaluated for violations. It could also mean a service or entity is offline or a periodic job has failed to run and no data is being sent to New Relic. In order to avoid unnecessary notifications, you can choose how long to wait before you're notified by a loss of signal violation. You can use loss of signal detection to open violations and be notified when a signal is lost. Alternately, you can use a loss of signal to close violations for ephemeral services or sporadic data, such as error counts. Gap filling Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with the last value received, a static value, or else do nothing and leave the gap there. The default is None. Gaps in streaming data can be caused by network or host issues, a signal may be sparse, or some signals, such as error counts, may only have data when something is wrong. By filling the gaps with known values, the alert evaluation process can process those gaps and determine how they should affect the loss of signal evaluation. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss and gap filling, see this Explorers Hub post.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.07324,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "sections": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "The streaming <em>alerts</em> platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into <em>New</em> <em>Relic</em>. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that&#x27;s processed"
      },
      "id": "6130c054e7b9d269edb6f21d"
    },
    {
      "sections": [
        "Create NRQL alert conditions",
        "Tip",
        "Create a NRQL alert condition",
        "Create a condition from a chart",
        "Caution",
        "NRQL alert syntax",
        "Important",
        "Reformatting incompatible NRQL",
        "NRQL alert threshold examples",
        "Alert on specific segments of your data",
        "Alert on Nth percentile of your data",
        "Alert on max, min, avg of your data",
        "Alert on a percentage of your data",
        "Alert on Apdex with any T-value",
        "NRQL conditions and query order of operations",
        "Example: null value returned",
        "Example: zero value returned",
        "Nested aggregation NRQL alerts",
        "Nested queries with a non-faceted innermost query are not currently supported",
        "Queries at all levels must have the same aggregation window size",
        "Signal loss is not yet supported for nested queries",
        "Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported",
        "NRQL condition creation tips",
        "Condition edits can reset condition evaluation",
        "Alert condition types",
        "Sum of query results (limited or intermittent data)",
        "Set the loss of signal threshold",
        "Advanced signal settings",
        "Aggregation window duration",
        "Delay/timer",
        "Fill data gaps"
      ],
      "title": "Create NRQL alert conditions",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert conditions"
      ],
      "external_id": "2872f770a5fc8a802f99b9f848906f7e351ad761",
      "image": "https://docs.newrelic.com/static/eb8e1b2d826f9fd9cf46fe3dd5455217/c1b63/nr1_nrql_alert_conditions.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/",
      "published_at": "2021-12-19T16:01:17Z",
      "updated_at": "2021-12-14T22:49:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use NRQL queries to create alert conditions. Once you've defined your signal, you can further define your warning and critical threshold levels. This determines when an alerts violation is created. Read on to learn more about how to do this. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Click NRQL, and then Next, define thresholds. Tip For more information on key concepts relating to NRQL alert conditions and streaming alerts, see Streaming alerts: key terms and concepts. Create a NRQL alert condition To create a NRQL alert condition for a policy: On one.newrelic.com, in the header click Alerts & AI, then in the left sidebar click Policies. Select an existing policy or click New alert policy to create a new policy. Click Add a condition. Under Select a product click NRQL, and then click Next, define thresholds. Note that editing an existing condition can result in resetting its evaluation. Create a condition from a chart You can use a chart to create a NRQL alert condition. To create a NRQL alerts condition from a chart, click the chart menu , then click Create alert condition. Once you've named and customized your condition, you can add it to an existing policy or create a new one. Caution A small number of our older charts don't include the option to create an alert condition. NRQL alert syntax Here's the basic syntax for creating all NRQL alert conditions. The FACET clause is required for outlier condition types. It's optional for static and baseline. SELECT function(attribute) FROM Event WHERE attribute [comparison] [AND|OR ...] Copy Clause Notes SELECT function(attribute) Required Supported functions that return numbers include: apdex average count latest max min percentage percentile sum uniqueCount Tip If you use the percentile aggregator in a faceted alert condition with many facets, this may cause the following error to appear: An error occurred while fetching chart data. If you see this error, use average instead. FROM data type Required Only one data type can be targeted. Supported data types: Event Metric (RAW data points will be returned) WHERE attribute [comparison] [AND|OR ...] Use the WHERE clause to specify a series of one or more conditions. All the operators are supported. FACET attribute Required for outlier conditions Include an optional FACET clause in your NRQL syntax depending on the threshold type: static or baseline. Use the FACET clause to separate your results by attribute and alert on each attribute independently. No LIMIT clause is allowed, but all queries will receive the maximum number of facets possible. Faceted queries can return a maximum of 5000 values for static and baseline conditions and a maximum of 500 values for outlier conditions. Important If the query returns more than the maximum number of values, the alert condition can't be created. If you create the condition and the query returns more than this number later, the alert will fail. Modify your query so that it returns a fewer number of values. Reformatting incompatible NRQL Some elements of NRQL used in charts don’t make sense in the streaming context of alerts. Here’s a list of the most common incompatible elements and suggestions for reformatting a NRQL alert query to achieve the same effect. Element Notes SINCE and UNTIL Example: SELECT percentile(largestContentfulPaint, 75) FROM PageViewTiming WHERE (appId = 837807) SINCE yesterday Copy NRQL conditions produce a never-ending stream of windowed query results, so the SINCE and UNTIL keywords to scope the query to a point in time are not compatible. As a convenience, we automatically strip SINCE and UNTIL from a query when creating a condition from the context of a chart. TIMESERIES In NRQL queries, the TIMESERIES clause is used to return data as a time series broken out by a specified period of time. For NRQL conditions, the equivalent property of a signal is the aggregation duration window. histogram() The histogram() aggregation function is used to generate histograms. histogram() is not compatible with NRQL alerting: histogram aggregations can not be formatted as a time series. To create an alert from a portion of a histogram (for example, 95th percentile), use the percentile() aggregation function. Multiple aggregation functions Each condition can only target a single aggregated value. To alert on multiple values simultaneously, you’ll need to decompose them into individual conditions within the same policy. Original Query: SELECT count(foo), average(bar), max(baz) from Transaction Copy Decomposed: SELECT count(foo) from Transaction SELECT average(bar) from Transaction SELECT max(baz) from Transaction Copy COMPARE WITH The COMPARE WITH clause is used to compare the values for two different time ranges. This type of query is incompatible with NRQL alerting. We recommend using a Baseline Alert Condition to dynamically detect deviations for a particular signal. SLIDE BY The SLIDE BY clause supports a feature known as sliding windows. With sliding windows, SLIDE BY data is gathered into \"windows\" of time that overlap with each other. These windows can help to smooth out line graphs with a lot of variation in cases where the rolling aggregate (such as a rolling mean) is more important than aggregates from narrow windows of time. Sliding windows are not currently supported in NRQL alerts. LIMIT In NRQL queries, the LIMIT clause is used to control the amount of data a query returns, either the maximum number of facet values returned by FACET queries or the maximum number of items returned by SELECT * queries. LIMIT is not compatible with NRQL alerting: evaluation is always performed on the full result set. NRQL alert threshold examples Here are some common use cases for NRQL conditions. These queries will work for static and baseline condition types. The outlier condition type will require additional FACET clauses. Alert on specific segments of your data Create constrained alerts that target a specific segment of your data, such as a few key customers or a range of data. Use the WHERE clause to define those conditions. SELECT average(duration) FROM Transaction WHERE account_id in (91290, 102021, 20230) Copy SELECT percentile(duration, 95) FROM Transaction WHERE name LIKE 'Controller/checkout/%' Copy Alert on Nth percentile of your data Create alerts when an Nth percentile of your data hits a specified threshold; for example, maintaining SLA service levels. Since we evaluate the NRQL query based on the aggregation window duration, percentiles will be calculated for each duration separately. SELECT percentile(duration, 95) FROM Transaction Copy SELECT percentile(databaseDuration, 75) FROM Transaction Copy Alert on max, min, avg of your data Create alerts when your data hits a certain maximum, minimum, or average; for example, ensuring that a duration or response time does not pass a certain threshold. SELECT max(duration) FROM Transaction Copy SELECT average(duration) FROM Transaction Copy Alert on a percentage of your data Create alerts when a proportion of your data goes above or below a certain threshold. SELECT percentage(count(*), WHERE duration > 2) FROM Transaction Copy SELECT percentage(count(*), WHERE httpResponseCode = '500') FROM Transaction Copy Alert on Apdex with any T-value Create alerts on Apdex, applying your own T-value for certain transactions. For example, get an alert notification when your Apdex for a T-value of 500ms on transactions for production apps goes below 0.8. SELECT apdex(duration, t:0.5) FROM Transaction WHERE appName like '%prod%' Copy NRQL conditions and query order of operations By default, the aggregation window duration is 1 minute, but you can change the window to suit your needs. Whatever the aggregation window, New Relic will collect data for that window using the function in the NRQL condition’s query. The query is parsed and executed by our systems in the following order: FROM clause – which event type needs to be grabbed? WHERE clause – what can be filtered out? SELECT clause – what information needs to be returned from the now-filtered data set? Example: null value returned Let's say this is your alert condition query: SELECT count(*) FROM SyntheticCheck WHERE monitorName = 'My Cool Monitor' AND result = 'FAILURE' Copy If there are no failures for the aggregation window: The system will execute the FROM clause by grabbing all SyntheticCheck events on your account. Then it will execute the WHERE clause to filter through those events by looking only for the ones that match the monitor name and result specified. If there are still events left to scan through after completing the FROM and WHERE operations, the SELECT clause will be executed. If there are no remaining events, the SELECT clause will not be executed. This means that aggregators like count() and uniqueCount() will never return a zero value. When there is a count of 0, the SELECT clause is ignored and no data is returned, resulting in a value of NULL. Example: zero value returned If you have a data source delivering legitimate numeric zeroes, the query will return zero values and not null values. Let's say this is your alert condition query, and that MyCoolEvent is an attribute that can sometimes return a zero value. SELECT average(MyCoolAttribute) FROM MyCoolEvent Copy If, in the aggregation window being evaluated, there's at least one instance of MyCoolEvent and if the average value of all MyCoolAttribute attributes from that window is equal to zero, then a 0 value will be returned. If there are no MyCoolEvent events during that minute, then a NULL will be returned due to the order of operations. Tip For more information about this topic, you can check out our blog post on troubleshooting for zero versus null values. Tip You can determine how null values will be handled by adjusting loss of signal and gap filling settings in the Alert Conditions UI. Tip You can avoid NULL values entirely with a query order of operations shortcut. Do this by using a filter sub-clause, then including all filter elements within that sub-clause. The main body of the query will run and return data, at which point the SELECT clause will then run and apply the filter elements. The query will return a value of 0 if the filter elements result in no matching data. Here's an example: SELECT filter(count(*), WHERE result = 'SUCCESS' AND monitorName = 'My Favorite Monitor') FROM SyntheticCheck Copy Nested aggregation NRQL alerts Nested aggregation queries are a powerful way to query your data. However, they have a few restrictions that are important to note. Nested queries with a non-faceted innermost query are not currently supported Without a FACET, the inner query produces a single result, giving the outer query nothing to aggregate. If you're using a nested query, make sure your inner query is faceted. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu) ​​​​​ Copy Queries at all levels must have the same aggregation window size With an alert aggregation window of 1 minute, the inner query would produce two smaller windows of 30 seconds. In theory, these two windows could be aggregated by the outer query. However, this is not currently supported. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu TIMESERIES 30 seconds)​​ Copy Signal loss is not yet supported for nested queries For more information on signal loss, see NerdGraph API: Loss of signal and gap filling. Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported You can't use a nested query containing the WITH METRIC_FORMAT in the inner query to create NRQL alert conditions. NRQL condition creation tips Here are some tips for creating and using a NRQL condition: Topic Tips Condition types NRQL condition types include static, baseline, and outlier. Create a description For NRQL conditions, you can create a custom description to add to each violation. Descriptions can be enhanced with variable substitution based on metadata in the specific violation. For details, see Description Query results Queries must return a number. The condition evaluates the returned number against the thresholds you've set. Time period NRQL conditions evaluate data based on how it's aggregated, using aggregation windows from 30 seconds to 15 minutes, in increments of 15 seconds. For best results, we recommend using the event flow or event timer aggregation methods. For the cadence aggregation method, the implicit SINCE ... UNTIL clause specifying which minute to evaluate is controlled by your delay/timer setting. Since very recent data may be incomplete, you may want to query data from 3 minutes ago or longer, especially for: Applications that run on multiple hosts. SyntheticCheck data: Timeouts can take 3 minutes, so 5 minutes or more is recommended. Also, if a query will generate intermittent data, consider using the sum of query results option. Lost signal threshold (loss of signal detection) You can use loss of signal detection to alert on when your data (a telemetry signal) should be considered lost. A signal loss can indicate that a service or entity is no longer online or that a periodic job failed to run. You can also use this to make sure that violations for sporadic data, such as error counts, are closed when no signal is coming in. Advanced signal settings These settings give you options for better handling continuous, streaming data signals that may sometimes be missing. These settings include the aggregation window duration, the delay/timer, and an option for filling data gaps. For more on using these, see Advanced signal settings. Condition settings Use the Condition settings to: Create a concise, descriptive condition name. Provide a custom violation description for the condition that will be included in violations and notifications. Add the runbook URL to include your organization's procedures for handling incidents. You may also add this information to the custom violation description. Limits on conditions See the maximum values. Health status In order for a NRQL alert condition health status display to function properly, use a FACET clause to scope each signal to a single entity (for example, FACET hostname or FACET appname). Examples For more information, see: Expected NRQL syntax Examples of NRQL condition queries Condition edits can reset condition evaluation When you edit NRQL alert conditions in some specific ways (detailed below), their evaluations are reset, meaning that any evaluation up until that point is lost, and the evaluation starts over from that point. The two ways this will affect you are: For \"for at least x minutes\" thresholds: because the evaluation window has been reset, there will be a delay of at least x minutes before any violations can be reported. For baseline conditions: the condition starts over again and all baseline learning is lost. The following actions cause an evaluation reset for NRQL conditions: Changing the query Changing the aggregation window, aggregation method, or aggregation delay/timer setting Changing the \"close violations on signal loss\" setting Changing any gap fill settings Changing the baseline direction (if applicable) – higher, lower, or higher/lower Change the threshold value, threshold window, or threshold operator The following actions (along with any other actions not covered in the above list) will not reset the evaluation: Changing the loss of signal time window (expiration duration) Changing the time function (switching \"for at least\" to \"at least once in,\" or vice-versa) Toggling the \"open violation on signal loss\" setting Alert condition types When you create a NRQL alert, you can choose from different types of conditions: NRQL alert condition types Description Static This is the simplest type of NRQL condition. It allows you to create a condition based on a NRQL query that returns a numeric value. Optional: Include a FACET clause. Baseline (Dynamic) Uses a self-adjusting condition based on the past behavior of the monitored values. Uses the same NRQL query form as the static type, including the optional FACET clause. Outlier Looks for group behavior and values that are outliers from those groups. Uses the same NRQL query form as the static type, but requires a FACET clause. Sum of query results (limited or intermittent data) Important Available only for static (basic) condition types. If a query returns intermittent or limited data, it may be difficult to set a meaningful threshold. Missing or limited data will sometimes generate false positives or false negatives. You can use loss of signal, aggregation duration, and gap filling settings to minimize these false notifications. To avoid this problem when using the static threshold type, you can set the selector to sum of query results. This lets you set the alert on an aggregated sum instead of a value from a single harvest cycle. Up to two hours of one-minute data checks can be aggregated. The duration you select determines the width of the rolling sum and the preview chart will update accordingly. Set the loss of signal threshold Loss of signal occurs when no data matches the NRQL condition over a specific period of time. You can set your loss of signal threshold duration and also what happens when the threshold is crossed. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Loss of signal is only available for NRQL conditions. You may also manage these settings using the GraphQL API (recommended), or the REST API. Go here for specific GraphQL API examples. Loss of signal settings: Loss of signal settings include a time duration and two possible actions. Signal loss expiration time UI label: Signal is lost after: GraphQL Node: expiration.expirationDuration Expiration duration is a timer that starts and resets when we receive a data point in the streaming alerts pipeline. If we don't receive another data point before your 'expiration time' expires, we consider that signal to be lost. This can be because no data is being sent to New Relic or the WHERE clause of your NRQL query is filtering that data out before it is streamed to the alerts pipeline. Note that when you have a faceted query, each facet is a signal. So if any one of those signals ends during the duration specified, that will be considered a loss of signal. The loss of signal expiration time is independent of the threshold duration and triggers as soon as the timer expires. The maximum expiration duration is 48 hours. This is helpful when monitoring for the execution of infrequent jobs. The minimum is 30 seconds, but we recommend using at least 3-5 minutes. Loss of signal actions Once a signal is considered lost, you can close open violations, open new violations, or both. Close all current open violations: This closes all open violations that are related to a specific signal. It won't necessarily close all violations for a condition. If you're alerting on an ephemeral service, or on a sporadic signal, you'll want to choose this action to ensure that violations are closed properly. The GraphQL node name for this is \"closeViolationsOnExpiration\" Open new violations: This will open a new violation when the signal is considered lost. These violations will indicate that they are due to a loss of signal. Based on your incident preferences, this should trigger a notification. The graphQL node name for this is \"openViolationOnExpiration\" When you enable both actions, we'll close all open violations first, and then open a new violation for loss of signal. To create a NRQL alert configured with loss of signal detection in the UI: For a policy, when you create a condition, under Select a product, click NRQL, then click Next, define thresholds. Write a NRQL query that returns the values you want to alert on. For Threshold type, select Static or Baseline. Click + Add lost signal threshold, then set the signal expiration duration time in minutes or seconds in the Signal is lost after field. Choose what you want to happen when the signal is lost. You can check one or both of Close all current open violations and Open new \"lost signal\" violation. These control how loss of signal violations will be handled for the condition. Make sure you name your condition before you save it. Violations open due to loss of signal close when the signal comes back. Newly opened lost signal violations will close immediately when new data is evaluated. the condition they belong to expires. By default, conditions expire after 3 days. you manually close the violation with the Close all current open violations option. Tip Loss of signal detection doesn't work on NRQL queries that use nested aggregation or sub-queries. Advanced signal settings When creating a NRQL alert condition, use the advanced signal settings to control streaming alert data and avoid false alarms. When creating a NRQL condition, there are several advanced signal settings: Aggregation window duration Delay/timer Fill data gaps To read an explanation of what these settings are and how they relate to each other, see Streaming alerts concepts. Below are instructions and tips on how to configure them. Aggregation window duration You can set the aggregation window duration to choose how long data is accumulated in a streaming time window before it's aggregated. You can set it to anything between 30 seconds and 15 minutes. The default is one minute. Tip Baseline alert condition thresholds don't support editing the aggregation window. They use the 1 minute default. Delay/timer You can adjust the delay/timer to coordinate our streaming alerting algorithm with your data's behavior. If your data is sparse or inconsistent, you may want to use the event timer aggregation method. For the cadence method, the total supported latency is the sum of the aggregation window duration and the delay. If the data type comes from an APM language agent and is aggregated from many app instances (for example, Transactions, TransactionErrors, etc.), we recommend using the event flow method with the default settings. Important When creating NRQL conditions for data collected from Infrastructure Cloud Integrations such as AWS Cloudwatch or Azure, we recommend that you use the event timer method. Fill data gaps Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with one of these settings: None: (Default) Choose this if you don't want to take any action on empty aggregation windows. On evaluation, an empty aggregation window will reset the threshold duration timer. For example, if a condition says that all aggregation windows must have data points above the threshold for 5 minutes, and 1 of the 5 aggregation windows is empty, then the condition won't be in violation. Custom static value: Choose this if you'd like to insert a custom static value into the empty aggregation windows before they're evaluated. This option has an additional, required parameter of fillValue (as named in the API) that specifies what static value should be used. This defaults to 0. Last known value: This option inserts the last seen value before evaluation occurs. We maintain the state of the last seen value for 2 hours. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss, gap filling, and how to request access to these features, see this Explorers Hub post. Options for editing data gap settings: In the NRQL conditions UI, go to Condition settings > Advanced signal settings > fill data gaps with and choose an option. If using our Nerdgraph API (preferred), this node is located at: actor : account : alerts : nrqlCondition : signal : fillOption | fillValue NerdGraph is our recommended API for this but if you're using our REST API, you can find this setting in the REST API explorer under the \"signal\" section of the Alert NRQL conditions API.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.00891,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create NRQL <em>alert</em> conditions",
        "sections": "Nested aggregation NRQL <em>alerts</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": " a NRQL <em>alert</em> condition for a policy: On one.newrelic.com, in the header click <em>Alerts</em> &amp; AI, then in the left sidebar click Policies. Select an existing policy or click <em>New</em> <em>alert</em> policy to create a <em>new</em> policy. Click Add a condition. Under Select a product click NRQL, and then click Next, define"
      },
      "id": "603ef04864441fbc114e8883"
    }
  ],
  "/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/alerts-nerdgraph/nerdgraph-api-notification-channels": [
    {
      "sections": [
        "Streaming alerts: key terms and concepts",
        "Why it matters",
        "Streaming alerts process and descriptions",
        "Choose your aggregation method",
        "Event flow",
        "Caution",
        "Event timer",
        "Cadence",
        "Streaming alerts tools",
        "Tip",
        "Window duration",
        "Delay/timer",
        "Loss of signal detection",
        "Gap filling"
      ],
      "title": "Streaming alerts: key terms and concepts",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Get started"
      ],
      "external_id": "144c7f0721ec2acb1e8ac8aa06142911ea552aef",
      "image": "https://docs.newrelic.com/static/39158bd84483adf9516bb7ff5058c9bd/d30ee/streaming-alerts.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/streaming-alerts-key-terms-concepts/",
      "published_at": "2021-12-15T12:47:58Z",
      "updated_at": "2021-12-15T12:47:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The streaming alerts platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into New Relic. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that's processed by the streaming algorithm. There are three methods for aggregating the data filtered through your NRQL condition: Event flow (default) Event timer Cadence This short video describes the three aggregation method (5:31). Why it matters Understanding how streaming alerts works will help you fine-tune your NRQL conditions to be notified about what's important to you. Only data that matches the conditions of the NRQL WHERE clause is alerted on. For more details on each step of the process, see Streaming alerts process and descriptions. As data streams into New Relic, it's filtered by the NRQL condition. Before data is evaluated, it must meet the criteria defined by the NRQL query's WHERE clause. Instead of evaluating that data immediately for violations, the NRQL alert conditions collect the data over a period of time known as the aggregation window. An additional delay/timer allows for slower data points to arrive before the window is aggregated. Once the delay/timer time has elapsed, New Relic aggregates the data into a single data point. Alerts then evaluates the data point using the NRQL condition in order to determine whether it meets the violation threshold criteria. Even if a data point meets the criteria for a violation, a violation may not be triggered. A violation is only triggered when data points consistently meet the threshold criteria over a period of time. This is the threshold duration. If the data points are in violation for an entire threshold duration, we'll send you a notification based on your policy settings. All of these configurable delays give you more control over how you're alerted on sporadic and missing data. Streaming alerts process and descriptions Process Description Streaming data All data coming into New Relic. WHERE clause Filters all incoming streaming data. We only monitor for alerts on data that makes it through this filter. Aggregation methods One of three methods that control how data is collected before it's evaluated. They are: Event flow (Default) Event timer Cadence Aggregation window Data with timestamps that fall within this window will be aggregated and then evaluated. Delay/timer A time delay to ensure all data points have arrived in the aggregation window before aggregation occurs. Aggregated data Data in the aggregated window is collapsed to a single data point for alert evaluation. Evaluation The data point is evaluated by the NRQL condition, which is trigged by each incoming aggregated data point. Threshold duration A specific duration that determines if a violation is created. If your specified NRQL condition meets the threshold criteria over the threshold duration, a violation occurs. When a data point lacks data, a custom value is inserted to fill the gap. Choose your aggregation method You can choose between three different aggregation methods, depending on your needs. Event flow (default) works best for data that comes in frequently and mostly in order. Event timer works best for data that arrives infrequently in batches, such as cloud integration data or infrequent error logs. Cadence is our original and inferior aggregation method. It aggregates data on specific time intervals as detected by New Relic's internal wall clock, regardless of data timestamps. Event flow Event flow aggregates a window of data when the first data point arrives in a subsequent window. The custom delay defines which subsequent window data will start to populate to trigger aggregation of the current window. A custom delay provides extra time for data to arrive. These times are based on the data's timestamps and not New Relic's wall clock time. For example, suppose you're monitoring CPU usage in window durations of 1 minute and a 3 minute delay. When a CPU usage data point comes in with a timestamp between 12:00pm and 12:01pm, event flow will not aggregate that window until a data point shows up with a timestamp between 12:04pm and 12:05pm. When event flow receives the first data point with a timestamp of 12:04pm or later, it sends the 12:00 to 12:01 data to be aggregated. Caution If you expect your data points to arrive more than 30 minutes apart, please use the Event Timer method described below. Event timer Like event flow, event timer will only aggregate data for a given window when data arrives for that window. When a data point arrives for an aggregation window, a timer dedicated to that window starts to count down. If no further data arrives before the timer counts down, the data for that window is aggregated. If more data points arrive before the timer has completed counting down, the timer is reset. For example, suppose you're monitoring CloudWatch data that arrives fairly infrequently. You're using a window duration of 1 minute and a 3 minute timer. When a CloudWatch data point comes in with a timestamp between 12:00pm and 12:01pm, the timer will start to count down. If no further data points show up for that 12:00-12:01 window, the window will get aggregated 3 minutes later. If a new data point with a timestamp between 12:00 and 12:01 arrives, the timer resets. It keeps resetting every time more data points for that window arrive. The window will not be sent for aggregation until the timer reaches 0. If the timer for a later data point elapses before an earlier data point, the event timer method waits for the earlier timer to elapse before aggregating the later data point. For best results, make sure your timer is equal to or longer than your window duration time. If the timer is less than your window duration and your data flow is inconsistent, then your data may be evaluated before all of your data points arrive. This could cause you to be notified incorrectly. Cadence We recommend you use one of the other two methods. Cadence is our old streaming aggregation method. This method uses New Relic's wall clock time to determine when data is aggregated and evaluated. It doesn't take into account data point timestamps as they arrive. Streaming alerts tools Streaming alerts provide a set of tools to give you greater control over how your data is aggregated before it's evaluated to reduce incorrect notifications you receive. They are: Window duration Delay/timer Loss of signal detection Gap filling Tip This article covers these tools at a conceptual level. You'll find direct instructions on how to use these tools in Create NRQL alert conditions. Window duration In order to make loss of signal detection more effective and to reduce unnecessary notifications, you can customize aggregation windows to the duration that you need. An aggregation window is a specific block of time. We gather data points together in an aggregation window, before evaluating the data. A longer aggregation window can smooth out the data, since an outlier data point will have more data points to be aggregated with, giving it less of an influence on the aggregated data point that is sent for evaluation. When a data point arrives, its timestamp is used to put it in the proper aggregation window. You can set your aggregation window to anything between 30 seconds and 15 minutes. The default is 1 minute. Delay/timer The delay/timer setting controls how long the condition should wait before aggregating the data in the aggregation window. The event flow and cadence methods use delay. Event timer uses timer. The delay default is 2 minutes. The timer default is 1 minute and has a minimum value of 5 seconds. Loss of signal detection Loss of signal occurs when no data matches the NRQL condition over a specific period of time. A loss of signal is caused by different things. The WHERE clause in your NRQL query can filter out data before it's evaluated for violations. It could also mean a service or entity is offline or a periodic job has failed to run and no data is being sent to New Relic. In order to avoid unnecessary notifications, you can choose how long to wait before you're notified by a loss of signal violation. You can use loss of signal detection to open violations and be notified when a signal is lost. Alternately, you can use a loss of signal to close violations for ephemeral services or sporadic data, such as error counts. Gap filling Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with the last value received, a static value, or else do nothing and leave the gap there. The default is None. Gaps in streaming data can be caused by network or host issues, a signal may be sparse, or some signals, such as error counts, may only have data when something is wrong. By filling the gaps with known values, the alert evaluation process can process those gaps and determine how they should affect the loss of signal evaluation. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss and gap filling, see this Explorers Hub post.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.07315,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "sections": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "The streaming <em>alerts</em> platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into <em>New</em> <em>Relic</em>. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that&#x27;s processed"
      },
      "id": "6130c054e7b9d269edb6f21d"
    },
    {
      "sections": [
        "Create NRQL alert conditions",
        "Tip",
        "Create a NRQL alert condition",
        "Create a condition from a chart",
        "Caution",
        "NRQL alert syntax",
        "Important",
        "Reformatting incompatible NRQL",
        "NRQL alert threshold examples",
        "Alert on specific segments of your data",
        "Alert on Nth percentile of your data",
        "Alert on max, min, avg of your data",
        "Alert on a percentage of your data",
        "Alert on Apdex with any T-value",
        "NRQL conditions and query order of operations",
        "Example: null value returned",
        "Example: zero value returned",
        "Nested aggregation NRQL alerts",
        "Nested queries with a non-faceted innermost query are not currently supported",
        "Queries at all levels must have the same aggregation window size",
        "Signal loss is not yet supported for nested queries",
        "Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported",
        "NRQL condition creation tips",
        "Condition edits can reset condition evaluation",
        "Alert condition types",
        "Sum of query results (limited or intermittent data)",
        "Set the loss of signal threshold",
        "Advanced signal settings",
        "Aggregation window duration",
        "Delay/timer",
        "Fill data gaps"
      ],
      "title": "Create NRQL alert conditions",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert conditions"
      ],
      "external_id": "2872f770a5fc8a802f99b9f848906f7e351ad761",
      "image": "https://docs.newrelic.com/static/eb8e1b2d826f9fd9cf46fe3dd5455217/c1b63/nr1_nrql_alert_conditions.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/",
      "published_at": "2021-12-19T16:01:17Z",
      "updated_at": "2021-12-14T22:49:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use NRQL queries to create alert conditions. Once you've defined your signal, you can further define your warning and critical threshold levels. This determines when an alerts violation is created. Read on to learn more about how to do this. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Click NRQL, and then Next, define thresholds. Tip For more information on key concepts relating to NRQL alert conditions and streaming alerts, see Streaming alerts: key terms and concepts. Create a NRQL alert condition To create a NRQL alert condition for a policy: On one.newrelic.com, in the header click Alerts & AI, then in the left sidebar click Policies. Select an existing policy or click New alert policy to create a new policy. Click Add a condition. Under Select a product click NRQL, and then click Next, define thresholds. Note that editing an existing condition can result in resetting its evaluation. Create a condition from a chart You can use a chart to create a NRQL alert condition. To create a NRQL alerts condition from a chart, click the chart menu , then click Create alert condition. Once you've named and customized your condition, you can add it to an existing policy or create a new one. Caution A small number of our older charts don't include the option to create an alert condition. NRQL alert syntax Here's the basic syntax for creating all NRQL alert conditions. The FACET clause is required for outlier condition types. It's optional for static and baseline. SELECT function(attribute) FROM Event WHERE attribute [comparison] [AND|OR ...] Copy Clause Notes SELECT function(attribute) Required Supported functions that return numbers include: apdex average count latest max min percentage percentile sum uniqueCount Tip If you use the percentile aggregator in a faceted alert condition with many facets, this may cause the following error to appear: An error occurred while fetching chart data. If you see this error, use average instead. FROM data type Required Only one data type can be targeted. Supported data types: Event Metric (RAW data points will be returned) WHERE attribute [comparison] [AND|OR ...] Use the WHERE clause to specify a series of one or more conditions. All the operators are supported. FACET attribute Required for outlier conditions Include an optional FACET clause in your NRQL syntax depending on the threshold type: static or baseline. Use the FACET clause to separate your results by attribute and alert on each attribute independently. No LIMIT clause is allowed, but all queries will receive the maximum number of facets possible. Faceted queries can return a maximum of 5000 values for static and baseline conditions and a maximum of 500 values for outlier conditions. Important If the query returns more than the maximum number of values, the alert condition can't be created. If you create the condition and the query returns more than this number later, the alert will fail. Modify your query so that it returns a fewer number of values. Reformatting incompatible NRQL Some elements of NRQL used in charts don’t make sense in the streaming context of alerts. Here’s a list of the most common incompatible elements and suggestions for reformatting a NRQL alert query to achieve the same effect. Element Notes SINCE and UNTIL Example: SELECT percentile(largestContentfulPaint, 75) FROM PageViewTiming WHERE (appId = 837807) SINCE yesterday Copy NRQL conditions produce a never-ending stream of windowed query results, so the SINCE and UNTIL keywords to scope the query to a point in time are not compatible. As a convenience, we automatically strip SINCE and UNTIL from a query when creating a condition from the context of a chart. TIMESERIES In NRQL queries, the TIMESERIES clause is used to return data as a time series broken out by a specified period of time. For NRQL conditions, the equivalent property of a signal is the aggregation duration window. histogram() The histogram() aggregation function is used to generate histograms. histogram() is not compatible with NRQL alerting: histogram aggregations can not be formatted as a time series. To create an alert from a portion of a histogram (for example, 95th percentile), use the percentile() aggregation function. Multiple aggregation functions Each condition can only target a single aggregated value. To alert on multiple values simultaneously, you’ll need to decompose them into individual conditions within the same policy. Original Query: SELECT count(foo), average(bar), max(baz) from Transaction Copy Decomposed: SELECT count(foo) from Transaction SELECT average(bar) from Transaction SELECT max(baz) from Transaction Copy COMPARE WITH The COMPARE WITH clause is used to compare the values for two different time ranges. This type of query is incompatible with NRQL alerting. We recommend using a Baseline Alert Condition to dynamically detect deviations for a particular signal. SLIDE BY The SLIDE BY clause supports a feature known as sliding windows. With sliding windows, SLIDE BY data is gathered into \"windows\" of time that overlap with each other. These windows can help to smooth out line graphs with a lot of variation in cases where the rolling aggregate (such as a rolling mean) is more important than aggregates from narrow windows of time. Sliding windows are not currently supported in NRQL alerts. LIMIT In NRQL queries, the LIMIT clause is used to control the amount of data a query returns, either the maximum number of facet values returned by FACET queries or the maximum number of items returned by SELECT * queries. LIMIT is not compatible with NRQL alerting: evaluation is always performed on the full result set. NRQL alert threshold examples Here are some common use cases for NRQL conditions. These queries will work for static and baseline condition types. The outlier condition type will require additional FACET clauses. Alert on specific segments of your data Create constrained alerts that target a specific segment of your data, such as a few key customers or a range of data. Use the WHERE clause to define those conditions. SELECT average(duration) FROM Transaction WHERE account_id in (91290, 102021, 20230) Copy SELECT percentile(duration, 95) FROM Transaction WHERE name LIKE 'Controller/checkout/%' Copy Alert on Nth percentile of your data Create alerts when an Nth percentile of your data hits a specified threshold; for example, maintaining SLA service levels. Since we evaluate the NRQL query based on the aggregation window duration, percentiles will be calculated for each duration separately. SELECT percentile(duration, 95) FROM Transaction Copy SELECT percentile(databaseDuration, 75) FROM Transaction Copy Alert on max, min, avg of your data Create alerts when your data hits a certain maximum, minimum, or average; for example, ensuring that a duration or response time does not pass a certain threshold. SELECT max(duration) FROM Transaction Copy SELECT average(duration) FROM Transaction Copy Alert on a percentage of your data Create alerts when a proportion of your data goes above or below a certain threshold. SELECT percentage(count(*), WHERE duration > 2) FROM Transaction Copy SELECT percentage(count(*), WHERE httpResponseCode = '500') FROM Transaction Copy Alert on Apdex with any T-value Create alerts on Apdex, applying your own T-value for certain transactions. For example, get an alert notification when your Apdex for a T-value of 500ms on transactions for production apps goes below 0.8. SELECT apdex(duration, t:0.5) FROM Transaction WHERE appName like '%prod%' Copy NRQL conditions and query order of operations By default, the aggregation window duration is 1 minute, but you can change the window to suit your needs. Whatever the aggregation window, New Relic will collect data for that window using the function in the NRQL condition’s query. The query is parsed and executed by our systems in the following order: FROM clause – which event type needs to be grabbed? WHERE clause – what can be filtered out? SELECT clause – what information needs to be returned from the now-filtered data set? Example: null value returned Let's say this is your alert condition query: SELECT count(*) FROM SyntheticCheck WHERE monitorName = 'My Cool Monitor' AND result = 'FAILURE' Copy If there are no failures for the aggregation window: The system will execute the FROM clause by grabbing all SyntheticCheck events on your account. Then it will execute the WHERE clause to filter through those events by looking only for the ones that match the monitor name and result specified. If there are still events left to scan through after completing the FROM and WHERE operations, the SELECT clause will be executed. If there are no remaining events, the SELECT clause will not be executed. This means that aggregators like count() and uniqueCount() will never return a zero value. When there is a count of 0, the SELECT clause is ignored and no data is returned, resulting in a value of NULL. Example: zero value returned If you have a data source delivering legitimate numeric zeroes, the query will return zero values and not null values. Let's say this is your alert condition query, and that MyCoolEvent is an attribute that can sometimes return a zero value. SELECT average(MyCoolAttribute) FROM MyCoolEvent Copy If, in the aggregation window being evaluated, there's at least one instance of MyCoolEvent and if the average value of all MyCoolAttribute attributes from that window is equal to zero, then a 0 value will be returned. If there are no MyCoolEvent events during that minute, then a NULL will be returned due to the order of operations. Tip For more information about this topic, you can check out our blog post on troubleshooting for zero versus null values. Tip You can determine how null values will be handled by adjusting loss of signal and gap filling settings in the Alert Conditions UI. Tip You can avoid NULL values entirely with a query order of operations shortcut. Do this by using a filter sub-clause, then including all filter elements within that sub-clause. The main body of the query will run and return data, at which point the SELECT clause will then run and apply the filter elements. The query will return a value of 0 if the filter elements result in no matching data. Here's an example: SELECT filter(count(*), WHERE result = 'SUCCESS' AND monitorName = 'My Favorite Monitor') FROM SyntheticCheck Copy Nested aggregation NRQL alerts Nested aggregation queries are a powerful way to query your data. However, they have a few restrictions that are important to note. Nested queries with a non-faceted innermost query are not currently supported Without a FACET, the inner query produces a single result, giving the outer query nothing to aggregate. If you're using a nested query, make sure your inner query is faceted. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu) ​​​​​ Copy Queries at all levels must have the same aggregation window size With an alert aggregation window of 1 minute, the inner query would produce two smaller windows of 30 seconds. In theory, these two windows could be aggregated by the outer query. However, this is not currently supported. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu TIMESERIES 30 seconds)​​ Copy Signal loss is not yet supported for nested queries For more information on signal loss, see NerdGraph API: Loss of signal and gap filling. Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported You can't use a nested query containing the WITH METRIC_FORMAT in the inner query to create NRQL alert conditions. NRQL condition creation tips Here are some tips for creating and using a NRQL condition: Topic Tips Condition types NRQL condition types include static, baseline, and outlier. Create a description For NRQL conditions, you can create a custom description to add to each violation. Descriptions can be enhanced with variable substitution based on metadata in the specific violation. For details, see Description Query results Queries must return a number. The condition evaluates the returned number against the thresholds you've set. Time period NRQL conditions evaluate data based on how it's aggregated, using aggregation windows from 30 seconds to 15 minutes, in increments of 15 seconds. For best results, we recommend using the event flow or event timer aggregation methods. For the cadence aggregation method, the implicit SINCE ... UNTIL clause specifying which minute to evaluate is controlled by your delay/timer setting. Since very recent data may be incomplete, you may want to query data from 3 minutes ago or longer, especially for: Applications that run on multiple hosts. SyntheticCheck data: Timeouts can take 3 minutes, so 5 minutes or more is recommended. Also, if a query will generate intermittent data, consider using the sum of query results option. Lost signal threshold (loss of signal detection) You can use loss of signal detection to alert on when your data (a telemetry signal) should be considered lost. A signal loss can indicate that a service or entity is no longer online or that a periodic job failed to run. You can also use this to make sure that violations for sporadic data, such as error counts, are closed when no signal is coming in. Advanced signal settings These settings give you options for better handling continuous, streaming data signals that may sometimes be missing. These settings include the aggregation window duration, the delay/timer, and an option for filling data gaps. For more on using these, see Advanced signal settings. Condition settings Use the Condition settings to: Create a concise, descriptive condition name. Provide a custom violation description for the condition that will be included in violations and notifications. Add the runbook URL to include your organization's procedures for handling incidents. You may also add this information to the custom violation description. Limits on conditions See the maximum values. Health status In order for a NRQL alert condition health status display to function properly, use a FACET clause to scope each signal to a single entity (for example, FACET hostname or FACET appname). Examples For more information, see: Expected NRQL syntax Examples of NRQL condition queries Condition edits can reset condition evaluation When you edit NRQL alert conditions in some specific ways (detailed below), their evaluations are reset, meaning that any evaluation up until that point is lost, and the evaluation starts over from that point. The two ways this will affect you are: For \"for at least x minutes\" thresholds: because the evaluation window has been reset, there will be a delay of at least x minutes before any violations can be reported. For baseline conditions: the condition starts over again and all baseline learning is lost. The following actions cause an evaluation reset for NRQL conditions: Changing the query Changing the aggregation window, aggregation method, or aggregation delay/timer setting Changing the \"close violations on signal loss\" setting Changing any gap fill settings Changing the baseline direction (if applicable) – higher, lower, or higher/lower Change the threshold value, threshold window, or threshold operator The following actions (along with any other actions not covered in the above list) will not reset the evaluation: Changing the loss of signal time window (expiration duration) Changing the time function (switching \"for at least\" to \"at least once in,\" or vice-versa) Toggling the \"open violation on signal loss\" setting Alert condition types When you create a NRQL alert, you can choose from different types of conditions: NRQL alert condition types Description Static This is the simplest type of NRQL condition. It allows you to create a condition based on a NRQL query that returns a numeric value. Optional: Include a FACET clause. Baseline (Dynamic) Uses a self-adjusting condition based on the past behavior of the monitored values. Uses the same NRQL query form as the static type, including the optional FACET clause. Outlier Looks for group behavior and values that are outliers from those groups. Uses the same NRQL query form as the static type, but requires a FACET clause. Sum of query results (limited or intermittent data) Important Available only for static (basic) condition types. If a query returns intermittent or limited data, it may be difficult to set a meaningful threshold. Missing or limited data will sometimes generate false positives or false negatives. You can use loss of signal, aggregation duration, and gap filling settings to minimize these false notifications. To avoid this problem when using the static threshold type, you can set the selector to sum of query results. This lets you set the alert on an aggregated sum instead of a value from a single harvest cycle. Up to two hours of one-minute data checks can be aggregated. The duration you select determines the width of the rolling sum and the preview chart will update accordingly. Set the loss of signal threshold Loss of signal occurs when no data matches the NRQL condition over a specific period of time. You can set your loss of signal threshold duration and also what happens when the threshold is crossed. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Loss of signal is only available for NRQL conditions. You may also manage these settings using the GraphQL API (recommended), or the REST API. Go here for specific GraphQL API examples. Loss of signal settings: Loss of signal settings include a time duration and two possible actions. Signal loss expiration time UI label: Signal is lost after: GraphQL Node: expiration.expirationDuration Expiration duration is a timer that starts and resets when we receive a data point in the streaming alerts pipeline. If we don't receive another data point before your 'expiration time' expires, we consider that signal to be lost. This can be because no data is being sent to New Relic or the WHERE clause of your NRQL query is filtering that data out before it is streamed to the alerts pipeline. Note that when you have a faceted query, each facet is a signal. So if any one of those signals ends during the duration specified, that will be considered a loss of signal. The loss of signal expiration time is independent of the threshold duration and triggers as soon as the timer expires. The maximum expiration duration is 48 hours. This is helpful when monitoring for the execution of infrequent jobs. The minimum is 30 seconds, but we recommend using at least 3-5 minutes. Loss of signal actions Once a signal is considered lost, you can close open violations, open new violations, or both. Close all current open violations: This closes all open violations that are related to a specific signal. It won't necessarily close all violations for a condition. If you're alerting on an ephemeral service, or on a sporadic signal, you'll want to choose this action to ensure that violations are closed properly. The GraphQL node name for this is \"closeViolationsOnExpiration\" Open new violations: This will open a new violation when the signal is considered lost. These violations will indicate that they are due to a loss of signal. Based on your incident preferences, this should trigger a notification. The graphQL node name for this is \"openViolationOnExpiration\" When you enable both actions, we'll close all open violations first, and then open a new violation for loss of signal. To create a NRQL alert configured with loss of signal detection in the UI: For a policy, when you create a condition, under Select a product, click NRQL, then click Next, define thresholds. Write a NRQL query that returns the values you want to alert on. For Threshold type, select Static or Baseline. Click + Add lost signal threshold, then set the signal expiration duration time in minutes or seconds in the Signal is lost after field. Choose what you want to happen when the signal is lost. You can check one or both of Close all current open violations and Open new \"lost signal\" violation. These control how loss of signal violations will be handled for the condition. Make sure you name your condition before you save it. Violations open due to loss of signal close when the signal comes back. Newly opened lost signal violations will close immediately when new data is evaluated. the condition they belong to expires. By default, conditions expire after 3 days. you manually close the violation with the Close all current open violations option. Tip Loss of signal detection doesn't work on NRQL queries that use nested aggregation or sub-queries. Advanced signal settings When creating a NRQL alert condition, use the advanced signal settings to control streaming alert data and avoid false alarms. When creating a NRQL condition, there are several advanced signal settings: Aggregation window duration Delay/timer Fill data gaps To read an explanation of what these settings are and how they relate to each other, see Streaming alerts concepts. Below are instructions and tips on how to configure them. Aggregation window duration You can set the aggregation window duration to choose how long data is accumulated in a streaming time window before it's aggregated. You can set it to anything between 30 seconds and 15 minutes. The default is one minute. Tip Baseline alert condition thresholds don't support editing the aggregation window. They use the 1 minute default. Delay/timer You can adjust the delay/timer to coordinate our streaming alerting algorithm with your data's behavior. If your data is sparse or inconsistent, you may want to use the event timer aggregation method. For the cadence method, the total supported latency is the sum of the aggregation window duration and the delay. If the data type comes from an APM language agent and is aggregated from many app instances (for example, Transactions, TransactionErrors, etc.), we recommend using the event flow method with the default settings. Important When creating NRQL conditions for data collected from Infrastructure Cloud Integrations such as AWS Cloudwatch or Azure, we recommend that you use the event timer method. Fill data gaps Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with one of these settings: None: (Default) Choose this if you don't want to take any action on empty aggregation windows. On evaluation, an empty aggregation window will reset the threshold duration timer. For example, if a condition says that all aggregation windows must have data points above the threshold for 5 minutes, and 1 of the 5 aggregation windows is empty, then the condition won't be in violation. Custom static value: Choose this if you'd like to insert a custom static value into the empty aggregation windows before they're evaluated. This option has an additional, required parameter of fillValue (as named in the API) that specifies what static value should be used. This defaults to 0. Last known value: This option inserts the last seen value before evaluation occurs. We maintain the state of the last seen value for 2 hours. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss, gap filling, and how to request access to these features, see this Explorers Hub post. Options for editing data gap settings: In the NRQL conditions UI, go to Condition settings > Advanced signal settings > fill data gaps with and choose an option. If using our Nerdgraph API (preferred), this node is located at: actor : account : alerts : nrqlCondition : signal : fillOption | fillValue NerdGraph is our recommended API for this but if you're using our REST API, you can find this setting in the REST API explorer under the \"signal\" section of the Alert NRQL conditions API.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.00883,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create NRQL <em>alert</em> conditions",
        "sections": "Nested aggregation NRQL <em>alerts</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": " for nested queries For more information on signal loss, see <em>NerdGraph</em> API: Loss of signal and gap filling. Nested queries containing &#x27;WITH METRIC_FORMAT&#x27; in the inner query are not currently supported You can&#x27;t use a nested query containing the WITH METRIC_FORMAT in the inner query to create NRQL <em>alert</em>"
      },
      "id": "603ef04864441fbc114e8883"
    },
    {
      "sections": [
        "NerdGraph tutorial: NRQL condition alerts",
        "Tip",
        "Steps to create a NRQL condition",
        "NRQL static condition",
        "NRQL baseline condition",
        "NRQL outlier condition",
        "Update a condition",
        "Update mutations",
        "List and filter NRQL conditions",
        "Use cursor pagination",
        "Request type-specific fields",
        "Filter NRQL conditions",
        "Singular NRQL condition queries",
        "Update the description",
        "Delete conditions"
      ],
      "title": "NerdGraph tutorial: NRQL condition alerts",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alerts and Nerdgraph"
      ],
      "external_id": "659649f52d58010ca215dada0648a74254d9d96c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/alerts-nerdgraph/nerdgraph-api-nrql-condition-alerts/",
      "published_at": "2021-12-15T12:46:31Z",
      "updated_at": "2021-10-13T02:49:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can manage alerts conditions using our GraphQL NerdGraph API. Here are some conditions queries and mutations you can develop in our NerdGraph API explorer. Tip See the NerdGraph introduction for help getting started with NerdGraph API explorer. This document covers the following: Steps to create a NRQL condition NRQL static condition NRQL baseline condition NRQL outlier condition Update a condition Update mutations List and filter NRQL conditions Singular NRQL condition queries Create a description Delete conditions Steps to create a NRQL condition Follow these steps: Decide which condition type you want to create (see NRQL Condition threshold types). Find your relevant policyID by doing one of the following: Use the NerdGraph policies API. Go to one.newrelic.com, in the top nav click Alerts & AI, then click Policies. Choose a policy. Find the ID under the policy name. Provide the appropriate mutation for your NRQL condition type and the relevant values. Tip The NerdGraph GraphiQL explorer is the best place to find up-to-date documentation about the per-field specifics of the NerdGraph NRQL Conditions API. For example, questions like \"What does the valueFunction field accept?\" are best answered with the inline NerdGraph documentation. NRQL static condition Here's an example of creating a static condition: mutation { alertsNrqlConditionStaticCreate(accountId: YOUR_ACCOUNT_ID, policyId: YOUR_POLICY_ID, condition: { name: \"Low Host Count - Catastrophic\" enabled: true nrql: { query: \"SELECT uniqueCount(host) FROM Transaction WHERE appName='my-app-name'\" } signal: { aggregationWindow: 60 aggregationMethod: EVENT_FLOW aggregationDelay: 120 } terms: { threshold: 2 thresholdOccurrences: AT_LEAST_ONCE thresholdDuration: 600 operator: BELOW priority: CRITICAL } valueFunction: SINGLE_VALUE violationTimeLimitSeconds: 86400 }) { id name } } Copy NRQL baseline condition Here's an example of creating a baseline condition: mutation { alertsNrqlConditionBaselineCreate(accountId: YOUR_ACCOUNT_ID, policyId: YOUR_POLICY_ID, condition: { name: \"Baseline Condition\" enabled: true baselineDirection: UPPER_ONLY nrql: { query: \"SELECT average(duration) FROM Transaction\" } signal: { aggregationWindow: 60 aggregationMethod: EVENT_FLOW aggregationDelay: 120 } terms: { threshold: 13 thresholdDuration: 180 thresholdOccurrences: ALL operator: ABOVE priority: CRITICAL } violationTimeLimitSeconds: 86400 }) { id name baselineDirection } } Copy NRQL outlier condition Here's an example of creating an outlier condition: mutation { alertsNrqlConditionOutlierCreate(accountId: YOUR_ACCOUNT_ID, policyId: YOUR_POLICY_ID, condition: { name: \"Outlier Condition\" enabled: true expectedGroups: 4 openViolationOnGroupOverlap: false nrql: { query: \"SELECT average(duration) FROM Transaction FACET httpResponseCode\" } signal: { aggregationWindow: 60 aggregationMethod: EVENT_FLOW aggregationDelay: 120 } terms: { threshold: 1 thresholdDuration: 300 thresholdOccurrences: ALL operator: ABOVE priority: CRITICAL } violationTimeLimitSeconds: 86400 }) { id name expectedGroups openViolationOnGroupOverlap } } Copy Update a condition Complete the following: Determine the type of your existing condition by requesting the type field in a nrqlConditionsSearch query like this: { actor { account(id: YOUR_ACCOUNT_ID) { alerts { nrqlConditionsSearch { nrqlConditions { id type } } } } } } Copy Tip The type returned is what you use for your update mutation. For example, if the type returned is STATIC, use alertsNrqlConditionStaticUpdate. If the type returned is BASELINE, use alertsNrqlConditionBaselineUpdate. If the type returned is OUTLIER, use alertsNrqlConditionOutlierUpdate. Provide the id of your condition to your relevant condition type mutation. Note that you can only update conditions of the relevant type. Only provide update mutations for the fields you want to update. Fields you don't provide in the update are not touched. Update mutations Only fields that you provide in the update are changed. In the following example, baselineDirection returns unchanged, but name is updated. mutation { alertsNrqlConditionBaselineUpdate(id: YOUR_CONDITION_ID, accountId: YOUR_ACCOUNT_ID, condition: { name: \"Your updated name\" }) { id name baselineDirection } } Copy List and filter NRQL conditions To list or filter your NRQL conditions, use the nrqlConditionsSearch query in NerdGraph. Use cursor pagination The basic of list functionality for NRQL conditions allows you to paginate through your NRQL conditions as well as request the total count of conditions per account. The nrqlConditionsSearch query utilizes cursor pagination to paginate through resources. The idea behind cursor pagination is that the client will request a cursor in a programmatic loop until the cursor comes back empty. An initial list response will look something like this: { actor { account(id: YOUR_ACCOUNT_ID) { alerts { nrqlConditionsSearch { nextCursor nrqlConditions { id name type } totalCount } } } } } Copy This example returns a JSON response like this: { \"data\": { \"actor\": { \"account\": { \"alerts\": { \"nrqlConditionsSearch\": { \"nextCursor\": \"WOwfJ4+TWm9QTFeKMGyg+w==:QqkI8S4+Wwnpno6z+uk8kQ==\", \"nrqlConditions\": [ { \"id\": \"4432\", \"name\": \"Baseline Condition\", \"type\": \"BASELINE\" }, { \"id\": \"443\", \"name\": \"A static condition\", \"type\": \"STATIC\" }, // more conditions here in reality ], \"totalCount\": 435 } } } } } } Copy In order to paginate through conditions in the response, have the client request the cursor to be returned until the nextCursor returns from the response as null: { actor { account(id: YOUR_ACCOUNT_ID) { alerts { nrqlConditionsSearch(cursor: \"WOwfJ4+TWm9QTFeKMGyg+w==:QqkI8S4+Wwnpno6z+uk8kQ==\", ) { nextCursor nrqlConditions { id name type } totalCount } } } } } Copy Request type-specific fields Certain fields are only available on specific NRQL condition types. The main reason that mutations are split between the different condition types is because they have minor differences between the fields they accept. For example, valueFunction is only relevant for static NRQL conditions and baselineDirection is only relevant on baseline NRQL conditions. But if these fields are only available on these certain condition types, how do we return them in a list of all of our condition types? The answer is a GraphQL convention known as inline fragments. Inline fragments allow you to access the data on a specific type of NRQL condition: { actor { account(id: YOUR_ACCOUNT_ID) { alerts { nrqlConditionsSearch { nrqlConditions { id name type ...on AlertsNrqlStaticCondition { valueFunction } ...on AlertsNrqlBaselineCondition { baselineDirection } ...on AlertsNrqlOutlierCondition { expectedGroups } } } } } } } Copy In the previous example query, we are asking GraphQL to do the hard work for us to determine which NRQL conditions are the correct type. So, when the returned type is a static condition, it will return the valueFunction in the object. When the returned type is a baseline condition, it will return baselineDirection instead, and when the type is an outlier condition, it will return expectedGroups. Here is an example response: { \"data\": { \"actor\": { \"account\": { \"alerts\": { \"nrqlConditionsSearch\": { \"nrqlConditions\": [ { \"baselineDirection\": \"UPPER_ONLY\", \"id\": \"342\", \"name\": \"My baseline condition\", \"type\": \"BASELINE\" }, { \"id\": \"553\", \"name\": \"My static condition\", \"type\": \"STATIC\", \"valueFunction\": \"SINGLE_VALUE\" }, { \"expectedGroups\": 4, \"id\": \"802\", \"name\": \"My outlier condition\", \"type\": \"OUTLIER\" } ] } } } } } } Copy Filter NRQL conditions You can filter NRQL conditions with the searchCriteria argument of the nrqlConditionsSearch query: Here's an example of filtering NRQL conditions with matching by name. This query returns NRQL conditions that match the provided name. Note that this match is case insensitive. { actor { account(id: YOUR_ACCOUNT_ID) { alerts { nrqlConditionsSearch(searchCriteria: { name: \"Baseline Condition\" }) { nrqlConditions { id name type } } } } } } Copy Singular NRQL condition queries You can use the NRQL condition API to query for a singular condition. Run the nrqlCondition query in the alerts namespace. Similar to type specific fields on the nrqlConditionSearch query, you can also use these inline fragments to request fields that are restricted to a NRQL condition type. { actor { account(id: YOUR_ACCOUNT_ID) { alerts { nrqlCondition(id: YOUR_CONDITION_ID) { id name ...on AlertsNrqlStaticCondition { valueFunction } } } } } } Copy Update the description This will walk you through the procedure to create a description for a NRQL alert condition. Get all the conditions for a policy: { actor { account(id: YOUR_ACCOUNT_ID) { alerts { nrqlConditions(policyId: YOUR_POLICY_ID) { nextCursor results { id name description enabled nrql { query } signal { aggregationWindow aggregationMethod aggregationDelay aggregationTimer } policyId runbookUrl terms { duration operator priority timeFunction threshold } type violationTimeLimitSeconds } } } } } } Copy Get the details for a single condition: { actor { account(id: YOUR_ACCOUNT_ID) { alerts { nrqlCondition(id: \"YOUR_CONDITION_ID\") { description id enabled name nrql { query } signal { aggregationWindow aggregationMethod aggregationDelay aggregationTimer } policyId runbookUrl terms { operator priority threshold thresholdDuration thresholdOccurrences } type violationTimeLimitSeconds } } } } } Copy Create a mutation with the description. Here's an empty mutation template: mutation { alertsNrqlConditionStaticUpdate(accountId: YOUR_ACCOUNT_ID, id: \"YOUR_CONDITION_ID\", condition: {description: \"\"}) { description } } Copy Here's an example mutation with an included example description: mutation { alertsNrqlConditionStaticUpdate(accountId: 123456, id: \"123456\", condition: { description: \"timestamp : {{timestamp}} \\n accountId : {{accountId}} \\n type : {{type}} \\n event : {{event}} \\n description : {{description}} \\n policyId : {{policyId}} \\n policyName: {{policyName}} \\n conditionName : {{conditionName}} \\n conditionId : {{conditionId}} \\n product : {{product}} \\n conditionType : {{conditionType}} \\n RunbookUrl : {{runbookUrl}} \\n nrqlQuery : {{nrqlQuery}} \\n nrqlEventType : {{nrqlEventType}} \\n targetID : {{targetId}} \\n targetName : {{targetName}} \\n commandLine : {{tag.commandLine}} \\n entityGuid : {{tag.entityGuid}} \\n entityName : {{tag.entityName}} \\n fullHostname : {{tag.fullHostname}} \\n instanceType : {{tag.instanceType}} \\n processDisplayName : {{tag.processDisplayName}}\"} ) { description } } Copy Delete conditions You can use the alertsConditionDelete mutation to delete any type of condition. You can only request the id field on a delete mutation; for example: mutation { alertsConditionDelete(accountId: YOUR_ACCOUNT_ID, id: YOUR_CONDITION_ID) { id } } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.14505,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>NerdGraph</em> tutorial: NRQL condition <em>alerts</em>",
        "sections": "<em>NerdGraph</em> tutorial: NRQL condition <em>alerts</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "You can manage <em>alerts</em> conditions using our <em>Graph</em>QL <em>NerdGraph</em> API. Here are some conditions queries and mutations you can develop in our <em>NerdGraph</em> API explorer. Tip See the <em>NerdGraph</em> introduction for help getting started with <em>NerdGraph</em> API explorer. This document covers the following: Steps"
      },
      "id": "6130bf6528ccbcb0d856a821"
    }
  ],
  "/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/alerts-nerdgraph/nerdgraph-api-nrql-condition-alerts": [
    {
      "sections": [
        "NerdGraph tutorial: Notification channels",
        "Tip",
        "Get notification channels",
        "List all notification channels for an account",
        "Paginate through notification channels with cursor pagination",
        "Find a specific notification channel by id",
        "List notification channels with their associated policies",
        "Create a notification channel",
        "Caution",
        "Create an email notification channel",
        "Create an OpsGenie notification channel",
        "Create a PagerDuty notification channel",
        "Create a Slack notification channel",
        "Create a VictorOps notification channel",
        "Create a Webhook notification channel",
        "Create an xMatters notification channel",
        "Update a notification channel",
        "Update an email notification channel",
        "Update an OpsGenie notification channel",
        "Update a PagerDuty notification channel",
        "Update a Slack notification channel",
        "Update a VictorOps notification channel",
        "Update a Webhook notification channel",
        "Update an xMatters notification channel",
        "Delete a notification channel",
        "Associate channels to a policy",
        "Dissociate a channel from a policy"
      ],
      "title": "NerdGraph tutorial: Notification channels",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alerts and NerdGraph"
      ],
      "external_id": "d62b37e04d0601fec177951123d72e13f57458a0",
      "image": "",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/alerts-nerdgraph/nerdgraph-api-notification-channels/",
      "published_at": "2021-12-15T12:45:54Z",
      "updated_at": "2021-11-15T05:31:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can manage your alert notification channels using our GraphQL NerdGraph API. Here are some queries and mutations you can develop in our NerdGraph API explorer. Tip See the NerdGraph introduction for help getting started with NerdGraph API explorer. Get notification channels The notificationChannels query allows you to paginate through all of your notification channels per account. You can also use the notificationChannel query to get a specific notification channel by its ID. Tip Note that certain secret fields (for example, passwords or API keys) are obfuscated in the returned fields. List all notification channels for an account This example returns every field for every notification channel on the supplied account ID, up to the page limit of 200. Note how we use inline fragments to refer to the specific fields on the concrete types implementing the AlertsNotificationChannel interface. { actor { account(id: YOUR_ACCOUNT_ID) { alerts { notificationChannels { channels { id name type ... on AlertsXMattersNotificationChannel { config { integrationUrl } } ... on AlertsWebhookNotificationChannel { config { baseUrl basicAuth { password username } customHttpHeaders { name value } customPayloadBody customPayloadType } } ... on AlertsVictorOpsNotificationChannel { config { key routeKey } } ... on AlertsUserNotificationChannel { config { userId } } ... on AlertsSlackNotificationChannel { config { teamChannel url } } ... on AlertsPagerDutyNotificationChannel { config { apiKey } } ... on AlertsOpsGenieNotificationChannel { config { apiKey dataCenterRegion recipients tags teams } } ... on AlertsHipChatNotificationChannel { config { authToken baseUrl roomId } } ... on AlertsEmailNotificationChannel { config { emails includeJson } } ... on AlertsCampfireNotificationChannel { config { room subdomain token } } } totalCount nextCursor } } } } } Copy Paginate through notification channels with cursor pagination If a given account's list of notification channels exceeds the 200 channel page limit, you can use the pagination cursor to retrieve additional pages. With cursor pagination, you continue to request additional pages using the nextCursor until that field returns empty in the response. An empty nextCursor signals that you have reached the end of the result set. Here's an example: { actor { account(id: YOUR_ACCOUNT_ID) { alerts { notificationChannels { channels { id name type } totalCount nextCursor } } } } } Copy The code above returns a set of results like this: { \"data\": { \"actor\": { \"account\": { \"alerts\": { \"notificationChannels\": { \"channels\": [ { \"id\": \"250\", \"name\": \"Channel 1\", \"type\": \"SLACK\" }, { \"id\": \"713\", \"name\": \"Channel 2\", \"type\": \"WEBHOOK\" }, // ... +198 more notification channels in reality ], \"nextCursor\": \"Wh4LK9JYzfACVlNkyvf7Rg==:I5VbSEpgx3UWNA5AOVsUPv4=\", \"totalCount\": 268 } } } } } } Copy In your next request, provide the cursor like so, updating each subsequent request to return the updated cursor, until the cursor is empty: { actor { account(id: YOUR_ACCOUNT_ID) { alerts { notificationChannels(cursor: \"Wh4LK9JYzfACVlNkyvf7Rg==:I5VbSEpgx3UWNA5AOVsUPv4=\") { channels { id name type } totalCount nextCursor } } } } } Copy Find a specific notification channel by id If you have a specific notification channel's ID, the API allows you to look it up directly. Note that because the specific channel is a concrete type implementing the AlertsNotificationChannel interface, you may need to specify certain fields using the ... on syntax for inline fragments. In this example, we are retrieving a Slack channel: { actor { account(id: YOUR_ACCOUNT_ID) { alerts { notificationChannel(id: YOUR_CHANNEL_ID) { id name type ... on AlertsSlackNotificationChannel { config { teamChannel url } } } } } } } Copy List notification channels with their associated policies This example returns the ID, name, and type for every notification channel on the supplied account ID, as well as a list of every policy that is associated with that channel. { actor { account(id: YOUR_ACCOUNT_ID) { alerts { notificationChannels { channels { id name type associatedPolicies { policies { id name } totalCount } } nextCursor totalCount } } } } } Copy Create a notification channel In order to create an alert notification channel, you need to know the specific type of notification channel you want to create (for example email, Slack, etc.), as well as the details necessary to configure it (which will depend on the channel type). Once a notification channel has been created, it can be associated with one or more alert policies. Once associated, those channels will receive notifications from those policies when conditions are violated. Caution While you can query for any existing notification channel type, you can only create a subset of them. Specifically, the user channel type has no editable fields, and the Campfire and HipChat channel types are both deprecated. Create an email notification channel An example create mutation for an email notification channel: mutation { alertsNotificationChannelCreate(accountId: YOUR_ACCOUNT_ID, notificationChannel: { email: { emails: [\"email@example.com\"], includeJson: true, name: \"Some Name <email@example.com>\" } }) { notificationChannel { ... on AlertsEmailNotificationChannel { id name type config { emails includeJson } } } error { description errorType } } } Copy Create an OpsGenie notification channel An example create mutation for an OpsGenie notification channel: mutation { alertsNotificationChannelCreate(accountId: YOUR_ACCOUNT_ID, notificationChannel: { opsGenie: { apiKey: \"api-key-from-opsgenie\", dataCenterRegion: US, name: \"OpsGenie notification channel name\", recipients: [\"user@example.com\"], tags: [\"tag1\", \"tag2\"], teams: [\"team1\", \"team2\"] } }) { notificationChannel { ... on AlertsOpsGenieNotificationChannel { id name type config { apiKey teams tags recipients dataCenterRegion } } } error { description errorType } } } Copy Create a PagerDuty notification channel An example create mutation for a PagerDuty notification channel: mutation { alertsNotificationChannelCreate(accountId: YOUR_ACCOUNT_ID, notificationChannel: { pagerDuty: { name: \"PagerDuty notification channel name\", apiKey: \"api-key-from-pagerduty\" } }) { notificationChannel { ... on AlertsPagerDutyNotificationChannel { id name type config { apiKey } } } error { description errorType } } } Copy Create a Slack notification channel An example create mutation for a Slack notification channel: mutation { alertsNotificationChannelCreate(accountId: YOUR_ACCOUNT_ID, notificationChannel: { slack: { name: \"Slack notification channel name\", teamChannel: \"#team-channel\", url: \"https://hooks.slack.com/services/FAKE/MOREFAKE/IMAGINARYEXAMPLEURLCHUNK\" } }) { notificationChannel { ... on AlertsSlackNotificationChannel { id name type config { teamChannel url } } } error { description errorType } } } Copy Create a VictorOps notification channel An example create mutation for a VictorOps notification channel: mutation { alertsNotificationChannelCreate(accountId: YOUR_ACCOUNT_ID, notificationChannel: { victorOps: { key: \"example-api-key-from-victorops\", name: \"VictorOps notification channel name\", routeKey: \"example-route-key\" } }) { notificationChannel { ... on AlertsVictorOpsNotificationChannel { id name type config { key routeKey } } } error { description errorType } } } Copy Create a Webhook notification channel An example create mutation for a Webhook notification channel: mutation { alertsNotificationChannelCreate(accountId: YOUR_ACCOUNT_ID, notificationChannel: { webhook: { baseUrl: \"https://example.com/webhook\", basicAuth: { password: \"t0t4lly-s3cr3t-p455w0rd\", username: \"webhook-user\" }, customHttpHeaders: [ {name: \"X-Api-Key\", value: \"100%-real-api-key\"}, {name: \"X-Calling-Service\", value: \"New Relic Alerts\"} ], customPayloadBody: \"{ \\\"account_id\\\": \\\"$ACCOUNT_ID\\\", \\\"account_name\\\": \\\"$ACCOUNT_NAME\\\", \\\"closed_violations_count_critical\\\": \\\"$CLOSED_VIOLATIONS_COUNT_CRITICAL\\\", \\\"closed_violations_count_warning\\\": \\\"$CLOSED_VIOLATIONS_COUNT_WARNING\\\", \\\"condition_description\\\": \\\"$DESCRIPTION\\\", \\\"condition_family_id\\\": \\\"$CONDITION_FAMILY_ID\\\", \\\"condition_name\\\": \\\"$CONDITION_NAME\\\", \\\"current_state\\\": \\\"$EVENT_STATE\\\", \\\"details\\\": \\\"$EVENT_DETAILS\\\", \\\"duration\\\": \\\"$DURATION\\\", \\\"event_type\\\": \\\"$EVENT_TYPE\\\", \\\"incident_acknowledge_url\\\": \\\"$INCIDENT_ACKNOWLEDGE_URL\\\", \\\"incident_id\\\": \\\"$INCIDENT_ID\\\", \\\"incident_url\\\": \\\"$INCIDENT_URL\\\", \\\"metadata\\\": \\\"$METADATA\\\", \\\"open_violations_count_critical\\\": \\\"$OPEN_VIOLATIONS_COUNT_CRITICAL\\\", \\\"open_violations_count_warning\\\": \\\"$OPEN_VIOLATIONS_COUNT_WARNING\\\", \\\"owner\\\": \\\"$EVENT_OWNER\\\", \\\"policy_name\\\": \\\"$POLICY_NAME\\\", \\\"policy_url\\\": \\\"$POLICY_URL\\\", \\\"runbook_url\\\": \\\"$RUNBOOK_URL\\\", \\\"severity\\\": \\\"$SEVERITY\\\", \\\"targets\\\": \\\"$TARGETS\\\", \\\"timestamp\\\": \\\"$TIMESTAMP\\\", \\\"timestamp_utc_string\\\": \\\"$TIMESTAMP_UTC_STRING\\\", \\\"violation_callback_url\\\": \\\"$VIOLATION_CALLBACK_URL\\\", \\\"violation_chart_url\\\": \\\"$VIOLATION_CHART_URL\\\" }\", customPayloadType: JSON, name: \"Webhook notification channel name\" } }) { notificationChannel { ... on AlertsWebhookNotificationChannel { id name type config { customPayloadType customPayloadBody customHttpHeaders { value name } basicAuth { password username } baseUrl } } } error { description errorType } } } Copy Create an xMatters notification channel An example create mutation for an xMatters notification channel: mutation { alertsNotificationChannelCreate(accountId: YOUR_ACCOUNT_ID, notificationChannel: { xMatters: { integrationUrl: \"https://company.instance.xmatters.com/api/xm/v<version>/...\", name: \"xMatters notification channel name\" } }) { notificationChannel { ... on AlertsXMattersNotificationChannel { id name type config { integrationUrl } } } error { description errorType } } } Copy Update a notification channel In order to update an alert notification channel, you need to know the specific type of notification channel you want to change (for example email, Slack, etc.), as well as the details necessary to configure it (which will depend on the channel type). Consistent with other GraphQL APIs, you can update a single field on the channel without knowing anything other than the channel's ID. Caution While you can query for any existing notification channel type, you can only update a subset of them. Specifically, the user channel type has no editable fields, and the Campfire and HipChat channel types are both deprecated. Update an email notification channel An example update mutation for an email notification channel where we're updating only the name: mutation { alertsNotificationChannelUpdate(accountId: YOUR_ACCOUNT_ID, id: YOUR_CHANNEL_ID, notificationChannel: { email: { name: \"Updated Name <email@example.com>\" } }) { notificationChannel { ... on AlertsEmailNotificationChannel { id name type } } error { description errorType notificationChannelId } } } Copy Update an OpsGenie notification channel An example update mutation for an OpsGenie notification channel where we're updating only the name: mutation { alertsNotificationChannelUpdate(accountId: YOUR_ACCOUNT_ID, id: YOUR_CHANNEL_ID, notificationChannel: { opsGenie: { name: \"OpsGenie updated channel name\" } }) { notificationChannel { ... on AlertsOpsGenieNotificationChannel { id name type } } error { description errorType notificationChannelId } } } Copy Update a PagerDuty notification channel An example update mutation for a PagerDuty notification channel where we're updating only the name: mutation { alertsNotificationChannelUpdate(accountId: YOUR_ACCOUNT_ID, id: YOUR_CHANNEL_ID, notificationChannel: { pagerDuty: { name: \"PagerDuty updated channel name\" } }) { notificationChannel { ... on AlertsPagerDutyNotificationChannel { id name type } } error { description errorType notificationChannelId } } } Copy Update a Slack notification channel An example update mutation for a Slack notification channel where we're updating only the name: mutation { alertsNotificationChannelUpdate(accountId: YOUR_ACCOUNT_ID, id: YOUR_CHANNEL_ID, notificationChannel: { slack: { name: \"Slack updated channel name\" } }) { notificationChannel { ... on AlertsSlackNotificationChannel { id name type } } error { description errorType notificationChannelId } } } Copy Update a VictorOps notification channel An example update mutation for a VictorOps notification channel where we're updating only the name: mutation { alertsNotificationChannelUpdate(accountId: YOUR_ACCOUNT_ID, id: YOUR_CHANNEL_ID, id: YOUR_CHANNEL_ID, notificationChannel: { victorOps: { name: \"VictorOps updated channel name\" } }) { notificationChannel { ... on AlertsVictorOpsNotificationChannel { id name type } } error { description errorType notificationChannelId } } } Copy Update a Webhook notification channel An example update mutation for a Webhook notification channel where we're updating only the name: mutation { alertsNotificationChannelUpdate(accountId: YOUR_ACCOUNT_ID, id: YOUR_CHANNEL_ID, notificationChannel: { webhook: { name: \"Webhook updated channel name\" } }) { notificationChannel { ... on AlertsWebhookNotificationChannel { id name type } } error { description errorType notificationChannelId } } } Copy Update an xMatters notification channel An example update mutation for an xMatters notification channel where we're updating only the name: mutation { alertsNotificationChannelUpdate(accountId: YOUR_ACCOUNT_ID, id: YOUR_CHANNEL_ID, notificationChannel: { xMatters: { name: \"xMatters updated channel name\" } }) { notificationChannel { ... on AlertsXMattersNotificationChannel { id name type } } error { description errorType notificationChannelId } } } Copy Delete a notification channel You can delete a notification channel with only the account ID and the channel ID. Note that deleting a channel dissociates it from all policies, meaning that no further notifications will be sent to that channel. mutation { alertsNotificationChannelDelete(accountId: YOUR_ACCOUNT_ID, id: YOUR_CHANNEL_ID) { id error { description errorType notificationChannelId } } } Copy Associate channels to a policy Creating an alert notification channel is not enough: Once the channel has been created, it needs to be associated to one or more policies. Once associated to a policy, the channel can recieve alert notifications when conditions on that policy go into violation. In this example, we associate two channels with a policy: mutation { alertsNotificationChannelsAddToPolicy(accountId: YOUR_ACCOUNT_ID, notificationChannelIds: [FIRST_CHANNEL_ID, SECOND_CHANNEL_ID], policyId: YOUR_POLICY_ID ) { notificationChannels { id } policyId errors { description errorType notificationChannelId } } } Copy Dissociate a channel from a policy In those instances where a notification channel has outlived its usefulness (for example, an email list that has been retired), the time has come to dissociate that channel from the policy (or policies) that are sending alert notifications to it. This API call leaves the channel itself intact, but removes it from the specified policy. In this example, we are removing two channels from a policy (leaving any others in place), and getting back confirmation that those two channel IDs have been removed: mutation { alertsNotificationChannelsRemoveFromPolicy(accountId: YOUR_ACCOUNT_ID, notificationChannelIds: [FIRST_CHANNEL_ID, SECOND_CHANNEL_ID], policyId: YOUR_POLICY_ID ) { notificationChannels { id } policyId errors { description errorType notificationChannelId } } } Copy Tip Removing an alert notification channel from a policy does not delete the channel because it might be used by other policies. On the other hand, deleting a channel will cause all associated policies to stop sending alert notifications to that channel.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.80412,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>NerdGraph</em> tutorial: Notification channels",
        "sections": "<em>NerdGraph</em> tutorial: Notification channels",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": ": &quot;<em>New</em> <em>Relic</em> <em>Alerts</em>&quot;} ], customPayloadBody: &quot;{ \\&quot;account_id\\&quot;: \\&quot;$ACCOUNT_ID\\&quot;, \\&quot;account_name\\&quot;: \\&quot;$ACCOUNT_NAME\\&quot;, \\&quot;closed_violations_count_critical\\&quot;: \\&quot;$CLOSED_VIOLATIONS_COUNT_CRITICAL\\&quot;, \\&quot;closed_violations_count_warning\\&quot;: \\&quot;$CLOSED_VIOLATIONS_COUNT_WARNING\\&quot;, \\&quot;condition_description"
      },
      "id": "6130bf9c28ccbc027d56a863"
    },
    {
      "sections": [
        "Streaming alerts: key terms and concepts",
        "Why it matters",
        "Streaming alerts process and descriptions",
        "Choose your aggregation method",
        "Event flow",
        "Caution",
        "Event timer",
        "Cadence",
        "Streaming alerts tools",
        "Tip",
        "Window duration",
        "Delay/timer",
        "Loss of signal detection",
        "Gap filling"
      ],
      "title": "Streaming alerts: key terms and concepts",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Get started"
      ],
      "external_id": "144c7f0721ec2acb1e8ac8aa06142911ea552aef",
      "image": "https://docs.newrelic.com/static/39158bd84483adf9516bb7ff5058c9bd/d30ee/streaming-alerts.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/streaming-alerts-key-terms-concepts/",
      "published_at": "2021-12-15T12:47:58Z",
      "updated_at": "2021-12-15T12:47:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The streaming alerts platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into New Relic. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that's processed by the streaming algorithm. There are three methods for aggregating the data filtered through your NRQL condition: Event flow (default) Event timer Cadence This short video describes the three aggregation method (5:31). Why it matters Understanding how streaming alerts works will help you fine-tune your NRQL conditions to be notified about what's important to you. Only data that matches the conditions of the NRQL WHERE clause is alerted on. For more details on each step of the process, see Streaming alerts process and descriptions. As data streams into New Relic, it's filtered by the NRQL condition. Before data is evaluated, it must meet the criteria defined by the NRQL query's WHERE clause. Instead of evaluating that data immediately for violations, the NRQL alert conditions collect the data over a period of time known as the aggregation window. An additional delay/timer allows for slower data points to arrive before the window is aggregated. Once the delay/timer time has elapsed, New Relic aggregates the data into a single data point. Alerts then evaluates the data point using the NRQL condition in order to determine whether it meets the violation threshold criteria. Even if a data point meets the criteria for a violation, a violation may not be triggered. A violation is only triggered when data points consistently meet the threshold criteria over a period of time. This is the threshold duration. If the data points are in violation for an entire threshold duration, we'll send you a notification based on your policy settings. All of these configurable delays give you more control over how you're alerted on sporadic and missing data. Streaming alerts process and descriptions Process Description Streaming data All data coming into New Relic. WHERE clause Filters all incoming streaming data. We only monitor for alerts on data that makes it through this filter. Aggregation methods One of three methods that control how data is collected before it's evaluated. They are: Event flow (Default) Event timer Cadence Aggregation window Data with timestamps that fall within this window will be aggregated and then evaluated. Delay/timer A time delay to ensure all data points have arrived in the aggregation window before aggregation occurs. Aggregated data Data in the aggregated window is collapsed to a single data point for alert evaluation. Evaluation The data point is evaluated by the NRQL condition, which is trigged by each incoming aggregated data point. Threshold duration A specific duration that determines if a violation is created. If your specified NRQL condition meets the threshold criteria over the threshold duration, a violation occurs. When a data point lacks data, a custom value is inserted to fill the gap. Choose your aggregation method You can choose between three different aggregation methods, depending on your needs. Event flow (default) works best for data that comes in frequently and mostly in order. Event timer works best for data that arrives infrequently in batches, such as cloud integration data or infrequent error logs. Cadence is our original and inferior aggregation method. It aggregates data on specific time intervals as detected by New Relic's internal wall clock, regardless of data timestamps. Event flow Event flow aggregates a window of data when the first data point arrives in a subsequent window. The custom delay defines which subsequent window data will start to populate to trigger aggregation of the current window. A custom delay provides extra time for data to arrive. These times are based on the data's timestamps and not New Relic's wall clock time. For example, suppose you're monitoring CPU usage in window durations of 1 minute and a 3 minute delay. When a CPU usage data point comes in with a timestamp between 12:00pm and 12:01pm, event flow will not aggregate that window until a data point shows up with a timestamp between 12:04pm and 12:05pm. When event flow receives the first data point with a timestamp of 12:04pm or later, it sends the 12:00 to 12:01 data to be aggregated. Caution If you expect your data points to arrive more than 30 minutes apart, please use the Event Timer method described below. Event timer Like event flow, event timer will only aggregate data for a given window when data arrives for that window. When a data point arrives for an aggregation window, a timer dedicated to that window starts to count down. If no further data arrives before the timer counts down, the data for that window is aggregated. If more data points arrive before the timer has completed counting down, the timer is reset. For example, suppose you're monitoring CloudWatch data that arrives fairly infrequently. You're using a window duration of 1 minute and a 3 minute timer. When a CloudWatch data point comes in with a timestamp between 12:00pm and 12:01pm, the timer will start to count down. If no further data points show up for that 12:00-12:01 window, the window will get aggregated 3 minutes later. If a new data point with a timestamp between 12:00 and 12:01 arrives, the timer resets. It keeps resetting every time more data points for that window arrive. The window will not be sent for aggregation until the timer reaches 0. If the timer for a later data point elapses before an earlier data point, the event timer method waits for the earlier timer to elapse before aggregating the later data point. For best results, make sure your timer is equal to or longer than your window duration time. If the timer is less than your window duration and your data flow is inconsistent, then your data may be evaluated before all of your data points arrive. This could cause you to be notified incorrectly. Cadence We recommend you use one of the other two methods. Cadence is our old streaming aggregation method. This method uses New Relic's wall clock time to determine when data is aggregated and evaluated. It doesn't take into account data point timestamps as they arrive. Streaming alerts tools Streaming alerts provide a set of tools to give you greater control over how your data is aggregated before it's evaluated to reduce incorrect notifications you receive. They are: Window duration Delay/timer Loss of signal detection Gap filling Tip This article covers these tools at a conceptual level. You'll find direct instructions on how to use these tools in Create NRQL alert conditions. Window duration In order to make loss of signal detection more effective and to reduce unnecessary notifications, you can customize aggregation windows to the duration that you need. An aggregation window is a specific block of time. We gather data points together in an aggregation window, before evaluating the data. A longer aggregation window can smooth out the data, since an outlier data point will have more data points to be aggregated with, giving it less of an influence on the aggregated data point that is sent for evaluation. When a data point arrives, its timestamp is used to put it in the proper aggregation window. You can set your aggregation window to anything between 30 seconds and 15 minutes. The default is 1 minute. Delay/timer The delay/timer setting controls how long the condition should wait before aggregating the data in the aggregation window. The event flow and cadence methods use delay. Event timer uses timer. The delay default is 2 minutes. The timer default is 1 minute and has a minimum value of 5 seconds. Loss of signal detection Loss of signal occurs when no data matches the NRQL condition over a specific period of time. A loss of signal is caused by different things. The WHERE clause in your NRQL query can filter out data before it's evaluated for violations. It could also mean a service or entity is offline or a periodic job has failed to run and no data is being sent to New Relic. In order to avoid unnecessary notifications, you can choose how long to wait before you're notified by a loss of signal violation. You can use loss of signal detection to open violations and be notified when a signal is lost. Alternately, you can use a loss of signal to close violations for ephemeral services or sporadic data, such as error counts. Gap filling Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with the last value received, a static value, or else do nothing and leave the gap there. The default is None. Gaps in streaming data can be caused by network or host issues, a signal may be sparse, or some signals, such as error counts, may only have data when something is wrong. By filling the gaps with known values, the alert evaluation process can process those gaps and determine how they should affect the loss of signal evaluation. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss and gap filling, see this Explorers Hub post.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.07315,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "sections": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "The streaming <em>alerts</em> platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into <em>New</em> <em>Relic</em>. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that&#x27;s processed"
      },
      "id": "6130c054e7b9d269edb6f21d"
    },
    {
      "sections": [
        "Create NRQL alert conditions",
        "Tip",
        "Create a NRQL alert condition",
        "Create a condition from a chart",
        "Caution",
        "NRQL alert syntax",
        "Important",
        "Reformatting incompatible NRQL",
        "NRQL alert threshold examples",
        "Alert on specific segments of your data",
        "Alert on Nth percentile of your data",
        "Alert on max, min, avg of your data",
        "Alert on a percentage of your data",
        "Alert on Apdex with any T-value",
        "NRQL conditions and query order of operations",
        "Example: null value returned",
        "Example: zero value returned",
        "Nested aggregation NRQL alerts",
        "Nested queries with a non-faceted innermost query are not currently supported",
        "Queries at all levels must have the same aggregation window size",
        "Signal loss is not yet supported for nested queries",
        "Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported",
        "NRQL condition creation tips",
        "Condition edits can reset condition evaluation",
        "Alert condition types",
        "Sum of query results (limited or intermittent data)",
        "Set the loss of signal threshold",
        "Advanced signal settings",
        "Aggregation window duration",
        "Delay/timer",
        "Fill data gaps"
      ],
      "title": "Create NRQL alert conditions",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert conditions"
      ],
      "external_id": "2872f770a5fc8a802f99b9f848906f7e351ad761",
      "image": "https://docs.newrelic.com/static/eb8e1b2d826f9fd9cf46fe3dd5455217/c1b63/nr1_nrql_alert_conditions.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/",
      "published_at": "2021-12-19T16:01:17Z",
      "updated_at": "2021-12-14T22:49:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use NRQL queries to create alert conditions. Once you've defined your signal, you can further define your warning and critical threshold levels. This determines when an alerts violation is created. Read on to learn more about how to do this. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Click NRQL, and then Next, define thresholds. Tip For more information on key concepts relating to NRQL alert conditions and streaming alerts, see Streaming alerts: key terms and concepts. Create a NRQL alert condition To create a NRQL alert condition for a policy: On one.newrelic.com, in the header click Alerts & AI, then in the left sidebar click Policies. Select an existing policy or click New alert policy to create a new policy. Click Add a condition. Under Select a product click NRQL, and then click Next, define thresholds. Note that editing an existing condition can result in resetting its evaluation. Create a condition from a chart You can use a chart to create a NRQL alert condition. To create a NRQL alerts condition from a chart, click the chart menu , then click Create alert condition. Once you've named and customized your condition, you can add it to an existing policy or create a new one. Caution A small number of our older charts don't include the option to create an alert condition. NRQL alert syntax Here's the basic syntax for creating all NRQL alert conditions. The FACET clause is required for outlier condition types. It's optional for static and baseline. SELECT function(attribute) FROM Event WHERE attribute [comparison] [AND|OR ...] Copy Clause Notes SELECT function(attribute) Required Supported functions that return numbers include: apdex average count latest max min percentage percentile sum uniqueCount Tip If you use the percentile aggregator in a faceted alert condition with many facets, this may cause the following error to appear: An error occurred while fetching chart data. If you see this error, use average instead. FROM data type Required Only one data type can be targeted. Supported data types: Event Metric (RAW data points will be returned) WHERE attribute [comparison] [AND|OR ...] Use the WHERE clause to specify a series of one or more conditions. All the operators are supported. FACET attribute Required for outlier conditions Include an optional FACET clause in your NRQL syntax depending on the threshold type: static or baseline. Use the FACET clause to separate your results by attribute and alert on each attribute independently. No LIMIT clause is allowed, but all queries will receive the maximum number of facets possible. Faceted queries can return a maximum of 5000 values for static and baseline conditions and a maximum of 500 values for outlier conditions. Important If the query returns more than the maximum number of values, the alert condition can't be created. If you create the condition and the query returns more than this number later, the alert will fail. Modify your query so that it returns a fewer number of values. Reformatting incompatible NRQL Some elements of NRQL used in charts don’t make sense in the streaming context of alerts. Here’s a list of the most common incompatible elements and suggestions for reformatting a NRQL alert query to achieve the same effect. Element Notes SINCE and UNTIL Example: SELECT percentile(largestContentfulPaint, 75) FROM PageViewTiming WHERE (appId = 837807) SINCE yesterday Copy NRQL conditions produce a never-ending stream of windowed query results, so the SINCE and UNTIL keywords to scope the query to a point in time are not compatible. As a convenience, we automatically strip SINCE and UNTIL from a query when creating a condition from the context of a chart. TIMESERIES In NRQL queries, the TIMESERIES clause is used to return data as a time series broken out by a specified period of time. For NRQL conditions, the equivalent property of a signal is the aggregation duration window. histogram() The histogram() aggregation function is used to generate histograms. histogram() is not compatible with NRQL alerting: histogram aggregations can not be formatted as a time series. To create an alert from a portion of a histogram (for example, 95th percentile), use the percentile() aggregation function. Multiple aggregation functions Each condition can only target a single aggregated value. To alert on multiple values simultaneously, you’ll need to decompose them into individual conditions within the same policy. Original Query: SELECT count(foo), average(bar), max(baz) from Transaction Copy Decomposed: SELECT count(foo) from Transaction SELECT average(bar) from Transaction SELECT max(baz) from Transaction Copy COMPARE WITH The COMPARE WITH clause is used to compare the values for two different time ranges. This type of query is incompatible with NRQL alerting. We recommend using a Baseline Alert Condition to dynamically detect deviations for a particular signal. SLIDE BY The SLIDE BY clause supports a feature known as sliding windows. With sliding windows, SLIDE BY data is gathered into \"windows\" of time that overlap with each other. These windows can help to smooth out line graphs with a lot of variation in cases where the rolling aggregate (such as a rolling mean) is more important than aggregates from narrow windows of time. Sliding windows are not currently supported in NRQL alerts. LIMIT In NRQL queries, the LIMIT clause is used to control the amount of data a query returns, either the maximum number of facet values returned by FACET queries or the maximum number of items returned by SELECT * queries. LIMIT is not compatible with NRQL alerting: evaluation is always performed on the full result set. NRQL alert threshold examples Here are some common use cases for NRQL conditions. These queries will work for static and baseline condition types. The outlier condition type will require additional FACET clauses. Alert on specific segments of your data Create constrained alerts that target a specific segment of your data, such as a few key customers or a range of data. Use the WHERE clause to define those conditions. SELECT average(duration) FROM Transaction WHERE account_id in (91290, 102021, 20230) Copy SELECT percentile(duration, 95) FROM Transaction WHERE name LIKE 'Controller/checkout/%' Copy Alert on Nth percentile of your data Create alerts when an Nth percentile of your data hits a specified threshold; for example, maintaining SLA service levels. Since we evaluate the NRQL query based on the aggregation window duration, percentiles will be calculated for each duration separately. SELECT percentile(duration, 95) FROM Transaction Copy SELECT percentile(databaseDuration, 75) FROM Transaction Copy Alert on max, min, avg of your data Create alerts when your data hits a certain maximum, minimum, or average; for example, ensuring that a duration or response time does not pass a certain threshold. SELECT max(duration) FROM Transaction Copy SELECT average(duration) FROM Transaction Copy Alert on a percentage of your data Create alerts when a proportion of your data goes above or below a certain threshold. SELECT percentage(count(*), WHERE duration > 2) FROM Transaction Copy SELECT percentage(count(*), WHERE httpResponseCode = '500') FROM Transaction Copy Alert on Apdex with any T-value Create alerts on Apdex, applying your own T-value for certain transactions. For example, get an alert notification when your Apdex for a T-value of 500ms on transactions for production apps goes below 0.8. SELECT apdex(duration, t:0.5) FROM Transaction WHERE appName like '%prod%' Copy NRQL conditions and query order of operations By default, the aggregation window duration is 1 minute, but you can change the window to suit your needs. Whatever the aggregation window, New Relic will collect data for that window using the function in the NRQL condition’s query. The query is parsed and executed by our systems in the following order: FROM clause – which event type needs to be grabbed? WHERE clause – what can be filtered out? SELECT clause – what information needs to be returned from the now-filtered data set? Example: null value returned Let's say this is your alert condition query: SELECT count(*) FROM SyntheticCheck WHERE monitorName = 'My Cool Monitor' AND result = 'FAILURE' Copy If there are no failures for the aggregation window: The system will execute the FROM clause by grabbing all SyntheticCheck events on your account. Then it will execute the WHERE clause to filter through those events by looking only for the ones that match the monitor name and result specified. If there are still events left to scan through after completing the FROM and WHERE operations, the SELECT clause will be executed. If there are no remaining events, the SELECT clause will not be executed. This means that aggregators like count() and uniqueCount() will never return a zero value. When there is a count of 0, the SELECT clause is ignored and no data is returned, resulting in a value of NULL. Example: zero value returned If you have a data source delivering legitimate numeric zeroes, the query will return zero values and not null values. Let's say this is your alert condition query, and that MyCoolEvent is an attribute that can sometimes return a zero value. SELECT average(MyCoolAttribute) FROM MyCoolEvent Copy If, in the aggregation window being evaluated, there's at least one instance of MyCoolEvent and if the average value of all MyCoolAttribute attributes from that window is equal to zero, then a 0 value will be returned. If there are no MyCoolEvent events during that minute, then a NULL will be returned due to the order of operations. Tip For more information about this topic, you can check out our blog post on troubleshooting for zero versus null values. Tip You can determine how null values will be handled by adjusting loss of signal and gap filling settings in the Alert Conditions UI. Tip You can avoid NULL values entirely with a query order of operations shortcut. Do this by using a filter sub-clause, then including all filter elements within that sub-clause. The main body of the query will run and return data, at which point the SELECT clause will then run and apply the filter elements. The query will return a value of 0 if the filter elements result in no matching data. Here's an example: SELECT filter(count(*), WHERE result = 'SUCCESS' AND monitorName = 'My Favorite Monitor') FROM SyntheticCheck Copy Nested aggregation NRQL alerts Nested aggregation queries are a powerful way to query your data. However, they have a few restrictions that are important to note. Nested queries with a non-faceted innermost query are not currently supported Without a FACET, the inner query produces a single result, giving the outer query nothing to aggregate. If you're using a nested query, make sure your inner query is faceted. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu) ​​​​​ Copy Queries at all levels must have the same aggregation window size With an alert aggregation window of 1 minute, the inner query would produce two smaller windows of 30 seconds. In theory, these two windows could be aggregated by the outer query. However, this is not currently supported. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu TIMESERIES 30 seconds)​​ Copy Signal loss is not yet supported for nested queries For more information on signal loss, see NerdGraph API: Loss of signal and gap filling. Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported You can't use a nested query containing the WITH METRIC_FORMAT in the inner query to create NRQL alert conditions. NRQL condition creation tips Here are some tips for creating and using a NRQL condition: Topic Tips Condition types NRQL condition types include static, baseline, and outlier. Create a description For NRQL conditions, you can create a custom description to add to each violation. Descriptions can be enhanced with variable substitution based on metadata in the specific violation. For details, see Description Query results Queries must return a number. The condition evaluates the returned number against the thresholds you've set. Time period NRQL conditions evaluate data based on how it's aggregated, using aggregation windows from 30 seconds to 15 minutes, in increments of 15 seconds. For best results, we recommend using the event flow or event timer aggregation methods. For the cadence aggregation method, the implicit SINCE ... UNTIL clause specifying which minute to evaluate is controlled by your delay/timer setting. Since very recent data may be incomplete, you may want to query data from 3 minutes ago or longer, especially for: Applications that run on multiple hosts. SyntheticCheck data: Timeouts can take 3 minutes, so 5 minutes or more is recommended. Also, if a query will generate intermittent data, consider using the sum of query results option. Lost signal threshold (loss of signal detection) You can use loss of signal detection to alert on when your data (a telemetry signal) should be considered lost. A signal loss can indicate that a service or entity is no longer online or that a periodic job failed to run. You can also use this to make sure that violations for sporadic data, such as error counts, are closed when no signal is coming in. Advanced signal settings These settings give you options for better handling continuous, streaming data signals that may sometimes be missing. These settings include the aggregation window duration, the delay/timer, and an option for filling data gaps. For more on using these, see Advanced signal settings. Condition settings Use the Condition settings to: Create a concise, descriptive condition name. Provide a custom violation description for the condition that will be included in violations and notifications. Add the runbook URL to include your organization's procedures for handling incidents. You may also add this information to the custom violation description. Limits on conditions See the maximum values. Health status In order for a NRQL alert condition health status display to function properly, use a FACET clause to scope each signal to a single entity (for example, FACET hostname or FACET appname). Examples For more information, see: Expected NRQL syntax Examples of NRQL condition queries Condition edits can reset condition evaluation When you edit NRQL alert conditions in some specific ways (detailed below), their evaluations are reset, meaning that any evaluation up until that point is lost, and the evaluation starts over from that point. The two ways this will affect you are: For \"for at least x minutes\" thresholds: because the evaluation window has been reset, there will be a delay of at least x minutes before any violations can be reported. For baseline conditions: the condition starts over again and all baseline learning is lost. The following actions cause an evaluation reset for NRQL conditions: Changing the query Changing the aggregation window, aggregation method, or aggregation delay/timer setting Changing the \"close violations on signal loss\" setting Changing any gap fill settings Changing the baseline direction (if applicable) – higher, lower, or higher/lower Change the threshold value, threshold window, or threshold operator The following actions (along with any other actions not covered in the above list) will not reset the evaluation: Changing the loss of signal time window (expiration duration) Changing the time function (switching \"for at least\" to \"at least once in,\" or vice-versa) Toggling the \"open violation on signal loss\" setting Alert condition types When you create a NRQL alert, you can choose from different types of conditions: NRQL alert condition types Description Static This is the simplest type of NRQL condition. It allows you to create a condition based on a NRQL query that returns a numeric value. Optional: Include a FACET clause. Baseline (Dynamic) Uses a self-adjusting condition based on the past behavior of the monitored values. Uses the same NRQL query form as the static type, including the optional FACET clause. Outlier Looks for group behavior and values that are outliers from those groups. Uses the same NRQL query form as the static type, but requires a FACET clause. Sum of query results (limited or intermittent data) Important Available only for static (basic) condition types. If a query returns intermittent or limited data, it may be difficult to set a meaningful threshold. Missing or limited data will sometimes generate false positives or false negatives. You can use loss of signal, aggregation duration, and gap filling settings to minimize these false notifications. To avoid this problem when using the static threshold type, you can set the selector to sum of query results. This lets you set the alert on an aggregated sum instead of a value from a single harvest cycle. Up to two hours of one-minute data checks can be aggregated. The duration you select determines the width of the rolling sum and the preview chart will update accordingly. Set the loss of signal threshold Loss of signal occurs when no data matches the NRQL condition over a specific period of time. You can set your loss of signal threshold duration and also what happens when the threshold is crossed. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Loss of signal is only available for NRQL conditions. You may also manage these settings using the GraphQL API (recommended), or the REST API. Go here for specific GraphQL API examples. Loss of signal settings: Loss of signal settings include a time duration and two possible actions. Signal loss expiration time UI label: Signal is lost after: GraphQL Node: expiration.expirationDuration Expiration duration is a timer that starts and resets when we receive a data point in the streaming alerts pipeline. If we don't receive another data point before your 'expiration time' expires, we consider that signal to be lost. This can be because no data is being sent to New Relic or the WHERE clause of your NRQL query is filtering that data out before it is streamed to the alerts pipeline. Note that when you have a faceted query, each facet is a signal. So if any one of those signals ends during the duration specified, that will be considered a loss of signal. The loss of signal expiration time is independent of the threshold duration and triggers as soon as the timer expires. The maximum expiration duration is 48 hours. This is helpful when monitoring for the execution of infrequent jobs. The minimum is 30 seconds, but we recommend using at least 3-5 minutes. Loss of signal actions Once a signal is considered lost, you can close open violations, open new violations, or both. Close all current open violations: This closes all open violations that are related to a specific signal. It won't necessarily close all violations for a condition. If you're alerting on an ephemeral service, or on a sporadic signal, you'll want to choose this action to ensure that violations are closed properly. The GraphQL node name for this is \"closeViolationsOnExpiration\" Open new violations: This will open a new violation when the signal is considered lost. These violations will indicate that they are due to a loss of signal. Based on your incident preferences, this should trigger a notification. The graphQL node name for this is \"openViolationOnExpiration\" When you enable both actions, we'll close all open violations first, and then open a new violation for loss of signal. To create a NRQL alert configured with loss of signal detection in the UI: For a policy, when you create a condition, under Select a product, click NRQL, then click Next, define thresholds. Write a NRQL query that returns the values you want to alert on. For Threshold type, select Static or Baseline. Click + Add lost signal threshold, then set the signal expiration duration time in minutes or seconds in the Signal is lost after field. Choose what you want to happen when the signal is lost. You can check one or both of Close all current open violations and Open new \"lost signal\" violation. These control how loss of signal violations will be handled for the condition. Make sure you name your condition before you save it. Violations open due to loss of signal close when the signal comes back. Newly opened lost signal violations will close immediately when new data is evaluated. the condition they belong to expires. By default, conditions expire after 3 days. you manually close the violation with the Close all current open violations option. Tip Loss of signal detection doesn't work on NRQL queries that use nested aggregation or sub-queries. Advanced signal settings When creating a NRQL alert condition, use the advanced signal settings to control streaming alert data and avoid false alarms. When creating a NRQL condition, there are several advanced signal settings: Aggregation window duration Delay/timer Fill data gaps To read an explanation of what these settings are and how they relate to each other, see Streaming alerts concepts. Below are instructions and tips on how to configure them. Aggregation window duration You can set the aggregation window duration to choose how long data is accumulated in a streaming time window before it's aggregated. You can set it to anything between 30 seconds and 15 minutes. The default is one minute. Tip Baseline alert condition thresholds don't support editing the aggregation window. They use the 1 minute default. Delay/timer You can adjust the delay/timer to coordinate our streaming alerting algorithm with your data's behavior. If your data is sparse or inconsistent, you may want to use the event timer aggregation method. For the cadence method, the total supported latency is the sum of the aggregation window duration and the delay. If the data type comes from an APM language agent and is aggregated from many app instances (for example, Transactions, TransactionErrors, etc.), we recommend using the event flow method with the default settings. Important When creating NRQL conditions for data collected from Infrastructure Cloud Integrations such as AWS Cloudwatch or Azure, we recommend that you use the event timer method. Fill data gaps Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with one of these settings: None: (Default) Choose this if you don't want to take any action on empty aggregation windows. On evaluation, an empty aggregation window will reset the threshold duration timer. For example, if a condition says that all aggregation windows must have data points above the threshold for 5 minutes, and 1 of the 5 aggregation windows is empty, then the condition won't be in violation. Custom static value: Choose this if you'd like to insert a custom static value into the empty aggregation windows before they're evaluated. This option has an additional, required parameter of fillValue (as named in the API) that specifies what static value should be used. This defaults to 0. Last known value: This option inserts the last seen value before evaluation occurs. We maintain the state of the last seen value for 2 hours. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss, gap filling, and how to request access to these features, see this Explorers Hub post. Options for editing data gap settings: In the NRQL conditions UI, go to Condition settings > Advanced signal settings > fill data gaps with and choose an option. If using our Nerdgraph API (preferred), this node is located at: actor : account : alerts : nrqlCondition : signal : fillOption | fillValue NerdGraph is our recommended API for this but if you're using our REST API, you can find this setting in the REST API explorer under the \"signal\" section of the Alert NRQL conditions API.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.00883,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create NRQL <em>alert</em> conditions",
        "sections": "Nested aggregation NRQL <em>alerts</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": " a NRQL <em>alert</em> condition for a policy: On one.newrelic.com, in the header click <em>Alerts</em> &amp; AI, then in the left sidebar click Policies. Select an existing policy or click <em>New</em> <em>alert</em> policy to create a <em>new</em> policy. Click Add a condition. Under Select a product click NRQL, and then click Next, define"
      },
      "id": "603ef04864441fbc114e8883"
    }
  ],
  "/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/other-condition-types/apm-metric-alert-conditions": [
    {
      "sections": [
        "Create NRQL alert conditions",
        "Tip",
        "Create a NRQL alert condition",
        "Create a condition from a chart",
        "Caution",
        "NRQL alert syntax",
        "Important",
        "Reformatting incompatible NRQL",
        "NRQL alert threshold examples",
        "Alert on specific segments of your data",
        "Alert on Nth percentile of your data",
        "Alert on max, min, avg of your data",
        "Alert on a percentage of your data",
        "Alert on Apdex with any T-value",
        "NRQL conditions and query order of operations",
        "Example: null value returned",
        "Example: zero value returned",
        "Nested aggregation NRQL alerts",
        "Nested queries with a non-faceted innermost query are not currently supported",
        "Queries at all levels must have the same aggregation window size",
        "Signal loss is not yet supported for nested queries",
        "Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported",
        "NRQL condition creation tips",
        "Condition edits can reset condition evaluation",
        "Alert condition types",
        "Sum of query results (limited or intermittent data)",
        "Set the loss of signal threshold",
        "Advanced signal settings",
        "Aggregation window duration",
        "Delay/timer",
        "Fill data gaps"
      ],
      "title": "Create NRQL alert conditions",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert conditions"
      ],
      "external_id": "2872f770a5fc8a802f99b9f848906f7e351ad761",
      "image": "https://docs.newrelic.com/static/eb8e1b2d826f9fd9cf46fe3dd5455217/c1b63/nr1_nrql_alert_conditions.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/",
      "published_at": "2021-12-19T16:01:17Z",
      "updated_at": "2021-12-14T22:49:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use NRQL queries to create alert conditions. Once you've defined your signal, you can further define your warning and critical threshold levels. This determines when an alerts violation is created. Read on to learn more about how to do this. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Click NRQL, and then Next, define thresholds. Tip For more information on key concepts relating to NRQL alert conditions and streaming alerts, see Streaming alerts: key terms and concepts. Create a NRQL alert condition To create a NRQL alert condition for a policy: On one.newrelic.com, in the header click Alerts & AI, then in the left sidebar click Policies. Select an existing policy or click New alert policy to create a new policy. Click Add a condition. Under Select a product click NRQL, and then click Next, define thresholds. Note that editing an existing condition can result in resetting its evaluation. Create a condition from a chart You can use a chart to create a NRQL alert condition. To create a NRQL alerts condition from a chart, click the chart menu , then click Create alert condition. Once you've named and customized your condition, you can add it to an existing policy or create a new one. Caution A small number of our older charts don't include the option to create an alert condition. NRQL alert syntax Here's the basic syntax for creating all NRQL alert conditions. The FACET clause is required for outlier condition types. It's optional for static and baseline. SELECT function(attribute) FROM Event WHERE attribute [comparison] [AND|OR ...] Copy Clause Notes SELECT function(attribute) Required Supported functions that return numbers include: apdex average count latest max min percentage percentile sum uniqueCount Tip If you use the percentile aggregator in a faceted alert condition with many facets, this may cause the following error to appear: An error occurred while fetching chart data. If you see this error, use average instead. FROM data type Required Only one data type can be targeted. Supported data types: Event Metric (RAW data points will be returned) WHERE attribute [comparison] [AND|OR ...] Use the WHERE clause to specify a series of one or more conditions. All the operators are supported. FACET attribute Required for outlier conditions Include an optional FACET clause in your NRQL syntax depending on the threshold type: static or baseline. Use the FACET clause to separate your results by attribute and alert on each attribute independently. No LIMIT clause is allowed, but all queries will receive the maximum number of facets possible. Faceted queries can return a maximum of 5000 values for static and baseline conditions and a maximum of 500 values for outlier conditions. Important If the query returns more than the maximum number of values, the alert condition can't be created. If you create the condition and the query returns more than this number later, the alert will fail. Modify your query so that it returns a fewer number of values. Reformatting incompatible NRQL Some elements of NRQL used in charts don’t make sense in the streaming context of alerts. Here’s a list of the most common incompatible elements and suggestions for reformatting a NRQL alert query to achieve the same effect. Element Notes SINCE and UNTIL Example: SELECT percentile(largestContentfulPaint, 75) FROM PageViewTiming WHERE (appId = 837807) SINCE yesterday Copy NRQL conditions produce a never-ending stream of windowed query results, so the SINCE and UNTIL keywords to scope the query to a point in time are not compatible. As a convenience, we automatically strip SINCE and UNTIL from a query when creating a condition from the context of a chart. TIMESERIES In NRQL queries, the TIMESERIES clause is used to return data as a time series broken out by a specified period of time. For NRQL conditions, the equivalent property of a signal is the aggregation duration window. histogram() The histogram() aggregation function is used to generate histograms. histogram() is not compatible with NRQL alerting: histogram aggregations can not be formatted as a time series. To create an alert from a portion of a histogram (for example, 95th percentile), use the percentile() aggregation function. Multiple aggregation functions Each condition can only target a single aggregated value. To alert on multiple values simultaneously, you’ll need to decompose them into individual conditions within the same policy. Original Query: SELECT count(foo), average(bar), max(baz) from Transaction Copy Decomposed: SELECT count(foo) from Transaction SELECT average(bar) from Transaction SELECT max(baz) from Transaction Copy COMPARE WITH The COMPARE WITH clause is used to compare the values for two different time ranges. This type of query is incompatible with NRQL alerting. We recommend using a Baseline Alert Condition to dynamically detect deviations for a particular signal. SLIDE BY The SLIDE BY clause supports a feature known as sliding windows. With sliding windows, SLIDE BY data is gathered into \"windows\" of time that overlap with each other. These windows can help to smooth out line graphs with a lot of variation in cases where the rolling aggregate (such as a rolling mean) is more important than aggregates from narrow windows of time. Sliding windows are not currently supported in NRQL alerts. LIMIT In NRQL queries, the LIMIT clause is used to control the amount of data a query returns, either the maximum number of facet values returned by FACET queries or the maximum number of items returned by SELECT * queries. LIMIT is not compatible with NRQL alerting: evaluation is always performed on the full result set. NRQL alert threshold examples Here are some common use cases for NRQL conditions. These queries will work for static and baseline condition types. The outlier condition type will require additional FACET clauses. Alert on specific segments of your data Create constrained alerts that target a specific segment of your data, such as a few key customers or a range of data. Use the WHERE clause to define those conditions. SELECT average(duration) FROM Transaction WHERE account_id in (91290, 102021, 20230) Copy SELECT percentile(duration, 95) FROM Transaction WHERE name LIKE 'Controller/checkout/%' Copy Alert on Nth percentile of your data Create alerts when an Nth percentile of your data hits a specified threshold; for example, maintaining SLA service levels. Since we evaluate the NRQL query based on the aggregation window duration, percentiles will be calculated for each duration separately. SELECT percentile(duration, 95) FROM Transaction Copy SELECT percentile(databaseDuration, 75) FROM Transaction Copy Alert on max, min, avg of your data Create alerts when your data hits a certain maximum, minimum, or average; for example, ensuring that a duration or response time does not pass a certain threshold. SELECT max(duration) FROM Transaction Copy SELECT average(duration) FROM Transaction Copy Alert on a percentage of your data Create alerts when a proportion of your data goes above or below a certain threshold. SELECT percentage(count(*), WHERE duration > 2) FROM Transaction Copy SELECT percentage(count(*), WHERE httpResponseCode = '500') FROM Transaction Copy Alert on Apdex with any T-value Create alerts on Apdex, applying your own T-value for certain transactions. For example, get an alert notification when your Apdex for a T-value of 500ms on transactions for production apps goes below 0.8. SELECT apdex(duration, t:0.5) FROM Transaction WHERE appName like '%prod%' Copy NRQL conditions and query order of operations By default, the aggregation window duration is 1 minute, but you can change the window to suit your needs. Whatever the aggregation window, New Relic will collect data for that window using the function in the NRQL condition’s query. The query is parsed and executed by our systems in the following order: FROM clause – which event type needs to be grabbed? WHERE clause – what can be filtered out? SELECT clause – what information needs to be returned from the now-filtered data set? Example: null value returned Let's say this is your alert condition query: SELECT count(*) FROM SyntheticCheck WHERE monitorName = 'My Cool Monitor' AND result = 'FAILURE' Copy If there are no failures for the aggregation window: The system will execute the FROM clause by grabbing all SyntheticCheck events on your account. Then it will execute the WHERE clause to filter through those events by looking only for the ones that match the monitor name and result specified. If there are still events left to scan through after completing the FROM and WHERE operations, the SELECT clause will be executed. If there are no remaining events, the SELECT clause will not be executed. This means that aggregators like count() and uniqueCount() will never return a zero value. When there is a count of 0, the SELECT clause is ignored and no data is returned, resulting in a value of NULL. Example: zero value returned If you have a data source delivering legitimate numeric zeroes, the query will return zero values and not null values. Let's say this is your alert condition query, and that MyCoolEvent is an attribute that can sometimes return a zero value. SELECT average(MyCoolAttribute) FROM MyCoolEvent Copy If, in the aggregation window being evaluated, there's at least one instance of MyCoolEvent and if the average value of all MyCoolAttribute attributes from that window is equal to zero, then a 0 value will be returned. If there are no MyCoolEvent events during that minute, then a NULL will be returned due to the order of operations. Tip For more information about this topic, you can check out our blog post on troubleshooting for zero versus null values. Tip You can determine how null values will be handled by adjusting loss of signal and gap filling settings in the Alert Conditions UI. Tip You can avoid NULL values entirely with a query order of operations shortcut. Do this by using a filter sub-clause, then including all filter elements within that sub-clause. The main body of the query will run and return data, at which point the SELECT clause will then run and apply the filter elements. The query will return a value of 0 if the filter elements result in no matching data. Here's an example: SELECT filter(count(*), WHERE result = 'SUCCESS' AND monitorName = 'My Favorite Monitor') FROM SyntheticCheck Copy Nested aggregation NRQL alerts Nested aggregation queries are a powerful way to query your data. However, they have a few restrictions that are important to note. Nested queries with a non-faceted innermost query are not currently supported Without a FACET, the inner query produces a single result, giving the outer query nothing to aggregate. If you're using a nested query, make sure your inner query is faceted. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu) ​​​​​ Copy Queries at all levels must have the same aggregation window size With an alert aggregation window of 1 minute, the inner query would produce two smaller windows of 30 seconds. In theory, these two windows could be aggregated by the outer query. However, this is not currently supported. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu TIMESERIES 30 seconds)​​ Copy Signal loss is not yet supported for nested queries For more information on signal loss, see NerdGraph API: Loss of signal and gap filling. Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported You can't use a nested query containing the WITH METRIC_FORMAT in the inner query to create NRQL alert conditions. NRQL condition creation tips Here are some tips for creating and using a NRQL condition: Topic Tips Condition types NRQL condition types include static, baseline, and outlier. Create a description For NRQL conditions, you can create a custom description to add to each violation. Descriptions can be enhanced with variable substitution based on metadata in the specific violation. For details, see Description Query results Queries must return a number. The condition evaluates the returned number against the thresholds you've set. Time period NRQL conditions evaluate data based on how it's aggregated, using aggregation windows from 30 seconds to 15 minutes, in increments of 15 seconds. For best results, we recommend using the event flow or event timer aggregation methods. For the cadence aggregation method, the implicit SINCE ... UNTIL clause specifying which minute to evaluate is controlled by your delay/timer setting. Since very recent data may be incomplete, you may want to query data from 3 minutes ago or longer, especially for: Applications that run on multiple hosts. SyntheticCheck data: Timeouts can take 3 minutes, so 5 minutes or more is recommended. Also, if a query will generate intermittent data, consider using the sum of query results option. Lost signal threshold (loss of signal detection) You can use loss of signal detection to alert on when your data (a telemetry signal) should be considered lost. A signal loss can indicate that a service or entity is no longer online or that a periodic job failed to run. You can also use this to make sure that violations for sporadic data, such as error counts, are closed when no signal is coming in. Advanced signal settings These settings give you options for better handling continuous, streaming data signals that may sometimes be missing. These settings include the aggregation window duration, the delay/timer, and an option for filling data gaps. For more on using these, see Advanced signal settings. Condition settings Use the Condition settings to: Create a concise, descriptive condition name. Provide a custom violation description for the condition that will be included in violations and notifications. Add the runbook URL to include your organization's procedures for handling incidents. You may also add this information to the custom violation description. Limits on conditions See the maximum values. Health status In order for a NRQL alert condition health status display to function properly, use a FACET clause to scope each signal to a single entity (for example, FACET hostname or FACET appname). Examples For more information, see: Expected NRQL syntax Examples of NRQL condition queries Condition edits can reset condition evaluation When you edit NRQL alert conditions in some specific ways (detailed below), their evaluations are reset, meaning that any evaluation up until that point is lost, and the evaluation starts over from that point. The two ways this will affect you are: For \"for at least x minutes\" thresholds: because the evaluation window has been reset, there will be a delay of at least x minutes before any violations can be reported. For baseline conditions: the condition starts over again and all baseline learning is lost. The following actions cause an evaluation reset for NRQL conditions: Changing the query Changing the aggregation window, aggregation method, or aggregation delay/timer setting Changing the \"close violations on signal loss\" setting Changing any gap fill settings Changing the baseline direction (if applicable) – higher, lower, or higher/lower Change the threshold value, threshold window, or threshold operator The following actions (along with any other actions not covered in the above list) will not reset the evaluation: Changing the loss of signal time window (expiration duration) Changing the time function (switching \"for at least\" to \"at least once in,\" or vice-versa) Toggling the \"open violation on signal loss\" setting Alert condition types When you create a NRQL alert, you can choose from different types of conditions: NRQL alert condition types Description Static This is the simplest type of NRQL condition. It allows you to create a condition based on a NRQL query that returns a numeric value. Optional: Include a FACET clause. Baseline (Dynamic) Uses a self-adjusting condition based on the past behavior of the monitored values. Uses the same NRQL query form as the static type, including the optional FACET clause. Outlier Looks for group behavior and values that are outliers from those groups. Uses the same NRQL query form as the static type, but requires a FACET clause. Sum of query results (limited or intermittent data) Important Available only for static (basic) condition types. If a query returns intermittent or limited data, it may be difficult to set a meaningful threshold. Missing or limited data will sometimes generate false positives or false negatives. You can use loss of signal, aggregation duration, and gap filling settings to minimize these false notifications. To avoid this problem when using the static threshold type, you can set the selector to sum of query results. This lets you set the alert on an aggregated sum instead of a value from a single harvest cycle. Up to two hours of one-minute data checks can be aggregated. The duration you select determines the width of the rolling sum and the preview chart will update accordingly. Set the loss of signal threshold Loss of signal occurs when no data matches the NRQL condition over a specific period of time. You can set your loss of signal threshold duration and also what happens when the threshold is crossed. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Loss of signal is only available for NRQL conditions. You may also manage these settings using the GraphQL API (recommended), or the REST API. Go here for specific GraphQL API examples. Loss of signal settings: Loss of signal settings include a time duration and two possible actions. Signal loss expiration time UI label: Signal is lost after: GraphQL Node: expiration.expirationDuration Expiration duration is a timer that starts and resets when we receive a data point in the streaming alerts pipeline. If we don't receive another data point before your 'expiration time' expires, we consider that signal to be lost. This can be because no data is being sent to New Relic or the WHERE clause of your NRQL query is filtering that data out before it is streamed to the alerts pipeline. Note that when you have a faceted query, each facet is a signal. So if any one of those signals ends during the duration specified, that will be considered a loss of signal. The loss of signal expiration time is independent of the threshold duration and triggers as soon as the timer expires. The maximum expiration duration is 48 hours. This is helpful when monitoring for the execution of infrequent jobs. The minimum is 30 seconds, but we recommend using at least 3-5 minutes. Loss of signal actions Once a signal is considered lost, you can close open violations, open new violations, or both. Close all current open violations: This closes all open violations that are related to a specific signal. It won't necessarily close all violations for a condition. If you're alerting on an ephemeral service, or on a sporadic signal, you'll want to choose this action to ensure that violations are closed properly. The GraphQL node name for this is \"closeViolationsOnExpiration\" Open new violations: This will open a new violation when the signal is considered lost. These violations will indicate that they are due to a loss of signal. Based on your incident preferences, this should trigger a notification. The graphQL node name for this is \"openViolationOnExpiration\" When you enable both actions, we'll close all open violations first, and then open a new violation for loss of signal. To create a NRQL alert configured with loss of signal detection in the UI: For a policy, when you create a condition, under Select a product, click NRQL, then click Next, define thresholds. Write a NRQL query that returns the values you want to alert on. For Threshold type, select Static or Baseline. Click + Add lost signal threshold, then set the signal expiration duration time in minutes or seconds in the Signal is lost after field. Choose what you want to happen when the signal is lost. You can check one or both of Close all current open violations and Open new \"lost signal\" violation. These control how loss of signal violations will be handled for the condition. Make sure you name your condition before you save it. Violations open due to loss of signal close when the signal comes back. Newly opened lost signal violations will close immediately when new data is evaluated. the condition they belong to expires. By default, conditions expire after 3 days. you manually close the violation with the Close all current open violations option. Tip Loss of signal detection doesn't work on NRQL queries that use nested aggregation or sub-queries. Advanced signal settings When creating a NRQL alert condition, use the advanced signal settings to control streaming alert data and avoid false alarms. When creating a NRQL condition, there are several advanced signal settings: Aggregation window duration Delay/timer Fill data gaps To read an explanation of what these settings are and how they relate to each other, see Streaming alerts concepts. Below are instructions and tips on how to configure them. Aggregation window duration You can set the aggregation window duration to choose how long data is accumulated in a streaming time window before it's aggregated. You can set it to anything between 30 seconds and 15 minutes. The default is one minute. Tip Baseline alert condition thresholds don't support editing the aggregation window. They use the 1 minute default. Delay/timer You can adjust the delay/timer to coordinate our streaming alerting algorithm with your data's behavior. If your data is sparse or inconsistent, you may want to use the event timer aggregation method. For the cadence method, the total supported latency is the sum of the aggregation window duration and the delay. If the data type comes from an APM language agent and is aggregated from many app instances (for example, Transactions, TransactionErrors, etc.), we recommend using the event flow method with the default settings. Important When creating NRQL conditions for data collected from Infrastructure Cloud Integrations such as AWS Cloudwatch or Azure, we recommend that you use the event timer method. Fill data gaps Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with one of these settings: None: (Default) Choose this if you don't want to take any action on empty aggregation windows. On evaluation, an empty aggregation window will reset the threshold duration timer. For example, if a condition says that all aggregation windows must have data points above the threshold for 5 minutes, and 1 of the 5 aggregation windows is empty, then the condition won't be in violation. Custom static value: Choose this if you'd like to insert a custom static value into the empty aggregation windows before they're evaluated. This option has an additional, required parameter of fillValue (as named in the API) that specifies what static value should be used. This defaults to 0. Last known value: This option inserts the last seen value before evaluation occurs. We maintain the state of the last seen value for 2 hours. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss, gap filling, and how to request access to these features, see this Explorers Hub post. Options for editing data gap settings: In the NRQL conditions UI, go to Condition settings > Advanced signal settings > fill data gaps with and choose an option. If using our Nerdgraph API (preferred), this node is located at: actor : account : alerts : nrqlCondition : signal : fillOption | fillValue NerdGraph is our recommended API for this but if you're using our REST API, you can find this setting in the REST API explorer under the \"signal\" section of the Alert NRQL conditions API.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 251.85574,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create NRQL <em>alert</em> <em>conditions</em>",
        "sections": "Create NRQL <em>alert</em> <em>conditions</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "You can use NRQL queries to create <em>alert</em> <em>conditions</em>. Once you&#x27;ve defined your signal, you can further define your warning and critical threshold levels. This determines when an <em>alerts</em> violation is created. Read on to learn more about how to do this. Go to one.newrelic.com, click <em>Alerts</em> &amp; AI"
      },
      "id": "603ef04864441fbc114e8883"
    },
    {
      "sections": [
        "Streaming alerts: key terms and concepts",
        "Why it matters",
        "Streaming alerts process and descriptions",
        "Choose your aggregation method",
        "Event flow",
        "Caution",
        "Event timer",
        "Cadence",
        "Streaming alerts tools",
        "Tip",
        "Window duration",
        "Delay/timer",
        "Loss of signal detection",
        "Gap filling"
      ],
      "title": "Streaming alerts: key terms and concepts",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Get started"
      ],
      "external_id": "144c7f0721ec2acb1e8ac8aa06142911ea552aef",
      "image": "https://docs.newrelic.com/static/39158bd84483adf9516bb7ff5058c9bd/d30ee/streaming-alerts.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/streaming-alerts-key-terms-concepts/",
      "published_at": "2021-12-15T12:47:58Z",
      "updated_at": "2021-12-15T12:47:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The streaming alerts platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into New Relic. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that's processed by the streaming algorithm. There are three methods for aggregating the data filtered through your NRQL condition: Event flow (default) Event timer Cadence This short video describes the three aggregation method (5:31). Why it matters Understanding how streaming alerts works will help you fine-tune your NRQL conditions to be notified about what's important to you. Only data that matches the conditions of the NRQL WHERE clause is alerted on. For more details on each step of the process, see Streaming alerts process and descriptions. As data streams into New Relic, it's filtered by the NRQL condition. Before data is evaluated, it must meet the criteria defined by the NRQL query's WHERE clause. Instead of evaluating that data immediately for violations, the NRQL alert conditions collect the data over a period of time known as the aggregation window. An additional delay/timer allows for slower data points to arrive before the window is aggregated. Once the delay/timer time has elapsed, New Relic aggregates the data into a single data point. Alerts then evaluates the data point using the NRQL condition in order to determine whether it meets the violation threshold criteria. Even if a data point meets the criteria for a violation, a violation may not be triggered. A violation is only triggered when data points consistently meet the threshold criteria over a period of time. This is the threshold duration. If the data points are in violation for an entire threshold duration, we'll send you a notification based on your policy settings. All of these configurable delays give you more control over how you're alerted on sporadic and missing data. Streaming alerts process and descriptions Process Description Streaming data All data coming into New Relic. WHERE clause Filters all incoming streaming data. We only monitor for alerts on data that makes it through this filter. Aggregation methods One of three methods that control how data is collected before it's evaluated. They are: Event flow (Default) Event timer Cadence Aggregation window Data with timestamps that fall within this window will be aggregated and then evaluated. Delay/timer A time delay to ensure all data points have arrived in the aggregation window before aggregation occurs. Aggregated data Data in the aggregated window is collapsed to a single data point for alert evaluation. Evaluation The data point is evaluated by the NRQL condition, which is trigged by each incoming aggregated data point. Threshold duration A specific duration that determines if a violation is created. If your specified NRQL condition meets the threshold criteria over the threshold duration, a violation occurs. When a data point lacks data, a custom value is inserted to fill the gap. Choose your aggregation method You can choose between three different aggregation methods, depending on your needs. Event flow (default) works best for data that comes in frequently and mostly in order. Event timer works best for data that arrives infrequently in batches, such as cloud integration data or infrequent error logs. Cadence is our original and inferior aggregation method. It aggregates data on specific time intervals as detected by New Relic's internal wall clock, regardless of data timestamps. Event flow Event flow aggregates a window of data when the first data point arrives in a subsequent window. The custom delay defines which subsequent window data will start to populate to trigger aggregation of the current window. A custom delay provides extra time for data to arrive. These times are based on the data's timestamps and not New Relic's wall clock time. For example, suppose you're monitoring CPU usage in window durations of 1 minute and a 3 minute delay. When a CPU usage data point comes in with a timestamp between 12:00pm and 12:01pm, event flow will not aggregate that window until a data point shows up with a timestamp between 12:04pm and 12:05pm. When event flow receives the first data point with a timestamp of 12:04pm or later, it sends the 12:00 to 12:01 data to be aggregated. Caution If you expect your data points to arrive more than 30 minutes apart, please use the Event Timer method described below. Event timer Like event flow, event timer will only aggregate data for a given window when data arrives for that window. When a data point arrives for an aggregation window, a timer dedicated to that window starts to count down. If no further data arrives before the timer counts down, the data for that window is aggregated. If more data points arrive before the timer has completed counting down, the timer is reset. For example, suppose you're monitoring CloudWatch data that arrives fairly infrequently. You're using a window duration of 1 minute and a 3 minute timer. When a CloudWatch data point comes in with a timestamp between 12:00pm and 12:01pm, the timer will start to count down. If no further data points show up for that 12:00-12:01 window, the window will get aggregated 3 minutes later. If a new data point with a timestamp between 12:00 and 12:01 arrives, the timer resets. It keeps resetting every time more data points for that window arrive. The window will not be sent for aggregation until the timer reaches 0. If the timer for a later data point elapses before an earlier data point, the event timer method waits for the earlier timer to elapse before aggregating the later data point. For best results, make sure your timer is equal to or longer than your window duration time. If the timer is less than your window duration and your data flow is inconsistent, then your data may be evaluated before all of your data points arrive. This could cause you to be notified incorrectly. Cadence We recommend you use one of the other two methods. Cadence is our old streaming aggregation method. This method uses New Relic's wall clock time to determine when data is aggregated and evaluated. It doesn't take into account data point timestamps as they arrive. Streaming alerts tools Streaming alerts provide a set of tools to give you greater control over how your data is aggregated before it's evaluated to reduce incorrect notifications you receive. They are: Window duration Delay/timer Loss of signal detection Gap filling Tip This article covers these tools at a conceptual level. You'll find direct instructions on how to use these tools in Create NRQL alert conditions. Window duration In order to make loss of signal detection more effective and to reduce unnecessary notifications, you can customize aggregation windows to the duration that you need. An aggregation window is a specific block of time. We gather data points together in an aggregation window, before evaluating the data. A longer aggregation window can smooth out the data, since an outlier data point will have more data points to be aggregated with, giving it less of an influence on the aggregated data point that is sent for evaluation. When a data point arrives, its timestamp is used to put it in the proper aggregation window. You can set your aggregation window to anything between 30 seconds and 15 minutes. The default is 1 minute. Delay/timer The delay/timer setting controls how long the condition should wait before aggregating the data in the aggregation window. The event flow and cadence methods use delay. Event timer uses timer. The delay default is 2 minutes. The timer default is 1 minute and has a minimum value of 5 seconds. Loss of signal detection Loss of signal occurs when no data matches the NRQL condition over a specific period of time. A loss of signal is caused by different things. The WHERE clause in your NRQL query can filter out data before it's evaluated for violations. It could also mean a service or entity is offline or a periodic job has failed to run and no data is being sent to New Relic. In order to avoid unnecessary notifications, you can choose how long to wait before you're notified by a loss of signal violation. You can use loss of signal detection to open violations and be notified when a signal is lost. Alternately, you can use a loss of signal to close violations for ephemeral services or sporadic data, such as error counts. Gap filling Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with the last value received, a static value, or else do nothing and leave the gap there. The default is None. Gaps in streaming data can be caused by network or host issues, a signal may be sparse, or some signals, such as error counts, may only have data when something is wrong. By filling the gaps with known values, the alert evaluation process can process those gaps and determine how they should affect the loss of signal evaluation. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss and gap filling, see this Explorers Hub post.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 229.45535,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "sections": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "The streaming <em>alerts</em> platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into <em>New</em> <em>Relic</em>. You can use NRQL <em>conditions</em> to control what part of the signal you want to be notified about. Your NRQL condition filters the data that&#x27;s processed"
      },
      "id": "6130c054e7b9d269edb6f21d"
    },
    {
      "sections": [
        "Outlier detection (NRQL alert)",
        "Important",
        "What is outlier detection?",
        "Tip",
        "Example use cases",
        "Notify if load-balanced servers have uneven workload",
        "Notify if load-balanced application has misbehaving instances",
        "Notify of changes in different environments",
        "Notify for timezone-related changes",
        "Create an outlier alert condition",
        "Rules and logic",
        "Details about alert condition logic",
        "NRQL query rules and limits",
        "Zero values for unreturned data"
      ],
      "title": "Outlier detection (NRQL alert)",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert conditions"
      ],
      "external_id": "499fa55abd48a0ccdd897fbdf64ccea2d9f98d11",
      "image": "https://docs.newrelic.com/static/f235d0630576bc2010ff07adc7a69621/f73a1/NRQL_outlier_violations.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/outlier-detection-nrql-alert/",
      "published_at": "2021-12-15T12:44:32Z",
      "updated_at": "2021-11-25T19:50:19Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Alerts offers NRQL conditions in three threshold types: static, baseline, and outlier. This document explains how the outlier threshold type works, gives some example use cases and NRQL queries, and explains how to create an outlier condition. Important NRQL alerts do not affect Alerts policies for a Synthetic monitor. For example, muting a NRQL alert will not mute a Synthetic monitor's alerts. What is outlier detection? In software development and operations, it is common to have a group consisting of members you expect to behave approximately the same. For example: for servers using a load balancer, the traffic to the servers may go up or down, but the traffic for all the servers should remain in a fairly tight grouping. See outlier detection in action in this NerdBytes video (2:51 minutes). The NRQL alert outlier detection feature parses the data returned by your faceted NRQL query and: Looks for the number of expected groups that you specify Looks for outliers (values deviating from a group) based on the sensitivity and time range you set Additionally, for queries that have more than one group, you can choose to be notified when groups start behaving the same. This visual aid will help you understand the types of situations that will trigger a violation and those that won't. For more on the rules and logic behind this calculation, see Outlier detection rules. Tip Note: this feature does not take into account the past behavior of the monitored values; it looks for outliers only in the currently reported data. For an alert type that takes into account past behavior, see Baseline alerting. Example use cases These use cases will help you understand when to use the outlier threshold type. Note that the outlier feature requires a NRQL query with a FACET clause. Notify if load-balanced servers have uneven workload A load balancer divides web traffic approximately evenly across five different servers. You can set a notification to be sent if any server starts getting significantly more or less traffic than the other servers. Example query: SELECT average(cpuPercent) FROM SystemSample WHERE apmApplicationNames = 'MY-APP-NAME' FACET hostname Copy Notify if load-balanced application has misbehaving instances Application instances behind a load balancer should have similar throughput, error rates, and response times. If an instance is in a bad state, or a load balancer is misconfigured, this will not be the case. Detecting one or two bad app instances using aggregate metrics may be difficult if there is not a significant rise in the overall error rate of the application. You can set a notification for when an app instance’s throughput, error rate, or response time deviates too far from the rest of the group. Example query: SELECT average(duration) FROM Transaction WHERE appName = 'MY-APP-NAME' FACET host Copy Notify of changes in different environments An application is deployed in two different environments, with ten application instances in each. One environment is experimental and gets more errors than the other. But the instances that are in the same environment should get approximately the same number of errors. You can set a notification for when an instance starts getting more errors than the other instances in the same environment. Also, you can set a notification for when the two environments start to have the same number of errors as each other. Notify for timezone-related changes The number of logged in users for a company is about the same for each of four applications, but varies significantly by each of the three time zones the company operates in. You can set a notification for when any application starts getting more or less traffic from a certain timezone than the other applications. Sometimes the traffic from the different time zones are the same, so you would set up the alert condition to not be notified if the time zone groups overlap. For more details on how this feature works, see Outlier rules and logic. Create an outlier alert condition To create a NRQL alert that uses outlier detection: When creating a condition, under Select a product, select NRQL. For Threshold type, select Outlier. Create a NRQL query with a FACET clause that returns the values you want to alert on. Depending on how the returned values group together, set the Number of expected groups. Adjust the deviation from the center of the group(s) and the duration that will trigger a violation. Optional: Add a warning threshold and set its deviation. Set any remaining available options and save. Rules and logic Here are the rules and logic behind how outlier detection works: Details about alert condition logic After the condition is created, the query is run once every harvest cycle and the condition is applied. Unlike baseline alerts, outlier detection uses no historical data in its calculation; it's calculated using the currently collected data. Alerts will attempt to divide the data returned from the query into the number of groups selected during condition creation. For each group, the approximate average value is calculated. The allowable deviation you have chosen when creating the condition is centered around that average value. If a member of the group is outside the allowed deviation, it produces a violation. If Trigger when groups overlap has been selected, Alerts detects a convergence of groups. If the condition is looking for two or more groups, and the returned values cannot be separated into that number of distinct groups, then that will produce a violation. This type of “overlap” event is represented on a chart by group bands touching. Because this feature does not take past behavior into account, data is never considered to \"belong\" to a certain group. For example, a value that switches places with another value wouldn't trigger a violation. Additionally, an entire group that moves together also wouldn't trigger a violation. NRQL query rules and limits The NRQL query must be a faceted query. The number of unique values returned must be 500 or less. If the query returns more than this number of values, the condition won't be created. If the query later returns more than this number after being created, the alert will fail. Zero values for unreturned data When a query returns a set of values, only values that are actually returned are taken into account. If a value is not available for calculation (including if it goes from being collected one harvest cycle to not being collected), it is rendered as a zero and is not considered. In other words, the behavior of unreturned zero values will never trigger violations.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.94461,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Outlier detection (NRQL <em>alert</em>)",
        "sections": "Create an outlier <em>alert</em> <em>condition</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "<em>Alerts</em> offers NRQL <em>conditions</em> in three threshold types: static, baseline, and outlier. This document explains how the outlier threshold type works, gives some example use cases and NRQL queries, and explains how to create an outlier condition. Important NRQL <em>alerts</em> do not affect <em>Alerts</em> policies"
      },
      "id": "6130be72196a6793654948e7"
    }
  ],
  "/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/other-condition-types/create-baseline-alert-conditions": [
    {
      "sections": [
        "Create NRQL alert conditions",
        "Tip",
        "Create a NRQL alert condition",
        "Create a condition from a chart",
        "Caution",
        "NRQL alert syntax",
        "Important",
        "Reformatting incompatible NRQL",
        "NRQL alert threshold examples",
        "Alert on specific segments of your data",
        "Alert on Nth percentile of your data",
        "Alert on max, min, avg of your data",
        "Alert on a percentage of your data",
        "Alert on Apdex with any T-value",
        "NRQL conditions and query order of operations",
        "Example: null value returned",
        "Example: zero value returned",
        "Nested aggregation NRQL alerts",
        "Nested queries with a non-faceted innermost query are not currently supported",
        "Queries at all levels must have the same aggregation window size",
        "Signal loss is not yet supported for nested queries",
        "Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported",
        "NRQL condition creation tips",
        "Condition edits can reset condition evaluation",
        "Alert condition types",
        "Sum of query results (limited or intermittent data)",
        "Set the loss of signal threshold",
        "Advanced signal settings",
        "Aggregation window duration",
        "Delay/timer",
        "Fill data gaps"
      ],
      "title": "Create NRQL alert conditions",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert conditions"
      ],
      "external_id": "2872f770a5fc8a802f99b9f848906f7e351ad761",
      "image": "https://docs.newrelic.com/static/eb8e1b2d826f9fd9cf46fe3dd5455217/c1b63/nr1_nrql_alert_conditions.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/",
      "published_at": "2021-12-19T16:01:17Z",
      "updated_at": "2021-12-14T22:49:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use NRQL queries to create alert conditions. Once you've defined your signal, you can further define your warning and critical threshold levels. This determines when an alerts violation is created. Read on to learn more about how to do this. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Click NRQL, and then Next, define thresholds. Tip For more information on key concepts relating to NRQL alert conditions and streaming alerts, see Streaming alerts: key terms and concepts. Create a NRQL alert condition To create a NRQL alert condition for a policy: On one.newrelic.com, in the header click Alerts & AI, then in the left sidebar click Policies. Select an existing policy or click New alert policy to create a new policy. Click Add a condition. Under Select a product click NRQL, and then click Next, define thresholds. Note that editing an existing condition can result in resetting its evaluation. Create a condition from a chart You can use a chart to create a NRQL alert condition. To create a NRQL alerts condition from a chart, click the chart menu , then click Create alert condition. Once you've named and customized your condition, you can add it to an existing policy or create a new one. Caution A small number of our older charts don't include the option to create an alert condition. NRQL alert syntax Here's the basic syntax for creating all NRQL alert conditions. The FACET clause is required for outlier condition types. It's optional for static and baseline. SELECT function(attribute) FROM Event WHERE attribute [comparison] [AND|OR ...] Copy Clause Notes SELECT function(attribute) Required Supported functions that return numbers include: apdex average count latest max min percentage percentile sum uniqueCount Tip If you use the percentile aggregator in a faceted alert condition with many facets, this may cause the following error to appear: An error occurred while fetching chart data. If you see this error, use average instead. FROM data type Required Only one data type can be targeted. Supported data types: Event Metric (RAW data points will be returned) WHERE attribute [comparison] [AND|OR ...] Use the WHERE clause to specify a series of one or more conditions. All the operators are supported. FACET attribute Required for outlier conditions Include an optional FACET clause in your NRQL syntax depending on the threshold type: static or baseline. Use the FACET clause to separate your results by attribute and alert on each attribute independently. No LIMIT clause is allowed, but all queries will receive the maximum number of facets possible. Faceted queries can return a maximum of 5000 values for static and baseline conditions and a maximum of 500 values for outlier conditions. Important If the query returns more than the maximum number of values, the alert condition can't be created. If you create the condition and the query returns more than this number later, the alert will fail. Modify your query so that it returns a fewer number of values. Reformatting incompatible NRQL Some elements of NRQL used in charts don’t make sense in the streaming context of alerts. Here’s a list of the most common incompatible elements and suggestions for reformatting a NRQL alert query to achieve the same effect. Element Notes SINCE and UNTIL Example: SELECT percentile(largestContentfulPaint, 75) FROM PageViewTiming WHERE (appId = 837807) SINCE yesterday Copy NRQL conditions produce a never-ending stream of windowed query results, so the SINCE and UNTIL keywords to scope the query to a point in time are not compatible. As a convenience, we automatically strip SINCE and UNTIL from a query when creating a condition from the context of a chart. TIMESERIES In NRQL queries, the TIMESERIES clause is used to return data as a time series broken out by a specified period of time. For NRQL conditions, the equivalent property of a signal is the aggregation duration window. histogram() The histogram() aggregation function is used to generate histograms. histogram() is not compatible with NRQL alerting: histogram aggregations can not be formatted as a time series. To create an alert from a portion of a histogram (for example, 95th percentile), use the percentile() aggregation function. Multiple aggregation functions Each condition can only target a single aggregated value. To alert on multiple values simultaneously, you’ll need to decompose them into individual conditions within the same policy. Original Query: SELECT count(foo), average(bar), max(baz) from Transaction Copy Decomposed: SELECT count(foo) from Transaction SELECT average(bar) from Transaction SELECT max(baz) from Transaction Copy COMPARE WITH The COMPARE WITH clause is used to compare the values for two different time ranges. This type of query is incompatible with NRQL alerting. We recommend using a Baseline Alert Condition to dynamically detect deviations for a particular signal. SLIDE BY The SLIDE BY clause supports a feature known as sliding windows. With sliding windows, SLIDE BY data is gathered into \"windows\" of time that overlap with each other. These windows can help to smooth out line graphs with a lot of variation in cases where the rolling aggregate (such as a rolling mean) is more important than aggregates from narrow windows of time. Sliding windows are not currently supported in NRQL alerts. LIMIT In NRQL queries, the LIMIT clause is used to control the amount of data a query returns, either the maximum number of facet values returned by FACET queries or the maximum number of items returned by SELECT * queries. LIMIT is not compatible with NRQL alerting: evaluation is always performed on the full result set. NRQL alert threshold examples Here are some common use cases for NRQL conditions. These queries will work for static and baseline condition types. The outlier condition type will require additional FACET clauses. Alert on specific segments of your data Create constrained alerts that target a specific segment of your data, such as a few key customers or a range of data. Use the WHERE clause to define those conditions. SELECT average(duration) FROM Transaction WHERE account_id in (91290, 102021, 20230) Copy SELECT percentile(duration, 95) FROM Transaction WHERE name LIKE 'Controller/checkout/%' Copy Alert on Nth percentile of your data Create alerts when an Nth percentile of your data hits a specified threshold; for example, maintaining SLA service levels. Since we evaluate the NRQL query based on the aggregation window duration, percentiles will be calculated for each duration separately. SELECT percentile(duration, 95) FROM Transaction Copy SELECT percentile(databaseDuration, 75) FROM Transaction Copy Alert on max, min, avg of your data Create alerts when your data hits a certain maximum, minimum, or average; for example, ensuring that a duration or response time does not pass a certain threshold. SELECT max(duration) FROM Transaction Copy SELECT average(duration) FROM Transaction Copy Alert on a percentage of your data Create alerts when a proportion of your data goes above or below a certain threshold. SELECT percentage(count(*), WHERE duration > 2) FROM Transaction Copy SELECT percentage(count(*), WHERE httpResponseCode = '500') FROM Transaction Copy Alert on Apdex with any T-value Create alerts on Apdex, applying your own T-value for certain transactions. For example, get an alert notification when your Apdex for a T-value of 500ms on transactions for production apps goes below 0.8. SELECT apdex(duration, t:0.5) FROM Transaction WHERE appName like '%prod%' Copy NRQL conditions and query order of operations By default, the aggregation window duration is 1 minute, but you can change the window to suit your needs. Whatever the aggregation window, New Relic will collect data for that window using the function in the NRQL condition’s query. The query is parsed and executed by our systems in the following order: FROM clause – which event type needs to be grabbed? WHERE clause – what can be filtered out? SELECT clause – what information needs to be returned from the now-filtered data set? Example: null value returned Let's say this is your alert condition query: SELECT count(*) FROM SyntheticCheck WHERE monitorName = 'My Cool Monitor' AND result = 'FAILURE' Copy If there are no failures for the aggregation window: The system will execute the FROM clause by grabbing all SyntheticCheck events on your account. Then it will execute the WHERE clause to filter through those events by looking only for the ones that match the monitor name and result specified. If there are still events left to scan through after completing the FROM and WHERE operations, the SELECT clause will be executed. If there are no remaining events, the SELECT clause will not be executed. This means that aggregators like count() and uniqueCount() will never return a zero value. When there is a count of 0, the SELECT clause is ignored and no data is returned, resulting in a value of NULL. Example: zero value returned If you have a data source delivering legitimate numeric zeroes, the query will return zero values and not null values. Let's say this is your alert condition query, and that MyCoolEvent is an attribute that can sometimes return a zero value. SELECT average(MyCoolAttribute) FROM MyCoolEvent Copy If, in the aggregation window being evaluated, there's at least one instance of MyCoolEvent and if the average value of all MyCoolAttribute attributes from that window is equal to zero, then a 0 value will be returned. If there are no MyCoolEvent events during that minute, then a NULL will be returned due to the order of operations. Tip For more information about this topic, you can check out our blog post on troubleshooting for zero versus null values. Tip You can determine how null values will be handled by adjusting loss of signal and gap filling settings in the Alert Conditions UI. Tip You can avoid NULL values entirely with a query order of operations shortcut. Do this by using a filter sub-clause, then including all filter elements within that sub-clause. The main body of the query will run and return data, at which point the SELECT clause will then run and apply the filter elements. The query will return a value of 0 if the filter elements result in no matching data. Here's an example: SELECT filter(count(*), WHERE result = 'SUCCESS' AND monitorName = 'My Favorite Monitor') FROM SyntheticCheck Copy Nested aggregation NRQL alerts Nested aggregation queries are a powerful way to query your data. However, they have a few restrictions that are important to note. Nested queries with a non-faceted innermost query are not currently supported Without a FACET, the inner query produces a single result, giving the outer query nothing to aggregate. If you're using a nested query, make sure your inner query is faceted. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu) ​​​​​ Copy Queries at all levels must have the same aggregation window size With an alert aggregation window of 1 minute, the inner query would produce two smaller windows of 30 seconds. In theory, these two windows could be aggregated by the outer query. However, this is not currently supported. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu TIMESERIES 30 seconds)​​ Copy Signal loss is not yet supported for nested queries For more information on signal loss, see NerdGraph API: Loss of signal and gap filling. Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported You can't use a nested query containing the WITH METRIC_FORMAT in the inner query to create NRQL alert conditions. NRQL condition creation tips Here are some tips for creating and using a NRQL condition: Topic Tips Condition types NRQL condition types include static, baseline, and outlier. Create a description For NRQL conditions, you can create a custom description to add to each violation. Descriptions can be enhanced with variable substitution based on metadata in the specific violation. For details, see Description Query results Queries must return a number. The condition evaluates the returned number against the thresholds you've set. Time period NRQL conditions evaluate data based on how it's aggregated, using aggregation windows from 30 seconds to 15 minutes, in increments of 15 seconds. For best results, we recommend using the event flow or event timer aggregation methods. For the cadence aggregation method, the implicit SINCE ... UNTIL clause specifying which minute to evaluate is controlled by your delay/timer setting. Since very recent data may be incomplete, you may want to query data from 3 minutes ago or longer, especially for: Applications that run on multiple hosts. SyntheticCheck data: Timeouts can take 3 minutes, so 5 minutes or more is recommended. Also, if a query will generate intermittent data, consider using the sum of query results option. Lost signal threshold (loss of signal detection) You can use loss of signal detection to alert on when your data (a telemetry signal) should be considered lost. A signal loss can indicate that a service or entity is no longer online or that a periodic job failed to run. You can also use this to make sure that violations for sporadic data, such as error counts, are closed when no signal is coming in. Advanced signal settings These settings give you options for better handling continuous, streaming data signals that may sometimes be missing. These settings include the aggregation window duration, the delay/timer, and an option for filling data gaps. For more on using these, see Advanced signal settings. Condition settings Use the Condition settings to: Create a concise, descriptive condition name. Provide a custom violation description for the condition that will be included in violations and notifications. Add the runbook URL to include your organization's procedures for handling incidents. You may also add this information to the custom violation description. Limits on conditions See the maximum values. Health status In order for a NRQL alert condition health status display to function properly, use a FACET clause to scope each signal to a single entity (for example, FACET hostname or FACET appname). Examples For more information, see: Expected NRQL syntax Examples of NRQL condition queries Condition edits can reset condition evaluation When you edit NRQL alert conditions in some specific ways (detailed below), their evaluations are reset, meaning that any evaluation up until that point is lost, and the evaluation starts over from that point. The two ways this will affect you are: For \"for at least x minutes\" thresholds: because the evaluation window has been reset, there will be a delay of at least x minutes before any violations can be reported. For baseline conditions: the condition starts over again and all baseline learning is lost. The following actions cause an evaluation reset for NRQL conditions: Changing the query Changing the aggregation window, aggregation method, or aggregation delay/timer setting Changing the \"close violations on signal loss\" setting Changing any gap fill settings Changing the baseline direction (if applicable) – higher, lower, or higher/lower Change the threshold value, threshold window, or threshold operator The following actions (along with any other actions not covered in the above list) will not reset the evaluation: Changing the loss of signal time window (expiration duration) Changing the time function (switching \"for at least\" to \"at least once in,\" or vice-versa) Toggling the \"open violation on signal loss\" setting Alert condition types When you create a NRQL alert, you can choose from different types of conditions: NRQL alert condition types Description Static This is the simplest type of NRQL condition. It allows you to create a condition based on a NRQL query that returns a numeric value. Optional: Include a FACET clause. Baseline (Dynamic) Uses a self-adjusting condition based on the past behavior of the monitored values. Uses the same NRQL query form as the static type, including the optional FACET clause. Outlier Looks for group behavior and values that are outliers from those groups. Uses the same NRQL query form as the static type, but requires a FACET clause. Sum of query results (limited or intermittent data) Important Available only for static (basic) condition types. If a query returns intermittent or limited data, it may be difficult to set a meaningful threshold. Missing or limited data will sometimes generate false positives or false negatives. You can use loss of signal, aggregation duration, and gap filling settings to minimize these false notifications. To avoid this problem when using the static threshold type, you can set the selector to sum of query results. This lets you set the alert on an aggregated sum instead of a value from a single harvest cycle. Up to two hours of one-minute data checks can be aggregated. The duration you select determines the width of the rolling sum and the preview chart will update accordingly. Set the loss of signal threshold Loss of signal occurs when no data matches the NRQL condition over a specific period of time. You can set your loss of signal threshold duration and also what happens when the threshold is crossed. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Loss of signal is only available for NRQL conditions. You may also manage these settings using the GraphQL API (recommended), or the REST API. Go here for specific GraphQL API examples. Loss of signal settings: Loss of signal settings include a time duration and two possible actions. Signal loss expiration time UI label: Signal is lost after: GraphQL Node: expiration.expirationDuration Expiration duration is a timer that starts and resets when we receive a data point in the streaming alerts pipeline. If we don't receive another data point before your 'expiration time' expires, we consider that signal to be lost. This can be because no data is being sent to New Relic or the WHERE clause of your NRQL query is filtering that data out before it is streamed to the alerts pipeline. Note that when you have a faceted query, each facet is a signal. So if any one of those signals ends during the duration specified, that will be considered a loss of signal. The loss of signal expiration time is independent of the threshold duration and triggers as soon as the timer expires. The maximum expiration duration is 48 hours. This is helpful when monitoring for the execution of infrequent jobs. The minimum is 30 seconds, but we recommend using at least 3-5 minutes. Loss of signal actions Once a signal is considered lost, you can close open violations, open new violations, or both. Close all current open violations: This closes all open violations that are related to a specific signal. It won't necessarily close all violations for a condition. If you're alerting on an ephemeral service, or on a sporadic signal, you'll want to choose this action to ensure that violations are closed properly. The GraphQL node name for this is \"closeViolationsOnExpiration\" Open new violations: This will open a new violation when the signal is considered lost. These violations will indicate that they are due to a loss of signal. Based on your incident preferences, this should trigger a notification. The graphQL node name for this is \"openViolationOnExpiration\" When you enable both actions, we'll close all open violations first, and then open a new violation for loss of signal. To create a NRQL alert configured with loss of signal detection in the UI: For a policy, when you create a condition, under Select a product, click NRQL, then click Next, define thresholds. Write a NRQL query that returns the values you want to alert on. For Threshold type, select Static or Baseline. Click + Add lost signal threshold, then set the signal expiration duration time in minutes or seconds in the Signal is lost after field. Choose what you want to happen when the signal is lost. You can check one or both of Close all current open violations and Open new \"lost signal\" violation. These control how loss of signal violations will be handled for the condition. Make sure you name your condition before you save it. Violations open due to loss of signal close when the signal comes back. Newly opened lost signal violations will close immediately when new data is evaluated. the condition they belong to expires. By default, conditions expire after 3 days. you manually close the violation with the Close all current open violations option. Tip Loss of signal detection doesn't work on NRQL queries that use nested aggregation or sub-queries. Advanced signal settings When creating a NRQL alert condition, use the advanced signal settings to control streaming alert data and avoid false alarms. When creating a NRQL condition, there are several advanced signal settings: Aggregation window duration Delay/timer Fill data gaps To read an explanation of what these settings are and how they relate to each other, see Streaming alerts concepts. Below are instructions and tips on how to configure them. Aggregation window duration You can set the aggregation window duration to choose how long data is accumulated in a streaming time window before it's aggregated. You can set it to anything between 30 seconds and 15 minutes. The default is one minute. Tip Baseline alert condition thresholds don't support editing the aggregation window. They use the 1 minute default. Delay/timer You can adjust the delay/timer to coordinate our streaming alerting algorithm with your data's behavior. If your data is sparse or inconsistent, you may want to use the event timer aggregation method. For the cadence method, the total supported latency is the sum of the aggregation window duration and the delay. If the data type comes from an APM language agent and is aggregated from many app instances (for example, Transactions, TransactionErrors, etc.), we recommend using the event flow method with the default settings. Important When creating NRQL conditions for data collected from Infrastructure Cloud Integrations such as AWS Cloudwatch or Azure, we recommend that you use the event timer method. Fill data gaps Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with one of these settings: None: (Default) Choose this if you don't want to take any action on empty aggregation windows. On evaluation, an empty aggregation window will reset the threshold duration timer. For example, if a condition says that all aggregation windows must have data points above the threshold for 5 minutes, and 1 of the 5 aggregation windows is empty, then the condition won't be in violation. Custom static value: Choose this if you'd like to insert a custom static value into the empty aggregation windows before they're evaluated. This option has an additional, required parameter of fillValue (as named in the API) that specifies what static value should be used. This defaults to 0. Last known value: This option inserts the last seen value before evaluation occurs. We maintain the state of the last seen value for 2 hours. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss, gap filling, and how to request access to these features, see this Explorers Hub post. Options for editing data gap settings: In the NRQL conditions UI, go to Condition settings > Advanced signal settings > fill data gaps with and choose an option. If using our Nerdgraph API (preferred), this node is located at: actor : account : alerts : nrqlCondition : signal : fillOption | fillValue NerdGraph is our recommended API for this but if you're using our REST API, you can find this setting in the REST API explorer under the \"signal\" section of the Alert NRQL conditions API.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 251.85574,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create NRQL <em>alert</em> <em>conditions</em>",
        "sections": "Create NRQL <em>alert</em> <em>conditions</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "You can use NRQL queries to create <em>alert</em> <em>conditions</em>. Once you&#x27;ve defined your signal, you can further define your warning and critical threshold levels. This determines when an <em>alerts</em> violation is created. Read on to learn more about how to do this. Go to one.newrelic.com, click <em>Alerts</em> &amp; AI"
      },
      "id": "603ef04864441fbc114e8883"
    },
    {
      "sections": [
        "Streaming alerts: key terms and concepts",
        "Why it matters",
        "Streaming alerts process and descriptions",
        "Choose your aggregation method",
        "Event flow",
        "Caution",
        "Event timer",
        "Cadence",
        "Streaming alerts tools",
        "Tip",
        "Window duration",
        "Delay/timer",
        "Loss of signal detection",
        "Gap filling"
      ],
      "title": "Streaming alerts: key terms and concepts",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Get started"
      ],
      "external_id": "144c7f0721ec2acb1e8ac8aa06142911ea552aef",
      "image": "https://docs.newrelic.com/static/39158bd84483adf9516bb7ff5058c9bd/d30ee/streaming-alerts.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/streaming-alerts-key-terms-concepts/",
      "published_at": "2021-12-15T12:47:58Z",
      "updated_at": "2021-12-15T12:47:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The streaming alerts platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into New Relic. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that's processed by the streaming algorithm. There are three methods for aggregating the data filtered through your NRQL condition: Event flow (default) Event timer Cadence This short video describes the three aggregation method (5:31). Why it matters Understanding how streaming alerts works will help you fine-tune your NRQL conditions to be notified about what's important to you. Only data that matches the conditions of the NRQL WHERE clause is alerted on. For more details on each step of the process, see Streaming alerts process and descriptions. As data streams into New Relic, it's filtered by the NRQL condition. Before data is evaluated, it must meet the criteria defined by the NRQL query's WHERE clause. Instead of evaluating that data immediately for violations, the NRQL alert conditions collect the data over a period of time known as the aggregation window. An additional delay/timer allows for slower data points to arrive before the window is aggregated. Once the delay/timer time has elapsed, New Relic aggregates the data into a single data point. Alerts then evaluates the data point using the NRQL condition in order to determine whether it meets the violation threshold criteria. Even if a data point meets the criteria for a violation, a violation may not be triggered. A violation is only triggered when data points consistently meet the threshold criteria over a period of time. This is the threshold duration. If the data points are in violation for an entire threshold duration, we'll send you a notification based on your policy settings. All of these configurable delays give you more control over how you're alerted on sporadic and missing data. Streaming alerts process and descriptions Process Description Streaming data All data coming into New Relic. WHERE clause Filters all incoming streaming data. We only monitor for alerts on data that makes it through this filter. Aggregation methods One of three methods that control how data is collected before it's evaluated. They are: Event flow (Default) Event timer Cadence Aggregation window Data with timestamps that fall within this window will be aggregated and then evaluated. Delay/timer A time delay to ensure all data points have arrived in the aggregation window before aggregation occurs. Aggregated data Data in the aggregated window is collapsed to a single data point for alert evaluation. Evaluation The data point is evaluated by the NRQL condition, which is trigged by each incoming aggregated data point. Threshold duration A specific duration that determines if a violation is created. If your specified NRQL condition meets the threshold criteria over the threshold duration, a violation occurs. When a data point lacks data, a custom value is inserted to fill the gap. Choose your aggregation method You can choose between three different aggregation methods, depending on your needs. Event flow (default) works best for data that comes in frequently and mostly in order. Event timer works best for data that arrives infrequently in batches, such as cloud integration data or infrequent error logs. Cadence is our original and inferior aggregation method. It aggregates data on specific time intervals as detected by New Relic's internal wall clock, regardless of data timestamps. Event flow Event flow aggregates a window of data when the first data point arrives in a subsequent window. The custom delay defines which subsequent window data will start to populate to trigger aggregation of the current window. A custom delay provides extra time for data to arrive. These times are based on the data's timestamps and not New Relic's wall clock time. For example, suppose you're monitoring CPU usage in window durations of 1 minute and a 3 minute delay. When a CPU usage data point comes in with a timestamp between 12:00pm and 12:01pm, event flow will not aggregate that window until a data point shows up with a timestamp between 12:04pm and 12:05pm. When event flow receives the first data point with a timestamp of 12:04pm or later, it sends the 12:00 to 12:01 data to be aggregated. Caution If you expect your data points to arrive more than 30 minutes apart, please use the Event Timer method described below. Event timer Like event flow, event timer will only aggregate data for a given window when data arrives for that window. When a data point arrives for an aggregation window, a timer dedicated to that window starts to count down. If no further data arrives before the timer counts down, the data for that window is aggregated. If more data points arrive before the timer has completed counting down, the timer is reset. For example, suppose you're monitoring CloudWatch data that arrives fairly infrequently. You're using a window duration of 1 minute and a 3 minute timer. When a CloudWatch data point comes in with a timestamp between 12:00pm and 12:01pm, the timer will start to count down. If no further data points show up for that 12:00-12:01 window, the window will get aggregated 3 minutes later. If a new data point with a timestamp between 12:00 and 12:01 arrives, the timer resets. It keeps resetting every time more data points for that window arrive. The window will not be sent for aggregation until the timer reaches 0. If the timer for a later data point elapses before an earlier data point, the event timer method waits for the earlier timer to elapse before aggregating the later data point. For best results, make sure your timer is equal to or longer than your window duration time. If the timer is less than your window duration and your data flow is inconsistent, then your data may be evaluated before all of your data points arrive. This could cause you to be notified incorrectly. Cadence We recommend you use one of the other two methods. Cadence is our old streaming aggregation method. This method uses New Relic's wall clock time to determine when data is aggregated and evaluated. It doesn't take into account data point timestamps as they arrive. Streaming alerts tools Streaming alerts provide a set of tools to give you greater control over how your data is aggregated before it's evaluated to reduce incorrect notifications you receive. They are: Window duration Delay/timer Loss of signal detection Gap filling Tip This article covers these tools at a conceptual level. You'll find direct instructions on how to use these tools in Create NRQL alert conditions. Window duration In order to make loss of signal detection more effective and to reduce unnecessary notifications, you can customize aggregation windows to the duration that you need. An aggregation window is a specific block of time. We gather data points together in an aggregation window, before evaluating the data. A longer aggregation window can smooth out the data, since an outlier data point will have more data points to be aggregated with, giving it less of an influence on the aggregated data point that is sent for evaluation. When a data point arrives, its timestamp is used to put it in the proper aggregation window. You can set your aggregation window to anything between 30 seconds and 15 minutes. The default is 1 minute. Delay/timer The delay/timer setting controls how long the condition should wait before aggregating the data in the aggregation window. The event flow and cadence methods use delay. Event timer uses timer. The delay default is 2 minutes. The timer default is 1 minute and has a minimum value of 5 seconds. Loss of signal detection Loss of signal occurs when no data matches the NRQL condition over a specific period of time. A loss of signal is caused by different things. The WHERE clause in your NRQL query can filter out data before it's evaluated for violations. It could also mean a service or entity is offline or a periodic job has failed to run and no data is being sent to New Relic. In order to avoid unnecessary notifications, you can choose how long to wait before you're notified by a loss of signal violation. You can use loss of signal detection to open violations and be notified when a signal is lost. Alternately, you can use a loss of signal to close violations for ephemeral services or sporadic data, such as error counts. Gap filling Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with the last value received, a static value, or else do nothing and leave the gap there. The default is None. Gaps in streaming data can be caused by network or host issues, a signal may be sparse, or some signals, such as error counts, may only have data when something is wrong. By filling the gaps with known values, the alert evaluation process can process those gaps and determine how they should affect the loss of signal evaluation. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss and gap filling, see this Explorers Hub post.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 229.45535,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "sections": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "The streaming <em>alerts</em> platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into <em>New</em> <em>Relic</em>. You can use NRQL <em>conditions</em> to control what part of the signal you want to be notified about. Your NRQL condition filters the data that&#x27;s processed"
      },
      "id": "6130c054e7b9d269edb6f21d"
    },
    {
      "sections": [
        "Outlier detection (NRQL alert)",
        "Important",
        "What is outlier detection?",
        "Tip",
        "Example use cases",
        "Notify if load-balanced servers have uneven workload",
        "Notify if load-balanced application has misbehaving instances",
        "Notify of changes in different environments",
        "Notify for timezone-related changes",
        "Create an outlier alert condition",
        "Rules and logic",
        "Details about alert condition logic",
        "NRQL query rules and limits",
        "Zero values for unreturned data"
      ],
      "title": "Outlier detection (NRQL alert)",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert conditions"
      ],
      "external_id": "499fa55abd48a0ccdd897fbdf64ccea2d9f98d11",
      "image": "https://docs.newrelic.com/static/f235d0630576bc2010ff07adc7a69621/f73a1/NRQL_outlier_violations.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/outlier-detection-nrql-alert/",
      "published_at": "2021-12-15T12:44:32Z",
      "updated_at": "2021-11-25T19:50:19Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Alerts offers NRQL conditions in three threshold types: static, baseline, and outlier. This document explains how the outlier threshold type works, gives some example use cases and NRQL queries, and explains how to create an outlier condition. Important NRQL alerts do not affect Alerts policies for a Synthetic monitor. For example, muting a NRQL alert will not mute a Synthetic monitor's alerts. What is outlier detection? In software development and operations, it is common to have a group consisting of members you expect to behave approximately the same. For example: for servers using a load balancer, the traffic to the servers may go up or down, but the traffic for all the servers should remain in a fairly tight grouping. See outlier detection in action in this NerdBytes video (2:51 minutes). The NRQL alert outlier detection feature parses the data returned by your faceted NRQL query and: Looks for the number of expected groups that you specify Looks for outliers (values deviating from a group) based on the sensitivity and time range you set Additionally, for queries that have more than one group, you can choose to be notified when groups start behaving the same. This visual aid will help you understand the types of situations that will trigger a violation and those that won't. For more on the rules and logic behind this calculation, see Outlier detection rules. Tip Note: this feature does not take into account the past behavior of the monitored values; it looks for outliers only in the currently reported data. For an alert type that takes into account past behavior, see Baseline alerting. Example use cases These use cases will help you understand when to use the outlier threshold type. Note that the outlier feature requires a NRQL query with a FACET clause. Notify if load-balanced servers have uneven workload A load balancer divides web traffic approximately evenly across five different servers. You can set a notification to be sent if any server starts getting significantly more or less traffic than the other servers. Example query: SELECT average(cpuPercent) FROM SystemSample WHERE apmApplicationNames = 'MY-APP-NAME' FACET hostname Copy Notify if load-balanced application has misbehaving instances Application instances behind a load balancer should have similar throughput, error rates, and response times. If an instance is in a bad state, or a load balancer is misconfigured, this will not be the case. Detecting one or two bad app instances using aggregate metrics may be difficult if there is not a significant rise in the overall error rate of the application. You can set a notification for when an app instance’s throughput, error rate, or response time deviates too far from the rest of the group. Example query: SELECT average(duration) FROM Transaction WHERE appName = 'MY-APP-NAME' FACET host Copy Notify of changes in different environments An application is deployed in two different environments, with ten application instances in each. One environment is experimental and gets more errors than the other. But the instances that are in the same environment should get approximately the same number of errors. You can set a notification for when an instance starts getting more errors than the other instances in the same environment. Also, you can set a notification for when the two environments start to have the same number of errors as each other. Notify for timezone-related changes The number of logged in users for a company is about the same for each of four applications, but varies significantly by each of the three time zones the company operates in. You can set a notification for when any application starts getting more or less traffic from a certain timezone than the other applications. Sometimes the traffic from the different time zones are the same, so you would set up the alert condition to not be notified if the time zone groups overlap. For more details on how this feature works, see Outlier rules and logic. Create an outlier alert condition To create a NRQL alert that uses outlier detection: When creating a condition, under Select a product, select NRQL. For Threshold type, select Outlier. Create a NRQL query with a FACET clause that returns the values you want to alert on. Depending on how the returned values group together, set the Number of expected groups. Adjust the deviation from the center of the group(s) and the duration that will trigger a violation. Optional: Add a warning threshold and set its deviation. Set any remaining available options and save. Rules and logic Here are the rules and logic behind how outlier detection works: Details about alert condition logic After the condition is created, the query is run once every harvest cycle and the condition is applied. Unlike baseline alerts, outlier detection uses no historical data in its calculation; it's calculated using the currently collected data. Alerts will attempt to divide the data returned from the query into the number of groups selected during condition creation. For each group, the approximate average value is calculated. The allowable deviation you have chosen when creating the condition is centered around that average value. If a member of the group is outside the allowed deviation, it produces a violation. If Trigger when groups overlap has been selected, Alerts detects a convergence of groups. If the condition is looking for two or more groups, and the returned values cannot be separated into that number of distinct groups, then that will produce a violation. This type of “overlap” event is represented on a chart by group bands touching. Because this feature does not take past behavior into account, data is never considered to \"belong\" to a certain group. For example, a value that switches places with another value wouldn't trigger a violation. Additionally, an entire group that moves together also wouldn't trigger a violation. NRQL query rules and limits The NRQL query must be a faceted query. The number of unique values returned must be 500 or less. If the query returns more than this number of values, the condition won't be created. If the query later returns more than this number after being created, the alert will fail. Zero values for unreturned data When a query returns a set of values, only values that are actually returned are taken into account. If a value is not available for calculation (including if it goes from being collected one harvest cycle to not being collected), it is rendered as a zero and is not considered. In other words, the behavior of unreturned zero values will never trigger violations.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.94461,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Outlier detection (NRQL <em>alert</em>)",
        "sections": "Create an outlier <em>alert</em> <em>condition</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "<em>Alerts</em> offers NRQL <em>conditions</em> in three threshold types: static, baseline, and outlier. This document explains how the outlier threshold type works, gives some example use cases and NRQL queries, and explains how to create an outlier condition. Important NRQL <em>alerts</em> do not affect <em>Alerts</em> policies"
      },
      "id": "6130be72196a6793654948e7"
    }
  ],
  "/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/rest-api-alerts/alerts-conditions-api-field-names": [
    {
      "sections": [
        "Streaming alerts: key terms and concepts",
        "Why it matters",
        "Streaming alerts process and descriptions",
        "Choose your aggregation method",
        "Event flow",
        "Caution",
        "Event timer",
        "Cadence",
        "Streaming alerts tools",
        "Tip",
        "Window duration",
        "Delay/timer",
        "Loss of signal detection",
        "Gap filling"
      ],
      "title": "Streaming alerts: key terms and concepts",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Get started"
      ],
      "external_id": "144c7f0721ec2acb1e8ac8aa06142911ea552aef",
      "image": "https://docs.newrelic.com/static/39158bd84483adf9516bb7ff5058c9bd/d30ee/streaming-alerts.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/streaming-alerts-key-terms-concepts/",
      "published_at": "2021-12-15T12:47:58Z",
      "updated_at": "2021-12-15T12:47:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The streaming alerts platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into New Relic. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that's processed by the streaming algorithm. There are three methods for aggregating the data filtered through your NRQL condition: Event flow (default) Event timer Cadence This short video describes the three aggregation method (5:31). Why it matters Understanding how streaming alerts works will help you fine-tune your NRQL conditions to be notified about what's important to you. Only data that matches the conditions of the NRQL WHERE clause is alerted on. For more details on each step of the process, see Streaming alerts process and descriptions. As data streams into New Relic, it's filtered by the NRQL condition. Before data is evaluated, it must meet the criteria defined by the NRQL query's WHERE clause. Instead of evaluating that data immediately for violations, the NRQL alert conditions collect the data over a period of time known as the aggregation window. An additional delay/timer allows for slower data points to arrive before the window is aggregated. Once the delay/timer time has elapsed, New Relic aggregates the data into a single data point. Alerts then evaluates the data point using the NRQL condition in order to determine whether it meets the violation threshold criteria. Even if a data point meets the criteria for a violation, a violation may not be triggered. A violation is only triggered when data points consistently meet the threshold criteria over a period of time. This is the threshold duration. If the data points are in violation for an entire threshold duration, we'll send you a notification based on your policy settings. All of these configurable delays give you more control over how you're alerted on sporadic and missing data. Streaming alerts process and descriptions Process Description Streaming data All data coming into New Relic. WHERE clause Filters all incoming streaming data. We only monitor for alerts on data that makes it through this filter. Aggregation methods One of three methods that control how data is collected before it's evaluated. They are: Event flow (Default) Event timer Cadence Aggregation window Data with timestamps that fall within this window will be aggregated and then evaluated. Delay/timer A time delay to ensure all data points have arrived in the aggregation window before aggregation occurs. Aggregated data Data in the aggregated window is collapsed to a single data point for alert evaluation. Evaluation The data point is evaluated by the NRQL condition, which is trigged by each incoming aggregated data point. Threshold duration A specific duration that determines if a violation is created. If your specified NRQL condition meets the threshold criteria over the threshold duration, a violation occurs. When a data point lacks data, a custom value is inserted to fill the gap. Choose your aggregation method You can choose between three different aggregation methods, depending on your needs. Event flow (default) works best for data that comes in frequently and mostly in order. Event timer works best for data that arrives infrequently in batches, such as cloud integration data or infrequent error logs. Cadence is our original and inferior aggregation method. It aggregates data on specific time intervals as detected by New Relic's internal wall clock, regardless of data timestamps. Event flow Event flow aggregates a window of data when the first data point arrives in a subsequent window. The custom delay defines which subsequent window data will start to populate to trigger aggregation of the current window. A custom delay provides extra time for data to arrive. These times are based on the data's timestamps and not New Relic's wall clock time. For example, suppose you're monitoring CPU usage in window durations of 1 minute and a 3 minute delay. When a CPU usage data point comes in with a timestamp between 12:00pm and 12:01pm, event flow will not aggregate that window until a data point shows up with a timestamp between 12:04pm and 12:05pm. When event flow receives the first data point with a timestamp of 12:04pm or later, it sends the 12:00 to 12:01 data to be aggregated. Caution If you expect your data points to arrive more than 30 minutes apart, please use the Event Timer method described below. Event timer Like event flow, event timer will only aggregate data for a given window when data arrives for that window. When a data point arrives for an aggregation window, a timer dedicated to that window starts to count down. If no further data arrives before the timer counts down, the data for that window is aggregated. If more data points arrive before the timer has completed counting down, the timer is reset. For example, suppose you're monitoring CloudWatch data that arrives fairly infrequently. You're using a window duration of 1 minute and a 3 minute timer. When a CloudWatch data point comes in with a timestamp between 12:00pm and 12:01pm, the timer will start to count down. If no further data points show up for that 12:00-12:01 window, the window will get aggregated 3 minutes later. If a new data point with a timestamp between 12:00 and 12:01 arrives, the timer resets. It keeps resetting every time more data points for that window arrive. The window will not be sent for aggregation until the timer reaches 0. If the timer for a later data point elapses before an earlier data point, the event timer method waits for the earlier timer to elapse before aggregating the later data point. For best results, make sure your timer is equal to or longer than your window duration time. If the timer is less than your window duration and your data flow is inconsistent, then your data may be evaluated before all of your data points arrive. This could cause you to be notified incorrectly. Cadence We recommend you use one of the other two methods. Cadence is our old streaming aggregation method. This method uses New Relic's wall clock time to determine when data is aggregated and evaluated. It doesn't take into account data point timestamps as they arrive. Streaming alerts tools Streaming alerts provide a set of tools to give you greater control over how your data is aggregated before it's evaluated to reduce incorrect notifications you receive. They are: Window duration Delay/timer Loss of signal detection Gap filling Tip This article covers these tools at a conceptual level. You'll find direct instructions on how to use these tools in Create NRQL alert conditions. Window duration In order to make loss of signal detection more effective and to reduce unnecessary notifications, you can customize aggregation windows to the duration that you need. An aggregation window is a specific block of time. We gather data points together in an aggregation window, before evaluating the data. A longer aggregation window can smooth out the data, since an outlier data point will have more data points to be aggregated with, giving it less of an influence on the aggregated data point that is sent for evaluation. When a data point arrives, its timestamp is used to put it in the proper aggregation window. You can set your aggregation window to anything between 30 seconds and 15 minutes. The default is 1 minute. Delay/timer The delay/timer setting controls how long the condition should wait before aggregating the data in the aggregation window. The event flow and cadence methods use delay. Event timer uses timer. The delay default is 2 minutes. The timer default is 1 minute and has a minimum value of 5 seconds. Loss of signal detection Loss of signal occurs when no data matches the NRQL condition over a specific period of time. A loss of signal is caused by different things. The WHERE clause in your NRQL query can filter out data before it's evaluated for violations. It could also mean a service or entity is offline or a periodic job has failed to run and no data is being sent to New Relic. In order to avoid unnecessary notifications, you can choose how long to wait before you're notified by a loss of signal violation. You can use loss of signal detection to open violations and be notified when a signal is lost. Alternately, you can use a loss of signal to close violations for ephemeral services or sporadic data, such as error counts. Gap filling Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with the last value received, a static value, or else do nothing and leave the gap there. The default is None. Gaps in streaming data can be caused by network or host issues, a signal may be sparse, or some signals, such as error counts, may only have data when something is wrong. By filling the gaps with known values, the alert evaluation process can process those gaps and determine how they should affect the loss of signal evaluation. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss and gap filling, see this Explorers Hub post.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 177.89925,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "sections": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "The streaming <em>alerts</em> platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into <em>New</em> <em>Relic</em>. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that&#x27;s processed"
      },
      "id": "6130c054e7b9d269edb6f21d"
    },
    {
      "sections": [
        "REST API calls for alerts",
        "Important",
        "Available data and functions via API",
        "Conditions excluded from the REST API",
        "Alert policies",
        "Create new policies",
        "Update policies",
        "Delete existing policies",
        "List existing policies",
        "Notification channels",
        "Create new notification channels",
        "Email channel",
        "OpsGenie channel",
        "Slack channel",
        "VictorOps channel",
        "PagerDuty channel",
        "Webhook (json) channel",
        "Webhook (x-www-form-urlencoded) channel",
        "Delete existing notification channels",
        "List existing notification channels",
        "Update notification channels associated with policies",
        "Delete notification channels associated with policies",
        "Conditions for APM, browser, mobile",
        "Create conditions for policies",
        "Update conditions for policies",
        "Delete conditions from policies",
        "List existing conditions for policies",
        "Conditions for NRQL",
        "Create NRQL conditions for policies",
        "Update NRQL conditions for policies",
        "Delete NRQL conditions for policies",
        "List existing NRQL conditions for policies",
        "Conditions for external services",
        "External services: Create conditions for policies",
        "External services: Update conditions for policies",
        "External services: Delete conditions from policies",
        "External services: List existing conditions for policies",
        "Conditions for Synthetic monitoring",
        "Synthetics: Create conditions for policies",
        "Synthetic monitoring: Update conditions for policies",
        "Synthetic monitoring: Delete conditions from policies",
        "Synthetic monitoring: List existing conditions for policies",
        "Alert activity: Events, violations, incidents",
        "List Events",
        "List Violations",
        "Tip",
        "List Incidents",
        "Show Incident",
        "Acknowledge Incident",
        "Close Incident",
        "Alert entity conditions",
        "List condition by entity",
        "Add an entity to a condition",
        "Remove an entity from a condition"
      ],
      "title": "REST API calls for alerts",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "REST API alerts"
      ],
      "external_id": "af566e5c2f0a695c1146b5531849bb49248ea0ec",
      "image": "",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/rest-api-alerts/rest-api-calls-alerts/",
      "published_at": "2021-12-14T22:59:19Z",
      "updated_at": "2021-10-31T13:25:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our REST API (v2) allows you to configure settings for alerts. The API Explorer also includes the curl request format, available parameters, potential response status codes, and JSON response structure for each of the available API calls. You can also create alert conditions in the UI. Important For infrastructure alerting, see REST API for infrastructure monitoring alerts. Available data and functions via API REST API functions Comments View account data In general, any role can use a user key or REST API key with GET > List functions to view alerts data. The account Owner and Admins may use their API key. List output will be paginated. Available functions include: Alert policies Notification channels Conditions for APM, browser, and mobile (Some limitations apply.) Conditions for external services Conditions for Synthetic monitoring Conditions for NRQL (Some limitations apply.) Events Violations Incidents Maintain account data You may have an Owner or Admin role in your account and a user key or have a custom role that grants permissions to manage Alerts and a user key in order to use any maintenance function, including POST > Create, PUT > Add, PUT > Update, and DELETE. Conditions excluded from the REST API These types of conditions do not have available endpoints in the API: APM: Web transaction percentiles, conditions targeting labels, and baselines NRQL: Baselines Alert policies These API functions include links to the API Explorer, where you can create, delete, or list policies. Important If your account hosts data in the EU data center, ensure you're using the proper API endpoints for EU region accounts. Create new policies To add new policies, use your user key and include these two values in the API call: Required values Definition Incident incident_preference Determines how Alerts will create incidents and group violations. This must be one of the following: PER_POLICY (default): Roll up by policy. PER_CONDITION: Roll up by condition. PER_CONDITION_AND_TARGET: Roll up by target and condition. Policy name The policy name is required. Leaving it unchanged will create a policy called string. API Explorer: Alerts Policies > POST > Create curl -X POST 'https://api.newrelic.com/v2/alerts_policies.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -d \\ '{ \"policy\": { \"incident_preference\": \"string\", \"name\": \"string\" } }' Copy Update policies Use this API to update an existing policy's name and incident_preference. You'll need your user key. Required values Definition id Required. To find a policy's ID, use either of these options: From the UI: On a policy's UI page, find the ID under the policy name. With the API: Use the List policies API. Incident incident_preference Determines how alerts will create incidents and group violations. Must be one of the following: PER_POLICY (default): Roll up by policy. PER_CONDITION: Roll up by condition. PER_CONDITION_AND_TARGET: Roll up by target and condition. Policy name Required. If you do not change the name, it defaults to a policy called string. To find a policy's exact name, use the List policies API. API Explorer: Alerts Policies > PUT > Update curl -X PUT 'https://api.newrelic.com/v2/alerts_policies/{id}.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -d \\ '{ \"policy\": { \"incident_preference\": \"string\", \"name\": \"string\" } }' Copy Delete existing policies To delete an existing policy, use your user key, and include the policy_id (available from API Explorer: Alerts Policies > GET > List) in the API call: API Explorer: Alerts Policies > DELETE > Delete curl -X DELETE 'https://api.newrelic.com/v2/alerts_policies/$POLICY_ID.json' \\ -H 'Api-Key:$API_KEY' -i Copy List existing policies To view a list of existing policies for your account, use your user key, and include these optional values in the API call: Optional policy name filter Optional pagination value API Explorer: Alerts Policies > GET > List curl -X GET 'https://api.newrelic.com/v2/alerts_policies.json' \\ -H 'Api-Key:$API_KEY' -i Copy Notification channels These API functions include links to the API Explorer, where you can create, delete, or list Alerts notification channels. Important If your account hosts data in the EU data center, ensure you are using the proper API endpoints for EU region accounts. Create new notification channels To add new notification channels, use your user key and include these values in the API call: New channel's name Type of channel Configuration values The API Explorer shows the format for required configuration values for each type of notification channel. API Explorer: Alerts Channels > POST > Create curl -X POST 'https://api.newrelic.com/v2/alerts_channels.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -d \\ '{ \"channel\": { \"name\": \"string\", \"type\": \"string\", \"configuration\": \"hash\" } }' Copy The \"hash\" place holder in this example must be replaced by one of the following JSON blocks with the appropriate values substituted: Email channel { \"recipients\" : \"test@google.com\", \"include_json_attachment\" : true } Copy OpsGenie channel { \"api_key\": \"abc123\", \"teams\": \"team1\", \"tags\": \"tag1\", \"recipients\": \"me@me.com\" } Copy Slack channel { \"url\": \"http://example.com\", \"channel\": \"channel1\" } Copy VictorOps channel { \"key\": \"mykey\", \"route_key\": \"theroute\" } Copy PagerDuty channel { \"service_key\": \"myservicekey\" } Copy Webhook (json) channel This example shows the default payload inserted. (The payload attribute is optional.) The payload can be customized to have different keys than the ones shown and will be sent with the $ prefixed attributes interpolated prior to delivery. Important The condition_id is deprecated. Instead, use condition_family_id. { \"base_url\": \"http://example.com\", \"auth_username\": \"username\", \"auth_password\": \"password\", \"payload_type\": \"application/json\", \"headers\": { \"header1\": \"test\", \"header2\": \"test\" } \"payload\": { \"account_id\": \"$ACCOUNT_ID\", \"account_name\": \"$ACCOUNT_NAME\", \"closed_violations_count_critical\": \"$CLOSED_VIOLATIONS_COUNT_CRITICAL\", \"closed_violations_count_warning\": \"$CLOSED_VIOLATIONS_COUNT_WARNING\", \"condition_family_id\": \"$CONDITION_FAMILY_ID\", \"condition_id\": \"$CONDITION_ID\", \"condition_name\": \"$CONDITION_NAME\", \"current_state\": \"$EVENT_STATE\", \"details\": \"$EVENT_DETAILS\", \"duration\": \"$DURATION\", \"event_type\": \"$EVENT_TYPE\", \"incident_acknowledge_url\": \"$INCIDENT_ACKNOWLEDGE_URL\", \"incident_id\": \"$INCIDENT_ID\", \"incident_url\": \"$INCIDENT_URL\", \"open_violations_count_critical\": \"$OPEN_VIOLATIONS_COUNT_CRITICAL\", \"open_violations_count_warning\": \"$OPEN_VIOLATIONS_COUNT_WARNING\", \"owner\": \"$EVENT_OWNER\", \"policy_name\": \"$POLICY_NAME\", \"policy_url\": \"$POLICY_URL\", \"runbook_url\": \"$RUNBOOK_URL\", \"severity\": \"$SEVERITY\", \"targets\": \"$TARGETS\", \"timestamp\": \"$TIMESTAMP\", \"violation_callback_url\": \"$VIOLATION_CALLBACK_URL\", \"violation_chart_url\": \"$VIOLATION_CHART_URL\" } } Copy Webhook (x-www-form-urlencoded) channel This example shows the default payload inserted. (The payload attribute is optional.) The payload can be customized to have different keys than the ones shown and will be sent with the $ prefixed attributes interpolated prior to delivery. Important The condition_id is deprecated. Instead, use condition_family_id. { \"base_url\": \"http://example.com\", \"auth_username\": \"username\", \"auth_password\": \"password\", \"payload_type\": \"application/x-www-form-urlencoded\", \"headers\": { \"header1\": \"test\", \"header2\": \"test\" }, \"payload\": { \"account_id\": \"$ACCOUNT_ID\", \"account_name\": \"$ACCOUNT_NAME\", \"closed_violations_count_critical\": \"$CLOSED_VIOLATIONS_COUNT_CRITICAL\", \"closed_violations_count_warning\": \"$CLOSED_VIOLATIONS_COUNT_WARNING\", \"condition_family_id\": \"$CONDITION_FAMILY_ID\", \"condition_id\": \"$CONDITION_ID\", \"condition_name\": \"$CONDITION_NAME\", \"current_state\": \"$EVENT_STATE\", \"details\": \"$EVENT_DETAILS\", \"duration\": \"$DURATION\", \"event_type\": \"$EVENT_TYPE\", \"incident_acknowledge_url\": \"$INCIDENT_ACKNOWLEDGE_URL\", \"incident_id\": \"$INCIDENT_ID\", \"incident_url\": \"$INCIDENT_URL\", \"open_violations_count_critical\": \"$OPEN_VIOLATIONS_COUNT_CRITICAL\", \"open_violations_count_warning\": \"$OPEN_VIOLATIONS_COUNT_WARNING\", \"owner\": \"$EVENT_OWNER\", \"policy_name\": \"$POLICY_NAME\", \"policy_url\": \"$POLICY_URL\", \"runbook_url\": \"$RUNBOOK_URL\", \"severity\": \"$SEVERITY\", \"targets\": \"$TARGETS\", \"timestamp\": \"$TIMESTAMP\", \"violation_callback_url\": \"$VIOLATION_CALLBACK_URL\", \"violation_chart_url\": \"$VIOLATION_CHART_URL\" } } Copy Delete existing notification channels To delete an existing notification channels, use your user key, and include the channel_id (available from API Explorer: Alerts Channels > GET > List) in the API call: API Explorer: Alerts Channels > DELETE > Delete curl -X DELETE 'https://api.newrelic.com/v2/alerts_channels/{channel_id}.json' \\ -H 'Api-Key:$API_KEY' -i Copy List existing notification channels To view a list of existing notification channels for your account, use your user key and an optional pagination value in the API call. API Explorer: Alerts Channels > GET > List curl -X GET 'https://api.newrelic.com/v2/alerts_channels.json' \\ -H 'Api-Key:$API_KEY' -i Copy Update notification channels associated with policies You can associate a policy with one or more notification channels. You can also associate a notification channel with one or more policies. To add notification channels to policies, use your user key and these values in the API call: A policy_id value (available from API Explorer: Alerts Policies > GET > List) One or more channel_id values in an array, separated by commas or a new line (available from API Explorer: Alerts Channels > GET > List) API Explorer: Alerts Policy Channels > PUT > Update curl -X PUT 'https://api.newrelic.com/v2/alerts_policy_channels.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -G -d 'policy_id=$POLICY_ID&channel_ids=channel_id' Copy Delete notification channels associated with policies You can associate a policy with one or more notification channels. You can also associate a notification channel with one or more alert policies. To remove a policy from a channel, or to remove a channel from a policy, use your user key and these values in the API call: The policy_id (available from API Explorer: Alerts Policies > GET > List) The channel_id (available from API Explorer: Alerts Channels > GET > List) API Explorer: Alerts Policy Channels > DELETE > Delete curl -X DELETE 'https://api.newrelic.com/v2/alerts_policy_channels.json' \\ -H 'Api-Key:$API_KEY' -i \\ -d 'channel_id=CHANNEL_ID&policy_id=POLICY_ID' Copy Conditions for APM, browser, mobile These API functions include links to the API Explorer, where you can create, update, delete, or list policy conditions. These calls are for conditions with these types of entities: APM: Apps and key transactions Browser: Apps and key transactions Mobile: Mobile apps Exception: The following APM conditions are not available from this API endpoint: Conditions targeting labels (dynamic targeting) Web transaction percentile conditions Baseline conditions Important The API Explorer provides information about other types of conditions using separate endpoints, including external services (APM and mobile), and synthetic monitoring. Consider all types of alert conditions when searching or updating. If your account hosts data in the EU data center, ensure you are using the proper API endpoints for EU region accounts. Create conditions for policies To add conditions to policies, include these values in the API call: Your user key The policy_id (available from API Explorer: Alerts Policies > GET > List) The required condition values in the API call (described in the API Explorer page and in the Alerts conditions API glossary) API Explorer: Alerts Conditions > POST > Create curl -X POST 'https://api.newrelic.com/v2/alerts_conditions/policies/$POLICY_ID.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -d \\ '{ \"condition\": { \"type\": \"string\", \"name\": \"string\", \"enabled\": boolean, \"entities\": [ integer ], \"metric\": \"string\", \"gc_metric\": \"string\", \"condition_scope\": \"string\", \"violation_close_timer\": integer, \"runbook_url\": \"string\", \"terms\": [ { \"duration\": \"string\", \"operator\": \"string\", \"priority\": \"string\", \"threshold\": \"string\", \"time_function\": \"string\" } ], \"user_defined\": { \"metric\": \"string\", \"value_function\": \"string\" } } }' Copy The JSON response returns a condition id, which you will need to update or delete the condition. You can also view the condition id from API Explorer: Alerts Conditions > GET > List. Update conditions for policies To update conditions for policies, include these values in the API call: Your user key The condition's id (available from API Explorer: Alerts Conditions > GET > List) The required condition values in the API call (described in the API Explorer page and in the Alerts conditions API glossary) API Explorer: Alerts Conditions > PUT > Update curl -X PUT 'https://api.newrelic.com/v2/alerts_conditions/$CONDITION_ID.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -d \\ '{ \"condition\": { \"type\": \"string\", \"name\": \"string\", \"enabled\": boolean, \"entities\": [ integer ], \"metric\": \"string\", \"metric\": \"string\", \"gc_metric\": \"string\", \"condition_scope\": \"string\", \"violation_close_timer\": integer, \"runbook_url\": \"string\", \"terms\": [ { \"duration\": \"string\", \"operator\": \"string\", \"priority\": \"string\", \"threshold\": \"string\", \"time_function\": \"string\" } ], \"user_defined\": { \"metric\": \"string\", \"value_function\": \"string\" } } }' Copy Delete conditions from policies To delete conditions from policies, include these values in the API call: Your user key The condition_id (available from API Explorer: Alerts Conditions > GET > List) API Explorer: Alerts Conditions > DELETE > Delete curl -X DELETE 'https://api.newrelic.com/v2/alerts_conditions/$CONDITION_ID.json' \\ -H 'Api-Key:$API_KEY' -i Copy List existing conditions for policies To view a list of existing conditions for your policy, use your REST API key or user key, and the associated policy_id in the API call. API Explorer: Alerts Conditions > GET > List curl -X GET 'https://api.newrelic.com/v2/alerts_conditions.json' \\ -H 'Api-Key:$API_KEY' -i \\ -d 'policy_id=$POLICY_ID' Copy Conditions for NRQL These API functions include links to the API Explorer, where you can create, update, delete, or list NRQL conditions for your policies. Exception: NRQL baseline conditions are not available from this endpoint. Important If your account hosts data in the EU data center, ensure you are using the proper API endpoints for EU region accounts. Create NRQL conditions for policies Recommendation: Due to the way NRQL data is streamed, set the aggregation_method to EVENT_FLOW and use the default settings of 60 for aggregation_window and 120 for aggregation_delay. EVENT_FLOW works in most use-cases, but for a discussion on which use cases work better with EVENT_TIMER, see Which aggregation method to use?. API Explorer: Alerts Nrql Conditions > POST > Create To create NRQL conditions for policies: curl -X POST 'https://api.newrelic.com/v2/alerts_nrql_conditions/policies/$POLICY_ID.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -d \\ '{ \"nrql_condition\": { \"type\": \"string\", \"name\": \"string\", \"runbook_url\": \"string\", \"enabled\": boolean, \"terms\": [ { \"duration\": \"string\", \"operator\": \"string\", \"priority\": \"string\", \"threshold\": \"string\", \"time_function\": \"string\" } ], \"value_function\": \"string\", \"nrql\": { \"query\": \"string\" }, \"signal\": { \"aggregation_window\": \"string\", \"aggregation_method\": \"string\", \"aggregation_delay\": integer, \"aggregation_timer\": integer, \"fill_option\": \"string\", \"fill_value\": \"string\" }, \"expiration\": { \"expiration_duration\": \"string\", \"open_violation_on_expiration\": boolean, \"close_violations_on_expiration\": boolean } } }' Copy If you set type as outlier, you can omit the value_function. However, two additional fields are required: expected_groups and ignore_overlap. For more information, see Alerts Conditions API field names. If you omit type or set it as static, it will default to standard NRQL alerting. Update NRQL conditions for policies To update NRQL conditions for policies, include these values in the API call: Your user key The condition's id (available from API Explorer: Alerts Nrql Conditions > GET > List) The required condition values in the API call (described in the API Explorer page and in the Alerts conditions API glossary) API Explorer: Alerts Nrql Conditions > PUT > Update curl -X PUT 'https://api.newrelic.com/v2/alerts_nrql_conditions/$CONDITION_ID.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -d \\ '{ \"nrql_condition\": { \"name\": \"string\", \"runbook_url\": \"string\", \"enabled\": boolean, \"terms\": [ { \"duration\": \"string\", \"operator\": \"string\", \"priority\": \"string\", \"threshold\": \"string\", \"time_function\": \"string\" } ], \"value_function\": \"string\", \"nrql\": { \"query\": \"string\" } } }' Copy Delete NRQL conditions for policies To delete NRQL conditions from policies, include these values in the API call: Your user key The condition's id (available from API Explorer: Alerts Nrql Conditions > GET > List) API Explorer: Alerts Nrql Conditions > DELETE > Delete curl -X DELETE 'https://api.newrelic.com/v2/alerts_nrql_conditions/$CONDITION_ID.json' \\ -H 'Api-Key:$API_KEY' -i Copy List existing NRQL conditions for policies To view a list of existing conditions for your alert policy, use your user key or REST API key, and the associated policy_id in the API call. API Explorer: Alerts Nrql Conditions > GET > List curl -X GET 'https://api.newrelic.com/v2/alerts_nrql_conditions.json' \\ -H 'Api-Key:$API_KEY' -i \\ -d 'policy_id=$POLICY_ID' Copy Conditions for external services These API functions include links to the API Explorer, where you can create, update, delete, or list policy conditions. These calls are for conditions for external services. The API calls can be used with APM and mobile monitoring apps. Important If your account hosts data in the EU data center, ensure you are using the proper API endpoints for EU region accounts. External services: Create conditions for policies To add conditions for external services to policies that have APM or mobile monitoring apps, include these values in the API call: Your user key The policy_id (available from API Explorer: Alerts Policies > GET > List) The required external_service_condition values in the API call (described in the API Explorer page and in the Alerts conditions API glossary) API Explorer: Alerts External Service Conditions > POST > Create curl -X POST 'https://api.newrelic.com/v2/alerts_external_service_conditions/policies/$POLICY_ID.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -d \\ '{ \"external_service_condition\": { \"type\": \"string\", \"name\": \"string\", \"enabled\": boolean, \"entities\": [ integer ], \"external_service_url\": \"string\", \"metric\": \"string\", \"runbook_url\": \"string\", \"terms\": [ { \"duration\": \"string\", \"operator\": \"string\", \"priority\": \"string\", \"threshold\": \"string\", \"time_function\": \"string\" } ] } }' Copy The JSON response returns a condition id, which you will need to update or delete the condition. You can also view the condition id from API Explorer: Alerts External Service Conditions > GET > List. External services: Update conditions for policies To update conditions for external services to policies that have APM or mobile monitoring apps, include these values in the API call: Your user key The external service condition's id (available from API Explorer: Alerts External Service Conditions > GET > List) The required external_service_condition values in the API call (described in the API Explorer page and in the Alerts conditions API glossary) API Explorer: Alerts External Service Conditions > PUT > Update curl -X PUT 'https://api.newrelic.com/v2/alerts_external_service_conditions/$CONDITION_ID.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -d \\ '{ \"external_service_condition\": { \"type\": \"string\", \"name\": \"string\", \"enabled\": boolean, \"entities\": [ integer ], \"external_service_url\": \"string\", \"metric\": \"string\", \"runbook_url\": \"string\", \"terms\": [ { \"duration\": \"string\", \"operator\": \"string\", \"priority\": \"string\", \"threshold\": \"string\", \"time_function\": \"string\" } ] } }' Copy External services: Delete conditions from policies To delete conditions for external services from policies with APM or mobile monitoring apps, include these values in the API call: Your user key The condition_id (available from API Explorer: Alerts External Service Conditions > GET > List) API Explorer: Alerts External Service Conditions > DELETE > Delete curl -X DELETE 'https://api.newrelic.com/v2/alerts_external_service_conditions/$CONDITION_ID.json' \\ -H 'Api-Key:$API_KEY' -i Copy External services: List existing conditions for policies To view a list of existing conditions for policies with external service apps (APM or mobile monitoring), use your user key or REST API key and the associated policy_id in the API call. API Explorer: Alerts External Service Conditions > GET > List curl -X GET 'https://api.newrelic.com/v2/alerts_external_service_conditions.json' \\ -H 'Api-Key:$API_KEY' -i \\ -d 'policy_id=$POLICY_ID' Copy Conditions for Synthetic monitoring These API functions include links to the API Explorer, where you can create, update, delete, or list conditions for your alert policies. The API calls can be used with Synthetic monitoring. Synthetics: Create conditions for policies To add conditions to policies for Synthetic monitoring, include these values in the API call: Your user key The policy_id (available from API Explorer: Alerts Policies > GET > List) The required synthetics_condition values in the API call (described in the API Explorer page and in the Alerts conditions API glossary) API Explorer: Alerts Synthetics Conditions > POST > Create curl -X POST 'https://api.newrelic.com/v2/alerts_synthetics_conditions/policies/$POLICY_ID.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -d \\ '{ \"synthetics_condition\": { \"name\": \"string\", \"monitor_id\": \"string\", \"runbook_url\": \"string\", \"enabled\": boolean } }' Copy The JSON response returns a condition id, which you will need to update or delete the condition. You can also view the condition id from API Explorer: Alerts Synthetics Conditions > GET > List. Synthetic monitoring: Update conditions for policies To update policy conditions for Synthetic monitoring, include these values in the API call: Your user key The condition id (available from API Explorer: Alerts Synthetics Conditions > GET > List) The required synthetics_condition values in the API call (described in the API Explorer page to create alert conditions for Synthetics and in the Alerts conditions API glossary) API Explorer: Alerts Synthetics Conditions > PUT > Update curl -X PUT 'https://api.newrelic.com/v2/alerts_synthetics_conditions/$CONDITION_ID.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -d \\ '{ \"synthetics_condition\": { \"name\": \"string\", \"monitor_id\": \"string\", \"runbook_url\": \"string\", \"enabled\": boolean } }' Copy Synthetic monitoring: Delete conditions from policies To delete policy conditions for Synthetic monitoring, include these values in the API call: Your user key The condition_id (available from API Explorer: Alerts Synthetics Conditions > GET > List) API Explorer: Alerts Synthetics Conditions > DELETE > Delete curl -X DELETE 'https://api.newrelic.com/v2/alerts_synthetics_conditions/$CONDITION_ID.json' \\ -H 'Api-Key:$API_KEY' -i Copy Synthetic monitoring: List existing conditions for policies To view a list of existing policy conditions for Synthetic monitoring, use your user key or REST API key, and the associated policy_id in the API call. API Explorer: Alerts Synthetics Conditions > GET > List curl -X GET 'https://api.newrelic.com/v2/alerts_synthetics_conditions.json' \\ -H 'Api-Key:$API_KEY' -i \\ -d 'policy_id=$POLICY_ID' Copy Alert activity: Events, violations, incidents These API functions include links to the API Explorer, where you can view information about events, violations, and incidents for your alert policies. Important If your account hosts data in the EU data center, ensure you are using the proper API endpoints for EU region accounts. List Events To view events for entities monitored by APM, browser, mobile, synthetic monitoring, and alerts, include these values in the API call: Your user key or REST API key Other optional values to use as filters (described in the API Explorer page) that depend on the type of product (browser monitoring, mobile monitoring, etc.), entity (as apps or key transactions for APM, synthetic monitoring, etc.), and type of event (notification, deployment, instrumentation, etc.) An optional pagination value API Explorer: Alerts Events > GET > List curl -X GET 'https://api.newrelic.com/v2/alerts_events.json' \\ -H 'Api-Key:$API_KEY' -i Copy List Violations To view violations for any entity monitored for your account, include these values in the API call: Your user key or REST API key An optional flag to show only those violations that are currently open An optional pagination value API Explorer: Alerts Violations > GET > List curl -X GET 'https://api.newrelic.com/v2/alerts_violations.json' \\ -H 'Api-Key:$API_KEY' -i Copy Tip To find policy, condition, and incident information: take the IDs found in the links section in the violations property and place them into the URLs contained in the outer links property of the payload. List Incidents To view incidents for any entity monitored for your account, include these values in the API call: Your user key or REST API key An optional flag to show only those incidents that are currently open An optional flag to exclude violation data from response An optional pagination value API Explorer: Alerts Incidents > GET > List curl -X GET 'https://api.newrelic.com/v2/alerts_incidents.json' \\ -H 'Api-Key:$API_KEY' -i Copy Show Incident To show a single incident associated with your account, include these values in the API call: Your user key or REST API key An incident ID API Explorer: Alerts Incidents > GET > Show curl -X GET 'https://api.newrelic.com/v2/alerts_incidents/{id}.json' \\ -H 'Api-Key:$API_KEY' -i Copy Acknowledge Incident To acknowledge an incident associated with your account, include these values in the API call: Your user key An incident ID API Explorer: Alerts Incidents > PUT > Acknowledge curl -X PUT 'https://api.newrelic.com/v2/alerts_incidents/{id}/acknowledge.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' Copy Close Incident To close an incident associated with your account, include these values in the API call: Your user key An incident ID API Explorer: Alerts Incidents > PUT > Close curl -X PUT 'https://api.newrelic.com/v2/alerts_incidents/{id}/close.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' Copy Alert entity conditions These API functions include links to the API Explorer, where you can list, add and remove entities in the conditions for your alert policies. Important If your account hosts data in the EU data center, ensure you are using the proper API endpoints for EU region accounts. List condition by entity To view the conditions an entity that apply to an entity in APM, browser, mobile, or key transactions, include these values in the API call: Your user key or REST API key The entity_id This is the specific entity (alert target) to be monitored. The entity_type, which must be one of the following: Application BrowserApplication MobileApplication KeyTransaction API Explorer: Alerts Entity Conditions > GET > list curl -X GET 'https://api.newrelic.com/v2/alerts_entity_conditions/$ENTITY_ID.json' \\ -H 'Api-Key:$API_KEY' -i \\ -G -d 'entity_type=$ENTITY_TYPE' Copy Add an entity to a condition To add an entity to a condition, include these values in the API call: Your user key The entity_id This is the entity (alert target) to be monitored. It is the numeric ID for the APM application, browser app, key transaction, or mobile app. The condition_id (available from API Explorer: Alerts Conditions > GET > List) The entity_type, which must be one of the following: Application BrowserApplication MobileApplication KeyTransaction API Explorer: Alerts Entity Conditions > PUT > Add curl -X PUT 'https://api.newrelic.com/v2/alerts_entity_conditions/$ENTITY_ID.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -G -d 'entity_type=$ENTITY_TYPE&condition_id=$CONDITION_ID' Copy Remove an entity from a condition To remove an entity being monitored from a condition, include these values in the API call: Your user key The entity_id This is the specific monitored entity (alert target) to be removed. It is the numeric ID for the APM application, browser app, key transaction, or mobile app. The condition_id (available from API Explorer: Alerts Conditions > GET > List) The entity_type, which must be one of the following: Application BrowserApplication MobileApplication KeyTransaction API Explorer: Alerts Entity Conditions > DELETE > Remove curl -X DELETE 'https://api.newrelic.com/v2/alerts_entity_conditions/$ENTITY_ID.json' \\ -H 'Api-Key:$API_KEY' -i \\ -G -d 'entity_type=$ENTITY_ID&condition_id=$CONDITION_ID' Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.83958,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>REST</em> <em>API</em> calls for <em>alerts</em>",
        "sections": "<em>REST</em> <em>API</em> calls for <em>alerts</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "Our <em>REST</em> <em>API</em> (v2) allows you to configure settings for <em>alerts</em>. The <em>API</em> Explorer also includes the curl request format, available parameters, potential response status codes, and JSON response structure for each of the available <em>API</em> calls. You can also create <em>alert</em> conditions in the UI. Important"
      },
      "id": "6130c144e7b9d2d160b6f25d"
    },
    {
      "sections": [
        "Manage entities in alerts conditions",
        "Requirements",
        "General procedure",
        "Important",
        "Example: Add/remove an entity"
      ],
      "title": "Manage entities in alerts conditions",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "REST API alerts"
      ],
      "external_id": "7a33e56e9410082971e69e27422d4646cebb7180",
      "image": "",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/rest-api-alerts/manage-entities-alerts-conditions/",
      "published_at": "2021-12-15T09:12:25Z",
      "updated_at": "2021-10-31T08:58:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In alerts, an entity is defined as any target for monitoring, such as an application, a browser, or a host. The alerts UI shows available entities that you can select. You can also use the REST API and API Explorer to add or remove entities for a condition. Requirements Modifying the list of entities in a condition requires you to know: Your API key The { entity_ID} of the entity you want to monitor The { condition_ID} of the condition you want to modify General procedure To update the entity list for a condition: Locate the appropriate entity ID; for example, Application ID and browser ID. Identify the policy ID by name or type. Get the list of conditions associated with the policy and choose the one you want to modify for the appropriate category: APM, browser, and mobile External services Synthetic monitoring Modify the condition using the add or remove API requests. Important Follow the requirements for the minimum and maximum number of entities you can add to conditions. Example: Add/remove an entity The following example shows how to add a Ruby application named TimberTime in a condition and how to remove an entity from that same condition. Only the first step in this example is unique to choosing the Ruby app as the entity. The remaining steps will be the same for whichever entity you choose. Get the entity_id; for example, {application_id}: curl -X GET 'https://api.newrelic.com/v2/applications.json' \\ -H 'Api-Key:$API_KEY' -i Copy OR If you know the application name, use this command and specify the app_name: curl -X GET 'https://api.newrelic.com/v2/applications.json' \\ -H 'Api-Key:$API_KEY' -i \\ -d 'filter[name]=TimberTime' Copy Review the output to find the {application_id}, and use it as the {entity_id}: { \"applications\": [ { \"id\": 12345, <---<<< {application_id} == {entity_id} \"name\": \"TimberTime\", \"language\": \"ruby\", \"health_status\": \"gray\", ... }, Copy Get the policy_id you want to update; for example, the TimberTime app's Logjam Alert policy. To get the policy_id, use this command: curl -X GET 'https://api.newrelic.com/v2/alerts_policies.json' \\ -H 'Api-Key:$API_KEY' -i \\ -G -d 'filter[name]= Logjam Alert' <---<<< {policy_name} Copy Review the policy output; for example: { \"policies\": [ { \"id\": 85, <---<<< {policy_id} \"incident_preference\": \"PER_POLICY\", \"name\": \"Logjam Alert\", \"created_at\": 1461176510393, \"updated_at\": 1461176510393 }, Copy List all of this policy's conditions and locate the {condition_id}: curl -X GET 'https://api.newrelic.com/v2/alerts_conditions.json' \\ -H 'Api-Key:$API_KEY' -i \\ -G -d 'policy_id=85' Copy Review the policy conditions; for example: { \"conditions\": [ { \"id\": 234567, <---<<< {condition_id} \"type\": \"apm_app_metric\", \"name\": \"Throughput (web) (High)\", \"enabled\": true, \"entities\": [ \"8288171\" <---<<< Entity currently included in the policy ], \"metric\": \"response_time_web\", \"terms\": [ { \"duration\": \"5\", \"operator\": \"above\", \"priority\": \"critical\", \"threshold\": \"500\", \"time_function\": \"all\" } ] } ] } Copy Use API requests to add entities to or remove entities from the policy's condition: To add {entity_id} 12345 to {condition_id} 234567, with {entity_type} set as application: curl -X PUT 'https://api.newrelic.com/v2/alerts_entity_conditions/12345.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -G -d 'entity_type=Application&condition_id=234567' Copy To remove {entity_id} 8288171 from {condition_id} 234567, with {entity_type} set as application: curl -X DELETE 'https://api.newrelic.com/v2/alerts_entity_conditions/8288171.json' \\ -H 'Api-Key:$API_KEY' -i \\ -G -d 'entity_type=Application&condition_id=234567' Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.74602,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Manage entities in <em>alerts</em> conditions",
        "sections": "Manage entities in <em>alerts</em> conditions",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "In <em>alerts</em>, an entity is defined as any target for monitoring, such as an application, a browser, or a host. The <em>alerts</em> UI shows available entities that you can select. You can also use the <em>REST</em> <em>API</em> and <em>API</em> Explorer to add or remove entities for a condition. Requirements Modifying the list"
      },
      "id": "6130b580196a67eab24948fb"
    }
  ],
  "/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/rest-api-alerts/disable-enable-alerts-conditions-using-api": [
    {
      "sections": [
        "Streaming alerts: key terms and concepts",
        "Why it matters",
        "Streaming alerts process and descriptions",
        "Choose your aggregation method",
        "Event flow",
        "Caution",
        "Event timer",
        "Cadence",
        "Streaming alerts tools",
        "Tip",
        "Window duration",
        "Delay/timer",
        "Loss of signal detection",
        "Gap filling"
      ],
      "title": "Streaming alerts: key terms and concepts",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Get started"
      ],
      "external_id": "144c7f0721ec2acb1e8ac8aa06142911ea552aef",
      "image": "https://docs.newrelic.com/static/39158bd84483adf9516bb7ff5058c9bd/d30ee/streaming-alerts.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/streaming-alerts-key-terms-concepts/",
      "published_at": "2021-12-15T12:47:58Z",
      "updated_at": "2021-12-15T12:47:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The streaming alerts platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into New Relic. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that's processed by the streaming algorithm. There are three methods for aggregating the data filtered through your NRQL condition: Event flow (default) Event timer Cadence This short video describes the three aggregation method (5:31). Why it matters Understanding how streaming alerts works will help you fine-tune your NRQL conditions to be notified about what's important to you. Only data that matches the conditions of the NRQL WHERE clause is alerted on. For more details on each step of the process, see Streaming alerts process and descriptions. As data streams into New Relic, it's filtered by the NRQL condition. Before data is evaluated, it must meet the criteria defined by the NRQL query's WHERE clause. Instead of evaluating that data immediately for violations, the NRQL alert conditions collect the data over a period of time known as the aggregation window. An additional delay/timer allows for slower data points to arrive before the window is aggregated. Once the delay/timer time has elapsed, New Relic aggregates the data into a single data point. Alerts then evaluates the data point using the NRQL condition in order to determine whether it meets the violation threshold criteria. Even if a data point meets the criteria for a violation, a violation may not be triggered. A violation is only triggered when data points consistently meet the threshold criteria over a period of time. This is the threshold duration. If the data points are in violation for an entire threshold duration, we'll send you a notification based on your policy settings. All of these configurable delays give you more control over how you're alerted on sporadic and missing data. Streaming alerts process and descriptions Process Description Streaming data All data coming into New Relic. WHERE clause Filters all incoming streaming data. We only monitor for alerts on data that makes it through this filter. Aggregation methods One of three methods that control how data is collected before it's evaluated. They are: Event flow (Default) Event timer Cadence Aggregation window Data with timestamps that fall within this window will be aggregated and then evaluated. Delay/timer A time delay to ensure all data points have arrived in the aggregation window before aggregation occurs. Aggregated data Data in the aggregated window is collapsed to a single data point for alert evaluation. Evaluation The data point is evaluated by the NRQL condition, which is trigged by each incoming aggregated data point. Threshold duration A specific duration that determines if a violation is created. If your specified NRQL condition meets the threshold criteria over the threshold duration, a violation occurs. When a data point lacks data, a custom value is inserted to fill the gap. Choose your aggregation method You can choose between three different aggregation methods, depending on your needs. Event flow (default) works best for data that comes in frequently and mostly in order. Event timer works best for data that arrives infrequently in batches, such as cloud integration data or infrequent error logs. Cadence is our original and inferior aggregation method. It aggregates data on specific time intervals as detected by New Relic's internal wall clock, regardless of data timestamps. Event flow Event flow aggregates a window of data when the first data point arrives in a subsequent window. The custom delay defines which subsequent window data will start to populate to trigger aggregation of the current window. A custom delay provides extra time for data to arrive. These times are based on the data's timestamps and not New Relic's wall clock time. For example, suppose you're monitoring CPU usage in window durations of 1 minute and a 3 minute delay. When a CPU usage data point comes in with a timestamp between 12:00pm and 12:01pm, event flow will not aggregate that window until a data point shows up with a timestamp between 12:04pm and 12:05pm. When event flow receives the first data point with a timestamp of 12:04pm or later, it sends the 12:00 to 12:01 data to be aggregated. Caution If you expect your data points to arrive more than 30 minutes apart, please use the Event Timer method described below. Event timer Like event flow, event timer will only aggregate data for a given window when data arrives for that window. When a data point arrives for an aggregation window, a timer dedicated to that window starts to count down. If no further data arrives before the timer counts down, the data for that window is aggregated. If more data points arrive before the timer has completed counting down, the timer is reset. For example, suppose you're monitoring CloudWatch data that arrives fairly infrequently. You're using a window duration of 1 minute and a 3 minute timer. When a CloudWatch data point comes in with a timestamp between 12:00pm and 12:01pm, the timer will start to count down. If no further data points show up for that 12:00-12:01 window, the window will get aggregated 3 minutes later. If a new data point with a timestamp between 12:00 and 12:01 arrives, the timer resets. It keeps resetting every time more data points for that window arrive. The window will not be sent for aggregation until the timer reaches 0. If the timer for a later data point elapses before an earlier data point, the event timer method waits for the earlier timer to elapse before aggregating the later data point. For best results, make sure your timer is equal to or longer than your window duration time. If the timer is less than your window duration and your data flow is inconsistent, then your data may be evaluated before all of your data points arrive. This could cause you to be notified incorrectly. Cadence We recommend you use one of the other two methods. Cadence is our old streaming aggregation method. This method uses New Relic's wall clock time to determine when data is aggregated and evaluated. It doesn't take into account data point timestamps as they arrive. Streaming alerts tools Streaming alerts provide a set of tools to give you greater control over how your data is aggregated before it's evaluated to reduce incorrect notifications you receive. They are: Window duration Delay/timer Loss of signal detection Gap filling Tip This article covers these tools at a conceptual level. You'll find direct instructions on how to use these tools in Create NRQL alert conditions. Window duration In order to make loss of signal detection more effective and to reduce unnecessary notifications, you can customize aggregation windows to the duration that you need. An aggregation window is a specific block of time. We gather data points together in an aggregation window, before evaluating the data. A longer aggregation window can smooth out the data, since an outlier data point will have more data points to be aggregated with, giving it less of an influence on the aggregated data point that is sent for evaluation. When a data point arrives, its timestamp is used to put it in the proper aggregation window. You can set your aggregation window to anything between 30 seconds and 15 minutes. The default is 1 minute. Delay/timer The delay/timer setting controls how long the condition should wait before aggregating the data in the aggregation window. The event flow and cadence methods use delay. Event timer uses timer. The delay default is 2 minutes. The timer default is 1 minute and has a minimum value of 5 seconds. Loss of signal detection Loss of signal occurs when no data matches the NRQL condition over a specific period of time. A loss of signal is caused by different things. The WHERE clause in your NRQL query can filter out data before it's evaluated for violations. It could also mean a service or entity is offline or a periodic job has failed to run and no data is being sent to New Relic. In order to avoid unnecessary notifications, you can choose how long to wait before you're notified by a loss of signal violation. You can use loss of signal detection to open violations and be notified when a signal is lost. Alternately, you can use a loss of signal to close violations for ephemeral services or sporadic data, such as error counts. Gap filling Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with the last value received, a static value, or else do nothing and leave the gap there. The default is None. Gaps in streaming data can be caused by network or host issues, a signal may be sparse, or some signals, such as error counts, may only have data when something is wrong. By filling the gaps with known values, the alert evaluation process can process those gaps and determine how they should affect the loss of signal evaluation. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss and gap filling, see this Explorers Hub post.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 177.89925,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "sections": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "The streaming <em>alerts</em> platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into <em>New</em> <em>Relic</em>. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that&#x27;s processed"
      },
      "id": "6130c054e7b9d269edb6f21d"
    },
    {
      "sections": [
        "REST API calls for alerts",
        "Important",
        "Available data and functions via API",
        "Conditions excluded from the REST API",
        "Alert policies",
        "Create new policies",
        "Update policies",
        "Delete existing policies",
        "List existing policies",
        "Notification channels",
        "Create new notification channels",
        "Email channel",
        "OpsGenie channel",
        "Slack channel",
        "VictorOps channel",
        "PagerDuty channel",
        "Webhook (json) channel",
        "Webhook (x-www-form-urlencoded) channel",
        "Delete existing notification channels",
        "List existing notification channels",
        "Update notification channels associated with policies",
        "Delete notification channels associated with policies",
        "Conditions for APM, browser, mobile",
        "Create conditions for policies",
        "Update conditions for policies",
        "Delete conditions from policies",
        "List existing conditions for policies",
        "Conditions for NRQL",
        "Create NRQL conditions for policies",
        "Update NRQL conditions for policies",
        "Delete NRQL conditions for policies",
        "List existing NRQL conditions for policies",
        "Conditions for external services",
        "External services: Create conditions for policies",
        "External services: Update conditions for policies",
        "External services: Delete conditions from policies",
        "External services: List existing conditions for policies",
        "Conditions for Synthetic monitoring",
        "Synthetics: Create conditions for policies",
        "Synthetic monitoring: Update conditions for policies",
        "Synthetic monitoring: Delete conditions from policies",
        "Synthetic monitoring: List existing conditions for policies",
        "Alert activity: Events, violations, incidents",
        "List Events",
        "List Violations",
        "Tip",
        "List Incidents",
        "Show Incident",
        "Acknowledge Incident",
        "Close Incident",
        "Alert entity conditions",
        "List condition by entity",
        "Add an entity to a condition",
        "Remove an entity from a condition"
      ],
      "title": "REST API calls for alerts",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "REST API alerts"
      ],
      "external_id": "af566e5c2f0a695c1146b5531849bb49248ea0ec",
      "image": "",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/rest-api-alerts/rest-api-calls-alerts/",
      "published_at": "2021-12-14T22:59:19Z",
      "updated_at": "2021-10-31T13:25:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our REST API (v2) allows you to configure settings for alerts. The API Explorer also includes the curl request format, available parameters, potential response status codes, and JSON response structure for each of the available API calls. You can also create alert conditions in the UI. Important For infrastructure alerting, see REST API for infrastructure monitoring alerts. Available data and functions via API REST API functions Comments View account data In general, any role can use a user key or REST API key with GET > List functions to view alerts data. The account Owner and Admins may use their API key. List output will be paginated. Available functions include: Alert policies Notification channels Conditions for APM, browser, and mobile (Some limitations apply.) Conditions for external services Conditions for Synthetic monitoring Conditions for NRQL (Some limitations apply.) Events Violations Incidents Maintain account data You may have an Owner or Admin role in your account and a user key or have a custom role that grants permissions to manage Alerts and a user key in order to use any maintenance function, including POST > Create, PUT > Add, PUT > Update, and DELETE. Conditions excluded from the REST API These types of conditions do not have available endpoints in the API: APM: Web transaction percentiles, conditions targeting labels, and baselines NRQL: Baselines Alert policies These API functions include links to the API Explorer, where you can create, delete, or list policies. Important If your account hosts data in the EU data center, ensure you're using the proper API endpoints for EU region accounts. Create new policies To add new policies, use your user key and include these two values in the API call: Required values Definition Incident incident_preference Determines how Alerts will create incidents and group violations. This must be one of the following: PER_POLICY (default): Roll up by policy. PER_CONDITION: Roll up by condition. PER_CONDITION_AND_TARGET: Roll up by target and condition. Policy name The policy name is required. Leaving it unchanged will create a policy called string. API Explorer: Alerts Policies > POST > Create curl -X POST 'https://api.newrelic.com/v2/alerts_policies.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -d \\ '{ \"policy\": { \"incident_preference\": \"string\", \"name\": \"string\" } }' Copy Update policies Use this API to update an existing policy's name and incident_preference. You'll need your user key. Required values Definition id Required. To find a policy's ID, use either of these options: From the UI: On a policy's UI page, find the ID under the policy name. With the API: Use the List policies API. Incident incident_preference Determines how alerts will create incidents and group violations. Must be one of the following: PER_POLICY (default): Roll up by policy. PER_CONDITION: Roll up by condition. PER_CONDITION_AND_TARGET: Roll up by target and condition. Policy name Required. If you do not change the name, it defaults to a policy called string. To find a policy's exact name, use the List policies API. API Explorer: Alerts Policies > PUT > Update curl -X PUT 'https://api.newrelic.com/v2/alerts_policies/{id}.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -d \\ '{ \"policy\": { \"incident_preference\": \"string\", \"name\": \"string\" } }' Copy Delete existing policies To delete an existing policy, use your user key, and include the policy_id (available from API Explorer: Alerts Policies > GET > List) in the API call: API Explorer: Alerts Policies > DELETE > Delete curl -X DELETE 'https://api.newrelic.com/v2/alerts_policies/$POLICY_ID.json' \\ -H 'Api-Key:$API_KEY' -i Copy List existing policies To view a list of existing policies for your account, use your user key, and include these optional values in the API call: Optional policy name filter Optional pagination value API Explorer: Alerts Policies > GET > List curl -X GET 'https://api.newrelic.com/v2/alerts_policies.json' \\ -H 'Api-Key:$API_KEY' -i Copy Notification channels These API functions include links to the API Explorer, where you can create, delete, or list Alerts notification channels. Important If your account hosts data in the EU data center, ensure you are using the proper API endpoints for EU region accounts. Create new notification channels To add new notification channels, use your user key and include these values in the API call: New channel's name Type of channel Configuration values The API Explorer shows the format for required configuration values for each type of notification channel. API Explorer: Alerts Channels > POST > Create curl -X POST 'https://api.newrelic.com/v2/alerts_channels.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -d \\ '{ \"channel\": { \"name\": \"string\", \"type\": \"string\", \"configuration\": \"hash\" } }' Copy The \"hash\" place holder in this example must be replaced by one of the following JSON blocks with the appropriate values substituted: Email channel { \"recipients\" : \"test@google.com\", \"include_json_attachment\" : true } Copy OpsGenie channel { \"api_key\": \"abc123\", \"teams\": \"team1\", \"tags\": \"tag1\", \"recipients\": \"me@me.com\" } Copy Slack channel { \"url\": \"http://example.com\", \"channel\": \"channel1\" } Copy VictorOps channel { \"key\": \"mykey\", \"route_key\": \"theroute\" } Copy PagerDuty channel { \"service_key\": \"myservicekey\" } Copy Webhook (json) channel This example shows the default payload inserted. (The payload attribute is optional.) The payload can be customized to have different keys than the ones shown and will be sent with the $ prefixed attributes interpolated prior to delivery. Important The condition_id is deprecated. Instead, use condition_family_id. { \"base_url\": \"http://example.com\", \"auth_username\": \"username\", \"auth_password\": \"password\", \"payload_type\": \"application/json\", \"headers\": { \"header1\": \"test\", \"header2\": \"test\" } \"payload\": { \"account_id\": \"$ACCOUNT_ID\", \"account_name\": \"$ACCOUNT_NAME\", \"closed_violations_count_critical\": \"$CLOSED_VIOLATIONS_COUNT_CRITICAL\", \"closed_violations_count_warning\": \"$CLOSED_VIOLATIONS_COUNT_WARNING\", \"condition_family_id\": \"$CONDITION_FAMILY_ID\", \"condition_id\": \"$CONDITION_ID\", \"condition_name\": \"$CONDITION_NAME\", \"current_state\": \"$EVENT_STATE\", \"details\": \"$EVENT_DETAILS\", \"duration\": \"$DURATION\", \"event_type\": \"$EVENT_TYPE\", \"incident_acknowledge_url\": \"$INCIDENT_ACKNOWLEDGE_URL\", \"incident_id\": \"$INCIDENT_ID\", \"incident_url\": \"$INCIDENT_URL\", \"open_violations_count_critical\": \"$OPEN_VIOLATIONS_COUNT_CRITICAL\", \"open_violations_count_warning\": \"$OPEN_VIOLATIONS_COUNT_WARNING\", \"owner\": \"$EVENT_OWNER\", \"policy_name\": \"$POLICY_NAME\", \"policy_url\": \"$POLICY_URL\", \"runbook_url\": \"$RUNBOOK_URL\", \"severity\": \"$SEVERITY\", \"targets\": \"$TARGETS\", \"timestamp\": \"$TIMESTAMP\", \"violation_callback_url\": \"$VIOLATION_CALLBACK_URL\", \"violation_chart_url\": \"$VIOLATION_CHART_URL\" } } Copy Webhook (x-www-form-urlencoded) channel This example shows the default payload inserted. (The payload attribute is optional.) The payload can be customized to have different keys than the ones shown and will be sent with the $ prefixed attributes interpolated prior to delivery. Important The condition_id is deprecated. Instead, use condition_family_id. { \"base_url\": \"http://example.com\", \"auth_username\": \"username\", \"auth_password\": \"password\", \"payload_type\": \"application/x-www-form-urlencoded\", \"headers\": { \"header1\": \"test\", \"header2\": \"test\" }, \"payload\": { \"account_id\": \"$ACCOUNT_ID\", \"account_name\": \"$ACCOUNT_NAME\", \"closed_violations_count_critical\": \"$CLOSED_VIOLATIONS_COUNT_CRITICAL\", \"closed_violations_count_warning\": \"$CLOSED_VIOLATIONS_COUNT_WARNING\", \"condition_family_id\": \"$CONDITION_FAMILY_ID\", \"condition_id\": \"$CONDITION_ID\", \"condition_name\": \"$CONDITION_NAME\", \"current_state\": \"$EVENT_STATE\", \"details\": \"$EVENT_DETAILS\", \"duration\": \"$DURATION\", \"event_type\": \"$EVENT_TYPE\", \"incident_acknowledge_url\": \"$INCIDENT_ACKNOWLEDGE_URL\", \"incident_id\": \"$INCIDENT_ID\", \"incident_url\": \"$INCIDENT_URL\", \"open_violations_count_critical\": \"$OPEN_VIOLATIONS_COUNT_CRITICAL\", \"open_violations_count_warning\": \"$OPEN_VIOLATIONS_COUNT_WARNING\", \"owner\": \"$EVENT_OWNER\", \"policy_name\": \"$POLICY_NAME\", \"policy_url\": \"$POLICY_URL\", \"runbook_url\": \"$RUNBOOK_URL\", \"severity\": \"$SEVERITY\", \"targets\": \"$TARGETS\", \"timestamp\": \"$TIMESTAMP\", \"violation_callback_url\": \"$VIOLATION_CALLBACK_URL\", \"violation_chart_url\": \"$VIOLATION_CHART_URL\" } } Copy Delete existing notification channels To delete an existing notification channels, use your user key, and include the channel_id (available from API Explorer: Alerts Channels > GET > List) in the API call: API Explorer: Alerts Channels > DELETE > Delete curl -X DELETE 'https://api.newrelic.com/v2/alerts_channels/{channel_id}.json' \\ -H 'Api-Key:$API_KEY' -i Copy List existing notification channels To view a list of existing notification channels for your account, use your user key and an optional pagination value in the API call. API Explorer: Alerts Channels > GET > List curl -X GET 'https://api.newrelic.com/v2/alerts_channels.json' \\ -H 'Api-Key:$API_KEY' -i Copy Update notification channels associated with policies You can associate a policy with one or more notification channels. You can also associate a notification channel with one or more policies. To add notification channels to policies, use your user key and these values in the API call: A policy_id value (available from API Explorer: Alerts Policies > GET > List) One or more channel_id values in an array, separated by commas or a new line (available from API Explorer: Alerts Channels > GET > List) API Explorer: Alerts Policy Channels > PUT > Update curl -X PUT 'https://api.newrelic.com/v2/alerts_policy_channels.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -G -d 'policy_id=$POLICY_ID&channel_ids=channel_id' Copy Delete notification channels associated with policies You can associate a policy with one or more notification channels. You can also associate a notification channel with one or more alert policies. To remove a policy from a channel, or to remove a channel from a policy, use your user key and these values in the API call: The policy_id (available from API Explorer: Alerts Policies > GET > List) The channel_id (available from API Explorer: Alerts Channels > GET > List) API Explorer: Alerts Policy Channels > DELETE > Delete curl -X DELETE 'https://api.newrelic.com/v2/alerts_policy_channels.json' \\ -H 'Api-Key:$API_KEY' -i \\ -d 'channel_id=CHANNEL_ID&policy_id=POLICY_ID' Copy Conditions for APM, browser, mobile These API functions include links to the API Explorer, where you can create, update, delete, or list policy conditions. These calls are for conditions with these types of entities: APM: Apps and key transactions Browser: Apps and key transactions Mobile: Mobile apps Exception: The following APM conditions are not available from this API endpoint: Conditions targeting labels (dynamic targeting) Web transaction percentile conditions Baseline conditions Important The API Explorer provides information about other types of conditions using separate endpoints, including external services (APM and mobile), and synthetic monitoring. Consider all types of alert conditions when searching or updating. If your account hosts data in the EU data center, ensure you are using the proper API endpoints for EU region accounts. Create conditions for policies To add conditions to policies, include these values in the API call: Your user key The policy_id (available from API Explorer: Alerts Policies > GET > List) The required condition values in the API call (described in the API Explorer page and in the Alerts conditions API glossary) API Explorer: Alerts Conditions > POST > Create curl -X POST 'https://api.newrelic.com/v2/alerts_conditions/policies/$POLICY_ID.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -d \\ '{ \"condition\": { \"type\": \"string\", \"name\": \"string\", \"enabled\": boolean, \"entities\": [ integer ], \"metric\": \"string\", \"gc_metric\": \"string\", \"condition_scope\": \"string\", \"violation_close_timer\": integer, \"runbook_url\": \"string\", \"terms\": [ { \"duration\": \"string\", \"operator\": \"string\", \"priority\": \"string\", \"threshold\": \"string\", \"time_function\": \"string\" } ], \"user_defined\": { \"metric\": \"string\", \"value_function\": \"string\" } } }' Copy The JSON response returns a condition id, which you will need to update or delete the condition. You can also view the condition id from API Explorer: Alerts Conditions > GET > List. Update conditions for policies To update conditions for policies, include these values in the API call: Your user key The condition's id (available from API Explorer: Alerts Conditions > GET > List) The required condition values in the API call (described in the API Explorer page and in the Alerts conditions API glossary) API Explorer: Alerts Conditions > PUT > Update curl -X PUT 'https://api.newrelic.com/v2/alerts_conditions/$CONDITION_ID.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -d \\ '{ \"condition\": { \"type\": \"string\", \"name\": \"string\", \"enabled\": boolean, \"entities\": [ integer ], \"metric\": \"string\", \"metric\": \"string\", \"gc_metric\": \"string\", \"condition_scope\": \"string\", \"violation_close_timer\": integer, \"runbook_url\": \"string\", \"terms\": [ { \"duration\": \"string\", \"operator\": \"string\", \"priority\": \"string\", \"threshold\": \"string\", \"time_function\": \"string\" } ], \"user_defined\": { \"metric\": \"string\", \"value_function\": \"string\" } } }' Copy Delete conditions from policies To delete conditions from policies, include these values in the API call: Your user key The condition_id (available from API Explorer: Alerts Conditions > GET > List) API Explorer: Alerts Conditions > DELETE > Delete curl -X DELETE 'https://api.newrelic.com/v2/alerts_conditions/$CONDITION_ID.json' \\ -H 'Api-Key:$API_KEY' -i Copy List existing conditions for policies To view a list of existing conditions for your policy, use your REST API key or user key, and the associated policy_id in the API call. API Explorer: Alerts Conditions > GET > List curl -X GET 'https://api.newrelic.com/v2/alerts_conditions.json' \\ -H 'Api-Key:$API_KEY' -i \\ -d 'policy_id=$POLICY_ID' Copy Conditions for NRQL These API functions include links to the API Explorer, where you can create, update, delete, or list NRQL conditions for your policies. Exception: NRQL baseline conditions are not available from this endpoint. Important If your account hosts data in the EU data center, ensure you are using the proper API endpoints for EU region accounts. Create NRQL conditions for policies Recommendation: Due to the way NRQL data is streamed, set the aggregation_method to EVENT_FLOW and use the default settings of 60 for aggregation_window and 120 for aggregation_delay. EVENT_FLOW works in most use-cases, but for a discussion on which use cases work better with EVENT_TIMER, see Which aggregation method to use?. API Explorer: Alerts Nrql Conditions > POST > Create To create NRQL conditions for policies: curl -X POST 'https://api.newrelic.com/v2/alerts_nrql_conditions/policies/$POLICY_ID.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -d \\ '{ \"nrql_condition\": { \"type\": \"string\", \"name\": \"string\", \"runbook_url\": \"string\", \"enabled\": boolean, \"terms\": [ { \"duration\": \"string\", \"operator\": \"string\", \"priority\": \"string\", \"threshold\": \"string\", \"time_function\": \"string\" } ], \"value_function\": \"string\", \"nrql\": { \"query\": \"string\" }, \"signal\": { \"aggregation_window\": \"string\", \"aggregation_method\": \"string\", \"aggregation_delay\": integer, \"aggregation_timer\": integer, \"fill_option\": \"string\", \"fill_value\": \"string\" }, \"expiration\": { \"expiration_duration\": \"string\", \"open_violation_on_expiration\": boolean, \"close_violations_on_expiration\": boolean } } }' Copy If you set type as outlier, you can omit the value_function. However, two additional fields are required: expected_groups and ignore_overlap. For more information, see Alerts Conditions API field names. If you omit type or set it as static, it will default to standard NRQL alerting. Update NRQL conditions for policies To update NRQL conditions for policies, include these values in the API call: Your user key The condition's id (available from API Explorer: Alerts Nrql Conditions > GET > List) The required condition values in the API call (described in the API Explorer page and in the Alerts conditions API glossary) API Explorer: Alerts Nrql Conditions > PUT > Update curl -X PUT 'https://api.newrelic.com/v2/alerts_nrql_conditions/$CONDITION_ID.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -d \\ '{ \"nrql_condition\": { \"name\": \"string\", \"runbook_url\": \"string\", \"enabled\": boolean, \"terms\": [ { \"duration\": \"string\", \"operator\": \"string\", \"priority\": \"string\", \"threshold\": \"string\", \"time_function\": \"string\" } ], \"value_function\": \"string\", \"nrql\": { \"query\": \"string\" } } }' Copy Delete NRQL conditions for policies To delete NRQL conditions from policies, include these values in the API call: Your user key The condition's id (available from API Explorer: Alerts Nrql Conditions > GET > List) API Explorer: Alerts Nrql Conditions > DELETE > Delete curl -X DELETE 'https://api.newrelic.com/v2/alerts_nrql_conditions/$CONDITION_ID.json' \\ -H 'Api-Key:$API_KEY' -i Copy List existing NRQL conditions for policies To view a list of existing conditions for your alert policy, use your user key or REST API key, and the associated policy_id in the API call. API Explorer: Alerts Nrql Conditions > GET > List curl -X GET 'https://api.newrelic.com/v2/alerts_nrql_conditions.json' \\ -H 'Api-Key:$API_KEY' -i \\ -d 'policy_id=$POLICY_ID' Copy Conditions for external services These API functions include links to the API Explorer, where you can create, update, delete, or list policy conditions. These calls are for conditions for external services. The API calls can be used with APM and mobile monitoring apps. Important If your account hosts data in the EU data center, ensure you are using the proper API endpoints for EU region accounts. External services: Create conditions for policies To add conditions for external services to policies that have APM or mobile monitoring apps, include these values in the API call: Your user key The policy_id (available from API Explorer: Alerts Policies > GET > List) The required external_service_condition values in the API call (described in the API Explorer page and in the Alerts conditions API glossary) API Explorer: Alerts External Service Conditions > POST > Create curl -X POST 'https://api.newrelic.com/v2/alerts_external_service_conditions/policies/$POLICY_ID.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -d \\ '{ \"external_service_condition\": { \"type\": \"string\", \"name\": \"string\", \"enabled\": boolean, \"entities\": [ integer ], \"external_service_url\": \"string\", \"metric\": \"string\", \"runbook_url\": \"string\", \"terms\": [ { \"duration\": \"string\", \"operator\": \"string\", \"priority\": \"string\", \"threshold\": \"string\", \"time_function\": \"string\" } ] } }' Copy The JSON response returns a condition id, which you will need to update or delete the condition. You can also view the condition id from API Explorer: Alerts External Service Conditions > GET > List. External services: Update conditions for policies To update conditions for external services to policies that have APM or mobile monitoring apps, include these values in the API call: Your user key The external service condition's id (available from API Explorer: Alerts External Service Conditions > GET > List) The required external_service_condition values in the API call (described in the API Explorer page and in the Alerts conditions API glossary) API Explorer: Alerts External Service Conditions > PUT > Update curl -X PUT 'https://api.newrelic.com/v2/alerts_external_service_conditions/$CONDITION_ID.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -d \\ '{ \"external_service_condition\": { \"type\": \"string\", \"name\": \"string\", \"enabled\": boolean, \"entities\": [ integer ], \"external_service_url\": \"string\", \"metric\": \"string\", \"runbook_url\": \"string\", \"terms\": [ { \"duration\": \"string\", \"operator\": \"string\", \"priority\": \"string\", \"threshold\": \"string\", \"time_function\": \"string\" } ] } }' Copy External services: Delete conditions from policies To delete conditions for external services from policies with APM or mobile monitoring apps, include these values in the API call: Your user key The condition_id (available from API Explorer: Alerts External Service Conditions > GET > List) API Explorer: Alerts External Service Conditions > DELETE > Delete curl -X DELETE 'https://api.newrelic.com/v2/alerts_external_service_conditions/$CONDITION_ID.json' \\ -H 'Api-Key:$API_KEY' -i Copy External services: List existing conditions for policies To view a list of existing conditions for policies with external service apps (APM or mobile monitoring), use your user key or REST API key and the associated policy_id in the API call. API Explorer: Alerts External Service Conditions > GET > List curl -X GET 'https://api.newrelic.com/v2/alerts_external_service_conditions.json' \\ -H 'Api-Key:$API_KEY' -i \\ -d 'policy_id=$POLICY_ID' Copy Conditions for Synthetic monitoring These API functions include links to the API Explorer, where you can create, update, delete, or list conditions for your alert policies. The API calls can be used with Synthetic monitoring. Synthetics: Create conditions for policies To add conditions to policies for Synthetic monitoring, include these values in the API call: Your user key The policy_id (available from API Explorer: Alerts Policies > GET > List) The required synthetics_condition values in the API call (described in the API Explorer page and in the Alerts conditions API glossary) API Explorer: Alerts Synthetics Conditions > POST > Create curl -X POST 'https://api.newrelic.com/v2/alerts_synthetics_conditions/policies/$POLICY_ID.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -d \\ '{ \"synthetics_condition\": { \"name\": \"string\", \"monitor_id\": \"string\", \"runbook_url\": \"string\", \"enabled\": boolean } }' Copy The JSON response returns a condition id, which you will need to update or delete the condition. You can also view the condition id from API Explorer: Alerts Synthetics Conditions > GET > List. Synthetic monitoring: Update conditions for policies To update policy conditions for Synthetic monitoring, include these values in the API call: Your user key The condition id (available from API Explorer: Alerts Synthetics Conditions > GET > List) The required synthetics_condition values in the API call (described in the API Explorer page to create alert conditions for Synthetics and in the Alerts conditions API glossary) API Explorer: Alerts Synthetics Conditions > PUT > Update curl -X PUT 'https://api.newrelic.com/v2/alerts_synthetics_conditions/$CONDITION_ID.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -d \\ '{ \"synthetics_condition\": { \"name\": \"string\", \"monitor_id\": \"string\", \"runbook_url\": \"string\", \"enabled\": boolean } }' Copy Synthetic monitoring: Delete conditions from policies To delete policy conditions for Synthetic monitoring, include these values in the API call: Your user key The condition_id (available from API Explorer: Alerts Synthetics Conditions > GET > List) API Explorer: Alerts Synthetics Conditions > DELETE > Delete curl -X DELETE 'https://api.newrelic.com/v2/alerts_synthetics_conditions/$CONDITION_ID.json' \\ -H 'Api-Key:$API_KEY' -i Copy Synthetic monitoring: List existing conditions for policies To view a list of existing policy conditions for Synthetic monitoring, use your user key or REST API key, and the associated policy_id in the API call. API Explorer: Alerts Synthetics Conditions > GET > List curl -X GET 'https://api.newrelic.com/v2/alerts_synthetics_conditions.json' \\ -H 'Api-Key:$API_KEY' -i \\ -d 'policy_id=$POLICY_ID' Copy Alert activity: Events, violations, incidents These API functions include links to the API Explorer, where you can view information about events, violations, and incidents for your alert policies. Important If your account hosts data in the EU data center, ensure you are using the proper API endpoints for EU region accounts. List Events To view events for entities monitored by APM, browser, mobile, synthetic monitoring, and alerts, include these values in the API call: Your user key or REST API key Other optional values to use as filters (described in the API Explorer page) that depend on the type of product (browser monitoring, mobile monitoring, etc.), entity (as apps or key transactions for APM, synthetic monitoring, etc.), and type of event (notification, deployment, instrumentation, etc.) An optional pagination value API Explorer: Alerts Events > GET > List curl -X GET 'https://api.newrelic.com/v2/alerts_events.json' \\ -H 'Api-Key:$API_KEY' -i Copy List Violations To view violations for any entity monitored for your account, include these values in the API call: Your user key or REST API key An optional flag to show only those violations that are currently open An optional pagination value API Explorer: Alerts Violations > GET > List curl -X GET 'https://api.newrelic.com/v2/alerts_violations.json' \\ -H 'Api-Key:$API_KEY' -i Copy Tip To find policy, condition, and incident information: take the IDs found in the links section in the violations property and place them into the URLs contained in the outer links property of the payload. List Incidents To view incidents for any entity monitored for your account, include these values in the API call: Your user key or REST API key An optional flag to show only those incidents that are currently open An optional flag to exclude violation data from response An optional pagination value API Explorer: Alerts Incidents > GET > List curl -X GET 'https://api.newrelic.com/v2/alerts_incidents.json' \\ -H 'Api-Key:$API_KEY' -i Copy Show Incident To show a single incident associated with your account, include these values in the API call: Your user key or REST API key An incident ID API Explorer: Alerts Incidents > GET > Show curl -X GET 'https://api.newrelic.com/v2/alerts_incidents/{id}.json' \\ -H 'Api-Key:$API_KEY' -i Copy Acknowledge Incident To acknowledge an incident associated with your account, include these values in the API call: Your user key An incident ID API Explorer: Alerts Incidents > PUT > Acknowledge curl -X PUT 'https://api.newrelic.com/v2/alerts_incidents/{id}/acknowledge.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' Copy Close Incident To close an incident associated with your account, include these values in the API call: Your user key An incident ID API Explorer: Alerts Incidents > PUT > Close curl -X PUT 'https://api.newrelic.com/v2/alerts_incidents/{id}/close.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' Copy Alert entity conditions These API functions include links to the API Explorer, where you can list, add and remove entities in the conditions for your alert policies. Important If your account hosts data in the EU data center, ensure you are using the proper API endpoints for EU region accounts. List condition by entity To view the conditions an entity that apply to an entity in APM, browser, mobile, or key transactions, include these values in the API call: Your user key or REST API key The entity_id This is the specific entity (alert target) to be monitored. The entity_type, which must be one of the following: Application BrowserApplication MobileApplication KeyTransaction API Explorer: Alerts Entity Conditions > GET > list curl -X GET 'https://api.newrelic.com/v2/alerts_entity_conditions/$ENTITY_ID.json' \\ -H 'Api-Key:$API_KEY' -i \\ -G -d 'entity_type=$ENTITY_TYPE' Copy Add an entity to a condition To add an entity to a condition, include these values in the API call: Your user key The entity_id This is the entity (alert target) to be monitored. It is the numeric ID for the APM application, browser app, key transaction, or mobile app. The condition_id (available from API Explorer: Alerts Conditions > GET > List) The entity_type, which must be one of the following: Application BrowserApplication MobileApplication KeyTransaction API Explorer: Alerts Entity Conditions > PUT > Add curl -X PUT 'https://api.newrelic.com/v2/alerts_entity_conditions/$ENTITY_ID.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -G -d 'entity_type=$ENTITY_TYPE&condition_id=$CONDITION_ID' Copy Remove an entity from a condition To remove an entity being monitored from a condition, include these values in the API call: Your user key The entity_id This is the specific monitored entity (alert target) to be removed. It is the numeric ID for the APM application, browser app, key transaction, or mobile app. The condition_id (available from API Explorer: Alerts Conditions > GET > List) The entity_type, which must be one of the following: Application BrowserApplication MobileApplication KeyTransaction API Explorer: Alerts Entity Conditions > DELETE > Remove curl -X DELETE 'https://api.newrelic.com/v2/alerts_entity_conditions/$ENTITY_ID.json' \\ -H 'Api-Key:$API_KEY' -i \\ -G -d 'entity_type=$ENTITY_ID&condition_id=$CONDITION_ID' Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.83958,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>REST</em> <em>API</em> calls for <em>alerts</em>",
        "sections": "<em>REST</em> <em>API</em> calls for <em>alerts</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "Our <em>REST</em> <em>API</em> (v2) allows you to configure settings for <em>alerts</em>. The <em>API</em> Explorer also includes the curl request format, available parameters, potential response status codes, and JSON response structure for each of the available <em>API</em> calls. You can also create <em>alert</em> conditions in the UI. Important"
      },
      "id": "6130c144e7b9d2d160b6f25d"
    },
    {
      "sections": [
        "Manage entities in alerts conditions",
        "Requirements",
        "General procedure",
        "Important",
        "Example: Add/remove an entity"
      ],
      "title": "Manage entities in alerts conditions",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "REST API alerts"
      ],
      "external_id": "7a33e56e9410082971e69e27422d4646cebb7180",
      "image": "",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/rest-api-alerts/manage-entities-alerts-conditions/",
      "published_at": "2021-12-15T09:12:25Z",
      "updated_at": "2021-10-31T08:58:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In alerts, an entity is defined as any target for monitoring, such as an application, a browser, or a host. The alerts UI shows available entities that you can select. You can also use the REST API and API Explorer to add or remove entities for a condition. Requirements Modifying the list of entities in a condition requires you to know: Your API key The { entity_ID} of the entity you want to monitor The { condition_ID} of the condition you want to modify General procedure To update the entity list for a condition: Locate the appropriate entity ID; for example, Application ID and browser ID. Identify the policy ID by name or type. Get the list of conditions associated with the policy and choose the one you want to modify for the appropriate category: APM, browser, and mobile External services Synthetic monitoring Modify the condition using the add or remove API requests. Important Follow the requirements for the minimum and maximum number of entities you can add to conditions. Example: Add/remove an entity The following example shows how to add a Ruby application named TimberTime in a condition and how to remove an entity from that same condition. Only the first step in this example is unique to choosing the Ruby app as the entity. The remaining steps will be the same for whichever entity you choose. Get the entity_id; for example, {application_id}: curl -X GET 'https://api.newrelic.com/v2/applications.json' \\ -H 'Api-Key:$API_KEY' -i Copy OR If you know the application name, use this command and specify the app_name: curl -X GET 'https://api.newrelic.com/v2/applications.json' \\ -H 'Api-Key:$API_KEY' -i \\ -d 'filter[name]=TimberTime' Copy Review the output to find the {application_id}, and use it as the {entity_id}: { \"applications\": [ { \"id\": 12345, <---<<< {application_id} == {entity_id} \"name\": \"TimberTime\", \"language\": \"ruby\", \"health_status\": \"gray\", ... }, Copy Get the policy_id you want to update; for example, the TimberTime app's Logjam Alert policy. To get the policy_id, use this command: curl -X GET 'https://api.newrelic.com/v2/alerts_policies.json' \\ -H 'Api-Key:$API_KEY' -i \\ -G -d 'filter[name]= Logjam Alert' <---<<< {policy_name} Copy Review the policy output; for example: { \"policies\": [ { \"id\": 85, <---<<< {policy_id} \"incident_preference\": \"PER_POLICY\", \"name\": \"Logjam Alert\", \"created_at\": 1461176510393, \"updated_at\": 1461176510393 }, Copy List all of this policy's conditions and locate the {condition_id}: curl -X GET 'https://api.newrelic.com/v2/alerts_conditions.json' \\ -H 'Api-Key:$API_KEY' -i \\ -G -d 'policy_id=85' Copy Review the policy conditions; for example: { \"conditions\": [ { \"id\": 234567, <---<<< {condition_id} \"type\": \"apm_app_metric\", \"name\": \"Throughput (web) (High)\", \"enabled\": true, \"entities\": [ \"8288171\" <---<<< Entity currently included in the policy ], \"metric\": \"response_time_web\", \"terms\": [ { \"duration\": \"5\", \"operator\": \"above\", \"priority\": \"critical\", \"threshold\": \"500\", \"time_function\": \"all\" } ] } ] } Copy Use API requests to add entities to or remove entities from the policy's condition: To add {entity_id} 12345 to {condition_id} 234567, with {entity_type} set as application: curl -X PUT 'https://api.newrelic.com/v2/alerts_entity_conditions/12345.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -G -d 'entity_type=Application&condition_id=234567' Copy To remove {entity_id} 8288171 from {condition_id} 234567, with {entity_type} set as application: curl -X DELETE 'https://api.newrelic.com/v2/alerts_entity_conditions/8288171.json' \\ -H 'Api-Key:$API_KEY' -i \\ -G -d 'entity_type=Application&condition_id=234567' Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.74602,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Manage entities in <em>alerts</em> conditions",
        "sections": "Manage entities in <em>alerts</em> conditions",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "In <em>alerts</em>, an entity is defined as any target for monitoring, such as an application, a browser, or a host. The <em>alerts</em> UI shows available entities that you can select. You can also use the <em>REST</em> <em>API</em> and <em>API</em> Explorer to add or remove entities for a condition. Requirements Modifying the list"
      },
      "id": "6130b580196a67eab24948fb"
    }
  ],
  "/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/rest-api-alerts/manage-entities-alerts-conditions": [
    {
      "sections": [
        "Streaming alerts: key terms and concepts",
        "Why it matters",
        "Streaming alerts process and descriptions",
        "Choose your aggregation method",
        "Event flow",
        "Caution",
        "Event timer",
        "Cadence",
        "Streaming alerts tools",
        "Tip",
        "Window duration",
        "Delay/timer",
        "Loss of signal detection",
        "Gap filling"
      ],
      "title": "Streaming alerts: key terms and concepts",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Get started"
      ],
      "external_id": "144c7f0721ec2acb1e8ac8aa06142911ea552aef",
      "image": "https://docs.newrelic.com/static/39158bd84483adf9516bb7ff5058c9bd/d30ee/streaming-alerts.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/streaming-alerts-key-terms-concepts/",
      "published_at": "2021-12-15T12:47:58Z",
      "updated_at": "2021-12-15T12:47:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The streaming alerts platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into New Relic. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that's processed by the streaming algorithm. There are three methods for aggregating the data filtered through your NRQL condition: Event flow (default) Event timer Cadence This short video describes the three aggregation method (5:31). Why it matters Understanding how streaming alerts works will help you fine-tune your NRQL conditions to be notified about what's important to you. Only data that matches the conditions of the NRQL WHERE clause is alerted on. For more details on each step of the process, see Streaming alerts process and descriptions. As data streams into New Relic, it's filtered by the NRQL condition. Before data is evaluated, it must meet the criteria defined by the NRQL query's WHERE clause. Instead of evaluating that data immediately for violations, the NRQL alert conditions collect the data over a period of time known as the aggregation window. An additional delay/timer allows for slower data points to arrive before the window is aggregated. Once the delay/timer time has elapsed, New Relic aggregates the data into a single data point. Alerts then evaluates the data point using the NRQL condition in order to determine whether it meets the violation threshold criteria. Even if a data point meets the criteria for a violation, a violation may not be triggered. A violation is only triggered when data points consistently meet the threshold criteria over a period of time. This is the threshold duration. If the data points are in violation for an entire threshold duration, we'll send you a notification based on your policy settings. All of these configurable delays give you more control over how you're alerted on sporadic and missing data. Streaming alerts process and descriptions Process Description Streaming data All data coming into New Relic. WHERE clause Filters all incoming streaming data. We only monitor for alerts on data that makes it through this filter. Aggregation methods One of three methods that control how data is collected before it's evaluated. They are: Event flow (Default) Event timer Cadence Aggregation window Data with timestamps that fall within this window will be aggregated and then evaluated. Delay/timer A time delay to ensure all data points have arrived in the aggregation window before aggregation occurs. Aggregated data Data in the aggregated window is collapsed to a single data point for alert evaluation. Evaluation The data point is evaluated by the NRQL condition, which is trigged by each incoming aggregated data point. Threshold duration A specific duration that determines if a violation is created. If your specified NRQL condition meets the threshold criteria over the threshold duration, a violation occurs. When a data point lacks data, a custom value is inserted to fill the gap. Choose your aggregation method You can choose between three different aggregation methods, depending on your needs. Event flow (default) works best for data that comes in frequently and mostly in order. Event timer works best for data that arrives infrequently in batches, such as cloud integration data or infrequent error logs. Cadence is our original and inferior aggregation method. It aggregates data on specific time intervals as detected by New Relic's internal wall clock, regardless of data timestamps. Event flow Event flow aggregates a window of data when the first data point arrives in a subsequent window. The custom delay defines which subsequent window data will start to populate to trigger aggregation of the current window. A custom delay provides extra time for data to arrive. These times are based on the data's timestamps and not New Relic's wall clock time. For example, suppose you're monitoring CPU usage in window durations of 1 minute and a 3 minute delay. When a CPU usage data point comes in with a timestamp between 12:00pm and 12:01pm, event flow will not aggregate that window until a data point shows up with a timestamp between 12:04pm and 12:05pm. When event flow receives the first data point with a timestamp of 12:04pm or later, it sends the 12:00 to 12:01 data to be aggregated. Caution If you expect your data points to arrive more than 30 minutes apart, please use the Event Timer method described below. Event timer Like event flow, event timer will only aggregate data for a given window when data arrives for that window. When a data point arrives for an aggregation window, a timer dedicated to that window starts to count down. If no further data arrives before the timer counts down, the data for that window is aggregated. If more data points arrive before the timer has completed counting down, the timer is reset. For example, suppose you're monitoring CloudWatch data that arrives fairly infrequently. You're using a window duration of 1 minute and a 3 minute timer. When a CloudWatch data point comes in with a timestamp between 12:00pm and 12:01pm, the timer will start to count down. If no further data points show up for that 12:00-12:01 window, the window will get aggregated 3 minutes later. If a new data point with a timestamp between 12:00 and 12:01 arrives, the timer resets. It keeps resetting every time more data points for that window arrive. The window will not be sent for aggregation until the timer reaches 0. If the timer for a later data point elapses before an earlier data point, the event timer method waits for the earlier timer to elapse before aggregating the later data point. For best results, make sure your timer is equal to or longer than your window duration time. If the timer is less than your window duration and your data flow is inconsistent, then your data may be evaluated before all of your data points arrive. This could cause you to be notified incorrectly. Cadence We recommend you use one of the other two methods. Cadence is our old streaming aggregation method. This method uses New Relic's wall clock time to determine when data is aggregated and evaluated. It doesn't take into account data point timestamps as they arrive. Streaming alerts tools Streaming alerts provide a set of tools to give you greater control over how your data is aggregated before it's evaluated to reduce incorrect notifications you receive. They are: Window duration Delay/timer Loss of signal detection Gap filling Tip This article covers these tools at a conceptual level. You'll find direct instructions on how to use these tools in Create NRQL alert conditions. Window duration In order to make loss of signal detection more effective and to reduce unnecessary notifications, you can customize aggregation windows to the duration that you need. An aggregation window is a specific block of time. We gather data points together in an aggregation window, before evaluating the data. A longer aggregation window can smooth out the data, since an outlier data point will have more data points to be aggregated with, giving it less of an influence on the aggregated data point that is sent for evaluation. When a data point arrives, its timestamp is used to put it in the proper aggregation window. You can set your aggregation window to anything between 30 seconds and 15 minutes. The default is 1 minute. Delay/timer The delay/timer setting controls how long the condition should wait before aggregating the data in the aggregation window. The event flow and cadence methods use delay. Event timer uses timer. The delay default is 2 minutes. The timer default is 1 minute and has a minimum value of 5 seconds. Loss of signal detection Loss of signal occurs when no data matches the NRQL condition over a specific period of time. A loss of signal is caused by different things. The WHERE clause in your NRQL query can filter out data before it's evaluated for violations. It could also mean a service or entity is offline or a periodic job has failed to run and no data is being sent to New Relic. In order to avoid unnecessary notifications, you can choose how long to wait before you're notified by a loss of signal violation. You can use loss of signal detection to open violations and be notified when a signal is lost. Alternately, you can use a loss of signal to close violations for ephemeral services or sporadic data, such as error counts. Gap filling Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with the last value received, a static value, or else do nothing and leave the gap there. The default is None. Gaps in streaming data can be caused by network or host issues, a signal may be sparse, or some signals, such as error counts, may only have data when something is wrong. By filling the gaps with known values, the alert evaluation process can process those gaps and determine how they should affect the loss of signal evaluation. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss and gap filling, see this Explorers Hub post.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 177.89917,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "sections": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "The streaming <em>alerts</em> platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into <em>New</em> <em>Relic</em>. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that&#x27;s processed"
      },
      "id": "6130c054e7b9d269edb6f21d"
    },
    {
      "sections": [
        "REST API calls for alerts",
        "Important",
        "Available data and functions via API",
        "Conditions excluded from the REST API",
        "Alert policies",
        "Create new policies",
        "Update policies",
        "Delete existing policies",
        "List existing policies",
        "Notification channels",
        "Create new notification channels",
        "Email channel",
        "OpsGenie channel",
        "Slack channel",
        "VictorOps channel",
        "PagerDuty channel",
        "Webhook (json) channel",
        "Webhook (x-www-form-urlencoded) channel",
        "Delete existing notification channels",
        "List existing notification channels",
        "Update notification channels associated with policies",
        "Delete notification channels associated with policies",
        "Conditions for APM, browser, mobile",
        "Create conditions for policies",
        "Update conditions for policies",
        "Delete conditions from policies",
        "List existing conditions for policies",
        "Conditions for NRQL",
        "Create NRQL conditions for policies",
        "Update NRQL conditions for policies",
        "Delete NRQL conditions for policies",
        "List existing NRQL conditions for policies",
        "Conditions for external services",
        "External services: Create conditions for policies",
        "External services: Update conditions for policies",
        "External services: Delete conditions from policies",
        "External services: List existing conditions for policies",
        "Conditions for Synthetic monitoring",
        "Synthetics: Create conditions for policies",
        "Synthetic monitoring: Update conditions for policies",
        "Synthetic monitoring: Delete conditions from policies",
        "Synthetic monitoring: List existing conditions for policies",
        "Alert activity: Events, violations, incidents",
        "List Events",
        "List Violations",
        "Tip",
        "List Incidents",
        "Show Incident",
        "Acknowledge Incident",
        "Close Incident",
        "Alert entity conditions",
        "List condition by entity",
        "Add an entity to a condition",
        "Remove an entity from a condition"
      ],
      "title": "REST API calls for alerts",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "REST API alerts"
      ],
      "external_id": "af566e5c2f0a695c1146b5531849bb49248ea0ec",
      "image": "",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/rest-api-alerts/rest-api-calls-alerts/",
      "published_at": "2021-12-14T22:59:19Z",
      "updated_at": "2021-10-31T13:25:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our REST API (v2) allows you to configure settings for alerts. The API Explorer also includes the curl request format, available parameters, potential response status codes, and JSON response structure for each of the available API calls. You can also create alert conditions in the UI. Important For infrastructure alerting, see REST API for infrastructure monitoring alerts. Available data and functions via API REST API functions Comments View account data In general, any role can use a user key or REST API key with GET > List functions to view alerts data. The account Owner and Admins may use their API key. List output will be paginated. Available functions include: Alert policies Notification channels Conditions for APM, browser, and mobile (Some limitations apply.) Conditions for external services Conditions for Synthetic monitoring Conditions for NRQL (Some limitations apply.) Events Violations Incidents Maintain account data You may have an Owner or Admin role in your account and a user key or have a custom role that grants permissions to manage Alerts and a user key in order to use any maintenance function, including POST > Create, PUT > Add, PUT > Update, and DELETE. Conditions excluded from the REST API These types of conditions do not have available endpoints in the API: APM: Web transaction percentiles, conditions targeting labels, and baselines NRQL: Baselines Alert policies These API functions include links to the API Explorer, where you can create, delete, or list policies. Important If your account hosts data in the EU data center, ensure you're using the proper API endpoints for EU region accounts. Create new policies To add new policies, use your user key and include these two values in the API call: Required values Definition Incident incident_preference Determines how Alerts will create incidents and group violations. This must be one of the following: PER_POLICY (default): Roll up by policy. PER_CONDITION: Roll up by condition. PER_CONDITION_AND_TARGET: Roll up by target and condition. Policy name The policy name is required. Leaving it unchanged will create a policy called string. API Explorer: Alerts Policies > POST > Create curl -X POST 'https://api.newrelic.com/v2/alerts_policies.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -d \\ '{ \"policy\": { \"incident_preference\": \"string\", \"name\": \"string\" } }' Copy Update policies Use this API to update an existing policy's name and incident_preference. You'll need your user key. Required values Definition id Required. To find a policy's ID, use either of these options: From the UI: On a policy's UI page, find the ID under the policy name. With the API: Use the List policies API. Incident incident_preference Determines how alerts will create incidents and group violations. Must be one of the following: PER_POLICY (default): Roll up by policy. PER_CONDITION: Roll up by condition. PER_CONDITION_AND_TARGET: Roll up by target and condition. Policy name Required. If you do not change the name, it defaults to a policy called string. To find a policy's exact name, use the List policies API. API Explorer: Alerts Policies > PUT > Update curl -X PUT 'https://api.newrelic.com/v2/alerts_policies/{id}.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -d \\ '{ \"policy\": { \"incident_preference\": \"string\", \"name\": \"string\" } }' Copy Delete existing policies To delete an existing policy, use your user key, and include the policy_id (available from API Explorer: Alerts Policies > GET > List) in the API call: API Explorer: Alerts Policies > DELETE > Delete curl -X DELETE 'https://api.newrelic.com/v2/alerts_policies/$POLICY_ID.json' \\ -H 'Api-Key:$API_KEY' -i Copy List existing policies To view a list of existing policies for your account, use your user key, and include these optional values in the API call: Optional policy name filter Optional pagination value API Explorer: Alerts Policies > GET > List curl -X GET 'https://api.newrelic.com/v2/alerts_policies.json' \\ -H 'Api-Key:$API_KEY' -i Copy Notification channels These API functions include links to the API Explorer, where you can create, delete, or list Alerts notification channels. Important If your account hosts data in the EU data center, ensure you are using the proper API endpoints for EU region accounts. Create new notification channels To add new notification channels, use your user key and include these values in the API call: New channel's name Type of channel Configuration values The API Explorer shows the format for required configuration values for each type of notification channel. API Explorer: Alerts Channels > POST > Create curl -X POST 'https://api.newrelic.com/v2/alerts_channels.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -d \\ '{ \"channel\": { \"name\": \"string\", \"type\": \"string\", \"configuration\": \"hash\" } }' Copy The \"hash\" place holder in this example must be replaced by one of the following JSON blocks with the appropriate values substituted: Email channel { \"recipients\" : \"test@google.com\", \"include_json_attachment\" : true } Copy OpsGenie channel { \"api_key\": \"abc123\", \"teams\": \"team1\", \"tags\": \"tag1\", \"recipients\": \"me@me.com\" } Copy Slack channel { \"url\": \"http://example.com\", \"channel\": \"channel1\" } Copy VictorOps channel { \"key\": \"mykey\", \"route_key\": \"theroute\" } Copy PagerDuty channel { \"service_key\": \"myservicekey\" } Copy Webhook (json) channel This example shows the default payload inserted. (The payload attribute is optional.) The payload can be customized to have different keys than the ones shown and will be sent with the $ prefixed attributes interpolated prior to delivery. Important The condition_id is deprecated. Instead, use condition_family_id. { \"base_url\": \"http://example.com\", \"auth_username\": \"username\", \"auth_password\": \"password\", \"payload_type\": \"application/json\", \"headers\": { \"header1\": \"test\", \"header2\": \"test\" } \"payload\": { \"account_id\": \"$ACCOUNT_ID\", \"account_name\": \"$ACCOUNT_NAME\", \"closed_violations_count_critical\": \"$CLOSED_VIOLATIONS_COUNT_CRITICAL\", \"closed_violations_count_warning\": \"$CLOSED_VIOLATIONS_COUNT_WARNING\", \"condition_family_id\": \"$CONDITION_FAMILY_ID\", \"condition_id\": \"$CONDITION_ID\", \"condition_name\": \"$CONDITION_NAME\", \"current_state\": \"$EVENT_STATE\", \"details\": \"$EVENT_DETAILS\", \"duration\": \"$DURATION\", \"event_type\": \"$EVENT_TYPE\", \"incident_acknowledge_url\": \"$INCIDENT_ACKNOWLEDGE_URL\", \"incident_id\": \"$INCIDENT_ID\", \"incident_url\": \"$INCIDENT_URL\", \"open_violations_count_critical\": \"$OPEN_VIOLATIONS_COUNT_CRITICAL\", \"open_violations_count_warning\": \"$OPEN_VIOLATIONS_COUNT_WARNING\", \"owner\": \"$EVENT_OWNER\", \"policy_name\": \"$POLICY_NAME\", \"policy_url\": \"$POLICY_URL\", \"runbook_url\": \"$RUNBOOK_URL\", \"severity\": \"$SEVERITY\", \"targets\": \"$TARGETS\", \"timestamp\": \"$TIMESTAMP\", \"violation_callback_url\": \"$VIOLATION_CALLBACK_URL\", \"violation_chart_url\": \"$VIOLATION_CHART_URL\" } } Copy Webhook (x-www-form-urlencoded) channel This example shows the default payload inserted. (The payload attribute is optional.) The payload can be customized to have different keys than the ones shown and will be sent with the $ prefixed attributes interpolated prior to delivery. Important The condition_id is deprecated. Instead, use condition_family_id. { \"base_url\": \"http://example.com\", \"auth_username\": \"username\", \"auth_password\": \"password\", \"payload_type\": \"application/x-www-form-urlencoded\", \"headers\": { \"header1\": \"test\", \"header2\": \"test\" }, \"payload\": { \"account_id\": \"$ACCOUNT_ID\", \"account_name\": \"$ACCOUNT_NAME\", \"closed_violations_count_critical\": \"$CLOSED_VIOLATIONS_COUNT_CRITICAL\", \"closed_violations_count_warning\": \"$CLOSED_VIOLATIONS_COUNT_WARNING\", \"condition_family_id\": \"$CONDITION_FAMILY_ID\", \"condition_id\": \"$CONDITION_ID\", \"condition_name\": \"$CONDITION_NAME\", \"current_state\": \"$EVENT_STATE\", \"details\": \"$EVENT_DETAILS\", \"duration\": \"$DURATION\", \"event_type\": \"$EVENT_TYPE\", \"incident_acknowledge_url\": \"$INCIDENT_ACKNOWLEDGE_URL\", \"incident_id\": \"$INCIDENT_ID\", \"incident_url\": \"$INCIDENT_URL\", \"open_violations_count_critical\": \"$OPEN_VIOLATIONS_COUNT_CRITICAL\", \"open_violations_count_warning\": \"$OPEN_VIOLATIONS_COUNT_WARNING\", \"owner\": \"$EVENT_OWNER\", \"policy_name\": \"$POLICY_NAME\", \"policy_url\": \"$POLICY_URL\", \"runbook_url\": \"$RUNBOOK_URL\", \"severity\": \"$SEVERITY\", \"targets\": \"$TARGETS\", \"timestamp\": \"$TIMESTAMP\", \"violation_callback_url\": \"$VIOLATION_CALLBACK_URL\", \"violation_chart_url\": \"$VIOLATION_CHART_URL\" } } Copy Delete existing notification channels To delete an existing notification channels, use your user key, and include the channel_id (available from API Explorer: Alerts Channels > GET > List) in the API call: API Explorer: Alerts Channels > DELETE > Delete curl -X DELETE 'https://api.newrelic.com/v2/alerts_channels/{channel_id}.json' \\ -H 'Api-Key:$API_KEY' -i Copy List existing notification channels To view a list of existing notification channels for your account, use your user key and an optional pagination value in the API call. API Explorer: Alerts Channels > GET > List curl -X GET 'https://api.newrelic.com/v2/alerts_channels.json' \\ -H 'Api-Key:$API_KEY' -i Copy Update notification channels associated with policies You can associate a policy with one or more notification channels. You can also associate a notification channel with one or more policies. To add notification channels to policies, use your user key and these values in the API call: A policy_id value (available from API Explorer: Alerts Policies > GET > List) One or more channel_id values in an array, separated by commas or a new line (available from API Explorer: Alerts Channels > GET > List) API Explorer: Alerts Policy Channels > PUT > Update curl -X PUT 'https://api.newrelic.com/v2/alerts_policy_channels.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -G -d 'policy_id=$POLICY_ID&channel_ids=channel_id' Copy Delete notification channels associated with policies You can associate a policy with one or more notification channels. You can also associate a notification channel with one or more alert policies. To remove a policy from a channel, or to remove a channel from a policy, use your user key and these values in the API call: The policy_id (available from API Explorer: Alerts Policies > GET > List) The channel_id (available from API Explorer: Alerts Channels > GET > List) API Explorer: Alerts Policy Channels > DELETE > Delete curl -X DELETE 'https://api.newrelic.com/v2/alerts_policy_channels.json' \\ -H 'Api-Key:$API_KEY' -i \\ -d 'channel_id=CHANNEL_ID&policy_id=POLICY_ID' Copy Conditions for APM, browser, mobile These API functions include links to the API Explorer, where you can create, update, delete, or list policy conditions. These calls are for conditions with these types of entities: APM: Apps and key transactions Browser: Apps and key transactions Mobile: Mobile apps Exception: The following APM conditions are not available from this API endpoint: Conditions targeting labels (dynamic targeting) Web transaction percentile conditions Baseline conditions Important The API Explorer provides information about other types of conditions using separate endpoints, including external services (APM and mobile), and synthetic monitoring. Consider all types of alert conditions when searching or updating. If your account hosts data in the EU data center, ensure you are using the proper API endpoints for EU region accounts. Create conditions for policies To add conditions to policies, include these values in the API call: Your user key The policy_id (available from API Explorer: Alerts Policies > GET > List) The required condition values in the API call (described in the API Explorer page and in the Alerts conditions API glossary) API Explorer: Alerts Conditions > POST > Create curl -X POST 'https://api.newrelic.com/v2/alerts_conditions/policies/$POLICY_ID.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -d \\ '{ \"condition\": { \"type\": \"string\", \"name\": \"string\", \"enabled\": boolean, \"entities\": [ integer ], \"metric\": \"string\", \"gc_metric\": \"string\", \"condition_scope\": \"string\", \"violation_close_timer\": integer, \"runbook_url\": \"string\", \"terms\": [ { \"duration\": \"string\", \"operator\": \"string\", \"priority\": \"string\", \"threshold\": \"string\", \"time_function\": \"string\" } ], \"user_defined\": { \"metric\": \"string\", \"value_function\": \"string\" } } }' Copy The JSON response returns a condition id, which you will need to update or delete the condition. You can also view the condition id from API Explorer: Alerts Conditions > GET > List. Update conditions for policies To update conditions for policies, include these values in the API call: Your user key The condition's id (available from API Explorer: Alerts Conditions > GET > List) The required condition values in the API call (described in the API Explorer page and in the Alerts conditions API glossary) API Explorer: Alerts Conditions > PUT > Update curl -X PUT 'https://api.newrelic.com/v2/alerts_conditions/$CONDITION_ID.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -d \\ '{ \"condition\": { \"type\": \"string\", \"name\": \"string\", \"enabled\": boolean, \"entities\": [ integer ], \"metric\": \"string\", \"metric\": \"string\", \"gc_metric\": \"string\", \"condition_scope\": \"string\", \"violation_close_timer\": integer, \"runbook_url\": \"string\", \"terms\": [ { \"duration\": \"string\", \"operator\": \"string\", \"priority\": \"string\", \"threshold\": \"string\", \"time_function\": \"string\" } ], \"user_defined\": { \"metric\": \"string\", \"value_function\": \"string\" } } }' Copy Delete conditions from policies To delete conditions from policies, include these values in the API call: Your user key The condition_id (available from API Explorer: Alerts Conditions > GET > List) API Explorer: Alerts Conditions > DELETE > Delete curl -X DELETE 'https://api.newrelic.com/v2/alerts_conditions/$CONDITION_ID.json' \\ -H 'Api-Key:$API_KEY' -i Copy List existing conditions for policies To view a list of existing conditions for your policy, use your REST API key or user key, and the associated policy_id in the API call. API Explorer: Alerts Conditions > GET > List curl -X GET 'https://api.newrelic.com/v2/alerts_conditions.json' \\ -H 'Api-Key:$API_KEY' -i \\ -d 'policy_id=$POLICY_ID' Copy Conditions for NRQL These API functions include links to the API Explorer, where you can create, update, delete, or list NRQL conditions for your policies. Exception: NRQL baseline conditions are not available from this endpoint. Important If your account hosts data in the EU data center, ensure you are using the proper API endpoints for EU region accounts. Create NRQL conditions for policies Recommendation: Due to the way NRQL data is streamed, set the aggregation_method to EVENT_FLOW and use the default settings of 60 for aggregation_window and 120 for aggregation_delay. EVENT_FLOW works in most use-cases, but for a discussion on which use cases work better with EVENT_TIMER, see Which aggregation method to use?. API Explorer: Alerts Nrql Conditions > POST > Create To create NRQL conditions for policies: curl -X POST 'https://api.newrelic.com/v2/alerts_nrql_conditions/policies/$POLICY_ID.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -d \\ '{ \"nrql_condition\": { \"type\": \"string\", \"name\": \"string\", \"runbook_url\": \"string\", \"enabled\": boolean, \"terms\": [ { \"duration\": \"string\", \"operator\": \"string\", \"priority\": \"string\", \"threshold\": \"string\", \"time_function\": \"string\" } ], \"value_function\": \"string\", \"nrql\": { \"query\": \"string\" }, \"signal\": { \"aggregation_window\": \"string\", \"aggregation_method\": \"string\", \"aggregation_delay\": integer, \"aggregation_timer\": integer, \"fill_option\": \"string\", \"fill_value\": \"string\" }, \"expiration\": { \"expiration_duration\": \"string\", \"open_violation_on_expiration\": boolean, \"close_violations_on_expiration\": boolean } } }' Copy If you set type as outlier, you can omit the value_function. However, two additional fields are required: expected_groups and ignore_overlap. For more information, see Alerts Conditions API field names. If you omit type or set it as static, it will default to standard NRQL alerting. Update NRQL conditions for policies To update NRQL conditions for policies, include these values in the API call: Your user key The condition's id (available from API Explorer: Alerts Nrql Conditions > GET > List) The required condition values in the API call (described in the API Explorer page and in the Alerts conditions API glossary) API Explorer: Alerts Nrql Conditions > PUT > Update curl -X PUT 'https://api.newrelic.com/v2/alerts_nrql_conditions/$CONDITION_ID.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -d \\ '{ \"nrql_condition\": { \"name\": \"string\", \"runbook_url\": \"string\", \"enabled\": boolean, \"terms\": [ { \"duration\": \"string\", \"operator\": \"string\", \"priority\": \"string\", \"threshold\": \"string\", \"time_function\": \"string\" } ], \"value_function\": \"string\", \"nrql\": { \"query\": \"string\" } } }' Copy Delete NRQL conditions for policies To delete NRQL conditions from policies, include these values in the API call: Your user key The condition's id (available from API Explorer: Alerts Nrql Conditions > GET > List) API Explorer: Alerts Nrql Conditions > DELETE > Delete curl -X DELETE 'https://api.newrelic.com/v2/alerts_nrql_conditions/$CONDITION_ID.json' \\ -H 'Api-Key:$API_KEY' -i Copy List existing NRQL conditions for policies To view a list of existing conditions for your alert policy, use your user key or REST API key, and the associated policy_id in the API call. API Explorer: Alerts Nrql Conditions > GET > List curl -X GET 'https://api.newrelic.com/v2/alerts_nrql_conditions.json' \\ -H 'Api-Key:$API_KEY' -i \\ -d 'policy_id=$POLICY_ID' Copy Conditions for external services These API functions include links to the API Explorer, where you can create, update, delete, or list policy conditions. These calls are for conditions for external services. The API calls can be used with APM and mobile monitoring apps. Important If your account hosts data in the EU data center, ensure you are using the proper API endpoints for EU region accounts. External services: Create conditions for policies To add conditions for external services to policies that have APM or mobile monitoring apps, include these values in the API call: Your user key The policy_id (available from API Explorer: Alerts Policies > GET > List) The required external_service_condition values in the API call (described in the API Explorer page and in the Alerts conditions API glossary) API Explorer: Alerts External Service Conditions > POST > Create curl -X POST 'https://api.newrelic.com/v2/alerts_external_service_conditions/policies/$POLICY_ID.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -d \\ '{ \"external_service_condition\": { \"type\": \"string\", \"name\": \"string\", \"enabled\": boolean, \"entities\": [ integer ], \"external_service_url\": \"string\", \"metric\": \"string\", \"runbook_url\": \"string\", \"terms\": [ { \"duration\": \"string\", \"operator\": \"string\", \"priority\": \"string\", \"threshold\": \"string\", \"time_function\": \"string\" } ] } }' Copy The JSON response returns a condition id, which you will need to update or delete the condition. You can also view the condition id from API Explorer: Alerts External Service Conditions > GET > List. External services: Update conditions for policies To update conditions for external services to policies that have APM or mobile monitoring apps, include these values in the API call: Your user key The external service condition's id (available from API Explorer: Alerts External Service Conditions > GET > List) The required external_service_condition values in the API call (described in the API Explorer page and in the Alerts conditions API glossary) API Explorer: Alerts External Service Conditions > PUT > Update curl -X PUT 'https://api.newrelic.com/v2/alerts_external_service_conditions/$CONDITION_ID.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -d \\ '{ \"external_service_condition\": { \"type\": \"string\", \"name\": \"string\", \"enabled\": boolean, \"entities\": [ integer ], \"external_service_url\": \"string\", \"metric\": \"string\", \"runbook_url\": \"string\", \"terms\": [ { \"duration\": \"string\", \"operator\": \"string\", \"priority\": \"string\", \"threshold\": \"string\", \"time_function\": \"string\" } ] } }' Copy External services: Delete conditions from policies To delete conditions for external services from policies with APM or mobile monitoring apps, include these values in the API call: Your user key The condition_id (available from API Explorer: Alerts External Service Conditions > GET > List) API Explorer: Alerts External Service Conditions > DELETE > Delete curl -X DELETE 'https://api.newrelic.com/v2/alerts_external_service_conditions/$CONDITION_ID.json' \\ -H 'Api-Key:$API_KEY' -i Copy External services: List existing conditions for policies To view a list of existing conditions for policies with external service apps (APM or mobile monitoring), use your user key or REST API key and the associated policy_id in the API call. API Explorer: Alerts External Service Conditions > GET > List curl -X GET 'https://api.newrelic.com/v2/alerts_external_service_conditions.json' \\ -H 'Api-Key:$API_KEY' -i \\ -d 'policy_id=$POLICY_ID' Copy Conditions for Synthetic monitoring These API functions include links to the API Explorer, where you can create, update, delete, or list conditions for your alert policies. The API calls can be used with Synthetic monitoring. Synthetics: Create conditions for policies To add conditions to policies for Synthetic monitoring, include these values in the API call: Your user key The policy_id (available from API Explorer: Alerts Policies > GET > List) The required synthetics_condition values in the API call (described in the API Explorer page and in the Alerts conditions API glossary) API Explorer: Alerts Synthetics Conditions > POST > Create curl -X POST 'https://api.newrelic.com/v2/alerts_synthetics_conditions/policies/$POLICY_ID.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -d \\ '{ \"synthetics_condition\": { \"name\": \"string\", \"monitor_id\": \"string\", \"runbook_url\": \"string\", \"enabled\": boolean } }' Copy The JSON response returns a condition id, which you will need to update or delete the condition. You can also view the condition id from API Explorer: Alerts Synthetics Conditions > GET > List. Synthetic monitoring: Update conditions for policies To update policy conditions for Synthetic monitoring, include these values in the API call: Your user key The condition id (available from API Explorer: Alerts Synthetics Conditions > GET > List) The required synthetics_condition values in the API call (described in the API Explorer page to create alert conditions for Synthetics and in the Alerts conditions API glossary) API Explorer: Alerts Synthetics Conditions > PUT > Update curl -X PUT 'https://api.newrelic.com/v2/alerts_synthetics_conditions/$CONDITION_ID.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -d \\ '{ \"synthetics_condition\": { \"name\": \"string\", \"monitor_id\": \"string\", \"runbook_url\": \"string\", \"enabled\": boolean } }' Copy Synthetic monitoring: Delete conditions from policies To delete policy conditions for Synthetic monitoring, include these values in the API call: Your user key The condition_id (available from API Explorer: Alerts Synthetics Conditions > GET > List) API Explorer: Alerts Synthetics Conditions > DELETE > Delete curl -X DELETE 'https://api.newrelic.com/v2/alerts_synthetics_conditions/$CONDITION_ID.json' \\ -H 'Api-Key:$API_KEY' -i Copy Synthetic monitoring: List existing conditions for policies To view a list of existing policy conditions for Synthetic monitoring, use your user key or REST API key, and the associated policy_id in the API call. API Explorer: Alerts Synthetics Conditions > GET > List curl -X GET 'https://api.newrelic.com/v2/alerts_synthetics_conditions.json' \\ -H 'Api-Key:$API_KEY' -i \\ -d 'policy_id=$POLICY_ID' Copy Alert activity: Events, violations, incidents These API functions include links to the API Explorer, where you can view information about events, violations, and incidents for your alert policies. Important If your account hosts data in the EU data center, ensure you are using the proper API endpoints for EU region accounts. List Events To view events for entities monitored by APM, browser, mobile, synthetic monitoring, and alerts, include these values in the API call: Your user key or REST API key Other optional values to use as filters (described in the API Explorer page) that depend on the type of product (browser monitoring, mobile monitoring, etc.), entity (as apps or key transactions for APM, synthetic monitoring, etc.), and type of event (notification, deployment, instrumentation, etc.) An optional pagination value API Explorer: Alerts Events > GET > List curl -X GET 'https://api.newrelic.com/v2/alerts_events.json' \\ -H 'Api-Key:$API_KEY' -i Copy List Violations To view violations for any entity monitored for your account, include these values in the API call: Your user key or REST API key An optional flag to show only those violations that are currently open An optional pagination value API Explorer: Alerts Violations > GET > List curl -X GET 'https://api.newrelic.com/v2/alerts_violations.json' \\ -H 'Api-Key:$API_KEY' -i Copy Tip To find policy, condition, and incident information: take the IDs found in the links section in the violations property and place them into the URLs contained in the outer links property of the payload. List Incidents To view incidents for any entity monitored for your account, include these values in the API call: Your user key or REST API key An optional flag to show only those incidents that are currently open An optional flag to exclude violation data from response An optional pagination value API Explorer: Alerts Incidents > GET > List curl -X GET 'https://api.newrelic.com/v2/alerts_incidents.json' \\ -H 'Api-Key:$API_KEY' -i Copy Show Incident To show a single incident associated with your account, include these values in the API call: Your user key or REST API key An incident ID API Explorer: Alerts Incidents > GET > Show curl -X GET 'https://api.newrelic.com/v2/alerts_incidents/{id}.json' \\ -H 'Api-Key:$API_KEY' -i Copy Acknowledge Incident To acknowledge an incident associated with your account, include these values in the API call: Your user key An incident ID API Explorer: Alerts Incidents > PUT > Acknowledge curl -X PUT 'https://api.newrelic.com/v2/alerts_incidents/{id}/acknowledge.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' Copy Close Incident To close an incident associated with your account, include these values in the API call: Your user key An incident ID API Explorer: Alerts Incidents > PUT > Close curl -X PUT 'https://api.newrelic.com/v2/alerts_incidents/{id}/close.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' Copy Alert entity conditions These API functions include links to the API Explorer, where you can list, add and remove entities in the conditions for your alert policies. Important If your account hosts data in the EU data center, ensure you are using the proper API endpoints for EU region accounts. List condition by entity To view the conditions an entity that apply to an entity in APM, browser, mobile, or key transactions, include these values in the API call: Your user key or REST API key The entity_id This is the specific entity (alert target) to be monitored. The entity_type, which must be one of the following: Application BrowserApplication MobileApplication KeyTransaction API Explorer: Alerts Entity Conditions > GET > list curl -X GET 'https://api.newrelic.com/v2/alerts_entity_conditions/$ENTITY_ID.json' \\ -H 'Api-Key:$API_KEY' -i \\ -G -d 'entity_type=$ENTITY_TYPE' Copy Add an entity to a condition To add an entity to a condition, include these values in the API call: Your user key The entity_id This is the entity (alert target) to be monitored. It is the numeric ID for the APM application, browser app, key transaction, or mobile app. The condition_id (available from API Explorer: Alerts Conditions > GET > List) The entity_type, which must be one of the following: Application BrowserApplication MobileApplication KeyTransaction API Explorer: Alerts Entity Conditions > PUT > Add curl -X PUT 'https://api.newrelic.com/v2/alerts_entity_conditions/$ENTITY_ID.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -G -d 'entity_type=$ENTITY_TYPE&condition_id=$CONDITION_ID' Copy Remove an entity from a condition To remove an entity being monitored from a condition, include these values in the API call: Your user key The entity_id This is the specific monitored entity (alert target) to be removed. It is the numeric ID for the APM application, browser app, key transaction, or mobile app. The condition_id (available from API Explorer: Alerts Conditions > GET > List) The entity_type, which must be one of the following: Application BrowserApplication MobileApplication KeyTransaction API Explorer: Alerts Entity Conditions > DELETE > Remove curl -X DELETE 'https://api.newrelic.com/v2/alerts_entity_conditions/$ENTITY_ID.json' \\ -H 'Api-Key:$API_KEY' -i \\ -G -d 'entity_type=$ENTITY_ID&condition_id=$CONDITION_ID' Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.83958,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>REST</em> <em>API</em> calls for <em>alerts</em>",
        "sections": "<em>REST</em> <em>API</em> calls for <em>alerts</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "Our <em>REST</em> <em>API</em> (v2) allows you to configure settings for <em>alerts</em>. The <em>API</em> Explorer also includes the curl request format, available parameters, potential response status codes, and JSON response structure for each of the available <em>API</em> calls. You can also create <em>alert</em> conditions in the UI. Important"
      },
      "id": "6130c144e7b9d2d160b6f25d"
    },
    {
      "sections": [
        "Disable and enable alerts conditions using the API",
        "Requirements",
        "Enable and disable a condition",
        "Details on searching for condition ID",
        "Details on Update API requests",
        "Tip",
        "Example: Disable an APM condition"
      ],
      "title": "Disable and enable alerts conditions using the API",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "REST API alerts"
      ],
      "external_id": "4ffe50fd7e1a38a9dee007fe10cb63d28b955a8e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/rest-api-alerts/disable-enable-alerts-conditions-using-api/",
      "published_at": "2021-12-15T09:49:07Z",
      "updated_at": "2021-10-31T08:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In a policy, a condition identifies what triggers an alert. You can use the REST API to disable and enable conditions. You can also disable and enable conditions in New Relic One. Policies can't be enabled or disabled, whether via the API or the UI. Policies can only be created, deleted, or have their conditions changed. Requirements Modifying any attribute in a condition using the API requires: An API key and permissions to manage Alerts The condition's id (available from API Explorer: Alerts Conditions > GET > List) If your account hosts data in the EU data center, ensure you are using the proper API endpoints for EU region accounts. Enable and disable a condition The process for disabling or enabling a condition is the same general process for changing any attribute in a condition. A more detailed example comes after this general procedure: Find the ID for the policy that contains the condition you want to change. If the policy's ID is unknown, use the policy's name or type to make an API call and find the policy's ID. For more on this process, see List a single policy. With the policy ID, make an API call that returns the conditions associated with that policy. There are four different condition categories. If you don't know the category, this may require making up to four API calls in order to find the condition. Details on searching for condition ID If you don't know the category of the condition you want to change, you must search for it by making API calls using the four condition categories. Here are the different API call formats for the various condition categories. APM, browser, and mobile monitoring Conditions available: apm_app_metric, apm_kt_metric, browser_metric, and mobile_metric API Explorer link Get>List External services Conditions available: apm_external_service, mobile_external_service API Explorer link Get>List Synthetic monitoring API Explorer link Get>List For the returned JSON, find the JSON object of the condition you want to change. Copy and paste the condition's JSON in a text editor of your choice and edit the JSON. To enable the condition, set \"enabled\" to true. To disable the condition, set \"enabled\" to false. Update the condition by submitting your edited JSON via an Update API request. Our different products require different API requests. Details on Update API requests Use the Update API request that corresponds to the product in question: Conditions for APM, browser, and mobile Conditions available: apm_app_metric, apm_kt_metric, browser_metric, and mobile_metric API Explorer PUT>Update link Conditions for external services Conditions available: apm_external_service, mobile_external_service API Explorer PUT>Update Conditions for Synthetic monitoring) API Explorer PUT>Update Tip An Update API request can only change one condition at a time, it cannot update a vector of objects. For example, to change three conditions, you will have to make three separate requests. Example: Disable an APM condition The following example shows how to disable a condition for an apm_app_metric condition. With the exception of the types of API calls required, the process is similar to the process for changing other condition types. Obtain the policy_id of the policy you want to update. For an imaginary policy named Logjam Alert, the command would be: curl -X GET 'https://api.newrelic.com/v2/alerts_policies.json' \\ -H 'Api-Key:$API_KEY' -i \\ -G --data-urlencode 'filter[name]= Logjam Alert' <---<<< {policy_name} Copy The output for this request might look like: { \"policies\": [ { \"id\": 85, <---<<< $POLICY_ID \"incident_preference\": \"PER_POLICY\", \"name\": \"Logjam Alert\", \"created_at\": 1461176510393, \"updated_at\": 1461176510393 } ] } Copy List all of this policy's conditions and locate the {condition_id}: curl -X GET 'https://api.newrelic.com/v2/alerts_conditions.json' \\ -H 'X-Api-Key:$API_KEY' -i \\ -G -d 'policy_id=85' Copy The output for this request might look like: { \"conditions\": [ { \"id\": 12345, <---<<< $CONDITION_ID \"type\": \"apm_app_metric\", \"name\": \"Apdex (Low)\", \"enabled\": true, <---<<< Note the condition is enabled \"entities\": [ \"8288171\" ], \"metric\": \"apdex\", \"terms\": [ { \"duration\": \"5\", \"operator\": \"below\", \"priority\": \"critical\", \"threshold\": \"1\", \"time_function\": \"any\" } ] }, { \"id\": 2468, <---<<< another condition_id \"type\": \"apm_app_metric\", \"name\": \"Throughput (Low)\", ... } ] } Copy Copy the JSON for only the condition in question and paste it in a text editor. Change \"enabled\": true to \"enabled\": false. The edited JSON would look like: curl -X PUT 'https://api.newrelic.com/v2/alerts_conditions/12345.json' \\ -H 'X-Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -d \\ '{ \"condition\": { \"type\": \"apm_app_metric\", \"name\": \"Apdex (Low)\", \"enabled\": false, <---<<< Changed to false \"entities\": [ \"8288171\" ], \"metric\": \"apdex\", \"terms\": [ { \"duration\": \"5\", \"operator\": \"below\", \"priority\": \"critical\", \"threshold\": \"1\", \"time_function\": \"any\" } ] } }' Copy Update the condition by submitting the edited condition JSON via an Update API request. For this specific condition, you would follow the steps in Update conditions for APM policies. Other product conditions would have other API requests, as detailed in Update API requests.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.74263,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Disable <em>and</em> enable <em>alerts</em> conditions using the <em>API</em>",
        "sections": "Disable <em>and</em> enable <em>alerts</em> conditions using the <em>API</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "In a policy, a condition identifies what triggers an <em>alert</em>. You can use the <em>REST</em> <em>API</em> to disable and enable conditions. You can also disable and enable conditions in <em>New</em> <em>Relic</em> One. Policies can&#x27;t be enabled or disabled, whether via the <em>API</em> or the UI. Policies can only be created, deleted, or have"
      },
      "id": "6130bf9d28ccbc076856a85b"
    }
  ],
  "/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/rest-api-alerts/rest-api-calls-alerts": [
    {
      "sections": [
        "Streaming alerts: key terms and concepts",
        "Why it matters",
        "Streaming alerts process and descriptions",
        "Choose your aggregation method",
        "Event flow",
        "Caution",
        "Event timer",
        "Cadence",
        "Streaming alerts tools",
        "Tip",
        "Window duration",
        "Delay/timer",
        "Loss of signal detection",
        "Gap filling"
      ],
      "title": "Streaming alerts: key terms and concepts",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Get started"
      ],
      "external_id": "144c7f0721ec2acb1e8ac8aa06142911ea552aef",
      "image": "https://docs.newrelic.com/static/39158bd84483adf9516bb7ff5058c9bd/d30ee/streaming-alerts.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/streaming-alerts-key-terms-concepts/",
      "published_at": "2021-12-15T12:47:58Z",
      "updated_at": "2021-12-15T12:47:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The streaming alerts platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into New Relic. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that's processed by the streaming algorithm. There are three methods for aggregating the data filtered through your NRQL condition: Event flow (default) Event timer Cadence This short video describes the three aggregation method (5:31). Why it matters Understanding how streaming alerts works will help you fine-tune your NRQL conditions to be notified about what's important to you. Only data that matches the conditions of the NRQL WHERE clause is alerted on. For more details on each step of the process, see Streaming alerts process and descriptions. As data streams into New Relic, it's filtered by the NRQL condition. Before data is evaluated, it must meet the criteria defined by the NRQL query's WHERE clause. Instead of evaluating that data immediately for violations, the NRQL alert conditions collect the data over a period of time known as the aggregation window. An additional delay/timer allows for slower data points to arrive before the window is aggregated. Once the delay/timer time has elapsed, New Relic aggregates the data into a single data point. Alerts then evaluates the data point using the NRQL condition in order to determine whether it meets the violation threshold criteria. Even if a data point meets the criteria for a violation, a violation may not be triggered. A violation is only triggered when data points consistently meet the threshold criteria over a period of time. This is the threshold duration. If the data points are in violation for an entire threshold duration, we'll send you a notification based on your policy settings. All of these configurable delays give you more control over how you're alerted on sporadic and missing data. Streaming alerts process and descriptions Process Description Streaming data All data coming into New Relic. WHERE clause Filters all incoming streaming data. We only monitor for alerts on data that makes it through this filter. Aggregation methods One of three methods that control how data is collected before it's evaluated. They are: Event flow (Default) Event timer Cadence Aggregation window Data with timestamps that fall within this window will be aggregated and then evaluated. Delay/timer A time delay to ensure all data points have arrived in the aggregation window before aggregation occurs. Aggregated data Data in the aggregated window is collapsed to a single data point for alert evaluation. Evaluation The data point is evaluated by the NRQL condition, which is trigged by each incoming aggregated data point. Threshold duration A specific duration that determines if a violation is created. If your specified NRQL condition meets the threshold criteria over the threshold duration, a violation occurs. When a data point lacks data, a custom value is inserted to fill the gap. Choose your aggregation method You can choose between three different aggregation methods, depending on your needs. Event flow (default) works best for data that comes in frequently and mostly in order. Event timer works best for data that arrives infrequently in batches, such as cloud integration data or infrequent error logs. Cadence is our original and inferior aggregation method. It aggregates data on specific time intervals as detected by New Relic's internal wall clock, regardless of data timestamps. Event flow Event flow aggregates a window of data when the first data point arrives in a subsequent window. The custom delay defines which subsequent window data will start to populate to trigger aggregation of the current window. A custom delay provides extra time for data to arrive. These times are based on the data's timestamps and not New Relic's wall clock time. For example, suppose you're monitoring CPU usage in window durations of 1 minute and a 3 minute delay. When a CPU usage data point comes in with a timestamp between 12:00pm and 12:01pm, event flow will not aggregate that window until a data point shows up with a timestamp between 12:04pm and 12:05pm. When event flow receives the first data point with a timestamp of 12:04pm or later, it sends the 12:00 to 12:01 data to be aggregated. Caution If you expect your data points to arrive more than 30 minutes apart, please use the Event Timer method described below. Event timer Like event flow, event timer will only aggregate data for a given window when data arrives for that window. When a data point arrives for an aggregation window, a timer dedicated to that window starts to count down. If no further data arrives before the timer counts down, the data for that window is aggregated. If more data points arrive before the timer has completed counting down, the timer is reset. For example, suppose you're monitoring CloudWatch data that arrives fairly infrequently. You're using a window duration of 1 minute and a 3 minute timer. When a CloudWatch data point comes in with a timestamp between 12:00pm and 12:01pm, the timer will start to count down. If no further data points show up for that 12:00-12:01 window, the window will get aggregated 3 minutes later. If a new data point with a timestamp between 12:00 and 12:01 arrives, the timer resets. It keeps resetting every time more data points for that window arrive. The window will not be sent for aggregation until the timer reaches 0. If the timer for a later data point elapses before an earlier data point, the event timer method waits for the earlier timer to elapse before aggregating the later data point. For best results, make sure your timer is equal to or longer than your window duration time. If the timer is less than your window duration and your data flow is inconsistent, then your data may be evaluated before all of your data points arrive. This could cause you to be notified incorrectly. Cadence We recommend you use one of the other two methods. Cadence is our old streaming aggregation method. This method uses New Relic's wall clock time to determine when data is aggregated and evaluated. It doesn't take into account data point timestamps as they arrive. Streaming alerts tools Streaming alerts provide a set of tools to give you greater control over how your data is aggregated before it's evaluated to reduce incorrect notifications you receive. They are: Window duration Delay/timer Loss of signal detection Gap filling Tip This article covers these tools at a conceptual level. You'll find direct instructions on how to use these tools in Create NRQL alert conditions. Window duration In order to make loss of signal detection more effective and to reduce unnecessary notifications, you can customize aggregation windows to the duration that you need. An aggregation window is a specific block of time. We gather data points together in an aggregation window, before evaluating the data. A longer aggregation window can smooth out the data, since an outlier data point will have more data points to be aggregated with, giving it less of an influence on the aggregated data point that is sent for evaluation. When a data point arrives, its timestamp is used to put it in the proper aggregation window. You can set your aggregation window to anything between 30 seconds and 15 minutes. The default is 1 minute. Delay/timer The delay/timer setting controls how long the condition should wait before aggregating the data in the aggregation window. The event flow and cadence methods use delay. Event timer uses timer. The delay default is 2 minutes. The timer default is 1 minute and has a minimum value of 5 seconds. Loss of signal detection Loss of signal occurs when no data matches the NRQL condition over a specific period of time. A loss of signal is caused by different things. The WHERE clause in your NRQL query can filter out data before it's evaluated for violations. It could also mean a service or entity is offline or a periodic job has failed to run and no data is being sent to New Relic. In order to avoid unnecessary notifications, you can choose how long to wait before you're notified by a loss of signal violation. You can use loss of signal detection to open violations and be notified when a signal is lost. Alternately, you can use a loss of signal to close violations for ephemeral services or sporadic data, such as error counts. Gap filling Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with the last value received, a static value, or else do nothing and leave the gap there. The default is None. Gaps in streaming data can be caused by network or host issues, a signal may be sparse, or some signals, such as error counts, may only have data when something is wrong. By filling the gaps with known values, the alert evaluation process can process those gaps and determine how they should affect the loss of signal evaluation. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss and gap filling, see this Explorers Hub post.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 177.89917,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "sections": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "The streaming <em>alerts</em> platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into <em>New</em> <em>Relic</em>. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that&#x27;s processed"
      },
      "id": "6130c054e7b9d269edb6f21d"
    },
    {
      "sections": [
        "Manage entities in alerts conditions",
        "Requirements",
        "General procedure",
        "Important",
        "Example: Add/remove an entity"
      ],
      "title": "Manage entities in alerts conditions",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "REST API alerts"
      ],
      "external_id": "7a33e56e9410082971e69e27422d4646cebb7180",
      "image": "",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/rest-api-alerts/manage-entities-alerts-conditions/",
      "published_at": "2021-12-15T09:12:25Z",
      "updated_at": "2021-10-31T08:58:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In alerts, an entity is defined as any target for monitoring, such as an application, a browser, or a host. The alerts UI shows available entities that you can select. You can also use the REST API and API Explorer to add or remove entities for a condition. Requirements Modifying the list of entities in a condition requires you to know: Your API key The { entity_ID} of the entity you want to monitor The { condition_ID} of the condition you want to modify General procedure To update the entity list for a condition: Locate the appropriate entity ID; for example, Application ID and browser ID. Identify the policy ID by name or type. Get the list of conditions associated with the policy and choose the one you want to modify for the appropriate category: APM, browser, and mobile External services Synthetic monitoring Modify the condition using the add or remove API requests. Important Follow the requirements for the minimum and maximum number of entities you can add to conditions. Example: Add/remove an entity The following example shows how to add a Ruby application named TimberTime in a condition and how to remove an entity from that same condition. Only the first step in this example is unique to choosing the Ruby app as the entity. The remaining steps will be the same for whichever entity you choose. Get the entity_id; for example, {application_id}: curl -X GET 'https://api.newrelic.com/v2/applications.json' \\ -H 'Api-Key:$API_KEY' -i Copy OR If you know the application name, use this command and specify the app_name: curl -X GET 'https://api.newrelic.com/v2/applications.json' \\ -H 'Api-Key:$API_KEY' -i \\ -d 'filter[name]=TimberTime' Copy Review the output to find the {application_id}, and use it as the {entity_id}: { \"applications\": [ { \"id\": 12345, <---<<< {application_id} == {entity_id} \"name\": \"TimberTime\", \"language\": \"ruby\", \"health_status\": \"gray\", ... }, Copy Get the policy_id you want to update; for example, the TimberTime app's Logjam Alert policy. To get the policy_id, use this command: curl -X GET 'https://api.newrelic.com/v2/alerts_policies.json' \\ -H 'Api-Key:$API_KEY' -i \\ -G -d 'filter[name]= Logjam Alert' <---<<< {policy_name} Copy Review the policy output; for example: { \"policies\": [ { \"id\": 85, <---<<< {policy_id} \"incident_preference\": \"PER_POLICY\", \"name\": \"Logjam Alert\", \"created_at\": 1461176510393, \"updated_at\": 1461176510393 }, Copy List all of this policy's conditions and locate the {condition_id}: curl -X GET 'https://api.newrelic.com/v2/alerts_conditions.json' \\ -H 'Api-Key:$API_KEY' -i \\ -G -d 'policy_id=85' Copy Review the policy conditions; for example: { \"conditions\": [ { \"id\": 234567, <---<<< {condition_id} \"type\": \"apm_app_metric\", \"name\": \"Throughput (web) (High)\", \"enabled\": true, \"entities\": [ \"8288171\" <---<<< Entity currently included in the policy ], \"metric\": \"response_time_web\", \"terms\": [ { \"duration\": \"5\", \"operator\": \"above\", \"priority\": \"critical\", \"threshold\": \"500\", \"time_function\": \"all\" } ] } ] } Copy Use API requests to add entities to or remove entities from the policy's condition: To add {entity_id} 12345 to {condition_id} 234567, with {entity_type} set as application: curl -X PUT 'https://api.newrelic.com/v2/alerts_entity_conditions/12345.json' \\ -H 'Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -G -d 'entity_type=Application&condition_id=234567' Copy To remove {entity_id} 8288171 from {condition_id} 234567, with {entity_type} set as application: curl -X DELETE 'https://api.newrelic.com/v2/alerts_entity_conditions/8288171.json' \\ -H 'Api-Key:$API_KEY' -i \\ -G -d 'entity_type=Application&condition_id=234567' Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.74602,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Manage entities in <em>alerts</em> conditions",
        "sections": "Manage entities in <em>alerts</em> conditions",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "In <em>alerts</em>, an entity is defined as any target for monitoring, such as an application, a browser, or a host. The <em>alerts</em> UI shows available entities that you can select. You can also use the <em>REST</em> <em>API</em> and <em>API</em> Explorer to add or remove entities for a condition. Requirements Modifying the list"
      },
      "id": "6130b580196a67eab24948fb"
    },
    {
      "sections": [
        "Disable and enable alerts conditions using the API",
        "Requirements",
        "Enable and disable a condition",
        "Details on searching for condition ID",
        "Details on Update API requests",
        "Tip",
        "Example: Disable an APM condition"
      ],
      "title": "Disable and enable alerts conditions using the API",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "REST API alerts"
      ],
      "external_id": "4ffe50fd7e1a38a9dee007fe10cb63d28b955a8e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/rest-api-alerts/disable-enable-alerts-conditions-using-api/",
      "published_at": "2021-12-15T09:49:07Z",
      "updated_at": "2021-10-31T08:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In a policy, a condition identifies what triggers an alert. You can use the REST API to disable and enable conditions. You can also disable and enable conditions in New Relic One. Policies can't be enabled or disabled, whether via the API or the UI. Policies can only be created, deleted, or have their conditions changed. Requirements Modifying any attribute in a condition using the API requires: An API key and permissions to manage Alerts The condition's id (available from API Explorer: Alerts Conditions > GET > List) If your account hosts data in the EU data center, ensure you are using the proper API endpoints for EU region accounts. Enable and disable a condition The process for disabling or enabling a condition is the same general process for changing any attribute in a condition. A more detailed example comes after this general procedure: Find the ID for the policy that contains the condition you want to change. If the policy's ID is unknown, use the policy's name or type to make an API call and find the policy's ID. For more on this process, see List a single policy. With the policy ID, make an API call that returns the conditions associated with that policy. There are four different condition categories. If you don't know the category, this may require making up to four API calls in order to find the condition. Details on searching for condition ID If you don't know the category of the condition you want to change, you must search for it by making API calls using the four condition categories. Here are the different API call formats for the various condition categories. APM, browser, and mobile monitoring Conditions available: apm_app_metric, apm_kt_metric, browser_metric, and mobile_metric API Explorer link Get>List External services Conditions available: apm_external_service, mobile_external_service API Explorer link Get>List Synthetic monitoring API Explorer link Get>List For the returned JSON, find the JSON object of the condition you want to change. Copy and paste the condition's JSON in a text editor of your choice and edit the JSON. To enable the condition, set \"enabled\" to true. To disable the condition, set \"enabled\" to false. Update the condition by submitting your edited JSON via an Update API request. Our different products require different API requests. Details on Update API requests Use the Update API request that corresponds to the product in question: Conditions for APM, browser, and mobile Conditions available: apm_app_metric, apm_kt_metric, browser_metric, and mobile_metric API Explorer PUT>Update link Conditions for external services Conditions available: apm_external_service, mobile_external_service API Explorer PUT>Update Conditions for Synthetic monitoring) API Explorer PUT>Update Tip An Update API request can only change one condition at a time, it cannot update a vector of objects. For example, to change three conditions, you will have to make three separate requests. Example: Disable an APM condition The following example shows how to disable a condition for an apm_app_metric condition. With the exception of the types of API calls required, the process is similar to the process for changing other condition types. Obtain the policy_id of the policy you want to update. For an imaginary policy named Logjam Alert, the command would be: curl -X GET 'https://api.newrelic.com/v2/alerts_policies.json' \\ -H 'Api-Key:$API_KEY' -i \\ -G --data-urlencode 'filter[name]= Logjam Alert' <---<<< {policy_name} Copy The output for this request might look like: { \"policies\": [ { \"id\": 85, <---<<< $POLICY_ID \"incident_preference\": \"PER_POLICY\", \"name\": \"Logjam Alert\", \"created_at\": 1461176510393, \"updated_at\": 1461176510393 } ] } Copy List all of this policy's conditions and locate the {condition_id}: curl -X GET 'https://api.newrelic.com/v2/alerts_conditions.json' \\ -H 'X-Api-Key:$API_KEY' -i \\ -G -d 'policy_id=85' Copy The output for this request might look like: { \"conditions\": [ { \"id\": 12345, <---<<< $CONDITION_ID \"type\": \"apm_app_metric\", \"name\": \"Apdex (Low)\", \"enabled\": true, <---<<< Note the condition is enabled \"entities\": [ \"8288171\" ], \"metric\": \"apdex\", \"terms\": [ { \"duration\": \"5\", \"operator\": \"below\", \"priority\": \"critical\", \"threshold\": \"1\", \"time_function\": \"any\" } ] }, { \"id\": 2468, <---<<< another condition_id \"type\": \"apm_app_metric\", \"name\": \"Throughput (Low)\", ... } ] } Copy Copy the JSON for only the condition in question and paste it in a text editor. Change \"enabled\": true to \"enabled\": false. The edited JSON would look like: curl -X PUT 'https://api.newrelic.com/v2/alerts_conditions/12345.json' \\ -H 'X-Api-Key:$API_KEY' -i \\ -H 'Content-Type: application/json' \\ -d \\ '{ \"condition\": { \"type\": \"apm_app_metric\", \"name\": \"Apdex (Low)\", \"enabled\": false, <---<<< Changed to false \"entities\": [ \"8288171\" ], \"metric\": \"apdex\", \"terms\": [ { \"duration\": \"5\", \"operator\": \"below\", \"priority\": \"critical\", \"threshold\": \"1\", \"time_function\": \"any\" } ] } }' Copy Update the condition by submitting the edited condition JSON via an Update API request. For this specific condition, you would follow the steps in Update conditions for APM policies. Other product conditions would have other API requests, as detailed in Update API requests.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.74263,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Disable <em>and</em> enable <em>alerts</em> conditions using the <em>API</em>",
        "sections": "Disable <em>and</em> enable <em>alerts</em> conditions using the <em>API</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "In a policy, a condition identifies what triggers an <em>alert</em>. You can use the <em>REST</em> <em>API</em> to disable and enable conditions. You can also disable and enable conditions in <em>New</em> <em>Relic</em> One. Policies can&#x27;t be enabled or disabled, whether via the <em>API</em> or the UI. Policies can only be created, deleted, or have"
      },
      "id": "6130bf9d28ccbc076856a85b"
    }
  ],
  "/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/troubleshooting/missing-alert-notifications": [
    {
      "sections": [
        "Create NRQL alert conditions",
        "Tip",
        "Create a NRQL alert condition",
        "Create a condition from a chart",
        "Caution",
        "NRQL alert syntax",
        "Important",
        "Reformatting incompatible NRQL",
        "NRQL alert threshold examples",
        "Alert on specific segments of your data",
        "Alert on Nth percentile of your data",
        "Alert on max, min, avg of your data",
        "Alert on a percentage of your data",
        "Alert on Apdex with any T-value",
        "NRQL conditions and query order of operations",
        "Example: null value returned",
        "Example: zero value returned",
        "Nested aggregation NRQL alerts",
        "Nested queries with a non-faceted innermost query are not currently supported",
        "Queries at all levels must have the same aggregation window size",
        "Signal loss is not yet supported for nested queries",
        "Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported",
        "NRQL condition creation tips",
        "Condition edits can reset condition evaluation",
        "Alert condition types",
        "Sum of query results (limited or intermittent data)",
        "Set the loss of signal threshold",
        "Advanced signal settings",
        "Aggregation window duration",
        "Delay/timer",
        "Fill data gaps"
      ],
      "title": "Create NRQL alert conditions",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert conditions"
      ],
      "external_id": "2872f770a5fc8a802f99b9f848906f7e351ad761",
      "image": "https://docs.newrelic.com/static/eb8e1b2d826f9fd9cf46fe3dd5455217/c1b63/nr1_nrql_alert_conditions.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/",
      "published_at": "2021-12-19T16:01:17Z",
      "updated_at": "2021-12-14T22:49:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use NRQL queries to create alert conditions. Once you've defined your signal, you can further define your warning and critical threshold levels. This determines when an alerts violation is created. Read on to learn more about how to do this. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Click NRQL, and then Next, define thresholds. Tip For more information on key concepts relating to NRQL alert conditions and streaming alerts, see Streaming alerts: key terms and concepts. Create a NRQL alert condition To create a NRQL alert condition for a policy: On one.newrelic.com, in the header click Alerts & AI, then in the left sidebar click Policies. Select an existing policy or click New alert policy to create a new policy. Click Add a condition. Under Select a product click NRQL, and then click Next, define thresholds. Note that editing an existing condition can result in resetting its evaluation. Create a condition from a chart You can use a chart to create a NRQL alert condition. To create a NRQL alerts condition from a chart, click the chart menu , then click Create alert condition. Once you've named and customized your condition, you can add it to an existing policy or create a new one. Caution A small number of our older charts don't include the option to create an alert condition. NRQL alert syntax Here's the basic syntax for creating all NRQL alert conditions. The FACET clause is required for outlier condition types. It's optional for static and baseline. SELECT function(attribute) FROM Event WHERE attribute [comparison] [AND|OR ...] Copy Clause Notes SELECT function(attribute) Required Supported functions that return numbers include: apdex average count latest max min percentage percentile sum uniqueCount Tip If you use the percentile aggregator in a faceted alert condition with many facets, this may cause the following error to appear: An error occurred while fetching chart data. If you see this error, use average instead. FROM data type Required Only one data type can be targeted. Supported data types: Event Metric (RAW data points will be returned) WHERE attribute [comparison] [AND|OR ...] Use the WHERE clause to specify a series of one or more conditions. All the operators are supported. FACET attribute Required for outlier conditions Include an optional FACET clause in your NRQL syntax depending on the threshold type: static or baseline. Use the FACET clause to separate your results by attribute and alert on each attribute independently. No LIMIT clause is allowed, but all queries will receive the maximum number of facets possible. Faceted queries can return a maximum of 5000 values for static and baseline conditions and a maximum of 500 values for outlier conditions. Important If the query returns more than the maximum number of values, the alert condition can't be created. If you create the condition and the query returns more than this number later, the alert will fail. Modify your query so that it returns a fewer number of values. Reformatting incompatible NRQL Some elements of NRQL used in charts don’t make sense in the streaming context of alerts. Here’s a list of the most common incompatible elements and suggestions for reformatting a NRQL alert query to achieve the same effect. Element Notes SINCE and UNTIL Example: SELECT percentile(largestContentfulPaint, 75) FROM PageViewTiming WHERE (appId = 837807) SINCE yesterday Copy NRQL conditions produce a never-ending stream of windowed query results, so the SINCE and UNTIL keywords to scope the query to a point in time are not compatible. As a convenience, we automatically strip SINCE and UNTIL from a query when creating a condition from the context of a chart. TIMESERIES In NRQL queries, the TIMESERIES clause is used to return data as a time series broken out by a specified period of time. For NRQL conditions, the equivalent property of a signal is the aggregation duration window. histogram() The histogram() aggregation function is used to generate histograms. histogram() is not compatible with NRQL alerting: histogram aggregations can not be formatted as a time series. To create an alert from a portion of a histogram (for example, 95th percentile), use the percentile() aggregation function. Multiple aggregation functions Each condition can only target a single aggregated value. To alert on multiple values simultaneously, you’ll need to decompose them into individual conditions within the same policy. Original Query: SELECT count(foo), average(bar), max(baz) from Transaction Copy Decomposed: SELECT count(foo) from Transaction SELECT average(bar) from Transaction SELECT max(baz) from Transaction Copy COMPARE WITH The COMPARE WITH clause is used to compare the values for two different time ranges. This type of query is incompatible with NRQL alerting. We recommend using a Baseline Alert Condition to dynamically detect deviations for a particular signal. SLIDE BY The SLIDE BY clause supports a feature known as sliding windows. With sliding windows, SLIDE BY data is gathered into \"windows\" of time that overlap with each other. These windows can help to smooth out line graphs with a lot of variation in cases where the rolling aggregate (such as a rolling mean) is more important than aggregates from narrow windows of time. Sliding windows are not currently supported in NRQL alerts. LIMIT In NRQL queries, the LIMIT clause is used to control the amount of data a query returns, either the maximum number of facet values returned by FACET queries or the maximum number of items returned by SELECT * queries. LIMIT is not compatible with NRQL alerting: evaluation is always performed on the full result set. NRQL alert threshold examples Here are some common use cases for NRQL conditions. These queries will work for static and baseline condition types. The outlier condition type will require additional FACET clauses. Alert on specific segments of your data Create constrained alerts that target a specific segment of your data, such as a few key customers or a range of data. Use the WHERE clause to define those conditions. SELECT average(duration) FROM Transaction WHERE account_id in (91290, 102021, 20230) Copy SELECT percentile(duration, 95) FROM Transaction WHERE name LIKE 'Controller/checkout/%' Copy Alert on Nth percentile of your data Create alerts when an Nth percentile of your data hits a specified threshold; for example, maintaining SLA service levels. Since we evaluate the NRQL query based on the aggregation window duration, percentiles will be calculated for each duration separately. SELECT percentile(duration, 95) FROM Transaction Copy SELECT percentile(databaseDuration, 75) FROM Transaction Copy Alert on max, min, avg of your data Create alerts when your data hits a certain maximum, minimum, or average; for example, ensuring that a duration or response time does not pass a certain threshold. SELECT max(duration) FROM Transaction Copy SELECT average(duration) FROM Transaction Copy Alert on a percentage of your data Create alerts when a proportion of your data goes above or below a certain threshold. SELECT percentage(count(*), WHERE duration > 2) FROM Transaction Copy SELECT percentage(count(*), WHERE httpResponseCode = '500') FROM Transaction Copy Alert on Apdex with any T-value Create alerts on Apdex, applying your own T-value for certain transactions. For example, get an alert notification when your Apdex for a T-value of 500ms on transactions for production apps goes below 0.8. SELECT apdex(duration, t:0.5) FROM Transaction WHERE appName like '%prod%' Copy NRQL conditions and query order of operations By default, the aggregation window duration is 1 minute, but you can change the window to suit your needs. Whatever the aggregation window, New Relic will collect data for that window using the function in the NRQL condition’s query. The query is parsed and executed by our systems in the following order: FROM clause – which event type needs to be grabbed? WHERE clause – what can be filtered out? SELECT clause – what information needs to be returned from the now-filtered data set? Example: null value returned Let's say this is your alert condition query: SELECT count(*) FROM SyntheticCheck WHERE monitorName = 'My Cool Monitor' AND result = 'FAILURE' Copy If there are no failures for the aggregation window: The system will execute the FROM clause by grabbing all SyntheticCheck events on your account. Then it will execute the WHERE clause to filter through those events by looking only for the ones that match the monitor name and result specified. If there are still events left to scan through after completing the FROM and WHERE operations, the SELECT clause will be executed. If there are no remaining events, the SELECT clause will not be executed. This means that aggregators like count() and uniqueCount() will never return a zero value. When there is a count of 0, the SELECT clause is ignored and no data is returned, resulting in a value of NULL. Example: zero value returned If you have a data source delivering legitimate numeric zeroes, the query will return zero values and not null values. Let's say this is your alert condition query, and that MyCoolEvent is an attribute that can sometimes return a zero value. SELECT average(MyCoolAttribute) FROM MyCoolEvent Copy If, in the aggregation window being evaluated, there's at least one instance of MyCoolEvent and if the average value of all MyCoolAttribute attributes from that window is equal to zero, then a 0 value will be returned. If there are no MyCoolEvent events during that minute, then a NULL will be returned due to the order of operations. Tip For more information about this topic, you can check out our blog post on troubleshooting for zero versus null values. Tip You can determine how null values will be handled by adjusting loss of signal and gap filling settings in the Alert Conditions UI. Tip You can avoid NULL values entirely with a query order of operations shortcut. Do this by using a filter sub-clause, then including all filter elements within that sub-clause. The main body of the query will run and return data, at which point the SELECT clause will then run and apply the filter elements. The query will return a value of 0 if the filter elements result in no matching data. Here's an example: SELECT filter(count(*), WHERE result = 'SUCCESS' AND monitorName = 'My Favorite Monitor') FROM SyntheticCheck Copy Nested aggregation NRQL alerts Nested aggregation queries are a powerful way to query your data. However, they have a few restrictions that are important to note. Nested queries with a non-faceted innermost query are not currently supported Without a FACET, the inner query produces a single result, giving the outer query nothing to aggregate. If you're using a nested query, make sure your inner query is faceted. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu) ​​​​​ Copy Queries at all levels must have the same aggregation window size With an alert aggregation window of 1 minute, the inner query would produce two smaller windows of 30 seconds. In theory, these two windows could be aggregated by the outer query. However, this is not currently supported. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu TIMESERIES 30 seconds)​​ Copy Signal loss is not yet supported for nested queries For more information on signal loss, see NerdGraph API: Loss of signal and gap filling. Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported You can't use a nested query containing the WITH METRIC_FORMAT in the inner query to create NRQL alert conditions. NRQL condition creation tips Here are some tips for creating and using a NRQL condition: Topic Tips Condition types NRQL condition types include static, baseline, and outlier. Create a description For NRQL conditions, you can create a custom description to add to each violation. Descriptions can be enhanced with variable substitution based on metadata in the specific violation. For details, see Description Query results Queries must return a number. The condition evaluates the returned number against the thresholds you've set. Time period NRQL conditions evaluate data based on how it's aggregated, using aggregation windows from 30 seconds to 15 minutes, in increments of 15 seconds. For best results, we recommend using the event flow or event timer aggregation methods. For the cadence aggregation method, the implicit SINCE ... UNTIL clause specifying which minute to evaluate is controlled by your delay/timer setting. Since very recent data may be incomplete, you may want to query data from 3 minutes ago or longer, especially for: Applications that run on multiple hosts. SyntheticCheck data: Timeouts can take 3 minutes, so 5 minutes or more is recommended. Also, if a query will generate intermittent data, consider using the sum of query results option. Lost signal threshold (loss of signal detection) You can use loss of signal detection to alert on when your data (a telemetry signal) should be considered lost. A signal loss can indicate that a service or entity is no longer online or that a periodic job failed to run. You can also use this to make sure that violations for sporadic data, such as error counts, are closed when no signal is coming in. Advanced signal settings These settings give you options for better handling continuous, streaming data signals that may sometimes be missing. These settings include the aggregation window duration, the delay/timer, and an option for filling data gaps. For more on using these, see Advanced signal settings. Condition settings Use the Condition settings to: Create a concise, descriptive condition name. Provide a custom violation description for the condition that will be included in violations and notifications. Add the runbook URL to include your organization's procedures for handling incidents. You may also add this information to the custom violation description. Limits on conditions See the maximum values. Health status In order for a NRQL alert condition health status display to function properly, use a FACET clause to scope each signal to a single entity (for example, FACET hostname or FACET appname). Examples For more information, see: Expected NRQL syntax Examples of NRQL condition queries Condition edits can reset condition evaluation When you edit NRQL alert conditions in some specific ways (detailed below), their evaluations are reset, meaning that any evaluation up until that point is lost, and the evaluation starts over from that point. The two ways this will affect you are: For \"for at least x minutes\" thresholds: because the evaluation window has been reset, there will be a delay of at least x minutes before any violations can be reported. For baseline conditions: the condition starts over again and all baseline learning is lost. The following actions cause an evaluation reset for NRQL conditions: Changing the query Changing the aggregation window, aggregation method, or aggregation delay/timer setting Changing the \"close violations on signal loss\" setting Changing any gap fill settings Changing the baseline direction (if applicable) – higher, lower, or higher/lower Change the threshold value, threshold window, or threshold operator The following actions (along with any other actions not covered in the above list) will not reset the evaluation: Changing the loss of signal time window (expiration duration) Changing the time function (switching \"for at least\" to \"at least once in,\" or vice-versa) Toggling the \"open violation on signal loss\" setting Alert condition types When you create a NRQL alert, you can choose from different types of conditions: NRQL alert condition types Description Static This is the simplest type of NRQL condition. It allows you to create a condition based on a NRQL query that returns a numeric value. Optional: Include a FACET clause. Baseline (Dynamic) Uses a self-adjusting condition based on the past behavior of the monitored values. Uses the same NRQL query form as the static type, including the optional FACET clause. Outlier Looks for group behavior and values that are outliers from those groups. Uses the same NRQL query form as the static type, but requires a FACET clause. Sum of query results (limited or intermittent data) Important Available only for static (basic) condition types. If a query returns intermittent or limited data, it may be difficult to set a meaningful threshold. Missing or limited data will sometimes generate false positives or false negatives. You can use loss of signal, aggregation duration, and gap filling settings to minimize these false notifications. To avoid this problem when using the static threshold type, you can set the selector to sum of query results. This lets you set the alert on an aggregated sum instead of a value from a single harvest cycle. Up to two hours of one-minute data checks can be aggregated. The duration you select determines the width of the rolling sum and the preview chart will update accordingly. Set the loss of signal threshold Loss of signal occurs when no data matches the NRQL condition over a specific period of time. You can set your loss of signal threshold duration and also what happens when the threshold is crossed. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Loss of signal is only available for NRQL conditions. You may also manage these settings using the GraphQL API (recommended), or the REST API. Go here for specific GraphQL API examples. Loss of signal settings: Loss of signal settings include a time duration and two possible actions. Signal loss expiration time UI label: Signal is lost after: GraphQL Node: expiration.expirationDuration Expiration duration is a timer that starts and resets when we receive a data point in the streaming alerts pipeline. If we don't receive another data point before your 'expiration time' expires, we consider that signal to be lost. This can be because no data is being sent to New Relic or the WHERE clause of your NRQL query is filtering that data out before it is streamed to the alerts pipeline. Note that when you have a faceted query, each facet is a signal. So if any one of those signals ends during the duration specified, that will be considered a loss of signal. The loss of signal expiration time is independent of the threshold duration and triggers as soon as the timer expires. The maximum expiration duration is 48 hours. This is helpful when monitoring for the execution of infrequent jobs. The minimum is 30 seconds, but we recommend using at least 3-5 minutes. Loss of signal actions Once a signal is considered lost, you can close open violations, open new violations, or both. Close all current open violations: This closes all open violations that are related to a specific signal. It won't necessarily close all violations for a condition. If you're alerting on an ephemeral service, or on a sporadic signal, you'll want to choose this action to ensure that violations are closed properly. The GraphQL node name for this is \"closeViolationsOnExpiration\" Open new violations: This will open a new violation when the signal is considered lost. These violations will indicate that they are due to a loss of signal. Based on your incident preferences, this should trigger a notification. The graphQL node name for this is \"openViolationOnExpiration\" When you enable both actions, we'll close all open violations first, and then open a new violation for loss of signal. To create a NRQL alert configured with loss of signal detection in the UI: For a policy, when you create a condition, under Select a product, click NRQL, then click Next, define thresholds. Write a NRQL query that returns the values you want to alert on. For Threshold type, select Static or Baseline. Click + Add lost signal threshold, then set the signal expiration duration time in minutes or seconds in the Signal is lost after field. Choose what you want to happen when the signal is lost. You can check one or both of Close all current open violations and Open new \"lost signal\" violation. These control how loss of signal violations will be handled for the condition. Make sure you name your condition before you save it. Violations open due to loss of signal close when the signal comes back. Newly opened lost signal violations will close immediately when new data is evaluated. the condition they belong to expires. By default, conditions expire after 3 days. you manually close the violation with the Close all current open violations option. Tip Loss of signal detection doesn't work on NRQL queries that use nested aggregation or sub-queries. Advanced signal settings When creating a NRQL alert condition, use the advanced signal settings to control streaming alert data and avoid false alarms. When creating a NRQL condition, there are several advanced signal settings: Aggregation window duration Delay/timer Fill data gaps To read an explanation of what these settings are and how they relate to each other, see Streaming alerts concepts. Below are instructions and tips on how to configure them. Aggregation window duration You can set the aggregation window duration to choose how long data is accumulated in a streaming time window before it's aggregated. You can set it to anything between 30 seconds and 15 minutes. The default is one minute. Tip Baseline alert condition thresholds don't support editing the aggregation window. They use the 1 minute default. Delay/timer You can adjust the delay/timer to coordinate our streaming alerting algorithm with your data's behavior. If your data is sparse or inconsistent, you may want to use the event timer aggregation method. For the cadence method, the total supported latency is the sum of the aggregation window duration and the delay. If the data type comes from an APM language agent and is aggregated from many app instances (for example, Transactions, TransactionErrors, etc.), we recommend using the event flow method with the default settings. Important When creating NRQL conditions for data collected from Infrastructure Cloud Integrations such as AWS Cloudwatch or Azure, we recommend that you use the event timer method. Fill data gaps Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with one of these settings: None: (Default) Choose this if you don't want to take any action on empty aggregation windows. On evaluation, an empty aggregation window will reset the threshold duration timer. For example, if a condition says that all aggregation windows must have data points above the threshold for 5 minutes, and 1 of the 5 aggregation windows is empty, then the condition won't be in violation. Custom static value: Choose this if you'd like to insert a custom static value into the empty aggregation windows before they're evaluated. This option has an additional, required parameter of fillValue (as named in the API) that specifies what static value should be used. This defaults to 0. Last known value: This option inserts the last seen value before evaluation occurs. We maintain the state of the last seen value for 2 hours. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss, gap filling, and how to request access to these features, see this Explorers Hub post. Options for editing data gap settings: In the NRQL conditions UI, go to Condition settings > Advanced signal settings > fill data gaps with and choose an option. If using our Nerdgraph API (preferred), this node is located at: actor : account : alerts : nrqlCondition : signal : fillOption | fillValue NerdGraph is our recommended API for this but if you're using our REST API, you can find this setting in the REST API explorer under the \"signal\" section of the Alert NRQL conditions API.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.55426,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create NRQL <em>alert</em> conditions",
        "sections": "Nested aggregation NRQL <em>alerts</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": " a NRQL <em>alert</em> condition for a policy: On one.newrelic.com, in the header click <em>Alerts</em> &amp; AI, then in the left sidebar click Policies. Select an existing policy or click <em>New</em> <em>alert</em> policy to create a <em>new</em> policy. Click Add a condition. Under Select a product click NRQL, and then click Next, define"
      },
      "id": "603ef04864441fbc114e8883"
    },
    {
      "sections": [
        "Streaming alerts: key terms and concepts",
        "Why it matters",
        "Streaming alerts process and descriptions",
        "Choose your aggregation method",
        "Event flow",
        "Caution",
        "Event timer",
        "Cadence",
        "Streaming alerts tools",
        "Tip",
        "Window duration",
        "Delay/timer",
        "Loss of signal detection",
        "Gap filling"
      ],
      "title": "Streaming alerts: key terms and concepts",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Get started"
      ],
      "external_id": "144c7f0721ec2acb1e8ac8aa06142911ea552aef",
      "image": "https://docs.newrelic.com/static/39158bd84483adf9516bb7ff5058c9bd/d30ee/streaming-alerts.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/streaming-alerts-key-terms-concepts/",
      "published_at": "2021-12-15T12:47:58Z",
      "updated_at": "2021-12-15T12:47:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The streaming alerts platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into New Relic. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that's processed by the streaming algorithm. There are three methods for aggregating the data filtered through your NRQL condition: Event flow (default) Event timer Cadence This short video describes the three aggregation method (5:31). Why it matters Understanding how streaming alerts works will help you fine-tune your NRQL conditions to be notified about what's important to you. Only data that matches the conditions of the NRQL WHERE clause is alerted on. For more details on each step of the process, see Streaming alerts process and descriptions. As data streams into New Relic, it's filtered by the NRQL condition. Before data is evaluated, it must meet the criteria defined by the NRQL query's WHERE clause. Instead of evaluating that data immediately for violations, the NRQL alert conditions collect the data over a period of time known as the aggregation window. An additional delay/timer allows for slower data points to arrive before the window is aggregated. Once the delay/timer time has elapsed, New Relic aggregates the data into a single data point. Alerts then evaluates the data point using the NRQL condition in order to determine whether it meets the violation threshold criteria. Even if a data point meets the criteria for a violation, a violation may not be triggered. A violation is only triggered when data points consistently meet the threshold criteria over a period of time. This is the threshold duration. If the data points are in violation for an entire threshold duration, we'll send you a notification based on your policy settings. All of these configurable delays give you more control over how you're alerted on sporadic and missing data. Streaming alerts process and descriptions Process Description Streaming data All data coming into New Relic. WHERE clause Filters all incoming streaming data. We only monitor for alerts on data that makes it through this filter. Aggregation methods One of three methods that control how data is collected before it's evaluated. They are: Event flow (Default) Event timer Cadence Aggregation window Data with timestamps that fall within this window will be aggregated and then evaluated. Delay/timer A time delay to ensure all data points have arrived in the aggregation window before aggregation occurs. Aggregated data Data in the aggregated window is collapsed to a single data point for alert evaluation. Evaluation The data point is evaluated by the NRQL condition, which is trigged by each incoming aggregated data point. Threshold duration A specific duration that determines if a violation is created. If your specified NRQL condition meets the threshold criteria over the threshold duration, a violation occurs. When a data point lacks data, a custom value is inserted to fill the gap. Choose your aggregation method You can choose between three different aggregation methods, depending on your needs. Event flow (default) works best for data that comes in frequently and mostly in order. Event timer works best for data that arrives infrequently in batches, such as cloud integration data or infrequent error logs. Cadence is our original and inferior aggregation method. It aggregates data on specific time intervals as detected by New Relic's internal wall clock, regardless of data timestamps. Event flow Event flow aggregates a window of data when the first data point arrives in a subsequent window. The custom delay defines which subsequent window data will start to populate to trigger aggregation of the current window. A custom delay provides extra time for data to arrive. These times are based on the data's timestamps and not New Relic's wall clock time. For example, suppose you're monitoring CPU usage in window durations of 1 minute and a 3 minute delay. When a CPU usage data point comes in with a timestamp between 12:00pm and 12:01pm, event flow will not aggregate that window until a data point shows up with a timestamp between 12:04pm and 12:05pm. When event flow receives the first data point with a timestamp of 12:04pm or later, it sends the 12:00 to 12:01 data to be aggregated. Caution If you expect your data points to arrive more than 30 minutes apart, please use the Event Timer method described below. Event timer Like event flow, event timer will only aggregate data for a given window when data arrives for that window. When a data point arrives for an aggregation window, a timer dedicated to that window starts to count down. If no further data arrives before the timer counts down, the data for that window is aggregated. If more data points arrive before the timer has completed counting down, the timer is reset. For example, suppose you're monitoring CloudWatch data that arrives fairly infrequently. You're using a window duration of 1 minute and a 3 minute timer. When a CloudWatch data point comes in with a timestamp between 12:00pm and 12:01pm, the timer will start to count down. If no further data points show up for that 12:00-12:01 window, the window will get aggregated 3 minutes later. If a new data point with a timestamp between 12:00 and 12:01 arrives, the timer resets. It keeps resetting every time more data points for that window arrive. The window will not be sent for aggregation until the timer reaches 0. If the timer for a later data point elapses before an earlier data point, the event timer method waits for the earlier timer to elapse before aggregating the later data point. For best results, make sure your timer is equal to or longer than your window duration time. If the timer is less than your window duration and your data flow is inconsistent, then your data may be evaluated before all of your data points arrive. This could cause you to be notified incorrectly. Cadence We recommend you use one of the other two methods. Cadence is our old streaming aggregation method. This method uses New Relic's wall clock time to determine when data is aggregated and evaluated. It doesn't take into account data point timestamps as they arrive. Streaming alerts tools Streaming alerts provide a set of tools to give you greater control over how your data is aggregated before it's evaluated to reduce incorrect notifications you receive. They are: Window duration Delay/timer Loss of signal detection Gap filling Tip This article covers these tools at a conceptual level. You'll find direct instructions on how to use these tools in Create NRQL alert conditions. Window duration In order to make loss of signal detection more effective and to reduce unnecessary notifications, you can customize aggregation windows to the duration that you need. An aggregation window is a specific block of time. We gather data points together in an aggregation window, before evaluating the data. A longer aggregation window can smooth out the data, since an outlier data point will have more data points to be aggregated with, giving it less of an influence on the aggregated data point that is sent for evaluation. When a data point arrives, its timestamp is used to put it in the proper aggregation window. You can set your aggregation window to anything between 30 seconds and 15 minutes. The default is 1 minute. Delay/timer The delay/timer setting controls how long the condition should wait before aggregating the data in the aggregation window. The event flow and cadence methods use delay. Event timer uses timer. The delay default is 2 minutes. The timer default is 1 minute and has a minimum value of 5 seconds. Loss of signal detection Loss of signal occurs when no data matches the NRQL condition over a specific period of time. A loss of signal is caused by different things. The WHERE clause in your NRQL query can filter out data before it's evaluated for violations. It could also mean a service or entity is offline or a periodic job has failed to run and no data is being sent to New Relic. In order to avoid unnecessary notifications, you can choose how long to wait before you're notified by a loss of signal violation. You can use loss of signal detection to open violations and be notified when a signal is lost. Alternately, you can use a loss of signal to close violations for ephemeral services or sporadic data, such as error counts. Gap filling Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with the last value received, a static value, or else do nothing and leave the gap there. The default is None. Gaps in streaming data can be caused by network or host issues, a signal may be sparse, or some signals, such as error counts, may only have data when something is wrong. By filling the gaps with known values, the alert evaluation process can process those gaps and determine how they should affect the loss of signal evaluation. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss and gap filling, see this Explorers Hub post.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.07281,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "sections": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "The streaming <em>alerts</em> platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into <em>New</em> <em>Relic</em>. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that&#x27;s processed"
      },
      "id": "6130c054e7b9d269edb6f21d"
    },
    {
      "sections": [
        "Choose your aggregation method",
        "What's aggregation?",
        "Why it matters",
        "When to use event flow",
        "How event flow works",
        "Caution",
        "Event flow use cases",
        "When to use event timer",
        "How event timer works",
        "Event timer use cases",
        "Cadence",
        "Aggregation and loss of signal"
      ],
      "title": "Choose your aggregation method",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts"
      ],
      "external_id": "9bdcf28f192d61c2348909803e1c75876168936d",
      "image": "https://docs.newrelic.com/static/64df85a9f5694079b9ad1557de7fe5b5/c1b63/signal-consistency.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/get-started/choose-your-aggregation-method/",
      "published_at": "2021-12-19T15:02:53Z",
      "updated_at": "2021-12-04T15:31:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Alerts conditions provide a sophisticated set of tools for describing when you want to be notified about something that's happened or failed to happen on something you're monitoring. For best results, choose the aggregation method that best matches the way your data arrives. The three aggregation methods are event flow, event timer, and cadence. If you're interested in a conceptual overview, see our doc on streaming alerts, key terms and concepts. What's aggregation? When an application or service is monitored by New Relic, data can arrive in different ways. Some data arrives consistently and predictably, while other data arrives inconsistently and sporadically. Aggregation is how our alerting system gathers data together before analyzing it for exceeding warning or critical threshold levels. Your data is collected as data points in an aggregation window and then turned into a single numeric value. The data points are aggregated based on your NRQL query using methods like sum, average, min, and max, among others. This single numeric value is what's used to evaluate the condition's threshold. Once data has been aggregated, no more data points can be added to it. Our different aggregation methods will help you strike a balance between aggregating your data quickly and waiting for enough data points to arrive. Why it matters With the correct aggregation method, you're more likely to get notifications you care about, while preventing ones you don't. The most important questions to consider when deciding on your aggregation method: How often does my data arrive? How consistently does my data arrive? When data arrives frequently and consistently in a linear way, we recommend using event flow. When data arrives sporadically, inconsistently, and out of order, we recommend using event timer aggregation. When to use event flow With event flow, data is aggregated based on data point timestamps, so it's important that data points arrive in a consistent and linear manner. This aggregation method doesn't work as well for data point timestamps that arrive out of order or with a wide span of time within a short period. Event flow is the default aggregation method, because it applies for the most common use cases. How event flow works Event flow uses data point timestamps to determine when to open and close an aggregation window. For example, if you're using event flow with a 2 minute delay window, then an aggregation window will close when a timestamp arrives that's two minutes later than the last timestamp that was received. A data point with a 12:00pm timestamp arrives. An aggregation window opens. At some point, a 12:03pm data point arrives. Event flow closes the window, excluding the 12:03 data point, and evaluates that closed window against your thresholds. The event flow aggregation window will continue collecting data points until that later timestamp. The later timestamps are what moves the system forward, not the data points themselves. Event flow will wait as long as necessary for the next data point later than your delay setting to arrive, before aggregating the data. Event flow works best for data that arrives frequently and consistently. Caution If you expect your data points to arrive more than 30 minutes apart, please use the event timer method described below. Event flow use cases Here are some typical event flow use cases: APM agent data. Infrastructure agent data. Any data coming from a 3rd party that comes in frequently and reliably. Most AWS Cloudwatch metrics coming from the AWS Metric Stream (NOT polling). The main exception is that some AWS Cloudwatch data is very infrequent (like S3 volume data) regardless of whether it's streaming or polling and, in that case, you'd use Event timer. When to use event timer Event timer aggregation is based on a timer that counts down when a data point arrives. The timer resets every time a new data point arrives. If the timer counts down before a new data point arrives, event timer aggregates all of the data points received during that time. Event timer is best for alerting on events that happen sporadically and with large gaps of time. How event timer works Errors are a type of event that happens sporadically, unpredictably, and often with large gaps of time. For example, you might have a condition with a query that returns a count of errors. Many minutes may go by without any errors at all, and then suddenly 5 errors arrive within a minute. In this example, event timer would do nothing until the first of the 5 errors arrive. Then it would start the timer, resetting it each time a new error arrives. If the timer countdown reaches 0 without a new error, event timer aggregates the data, and evaluates it against your threshold. Event timer use cases Here are some typical event timer use cases: New Relic usage data. Cloud integration data that is being polled (such as with GCP, Azure, or AWS polling methods). Queries that deliver sparse or infrequent data, such as error counts. Cadence Cadence is our original aggregation method. It aggregates data on specific time intervals as detected by New Relic's internal wall clock, regardless of data timestamps. We recommend that you use one of our other aggregation methods instead. If you're currently using cadence, choose one of the other aggregation methods. Event flow is best for consistent, predictable data points. Event timer is best for inconsistent, sporadic data points. Aggregation and loss of signal Our loss of signal system runs separately from these aggregation methods and settings. If you set your alert condition to open a new violation when your signal is lost for 10 minutes, a loss of signal service watches for data points to arrive. If a new data point fails to arrive within 10 minutes, loss of signal causes a violation to open. For more information on when to use gap filling and loss single, see our forum post on when to use gap filling and loss of signal.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 162.8996,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Aggregation <em>and</em> loss of signal",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": " methods are event flow, event timer, and cadence. If you&#x27;re interested in a conceptual overview, see our doc on streaming <em>alerts</em>, key terms and concepts. What&#x27;s aggregation? When an application or service is monitored by <em>New</em> <em>Relic</em>, data can arrive in different ways. Some data arrives consistently"
      },
      "id": "61853bba28ccbc5a1b7fe94c"
    }
  ],
  "/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/troubleshooting/tag-information-not-showing-entity-infra-alert-condition": [
    {
      "sections": [
        "Create NRQL alert conditions",
        "Tip",
        "Create a NRQL alert condition",
        "Create a condition from a chart",
        "Caution",
        "NRQL alert syntax",
        "Important",
        "Reformatting incompatible NRQL",
        "NRQL alert threshold examples",
        "Alert on specific segments of your data",
        "Alert on Nth percentile of your data",
        "Alert on max, min, avg of your data",
        "Alert on a percentage of your data",
        "Alert on Apdex with any T-value",
        "NRQL conditions and query order of operations",
        "Example: null value returned",
        "Example: zero value returned",
        "Nested aggregation NRQL alerts",
        "Nested queries with a non-faceted innermost query are not currently supported",
        "Queries at all levels must have the same aggregation window size",
        "Signal loss is not yet supported for nested queries",
        "Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported",
        "NRQL condition creation tips",
        "Condition edits can reset condition evaluation",
        "Alert condition types",
        "Sum of query results (limited or intermittent data)",
        "Set the loss of signal threshold",
        "Advanced signal settings",
        "Aggregation window duration",
        "Delay/timer",
        "Fill data gaps"
      ],
      "title": "Create NRQL alert conditions",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert conditions"
      ],
      "external_id": "2872f770a5fc8a802f99b9f848906f7e351ad761",
      "image": "https://docs.newrelic.com/static/eb8e1b2d826f9fd9cf46fe3dd5455217/c1b63/nr1_nrql_alert_conditions.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/",
      "published_at": "2021-12-19T16:01:17Z",
      "updated_at": "2021-12-14T22:49:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use NRQL queries to create alert conditions. Once you've defined your signal, you can further define your warning and critical threshold levels. This determines when an alerts violation is created. Read on to learn more about how to do this. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Click NRQL, and then Next, define thresholds. Tip For more information on key concepts relating to NRQL alert conditions and streaming alerts, see Streaming alerts: key terms and concepts. Create a NRQL alert condition To create a NRQL alert condition for a policy: On one.newrelic.com, in the header click Alerts & AI, then in the left sidebar click Policies. Select an existing policy or click New alert policy to create a new policy. Click Add a condition. Under Select a product click NRQL, and then click Next, define thresholds. Note that editing an existing condition can result in resetting its evaluation. Create a condition from a chart You can use a chart to create a NRQL alert condition. To create a NRQL alerts condition from a chart, click the chart menu , then click Create alert condition. Once you've named and customized your condition, you can add it to an existing policy or create a new one. Caution A small number of our older charts don't include the option to create an alert condition. NRQL alert syntax Here's the basic syntax for creating all NRQL alert conditions. The FACET clause is required for outlier condition types. It's optional for static and baseline. SELECT function(attribute) FROM Event WHERE attribute [comparison] [AND|OR ...] Copy Clause Notes SELECT function(attribute) Required Supported functions that return numbers include: apdex average count latest max min percentage percentile sum uniqueCount Tip If you use the percentile aggregator in a faceted alert condition with many facets, this may cause the following error to appear: An error occurred while fetching chart data. If you see this error, use average instead. FROM data type Required Only one data type can be targeted. Supported data types: Event Metric (RAW data points will be returned) WHERE attribute [comparison] [AND|OR ...] Use the WHERE clause to specify a series of one or more conditions. All the operators are supported. FACET attribute Required for outlier conditions Include an optional FACET clause in your NRQL syntax depending on the threshold type: static or baseline. Use the FACET clause to separate your results by attribute and alert on each attribute independently. No LIMIT clause is allowed, but all queries will receive the maximum number of facets possible. Faceted queries can return a maximum of 5000 values for static and baseline conditions and a maximum of 500 values for outlier conditions. Important If the query returns more than the maximum number of values, the alert condition can't be created. If you create the condition and the query returns more than this number later, the alert will fail. Modify your query so that it returns a fewer number of values. Reformatting incompatible NRQL Some elements of NRQL used in charts don’t make sense in the streaming context of alerts. Here’s a list of the most common incompatible elements and suggestions for reformatting a NRQL alert query to achieve the same effect. Element Notes SINCE and UNTIL Example: SELECT percentile(largestContentfulPaint, 75) FROM PageViewTiming WHERE (appId = 837807) SINCE yesterday Copy NRQL conditions produce a never-ending stream of windowed query results, so the SINCE and UNTIL keywords to scope the query to a point in time are not compatible. As a convenience, we automatically strip SINCE and UNTIL from a query when creating a condition from the context of a chart. TIMESERIES In NRQL queries, the TIMESERIES clause is used to return data as a time series broken out by a specified period of time. For NRQL conditions, the equivalent property of a signal is the aggregation duration window. histogram() The histogram() aggregation function is used to generate histograms. histogram() is not compatible with NRQL alerting: histogram aggregations can not be formatted as a time series. To create an alert from a portion of a histogram (for example, 95th percentile), use the percentile() aggregation function. Multiple aggregation functions Each condition can only target a single aggregated value. To alert on multiple values simultaneously, you’ll need to decompose them into individual conditions within the same policy. Original Query: SELECT count(foo), average(bar), max(baz) from Transaction Copy Decomposed: SELECT count(foo) from Transaction SELECT average(bar) from Transaction SELECT max(baz) from Transaction Copy COMPARE WITH The COMPARE WITH clause is used to compare the values for two different time ranges. This type of query is incompatible with NRQL alerting. We recommend using a Baseline Alert Condition to dynamically detect deviations for a particular signal. SLIDE BY The SLIDE BY clause supports a feature known as sliding windows. With sliding windows, SLIDE BY data is gathered into \"windows\" of time that overlap with each other. These windows can help to smooth out line graphs with a lot of variation in cases where the rolling aggregate (such as a rolling mean) is more important than aggregates from narrow windows of time. Sliding windows are not currently supported in NRQL alerts. LIMIT In NRQL queries, the LIMIT clause is used to control the amount of data a query returns, either the maximum number of facet values returned by FACET queries or the maximum number of items returned by SELECT * queries. LIMIT is not compatible with NRQL alerting: evaluation is always performed on the full result set. NRQL alert threshold examples Here are some common use cases for NRQL conditions. These queries will work for static and baseline condition types. The outlier condition type will require additional FACET clauses. Alert on specific segments of your data Create constrained alerts that target a specific segment of your data, such as a few key customers or a range of data. Use the WHERE clause to define those conditions. SELECT average(duration) FROM Transaction WHERE account_id in (91290, 102021, 20230) Copy SELECT percentile(duration, 95) FROM Transaction WHERE name LIKE 'Controller/checkout/%' Copy Alert on Nth percentile of your data Create alerts when an Nth percentile of your data hits a specified threshold; for example, maintaining SLA service levels. Since we evaluate the NRQL query based on the aggregation window duration, percentiles will be calculated for each duration separately. SELECT percentile(duration, 95) FROM Transaction Copy SELECT percentile(databaseDuration, 75) FROM Transaction Copy Alert on max, min, avg of your data Create alerts when your data hits a certain maximum, minimum, or average; for example, ensuring that a duration or response time does not pass a certain threshold. SELECT max(duration) FROM Transaction Copy SELECT average(duration) FROM Transaction Copy Alert on a percentage of your data Create alerts when a proportion of your data goes above or below a certain threshold. SELECT percentage(count(*), WHERE duration > 2) FROM Transaction Copy SELECT percentage(count(*), WHERE httpResponseCode = '500') FROM Transaction Copy Alert on Apdex with any T-value Create alerts on Apdex, applying your own T-value for certain transactions. For example, get an alert notification when your Apdex for a T-value of 500ms on transactions for production apps goes below 0.8. SELECT apdex(duration, t:0.5) FROM Transaction WHERE appName like '%prod%' Copy NRQL conditions and query order of operations By default, the aggregation window duration is 1 minute, but you can change the window to suit your needs. Whatever the aggregation window, New Relic will collect data for that window using the function in the NRQL condition’s query. The query is parsed and executed by our systems in the following order: FROM clause – which event type needs to be grabbed? WHERE clause – what can be filtered out? SELECT clause – what information needs to be returned from the now-filtered data set? Example: null value returned Let's say this is your alert condition query: SELECT count(*) FROM SyntheticCheck WHERE monitorName = 'My Cool Monitor' AND result = 'FAILURE' Copy If there are no failures for the aggregation window: The system will execute the FROM clause by grabbing all SyntheticCheck events on your account. Then it will execute the WHERE clause to filter through those events by looking only for the ones that match the monitor name and result specified. If there are still events left to scan through after completing the FROM and WHERE operations, the SELECT clause will be executed. If there are no remaining events, the SELECT clause will not be executed. This means that aggregators like count() and uniqueCount() will never return a zero value. When there is a count of 0, the SELECT clause is ignored and no data is returned, resulting in a value of NULL. Example: zero value returned If you have a data source delivering legitimate numeric zeroes, the query will return zero values and not null values. Let's say this is your alert condition query, and that MyCoolEvent is an attribute that can sometimes return a zero value. SELECT average(MyCoolAttribute) FROM MyCoolEvent Copy If, in the aggregation window being evaluated, there's at least one instance of MyCoolEvent and if the average value of all MyCoolAttribute attributes from that window is equal to zero, then a 0 value will be returned. If there are no MyCoolEvent events during that minute, then a NULL will be returned due to the order of operations. Tip For more information about this topic, you can check out our blog post on troubleshooting for zero versus null values. Tip You can determine how null values will be handled by adjusting loss of signal and gap filling settings in the Alert Conditions UI. Tip You can avoid NULL values entirely with a query order of operations shortcut. Do this by using a filter sub-clause, then including all filter elements within that sub-clause. The main body of the query will run and return data, at which point the SELECT clause will then run and apply the filter elements. The query will return a value of 0 if the filter elements result in no matching data. Here's an example: SELECT filter(count(*), WHERE result = 'SUCCESS' AND monitorName = 'My Favorite Monitor') FROM SyntheticCheck Copy Nested aggregation NRQL alerts Nested aggregation queries are a powerful way to query your data. However, they have a few restrictions that are important to note. Nested queries with a non-faceted innermost query are not currently supported Without a FACET, the inner query produces a single result, giving the outer query nothing to aggregate. If you're using a nested query, make sure your inner query is faceted. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu) ​​​​​ Copy Queries at all levels must have the same aggregation window size With an alert aggregation window of 1 minute, the inner query would produce two smaller windows of 30 seconds. In theory, these two windows could be aggregated by the outer query. However, this is not currently supported. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu TIMESERIES 30 seconds)​​ Copy Signal loss is not yet supported for nested queries For more information on signal loss, see NerdGraph API: Loss of signal and gap filling. Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported You can't use a nested query containing the WITH METRIC_FORMAT in the inner query to create NRQL alert conditions. NRQL condition creation tips Here are some tips for creating and using a NRQL condition: Topic Tips Condition types NRQL condition types include static, baseline, and outlier. Create a description For NRQL conditions, you can create a custom description to add to each violation. Descriptions can be enhanced with variable substitution based on metadata in the specific violation. For details, see Description Query results Queries must return a number. The condition evaluates the returned number against the thresholds you've set. Time period NRQL conditions evaluate data based on how it's aggregated, using aggregation windows from 30 seconds to 15 minutes, in increments of 15 seconds. For best results, we recommend using the event flow or event timer aggregation methods. For the cadence aggregation method, the implicit SINCE ... UNTIL clause specifying which minute to evaluate is controlled by your delay/timer setting. Since very recent data may be incomplete, you may want to query data from 3 minutes ago or longer, especially for: Applications that run on multiple hosts. SyntheticCheck data: Timeouts can take 3 minutes, so 5 minutes or more is recommended. Also, if a query will generate intermittent data, consider using the sum of query results option. Lost signal threshold (loss of signal detection) You can use loss of signal detection to alert on when your data (a telemetry signal) should be considered lost. A signal loss can indicate that a service or entity is no longer online or that a periodic job failed to run. You can also use this to make sure that violations for sporadic data, such as error counts, are closed when no signal is coming in. Advanced signal settings These settings give you options for better handling continuous, streaming data signals that may sometimes be missing. These settings include the aggregation window duration, the delay/timer, and an option for filling data gaps. For more on using these, see Advanced signal settings. Condition settings Use the Condition settings to: Create a concise, descriptive condition name. Provide a custom violation description for the condition that will be included in violations and notifications. Add the runbook URL to include your organization's procedures for handling incidents. You may also add this information to the custom violation description. Limits on conditions See the maximum values. Health status In order for a NRQL alert condition health status display to function properly, use a FACET clause to scope each signal to a single entity (for example, FACET hostname or FACET appname). Examples For more information, see: Expected NRQL syntax Examples of NRQL condition queries Condition edits can reset condition evaluation When you edit NRQL alert conditions in some specific ways (detailed below), their evaluations are reset, meaning that any evaluation up until that point is lost, and the evaluation starts over from that point. The two ways this will affect you are: For \"for at least x minutes\" thresholds: because the evaluation window has been reset, there will be a delay of at least x minutes before any violations can be reported. For baseline conditions: the condition starts over again and all baseline learning is lost. The following actions cause an evaluation reset for NRQL conditions: Changing the query Changing the aggregation window, aggregation method, or aggregation delay/timer setting Changing the \"close violations on signal loss\" setting Changing any gap fill settings Changing the baseline direction (if applicable) – higher, lower, or higher/lower Change the threshold value, threshold window, or threshold operator The following actions (along with any other actions not covered in the above list) will not reset the evaluation: Changing the loss of signal time window (expiration duration) Changing the time function (switching \"for at least\" to \"at least once in,\" or vice-versa) Toggling the \"open violation on signal loss\" setting Alert condition types When you create a NRQL alert, you can choose from different types of conditions: NRQL alert condition types Description Static This is the simplest type of NRQL condition. It allows you to create a condition based on a NRQL query that returns a numeric value. Optional: Include a FACET clause. Baseline (Dynamic) Uses a self-adjusting condition based on the past behavior of the monitored values. Uses the same NRQL query form as the static type, including the optional FACET clause. Outlier Looks for group behavior and values that are outliers from those groups. Uses the same NRQL query form as the static type, but requires a FACET clause. Sum of query results (limited or intermittent data) Important Available only for static (basic) condition types. If a query returns intermittent or limited data, it may be difficult to set a meaningful threshold. Missing or limited data will sometimes generate false positives or false negatives. You can use loss of signal, aggregation duration, and gap filling settings to minimize these false notifications. To avoid this problem when using the static threshold type, you can set the selector to sum of query results. This lets you set the alert on an aggregated sum instead of a value from a single harvest cycle. Up to two hours of one-minute data checks can be aggregated. The duration you select determines the width of the rolling sum and the preview chart will update accordingly. Set the loss of signal threshold Loss of signal occurs when no data matches the NRQL condition over a specific period of time. You can set your loss of signal threshold duration and also what happens when the threshold is crossed. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Loss of signal is only available for NRQL conditions. You may also manage these settings using the GraphQL API (recommended), or the REST API. Go here for specific GraphQL API examples. Loss of signal settings: Loss of signal settings include a time duration and two possible actions. Signal loss expiration time UI label: Signal is lost after: GraphQL Node: expiration.expirationDuration Expiration duration is a timer that starts and resets when we receive a data point in the streaming alerts pipeline. If we don't receive another data point before your 'expiration time' expires, we consider that signal to be lost. This can be because no data is being sent to New Relic or the WHERE clause of your NRQL query is filtering that data out before it is streamed to the alerts pipeline. Note that when you have a faceted query, each facet is a signal. So if any one of those signals ends during the duration specified, that will be considered a loss of signal. The loss of signal expiration time is independent of the threshold duration and triggers as soon as the timer expires. The maximum expiration duration is 48 hours. This is helpful when monitoring for the execution of infrequent jobs. The minimum is 30 seconds, but we recommend using at least 3-5 minutes. Loss of signal actions Once a signal is considered lost, you can close open violations, open new violations, or both. Close all current open violations: This closes all open violations that are related to a specific signal. It won't necessarily close all violations for a condition. If you're alerting on an ephemeral service, or on a sporadic signal, you'll want to choose this action to ensure that violations are closed properly. The GraphQL node name for this is \"closeViolationsOnExpiration\" Open new violations: This will open a new violation when the signal is considered lost. These violations will indicate that they are due to a loss of signal. Based on your incident preferences, this should trigger a notification. The graphQL node name for this is \"openViolationOnExpiration\" When you enable both actions, we'll close all open violations first, and then open a new violation for loss of signal. To create a NRQL alert configured with loss of signal detection in the UI: For a policy, when you create a condition, under Select a product, click NRQL, then click Next, define thresholds. Write a NRQL query that returns the values you want to alert on. For Threshold type, select Static or Baseline. Click + Add lost signal threshold, then set the signal expiration duration time in minutes or seconds in the Signal is lost after field. Choose what you want to happen when the signal is lost. You can check one or both of Close all current open violations and Open new \"lost signal\" violation. These control how loss of signal violations will be handled for the condition. Make sure you name your condition before you save it. Violations open due to loss of signal close when the signal comes back. Newly opened lost signal violations will close immediately when new data is evaluated. the condition they belong to expires. By default, conditions expire after 3 days. you manually close the violation with the Close all current open violations option. Tip Loss of signal detection doesn't work on NRQL queries that use nested aggregation or sub-queries. Advanced signal settings When creating a NRQL alert condition, use the advanced signal settings to control streaming alert data and avoid false alarms. When creating a NRQL condition, there are several advanced signal settings: Aggregation window duration Delay/timer Fill data gaps To read an explanation of what these settings are and how they relate to each other, see Streaming alerts concepts. Below are instructions and tips on how to configure them. Aggregation window duration You can set the aggregation window duration to choose how long data is accumulated in a streaming time window before it's aggregated. You can set it to anything between 30 seconds and 15 minutes. The default is one minute. Tip Baseline alert condition thresholds don't support editing the aggregation window. They use the 1 minute default. Delay/timer You can adjust the delay/timer to coordinate our streaming alerting algorithm with your data's behavior. If your data is sparse or inconsistent, you may want to use the event timer aggregation method. For the cadence method, the total supported latency is the sum of the aggregation window duration and the delay. If the data type comes from an APM language agent and is aggregated from many app instances (for example, Transactions, TransactionErrors, etc.), we recommend using the event flow method with the default settings. Important When creating NRQL conditions for data collected from Infrastructure Cloud Integrations such as AWS Cloudwatch or Azure, we recommend that you use the event timer method. Fill data gaps Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with one of these settings: None: (Default) Choose this if you don't want to take any action on empty aggregation windows. On evaluation, an empty aggregation window will reset the threshold duration timer. For example, if a condition says that all aggregation windows must have data points above the threshold for 5 minutes, and 1 of the 5 aggregation windows is empty, then the condition won't be in violation. Custom static value: Choose this if you'd like to insert a custom static value into the empty aggregation windows before they're evaluated. This option has an additional, required parameter of fillValue (as named in the API) that specifies what static value should be used. This defaults to 0. Last known value: This option inserts the last seen value before evaluation occurs. We maintain the state of the last seen value for 2 hours. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss, gap filling, and how to request access to these features, see this Explorers Hub post. Options for editing data gap settings: In the NRQL conditions UI, go to Condition settings > Advanced signal settings > fill data gaps with and choose an option. If using our Nerdgraph API (preferred), this node is located at: actor : account : alerts : nrqlCondition : signal : fillOption | fillValue NerdGraph is our recommended API for this but if you're using our REST API, you can find this setting in the REST API explorer under the \"signal\" section of the Alert NRQL conditions API.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.55426,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create NRQL <em>alert</em> conditions",
        "sections": "Nested aggregation NRQL <em>alerts</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": " a NRQL <em>alert</em> condition for a policy: On one.newrelic.com, in the header click <em>Alerts</em> &amp; AI, then in the left sidebar click Policies. Select an existing policy or click <em>New</em> <em>alert</em> policy to create a <em>new</em> policy. Click Add a condition. Under Select a product click NRQL, and then click Next, define"
      },
      "id": "603ef04864441fbc114e8883"
    },
    {
      "sections": [
        "Streaming alerts: key terms and concepts",
        "Why it matters",
        "Streaming alerts process and descriptions",
        "Choose your aggregation method",
        "Event flow",
        "Caution",
        "Event timer",
        "Cadence",
        "Streaming alerts tools",
        "Tip",
        "Window duration",
        "Delay/timer",
        "Loss of signal detection",
        "Gap filling"
      ],
      "title": "Streaming alerts: key terms and concepts",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Get started"
      ],
      "external_id": "144c7f0721ec2acb1e8ac8aa06142911ea552aef",
      "image": "https://docs.newrelic.com/static/39158bd84483adf9516bb7ff5058c9bd/d30ee/streaming-alerts.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/streaming-alerts-key-terms-concepts/",
      "published_at": "2021-12-15T12:47:58Z",
      "updated_at": "2021-12-15T12:47:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The streaming alerts platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into New Relic. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that's processed by the streaming algorithm. There are three methods for aggregating the data filtered through your NRQL condition: Event flow (default) Event timer Cadence This short video describes the three aggregation method (5:31). Why it matters Understanding how streaming alerts works will help you fine-tune your NRQL conditions to be notified about what's important to you. Only data that matches the conditions of the NRQL WHERE clause is alerted on. For more details on each step of the process, see Streaming alerts process and descriptions. As data streams into New Relic, it's filtered by the NRQL condition. Before data is evaluated, it must meet the criteria defined by the NRQL query's WHERE clause. Instead of evaluating that data immediately for violations, the NRQL alert conditions collect the data over a period of time known as the aggregation window. An additional delay/timer allows for slower data points to arrive before the window is aggregated. Once the delay/timer time has elapsed, New Relic aggregates the data into a single data point. Alerts then evaluates the data point using the NRQL condition in order to determine whether it meets the violation threshold criteria. Even if a data point meets the criteria for a violation, a violation may not be triggered. A violation is only triggered when data points consistently meet the threshold criteria over a period of time. This is the threshold duration. If the data points are in violation for an entire threshold duration, we'll send you a notification based on your policy settings. All of these configurable delays give you more control over how you're alerted on sporadic and missing data. Streaming alerts process and descriptions Process Description Streaming data All data coming into New Relic. WHERE clause Filters all incoming streaming data. We only monitor for alerts on data that makes it through this filter. Aggregation methods One of three methods that control how data is collected before it's evaluated. They are: Event flow (Default) Event timer Cadence Aggregation window Data with timestamps that fall within this window will be aggregated and then evaluated. Delay/timer A time delay to ensure all data points have arrived in the aggregation window before aggregation occurs. Aggregated data Data in the aggregated window is collapsed to a single data point for alert evaluation. Evaluation The data point is evaluated by the NRQL condition, which is trigged by each incoming aggregated data point. Threshold duration A specific duration that determines if a violation is created. If your specified NRQL condition meets the threshold criteria over the threshold duration, a violation occurs. When a data point lacks data, a custom value is inserted to fill the gap. Choose your aggregation method You can choose between three different aggregation methods, depending on your needs. Event flow (default) works best for data that comes in frequently and mostly in order. Event timer works best for data that arrives infrequently in batches, such as cloud integration data or infrequent error logs. Cadence is our original and inferior aggregation method. It aggregates data on specific time intervals as detected by New Relic's internal wall clock, regardless of data timestamps. Event flow Event flow aggregates a window of data when the first data point arrives in a subsequent window. The custom delay defines which subsequent window data will start to populate to trigger aggregation of the current window. A custom delay provides extra time for data to arrive. These times are based on the data's timestamps and not New Relic's wall clock time. For example, suppose you're monitoring CPU usage in window durations of 1 minute and a 3 minute delay. When a CPU usage data point comes in with a timestamp between 12:00pm and 12:01pm, event flow will not aggregate that window until a data point shows up with a timestamp between 12:04pm and 12:05pm. When event flow receives the first data point with a timestamp of 12:04pm or later, it sends the 12:00 to 12:01 data to be aggregated. Caution If you expect your data points to arrive more than 30 minutes apart, please use the Event Timer method described below. Event timer Like event flow, event timer will only aggregate data for a given window when data arrives for that window. When a data point arrives for an aggregation window, a timer dedicated to that window starts to count down. If no further data arrives before the timer counts down, the data for that window is aggregated. If more data points arrive before the timer has completed counting down, the timer is reset. For example, suppose you're monitoring CloudWatch data that arrives fairly infrequently. You're using a window duration of 1 minute and a 3 minute timer. When a CloudWatch data point comes in with a timestamp between 12:00pm and 12:01pm, the timer will start to count down. If no further data points show up for that 12:00-12:01 window, the window will get aggregated 3 minutes later. If a new data point with a timestamp between 12:00 and 12:01 arrives, the timer resets. It keeps resetting every time more data points for that window arrive. The window will not be sent for aggregation until the timer reaches 0. If the timer for a later data point elapses before an earlier data point, the event timer method waits for the earlier timer to elapse before aggregating the later data point. For best results, make sure your timer is equal to or longer than your window duration time. If the timer is less than your window duration and your data flow is inconsistent, then your data may be evaluated before all of your data points arrive. This could cause you to be notified incorrectly. Cadence We recommend you use one of the other two methods. Cadence is our old streaming aggregation method. This method uses New Relic's wall clock time to determine when data is aggregated and evaluated. It doesn't take into account data point timestamps as they arrive. Streaming alerts tools Streaming alerts provide a set of tools to give you greater control over how your data is aggregated before it's evaluated to reduce incorrect notifications you receive. They are: Window duration Delay/timer Loss of signal detection Gap filling Tip This article covers these tools at a conceptual level. You'll find direct instructions on how to use these tools in Create NRQL alert conditions. Window duration In order to make loss of signal detection more effective and to reduce unnecessary notifications, you can customize aggregation windows to the duration that you need. An aggregation window is a specific block of time. We gather data points together in an aggregation window, before evaluating the data. A longer aggregation window can smooth out the data, since an outlier data point will have more data points to be aggregated with, giving it less of an influence on the aggregated data point that is sent for evaluation. When a data point arrives, its timestamp is used to put it in the proper aggregation window. You can set your aggregation window to anything between 30 seconds and 15 minutes. The default is 1 minute. Delay/timer The delay/timer setting controls how long the condition should wait before aggregating the data in the aggregation window. The event flow and cadence methods use delay. Event timer uses timer. The delay default is 2 minutes. The timer default is 1 minute and has a minimum value of 5 seconds. Loss of signal detection Loss of signal occurs when no data matches the NRQL condition over a specific period of time. A loss of signal is caused by different things. The WHERE clause in your NRQL query can filter out data before it's evaluated for violations. It could also mean a service or entity is offline or a periodic job has failed to run and no data is being sent to New Relic. In order to avoid unnecessary notifications, you can choose how long to wait before you're notified by a loss of signal violation. You can use loss of signal detection to open violations and be notified when a signal is lost. Alternately, you can use a loss of signal to close violations for ephemeral services or sporadic data, such as error counts. Gap filling Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with the last value received, a static value, or else do nothing and leave the gap there. The default is None. Gaps in streaming data can be caused by network or host issues, a signal may be sparse, or some signals, such as error counts, may only have data when something is wrong. By filling the gaps with known values, the alert evaluation process can process those gaps and determine how they should affect the loss of signal evaluation. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss and gap filling, see this Explorers Hub post.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.07281,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "sections": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "The streaming <em>alerts</em> platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into <em>New</em> <em>Relic</em>. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that&#x27;s processed"
      },
      "id": "6130c054e7b9d269edb6f21d"
    },
    {
      "sections": [
        "Choose your aggregation method",
        "What's aggregation?",
        "Why it matters",
        "When to use event flow",
        "How event flow works",
        "Caution",
        "Event flow use cases",
        "When to use event timer",
        "How event timer works",
        "Event timer use cases",
        "Cadence",
        "Aggregation and loss of signal"
      ],
      "title": "Choose your aggregation method",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts"
      ],
      "external_id": "9bdcf28f192d61c2348909803e1c75876168936d",
      "image": "https://docs.newrelic.com/static/64df85a9f5694079b9ad1557de7fe5b5/c1b63/signal-consistency.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/get-started/choose-your-aggregation-method/",
      "published_at": "2021-12-19T15:02:53Z",
      "updated_at": "2021-12-04T15:31:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Alerts conditions provide a sophisticated set of tools for describing when you want to be notified about something that's happened or failed to happen on something you're monitoring. For best results, choose the aggregation method that best matches the way your data arrives. The three aggregation methods are event flow, event timer, and cadence. If you're interested in a conceptual overview, see our doc on streaming alerts, key terms and concepts. What's aggregation? When an application or service is monitored by New Relic, data can arrive in different ways. Some data arrives consistently and predictably, while other data arrives inconsistently and sporadically. Aggregation is how our alerting system gathers data together before analyzing it for exceeding warning or critical threshold levels. Your data is collected as data points in an aggregation window and then turned into a single numeric value. The data points are aggregated based on your NRQL query using methods like sum, average, min, and max, among others. This single numeric value is what's used to evaluate the condition's threshold. Once data has been aggregated, no more data points can be added to it. Our different aggregation methods will help you strike a balance between aggregating your data quickly and waiting for enough data points to arrive. Why it matters With the correct aggregation method, you're more likely to get notifications you care about, while preventing ones you don't. The most important questions to consider when deciding on your aggregation method: How often does my data arrive? How consistently does my data arrive? When data arrives frequently and consistently in a linear way, we recommend using event flow. When data arrives sporadically, inconsistently, and out of order, we recommend using event timer aggregation. When to use event flow With event flow, data is aggregated based on data point timestamps, so it's important that data points arrive in a consistent and linear manner. This aggregation method doesn't work as well for data point timestamps that arrive out of order or with a wide span of time within a short period. Event flow is the default aggregation method, because it applies for the most common use cases. How event flow works Event flow uses data point timestamps to determine when to open and close an aggregation window. For example, if you're using event flow with a 2 minute delay window, then an aggregation window will close when a timestamp arrives that's two minutes later than the last timestamp that was received. A data point with a 12:00pm timestamp arrives. An aggregation window opens. At some point, a 12:03pm data point arrives. Event flow closes the window, excluding the 12:03 data point, and evaluates that closed window against your thresholds. The event flow aggregation window will continue collecting data points until that later timestamp. The later timestamps are what moves the system forward, not the data points themselves. Event flow will wait as long as necessary for the next data point later than your delay setting to arrive, before aggregating the data. Event flow works best for data that arrives frequently and consistently. Caution If you expect your data points to arrive more than 30 minutes apart, please use the event timer method described below. Event flow use cases Here are some typical event flow use cases: APM agent data. Infrastructure agent data. Any data coming from a 3rd party that comes in frequently and reliably. Most AWS Cloudwatch metrics coming from the AWS Metric Stream (NOT polling). The main exception is that some AWS Cloudwatch data is very infrequent (like S3 volume data) regardless of whether it's streaming or polling and, in that case, you'd use Event timer. When to use event timer Event timer aggregation is based on a timer that counts down when a data point arrives. The timer resets every time a new data point arrives. If the timer counts down before a new data point arrives, event timer aggregates all of the data points received during that time. Event timer is best for alerting on events that happen sporadically and with large gaps of time. How event timer works Errors are a type of event that happens sporadically, unpredictably, and often with large gaps of time. For example, you might have a condition with a query that returns a count of errors. Many minutes may go by without any errors at all, and then suddenly 5 errors arrive within a minute. In this example, event timer would do nothing until the first of the 5 errors arrive. Then it would start the timer, resetting it each time a new error arrives. If the timer countdown reaches 0 without a new error, event timer aggregates the data, and evaluates it against your threshold. Event timer use cases Here are some typical event timer use cases: New Relic usage data. Cloud integration data that is being polled (such as with GCP, Azure, or AWS polling methods). Queries that deliver sparse or infrequent data, such as error counts. Cadence Cadence is our original aggregation method. It aggregates data on specific time intervals as detected by New Relic's internal wall clock, regardless of data timestamps. We recommend that you use one of our other aggregation methods instead. If you're currently using cadence, choose one of the other aggregation methods. Event flow is best for consistent, predictable data points. Event timer is best for inconsistent, sporadic data points. Aggregation and loss of signal Our loss of signal system runs separately from these aggregation methods and settings. If you set your alert condition to open a new violation when your signal is lost for 10 minutes, a loss of signal service watches for data points to arrive. If a new data point fails to arrive within 10 minutes, loss of signal causes a violation to open. For more information on when to use gap filling and loss single, see our forum post on when to use gap filling and loss of signal.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 162.8996,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Aggregation <em>and</em> loss of signal",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": " methods are event flow, event timer, and cadence. If you&#x27;re interested in a conceptual overview, see our doc on streaming <em>alerts</em>, key terms and concepts. What&#x27;s aggregation? When an application or service is monitored by <em>New</em> <em>Relic</em>, data can arrive in different ways. Some data arrives consistently"
      },
      "id": "61853bba28ccbc5a1b7fe94c"
    }
  ],
  "/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/provide-runbook-instructions-alert-activity": [
    {
      "sections": [
        "Create NRQL alert conditions",
        "Tip",
        "Create a NRQL alert condition",
        "Create a condition from a chart",
        "Caution",
        "NRQL alert syntax",
        "Important",
        "Reformatting incompatible NRQL",
        "NRQL alert threshold examples",
        "Alert on specific segments of your data",
        "Alert on Nth percentile of your data",
        "Alert on max, min, avg of your data",
        "Alert on a percentage of your data",
        "Alert on Apdex with any T-value",
        "NRQL conditions and query order of operations",
        "Example: null value returned",
        "Example: zero value returned",
        "Nested aggregation NRQL alerts",
        "Nested queries with a non-faceted innermost query are not currently supported",
        "Queries at all levels must have the same aggregation window size",
        "Signal loss is not yet supported for nested queries",
        "Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported",
        "NRQL condition creation tips",
        "Condition edits can reset condition evaluation",
        "Alert condition types",
        "Sum of query results (limited or intermittent data)",
        "Set the loss of signal threshold",
        "Advanced signal settings",
        "Aggregation window duration",
        "Delay/timer",
        "Fill data gaps"
      ],
      "title": "Create NRQL alert conditions",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert conditions"
      ],
      "external_id": "2872f770a5fc8a802f99b9f848906f7e351ad761",
      "image": "https://docs.newrelic.com/static/eb8e1b2d826f9fd9cf46fe3dd5455217/c1b63/nr1_nrql_alert_conditions.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/",
      "published_at": "2021-12-19T16:01:17Z",
      "updated_at": "2021-12-14T22:49:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use NRQL queries to create alert conditions. Once you've defined your signal, you can further define your warning and critical threshold levels. This determines when an alerts violation is created. Read on to learn more about how to do this. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Click NRQL, and then Next, define thresholds. Tip For more information on key concepts relating to NRQL alert conditions and streaming alerts, see Streaming alerts: key terms and concepts. Create a NRQL alert condition To create a NRQL alert condition for a policy: On one.newrelic.com, in the header click Alerts & AI, then in the left sidebar click Policies. Select an existing policy or click New alert policy to create a new policy. Click Add a condition. Under Select a product click NRQL, and then click Next, define thresholds. Note that editing an existing condition can result in resetting its evaluation. Create a condition from a chart You can use a chart to create a NRQL alert condition. To create a NRQL alerts condition from a chart, click the chart menu , then click Create alert condition. Once you've named and customized your condition, you can add it to an existing policy or create a new one. Caution A small number of our older charts don't include the option to create an alert condition. NRQL alert syntax Here's the basic syntax for creating all NRQL alert conditions. The FACET clause is required for outlier condition types. It's optional for static and baseline. SELECT function(attribute) FROM Event WHERE attribute [comparison] [AND|OR ...] Copy Clause Notes SELECT function(attribute) Required Supported functions that return numbers include: apdex average count latest max min percentage percentile sum uniqueCount Tip If you use the percentile aggregator in a faceted alert condition with many facets, this may cause the following error to appear: An error occurred while fetching chart data. If you see this error, use average instead. FROM data type Required Only one data type can be targeted. Supported data types: Event Metric (RAW data points will be returned) WHERE attribute [comparison] [AND|OR ...] Use the WHERE clause to specify a series of one or more conditions. All the operators are supported. FACET attribute Required for outlier conditions Include an optional FACET clause in your NRQL syntax depending on the threshold type: static or baseline. Use the FACET clause to separate your results by attribute and alert on each attribute independently. No LIMIT clause is allowed, but all queries will receive the maximum number of facets possible. Faceted queries can return a maximum of 5000 values for static and baseline conditions and a maximum of 500 values for outlier conditions. Important If the query returns more than the maximum number of values, the alert condition can't be created. If you create the condition and the query returns more than this number later, the alert will fail. Modify your query so that it returns a fewer number of values. Reformatting incompatible NRQL Some elements of NRQL used in charts don’t make sense in the streaming context of alerts. Here’s a list of the most common incompatible elements and suggestions for reformatting a NRQL alert query to achieve the same effect. Element Notes SINCE and UNTIL Example: SELECT percentile(largestContentfulPaint, 75) FROM PageViewTiming WHERE (appId = 837807) SINCE yesterday Copy NRQL conditions produce a never-ending stream of windowed query results, so the SINCE and UNTIL keywords to scope the query to a point in time are not compatible. As a convenience, we automatically strip SINCE and UNTIL from a query when creating a condition from the context of a chart. TIMESERIES In NRQL queries, the TIMESERIES clause is used to return data as a time series broken out by a specified period of time. For NRQL conditions, the equivalent property of a signal is the aggregation duration window. histogram() The histogram() aggregation function is used to generate histograms. histogram() is not compatible with NRQL alerting: histogram aggregations can not be formatted as a time series. To create an alert from a portion of a histogram (for example, 95th percentile), use the percentile() aggregation function. Multiple aggregation functions Each condition can only target a single aggregated value. To alert on multiple values simultaneously, you’ll need to decompose them into individual conditions within the same policy. Original Query: SELECT count(foo), average(bar), max(baz) from Transaction Copy Decomposed: SELECT count(foo) from Transaction SELECT average(bar) from Transaction SELECT max(baz) from Transaction Copy COMPARE WITH The COMPARE WITH clause is used to compare the values for two different time ranges. This type of query is incompatible with NRQL alerting. We recommend using a Baseline Alert Condition to dynamically detect deviations for a particular signal. SLIDE BY The SLIDE BY clause supports a feature known as sliding windows. With sliding windows, SLIDE BY data is gathered into \"windows\" of time that overlap with each other. These windows can help to smooth out line graphs with a lot of variation in cases where the rolling aggregate (such as a rolling mean) is more important than aggregates from narrow windows of time. Sliding windows are not currently supported in NRQL alerts. LIMIT In NRQL queries, the LIMIT clause is used to control the amount of data a query returns, either the maximum number of facet values returned by FACET queries or the maximum number of items returned by SELECT * queries. LIMIT is not compatible with NRQL alerting: evaluation is always performed on the full result set. NRQL alert threshold examples Here are some common use cases for NRQL conditions. These queries will work for static and baseline condition types. The outlier condition type will require additional FACET clauses. Alert on specific segments of your data Create constrained alerts that target a specific segment of your data, such as a few key customers or a range of data. Use the WHERE clause to define those conditions. SELECT average(duration) FROM Transaction WHERE account_id in (91290, 102021, 20230) Copy SELECT percentile(duration, 95) FROM Transaction WHERE name LIKE 'Controller/checkout/%' Copy Alert on Nth percentile of your data Create alerts when an Nth percentile of your data hits a specified threshold; for example, maintaining SLA service levels. Since we evaluate the NRQL query based on the aggregation window duration, percentiles will be calculated for each duration separately. SELECT percentile(duration, 95) FROM Transaction Copy SELECT percentile(databaseDuration, 75) FROM Transaction Copy Alert on max, min, avg of your data Create alerts when your data hits a certain maximum, minimum, or average; for example, ensuring that a duration or response time does not pass a certain threshold. SELECT max(duration) FROM Transaction Copy SELECT average(duration) FROM Transaction Copy Alert on a percentage of your data Create alerts when a proportion of your data goes above or below a certain threshold. SELECT percentage(count(*), WHERE duration > 2) FROM Transaction Copy SELECT percentage(count(*), WHERE httpResponseCode = '500') FROM Transaction Copy Alert on Apdex with any T-value Create alerts on Apdex, applying your own T-value for certain transactions. For example, get an alert notification when your Apdex for a T-value of 500ms on transactions for production apps goes below 0.8. SELECT apdex(duration, t:0.5) FROM Transaction WHERE appName like '%prod%' Copy NRQL conditions and query order of operations By default, the aggregation window duration is 1 minute, but you can change the window to suit your needs. Whatever the aggregation window, New Relic will collect data for that window using the function in the NRQL condition’s query. The query is parsed and executed by our systems in the following order: FROM clause – which event type needs to be grabbed? WHERE clause – what can be filtered out? SELECT clause – what information needs to be returned from the now-filtered data set? Example: null value returned Let's say this is your alert condition query: SELECT count(*) FROM SyntheticCheck WHERE monitorName = 'My Cool Monitor' AND result = 'FAILURE' Copy If there are no failures for the aggregation window: The system will execute the FROM clause by grabbing all SyntheticCheck events on your account. Then it will execute the WHERE clause to filter through those events by looking only for the ones that match the monitor name and result specified. If there are still events left to scan through after completing the FROM and WHERE operations, the SELECT clause will be executed. If there are no remaining events, the SELECT clause will not be executed. This means that aggregators like count() and uniqueCount() will never return a zero value. When there is a count of 0, the SELECT clause is ignored and no data is returned, resulting in a value of NULL. Example: zero value returned If you have a data source delivering legitimate numeric zeroes, the query will return zero values and not null values. Let's say this is your alert condition query, and that MyCoolEvent is an attribute that can sometimes return a zero value. SELECT average(MyCoolAttribute) FROM MyCoolEvent Copy If, in the aggregation window being evaluated, there's at least one instance of MyCoolEvent and if the average value of all MyCoolAttribute attributes from that window is equal to zero, then a 0 value will be returned. If there are no MyCoolEvent events during that minute, then a NULL will be returned due to the order of operations. Tip For more information about this topic, you can check out our blog post on troubleshooting for zero versus null values. Tip You can determine how null values will be handled by adjusting loss of signal and gap filling settings in the Alert Conditions UI. Tip You can avoid NULL values entirely with a query order of operations shortcut. Do this by using a filter sub-clause, then including all filter elements within that sub-clause. The main body of the query will run and return data, at which point the SELECT clause will then run and apply the filter elements. The query will return a value of 0 if the filter elements result in no matching data. Here's an example: SELECT filter(count(*), WHERE result = 'SUCCESS' AND monitorName = 'My Favorite Monitor') FROM SyntheticCheck Copy Nested aggregation NRQL alerts Nested aggregation queries are a powerful way to query your data. However, they have a few restrictions that are important to note. Nested queries with a non-faceted innermost query are not currently supported Without a FACET, the inner query produces a single result, giving the outer query nothing to aggregate. If you're using a nested query, make sure your inner query is faceted. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu) ​​​​​ Copy Queries at all levels must have the same aggregation window size With an alert aggregation window of 1 minute, the inner query would produce two smaller windows of 30 seconds. In theory, these two windows could be aggregated by the outer query. However, this is not currently supported. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu TIMESERIES 30 seconds)​​ Copy Signal loss is not yet supported for nested queries For more information on signal loss, see NerdGraph API: Loss of signal and gap filling. Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported You can't use a nested query containing the WITH METRIC_FORMAT in the inner query to create NRQL alert conditions. NRQL condition creation tips Here are some tips for creating and using a NRQL condition: Topic Tips Condition types NRQL condition types include static, baseline, and outlier. Create a description For NRQL conditions, you can create a custom description to add to each violation. Descriptions can be enhanced with variable substitution based on metadata in the specific violation. For details, see Description Query results Queries must return a number. The condition evaluates the returned number against the thresholds you've set. Time period NRQL conditions evaluate data based on how it's aggregated, using aggregation windows from 30 seconds to 15 minutes, in increments of 15 seconds. For best results, we recommend using the event flow or event timer aggregation methods. For the cadence aggregation method, the implicit SINCE ... UNTIL clause specifying which minute to evaluate is controlled by your delay/timer setting. Since very recent data may be incomplete, you may want to query data from 3 minutes ago or longer, especially for: Applications that run on multiple hosts. SyntheticCheck data: Timeouts can take 3 minutes, so 5 minutes or more is recommended. Also, if a query will generate intermittent data, consider using the sum of query results option. Lost signal threshold (loss of signal detection) You can use loss of signal detection to alert on when your data (a telemetry signal) should be considered lost. A signal loss can indicate that a service or entity is no longer online or that a periodic job failed to run. You can also use this to make sure that violations for sporadic data, such as error counts, are closed when no signal is coming in. Advanced signal settings These settings give you options for better handling continuous, streaming data signals that may sometimes be missing. These settings include the aggregation window duration, the delay/timer, and an option for filling data gaps. For more on using these, see Advanced signal settings. Condition settings Use the Condition settings to: Create a concise, descriptive condition name. Provide a custom violation description for the condition that will be included in violations and notifications. Add the runbook URL to include your organization's procedures for handling incidents. You may also add this information to the custom violation description. Limits on conditions See the maximum values. Health status In order for a NRQL alert condition health status display to function properly, use a FACET clause to scope each signal to a single entity (for example, FACET hostname or FACET appname). Examples For more information, see: Expected NRQL syntax Examples of NRQL condition queries Condition edits can reset condition evaluation When you edit NRQL alert conditions in some specific ways (detailed below), their evaluations are reset, meaning that any evaluation up until that point is lost, and the evaluation starts over from that point. The two ways this will affect you are: For \"for at least x minutes\" thresholds: because the evaluation window has been reset, there will be a delay of at least x minutes before any violations can be reported. For baseline conditions: the condition starts over again and all baseline learning is lost. The following actions cause an evaluation reset for NRQL conditions: Changing the query Changing the aggregation window, aggregation method, or aggregation delay/timer setting Changing the \"close violations on signal loss\" setting Changing any gap fill settings Changing the baseline direction (if applicable) – higher, lower, or higher/lower Change the threshold value, threshold window, or threshold operator The following actions (along with any other actions not covered in the above list) will not reset the evaluation: Changing the loss of signal time window (expiration duration) Changing the time function (switching \"for at least\" to \"at least once in,\" or vice-versa) Toggling the \"open violation on signal loss\" setting Alert condition types When you create a NRQL alert, you can choose from different types of conditions: NRQL alert condition types Description Static This is the simplest type of NRQL condition. It allows you to create a condition based on a NRQL query that returns a numeric value. Optional: Include a FACET clause. Baseline (Dynamic) Uses a self-adjusting condition based on the past behavior of the monitored values. Uses the same NRQL query form as the static type, including the optional FACET clause. Outlier Looks for group behavior and values that are outliers from those groups. Uses the same NRQL query form as the static type, but requires a FACET clause. Sum of query results (limited or intermittent data) Important Available only for static (basic) condition types. If a query returns intermittent or limited data, it may be difficult to set a meaningful threshold. Missing or limited data will sometimes generate false positives or false negatives. You can use loss of signal, aggregation duration, and gap filling settings to minimize these false notifications. To avoid this problem when using the static threshold type, you can set the selector to sum of query results. This lets you set the alert on an aggregated sum instead of a value from a single harvest cycle. Up to two hours of one-minute data checks can be aggregated. The duration you select determines the width of the rolling sum and the preview chart will update accordingly. Set the loss of signal threshold Loss of signal occurs when no data matches the NRQL condition over a specific period of time. You can set your loss of signal threshold duration and also what happens when the threshold is crossed. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Loss of signal is only available for NRQL conditions. You may also manage these settings using the GraphQL API (recommended), or the REST API. Go here for specific GraphQL API examples. Loss of signal settings: Loss of signal settings include a time duration and two possible actions. Signal loss expiration time UI label: Signal is lost after: GraphQL Node: expiration.expirationDuration Expiration duration is a timer that starts and resets when we receive a data point in the streaming alerts pipeline. If we don't receive another data point before your 'expiration time' expires, we consider that signal to be lost. This can be because no data is being sent to New Relic or the WHERE clause of your NRQL query is filtering that data out before it is streamed to the alerts pipeline. Note that when you have a faceted query, each facet is a signal. So if any one of those signals ends during the duration specified, that will be considered a loss of signal. The loss of signal expiration time is independent of the threshold duration and triggers as soon as the timer expires. The maximum expiration duration is 48 hours. This is helpful when monitoring for the execution of infrequent jobs. The minimum is 30 seconds, but we recommend using at least 3-5 minutes. Loss of signal actions Once a signal is considered lost, you can close open violations, open new violations, or both. Close all current open violations: This closes all open violations that are related to a specific signal. It won't necessarily close all violations for a condition. If you're alerting on an ephemeral service, or on a sporadic signal, you'll want to choose this action to ensure that violations are closed properly. The GraphQL node name for this is \"closeViolationsOnExpiration\" Open new violations: This will open a new violation when the signal is considered lost. These violations will indicate that they are due to a loss of signal. Based on your incident preferences, this should trigger a notification. The graphQL node name for this is \"openViolationOnExpiration\" When you enable both actions, we'll close all open violations first, and then open a new violation for loss of signal. To create a NRQL alert configured with loss of signal detection in the UI: For a policy, when you create a condition, under Select a product, click NRQL, then click Next, define thresholds. Write a NRQL query that returns the values you want to alert on. For Threshold type, select Static or Baseline. Click + Add lost signal threshold, then set the signal expiration duration time in minutes or seconds in the Signal is lost after field. Choose what you want to happen when the signal is lost. You can check one or both of Close all current open violations and Open new \"lost signal\" violation. These control how loss of signal violations will be handled for the condition. Make sure you name your condition before you save it. Violations open due to loss of signal close when the signal comes back. Newly opened lost signal violations will close immediately when new data is evaluated. the condition they belong to expires. By default, conditions expire after 3 days. you manually close the violation with the Close all current open violations option. Tip Loss of signal detection doesn't work on NRQL queries that use nested aggregation or sub-queries. Advanced signal settings When creating a NRQL alert condition, use the advanced signal settings to control streaming alert data and avoid false alarms. When creating a NRQL condition, there are several advanced signal settings: Aggregation window duration Delay/timer Fill data gaps To read an explanation of what these settings are and how they relate to each other, see Streaming alerts concepts. Below are instructions and tips on how to configure them. Aggregation window duration You can set the aggregation window duration to choose how long data is accumulated in a streaming time window before it's aggregated. You can set it to anything between 30 seconds and 15 minutes. The default is one minute. Tip Baseline alert condition thresholds don't support editing the aggregation window. They use the 1 minute default. Delay/timer You can adjust the delay/timer to coordinate our streaming alerting algorithm with your data's behavior. If your data is sparse or inconsistent, you may want to use the event timer aggregation method. For the cadence method, the total supported latency is the sum of the aggregation window duration and the delay. If the data type comes from an APM language agent and is aggregated from many app instances (for example, Transactions, TransactionErrors, etc.), we recommend using the event flow method with the default settings. Important When creating NRQL conditions for data collected from Infrastructure Cloud Integrations such as AWS Cloudwatch or Azure, we recommend that you use the event timer method. Fill data gaps Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with one of these settings: None: (Default) Choose this if you don't want to take any action on empty aggregation windows. On evaluation, an empty aggregation window will reset the threshold duration timer. For example, if a condition says that all aggregation windows must have data points above the threshold for 5 minutes, and 1 of the 5 aggregation windows is empty, then the condition won't be in violation. Custom static value: Choose this if you'd like to insert a custom static value into the empty aggregation windows before they're evaluated. This option has an additional, required parameter of fillValue (as named in the API) that specifies what static value should be used. This defaults to 0. Last known value: This option inserts the last seen value before evaluation occurs. We maintain the state of the last seen value for 2 hours. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss, gap filling, and how to request access to these features, see this Explorers Hub post. Options for editing data gap settings: In the NRQL conditions UI, go to Condition settings > Advanced signal settings > fill data gaps with and choose an option. If using our Nerdgraph API (preferred), this node is located at: actor : account : alerts : nrqlCondition : signal : fillOption | fillValue NerdGraph is our recommended API for this but if you're using our REST API, you can find this setting in the REST API explorer under the \"signal\" section of the Alert NRQL conditions API.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 251.85529,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create NRQL <em>alert</em> <em>conditions</em>",
        "sections": "Create NRQL <em>alert</em> <em>conditions</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "You can use NRQL queries to create <em>alert</em> <em>conditions</em>. Once you&#x27;ve defined your signal, you can further define your warning and critical threshold levels. This determines when an <em>alerts</em> violation is created. Read on to learn more about how to do this. Go to one.newrelic.com, click <em>Alerts</em> &amp; AI"
      },
      "id": "603ef04864441fbc114e8883"
    },
    {
      "sections": [
        "Streaming alerts: key terms and concepts",
        "Why it matters",
        "Streaming alerts process and descriptions",
        "Choose your aggregation method",
        "Event flow",
        "Caution",
        "Event timer",
        "Cadence",
        "Streaming alerts tools",
        "Tip",
        "Window duration",
        "Delay/timer",
        "Loss of signal detection",
        "Gap filling"
      ],
      "title": "Streaming alerts: key terms and concepts",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Get started"
      ],
      "external_id": "144c7f0721ec2acb1e8ac8aa06142911ea552aef",
      "image": "https://docs.newrelic.com/static/39158bd84483adf9516bb7ff5058c9bd/d30ee/streaming-alerts.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/streaming-alerts-key-terms-concepts/",
      "published_at": "2021-12-15T12:47:58Z",
      "updated_at": "2021-12-15T12:47:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The streaming alerts platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into New Relic. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that's processed by the streaming algorithm. There are three methods for aggregating the data filtered through your NRQL condition: Event flow (default) Event timer Cadence This short video describes the three aggregation method (5:31). Why it matters Understanding how streaming alerts works will help you fine-tune your NRQL conditions to be notified about what's important to you. Only data that matches the conditions of the NRQL WHERE clause is alerted on. For more details on each step of the process, see Streaming alerts process and descriptions. As data streams into New Relic, it's filtered by the NRQL condition. Before data is evaluated, it must meet the criteria defined by the NRQL query's WHERE clause. Instead of evaluating that data immediately for violations, the NRQL alert conditions collect the data over a period of time known as the aggregation window. An additional delay/timer allows for slower data points to arrive before the window is aggregated. Once the delay/timer time has elapsed, New Relic aggregates the data into a single data point. Alerts then evaluates the data point using the NRQL condition in order to determine whether it meets the violation threshold criteria. Even if a data point meets the criteria for a violation, a violation may not be triggered. A violation is only triggered when data points consistently meet the threshold criteria over a period of time. This is the threshold duration. If the data points are in violation for an entire threshold duration, we'll send you a notification based on your policy settings. All of these configurable delays give you more control over how you're alerted on sporadic and missing data. Streaming alerts process and descriptions Process Description Streaming data All data coming into New Relic. WHERE clause Filters all incoming streaming data. We only monitor for alerts on data that makes it through this filter. Aggregation methods One of three methods that control how data is collected before it's evaluated. They are: Event flow (Default) Event timer Cadence Aggregation window Data with timestamps that fall within this window will be aggregated and then evaluated. Delay/timer A time delay to ensure all data points have arrived in the aggregation window before aggregation occurs. Aggregated data Data in the aggregated window is collapsed to a single data point for alert evaluation. Evaluation The data point is evaluated by the NRQL condition, which is trigged by each incoming aggregated data point. Threshold duration A specific duration that determines if a violation is created. If your specified NRQL condition meets the threshold criteria over the threshold duration, a violation occurs. When a data point lacks data, a custom value is inserted to fill the gap. Choose your aggregation method You can choose between three different aggregation methods, depending on your needs. Event flow (default) works best for data that comes in frequently and mostly in order. Event timer works best for data that arrives infrequently in batches, such as cloud integration data or infrequent error logs. Cadence is our original and inferior aggregation method. It aggregates data on specific time intervals as detected by New Relic's internal wall clock, regardless of data timestamps. Event flow Event flow aggregates a window of data when the first data point arrives in a subsequent window. The custom delay defines which subsequent window data will start to populate to trigger aggregation of the current window. A custom delay provides extra time for data to arrive. These times are based on the data's timestamps and not New Relic's wall clock time. For example, suppose you're monitoring CPU usage in window durations of 1 minute and a 3 minute delay. When a CPU usage data point comes in with a timestamp between 12:00pm and 12:01pm, event flow will not aggregate that window until a data point shows up with a timestamp between 12:04pm and 12:05pm. When event flow receives the first data point with a timestamp of 12:04pm or later, it sends the 12:00 to 12:01 data to be aggregated. Caution If you expect your data points to arrive more than 30 minutes apart, please use the Event Timer method described below. Event timer Like event flow, event timer will only aggregate data for a given window when data arrives for that window. When a data point arrives for an aggregation window, a timer dedicated to that window starts to count down. If no further data arrives before the timer counts down, the data for that window is aggregated. If more data points arrive before the timer has completed counting down, the timer is reset. For example, suppose you're monitoring CloudWatch data that arrives fairly infrequently. You're using a window duration of 1 minute and a 3 minute timer. When a CloudWatch data point comes in with a timestamp between 12:00pm and 12:01pm, the timer will start to count down. If no further data points show up for that 12:00-12:01 window, the window will get aggregated 3 minutes later. If a new data point with a timestamp between 12:00 and 12:01 arrives, the timer resets. It keeps resetting every time more data points for that window arrive. The window will not be sent for aggregation until the timer reaches 0. If the timer for a later data point elapses before an earlier data point, the event timer method waits for the earlier timer to elapse before aggregating the later data point. For best results, make sure your timer is equal to or longer than your window duration time. If the timer is less than your window duration and your data flow is inconsistent, then your data may be evaluated before all of your data points arrive. This could cause you to be notified incorrectly. Cadence We recommend you use one of the other two methods. Cadence is our old streaming aggregation method. This method uses New Relic's wall clock time to determine when data is aggregated and evaluated. It doesn't take into account data point timestamps as they arrive. Streaming alerts tools Streaming alerts provide a set of tools to give you greater control over how your data is aggregated before it's evaluated to reduce incorrect notifications you receive. They are: Window duration Delay/timer Loss of signal detection Gap filling Tip This article covers these tools at a conceptual level. You'll find direct instructions on how to use these tools in Create NRQL alert conditions. Window duration In order to make loss of signal detection more effective and to reduce unnecessary notifications, you can customize aggregation windows to the duration that you need. An aggregation window is a specific block of time. We gather data points together in an aggregation window, before evaluating the data. A longer aggregation window can smooth out the data, since an outlier data point will have more data points to be aggregated with, giving it less of an influence on the aggregated data point that is sent for evaluation. When a data point arrives, its timestamp is used to put it in the proper aggregation window. You can set your aggregation window to anything between 30 seconds and 15 minutes. The default is 1 minute. Delay/timer The delay/timer setting controls how long the condition should wait before aggregating the data in the aggregation window. The event flow and cadence methods use delay. Event timer uses timer. The delay default is 2 minutes. The timer default is 1 minute and has a minimum value of 5 seconds. Loss of signal detection Loss of signal occurs when no data matches the NRQL condition over a specific period of time. A loss of signal is caused by different things. The WHERE clause in your NRQL query can filter out data before it's evaluated for violations. It could also mean a service or entity is offline or a periodic job has failed to run and no data is being sent to New Relic. In order to avoid unnecessary notifications, you can choose how long to wait before you're notified by a loss of signal violation. You can use loss of signal detection to open violations and be notified when a signal is lost. Alternately, you can use a loss of signal to close violations for ephemeral services or sporadic data, such as error counts. Gap filling Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with the last value received, a static value, or else do nothing and leave the gap there. The default is None. Gaps in streaming data can be caused by network or host issues, a signal may be sparse, or some signals, such as error counts, may only have data when something is wrong. By filling the gaps with known values, the alert evaluation process can process those gaps and determine how they should affect the loss of signal evaluation. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss and gap filling, see this Explorers Hub post.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 229.45493,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "sections": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "The streaming <em>alerts</em> platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into <em>New</em> <em>Relic</em>. You can use NRQL <em>conditions</em> to control what part of the signal you want to be notified about. Your NRQL condition filters the data that&#x27;s processed"
      },
      "id": "6130c054e7b9d269edb6f21d"
    },
    {
      "sections": [
        "Outlier detection (NRQL alert)",
        "Important",
        "What is outlier detection?",
        "Tip",
        "Example use cases",
        "Notify if load-balanced servers have uneven workload",
        "Notify if load-balanced application has misbehaving instances",
        "Notify of changes in different environments",
        "Notify for timezone-related changes",
        "Create an outlier alert condition",
        "Rules and logic",
        "Details about alert condition logic",
        "NRQL query rules and limits",
        "Zero values for unreturned data"
      ],
      "title": "Outlier detection (NRQL alert)",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert conditions"
      ],
      "external_id": "499fa55abd48a0ccdd897fbdf64ccea2d9f98d11",
      "image": "https://docs.newrelic.com/static/f235d0630576bc2010ff07adc7a69621/f73a1/NRQL_outlier_violations.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/outlier-detection-nrql-alert/",
      "published_at": "2021-12-15T12:44:32Z",
      "updated_at": "2021-11-25T19:50:19Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Alerts offers NRQL conditions in three threshold types: static, baseline, and outlier. This document explains how the outlier threshold type works, gives some example use cases and NRQL queries, and explains how to create an outlier condition. Important NRQL alerts do not affect Alerts policies for a Synthetic monitor. For example, muting a NRQL alert will not mute a Synthetic monitor's alerts. What is outlier detection? In software development and operations, it is common to have a group consisting of members you expect to behave approximately the same. For example: for servers using a load balancer, the traffic to the servers may go up or down, but the traffic for all the servers should remain in a fairly tight grouping. See outlier detection in action in this NerdBytes video (2:51 minutes). The NRQL alert outlier detection feature parses the data returned by your faceted NRQL query and: Looks for the number of expected groups that you specify Looks for outliers (values deviating from a group) based on the sensitivity and time range you set Additionally, for queries that have more than one group, you can choose to be notified when groups start behaving the same. This visual aid will help you understand the types of situations that will trigger a violation and those that won't. For more on the rules and logic behind this calculation, see Outlier detection rules. Tip Note: this feature does not take into account the past behavior of the monitored values; it looks for outliers only in the currently reported data. For an alert type that takes into account past behavior, see Baseline alerting. Example use cases These use cases will help you understand when to use the outlier threshold type. Note that the outlier feature requires a NRQL query with a FACET clause. Notify if load-balanced servers have uneven workload A load balancer divides web traffic approximately evenly across five different servers. You can set a notification to be sent if any server starts getting significantly more or less traffic than the other servers. Example query: SELECT average(cpuPercent) FROM SystemSample WHERE apmApplicationNames = 'MY-APP-NAME' FACET hostname Copy Notify if load-balanced application has misbehaving instances Application instances behind a load balancer should have similar throughput, error rates, and response times. If an instance is in a bad state, or a load balancer is misconfigured, this will not be the case. Detecting one or two bad app instances using aggregate metrics may be difficult if there is not a significant rise in the overall error rate of the application. You can set a notification for when an app instance’s throughput, error rate, or response time deviates too far from the rest of the group. Example query: SELECT average(duration) FROM Transaction WHERE appName = 'MY-APP-NAME' FACET host Copy Notify of changes in different environments An application is deployed in two different environments, with ten application instances in each. One environment is experimental and gets more errors than the other. But the instances that are in the same environment should get approximately the same number of errors. You can set a notification for when an instance starts getting more errors than the other instances in the same environment. Also, you can set a notification for when the two environments start to have the same number of errors as each other. Notify for timezone-related changes The number of logged in users for a company is about the same for each of four applications, but varies significantly by each of the three time zones the company operates in. You can set a notification for when any application starts getting more or less traffic from a certain timezone than the other applications. Sometimes the traffic from the different time zones are the same, so you would set up the alert condition to not be notified if the time zone groups overlap. For more details on how this feature works, see Outlier rules and logic. Create an outlier alert condition To create a NRQL alert that uses outlier detection: When creating a condition, under Select a product, select NRQL. For Threshold type, select Outlier. Create a NRQL query with a FACET clause that returns the values you want to alert on. Depending on how the returned values group together, set the Number of expected groups. Adjust the deviation from the center of the group(s) and the duration that will trigger a violation. Optional: Add a warning threshold and set its deviation. Set any remaining available options and save. Rules and logic Here are the rules and logic behind how outlier detection works: Details about alert condition logic After the condition is created, the query is run once every harvest cycle and the condition is applied. Unlike baseline alerts, outlier detection uses no historical data in its calculation; it's calculated using the currently collected data. Alerts will attempt to divide the data returned from the query into the number of groups selected during condition creation. For each group, the approximate average value is calculated. The allowable deviation you have chosen when creating the condition is centered around that average value. If a member of the group is outside the allowed deviation, it produces a violation. If Trigger when groups overlap has been selected, Alerts detects a convergence of groups. If the condition is looking for two or more groups, and the returned values cannot be separated into that number of distinct groups, then that will produce a violation. This type of “overlap” event is represented on a chart by group bands touching. Because this feature does not take past behavior into account, data is never considered to \"belong\" to a certain group. For example, a value that switches places with another value wouldn't trigger a violation. Additionally, an entire group that moves together also wouldn't trigger a violation. NRQL query rules and limits The NRQL query must be a faceted query. The number of unique values returned must be 500 or less. If the query returns more than this number of values, the condition won't be created. If the query later returns more than this number after being created, the alert will fail. Zero values for unreturned data When a query returns a set of values, only values that are actually returned are taken into account. If a value is not available for calculation (including if it goes from being collected one harvest cycle to not being collected), it is rendered as a zero and is not considered. In other words, the behavior of unreturned zero values will never trigger violations.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.94453,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Outlier detection (NRQL <em>alert</em>)",
        "sections": "Create an outlier <em>alert</em> <em>condition</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "<em>Alerts</em> offers NRQL <em>conditions</em> in three threshold types: static, baseline, and outlier. This document explains how the outlier threshold type works, gives some example use cases and NRQL queries, and explains how to create an outlier condition. Important NRQL <em>alerts</em> do not affect <em>Alerts</em> policies"
      },
      "id": "6130be72196a6793654948e7"
    }
  ],
  "/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/streaming-alerts-key-terms-concepts": [
    {
      "sections": [
        "Create NRQL alert conditions",
        "Tip",
        "Create a NRQL alert condition",
        "Create a condition from a chart",
        "Caution",
        "NRQL alert syntax",
        "Important",
        "Reformatting incompatible NRQL",
        "NRQL alert threshold examples",
        "Alert on specific segments of your data",
        "Alert on Nth percentile of your data",
        "Alert on max, min, avg of your data",
        "Alert on a percentage of your data",
        "Alert on Apdex with any T-value",
        "NRQL conditions and query order of operations",
        "Example: null value returned",
        "Example: zero value returned",
        "Nested aggregation NRQL alerts",
        "Nested queries with a non-faceted innermost query are not currently supported",
        "Queries at all levels must have the same aggregation window size",
        "Signal loss is not yet supported for nested queries",
        "Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported",
        "NRQL condition creation tips",
        "Condition edits can reset condition evaluation",
        "Alert condition types",
        "Sum of query results (limited or intermittent data)",
        "Set the loss of signal threshold",
        "Advanced signal settings",
        "Aggregation window duration",
        "Delay/timer",
        "Fill data gaps"
      ],
      "title": "Create NRQL alert conditions",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert conditions"
      ],
      "external_id": "2872f770a5fc8a802f99b9f848906f7e351ad761",
      "image": "https://docs.newrelic.com/static/eb8e1b2d826f9fd9cf46fe3dd5455217/c1b63/nr1_nrql_alert_conditions.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/",
      "published_at": "2021-12-19T16:01:17Z",
      "updated_at": "2021-12-14T22:49:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use NRQL queries to create alert conditions. Once you've defined your signal, you can further define your warning and critical threshold levels. This determines when an alerts violation is created. Read on to learn more about how to do this. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Click NRQL, and then Next, define thresholds. Tip For more information on key concepts relating to NRQL alert conditions and streaming alerts, see Streaming alerts: key terms and concepts. Create a NRQL alert condition To create a NRQL alert condition for a policy: On one.newrelic.com, in the header click Alerts & AI, then in the left sidebar click Policies. Select an existing policy or click New alert policy to create a new policy. Click Add a condition. Under Select a product click NRQL, and then click Next, define thresholds. Note that editing an existing condition can result in resetting its evaluation. Create a condition from a chart You can use a chart to create a NRQL alert condition. To create a NRQL alerts condition from a chart, click the chart menu , then click Create alert condition. Once you've named and customized your condition, you can add it to an existing policy or create a new one. Caution A small number of our older charts don't include the option to create an alert condition. NRQL alert syntax Here's the basic syntax for creating all NRQL alert conditions. The FACET clause is required for outlier condition types. It's optional for static and baseline. SELECT function(attribute) FROM Event WHERE attribute [comparison] [AND|OR ...] Copy Clause Notes SELECT function(attribute) Required Supported functions that return numbers include: apdex average count latest max min percentage percentile sum uniqueCount Tip If you use the percentile aggregator in a faceted alert condition with many facets, this may cause the following error to appear: An error occurred while fetching chart data. If you see this error, use average instead. FROM data type Required Only one data type can be targeted. Supported data types: Event Metric (RAW data points will be returned) WHERE attribute [comparison] [AND|OR ...] Use the WHERE clause to specify a series of one or more conditions. All the operators are supported. FACET attribute Required for outlier conditions Include an optional FACET clause in your NRQL syntax depending on the threshold type: static or baseline. Use the FACET clause to separate your results by attribute and alert on each attribute independently. No LIMIT clause is allowed, but all queries will receive the maximum number of facets possible. Faceted queries can return a maximum of 5000 values for static and baseline conditions and a maximum of 500 values for outlier conditions. Important If the query returns more than the maximum number of values, the alert condition can't be created. If you create the condition and the query returns more than this number later, the alert will fail. Modify your query so that it returns a fewer number of values. Reformatting incompatible NRQL Some elements of NRQL used in charts don’t make sense in the streaming context of alerts. Here’s a list of the most common incompatible elements and suggestions for reformatting a NRQL alert query to achieve the same effect. Element Notes SINCE and UNTIL Example: SELECT percentile(largestContentfulPaint, 75) FROM PageViewTiming WHERE (appId = 837807) SINCE yesterday Copy NRQL conditions produce a never-ending stream of windowed query results, so the SINCE and UNTIL keywords to scope the query to a point in time are not compatible. As a convenience, we automatically strip SINCE and UNTIL from a query when creating a condition from the context of a chart. TIMESERIES In NRQL queries, the TIMESERIES clause is used to return data as a time series broken out by a specified period of time. For NRQL conditions, the equivalent property of a signal is the aggregation duration window. histogram() The histogram() aggregation function is used to generate histograms. histogram() is not compatible with NRQL alerting: histogram aggregations can not be formatted as a time series. To create an alert from a portion of a histogram (for example, 95th percentile), use the percentile() aggregation function. Multiple aggregation functions Each condition can only target a single aggregated value. To alert on multiple values simultaneously, you’ll need to decompose them into individual conditions within the same policy. Original Query: SELECT count(foo), average(bar), max(baz) from Transaction Copy Decomposed: SELECT count(foo) from Transaction SELECT average(bar) from Transaction SELECT max(baz) from Transaction Copy COMPARE WITH The COMPARE WITH clause is used to compare the values for two different time ranges. This type of query is incompatible with NRQL alerting. We recommend using a Baseline Alert Condition to dynamically detect deviations for a particular signal. SLIDE BY The SLIDE BY clause supports a feature known as sliding windows. With sliding windows, SLIDE BY data is gathered into \"windows\" of time that overlap with each other. These windows can help to smooth out line graphs with a lot of variation in cases where the rolling aggregate (such as a rolling mean) is more important than aggregates from narrow windows of time. Sliding windows are not currently supported in NRQL alerts. LIMIT In NRQL queries, the LIMIT clause is used to control the amount of data a query returns, either the maximum number of facet values returned by FACET queries or the maximum number of items returned by SELECT * queries. LIMIT is not compatible with NRQL alerting: evaluation is always performed on the full result set. NRQL alert threshold examples Here are some common use cases for NRQL conditions. These queries will work for static and baseline condition types. The outlier condition type will require additional FACET clauses. Alert on specific segments of your data Create constrained alerts that target a specific segment of your data, such as a few key customers or a range of data. Use the WHERE clause to define those conditions. SELECT average(duration) FROM Transaction WHERE account_id in (91290, 102021, 20230) Copy SELECT percentile(duration, 95) FROM Transaction WHERE name LIKE 'Controller/checkout/%' Copy Alert on Nth percentile of your data Create alerts when an Nth percentile of your data hits a specified threshold; for example, maintaining SLA service levels. Since we evaluate the NRQL query based on the aggregation window duration, percentiles will be calculated for each duration separately. SELECT percentile(duration, 95) FROM Transaction Copy SELECT percentile(databaseDuration, 75) FROM Transaction Copy Alert on max, min, avg of your data Create alerts when your data hits a certain maximum, minimum, or average; for example, ensuring that a duration or response time does not pass a certain threshold. SELECT max(duration) FROM Transaction Copy SELECT average(duration) FROM Transaction Copy Alert on a percentage of your data Create alerts when a proportion of your data goes above or below a certain threshold. SELECT percentage(count(*), WHERE duration > 2) FROM Transaction Copy SELECT percentage(count(*), WHERE httpResponseCode = '500') FROM Transaction Copy Alert on Apdex with any T-value Create alerts on Apdex, applying your own T-value for certain transactions. For example, get an alert notification when your Apdex for a T-value of 500ms on transactions for production apps goes below 0.8. SELECT apdex(duration, t:0.5) FROM Transaction WHERE appName like '%prod%' Copy NRQL conditions and query order of operations By default, the aggregation window duration is 1 minute, but you can change the window to suit your needs. Whatever the aggregation window, New Relic will collect data for that window using the function in the NRQL condition’s query. The query is parsed and executed by our systems in the following order: FROM clause – which event type needs to be grabbed? WHERE clause – what can be filtered out? SELECT clause – what information needs to be returned from the now-filtered data set? Example: null value returned Let's say this is your alert condition query: SELECT count(*) FROM SyntheticCheck WHERE monitorName = 'My Cool Monitor' AND result = 'FAILURE' Copy If there are no failures for the aggregation window: The system will execute the FROM clause by grabbing all SyntheticCheck events on your account. Then it will execute the WHERE clause to filter through those events by looking only for the ones that match the monitor name and result specified. If there are still events left to scan through after completing the FROM and WHERE operations, the SELECT clause will be executed. If there are no remaining events, the SELECT clause will not be executed. This means that aggregators like count() and uniqueCount() will never return a zero value. When there is a count of 0, the SELECT clause is ignored and no data is returned, resulting in a value of NULL. Example: zero value returned If you have a data source delivering legitimate numeric zeroes, the query will return zero values and not null values. Let's say this is your alert condition query, and that MyCoolEvent is an attribute that can sometimes return a zero value. SELECT average(MyCoolAttribute) FROM MyCoolEvent Copy If, in the aggregation window being evaluated, there's at least one instance of MyCoolEvent and if the average value of all MyCoolAttribute attributes from that window is equal to zero, then a 0 value will be returned. If there are no MyCoolEvent events during that minute, then a NULL will be returned due to the order of operations. Tip For more information about this topic, you can check out our blog post on troubleshooting for zero versus null values. Tip You can determine how null values will be handled by adjusting loss of signal and gap filling settings in the Alert Conditions UI. Tip You can avoid NULL values entirely with a query order of operations shortcut. Do this by using a filter sub-clause, then including all filter elements within that sub-clause. The main body of the query will run and return data, at which point the SELECT clause will then run and apply the filter elements. The query will return a value of 0 if the filter elements result in no matching data. Here's an example: SELECT filter(count(*), WHERE result = 'SUCCESS' AND monitorName = 'My Favorite Monitor') FROM SyntheticCheck Copy Nested aggregation NRQL alerts Nested aggregation queries are a powerful way to query your data. However, they have a few restrictions that are important to note. Nested queries with a non-faceted innermost query are not currently supported Without a FACET, the inner query produces a single result, giving the outer query nothing to aggregate. If you're using a nested query, make sure your inner query is faceted. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu) ​​​​​ Copy Queries at all levels must have the same aggregation window size With an alert aggregation window of 1 minute, the inner query would produce two smaller windows of 30 seconds. In theory, these two windows could be aggregated by the outer query. However, this is not currently supported. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu TIMESERIES 30 seconds)​​ Copy Signal loss is not yet supported for nested queries For more information on signal loss, see NerdGraph API: Loss of signal and gap filling. Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported You can't use a nested query containing the WITH METRIC_FORMAT in the inner query to create NRQL alert conditions. NRQL condition creation tips Here are some tips for creating and using a NRQL condition: Topic Tips Condition types NRQL condition types include static, baseline, and outlier. Create a description For NRQL conditions, you can create a custom description to add to each violation. Descriptions can be enhanced with variable substitution based on metadata in the specific violation. For details, see Description Query results Queries must return a number. The condition evaluates the returned number against the thresholds you've set. Time period NRQL conditions evaluate data based on how it's aggregated, using aggregation windows from 30 seconds to 15 minutes, in increments of 15 seconds. For best results, we recommend using the event flow or event timer aggregation methods. For the cadence aggregation method, the implicit SINCE ... UNTIL clause specifying which minute to evaluate is controlled by your delay/timer setting. Since very recent data may be incomplete, you may want to query data from 3 minutes ago or longer, especially for: Applications that run on multiple hosts. SyntheticCheck data: Timeouts can take 3 minutes, so 5 minutes or more is recommended. Also, if a query will generate intermittent data, consider using the sum of query results option. Lost signal threshold (loss of signal detection) You can use loss of signal detection to alert on when your data (a telemetry signal) should be considered lost. A signal loss can indicate that a service or entity is no longer online or that a periodic job failed to run. You can also use this to make sure that violations for sporadic data, such as error counts, are closed when no signal is coming in. Advanced signal settings These settings give you options for better handling continuous, streaming data signals that may sometimes be missing. These settings include the aggregation window duration, the delay/timer, and an option for filling data gaps. For more on using these, see Advanced signal settings. Condition settings Use the Condition settings to: Create a concise, descriptive condition name. Provide a custom violation description for the condition that will be included in violations and notifications. Add the runbook URL to include your organization's procedures for handling incidents. You may also add this information to the custom violation description. Limits on conditions See the maximum values. Health status In order for a NRQL alert condition health status display to function properly, use a FACET clause to scope each signal to a single entity (for example, FACET hostname or FACET appname). Examples For more information, see: Expected NRQL syntax Examples of NRQL condition queries Condition edits can reset condition evaluation When you edit NRQL alert conditions in some specific ways (detailed below), their evaluations are reset, meaning that any evaluation up until that point is lost, and the evaluation starts over from that point. The two ways this will affect you are: For \"for at least x minutes\" thresholds: because the evaluation window has been reset, there will be a delay of at least x minutes before any violations can be reported. For baseline conditions: the condition starts over again and all baseline learning is lost. The following actions cause an evaluation reset for NRQL conditions: Changing the query Changing the aggregation window, aggregation method, or aggregation delay/timer setting Changing the \"close violations on signal loss\" setting Changing any gap fill settings Changing the baseline direction (if applicable) – higher, lower, or higher/lower Change the threshold value, threshold window, or threshold operator The following actions (along with any other actions not covered in the above list) will not reset the evaluation: Changing the loss of signal time window (expiration duration) Changing the time function (switching \"for at least\" to \"at least once in,\" or vice-versa) Toggling the \"open violation on signal loss\" setting Alert condition types When you create a NRQL alert, you can choose from different types of conditions: NRQL alert condition types Description Static This is the simplest type of NRQL condition. It allows you to create a condition based on a NRQL query that returns a numeric value. Optional: Include a FACET clause. Baseline (Dynamic) Uses a self-adjusting condition based on the past behavior of the monitored values. Uses the same NRQL query form as the static type, including the optional FACET clause. Outlier Looks for group behavior and values that are outliers from those groups. Uses the same NRQL query form as the static type, but requires a FACET clause. Sum of query results (limited or intermittent data) Important Available only for static (basic) condition types. If a query returns intermittent or limited data, it may be difficult to set a meaningful threshold. Missing or limited data will sometimes generate false positives or false negatives. You can use loss of signal, aggregation duration, and gap filling settings to minimize these false notifications. To avoid this problem when using the static threshold type, you can set the selector to sum of query results. This lets you set the alert on an aggregated sum instead of a value from a single harvest cycle. Up to two hours of one-minute data checks can be aggregated. The duration you select determines the width of the rolling sum and the preview chart will update accordingly. Set the loss of signal threshold Loss of signal occurs when no data matches the NRQL condition over a specific period of time. You can set your loss of signal threshold duration and also what happens when the threshold is crossed. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Loss of signal is only available for NRQL conditions. You may also manage these settings using the GraphQL API (recommended), or the REST API. Go here for specific GraphQL API examples. Loss of signal settings: Loss of signal settings include a time duration and two possible actions. Signal loss expiration time UI label: Signal is lost after: GraphQL Node: expiration.expirationDuration Expiration duration is a timer that starts and resets when we receive a data point in the streaming alerts pipeline. If we don't receive another data point before your 'expiration time' expires, we consider that signal to be lost. This can be because no data is being sent to New Relic or the WHERE clause of your NRQL query is filtering that data out before it is streamed to the alerts pipeline. Note that when you have a faceted query, each facet is a signal. So if any one of those signals ends during the duration specified, that will be considered a loss of signal. The loss of signal expiration time is independent of the threshold duration and triggers as soon as the timer expires. The maximum expiration duration is 48 hours. This is helpful when monitoring for the execution of infrequent jobs. The minimum is 30 seconds, but we recommend using at least 3-5 minutes. Loss of signal actions Once a signal is considered lost, you can close open violations, open new violations, or both. Close all current open violations: This closes all open violations that are related to a specific signal. It won't necessarily close all violations for a condition. If you're alerting on an ephemeral service, or on a sporadic signal, you'll want to choose this action to ensure that violations are closed properly. The GraphQL node name for this is \"closeViolationsOnExpiration\" Open new violations: This will open a new violation when the signal is considered lost. These violations will indicate that they are due to a loss of signal. Based on your incident preferences, this should trigger a notification. The graphQL node name for this is \"openViolationOnExpiration\" When you enable both actions, we'll close all open violations first, and then open a new violation for loss of signal. To create a NRQL alert configured with loss of signal detection in the UI: For a policy, when you create a condition, under Select a product, click NRQL, then click Next, define thresholds. Write a NRQL query that returns the values you want to alert on. For Threshold type, select Static or Baseline. Click + Add lost signal threshold, then set the signal expiration duration time in minutes or seconds in the Signal is lost after field. Choose what you want to happen when the signal is lost. You can check one or both of Close all current open violations and Open new \"lost signal\" violation. These control how loss of signal violations will be handled for the condition. Make sure you name your condition before you save it. Violations open due to loss of signal close when the signal comes back. Newly opened lost signal violations will close immediately when new data is evaluated. the condition they belong to expires. By default, conditions expire after 3 days. you manually close the violation with the Close all current open violations option. Tip Loss of signal detection doesn't work on NRQL queries that use nested aggregation or sub-queries. Advanced signal settings When creating a NRQL alert condition, use the advanced signal settings to control streaming alert data and avoid false alarms. When creating a NRQL condition, there are several advanced signal settings: Aggregation window duration Delay/timer Fill data gaps To read an explanation of what these settings are and how they relate to each other, see Streaming alerts concepts. Below are instructions and tips on how to configure them. Aggregation window duration You can set the aggregation window duration to choose how long data is accumulated in a streaming time window before it's aggregated. You can set it to anything between 30 seconds and 15 minutes. The default is one minute. Tip Baseline alert condition thresholds don't support editing the aggregation window. They use the 1 minute default. Delay/timer You can adjust the delay/timer to coordinate our streaming alerting algorithm with your data's behavior. If your data is sparse or inconsistent, you may want to use the event timer aggregation method. For the cadence method, the total supported latency is the sum of the aggregation window duration and the delay. If the data type comes from an APM language agent and is aggregated from many app instances (for example, Transactions, TransactionErrors, etc.), we recommend using the event flow method with the default settings. Important When creating NRQL conditions for data collected from Infrastructure Cloud Integrations such as AWS Cloudwatch or Azure, we recommend that you use the event timer method. Fill data gaps Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with one of these settings: None: (Default) Choose this if you don't want to take any action on empty aggregation windows. On evaluation, an empty aggregation window will reset the threshold duration timer. For example, if a condition says that all aggregation windows must have data points above the threshold for 5 minutes, and 1 of the 5 aggregation windows is empty, then the condition won't be in violation. Custom static value: Choose this if you'd like to insert a custom static value into the empty aggregation windows before they're evaluated. This option has an additional, required parameter of fillValue (as named in the API) that specifies what static value should be used. This defaults to 0. Last known value: This option inserts the last seen value before evaluation occurs. We maintain the state of the last seen value for 2 hours. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss, gap filling, and how to request access to these features, see this Explorers Hub post. Options for editing data gap settings: In the NRQL conditions UI, go to Condition settings > Advanced signal settings > fill data gaps with and choose an option. If using our Nerdgraph API (preferred), this node is located at: actor : account : alerts : nrqlCondition : signal : fillOption | fillValue NerdGraph is our recommended API for this but if you're using our REST API, you can find this setting in the REST API explorer under the \"signal\" section of the Alert NRQL conditions API.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 173.86136,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create NRQL <em>alert</em> conditions",
        "sections": "Nested aggregation NRQL <em>alerts</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": " a NRQL <em>alert</em> condition for a policy: On one.newrelic.com, in the header click <em>Alerts</em> &amp; AI, then in the left sidebar click Policies. Select an existing policy or click <em>New</em> <em>alert</em> policy to create a <em>new</em> policy. Click Add a condition. Under Select a product click NRQL, and then click Next, define"
      },
      "id": "603ef04864441fbc114e8883"
    },
    {
      "sections": [
        "Alerts and applied intelligence notification integrations",
        "Early access",
        "Integration details",
        "Atlassian Jira",
        "Permissions",
        "Set up a Jira destination",
        "Important",
        "Two-way sync",
        "Configure the message template",
        "Send a test notification",
        "ServiceNow (Incident-Management)",
        "Roles",
        "Set up a destination",
        "Slack",
        "Prerequisites",
        "Set up a Slack destination",
        "Configure the Slack message settings",
        "Webhook",
        "Set up a webhook destination",
        "Configure the webhook event template",
        "Customize the webhook payload",
        "Email",
        "Configure the email settings"
      ],
      "title": "Alerts and applied intelligence notification integrations",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "Applied intelligence",
        "Incident Intelligence",
        "Destinations"
      ],
      "external_id": "7220c630fc187bb61784ff2cc2213e588b269b00",
      "image": "https://docs.newrelic.com/static/d4e9baecc3a76dd1a5945f8ec0aeca66/c1b63/webhook-notification-template.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/notifications/notification-integrations/",
      "published_at": "2021-12-19T15:28:40Z",
      "updated_at": "2021-12-19T15:28:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Early access The features described here are early access. You won't be able to use these features if you're not part of the early access program. For more information on related features, see our docs on Alerts notification channels, Incident Intelligence destinations, and Proactive Detection notifications. Alerts and Applied Intelligence notification integrations are specific services and platforms you can use to send notifications from New Relic. Integration details Read more about each of our specific notification integrations. Atlassian Jira Integrate New Relic with Atlassian Jira(Cloud) and automatically create and update Jira issues. Permissions The required permissions from the Jira API-Token are create, edit, and close tickets. To enable the two-way sync toggle, the provided Jira API-Key should have an Admin role. Set up a Jira destination Create Jira issues, then enable Jira and New Relic to share updates and stay synced. To create a Jira destination, enter the following information: Destination name: Custom name to identify the destination Jira account endpoint: the URL of the destination User-name: this will be the email address of the user making the connection API token: generated from your Atlassian account Important New Relic currently supports Atlassian-Jira Classic (company-managed) projects. Before saving the destination, we recommend you test the connection via the test connection button. Jira destination configuration. We recommand to test the connection before saving. Two-way sync You can enable a two-way integration with Jira to keep the issues' state synced with the corresponding state in New Relic. To enable two-way sync, turn on the ‘two-way integration’ toggle. When turned on, a Jira Webhook would be created in your Jira account at a later stage, for the selected project (see ‘customize a message template’). The webhook would contain access details to Newrelic (URL and Newrelic-API-KEY) Configure the message template To configure a template for a Jira issue, you first need to choose a destination. You will be able to create a new destination at this stage. Upon successful connection to the destination, you will need to choose a project, and then select the Jira issue type you would like to be used. Once the issue-type is selected, the configured project's fields are fetched from your account and automatically mapped to your Jira instance. To help you get started, we automatically present the required and recommended fields and values. Required fields must be set with a value. You can add or remove optional fields(use the X mark on their right side) Jira message template. Send a test notification You can see how the JIRA issue will appear by clicking a test notification with default field values. If successful, a JIRA issue will be created and a link will appear. ServiceNow (Incident-Management) Integrate New Relic with ServiceNow Incident-Management and automatically create and update incidents. Roles As part of the integrations, we fetch fields from the your serviceNow incident table and optional values. For this, the provided ServiceNow user details required read permissions for the tables: sys_dictionary, sys_choice, sys_user and task. A read/write permission to incident To be able to fetch users for the caller column, we required read permissions for the sys_users table. The above permissions can be achieved with the roles personalize_choices, personalize_dictionary, rest_service, itil. Read/Write permissions to the api_key_credentials table is required to enable two-way integration. This can be covered with the roles credentials_admin and discovery_admin. Set up a destination To create a ServiceNow destination, enter the following information: Destination Name: custom name to identify the destination Domain: the URL of the destination User-name: the name of the user Password: the user name’s password Before saving the destination, we recommend testing the connection by clicking the test connection button. Two-way sync You can configure a two-way integration with ServiceNow Incidents Management to keep the incidents' state synced with the corresponding state in New Relic. Here are some required steps to remember when configuring the two-way integration: Turn on the two-way integration toggle. Open and download this XML file, which includes the business rule triggering events back to New Relic One. In the ServiceNow sidebar menu, go to System Definition > Business Rules. Click the menu icon in one of the column headers, select Import XML and upload the XML file you downloaded. Once the Destination is saved, a New-Relic API-Key will be kept in the api_key_credentials. The key would sent in a header as part of the callback REST call to New-Relic Configure the message template Upon a successful connection, ServiceNow incident table columns are fetched from your account and automatically mapped to your ServiceNow instance. To help you get started, we automatically present the required and recommended fields and values. Required fields must be set with a value. You can add or remove optional fields(use the X mark on their right side) Select, edit or remove fields for the ServiceNow-Incident template. Send a test notification You can see how the ServiceNow incident will appear by clicking a test notification with default field values. If successful, an incident will be created and a link will appear. Slack Send notifications-messages to your Slack channels. Prerequisites Your Slack workspace needs to have the New Relic application installed. The application must be approved by a workspace admin before it can be individually installed by users Set up a Slack destination Click on the `one-click Slack authentication' will lead you to the Slack landing page to continue the OAuth2 authentication process. On the Slack landing page, if you're not signed into the required workspace, you're redirected to Slack to sign in. Add your workspace name or select the relevant workspace and click Continue. When signed in to the selected workspace, you are requested to allow New Relic to perform the specified actions. Clicking `Allow' will redirect you back to the Destination page. Configure the Slack message settings Select a Destination(Workspace) and select a Slack-channel where the messages will be sent. You can create a new destination if there is no pre-defined destination for the required workspace. Note that, for privacy reasons, users need to be authenticated to select private channels (one-time process) Send a test notification You can send a test notification with a pre-defined example payload to the channel. This creates a message in the selected Slack-channel. Webhook Use the webhook notifier to send the notification messages to any endpoint you like. Set up a webhook destination To create a webhook destination, you need the following: Destination Name: A unique destination name URL: the endpoint of the target application, authentication and custom headers if needed. Authorization mechanism (Optional):. Can be basic authentication or a bearer token Configure the webhook event template Pick a webhook destination from the list and configure the HTTP-POST request. The request configuration requires you to: Set a name for the template. Select a pre-configured destination from the destinations list or create a new one. Add custom headers (optional). Configure the request’s payload. Customize the webhook payload You can use the default payload or customize it to contain the required data. Pick Variables from the variables menu and apply handlebars syntax to enrich your webhook. Note that the request’s content-type is JSON by default. Hence, the payload needs to keep the JSON form. See Usage examples The ‘preview’ section on the right hand-side shows an expected payload after the template is rendered. If the eventual payload would not form a valid Json, an error will be shown and it won’t be possible to save the template. If the webhook payload conforms a valid Json, you can send a test notification to your defined webhook destination We recommend sending a test notification to make sure that everything's connected correctly. Email Send email notifications to users. Configure the email settings Add one or more recipients. Users with New Relic accounts can be found via autocomplete when searching for their name or email address. To add a user without a New Relic account or email distribution list, add the full email address. Every recipient will be translated into a 'destination'. You can follow the email notifications per destination in the notifications-log Send a test notification You can send a test notification to make sure the email notifications reach the inbox.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 163.52827,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Alerts</em> <em>and</em> <em>applied</em> <em>intelligence</em> notification integrations",
        "sections": "<em>Alerts</em> <em>and</em> <em>applied</em> <em>intelligence</em> notification integrations",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": " notifications. <em>Alerts</em> and <em>Applied</em> <em>Intelligence</em> notification integrations are specific services and platforms you can use to send notifications from <em>New</em> <em>Relic</em>. Integration details Read more about each of our specific notification integrations. Atlassian Jira Integrate <em>New</em> <em>Relic</em> with Atlassian Jira(Cloud"
      },
      "id": "618ff71628ccbc60710321e4"
    },
    {
      "sections": [
        "Choose your aggregation method",
        "What's aggregation?",
        "Why it matters",
        "When to use event flow",
        "How event flow works",
        "Caution",
        "Event flow use cases",
        "When to use event timer",
        "How event timer works",
        "Event timer use cases",
        "Cadence",
        "Aggregation and loss of signal"
      ],
      "title": "Choose your aggregation method",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts"
      ],
      "external_id": "9bdcf28f192d61c2348909803e1c75876168936d",
      "image": "https://docs.newrelic.com/static/64df85a9f5694079b9ad1557de7fe5b5/c1b63/signal-consistency.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/get-started/choose-your-aggregation-method/",
      "published_at": "2021-12-19T15:02:53Z",
      "updated_at": "2021-12-04T15:31:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Alerts conditions provide a sophisticated set of tools for describing when you want to be notified about something that's happened or failed to happen on something you're monitoring. For best results, choose the aggregation method that best matches the way your data arrives. The three aggregation methods are event flow, event timer, and cadence. If you're interested in a conceptual overview, see our doc on streaming alerts, key terms and concepts. What's aggregation? When an application or service is monitored by New Relic, data can arrive in different ways. Some data arrives consistently and predictably, while other data arrives inconsistently and sporadically. Aggregation is how our alerting system gathers data together before analyzing it for exceeding warning or critical threshold levels. Your data is collected as data points in an aggregation window and then turned into a single numeric value. The data points are aggregated based on your NRQL query using methods like sum, average, min, and max, among others. This single numeric value is what's used to evaluate the condition's threshold. Once data has been aggregated, no more data points can be added to it. Our different aggregation methods will help you strike a balance between aggregating your data quickly and waiting for enough data points to arrive. Why it matters With the correct aggregation method, you're more likely to get notifications you care about, while preventing ones you don't. The most important questions to consider when deciding on your aggregation method: How often does my data arrive? How consistently does my data arrive? When data arrives frequently and consistently in a linear way, we recommend using event flow. When data arrives sporadically, inconsistently, and out of order, we recommend using event timer aggregation. When to use event flow With event flow, data is aggregated based on data point timestamps, so it's important that data points arrive in a consistent and linear manner. This aggregation method doesn't work as well for data point timestamps that arrive out of order or with a wide span of time within a short period. Event flow is the default aggregation method, because it applies for the most common use cases. How event flow works Event flow uses data point timestamps to determine when to open and close an aggregation window. For example, if you're using event flow with a 2 minute delay window, then an aggregation window will close when a timestamp arrives that's two minutes later than the last timestamp that was received. A data point with a 12:00pm timestamp arrives. An aggregation window opens. At some point, a 12:03pm data point arrives. Event flow closes the window, excluding the 12:03 data point, and evaluates that closed window against your thresholds. The event flow aggregation window will continue collecting data points until that later timestamp. The later timestamps are what moves the system forward, not the data points themselves. Event flow will wait as long as necessary for the next data point later than your delay setting to arrive, before aggregating the data. Event flow works best for data that arrives frequently and consistently. Caution If you expect your data points to arrive more than 30 minutes apart, please use the event timer method described below. Event flow use cases Here are some typical event flow use cases: APM agent data. Infrastructure agent data. Any data coming from a 3rd party that comes in frequently and reliably. Most AWS Cloudwatch metrics coming from the AWS Metric Stream (NOT polling). The main exception is that some AWS Cloudwatch data is very infrequent (like S3 volume data) regardless of whether it's streaming or polling and, in that case, you'd use Event timer. When to use event timer Event timer aggregation is based on a timer that counts down when a data point arrives. The timer resets every time a new data point arrives. If the timer counts down before a new data point arrives, event timer aggregates all of the data points received during that time. Event timer is best for alerting on events that happen sporadically and with large gaps of time. How event timer works Errors are a type of event that happens sporadically, unpredictably, and often with large gaps of time. For example, you might have a condition with a query that returns a count of errors. Many minutes may go by without any errors at all, and then suddenly 5 errors arrive within a minute. In this example, event timer would do nothing until the first of the 5 errors arrive. Then it would start the timer, resetting it each time a new error arrives. If the timer countdown reaches 0 without a new error, event timer aggregates the data, and evaluates it against your threshold. Event timer use cases Here are some typical event timer use cases: New Relic usage data. Cloud integration data that is being polled (such as with GCP, Azure, or AWS polling methods). Queries that deliver sparse or infrequent data, such as error counts. Cadence Cadence is our original aggregation method. It aggregates data on specific time intervals as detected by New Relic's internal wall clock, regardless of data timestamps. We recommend that you use one of our other aggregation methods instead. If you're currently using cadence, choose one of the other aggregation methods. Event flow is best for consistent, predictable data points. Event timer is best for inconsistent, sporadic data points. Aggregation and loss of signal Our loss of signal system runs separately from these aggregation methods and settings. If you set your alert condition to open a new violation when your signal is lost for 10 minutes, a loss of signal service watches for data points to arrive. If a new data point fails to arrive within 10 minutes, loss of signal causes a violation to open. For more information on when to use gap filling and loss single, see our forum post on when to use gap filling and loss of signal.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 161.83618,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Aggregation <em>and</em> loss of signal",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": " methods are event flow, event timer, and cadence. If you&#x27;re interested in a conceptual overview, see our doc on streaming <em>alerts</em>, key terms and concepts. What&#x27;s aggregation? When an application or service is monitored by <em>New</em> <em>Relic</em>, data can arrive in different ways. Some data arrives consistently"
      },
      "id": "61853bba28ccbc5a1b7fe94c"
    }
  ],
  "/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/view-entity-health-status-find-entities-without-alert-conditions": [
    {
      "sections": [
        "Create NRQL alert conditions",
        "Tip",
        "Create a NRQL alert condition",
        "Create a condition from a chart",
        "Caution",
        "NRQL alert syntax",
        "Important",
        "Reformatting incompatible NRQL",
        "NRQL alert threshold examples",
        "Alert on specific segments of your data",
        "Alert on Nth percentile of your data",
        "Alert on max, min, avg of your data",
        "Alert on a percentage of your data",
        "Alert on Apdex with any T-value",
        "NRQL conditions and query order of operations",
        "Example: null value returned",
        "Example: zero value returned",
        "Nested aggregation NRQL alerts",
        "Nested queries with a non-faceted innermost query are not currently supported",
        "Queries at all levels must have the same aggregation window size",
        "Signal loss is not yet supported for nested queries",
        "Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported",
        "NRQL condition creation tips",
        "Condition edits can reset condition evaluation",
        "Alert condition types",
        "Sum of query results (limited or intermittent data)",
        "Set the loss of signal threshold",
        "Advanced signal settings",
        "Aggregation window duration",
        "Delay/timer",
        "Fill data gaps"
      ],
      "title": "Create NRQL alert conditions",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert conditions"
      ],
      "external_id": "2872f770a5fc8a802f99b9f848906f7e351ad761",
      "image": "https://docs.newrelic.com/static/eb8e1b2d826f9fd9cf46fe3dd5455217/c1b63/nr1_nrql_alert_conditions.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/",
      "published_at": "2021-12-19T16:01:17Z",
      "updated_at": "2021-12-14T22:49:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use NRQL queries to create alert conditions. Once you've defined your signal, you can further define your warning and critical threshold levels. This determines when an alerts violation is created. Read on to learn more about how to do this. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Click NRQL, and then Next, define thresholds. Tip For more information on key concepts relating to NRQL alert conditions and streaming alerts, see Streaming alerts: key terms and concepts. Create a NRQL alert condition To create a NRQL alert condition for a policy: On one.newrelic.com, in the header click Alerts & AI, then in the left sidebar click Policies. Select an existing policy or click New alert policy to create a new policy. Click Add a condition. Under Select a product click NRQL, and then click Next, define thresholds. Note that editing an existing condition can result in resetting its evaluation. Create a condition from a chart You can use a chart to create a NRQL alert condition. To create a NRQL alerts condition from a chart, click the chart menu , then click Create alert condition. Once you've named and customized your condition, you can add it to an existing policy or create a new one. Caution A small number of our older charts don't include the option to create an alert condition. NRQL alert syntax Here's the basic syntax for creating all NRQL alert conditions. The FACET clause is required for outlier condition types. It's optional for static and baseline. SELECT function(attribute) FROM Event WHERE attribute [comparison] [AND|OR ...] Copy Clause Notes SELECT function(attribute) Required Supported functions that return numbers include: apdex average count latest max min percentage percentile sum uniqueCount Tip If you use the percentile aggregator in a faceted alert condition with many facets, this may cause the following error to appear: An error occurred while fetching chart data. If you see this error, use average instead. FROM data type Required Only one data type can be targeted. Supported data types: Event Metric (RAW data points will be returned) WHERE attribute [comparison] [AND|OR ...] Use the WHERE clause to specify a series of one or more conditions. All the operators are supported. FACET attribute Required for outlier conditions Include an optional FACET clause in your NRQL syntax depending on the threshold type: static or baseline. Use the FACET clause to separate your results by attribute and alert on each attribute independently. No LIMIT clause is allowed, but all queries will receive the maximum number of facets possible. Faceted queries can return a maximum of 5000 values for static and baseline conditions and a maximum of 500 values for outlier conditions. Important If the query returns more than the maximum number of values, the alert condition can't be created. If you create the condition and the query returns more than this number later, the alert will fail. Modify your query so that it returns a fewer number of values. Reformatting incompatible NRQL Some elements of NRQL used in charts don’t make sense in the streaming context of alerts. Here’s a list of the most common incompatible elements and suggestions for reformatting a NRQL alert query to achieve the same effect. Element Notes SINCE and UNTIL Example: SELECT percentile(largestContentfulPaint, 75) FROM PageViewTiming WHERE (appId = 837807) SINCE yesterday Copy NRQL conditions produce a never-ending stream of windowed query results, so the SINCE and UNTIL keywords to scope the query to a point in time are not compatible. As a convenience, we automatically strip SINCE and UNTIL from a query when creating a condition from the context of a chart. TIMESERIES In NRQL queries, the TIMESERIES clause is used to return data as a time series broken out by a specified period of time. For NRQL conditions, the equivalent property of a signal is the aggregation duration window. histogram() The histogram() aggregation function is used to generate histograms. histogram() is not compatible with NRQL alerting: histogram aggregations can not be formatted as a time series. To create an alert from a portion of a histogram (for example, 95th percentile), use the percentile() aggregation function. Multiple aggregation functions Each condition can only target a single aggregated value. To alert on multiple values simultaneously, you’ll need to decompose them into individual conditions within the same policy. Original Query: SELECT count(foo), average(bar), max(baz) from Transaction Copy Decomposed: SELECT count(foo) from Transaction SELECT average(bar) from Transaction SELECT max(baz) from Transaction Copy COMPARE WITH The COMPARE WITH clause is used to compare the values for two different time ranges. This type of query is incompatible with NRQL alerting. We recommend using a Baseline Alert Condition to dynamically detect deviations for a particular signal. SLIDE BY The SLIDE BY clause supports a feature known as sliding windows. With sliding windows, SLIDE BY data is gathered into \"windows\" of time that overlap with each other. These windows can help to smooth out line graphs with a lot of variation in cases where the rolling aggregate (such as a rolling mean) is more important than aggregates from narrow windows of time. Sliding windows are not currently supported in NRQL alerts. LIMIT In NRQL queries, the LIMIT clause is used to control the amount of data a query returns, either the maximum number of facet values returned by FACET queries or the maximum number of items returned by SELECT * queries. LIMIT is not compatible with NRQL alerting: evaluation is always performed on the full result set. NRQL alert threshold examples Here are some common use cases for NRQL conditions. These queries will work for static and baseline condition types. The outlier condition type will require additional FACET clauses. Alert on specific segments of your data Create constrained alerts that target a specific segment of your data, such as a few key customers or a range of data. Use the WHERE clause to define those conditions. SELECT average(duration) FROM Transaction WHERE account_id in (91290, 102021, 20230) Copy SELECT percentile(duration, 95) FROM Transaction WHERE name LIKE 'Controller/checkout/%' Copy Alert on Nth percentile of your data Create alerts when an Nth percentile of your data hits a specified threshold; for example, maintaining SLA service levels. Since we evaluate the NRQL query based on the aggregation window duration, percentiles will be calculated for each duration separately. SELECT percentile(duration, 95) FROM Transaction Copy SELECT percentile(databaseDuration, 75) FROM Transaction Copy Alert on max, min, avg of your data Create alerts when your data hits a certain maximum, minimum, or average; for example, ensuring that a duration or response time does not pass a certain threshold. SELECT max(duration) FROM Transaction Copy SELECT average(duration) FROM Transaction Copy Alert on a percentage of your data Create alerts when a proportion of your data goes above or below a certain threshold. SELECT percentage(count(*), WHERE duration > 2) FROM Transaction Copy SELECT percentage(count(*), WHERE httpResponseCode = '500') FROM Transaction Copy Alert on Apdex with any T-value Create alerts on Apdex, applying your own T-value for certain transactions. For example, get an alert notification when your Apdex for a T-value of 500ms on transactions for production apps goes below 0.8. SELECT apdex(duration, t:0.5) FROM Transaction WHERE appName like '%prod%' Copy NRQL conditions and query order of operations By default, the aggregation window duration is 1 minute, but you can change the window to suit your needs. Whatever the aggregation window, New Relic will collect data for that window using the function in the NRQL condition’s query. The query is parsed and executed by our systems in the following order: FROM clause – which event type needs to be grabbed? WHERE clause – what can be filtered out? SELECT clause – what information needs to be returned from the now-filtered data set? Example: null value returned Let's say this is your alert condition query: SELECT count(*) FROM SyntheticCheck WHERE monitorName = 'My Cool Monitor' AND result = 'FAILURE' Copy If there are no failures for the aggregation window: The system will execute the FROM clause by grabbing all SyntheticCheck events on your account. Then it will execute the WHERE clause to filter through those events by looking only for the ones that match the monitor name and result specified. If there are still events left to scan through after completing the FROM and WHERE operations, the SELECT clause will be executed. If there are no remaining events, the SELECT clause will not be executed. This means that aggregators like count() and uniqueCount() will never return a zero value. When there is a count of 0, the SELECT clause is ignored and no data is returned, resulting in a value of NULL. Example: zero value returned If you have a data source delivering legitimate numeric zeroes, the query will return zero values and not null values. Let's say this is your alert condition query, and that MyCoolEvent is an attribute that can sometimes return a zero value. SELECT average(MyCoolAttribute) FROM MyCoolEvent Copy If, in the aggregation window being evaluated, there's at least one instance of MyCoolEvent and if the average value of all MyCoolAttribute attributes from that window is equal to zero, then a 0 value will be returned. If there are no MyCoolEvent events during that minute, then a NULL will be returned due to the order of operations. Tip For more information about this topic, you can check out our blog post on troubleshooting for zero versus null values. Tip You can determine how null values will be handled by adjusting loss of signal and gap filling settings in the Alert Conditions UI. Tip You can avoid NULL values entirely with a query order of operations shortcut. Do this by using a filter sub-clause, then including all filter elements within that sub-clause. The main body of the query will run and return data, at which point the SELECT clause will then run and apply the filter elements. The query will return a value of 0 if the filter elements result in no matching data. Here's an example: SELECT filter(count(*), WHERE result = 'SUCCESS' AND monitorName = 'My Favorite Monitor') FROM SyntheticCheck Copy Nested aggregation NRQL alerts Nested aggregation queries are a powerful way to query your data. However, they have a few restrictions that are important to note. Nested queries with a non-faceted innermost query are not currently supported Without a FACET, the inner query produces a single result, giving the outer query nothing to aggregate. If you're using a nested query, make sure your inner query is faceted. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu) ​​​​​ Copy Queries at all levels must have the same aggregation window size With an alert aggregation window of 1 minute, the inner query would produce two smaller windows of 30 seconds. In theory, these two windows could be aggregated by the outer query. However, this is not currently supported. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu TIMESERIES 30 seconds)​​ Copy Signal loss is not yet supported for nested queries For more information on signal loss, see NerdGraph API: Loss of signal and gap filling. Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported You can't use a nested query containing the WITH METRIC_FORMAT in the inner query to create NRQL alert conditions. NRQL condition creation tips Here are some tips for creating and using a NRQL condition: Topic Tips Condition types NRQL condition types include static, baseline, and outlier. Create a description For NRQL conditions, you can create a custom description to add to each violation. Descriptions can be enhanced with variable substitution based on metadata in the specific violation. For details, see Description Query results Queries must return a number. The condition evaluates the returned number against the thresholds you've set. Time period NRQL conditions evaluate data based on how it's aggregated, using aggregation windows from 30 seconds to 15 minutes, in increments of 15 seconds. For best results, we recommend using the event flow or event timer aggregation methods. For the cadence aggregation method, the implicit SINCE ... UNTIL clause specifying which minute to evaluate is controlled by your delay/timer setting. Since very recent data may be incomplete, you may want to query data from 3 minutes ago or longer, especially for: Applications that run on multiple hosts. SyntheticCheck data: Timeouts can take 3 minutes, so 5 minutes or more is recommended. Also, if a query will generate intermittent data, consider using the sum of query results option. Lost signal threshold (loss of signal detection) You can use loss of signal detection to alert on when your data (a telemetry signal) should be considered lost. A signal loss can indicate that a service or entity is no longer online or that a periodic job failed to run. You can also use this to make sure that violations for sporadic data, such as error counts, are closed when no signal is coming in. Advanced signal settings These settings give you options for better handling continuous, streaming data signals that may sometimes be missing. These settings include the aggregation window duration, the delay/timer, and an option for filling data gaps. For more on using these, see Advanced signal settings. Condition settings Use the Condition settings to: Create a concise, descriptive condition name. Provide a custom violation description for the condition that will be included in violations and notifications. Add the runbook URL to include your organization's procedures for handling incidents. You may also add this information to the custom violation description. Limits on conditions See the maximum values. Health status In order for a NRQL alert condition health status display to function properly, use a FACET clause to scope each signal to a single entity (for example, FACET hostname or FACET appname). Examples For more information, see: Expected NRQL syntax Examples of NRQL condition queries Condition edits can reset condition evaluation When you edit NRQL alert conditions in some specific ways (detailed below), their evaluations are reset, meaning that any evaluation up until that point is lost, and the evaluation starts over from that point. The two ways this will affect you are: For \"for at least x minutes\" thresholds: because the evaluation window has been reset, there will be a delay of at least x minutes before any violations can be reported. For baseline conditions: the condition starts over again and all baseline learning is lost. The following actions cause an evaluation reset for NRQL conditions: Changing the query Changing the aggregation window, aggregation method, or aggregation delay/timer setting Changing the \"close violations on signal loss\" setting Changing any gap fill settings Changing the baseline direction (if applicable) – higher, lower, or higher/lower Change the threshold value, threshold window, or threshold operator The following actions (along with any other actions not covered in the above list) will not reset the evaluation: Changing the loss of signal time window (expiration duration) Changing the time function (switching \"for at least\" to \"at least once in,\" or vice-versa) Toggling the \"open violation on signal loss\" setting Alert condition types When you create a NRQL alert, you can choose from different types of conditions: NRQL alert condition types Description Static This is the simplest type of NRQL condition. It allows you to create a condition based on a NRQL query that returns a numeric value. Optional: Include a FACET clause. Baseline (Dynamic) Uses a self-adjusting condition based on the past behavior of the monitored values. Uses the same NRQL query form as the static type, including the optional FACET clause. Outlier Looks for group behavior and values that are outliers from those groups. Uses the same NRQL query form as the static type, but requires a FACET clause. Sum of query results (limited or intermittent data) Important Available only for static (basic) condition types. If a query returns intermittent or limited data, it may be difficult to set a meaningful threshold. Missing or limited data will sometimes generate false positives or false negatives. You can use loss of signal, aggregation duration, and gap filling settings to minimize these false notifications. To avoid this problem when using the static threshold type, you can set the selector to sum of query results. This lets you set the alert on an aggregated sum instead of a value from a single harvest cycle. Up to two hours of one-minute data checks can be aggregated. The duration you select determines the width of the rolling sum and the preview chart will update accordingly. Set the loss of signal threshold Loss of signal occurs when no data matches the NRQL condition over a specific period of time. You can set your loss of signal threshold duration and also what happens when the threshold is crossed. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Loss of signal is only available for NRQL conditions. You may also manage these settings using the GraphQL API (recommended), or the REST API. Go here for specific GraphQL API examples. Loss of signal settings: Loss of signal settings include a time duration and two possible actions. Signal loss expiration time UI label: Signal is lost after: GraphQL Node: expiration.expirationDuration Expiration duration is a timer that starts and resets when we receive a data point in the streaming alerts pipeline. If we don't receive another data point before your 'expiration time' expires, we consider that signal to be lost. This can be because no data is being sent to New Relic or the WHERE clause of your NRQL query is filtering that data out before it is streamed to the alerts pipeline. Note that when you have a faceted query, each facet is a signal. So if any one of those signals ends during the duration specified, that will be considered a loss of signal. The loss of signal expiration time is independent of the threshold duration and triggers as soon as the timer expires. The maximum expiration duration is 48 hours. This is helpful when monitoring for the execution of infrequent jobs. The minimum is 30 seconds, but we recommend using at least 3-5 minutes. Loss of signal actions Once a signal is considered lost, you can close open violations, open new violations, or both. Close all current open violations: This closes all open violations that are related to a specific signal. It won't necessarily close all violations for a condition. If you're alerting on an ephemeral service, or on a sporadic signal, you'll want to choose this action to ensure that violations are closed properly. The GraphQL node name for this is \"closeViolationsOnExpiration\" Open new violations: This will open a new violation when the signal is considered lost. These violations will indicate that they are due to a loss of signal. Based on your incident preferences, this should trigger a notification. The graphQL node name for this is \"openViolationOnExpiration\" When you enable both actions, we'll close all open violations first, and then open a new violation for loss of signal. To create a NRQL alert configured with loss of signal detection in the UI: For a policy, when you create a condition, under Select a product, click NRQL, then click Next, define thresholds. Write a NRQL query that returns the values you want to alert on. For Threshold type, select Static or Baseline. Click + Add lost signal threshold, then set the signal expiration duration time in minutes or seconds in the Signal is lost after field. Choose what you want to happen when the signal is lost. You can check one or both of Close all current open violations and Open new \"lost signal\" violation. These control how loss of signal violations will be handled for the condition. Make sure you name your condition before you save it. Violations open due to loss of signal close when the signal comes back. Newly opened lost signal violations will close immediately when new data is evaluated. the condition they belong to expires. By default, conditions expire after 3 days. you manually close the violation with the Close all current open violations option. Tip Loss of signal detection doesn't work on NRQL queries that use nested aggregation or sub-queries. Advanced signal settings When creating a NRQL alert condition, use the advanced signal settings to control streaming alert data and avoid false alarms. When creating a NRQL condition, there are several advanced signal settings: Aggregation window duration Delay/timer Fill data gaps To read an explanation of what these settings are and how they relate to each other, see Streaming alerts concepts. Below are instructions and tips on how to configure them. Aggregation window duration You can set the aggregation window duration to choose how long data is accumulated in a streaming time window before it's aggregated. You can set it to anything between 30 seconds and 15 minutes. The default is one minute. Tip Baseline alert condition thresholds don't support editing the aggregation window. They use the 1 minute default. Delay/timer You can adjust the delay/timer to coordinate our streaming alerting algorithm with your data's behavior. If your data is sparse or inconsistent, you may want to use the event timer aggregation method. For the cadence method, the total supported latency is the sum of the aggregation window duration and the delay. If the data type comes from an APM language agent and is aggregated from many app instances (for example, Transactions, TransactionErrors, etc.), we recommend using the event flow method with the default settings. Important When creating NRQL conditions for data collected from Infrastructure Cloud Integrations such as AWS Cloudwatch or Azure, we recommend that you use the event timer method. Fill data gaps Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with one of these settings: None: (Default) Choose this if you don't want to take any action on empty aggregation windows. On evaluation, an empty aggregation window will reset the threshold duration timer. For example, if a condition says that all aggregation windows must have data points above the threshold for 5 minutes, and 1 of the 5 aggregation windows is empty, then the condition won't be in violation. Custom static value: Choose this if you'd like to insert a custom static value into the empty aggregation windows before they're evaluated. This option has an additional, required parameter of fillValue (as named in the API) that specifies what static value should be used. This defaults to 0. Last known value: This option inserts the last seen value before evaluation occurs. We maintain the state of the last seen value for 2 hours. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss, gap filling, and how to request access to these features, see this Explorers Hub post. Options for editing data gap settings: In the NRQL conditions UI, go to Condition settings > Advanced signal settings > fill data gaps with and choose an option. If using our Nerdgraph API (preferred), this node is located at: actor : account : alerts : nrqlCondition : signal : fillOption | fillValue NerdGraph is our recommended API for this but if you're using our REST API, you can find this setting in the REST API explorer under the \"signal\" section of the Alert NRQL conditions API.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 251.85518,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create NRQL <em>alert</em> <em>conditions</em>",
        "sections": "Create NRQL <em>alert</em> <em>conditions</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "You can use NRQL queries to create <em>alert</em> <em>conditions</em>. Once you&#x27;ve defined your signal, you can further define your warning and critical threshold levels. This determines when an <em>alerts</em> violation is created. Read on to learn more about how to do this. Go to one.newrelic.com, click <em>Alerts</em> &amp; AI"
      },
      "id": "603ef04864441fbc114e8883"
    },
    {
      "sections": [
        "Streaming alerts: key terms and concepts",
        "Why it matters",
        "Streaming alerts process and descriptions",
        "Choose your aggregation method",
        "Event flow",
        "Caution",
        "Event timer",
        "Cadence",
        "Streaming alerts tools",
        "Tip",
        "Window duration",
        "Delay/timer",
        "Loss of signal detection",
        "Gap filling"
      ],
      "title": "Streaming alerts: key terms and concepts",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Get started"
      ],
      "external_id": "144c7f0721ec2acb1e8ac8aa06142911ea552aef",
      "image": "https://docs.newrelic.com/static/39158bd84483adf9516bb7ff5058c9bd/d30ee/streaming-alerts.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/streaming-alerts-key-terms-concepts/",
      "published_at": "2021-12-15T12:47:58Z",
      "updated_at": "2021-12-15T12:47:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The streaming alerts platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into New Relic. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that's processed by the streaming algorithm. There are three methods for aggregating the data filtered through your NRQL condition: Event flow (default) Event timer Cadence This short video describes the three aggregation method (5:31). Why it matters Understanding how streaming alerts works will help you fine-tune your NRQL conditions to be notified about what's important to you. Only data that matches the conditions of the NRQL WHERE clause is alerted on. For more details on each step of the process, see Streaming alerts process and descriptions. As data streams into New Relic, it's filtered by the NRQL condition. Before data is evaluated, it must meet the criteria defined by the NRQL query's WHERE clause. Instead of evaluating that data immediately for violations, the NRQL alert conditions collect the data over a period of time known as the aggregation window. An additional delay/timer allows for slower data points to arrive before the window is aggregated. Once the delay/timer time has elapsed, New Relic aggregates the data into a single data point. Alerts then evaluates the data point using the NRQL condition in order to determine whether it meets the violation threshold criteria. Even if a data point meets the criteria for a violation, a violation may not be triggered. A violation is only triggered when data points consistently meet the threshold criteria over a period of time. This is the threshold duration. If the data points are in violation for an entire threshold duration, we'll send you a notification based on your policy settings. All of these configurable delays give you more control over how you're alerted on sporadic and missing data. Streaming alerts process and descriptions Process Description Streaming data All data coming into New Relic. WHERE clause Filters all incoming streaming data. We only monitor for alerts on data that makes it through this filter. Aggregation methods One of three methods that control how data is collected before it's evaluated. They are: Event flow (Default) Event timer Cadence Aggregation window Data with timestamps that fall within this window will be aggregated and then evaluated. Delay/timer A time delay to ensure all data points have arrived in the aggregation window before aggregation occurs. Aggregated data Data in the aggregated window is collapsed to a single data point for alert evaluation. Evaluation The data point is evaluated by the NRQL condition, which is trigged by each incoming aggregated data point. Threshold duration A specific duration that determines if a violation is created. If your specified NRQL condition meets the threshold criteria over the threshold duration, a violation occurs. When a data point lacks data, a custom value is inserted to fill the gap. Choose your aggregation method You can choose between three different aggregation methods, depending on your needs. Event flow (default) works best for data that comes in frequently and mostly in order. Event timer works best for data that arrives infrequently in batches, such as cloud integration data or infrequent error logs. Cadence is our original and inferior aggregation method. It aggregates data on specific time intervals as detected by New Relic's internal wall clock, regardless of data timestamps. Event flow Event flow aggregates a window of data when the first data point arrives in a subsequent window. The custom delay defines which subsequent window data will start to populate to trigger aggregation of the current window. A custom delay provides extra time for data to arrive. These times are based on the data's timestamps and not New Relic's wall clock time. For example, suppose you're monitoring CPU usage in window durations of 1 minute and a 3 minute delay. When a CPU usage data point comes in with a timestamp between 12:00pm and 12:01pm, event flow will not aggregate that window until a data point shows up with a timestamp between 12:04pm and 12:05pm. When event flow receives the first data point with a timestamp of 12:04pm or later, it sends the 12:00 to 12:01 data to be aggregated. Caution If you expect your data points to arrive more than 30 minutes apart, please use the Event Timer method described below. Event timer Like event flow, event timer will only aggregate data for a given window when data arrives for that window. When a data point arrives for an aggregation window, a timer dedicated to that window starts to count down. If no further data arrives before the timer counts down, the data for that window is aggregated. If more data points arrive before the timer has completed counting down, the timer is reset. For example, suppose you're monitoring CloudWatch data that arrives fairly infrequently. You're using a window duration of 1 minute and a 3 minute timer. When a CloudWatch data point comes in with a timestamp between 12:00pm and 12:01pm, the timer will start to count down. If no further data points show up for that 12:00-12:01 window, the window will get aggregated 3 minutes later. If a new data point with a timestamp between 12:00 and 12:01 arrives, the timer resets. It keeps resetting every time more data points for that window arrive. The window will not be sent for aggregation until the timer reaches 0. If the timer for a later data point elapses before an earlier data point, the event timer method waits for the earlier timer to elapse before aggregating the later data point. For best results, make sure your timer is equal to or longer than your window duration time. If the timer is less than your window duration and your data flow is inconsistent, then your data may be evaluated before all of your data points arrive. This could cause you to be notified incorrectly. Cadence We recommend you use one of the other two methods. Cadence is our old streaming aggregation method. This method uses New Relic's wall clock time to determine when data is aggregated and evaluated. It doesn't take into account data point timestamps as they arrive. Streaming alerts tools Streaming alerts provide a set of tools to give you greater control over how your data is aggregated before it's evaluated to reduce incorrect notifications you receive. They are: Window duration Delay/timer Loss of signal detection Gap filling Tip This article covers these tools at a conceptual level. You'll find direct instructions on how to use these tools in Create NRQL alert conditions. Window duration In order to make loss of signal detection more effective and to reduce unnecessary notifications, you can customize aggregation windows to the duration that you need. An aggregation window is a specific block of time. We gather data points together in an aggregation window, before evaluating the data. A longer aggregation window can smooth out the data, since an outlier data point will have more data points to be aggregated with, giving it less of an influence on the aggregated data point that is sent for evaluation. When a data point arrives, its timestamp is used to put it in the proper aggregation window. You can set your aggregation window to anything between 30 seconds and 15 minutes. The default is 1 minute. Delay/timer The delay/timer setting controls how long the condition should wait before aggregating the data in the aggregation window. The event flow and cadence methods use delay. Event timer uses timer. The delay default is 2 minutes. The timer default is 1 minute and has a minimum value of 5 seconds. Loss of signal detection Loss of signal occurs when no data matches the NRQL condition over a specific period of time. A loss of signal is caused by different things. The WHERE clause in your NRQL query can filter out data before it's evaluated for violations. It could also mean a service or entity is offline or a periodic job has failed to run and no data is being sent to New Relic. In order to avoid unnecessary notifications, you can choose how long to wait before you're notified by a loss of signal violation. You can use loss of signal detection to open violations and be notified when a signal is lost. Alternately, you can use a loss of signal to close violations for ephemeral services or sporadic data, such as error counts. Gap filling Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with the last value received, a static value, or else do nothing and leave the gap there. The default is None. Gaps in streaming data can be caused by network or host issues, a signal may be sparse, or some signals, such as error counts, may only have data when something is wrong. By filling the gaps with known values, the alert evaluation process can process those gaps and determine how they should affect the loss of signal evaluation. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss and gap filling, see this Explorers Hub post.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 229.4548,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "sections": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "The streaming <em>alerts</em> platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into <em>New</em> <em>Relic</em>. You can use NRQL <em>conditions</em> to control what part of the signal you want to be notified about. Your NRQL condition filters the data that&#x27;s processed"
      },
      "id": "6130c054e7b9d269edb6f21d"
    },
    {
      "sections": [
        "Outlier detection (NRQL alert)",
        "Important",
        "What is outlier detection?",
        "Tip",
        "Example use cases",
        "Notify if load-balanced servers have uneven workload",
        "Notify if load-balanced application has misbehaving instances",
        "Notify of changes in different environments",
        "Notify for timezone-related changes",
        "Create an outlier alert condition",
        "Rules and logic",
        "Details about alert condition logic",
        "NRQL query rules and limits",
        "Zero values for unreturned data"
      ],
      "title": "Outlier detection (NRQL alert)",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert conditions"
      ],
      "external_id": "499fa55abd48a0ccdd897fbdf64ccea2d9f98d11",
      "image": "https://docs.newrelic.com/static/f235d0630576bc2010ff07adc7a69621/f73a1/NRQL_outlier_violations.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/outlier-detection-nrql-alert/",
      "published_at": "2021-12-15T12:44:32Z",
      "updated_at": "2021-11-25T19:50:19Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Alerts offers NRQL conditions in three threshold types: static, baseline, and outlier. This document explains how the outlier threshold type works, gives some example use cases and NRQL queries, and explains how to create an outlier condition. Important NRQL alerts do not affect Alerts policies for a Synthetic monitor. For example, muting a NRQL alert will not mute a Synthetic monitor's alerts. What is outlier detection? In software development and operations, it is common to have a group consisting of members you expect to behave approximately the same. For example: for servers using a load balancer, the traffic to the servers may go up or down, but the traffic for all the servers should remain in a fairly tight grouping. See outlier detection in action in this NerdBytes video (2:51 minutes). The NRQL alert outlier detection feature parses the data returned by your faceted NRQL query and: Looks for the number of expected groups that you specify Looks for outliers (values deviating from a group) based on the sensitivity and time range you set Additionally, for queries that have more than one group, you can choose to be notified when groups start behaving the same. This visual aid will help you understand the types of situations that will trigger a violation and those that won't. For more on the rules and logic behind this calculation, see Outlier detection rules. Tip Note: this feature does not take into account the past behavior of the monitored values; it looks for outliers only in the currently reported data. For an alert type that takes into account past behavior, see Baseline alerting. Example use cases These use cases will help you understand when to use the outlier threshold type. Note that the outlier feature requires a NRQL query with a FACET clause. Notify if load-balanced servers have uneven workload A load balancer divides web traffic approximately evenly across five different servers. You can set a notification to be sent if any server starts getting significantly more or less traffic than the other servers. Example query: SELECT average(cpuPercent) FROM SystemSample WHERE apmApplicationNames = 'MY-APP-NAME' FACET hostname Copy Notify if load-balanced application has misbehaving instances Application instances behind a load balancer should have similar throughput, error rates, and response times. If an instance is in a bad state, or a load balancer is misconfigured, this will not be the case. Detecting one or two bad app instances using aggregate metrics may be difficult if there is not a significant rise in the overall error rate of the application. You can set a notification for when an app instance’s throughput, error rate, or response time deviates too far from the rest of the group. Example query: SELECT average(duration) FROM Transaction WHERE appName = 'MY-APP-NAME' FACET host Copy Notify of changes in different environments An application is deployed in two different environments, with ten application instances in each. One environment is experimental and gets more errors than the other. But the instances that are in the same environment should get approximately the same number of errors. You can set a notification for when an instance starts getting more errors than the other instances in the same environment. Also, you can set a notification for when the two environments start to have the same number of errors as each other. Notify for timezone-related changes The number of logged in users for a company is about the same for each of four applications, but varies significantly by each of the three time zones the company operates in. You can set a notification for when any application starts getting more or less traffic from a certain timezone than the other applications. Sometimes the traffic from the different time zones are the same, so you would set up the alert condition to not be notified if the time zone groups overlap. For more details on how this feature works, see Outlier rules and logic. Create an outlier alert condition To create a NRQL alert that uses outlier detection: When creating a condition, under Select a product, select NRQL. For Threshold type, select Outlier. Create a NRQL query with a FACET clause that returns the values you want to alert on. Depending on how the returned values group together, set the Number of expected groups. Adjust the deviation from the center of the group(s) and the duration that will trigger a violation. Optional: Add a warning threshold and set its deviation. Set any remaining available options and save. Rules and logic Here are the rules and logic behind how outlier detection works: Details about alert condition logic After the condition is created, the query is run once every harvest cycle and the condition is applied. Unlike baseline alerts, outlier detection uses no historical data in its calculation; it's calculated using the currently collected data. Alerts will attempt to divide the data returned from the query into the number of groups selected during condition creation. For each group, the approximate average value is calculated. The allowable deviation you have chosen when creating the condition is centered around that average value. If a member of the group is outside the allowed deviation, it produces a violation. If Trigger when groups overlap has been selected, Alerts detects a convergence of groups. If the condition is looking for two or more groups, and the returned values cannot be separated into that number of distinct groups, then that will produce a violation. This type of “overlap” event is represented on a chart by group bands touching. Because this feature does not take past behavior into account, data is never considered to \"belong\" to a certain group. For example, a value that switches places with another value wouldn't trigger a violation. Additionally, an entire group that moves together also wouldn't trigger a violation. NRQL query rules and limits The NRQL query must be a faceted query. The number of unique values returned must be 500 or less. If the query returns more than this number of values, the condition won't be created. If the query later returns more than this number after being created, the alert will fail. Zero values for unreturned data When a query returns a set of values, only values that are actually returned are taken into account. If a value is not available for calculation (including if it goes from being collected one harvest cycle to not being collected), it is rendered as a zero and is not considered. In other words, the behavior of unreturned zero values will never trigger violations.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.94452,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Outlier detection (NRQL <em>alert</em>)",
        "sections": "Create an outlier <em>alert</em> <em>condition</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "<em>Alerts</em> offers NRQL <em>conditions</em> in three threshold types: static, baseline, and outlier. This document explains how the outlier threshold type works, gives some example use cases and NRQL queries, and explains how to create an outlier condition. Important NRQL <em>alerts</em> do not affect <em>Alerts</em> policies"
      },
      "id": "6130be72196a6793654948e7"
    }
  ],
  "/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/violation-event-attributes": [
    {
      "sections": [
        "Streaming alerts: key terms and concepts",
        "Why it matters",
        "Streaming alerts process and descriptions",
        "Choose your aggregation method",
        "Event flow",
        "Caution",
        "Event timer",
        "Cadence",
        "Streaming alerts tools",
        "Tip",
        "Window duration",
        "Delay/timer",
        "Loss of signal detection",
        "Gap filling"
      ],
      "title": "Streaming alerts: key terms and concepts",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Get started"
      ],
      "external_id": "144c7f0721ec2acb1e8ac8aa06142911ea552aef",
      "image": "https://docs.newrelic.com/static/39158bd84483adf9516bb7ff5058c9bd/d30ee/streaming-alerts.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/streaming-alerts-key-terms-concepts/",
      "published_at": "2021-12-15T12:47:58Z",
      "updated_at": "2021-12-15T12:47:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The streaming alerts platform checks for violations based on data that’s present or missing in your stream of data, or signal, coming into New Relic. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that's processed by the streaming algorithm. There are three methods for aggregating the data filtered through your NRQL condition: Event flow (default) Event timer Cadence This short video describes the three aggregation method (5:31). Why it matters Understanding how streaming alerts works will help you fine-tune your NRQL conditions to be notified about what's important to you. Only data that matches the conditions of the NRQL WHERE clause is alerted on. For more details on each step of the process, see Streaming alerts process and descriptions. As data streams into New Relic, it's filtered by the NRQL condition. Before data is evaluated, it must meet the criteria defined by the NRQL query's WHERE clause. Instead of evaluating that data immediately for violations, the NRQL alert conditions collect the data over a period of time known as the aggregation window. An additional delay/timer allows for slower data points to arrive before the window is aggregated. Once the delay/timer time has elapsed, New Relic aggregates the data into a single data point. Alerts then evaluates the data point using the NRQL condition in order to determine whether it meets the violation threshold criteria. Even if a data point meets the criteria for a violation, a violation may not be triggered. A violation is only triggered when data points consistently meet the threshold criteria over a period of time. This is the threshold duration. If the data points are in violation for an entire threshold duration, we'll send you a notification based on your policy settings. All of these configurable delays give you more control over how you're alerted on sporadic and missing data. Streaming alerts process and descriptions Process Description Streaming data All data coming into New Relic. WHERE clause Filters all incoming streaming data. We only monitor for alerts on data that makes it through this filter. Aggregation methods One of three methods that control how data is collected before it's evaluated. They are: Event flow (Default) Event timer Cadence Aggregation window Data with timestamps that fall within this window will be aggregated and then evaluated. Delay/timer A time delay to ensure all data points have arrived in the aggregation window before aggregation occurs. Aggregated data Data in the aggregated window is collapsed to a single data point for alert evaluation. Evaluation The data point is evaluated by the NRQL condition, which is trigged by each incoming aggregated data point. Threshold duration A specific duration that determines if a violation is created. If your specified NRQL condition meets the threshold criteria over the threshold duration, a violation occurs. When a data point lacks data, a custom value is inserted to fill the gap. Choose your aggregation method You can choose between three different aggregation methods, depending on your needs. Event flow (default) works best for data that comes in frequently and mostly in order. Event timer works best for data that arrives infrequently in batches, such as cloud integration data or infrequent error logs. Cadence is our original and inferior aggregation method. It aggregates data on specific time intervals as detected by New Relic's internal wall clock, regardless of data timestamps. Event flow Event flow aggregates a window of data when the first data point arrives in a subsequent window. The custom delay defines which subsequent window data will start to populate to trigger aggregation of the current window. A custom delay provides extra time for data to arrive. These times are based on the data's timestamps and not New Relic's wall clock time. For example, suppose you're monitoring CPU usage in window durations of 1 minute and a 3 minute delay. When a CPU usage data point comes in with a timestamp between 12:00pm and 12:01pm, event flow will not aggregate that window until a data point shows up with a timestamp between 12:04pm and 12:05pm. When event flow receives the first data point with a timestamp of 12:04pm or later, it sends the 12:00 to 12:01 data to be aggregated. Caution If you expect your data points to arrive more than 30 minutes apart, please use the Event Timer method described below. Event timer Like event flow, event timer will only aggregate data for a given window when data arrives for that window. When a data point arrives for an aggregation window, a timer dedicated to that window starts to count down. If no further data arrives before the timer counts down, the data for that window is aggregated. If more data points arrive before the timer has completed counting down, the timer is reset. For example, suppose you're monitoring CloudWatch data that arrives fairly infrequently. You're using a window duration of 1 minute and a 3 minute timer. When a CloudWatch data point comes in with a timestamp between 12:00pm and 12:01pm, the timer will start to count down. If no further data points show up for that 12:00-12:01 window, the window will get aggregated 3 minutes later. If a new data point with a timestamp between 12:00 and 12:01 arrives, the timer resets. It keeps resetting every time more data points for that window arrive. The window will not be sent for aggregation until the timer reaches 0. If the timer for a later data point elapses before an earlier data point, the event timer method waits for the earlier timer to elapse before aggregating the later data point. For best results, make sure your timer is equal to or longer than your window duration time. If the timer is less than your window duration and your data flow is inconsistent, then your data may be evaluated before all of your data points arrive. This could cause you to be notified incorrectly. Cadence We recommend you use one of the other two methods. Cadence is our old streaming aggregation method. This method uses New Relic's wall clock time to determine when data is aggregated and evaluated. It doesn't take into account data point timestamps as they arrive. Streaming alerts tools Streaming alerts provide a set of tools to give you greater control over how your data is aggregated before it's evaluated to reduce incorrect notifications you receive. They are: Window duration Delay/timer Loss of signal detection Gap filling Tip This article covers these tools at a conceptual level. You'll find direct instructions on how to use these tools in Create NRQL alert conditions. Window duration In order to make loss of signal detection more effective and to reduce unnecessary notifications, you can customize aggregation windows to the duration that you need. An aggregation window is a specific block of time. We gather data points together in an aggregation window, before evaluating the data. A longer aggregation window can smooth out the data, since an outlier data point will have more data points to be aggregated with, giving it less of an influence on the aggregated data point that is sent for evaluation. When a data point arrives, its timestamp is used to put it in the proper aggregation window. You can set your aggregation window to anything between 30 seconds and 15 minutes. The default is 1 minute. Delay/timer The delay/timer setting controls how long the condition should wait before aggregating the data in the aggregation window. The event flow and cadence methods use delay. Event timer uses timer. The delay default is 2 minutes. The timer default is 1 minute and has a minimum value of 5 seconds. Loss of signal detection Loss of signal occurs when no data matches the NRQL condition over a specific period of time. A loss of signal is caused by different things. The WHERE clause in your NRQL query can filter out data before it's evaluated for violations. It could also mean a service or entity is offline or a periodic job has failed to run and no data is being sent to New Relic. In order to avoid unnecessary notifications, you can choose how long to wait before you're notified by a loss of signal violation. You can use loss of signal detection to open violations and be notified when a signal is lost. Alternately, you can use a loss of signal to close violations for ephemeral services or sporadic data, such as error counts. Gap filling Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with the last value received, a static value, or else do nothing and leave the gap there. The default is None. Gaps in streaming data can be caused by network or host issues, a signal may be sparse, or some signals, such as error counts, may only have data when something is wrong. By filling the gaps with known values, the alert evaluation process can process those gaps and determine how they should affect the loss of signal evaluation. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss and gap filling, see this Explorers Hub post.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.07256,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "sections": "Streaming <em>alerts</em>: key terms <em>and</em> concepts",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": "The streaming <em>alerts</em> platform checks for <em>violations</em> based on data that’s present or missing in your stream of data, or signal, coming into <em>New</em> <em>Relic</em>. You can use NRQL conditions to control what part of the signal you want to be notified about. Your NRQL condition filters the data that&#x27;s processed"
      },
      "id": "6130c054e7b9d269edb6f21d"
    },
    {
      "sections": [
        "Create NRQL alert conditions",
        "Tip",
        "Create a NRQL alert condition",
        "Create a condition from a chart",
        "Caution",
        "NRQL alert syntax",
        "Important",
        "Reformatting incompatible NRQL",
        "NRQL alert threshold examples",
        "Alert on specific segments of your data",
        "Alert on Nth percentile of your data",
        "Alert on max, min, avg of your data",
        "Alert on a percentage of your data",
        "Alert on Apdex with any T-value",
        "NRQL conditions and query order of operations",
        "Example: null value returned",
        "Example: zero value returned",
        "Nested aggregation NRQL alerts",
        "Nested queries with a non-faceted innermost query are not currently supported",
        "Queries at all levels must have the same aggregation window size",
        "Signal loss is not yet supported for nested queries",
        "Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported",
        "NRQL condition creation tips",
        "Condition edits can reset condition evaluation",
        "Alert condition types",
        "Sum of query results (limited or intermittent data)",
        "Set the loss of signal threshold",
        "Advanced signal settings",
        "Aggregation window duration",
        "Delay/timer",
        "Fill data gaps"
      ],
      "title": "Create NRQL alert conditions",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts",
        "Alert conditions"
      ],
      "external_id": "2872f770a5fc8a802f99b9f848906f7e351ad761",
      "image": "https://docs.newrelic.com/static/eb8e1b2d826f9fd9cf46fe3dd5455217/c1b63/nr1_nrql_alert_conditions.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/",
      "published_at": "2021-12-19T16:01:17Z",
      "updated_at": "2021-12-14T22:49:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use NRQL queries to create alert conditions. Once you've defined your signal, you can further define your warning and critical threshold levels. This determines when an alerts violation is created. Read on to learn more about how to do this. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Click NRQL, and then Next, define thresholds. Tip For more information on key concepts relating to NRQL alert conditions and streaming alerts, see Streaming alerts: key terms and concepts. Create a NRQL alert condition To create a NRQL alert condition for a policy: On one.newrelic.com, in the header click Alerts & AI, then in the left sidebar click Policies. Select an existing policy or click New alert policy to create a new policy. Click Add a condition. Under Select a product click NRQL, and then click Next, define thresholds. Note that editing an existing condition can result in resetting its evaluation. Create a condition from a chart You can use a chart to create a NRQL alert condition. To create a NRQL alerts condition from a chart, click the chart menu , then click Create alert condition. Once you've named and customized your condition, you can add it to an existing policy or create a new one. Caution A small number of our older charts don't include the option to create an alert condition. NRQL alert syntax Here's the basic syntax for creating all NRQL alert conditions. The FACET clause is required for outlier condition types. It's optional for static and baseline. SELECT function(attribute) FROM Event WHERE attribute [comparison] [AND|OR ...] Copy Clause Notes SELECT function(attribute) Required Supported functions that return numbers include: apdex average count latest max min percentage percentile sum uniqueCount Tip If you use the percentile aggregator in a faceted alert condition with many facets, this may cause the following error to appear: An error occurred while fetching chart data. If you see this error, use average instead. FROM data type Required Only one data type can be targeted. Supported data types: Event Metric (RAW data points will be returned) WHERE attribute [comparison] [AND|OR ...] Use the WHERE clause to specify a series of one or more conditions. All the operators are supported. FACET attribute Required for outlier conditions Include an optional FACET clause in your NRQL syntax depending on the threshold type: static or baseline. Use the FACET clause to separate your results by attribute and alert on each attribute independently. No LIMIT clause is allowed, but all queries will receive the maximum number of facets possible. Faceted queries can return a maximum of 5000 values for static and baseline conditions and a maximum of 500 values for outlier conditions. Important If the query returns more than the maximum number of values, the alert condition can't be created. If you create the condition and the query returns more than this number later, the alert will fail. Modify your query so that it returns a fewer number of values. Reformatting incompatible NRQL Some elements of NRQL used in charts don’t make sense in the streaming context of alerts. Here’s a list of the most common incompatible elements and suggestions for reformatting a NRQL alert query to achieve the same effect. Element Notes SINCE and UNTIL Example: SELECT percentile(largestContentfulPaint, 75) FROM PageViewTiming WHERE (appId = 837807) SINCE yesterday Copy NRQL conditions produce a never-ending stream of windowed query results, so the SINCE and UNTIL keywords to scope the query to a point in time are not compatible. As a convenience, we automatically strip SINCE and UNTIL from a query when creating a condition from the context of a chart. TIMESERIES In NRQL queries, the TIMESERIES clause is used to return data as a time series broken out by a specified period of time. For NRQL conditions, the equivalent property of a signal is the aggregation duration window. histogram() The histogram() aggregation function is used to generate histograms. histogram() is not compatible with NRQL alerting: histogram aggregations can not be formatted as a time series. To create an alert from a portion of a histogram (for example, 95th percentile), use the percentile() aggregation function. Multiple aggregation functions Each condition can only target a single aggregated value. To alert on multiple values simultaneously, you’ll need to decompose them into individual conditions within the same policy. Original Query: SELECT count(foo), average(bar), max(baz) from Transaction Copy Decomposed: SELECT count(foo) from Transaction SELECT average(bar) from Transaction SELECT max(baz) from Transaction Copy COMPARE WITH The COMPARE WITH clause is used to compare the values for two different time ranges. This type of query is incompatible with NRQL alerting. We recommend using a Baseline Alert Condition to dynamically detect deviations for a particular signal. SLIDE BY The SLIDE BY clause supports a feature known as sliding windows. With sliding windows, SLIDE BY data is gathered into \"windows\" of time that overlap with each other. These windows can help to smooth out line graphs with a lot of variation in cases where the rolling aggregate (such as a rolling mean) is more important than aggregates from narrow windows of time. Sliding windows are not currently supported in NRQL alerts. LIMIT In NRQL queries, the LIMIT clause is used to control the amount of data a query returns, either the maximum number of facet values returned by FACET queries or the maximum number of items returned by SELECT * queries. LIMIT is not compatible with NRQL alerting: evaluation is always performed on the full result set. NRQL alert threshold examples Here are some common use cases for NRQL conditions. These queries will work for static and baseline condition types. The outlier condition type will require additional FACET clauses. Alert on specific segments of your data Create constrained alerts that target a specific segment of your data, such as a few key customers or a range of data. Use the WHERE clause to define those conditions. SELECT average(duration) FROM Transaction WHERE account_id in (91290, 102021, 20230) Copy SELECT percentile(duration, 95) FROM Transaction WHERE name LIKE 'Controller/checkout/%' Copy Alert on Nth percentile of your data Create alerts when an Nth percentile of your data hits a specified threshold; for example, maintaining SLA service levels. Since we evaluate the NRQL query based on the aggregation window duration, percentiles will be calculated for each duration separately. SELECT percentile(duration, 95) FROM Transaction Copy SELECT percentile(databaseDuration, 75) FROM Transaction Copy Alert on max, min, avg of your data Create alerts when your data hits a certain maximum, minimum, or average; for example, ensuring that a duration or response time does not pass a certain threshold. SELECT max(duration) FROM Transaction Copy SELECT average(duration) FROM Transaction Copy Alert on a percentage of your data Create alerts when a proportion of your data goes above or below a certain threshold. SELECT percentage(count(*), WHERE duration > 2) FROM Transaction Copy SELECT percentage(count(*), WHERE httpResponseCode = '500') FROM Transaction Copy Alert on Apdex with any T-value Create alerts on Apdex, applying your own T-value for certain transactions. For example, get an alert notification when your Apdex for a T-value of 500ms on transactions for production apps goes below 0.8. SELECT apdex(duration, t:0.5) FROM Transaction WHERE appName like '%prod%' Copy NRQL conditions and query order of operations By default, the aggregation window duration is 1 minute, but you can change the window to suit your needs. Whatever the aggregation window, New Relic will collect data for that window using the function in the NRQL condition’s query. The query is parsed and executed by our systems in the following order: FROM clause – which event type needs to be grabbed? WHERE clause – what can be filtered out? SELECT clause – what information needs to be returned from the now-filtered data set? Example: null value returned Let's say this is your alert condition query: SELECT count(*) FROM SyntheticCheck WHERE monitorName = 'My Cool Monitor' AND result = 'FAILURE' Copy If there are no failures for the aggregation window: The system will execute the FROM clause by grabbing all SyntheticCheck events on your account. Then it will execute the WHERE clause to filter through those events by looking only for the ones that match the monitor name and result specified. If there are still events left to scan through after completing the FROM and WHERE operations, the SELECT clause will be executed. If there are no remaining events, the SELECT clause will not be executed. This means that aggregators like count() and uniqueCount() will never return a zero value. When there is a count of 0, the SELECT clause is ignored and no data is returned, resulting in a value of NULL. Example: zero value returned If you have a data source delivering legitimate numeric zeroes, the query will return zero values and not null values. Let's say this is your alert condition query, and that MyCoolEvent is an attribute that can sometimes return a zero value. SELECT average(MyCoolAttribute) FROM MyCoolEvent Copy If, in the aggregation window being evaluated, there's at least one instance of MyCoolEvent and if the average value of all MyCoolAttribute attributes from that window is equal to zero, then a 0 value will be returned. If there are no MyCoolEvent events during that minute, then a NULL will be returned due to the order of operations. Tip For more information about this topic, you can check out our blog post on troubleshooting for zero versus null values. Tip You can determine how null values will be handled by adjusting loss of signal and gap filling settings in the Alert Conditions UI. Tip You can avoid NULL values entirely with a query order of operations shortcut. Do this by using a filter sub-clause, then including all filter elements within that sub-clause. The main body of the query will run and return data, at which point the SELECT clause will then run and apply the filter elements. The query will return a value of 0 if the filter elements result in no matching data. Here's an example: SELECT filter(count(*), WHERE result = 'SUCCESS' AND monitorName = 'My Favorite Monitor') FROM SyntheticCheck Copy Nested aggregation NRQL alerts Nested aggregation queries are a powerful way to query your data. However, they have a few restrictions that are important to note. Nested queries with a non-faceted innermost query are not currently supported Without a FACET, the inner query produces a single result, giving the outer query nothing to aggregate. If you're using a nested query, make sure your inner query is faceted. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu) ​​​​​ Copy Queries at all levels must have the same aggregation window size With an alert aggregation window of 1 minute, the inner query would produce two smaller windows of 30 seconds. In theory, these two windows could be aggregated by the outer query. However, this is not currently supported. SELECT max(cpu) FROM (FROM Event SELECT min(cpuTime) as cpu TIMESERIES 30 seconds)​​ Copy Signal loss is not yet supported for nested queries For more information on signal loss, see NerdGraph API: Loss of signal and gap filling. Nested queries containing 'WITH METRIC_FORMAT' in the inner query are not currently supported You can't use a nested query containing the WITH METRIC_FORMAT in the inner query to create NRQL alert conditions. NRQL condition creation tips Here are some tips for creating and using a NRQL condition: Topic Tips Condition types NRQL condition types include static, baseline, and outlier. Create a description For NRQL conditions, you can create a custom description to add to each violation. Descriptions can be enhanced with variable substitution based on metadata in the specific violation. For details, see Description Query results Queries must return a number. The condition evaluates the returned number against the thresholds you've set. Time period NRQL conditions evaluate data based on how it's aggregated, using aggregation windows from 30 seconds to 15 minutes, in increments of 15 seconds. For best results, we recommend using the event flow or event timer aggregation methods. For the cadence aggregation method, the implicit SINCE ... UNTIL clause specifying which minute to evaluate is controlled by your delay/timer setting. Since very recent data may be incomplete, you may want to query data from 3 minutes ago or longer, especially for: Applications that run on multiple hosts. SyntheticCheck data: Timeouts can take 3 minutes, so 5 minutes or more is recommended. Also, if a query will generate intermittent data, consider using the sum of query results option. Lost signal threshold (loss of signal detection) You can use loss of signal detection to alert on when your data (a telemetry signal) should be considered lost. A signal loss can indicate that a service or entity is no longer online or that a periodic job failed to run. You can also use this to make sure that violations for sporadic data, such as error counts, are closed when no signal is coming in. Advanced signal settings These settings give you options for better handling continuous, streaming data signals that may sometimes be missing. These settings include the aggregation window duration, the delay/timer, and an option for filling data gaps. For more on using these, see Advanced signal settings. Condition settings Use the Condition settings to: Create a concise, descriptive condition name. Provide a custom violation description for the condition that will be included in violations and notifications. Add the runbook URL to include your organization's procedures for handling incidents. You may also add this information to the custom violation description. Limits on conditions See the maximum values. Health status In order for a NRQL alert condition health status display to function properly, use a FACET clause to scope each signal to a single entity (for example, FACET hostname or FACET appname). Examples For more information, see: Expected NRQL syntax Examples of NRQL condition queries Condition edits can reset condition evaluation When you edit NRQL alert conditions in some specific ways (detailed below), their evaluations are reset, meaning that any evaluation up until that point is lost, and the evaluation starts over from that point. The two ways this will affect you are: For \"for at least x minutes\" thresholds: because the evaluation window has been reset, there will be a delay of at least x minutes before any violations can be reported. For baseline conditions: the condition starts over again and all baseline learning is lost. The following actions cause an evaluation reset for NRQL conditions: Changing the query Changing the aggregation window, aggregation method, or aggregation delay/timer setting Changing the \"close violations on signal loss\" setting Changing any gap fill settings Changing the baseline direction (if applicable) – higher, lower, or higher/lower Change the threshold value, threshold window, or threshold operator The following actions (along with any other actions not covered in the above list) will not reset the evaluation: Changing the loss of signal time window (expiration duration) Changing the time function (switching \"for at least\" to \"at least once in,\" or vice-versa) Toggling the \"open violation on signal loss\" setting Alert condition types When you create a NRQL alert, you can choose from different types of conditions: NRQL alert condition types Description Static This is the simplest type of NRQL condition. It allows you to create a condition based on a NRQL query that returns a numeric value. Optional: Include a FACET clause. Baseline (Dynamic) Uses a self-adjusting condition based on the past behavior of the monitored values. Uses the same NRQL query form as the static type, including the optional FACET clause. Outlier Looks for group behavior and values that are outliers from those groups. Uses the same NRQL query form as the static type, but requires a FACET clause. Sum of query results (limited or intermittent data) Important Available only for static (basic) condition types. If a query returns intermittent or limited data, it may be difficult to set a meaningful threshold. Missing or limited data will sometimes generate false positives or false negatives. You can use loss of signal, aggregation duration, and gap filling settings to minimize these false notifications. To avoid this problem when using the static threshold type, you can set the selector to sum of query results. This lets you set the alert on an aggregated sum instead of a value from a single harvest cycle. Up to two hours of one-minute data checks can be aggregated. The duration you select determines the width of the rolling sum and the preview chart will update accordingly. Set the loss of signal threshold Loss of signal occurs when no data matches the NRQL condition over a specific period of time. You can set your loss of signal threshold duration and also what happens when the threshold is crossed. Go to one.newrelic.com, click Alerts & AI, in the left sidebar click Policies, select a policy, then Add a condition. Loss of signal is only available for NRQL conditions. You may also manage these settings using the GraphQL API (recommended), or the REST API. Go here for specific GraphQL API examples. Loss of signal settings: Loss of signal settings include a time duration and two possible actions. Signal loss expiration time UI label: Signal is lost after: GraphQL Node: expiration.expirationDuration Expiration duration is a timer that starts and resets when we receive a data point in the streaming alerts pipeline. If we don't receive another data point before your 'expiration time' expires, we consider that signal to be lost. This can be because no data is being sent to New Relic or the WHERE clause of your NRQL query is filtering that data out before it is streamed to the alerts pipeline. Note that when you have a faceted query, each facet is a signal. So if any one of those signals ends during the duration specified, that will be considered a loss of signal. The loss of signal expiration time is independent of the threshold duration and triggers as soon as the timer expires. The maximum expiration duration is 48 hours. This is helpful when monitoring for the execution of infrequent jobs. The minimum is 30 seconds, but we recommend using at least 3-5 minutes. Loss of signal actions Once a signal is considered lost, you can close open violations, open new violations, or both. Close all current open violations: This closes all open violations that are related to a specific signal. It won't necessarily close all violations for a condition. If you're alerting on an ephemeral service, or on a sporadic signal, you'll want to choose this action to ensure that violations are closed properly. The GraphQL node name for this is \"closeViolationsOnExpiration\" Open new violations: This will open a new violation when the signal is considered lost. These violations will indicate that they are due to a loss of signal. Based on your incident preferences, this should trigger a notification. The graphQL node name for this is \"openViolationOnExpiration\" When you enable both actions, we'll close all open violations first, and then open a new violation for loss of signal. To create a NRQL alert configured with loss of signal detection in the UI: For a policy, when you create a condition, under Select a product, click NRQL, then click Next, define thresholds. Write a NRQL query that returns the values you want to alert on. For Threshold type, select Static or Baseline. Click + Add lost signal threshold, then set the signal expiration duration time in minutes or seconds in the Signal is lost after field. Choose what you want to happen when the signal is lost. You can check one or both of Close all current open violations and Open new \"lost signal\" violation. These control how loss of signal violations will be handled for the condition. Make sure you name your condition before you save it. Violations open due to loss of signal close when the signal comes back. Newly opened lost signal violations will close immediately when new data is evaluated. the condition they belong to expires. By default, conditions expire after 3 days. you manually close the violation with the Close all current open violations option. Tip Loss of signal detection doesn't work on NRQL queries that use nested aggregation or sub-queries. Advanced signal settings When creating a NRQL alert condition, use the advanced signal settings to control streaming alert data and avoid false alarms. When creating a NRQL condition, there are several advanced signal settings: Aggregation window duration Delay/timer Fill data gaps To read an explanation of what these settings are and how they relate to each other, see Streaming alerts concepts. Below are instructions and tips on how to configure them. Aggregation window duration You can set the aggregation window duration to choose how long data is accumulated in a streaming time window before it's aggregated. You can set it to anything between 30 seconds and 15 minutes. The default is one minute. Tip Baseline alert condition thresholds don't support editing the aggregation window. They use the 1 minute default. Delay/timer You can adjust the delay/timer to coordinate our streaming alerting algorithm with your data's behavior. If your data is sparse or inconsistent, you may want to use the event timer aggregation method. For the cadence method, the total supported latency is the sum of the aggregation window duration and the delay. If the data type comes from an APM language agent and is aggregated from many app instances (for example, Transactions, TransactionErrors, etc.), we recommend using the event flow method with the default settings. Important When creating NRQL conditions for data collected from Infrastructure Cloud Integrations such as AWS Cloudwatch or Azure, we recommend that you use the event timer method. Fill data gaps Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with one of these settings: None: (Default) Choose this if you don't want to take any action on empty aggregation windows. On evaluation, an empty aggregation window will reset the threshold duration timer. For example, if a condition says that all aggregation windows must have data points above the threshold for 5 minutes, and 1 of the 5 aggregation windows is empty, then the condition won't be in violation. Custom static value: Choose this if you'd like to insert a custom static value into the empty aggregation windows before they're evaluated. This option has an additional, required parameter of fillValue (as named in the API) that specifies what static value should be used. This defaults to 0. Last known value: This option inserts the last seen value before evaluation occurs. We maintain the state of the last seen value for 2 hours. Tip The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals. To learn more about signal loss, gap filling, and how to request access to these features, see this Explorers Hub post. Options for editing data gap settings: In the NRQL conditions UI, go to Condition settings > Advanced signal settings > fill data gaps with and choose an option. If using our Nerdgraph API (preferred), this node is located at: actor : account : alerts : nrqlCondition : signal : fillOption | fillValue NerdGraph is our recommended API for this but if you're using our REST API, you can find this setting in the REST API explorer under the \"signal\" section of the Alert NRQL conditions API.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.0083,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create NRQL <em>alert</em> conditions",
        "sections": "Nested aggregation NRQL <em>alerts</em>",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": " a NRQL <em>alert</em> condition for a policy: On one.newrelic.com, in the header click <em>Alerts</em> &amp; AI, then in the left sidebar click Policies. Select an existing policy or click <em>New</em> <em>alert</em> policy to create a <em>new</em> policy. Click Add a condition. Under Select a product click NRQL, and then click Next, define"
      },
      "id": "603ef04864441fbc114e8883"
    },
    {
      "sections": [
        "Choose your aggregation method",
        "What's aggregation?",
        "Why it matters",
        "When to use event flow",
        "How event flow works",
        "Caution",
        "Event flow use cases",
        "When to use event timer",
        "How event timer works",
        "Event timer use cases",
        "Cadence",
        "Aggregation and loss of signal"
      ],
      "title": "Choose your aggregation method",
      "type": "docs",
      "tags": [
        "Alerts and Applied Intelligence",
        "New Relic Alerts"
      ],
      "external_id": "9bdcf28f192d61c2348909803e1c75876168936d",
      "image": "https://docs.newrelic.com/static/64df85a9f5694079b9ad1557de7fe5b5/c1b63/signal-consistency.png",
      "url": "https://docs.newrelic.com/docs/alerts-applied-intelligence/new-relic-alerts/get-started/choose-your-aggregation-method/",
      "published_at": "2021-12-19T15:02:53Z",
      "updated_at": "2021-12-04T15:31:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Alerts conditions provide a sophisticated set of tools for describing when you want to be notified about something that's happened or failed to happen on something you're monitoring. For best results, choose the aggregation method that best matches the way your data arrives. The three aggregation methods are event flow, event timer, and cadence. If you're interested in a conceptual overview, see our doc on streaming alerts, key terms and concepts. What's aggregation? When an application or service is monitored by New Relic, data can arrive in different ways. Some data arrives consistently and predictably, while other data arrives inconsistently and sporadically. Aggregation is how our alerting system gathers data together before analyzing it for exceeding warning or critical threshold levels. Your data is collected as data points in an aggregation window and then turned into a single numeric value. The data points are aggregated based on your NRQL query using methods like sum, average, min, and max, among others. This single numeric value is what's used to evaluate the condition's threshold. Once data has been aggregated, no more data points can be added to it. Our different aggregation methods will help you strike a balance between aggregating your data quickly and waiting for enough data points to arrive. Why it matters With the correct aggregation method, you're more likely to get notifications you care about, while preventing ones you don't. The most important questions to consider when deciding on your aggregation method: How often does my data arrive? How consistently does my data arrive? When data arrives frequently and consistently in a linear way, we recommend using event flow. When data arrives sporadically, inconsistently, and out of order, we recommend using event timer aggregation. When to use event flow With event flow, data is aggregated based on data point timestamps, so it's important that data points arrive in a consistent and linear manner. This aggregation method doesn't work as well for data point timestamps that arrive out of order or with a wide span of time within a short period. Event flow is the default aggregation method, because it applies for the most common use cases. How event flow works Event flow uses data point timestamps to determine when to open and close an aggregation window. For example, if you're using event flow with a 2 minute delay window, then an aggregation window will close when a timestamp arrives that's two minutes later than the last timestamp that was received. A data point with a 12:00pm timestamp arrives. An aggregation window opens. At some point, a 12:03pm data point arrives. Event flow closes the window, excluding the 12:03 data point, and evaluates that closed window against your thresholds. The event flow aggregation window will continue collecting data points until that later timestamp. The later timestamps are what moves the system forward, not the data points themselves. Event flow will wait as long as necessary for the next data point later than your delay setting to arrive, before aggregating the data. Event flow works best for data that arrives frequently and consistently. Caution If you expect your data points to arrive more than 30 minutes apart, please use the event timer method described below. Event flow use cases Here are some typical event flow use cases: APM agent data. Infrastructure agent data. Any data coming from a 3rd party that comes in frequently and reliably. Most AWS Cloudwatch metrics coming from the AWS Metric Stream (NOT polling). The main exception is that some AWS Cloudwatch data is very infrequent (like S3 volume data) regardless of whether it's streaming or polling and, in that case, you'd use Event timer. When to use event timer Event timer aggregation is based on a timer that counts down when a data point arrives. The timer resets every time a new data point arrives. If the timer counts down before a new data point arrives, event timer aggregates all of the data points received during that time. Event timer is best for alerting on events that happen sporadically and with large gaps of time. How event timer works Errors are a type of event that happens sporadically, unpredictably, and often with large gaps of time. For example, you might have a condition with a query that returns a count of errors. Many minutes may go by without any errors at all, and then suddenly 5 errors arrive within a minute. In this example, event timer would do nothing until the first of the 5 errors arrive. Then it would start the timer, resetting it each time a new error arrives. If the timer countdown reaches 0 without a new error, event timer aggregates the data, and evaluates it against your threshold. Event timer use cases Here are some typical event timer use cases: New Relic usage data. Cloud integration data that is being polled (such as with GCP, Azure, or AWS polling methods). Queries that deliver sparse or infrequent data, such as error counts. Cadence Cadence is our original aggregation method. It aggregates data on specific time intervals as detected by New Relic's internal wall clock, regardless of data timestamps. We recommend that you use one of our other aggregation methods instead. If you're currently using cadence, choose one of the other aggregation methods. Event flow is best for consistent, predictable data points. Event timer is best for inconsistent, sporadic data points. Aggregation and loss of signal Our loss of signal system runs separately from these aggregation methods and settings. If you set your alert condition to open a new violation when your signal is lost for 10 minutes, a loss of signal service watches for data points to arrive. If a new data point fails to arrive within 10 minutes, loss of signal causes a violation to open. For more information on when to use gap filling and loss single, see our forum post on when to use gap filling and loss of signal.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 162.89952,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Aggregation <em>and</em> loss of signal",
        "tags": "<em>Alerts</em> <em>and</em> <em>Applied</em> <em>Intelligence</em>",
        "body": " methods are event flow, event timer, and cadence. If you&#x27;re interested in a conceptual overview, see our doc on streaming <em>alerts</em>, key terms and concepts. What&#x27;s aggregation? When an application or service is monitored by <em>New</em> <em>Relic</em>, data can arrive in different ways. Some data arrives consistently"
      },
      "id": "61853bba28ccbc5a1b7fe94c"
    }
  ]
}