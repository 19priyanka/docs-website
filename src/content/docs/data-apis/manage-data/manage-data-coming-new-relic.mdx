---
title: Manage data coming into New Relic
tags:
  - Ingest and manage data
  - Manage data
metaDescription: 'For accounts on the New Relic One pricing model: how we ingest and store and calculate ingested data.'
redirects:
  - /docs/telemetry-data-platform/manage-data/manage-data-coming-new-relic
  - /docs/manage-data-coming-new-relic
  - /docs/telemetry-data-platform/get-started/manage-data/manage-data-coming-in
  - /docs/manage-data-coming-in
  - /docs/telemetry-data-platform/get-data-new-relic/manage-data/manage-data-coming-new-relic
  - /telemetry-data-platform/get-started/manage-data/manage-data-coming-in
---

When you connect your data to New Relic, we process what we receive and apply data dropping and transformation rules. Then we count the bytes needed to represent your data in a standard format, like JSON. If you're on our [New Relic One pricing model](/docs/accounts/accounts-billing/new-relic-one-pricing-users/pricing-billing), you're charged by the number of bytes written to our database, above and beyond the standard amount thatâ€™s free.

<Callout variant="important">
  This doc is for accounts on our [New Relic One pricing model](/docs/accounts/accounts-billing/new-relic-one-pricing-users/pricing-billing). If you're on our original product-based pricing model, see [Original pricing model]](/docs/accounts/original-accounts-billing/product-pricing/product-based-pricing/). Not sure which you're on? See [Overview of pricing models](/docs/transition-guide-our-new-pricing-plan-user-model).
</Callout>

## Data ingest UI [#data-ingest-ui]

The **Data ingestion** tab is located in the [Data management UI](/docs/data-apis/manage-data/manage-your-data). The **Data ingestion** UI shows your ingest rates for the time period specified by the time picker in the upper right.

The page shows your daily average GBs, and the total for the range you set. You can toggle between an overall ingest view and an account view to identify which of your accounts are sending the most data. The page also provides the current month-to-date, and the projected end-of-month total ingest rates. With this information, you can proactively drop data or turn off agents in order to manage ingest and, therefore, costs.

If you want to take a look at how we query the data, click **View query** to see the query. If you want to drill down further into your data usage, check out the [example usage queries](/docs/accounts/accounts-billing/new-relic-one-pricing-users/usage-queries-alerts#data-queries).

![Gif shows data ingestion with source and account views](./images/data-ingest-source-account.gif "data-ingest-source-account.gif")

<figcaption>
  From the [account dropdown](/docs/glossary/glossary/#account-dropdown), select **Manage your data**, and then select **Data ingestion**. 
</figcaption>

## Data ingestion sources [#sources-list]

The **Data ingestion** page describes which of your data sources provide the most data on average and during specific data ranges. The sources are described here.

<table>
  <thead>
    <tr>
      <th style={{ width: "200px" }}>
        Billable data sources
      </th>

      <th>
        Description
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        Timeslices (1-minute) and Metric:Raw
      </td>

      <td>
        Metrics are timeslices + MetricRaw

        Metric group: `MetricsBytes`

        Metric timeslice data averages to one-hour periods after eight days. After 90 days, the permanent metric data continues to be stored in one-hour periods. We currently store the raw metric data for 30 days.

        You are only billed for the initial ingest volume. You are not billed for subsequent rollups.
      </td>
    </tr>

    <tr>
      <td>
        APM (transactions and errors)
      </td>

      <td>
        [APM events](/docs/insights/insights-data-sources/default-data/apm-default-events-insights)

        Metric group: `ApmEventsBytes`
      </td>
    </tr>

    <tr>
      <td>
        InfraSamples:Raw
      </td>

      <td>
        Includes multiple [Infrastructure events](/docs/infrastructure/manage-your-data/data-instrumentation/default-infrastructure-events).
      </td>
    </tr>

    <tr>
      <td/>

      <td>
        Infrastructure host data

        Metric group:`InfraHostBytes`

        Information related to your servers and virtual machines coming from infrastructure agents, including storage and network data.
      </td>
    </tr>

    <tr>
      <td/>

      <td>
        Infrastructure process data stored in `ProcessSample`.

        Metric group: `InfraProcessBytes`

        Data related to each process running on the hosts running the infrastructure agent. This feature is turned off by default. For more information, see [Process metrics](/docs/infrastructure/install-infrastructure-agent/configuration/infrastructure-agent-configuration-settings/#enable-process-metrics).
      </td>
    </tr>

    <tr>
      <td/>

      <td>
        Infrastructure integrations

        Metric group: `InfraIntegrationBytes`

        Performance data related to applications and services, typically managed by the customer, including data related to Docker containers, Windows services, Nagios checks, and cloud integrations such as managed services in AWS, Azure, and GCP.
      </td>
    </tr>

    <tr>
      <td>
        Logging
      </td>

      <td>
        Includes logs and any `Log_<value>` custom data partition created

        Metric group: `LoggingBytes`

        Log records are stored into the `Log` event type by default. Additional custom data partitions will create new event types, which are always prefixed with `Log_` and will be counted as part of the overall set of log data stored.

        With `LogExtendedRecord`, log messages longer than 4KB are split into multiple events that, when needed, are stitched together to display the original message; this reduces the size of message data.

        As of September 2021, log storage as blobs replaces `LogExtendedRecord`. With blob storage, NRDB can store up to 128,000 bytes for any attribute, not just messages. For more information, see our [blobs documentation for logs](/docs/logs/log-management/ui-data/long-logs-blobs).
      </td>
    </tr>

    <tr>
      <td>
        Default
      </td>

      <td>
        [Custom events](/docs/insights/insights-data-sources/custom-data/report-custom-event-data)

        Metric group: `CustomEventsBytes`
      </td>
    </tr>

    <tr>
      <td>
        Mobile error

        Mobile general

        Breadcrumb crash event trails

        Mobile session

        Mobile exception

        Mobile crash
      </td>

      <td>
        [Mobile events](/docs/insights/insights-data-sources/default-data/mobile-default-events-insights)

        Metric group: `MobileEventsBytes`
      </td>
    </tr>

    <tr>
      <td>
        Tracing
      </td>

      <td>
        Metric group: `TracingBytes`

        * `TracingBytes` includes `Span` and OpenTelemetry's `SpanEvent`.
        * You are not charged for `DistributedTraceSummary` events.
      </td>
    </tr>

    <tr>
      <td>
        Browser:EventLog

        Browser

        Browser:JSErrors

        PcvPerf (PageView timing)
      </td>

      <td>
        [Browser events](/docs/insights/insights-data-sources/default-data/browser-default-events-insights)

        Metric group: `BrowserEventsBytes`
      </td>
    </tr>

    <tr>
      <td>
        Lambda
      </td>

      <td>
        Serverless

        Metric group: `ServerlessBytes`
      </td>
    </tr>
  </tbody>
</table>

## Understand where data is coming from [#facet-data-ingest]

You can inspect your data ingest to gain more information about your ingest health. This way, you'll know your baselines, and can more easily spot an anomaly like an ingest spike, as well as its source. 

To break down your ingested data, start from the chart on the **Data ingestion** page. Think of the data source bands as the Y axis, and the dates as the X axis. Click on the band you want to inspect at the spot in the X axis that corresponds with the date you want to investigate. 

![screenshot of data facet selection](./images/data-facet.png "data-facet.png")

<figcaption>
   This image shows the metrics band for June 15 right before it's clicked. 
</figcaption>

A modal opens with the account, data source, and facet selected. You can do a handful of things on this page:

* Change the account, data source, or facet you want to drill down into.
* Change the date and time to investigate. 
* Review the results of the query in chart form. The chart displays the top 15 results for the facet query. 
* Open the NRQL query in the **Query builder** where you'll find additional facets that you can use.  

Learn more about [NRQL queries here](/docs/query-your-data/nrql-new-relic-query-language/get-started/introduction-nrql-new-relics-query-language/). 

### How we break your ingest data down

Some of the content in the UI is variable, depending on your account. This information is intended to help you understand how we're working with your ingest data.

* The chart on the **Data ingestion** page shows data usage for a little longer time frame than that covered by your retention settings for each data ingest source. If you choose a date outside of your designated retention period for an ingest source, you'll get the message that there's no chart data available. Select a more recent date to fix this problem.
*  If you inspect a data source for an account that has less than a terrabyte of data, we compute the ingest volume over a 24 hour period; otherwise, we compute it for a 1 hour period.
* The ingest value provided on the main **Data ingestion** chart will be slightly different from that reflected during inspection. This is because our facet computation is an estimate.

## Set alerts for data use [#set-alerts]

[Query and alert on usage data](/docs/accounts/accounts-billing/new-relic-one-pricing-users/usage-queries-alerts) describes how to set alerts to get notified if you're nearing data ingest limits you don't want to cross. For example, you might set an alert on logs, which can stack up quickly in an active system.

## Adjust your data ingest [#adjust-ingest]

Here are some ideas for managing your data: 

### Drop unwanted data [#drop-data]

On ingest, we apply data dropping rules so you won't be charged for data that's not useful. Learn how to set additional [data dropping rules](/docs/accounts/accounts/data-management/drop-data-using-nerdgraph) yourself. For how to drop log data, see [Drop log data](/docs/logs/new-relic-logs/ui-data/drop-data-drop-filter-rules).

### Disable agents and integrations [#disable]

If you have agents or integrations that you don't need at all, you can uninstall/delete those tools. For instructions, see the specific docs for that tool.

### Adjust APM data ingest [#adjust-apm-ingest]

Options for adjusting APM data include:
* Configure the [sampling rate](/docs/agents/java-agent/configuration/java-agent-configuration-config-file#Transaction_Events) for transaction events.
* [Set appropriate Apdex scores](/docs/apm/new-relic-apm/apdex/change-your-apdex-settings/), for example, for frequency of traces.
* Optimize [custom instrumentation](/docs/apm/agents/manage-apm-agents/agent-data/custom-instrumentation/) and/or [custom metrics](/docs/apm/agents/manage-apm-agents/agent-data/collect-custom-metrics/).

### Adjust infrastructure data ingest [#adjust-infra-ingest]

Options for adjusting infrastructure data include: 
* Adjust [sampling rate](/docs/infrastructure/install-infrastructure-agent/configuration/infrastructure-agent-configuration-settings#samples-variables) for network, storage, and system events.
* [Disable process metrics](/docs/infrastructure/install-infrastructure-agent/configuration/infrastructure-agent-configuration-settings/#enable-process-metrics).
* Adjust [polling intervals](/docs/infrastructure/infrastructure-integrations/cloud-integrations/configure-polling-frequency-data-collection-cloud-integrations) for on-host and cloud integrations. 
* [Control the reporting of specific attributes.](/docs/infrastructure/install-infrastructure-agent/configuration/infrastructure-agent-configuration-settings/#include-matching-metrics)
* Manage [Kubernetes events integration](/docs/kubernetes-pixie/kubernetes-integration/kubernetes-events/install-kubernetes-events-integration/#install).

/docs/infrastructure/install-infrastructure-agent/configuration/infrastructure-agent-configuration-settings/#include-matching-metrics

### Adjust log data ingest [#adjust-log-ingest]

Options for adjusting log data ingest include: 
* Use the log forwarder to filter log events on the sending side.
* Drop log data, either [via the UI](/docs/logs/ui-data/drop-data-drop-filter-rules) or [with NerdGraph](/docs/data-apis/manage-data/drop-data-using-nerdgraph).
