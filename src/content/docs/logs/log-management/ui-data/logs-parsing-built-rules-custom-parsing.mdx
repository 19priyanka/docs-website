---
title: 'Logs parsing: Built-in rules and custom parsing'
tags:
  - Logs
  - Log management
  - UI and data
redirects:
  - /docs/logs/new-relic-logs/ui-data/customizing-log-data
  - /docs/logs/new-relic-logs/ui-data/customize-logs-using-parsing
  - /docs/logs/new-relic-logs/ui-data/new-relic-logs-parsing-built-rules-custom-parsing
  - /docs/logs/log-management/ui-data/new-relic-logs-parsing-built-rules-custom-parsing
---

[New Relic](/docs/logs/new-relic-logs/get-started/introduction-new-relic-logs) parses log data according to rulesets. Learn how logs parsing works, how to use built-in rulesets, and how to create custom rules to filter your logs.

## Why it matters [#parsing-defined]

Parsing is the process of taking an unstructured text log and organizing it into attribute/value pairs. Parsing is valuable because it allows for aggregations and alerts to be applied to a subset of the log data.

A good example is an [NGINX](/docs/integrations/host-integrations/host-integrations-list/nginx-monitoring-integration) access log. By default, that type of log is an unstructured collection of text, useful for full-text searching but not much else. Here's an example of a typical line from an access log:

```
93.180.71.3 - - [10/May/1997:08:05:32 +0000] "GET /downloads/product_1 HTTP/1.1" 304 0 "-" "Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)"
```

After parsing, the log can be organized into attributes, like `response code` and `request URL`:

```
{
   "remote_addr":"93.180.71.3",
   "time":"1586514731",
   "method":"GET",
   "path":"/downloads/product_1",
   "version":"HTTP/1.1",
   "response":"304",
   "bytesSent": 0,
   "user_agent": "Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)"
}
```

Parsing makes it possible to create [custom queries](/docs/using-new-relic/data/understand-data/query-new-relic-data) that facet on those values, letting you understand the distribution of response codes per-request URL and quickly find problematic pages.

## How we parse logs [#overview]

Here's an overview of how New Relic implements parsing of logs:

* All parsing takes place against the message field; this is the only field that is parsed.
* Parsing is applied based on an [attribute](/docs/using-new-relic/welcome-new-relic/get-started/glossary#attribute) (key-value pair) included in the unparsed log event. It must be available before parsing takes place, to help match the rule to the right logs. Attributes can either be a part of the original log or added when the log is shipped.
* We highly recommend that you use `logtype` as the attribute name.
* Parsing rules are written in [Grok](https://grokdebug.herokuapp.com/patterns#), a collection of patterns that abstract away complicated regular expressions. You must know Grok in order to create a parsing rule.
* If the content of the message field is valid JSON, it's automatically parsed into attribute/value pairs.

## How does our log parsing work? [#how-it-works]

[New Relic](/docs/introduction-new-relic-logs) provides a log ingestion pipeline that can parse data. Parsing is done by matching a log event to a rule that describes how the log should be parsed. There are two ways log events can be parsed: using a [built-in rule](#built-in-rules) or by defining a [custom rule](#custom-parsing).

Rules are a combination of matching logic and parsing logic. Matching is done by defining a query match on an attribute of the logs. Rules are not applied retroactively: logs collected prior to the creation of a rule are not parsed by that rule.

The simplest way to organize your logs and how they are parsed is to include the `logtype` field in your log event, which tells New Relic what built-in ruleset must be applied to the logs.

<Callout variant="important">
  Once a parsing rule is active, data parsed by the rule is permanently changed. This cannot be reverted.
</Callout>

## Built-in parsing rulesets [#built-in-rules]

Common log formats have well-established parsing rules already created for them. To get the benefit of built-in parsing rules, add the `logtype` attribute when shipping the logs. Set the value to one of those listed in the table below, and the rules for that type of log will be applied automatically.

### List of built-in rulesets [#rulesets]

The following `logtype` attribute values map to a standard parsing rulesets. See [Built-in log parsing rulesets](/docs/logs/log-management/ui-data/built-log-parsing-rulesets) to learn what fields are parsed for each ruleset.

<Table>
  <thead>
    <tr>
      <th style={{ width: "250px" }}>
        `logtype`
      </th>

      <th>
        Example matching query
      </th>

      <th>
        Description
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        [`alb`](/docs/logs/log-management/ui-data/built-log-parsing-rulesets#application-load-balancer)
      </td>

      <td>
        `logtype:alb`
      </td>

      <td>
        AWS Application Load Balancer
      </td>
    </tr>

    <tr>
      <td>
        [`apache`](/docs/logs/log-management/ui-data/built-log-parsing-rulesets#apache)
      </td>

      <td>
        `logtype:apache`
      </td>

      <td>
        Apache Access
      </td>
    </tr>

    <tr>
      <td>
        [`cloudfront-web`](/docs/logs/log-management/ui-data/built-log-parsing-rulesets#cloudfront)
      </td>

      <td>
        `logtype:cloudfront-web`
      </td>

      <td>
        Cloudfront Web
      </td>
    </tr>

    <tr>
      <td>
        [`elb`](/docs/logs/log-management/ui-data/built-log-parsing-rulesets#elastic-load-balancer)
      </td>

      <td>
        `logtype:elb`
      </td>

      <td>
        Amazon Elastic Load Balancer
      </td>
    </tr>

    <tr>
      <td>
        `iis_w3c`
      </td>

      <td>
        `logtype:iis_w3c`
      </td>

      <td>
        IIS server logs - W3C format
      </td>
    </tr>

    <tr>
      <td>
        [`monit`](/docs/logs/log-management/ui-data/built-log-parsing-rulesets#monit)
      </td>

      <td>
        `logtype:monit`
      </td>

      <td>
        Monit logs
      </td>
    </tr>

    <tr>
      <td>
        [`mysql-error`](https://docs.newrelic.com/docs/logs/log-management/ui-data/built-log-parsing-rulesets#mysql-error)
      </td>

      <td>
        l`ogtype:mysql-error`
      </td>

      <td>
        MySQL Error
      </td>
    </tr>

    <tr>
      <td>
        [`nginx`](/docs/logs/log-management/ui-data/built-log-parsing-rulesets#nginx)
      </td>

      <td>
        `logtype:nginx`
      </td>

      <td>
        NGINX access logs
      </td>
    </tr>

    <tr>
      <td>
        [`nginx-error`](/docs/logs/log-management/ui-data/built-log-parsing-rulesets#nginx-error)
      </td>

      <td>
        `logtype:nginx-error`
      </td>

      <td>
        NGINX error logs
      </td>
    </tr>

    <tr>
      <td>
        [`route-53`](/docs/logs/log-management/ui-data/built-log-parsing-rulesets#route-53)
      </td>

      <td>
        `logtype:route-53`
      </td>

      <td>
        Amazon Route 53 logs
      </td>
    </tr>

    <tr>
      <td>
        [`syslog-rfc5424`](/docs/logs/log-management/ui-data/built-log-parsing-rulesets#syslog-rfc5424)
      </td>

      <td>
        `logtype:syslog-rfc5424`
      </td>

      <td>
        Syslog
      </td>
    </tr>
  </tbody>
</Table>

### How to add the `logtype` attribute [#logtype]

When aggregating logs, it's important to provide metadata that makes it easy to organize, search, and parse those logs. One simple way of doing this is to add the attribute `logtype` to the log messages when they are shipped. [Built-in parsing rules](#built-in-rules) are applied by default to certain `logtype` values.

Below are some examples of how to add `logtype` to logs sent by some of our [supported shipping methods](/docs/logs/enable-new-relic-logs).

<CollapserGroup>
  <Collapser
    className="freq-link"
    id="infrastructure-log-forwarder-example"
    title="New Relic infrastructure agent example"
  >
    Add `logtype` as an [`attribute`](/docs/logs/enable-log-management-new-relic/enable-log-monitoring-new-relic/forward-your-logs-using-infrastructure-agent#attributes). You must set the logtype for each named source.

    ```
    logs:
      - name: file-simple
        file: /path/to/file
        attributes:
          logtype: fileRaw  
      - name: nginx-example
        file: /var/log/nginx.log
        attributes:
          logtype: nginx
    ```
  </Collapser>

  <Collapser
    className="freq-link"
    id="fluentd-example"
    title="Fluentd example"
  >
    Add a filter block to the .conf file, which uses a `record_transformer` to add a new field. In this example we use a `logtype` of `nginx` to trigger the build-in NGINX parsing rule. Check out other [Fluentd examples here](https://github.com/newrelic/fluentd-examples).

    ```
    <filter containers>
      @type record_transformer
      enable_ruby true
      <record>
        #Add logtype to trigger a built-in parsing rule for nginx access logs
        logtype nginx
        #Set timestamp from the value contained in the field "time"
        timestamp record["time"]
        #Add hostname and tag fields to all records
        hostname "#{Socket.gethostname}"
        tag ${tag}
      </record>
    </filter>
    ```
  </Collapser>

  <Collapser
    className="freq-link"
    id="fluentbit-example"
    title="Fluent Bit example"
  >
    Add a filter block to the .conf file that uses a `record_modifier` to add a new field. In this example we use a `logtype` of `nginx` to trigger the build-in NGINX parsing rule. Check out other [Fluent Bit examples here](https://github.com/newrelic/fluentbit-examples).

    ```
    [FILTER]
        Name record_modifier
        Match *
        Record logtype nginx
        Record hostname ${HOSTNAME}
        Record service_name Sample-App-Name
    ```
  </Collapser>

  <Collapser
    className="freq-link"
    id="logstash-example"
    title="Logstash example"
  >
    Add a filter block to the Logstash configuration which uses an `add_field` mutate filter to add a new field. In this example we use a `logtype` of `nginx` to trigger the build-in NGINX parsing rule. Check out other [Logstash examples here](https://github.com/newrelic/logstash-examples).

    ```
    filter {
      mutate {
        add_field => {
          "logtype" => "nginx"
          "service_name" => "myservicename"
          "hostname" => "%{host}"
        }
      }
    }
    ```
  </Collapser>

  <Collapser
    className="freq-link"
    id="api-example"
    title="Logs API example"
  >
    You can add attributes to the JSON request sent to New Relic. In this example we add a `logtype` attribute of value `nginx` to trigger the build-in NGINX parsing rule. Learn more about using the [Logs API here](https://docs.newrelic.com/docs/logs/new-relic-logs/log-api/introduction-log-api).

    ```
    POST /log/v1 HTTP/1.1
    Host: log-api.newrelic.com
    Content-Type: application/json
    X-License-Key: YOUR_LICENSE_KEY
    Accept: */*
    Content-Length: 133
    {
      "timestamp": TIMESTAMP_IN_UNIX_EPOCH,
      "message": "User 'xyz' logged in",
      "logtype": "accesslogs",
      "service": "login-service",
      "hostname": "login.example.com"
    }
    ```
  </Collapser>
</CollapserGroup>

## Create custom parsing rules [#custom-parsing]

Many logs are formatted or structured in a unique way. In order to parse them, custom logic must be built and applied. New Relic's log management lets you create and manage custom parsing rules.

1. Choose **Manage parsing** from the action menu dropdown.

   ![manage-parsing.png](./images/manage-parsing.png "manage-parsing.png")
2. Click **Create parsing rule**.
3. Choose an attribute and value to match on.
4. Write your Grok pattern and test the rule.
5. Enable and save the custom parsing rule.

To learn about Grok and custom parsing rules, read our blog post, [How to parse logs with Grok patterns](https://blog.newrelic.com/product-news/how-to-use-grok-log-parsing/).
