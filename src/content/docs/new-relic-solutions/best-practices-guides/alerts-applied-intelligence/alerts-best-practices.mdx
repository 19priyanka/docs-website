---
title: Alerts best practices
tags:
  - New Relic solutions
  - Best practices guides
  - Alerts and Applied Intelligence
translate:
  - jp
metaDescription: 'Best practices for deciding what to alert on, when to trigger notifications, and who receives them.'
redirects:
  - /docs/alerts/new-relic-alerting/configuring-alert-policies/best-practices-alert-policies
  - /docs/alerts/new-relic-alerting/getting-started/best-practices-alert-policies
  - /docs/alerts/new-relic-alerts-beta/getting-started/best-practices-alert-policies
  - /docs/alerts/new-relic-alerts/getting-started/best-practices-alert-policies
  - /docs/alerts/new-relic-alerts/getting-started/alerts-best-practices
  - /docs/alerts-applied-intelligence/new-relic-alerts/get-started/alerts-best-practices
---

Improve your alert coverage by implementing the following recommendations. 

Read on to learn the best practices for:
- [Policies](/docs/new-relic-solutions/best-practices-guides/alerts-applied-intelligence/alerts-best-practices/#policy-practices)
- [Notification channels](/docs/new-relic-solutions/best-practices-guides/alerts-applied-intelligence/alerts-best-practices/#channel-practices)
- [Incident preferences](/docs/new-relic-solutions/best-practices-guides/alerts-applied-intelligence/alerts-best-practices/#incident-practices)
- [Thresholds and violations](/docs/new-relic-solutions/best-practices-guides/alerts-applied-intelligence/alerts-best-practices/#threshold-practices)
- [Muting roles](/docs/new-relic-solutions/best-practices-guides/alerts-applied-intelligence/alerts-best-practices/#mute-practices)

<Callout variant="important">
  Before reading this, we recommend you also read [Alerts concepts and workflow](/docs/alerts/new-relic-alerts/getting-started/alert-policy-workflow).
</Callout>

## Recommended alerts [#recommend-alerts]

Use [recommended practices](https://discuss.newrelic.com/t/announcing-alert-recommendations-for-apm-applications/154542) if you are new to Alerts or if you want suggestions that optimize your alert coverage.

## Policies [#policy-practices]

A policy is a container for alike conditions.

<Callout variant="tip">
Learn how to [create, edit, or find policies](/docs/alerts-applied-intelligence/new-relic-alerts/alert-policies/create-edit-or-find-alert-policy/) if you’re new to Alerts.
</Callout>

Organize your policy by surrounding its conditions to a single entity and to the specific team that needs to be notified when a problem arises within that entity.

If a team is monitoring several groups of the same entity type, combine those entity clusters (like servers) together into one policy. This way, your team can be notified from one policy rather than navigating several policies at once.

Consider your team’s role when assigning them to policies:

* **Software developers** may need [notifications](/docs/alerts/new-relic-alerts/managing-notification-channels/notification-channels-controlling-where-send-alerts) for both front-end and back-end performance, such as webpage response time and page load JavaScript errors.
* **Operations personnel** may need notifications for poor back-end performance, such as server memory and load averages.
* **The product owner** may need notifications for positive front-end performance, such as improved end user Apdex scores or sales being monitored in [dashboards](/docs/query-your-data/explore-query-data/dashboards/introduction-new-relic-one-dashboards).

Refrain from creating policies that include multiple teams to keep policies centralized.

## Notification channels [#channel-practices]

Tailor notifications to the most useful channel and policy so you can avoid alert fatigue and help the right personnel receive and respond to incidents they care about in a systematic way.

<Callout variant="tip">
Learn how to [set up notification channels](/docs/alerts-applied-intelligence/new-relic-alerts/alert-notifications/notification-channels-control-where-send-alerts/) if you’re new to Alerts.
</Callout>

Notify teams and individuals who:
- Can resolve a problem when an incident arises
- Need to stay updated when an incident arises

For staying updated, select a notification channel that is less intrusive like email.

For vital notifications and responding to the incident, select a notification channel that is immediate and reliable like PagerDuty or Slack. You are given a lot more control over who gets notified and when with PagerDuty and OpsGenie. 

Do not rely on email for quick notifications in case of mail delays. An email list will also bounce if one is no longer valid.


## Incident preferences [#incident-practices]

Decide when you get incident notifications so you can respond to incidents when they happen.

<Callout variant="tip">
Learn more about your [incident preferences options](https://discuss.newrelic.com/t/relic-solution-alert-incident-preferences-are-the-key-to-consistent-alert-notifications/40867) if you’re new to Alerts.
</Callout>

The default incident preference setting combines all conditions within a policy into one incident. Change your default incident preference setting to better understand the exact scope and number of all violations.

Each organization will have different needs, and each policy within an organization will have different needs too. Ask your team two important questions when deciding your incident preferences:
- Do we want to be notified every time something goes wrong? 
- Do we want to group all alike notifications together and be notified once?

When a policy and its conditions have a broader scope (like managing several entities), increase your incident preference sensitivity. You will need more notifications because two incidents will not necessarily relate to each other.

When a policy and its conditions have a focused scope (like managing one entity), opt for one notification. You will need less notifications when two incidents are related to each other or when the team is already notified and fixing an existing problem.

Decide how you get incident notifications by using our best [notification channel practices](/docs/new-relic-solutions/best-practices-guides/alerts-applied-intelligence/alerts-best-practices/#channel-practices).


## Thresholds and violations [#threshold-practices]

Set meaningful [threshold](/docs/alerts/new-relic-alerts-beta/configuring-alert-policies/define-thresholds-trigger-alert) levels to optimize Alerts to your business. Here are some suggested guidelines:

<table>
  <thead>
    <tr>
      <th width={200}>
        Action
      </th>

      <th>
        **Recommendations**
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        Set threshold levels
      </td>

      <td>
        **Avoid setting thresholds too low.** For example, if you set a CPU condition threshold of 75% for 5 minutes on your production servers, and it routinely goes over that level, this will increase the likelihood of un-actionable alerts or false positives.
      </td>
    </tr>

    <tr>
      <td>
        Experimenting with settings
      </td>

      <td>
        **You do not need to edit files or restart software**, so feel free to make quick changes to your threshold levels and adjust as necessary.
      </td>
    </tr>

    <tr>
      <td>
        Adjust settings
      </td>

      <td>
        **Adjust your conditions over time.**

        * As you use our products to help you optimize your entity's performance, tighten your thresholds to keep pace with your improved performance.
        * If you are rolling out something that you know will negatively impact your performance for a period of time, loosen your thresholds to allow for this.
      </td>
    </tr>

    <tr>
      <td>
        Disable settings
      </td>

      <td>
        You can **disable any condition** in a policy. This is useful, for example, if you want to continue using other conditions in the policy while you experiment with other metrics or thresholds.
      </td>
    </tr>
  </tbody>
</table>

In most of our products (except Infrastructure), the color-coded [health status indicator](/docs/using-new-relic/welcome-new-relic/get-started/glossary/#health-status) in the user interface changes as the alerting threshold escalates or returns to normal. This allows you to monitor a situation through our UI before a critical threshold passes, without needing to receive specific notifications about it.
 
There are two violation thresholds: critical (red) and warning (yellow). Define these thresholds with different criteria keeping in mind the suggestions above. 

<Callout variant='important'>
Warning violations do **not** open incidents. A critical violation can open incidents, but you must define that decision through your [incident preferences](/docs/new-relic-solutions/best-practices-guides/alerts-applied-intelligence/alerts-best-practices/#incident-practices).
</Callout>

## Muting roles [#mute-practices]

Mute alerts during routined events such as maintenance or planned downtime.

You can also silence another aspect of Alerts (like a policy, a specific entity, a condition) when needed. Incidents can still be opened, but you will not be notified.

<Callout variant='tip'>
Learn how to [create and manage muting roles](/docs/alerts-applied-intelligence/new-relic-alerts/alert-notifications/muting-rules-suppress-notifications/) if you’re new to Alerts.
</Callout>

## What's next?

To learn more about using alerts:

* Learn about the [API](https://docs.newrelic.com/docs/alerts/rest-api-alerts/new-relic-alerts-rest-api/rest-api-calls-new-relic-alerts).
* Read technical details about [min/max limits and rules](https://docs.newrelic.com/docs/alerts/new-relic-alerts/getting-started/minimum-maximum-values).
