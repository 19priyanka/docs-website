---
title: Alert quality management tutorial
tags:
  - New Relic solutions
  - Best practices guides
  - Tutorials
  - Alerts and Applied Intelligence
metaDescription: 'How to reduce alert fatigue so you can focus on alerts with true business impact in New Relic.'
---

## The Problem [#problem]

Teams suffer from alert fatigue when they are subject to high alert volumes and alerts that are not aligned to business impact. This situation will condition responders to believe that most alerts are false, will cause them to prioritize easy to resolve alerts over others, and may drive them to close unresolved incidents so they can stay within their SLA targets.  Overall, these behaviors will result in slower incident response, which will magnify issue scope and severity when true business impacting issues occur.

Alert Quality Management (AQM) focuses on reducing the number of nuisance incidents so that only alerts with true business impact are asserted.  This reduces alert fatigue and ensures that attention is focused on the right places at the right times.

You are a good candidate for AQM if any of the following are true:

* You have too many alerts.
* You have alerts that are asserted for long time periods.
* Your alerts are not relevant.
* You perceive that customers discover your issues before your monitoring tools do.
* You can’t see the value of your observability tool(s).

## Desired Outcome [#solution]

### Overview [#overview]

An alert strategy based on measuring business impact will result in faster response times and greater proactive awareness of critical events.  An improved alert signal to noise ratio reduces confusion and improves rapid identification and problem isolation.

AQM’s overall goal is to ensure that fewer, more valuable, incidents are created, resulting in increased uptime and availability and reduced MTTR.  As you move towards this goal, alert volume will decrease and alerts that are not valuable will be identified and either made valuable or removed.

The AQM process described in this guide will generate the key performance metrics that you will use to measure progress towards these goals.  Those KPIs will be used to drive a continuous improvement process that identifies and reduces nuisance alerts and increases user engagement in incident investigation.  Those business metrics are measured in real time in the AQM dashboard.

AQM is intended to improve the value of existing alert configurations and to detect known or expected modes of failure.  It does not encompass anomaly detection or AIOps, which are designed to detect unknown or unexpected modes of failure.  The two practices (AQM and ML/AI) work hand in hand, they are not mutually exclusive.

## Key Performance Metrics [#kpi]

AQM measures the following KPIs:

* Incident volume:
  * Incident Count
  * Accumulated incident time
  * Mean Time to Close (MTTC)
  * Percent Under 5 Minutes

* User Engagement:
  * Mean Time to Investigate  (MTTI)
  * % of Incidents Investigated

Detailed information on each metric follows.

### Incident Volume [#incident-volume]

#### Summary [#iv-summary]

Incidents (with or without alerts) should be treated like a queue of tasks and, just like a queue, the number of alerts should spend time near zero. For each incident, an action should be taken to resolve the condition. If an alert does not result in action, then the value of the alert condition should be questioned.

If you see a constant rate of incidents or specific incidents that are “always-on”, you should question why. Are you in a constant state of business impact, or do you simply have a large volume of noise? The alert volume KPIs help you to answer those questions and to measure progress towards a healthy state of high quality alerting.

#### Volume - Incident Count [#iv-count]

<table>
  <thead>
    <tr>
      <th width={200}>
        Volume - Incident Count
      </th>

      <th>
        Notes
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        Description
      </td>

      <td>
        Incident Count is the number of incidents generated over a period of time. Typically you should compare the current and previous weeks.
      </td>
    </tr>

    <tr>
      <td>
        Goal
      </td>

      <td>
        Reduce the number of low value / nuisance incidents.
      </td>
    </tr>

    <tr>
      <td>
        Best Practices
      </td>

      <td>
        * Ensure condition settings are intended to detect real business impact.
See “Service Level Objectives” [link tbd].
        * Ensure condition settings are detecting abnormal behavior.
        * Communicate that the incident details “Acknowledge” feature helps measure meaningful and actionable alerts. See “Percent Investigated KPI” [link tbd].
        * Report AQM KPIs to all stakeholders.
      </td>
    </tr>
  </tbody>
</table>

#### Volume - Accumulated Incident Duration [#iv-duration]

<table>
  <thead>
    <tr>
      <th width={200}>
        Volume - Incident Duration
      </th>

      <th>
        Notes
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        Description
      </td>

      <td>
        Accumulated incident duration is the total sum of minutes that all the incidents accumulated over a period of time. Typically you should compare the current and previous weeks.
      </td>
    </tr>

    <tr>
      <td>
        Goal
      </td>

      <td>
        Reduce the total accumulated minutes of incidents.
      </td>
    </tr>

    <tr>
      <td>
        Best Practices
      </td>

      <td>
        * Do not manually close incidents. Manual closure will skew the real duration of incident length.
        * Eliminate alerts that do not result in any remediation actions from the recipients.
        * Improve percent investigated and mean-time-to-investigate KPIs (below) by communicating their importance in improving detection and response time.
        * Report AQM KPIs to all stakeholders.
      </td>
    </tr>
  </tbody>
</table>

#### Volume - Mean-Time-To-Close (MTTC) [#iv-mttc]

<table>
  <thead>
    <tr>
      <th width={200}>
        Volume - Mean-Time-To-Close (MTTC)
      </th>

      <th>
        Notes
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        Description
      </td>

      <td>
        Average duration of incidents within the period of time measured.
      </td>
    </tr>

    <tr>
      <td>
        Goal
      </td>

      <td>
        Reduce MTTC.
      </td>
    </tr>

    <tr>
      <td>
        Best Practices
      </td>

      <td>
        * Do not manually close incidents. Manual closure will skew the real duration of incident length.
        * Improve Reliability Engineering skills [link tbd].
        * Report AQM KPIs to all stakeholders.
      </td>
    </tr>
  </tbody>
</table>

#### Volume - Percent Under Five Minutes [#iv-percent]

<table>
  <thead>
    <tr>
      <th width={200}>
        Volume - Percent Under Five Minutes
      </th>

      <th>
        Notes
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        Description
      </td>

      <td>
        Percentage of incidents where the duration of the incident is under five minutes.  This can be an indicator of incident flapping.
      </td>
    </tr>

    <tr>
      <td>
        Goal
      </td>

      <td>
        Minimize percentage of incidents with short durations.
      </td>
    </tr>
    
    <tr>
      <td>
        Best Practices
      </td>

      <td>
        * Ensure that conditions are detecting legitimate deviations from expected behavior. See Baselining and Service Level Management [links tbd].
        * Ensure that conditions are detecting legitimate deviations that correlate to business impact or impending business impact.
      </td>
    </tr>
  </tbody>
</table>

## H2 [#h2]

text.

## What's next?

To learn more about using alerts:

* Learn about the [API](https://docs.newrelic.com/docs/alerts/rest-api-alerts/new-relic-alerts-rest-api/rest-api-calls-new-relic-alerts).
* Read technical details about [min/max limits and rules](https://docs.newrelic.com/docs/alerts/new-relic-alerts/getting-started/minimum-maximum-values).
