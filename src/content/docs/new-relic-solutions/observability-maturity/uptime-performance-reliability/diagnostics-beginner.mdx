---
title: "Reliability engineering diagnostics - a beginners guide to SRE diagnostics of application performance"
tags:
  - Observability maturity
  - Uptime, performance, and reliability 
  - Site reliability engineering
  - SRE
metaDescription: "New Relic observability maturity series: a beginners guide on how to identify common application performance issues."
---

import patternNormal from 'images/oma-upr-pattern-normal.png'

import patternNormalPercentile from 'images/oma-upr-pattern-normal-percentile.png'

import patternAbnormal from 'images/oma-upr-pattern-abnormal.png'

import patternAbnormalCompare from 'images/oma-upr-pattern-abnormal-compare.png'


## Overview

This guide is an introduction to improving your diagnostics skill of customer impacting issues. You will be able to recover from application performance issues more quickly by following the disciplined process outlined in this guide.

## Prerequisites

* Sufficient observability coverage
  * __Required__: [APM with distributed tracing](/docs/apm/apm-ui-pages/monitoring/apm-summary-page-view-transaction-apdex-usage-data), [logs in context](/docs/apm/new-relic-apm/getting-started/get-started-logs-context), and Infrastructure
  * __Recommended__: [Logs](/docs/logs/get-started/get-started-log-management) and [Network Monitoring](/docs/network-performance-monitoring/get-started/npm-introduction) (NPM)
* __Required__: [Service Level Management](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide/)
* __Recommended__: Platform experience in APM, Distributed Tracing, NRQL and Logs

## Desired outcomes [#desired-outcomes]

### Summary

The value to business is:

* Reduce the number of business-disrupting incidents
* Reduce the time required to resolve issues (MTTR)
* Reduce the operational cost of incident

The value to IT operations and SRE is:

* Improve time to know
  * [Alert Quality Management](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/alert-quality-management-guide/)
  * [Service Level Management](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide)
* Improve time to understand and resolve
  * Reliability Engineering Diagnostics (this guide)

### Business outcome [#business-outcome]

[Gartner estimates](https://blogs.gartner.com/andrew-lerner/2014/07/16/the-cost-of-downtime/) that the average cost of IT downtime is $5,600 per minute. The cumulative cost of business-impacting incidents is determined by factors like time to know, frequency, time to repair, revenue impact, and the number of engineers triaging and resolving the incidents. Simply put, you want fewer business-impacting incidents, shorter incident duration, and faster diagnostics with fewer people needed to resolve performance impacts.

Ultimately, the business goal is to maximize uptime and minimize downtime where the cost of downtime is:

__`Downtime minutes x Cost per minute = Downtime Cost`__

Downtime is driven by the number of business-disrupting incidents and their duration. Cost of Downtime includes many factors but the most directly measurable are operational cost and lost revenue.

The business must drive a reduction in the following:

* number of business-disrupting incidents
* operational cost of incidents


### Operational outcome [#operational-outcome]

The required operational outcome is to maintain product-tier service level objective compliance. You do this by diagnosing degraded service levels, communicating your diagnosis, and performing rapid resolution. But unexpected degradations and incidents always happen and you need to respond quickly and effectively.

IT Operations and SRE must drive improvement in the following:

* Improve time to know
  * [Alert Quality Management](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/alert-quality-management-guide/) (reactive)
  * [Service Level Management](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide) (proactive)
* Improve time to understand and resolve
  * Reliability Engineering Diagnostics (this guide)

## End-user’s perception of reliability [#end-user-perception]

How the customer perceives performance of your product is critical to understanding how to measure urgency, and priority. Also, understanding the customer’s perspective also helps to understand how the business views the problem as well as understanding the required workflows to support the impacted capabilities. Once you understand the perception of the customer and the business we can better understand what might be impacting the reliability of said capabilities.

Ultimately, observability from the customer perspective is the first step to becoming proactive and proficient in reliability engineering.

There are two primary experiences that impact an end-users’ perceived performance of your digital product and its capabilities. The terms below are from the customer’s perspective and the customer’s common language.

### Availability - it doesn’t work

Also-known-as; connectivity, uptime, reachability but also conflated with success (non-errors)

An end-user may state that they cannot access a required capability, such as login, browse, search, view inventory – or they may simply state that the entire service is unavailable. This is a symptom of either the inability to connect to a service or a service returning an error.

Traditionally “Availability” or "Uptime" was measured in a binary “UP/DOWN” methodology by measuring the ability to connect to a service. The traditional method has a critical gap in that it only measures when an entire service becomes completely unavailable. This classic measure of reliability results in significant observability gaps, difficult diagnostics, and the end-users being significantly impacted before you can react.

Availability is measured by both “the ability to reach a service”, a.k.a. "Uptime", AND “the ability of the service to return the expected response”, in other words “non-error”. New Relic’s observability maturity framework distinguishes the two by Input Performance (connectivity) and Output Performance (success and latency of the response).

### Performance - its too slow

Also-known-as; Latency, Response Time

An end user may state that the service is too slow.

For both the IT and business leaders, the term “performance” can be an array of issues. In New Relic’s service level management “slowness” is measured in both the “output” and “client” categories; however the majority of slowness problems occur due to an output issue from what is traditionally referred to as “the backend services”.

## Root cause vs problem (direct cause)

The root cause of a problem is not the same as the problem. Likewise, fixing the problem does not usually mean you have fixed the root cause of the problem. It is very important to make this distinction.

When looking for a performance issue you are first seeking to find the source of the problem by asking the question “...what changed?”. The component or behavior that changed is not usually the root cause but is in fact the problem you need to resolve first. Resolving the root cause is important but usually requires a post-incident retroactive discussion and long-term problem management.

For example; service levels for the login capability suddenly drop. It is immediately found that traffic patterns are much higher than usual. You trace the performance issue to an open TCP connection limit configuration causing a much larger TCP connection queue. You immediately resolve the issue by deploying a TCP limit increase and some extra server instances. You solved the problem for the short term but the root cause could be anything from improper capacity planning, missed communication from marketing, or a failed load balancer configuration change.

This distinction is also made in ITIL/ITSM Incident Management vs. Problem Management. Root causes are discussed in post-incident talks then resolved in longer-term Problem Management processes.

## Diagnostic steps (overview)

### Step 1: create your problem statement

The first rule is to quickly establish the problem statement. There are plenty of guides on building problem statements; however, simple and effective is the best. A well formed problem statement will do the following:

1. Describe what changed for the end-user, customer or consumer. What could the end-user or consumer do before that they cannot do now? This is the customer’s perception of the problem.
2. Describe the expected behavior of the product capability. This is a technical assessment of the problem.
3. Describe the current behavior of the product capability. This is a technical description of the desired result of a fix.

Avoid any assumptions to your problem statement. Stick to the facts.


### Step 2: find the source

The "source" is the component or code that is closest to the problem causing the symptoms.

Think of many small water hoses connected through many junctions, splitters, and valves. You are alerted that your water output service level is degraded. You trace the problem from the water output through the components until you determine which junction, split, valve, or hose that is causing the problem. You discover one of the valves is shorted. That valve is the source. The short is the problem (direct cause).

This is the same concept for diagnosing complex technology stacks. If your "Login" capability is limited (output), you must trace back to the component that is causing that limit and fix it. It could be the API software (service boundary), the middelware services, the database, resource constraints, or a third party service, etc.

In IT there are three primary breakpoints sources; Output, Input, and Client. Measuring these categories is covered in [service level management](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide). Understanding how to use them in diagnostics is described below.

### Step 3: find the change

Once you are close to the source of the problem, determine what changed. This will help you quickly determine how to immediately resolve the problem in the short term. In Step 2 the change is that the valve was no longer functioning due to degraded hardware causing a short.

Examples of common changes in IT are:

1. Througput (traffic)
2. Code (deployments)
3. Resources (hardware allocation)
4. Upstream or downstream dependency changes
5. Data volume

See "Problem matrix" below for more common examples of performance impacting problems.

## Use health data points

There are three primary performance categories that jump-start your diagnostic journey. Understanding these health data points will significantly reduce your time to understanding where the source of the problem is.

### Output Performance

__Requires__: APM

Output performance is the ability of your internal technology stack to deliver the expected responses (output) to the end-user. This is traditionally referred to as the “back-end” services.

In the great majority of scenarios output performance is measured simply by the speed of the response and the quality of the response (a.k.a it is error free). Remember the user perspective described above. The end-user will state that the service is either slow, not working or inaccessible.

The most common problem is the ability to respond to end-user requests in a timely AND successful manner.

This is easily identified by a Latency anomaly or Error anomaly on the services that support the troubled product capability.

### Input Performance

__Requires__: Synthetics

Input performance is simply the ability of your services to receive a request from the client. This is not the same as the client’s ability to send a request.

Your output performance, your back-end services, could be exceeding expected performance levels however something between the client and your services is breaking the request-response lifecycle. This could be anything between the client and your services.

### Client Performance

__Requires__: Browser and/or Mobile agents

Client performance is the ability for a Browser and/or Mobile application to both make requests and render responses. Browser and/or mobile are easily identified as the source of the problem once both output (back-end) and input performance (Synthetics) has been quickly ruled-out.

Output and Input performance is relatively easy to rule-out (or rule-in). Browser and mobile will be covered in an advance diagnostics guide in the future – due to the depth of diagnostics in input and output diagnostic.

## Problem matrix

The problem matrix is a cheat-sheet of common problems categorized by the three health data points.

The common problem sources are listed in order of most common top row to bottom row, left to right. A more detailed breakdown is listed below. Service level management done well will help you rule-out 2 out of three of these data points quickly.

| Data point  | Platform capability  | Common problem sources  |
|---|---|---|
| Output  | APM, Infra, Logs, NPM  | Application, Data sources, Hardware config change, Infrastructure, Internal Networking, 3rd party provider (AWS, GCP)  |
| Input  | Synthetic, Logs  | External Routing (CDN, Gateways, etc), Internal Routing, Things on the internet (ISP, etc..).  |
| Client  |  Browser, Mobile |  Browser or Mobile code |

Problems tend to be compounded; however, the goal is to "find the source" and then determine “what changed” in order to quickly restore service levels.

For example, a significant increase in requests due to a recent deployment of a new product causes unacceptable response times. The source was discovered in the login middleware service. The problem is TCP queue times jumped.

* Category: Output performance
* Source: login middleware
* Problem: TCP queue times from additional request load
* Solution: increased TCP connection limit and scaled resources
* Root-cause: insufficient capacity planning and quality assurance testing on downstream service impacting login middleware

A second example; a sudden increase in 500 Gateway errors on login... the login API response times increased to the point where timeouts began... the timeouts were traced to the database connections in the middleware layer... transaction traces revealed signficant increase in number of database queries per login request... deployment marker was found just before the problem occurred.

* Category: Output performance degredation leading to Input performance failure
* Source: middleware service calling database
* Problem: 10x increased database queries after code deployment
* Solution: deployment rollback
* Root-cause: insufficient quality assurance testing

<table>
  <thead>
    <tr>
      <th>
        **Source**
      </th>
      <th>
        **Common problems**
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        Application
      </td>
      <td>
        1. Recent deployment (code)
        2. Hardware resource constraints
        3. Database constraints
        4. Config change (hardware / routing / networking)
        5. 3rd party dependency
      </td>
    </tr>
    <tr>
      <td>
        Data source
      </td>
      <td>
        1. Database constraints
        2. Change in query logic (n+1)
        3. Message queues (usually resulting in poor producer or consumer performance)
      </td>
    </tr>
    <tr>
      <td>
        Internal networking and routing
      </td>
      <td>
        1. Load balancers
        2. Proxies
        3. API Gateways
        4. Routers (rare)
        5. ISP/CDN (rare)
      </td>
    </tr>
  </tbody>
</table>

## Identifying pattern anomalies

There’s plenty of great information and free online classes on identifying patterns; however, it’s a rather simple process that enables a powerful skill for diagnostics.

The key to identifying patterns and anomalies in performance data is that you do not need to know how the service should be performing – you only need to determine if the recent behavior has changed.

The examples provided in this section use “response time” or “latency” as the metric but you can apply the same analysis to almost any dataset like “errors”, “throughput”, “queue depths”, etc.

### Normal

Below is an example of a seemingly volatile response time chart (7 days) in APM; however, you’ll notice that the behavior of the response time is repetitive. In other words there is no radical change in the behavior across a 7 day period. The “spikes” you see are repetitive and not unusual compared to the rest of the timeline.

<img
  alt="normal pattern"
  title="Normal pattern"     
  src={patternNormal}
/>

In fact, if you change the view of the data from “average over time” to “percentiles over time” you’ll see how “regular” the changes in response time are.

<img
  alt="normal pattern with percentile"
  title="Normal pattern with percentile"
  src={patternNormalPercentile}
/>

### Abnormal

Below you will find that an application response time has seemed to increase unusually compared to recent behavior.

<img
  alt="abnormal pattern"
  title="Abnormal pattern"
  src={patternAbnormal}

/>

This can be confirmed by using the week over week comparison.

<img
  alt="abnormal pattern week-over-week"
  title="Abnormal pattern week-over-week comparison"     
  src={patternAbnormalCompare}
/>

We see that the pattern has changed and that it appears to be worsening from last week’s comparison.

## Finding the source [#finding-source]

Once you’ve identified a negative change in recent behavior patterns you can then trace that behavior to its relative source.

This is where you refer back to the Problem Matrix. In most cases the problem is output performance in latency or errors that are impacting the user experience. The question is if the endpoint API application (the first application that receives the request) is causing the problem or is it a dependency behind that endpoint.

Find the application causing the latency or errors experienced by the end-user. This doesn’t mean the application code is the problem; however finding that application leads us close to the source. You can then rule-out components starting with code, database, configuration, resources, etc.

Today we can use Distributed tracing but first we must identify the source transaction. This means we must look at APM Transactions analysis in order identify if one, some, or all the transactions are affected by latency.

Once the impacted transactions are identified you would search for distributed traces of that transaction. Those distributed traces will give you a more complete and holistic view of the end-to-end services within that transaction group. Distributed tracing will help you more quickly identify where the latency is or where the errors are occurring.

Here’s a summary of the steps described above:

1. Examine applications supporting the affected capabilities using APM.
2. Identify if the problem is latency or errors.
3. If latency, identify if “web external” on the APM summary screen is causing the bulk of the latency.
4. If it is not “web external”, your problem is in this application or around the resources of this application. Begin ruling-out code, hardware, database, etc.
5. If it is “web external” then find the transaction or transactions affected and use Distributed tracing to examine the end-to-end flow for the source of the latency.

## Continuous improvement [#continuous-improvement]

This guide is by no means exhaustive. In fact, this is just a beginners guide in how to identify the most common performance issues with symptoms of latency and/or errors experienced by end-users.

The next step is to practice the process outlined in this guide. You can do this outside at your leisure by simply exploring service level degradations and applications at random within the New Relic platform. You will be surprised at the performance issues you may encounter even if service levels are not significantly impacted.

Ultimately, this guide helps you rule-out the first two most common issues commonly referred to as “back-end” performance and connectivity issues. This provides you with a data-driven analysis that can be used to quickly rule-out or rule-in client-side issues.
