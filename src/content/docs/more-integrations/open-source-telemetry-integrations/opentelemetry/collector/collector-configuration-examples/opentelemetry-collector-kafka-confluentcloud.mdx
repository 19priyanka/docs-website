---
title: Collector for Confluent Cloud & Kafka monitoring
tags:
  - Integrations
  - Open source telemetry integrations
  - OpenTelemetry
  - Kafka
  - Confluent Cloud
metaDescription: The OpenTelemetry collector is a central tool to collect, process, and export your telemetry.
---
You can collect metrics about your Confluent Cloud-managed Kafka deployment with the OpenTelemetry Collector. The collector is a component of OpenTelemetry that collects, processes, and exports telemetry data to New Relic (or any observability back-end).

If you're looking for help with other collector use cases, see the [newrelic-opentelemetry-examples](https://github.com/newrelic/newrelic-opentelemetry-examples) repository.


- [Signup for New Relic](#signup)
- [Prerequisites](#prerequisites)
- [Installing Opentelemetry Collectors](#installing-otel-collectors)
- [Configuring OteOpentelemetry Collectors](#configuring-teOpentelemetry-collectors)
- [Available Metrics from Confluent Cloud](#available-metrics-from-confluent-cloud)
- [Setting up Dashboards in New Relic](#setting-up-dashboards-in-new-relic)

## Step 1: Sign up for New Relic! [#signup]

* If you haven't already done so, sign up for a free [New Relic account](https://newrelic.com/signup).
* Get the [license key](https://one.newrelic.com/launcher/api-keys-ui.launcher) for the New Relic account to which you want to report data.

## Step 2: Prerequisites

* Ensure [go](https://go.dev/doc/install) is installed before proceeding. 
* Set [GOPATH](https://go.dev/doc/gopath_code) to $GOPATH/bin and added to the PATH variable.

## Step 3: Installing Opentelemetry Collectors

<Callout variant="important">
  New Relic supports the OpenTelemetry community by contributing our work upstream to both the [Core](https://github.com/open-telemetry/opentelemetry-collector) and [Contrib](https://github.com/open-telemetry/opentelemetry-collector-contrib)repos.
  
  When [PR14167](https://github.com/open-telemetry/opentelemetry-collector-contrib/pull/14167) on the [OpenTelemetry Collector Contrib repo](https://github.com/open-telemetry/opentelemetry-collector-contrib) has been merged, the documentation below will be updated to reflect the main branch of the Contrib repo.
</Callout>

### Compiling from PR source repo

See [https://github.com/abeach-nr/opentelemetry-collector-contrib.git](https://github.com/abeach-nr/opentelemetry-collector-contrib.git) for latest installation instructions.

```bash
$ git clone https://github.com/abeach-nr/opentelemetry-collector-contrib.git
$ cd opentelemetry-collector-contrib
$ make otelcontribcol

```

The binary will be installed under `./bin`

## Step 4: Configuring Opentelemetry Collectors 

Create a new file called `config.yaml` from the example. There a are a number of variables that must be replaced in the yaml

There are 2 api keys required for this configuration in addition to the New Relic key. A Confluent Cloud API Key, and a resorce specific key (for your cluster)

1. [Cloud API Key](https://docs.confluent.io/cloud/current/access-management/authenticate/api-keys/api-keys.html#cloud-cloud-api-keys)
	- CONFLUENT_API_ID
	- CONFLUENT_API_SECRET
2. [Kafka Client API Key](https://docs.confluent.io/cloud/current/access-management/authenticate/api-keys/api-keys.html#resource-specific-api-keys)
	- CONFLUENT_API_ID
	- CONFLUENT_API_SECRET
3. [New Relic Ingest Key](https://docs.newrelic.com/docs/apis/intro-apis/new-relic-api-keys/#license-key)
	- NEW RELIC LICENSE KEY
4. CLUSTER_ID
	- cluster id from confluent cloud
	- cluster key/secret should be specific to this cluster

```yaml
receivers:
  kafkametrics:
    brokers: 
      - 
    protocol_version: 2.0.0
    scrapers:
      - brokers
      - topics
      - consumers
    auth:
      sasl:
        username: {{ CONFLUENT_CLUSTER_API_ID }}
        password: {{ CONFLUENT_CLUSTER_SECRET }}
        mechanism: PLAIN
      tls:
        insecure_skip_verify: false
    collection_interval: 30s


  prometheus:
    config:
      scrape_configs:
        - job_name: "confluent"
          scrape_interval: 60s # Do not go any lower than this or you'll hit rate limits
          static_configs:
            - targets: ["api.telemetry.confluent.cloud"]
          scheme: https
          basic_auth:
            username: {{ CONFLUENT_API_ID }}
            password: {{ CONFLUENT_API_SECRET }}
          params:
            "resource.kafka.id":
              - {{ CLUSTER_ID }}
exporters:
  otlp:
    endpoint: https://otlp.nr-data.net:4317
    headers:
      api-key: {{ NEW RELIC LICENSE KEY }}
processors:
  batch:
  memory_limiter:
    limit_mib: 400
    spike_limit_mib: 100
    check_interval: 5s
service:
  telemetry:
    logs:
  pipelines:
    metrics:
      receivers: [prometheus]
      processors: [batch]
      exporters: [otlp]
    metrics/kafka:
      receivers: [kafkametrics]
      processors: [batch]
      exporters: [otlp]


```

## Step 5: Running the Collector

```bash
./bin/otelcontribcol_${OS}_amd64 --config config.yaml
```
* Replace ${OS} with `darwin|linux` as appropriate

## Step 6: Set up Dashboards in New Relic

[Here](./dashboard.json) is an example New Relic Dashboard that utilizes these metrics:


# Available Metrics

### Kafka instance metrics

| Name                                      | Description.                                                                
| ----------------------------------------- | ---------------------------------------------------------------------------- | 
| **kafka.brokers**                         | Number of brokers in the cluster                                             | 
| **kafka.brokers.consumer_fetch_rate_avg** | Average consumer fetch Rate                                                  |
| **kafka.brokers.incoming_byte_rate_avg**  | Average tncoming Byte Rate in bytes/second                                   |
| **kafka.brokers.outgoing_byte_rate_avg**  | Average outgoing Byte Rate in bytes/second                                   |
| **kafka.brokers.request_latency_avg**     | Request latency Average in ms                                                |
| **kafka.brokers.request_rate_avg**        | Average request rate per second                                              |
| **kafka.brokers.request_size_avg**        | Average request size in bytes                                                |
| **kafka.brokers.requests_in_flight**      | Requests in flight                                                           |
| **kafka.brokers.response_rate_avg**       | Average response rate per second                                             |
| **kafka.brokers.response_size_avg**       | Average response size in bytes                                               |
| **kafka.consumer_group.lag**              | Current approximate lag of consumer group at partition of topic              |
| **kafka.consumer_group.lag_sum**          | Current approximate sum of consumer group lag across all partitions of topic |
| **kafka.consumer_group.members**          | Count of members in the consumer group                                       |


### Confluent Cloud metrics
| Name                                                       | Description                                                                                                                                                                                   |
| ---------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **confluent_kafka_server_received_bytes**                  | The delta count of bytes of the customer's data received from the network. Each sample is the number of bytes received since the previous data sample. The count is sampled every 60 seconds. |
| **confluent_kafka_server_sent_bytes**                      | The delta count of bytes of the customer's data sent over the network. Each sample is the number of bytes sent since the previous data point. The count is sampled every 60 seconds.          |
| **confluent_kafka_server_received_records**                | The delta count of records received. Each sample is the number of records received since the previous data sample. The count is sampled every 60 seconds.                                     |
| **confluent_kafka_server_sent_records**                    | The delta count of records sent. Each sample is the number of records sent since the previous data point. The count is sampled every 60 seconds.                                              |    
| **confluent_kafka_server_retained_bytes**                  | The current count of bytes retained by the cluster. The count is sampled every 60 seconds.                                                                                                    |
| **confluent_kafka_server_active_connection_count**         | The count of active authenticated connections.                                                                                                                                                |
| **confluent_kafka_server_request_count**                   | The delta count of requests received over the network. Each sample is the number of requests received since the previous data point. The count sampled every 60 seconds.                      |
| **confluent_kafka_server_partition_count**                 | The number of partitions.																																								     |
| **confluent_kafka_server_successful_authentication_count** | The delta count of successful authentications. Each sample is the number of successful authentications since the previous data point. The count sampled every 60 seconds.                     |
| **confluent_kafka_server_consumer_lag_offsets**			 | The lag between a group member's committed offset and the partition's high watermark.	                                                                                                     |


