---
title: 'Streaming alerts: key terms and concepts'
tags:
  - Alerts and Applied Intelligence
  - New Relic Alerts
  - Get started
redirects:
  - /docs/alerts-applied-intelligence/new-relic-alerts/get-started/streaming-alerts-key-terms-concepts
---

The streaming alerts platform checks for violations based on data thatâ€™s present or missing in your stream of data, or [signal](/docs/using-new-relic/welcome-new-relic/get-started/glossary#signal), coming into New Relic. 

You can use [NRQL conditions](/docs/query-your-data/nrql-new-relic-query-language/get-started/introduction-nrql-new-relics-query-language) to control what part of the signal you want to be notified about. Your NRQL condition filters the data that's processed by the [streaming algorithm](/docs/using-new-relic/welcome-new-relic/get-started/glossary#streaming-algorithm).

There are three methods for aggregating the data filtered through your NRQL condition:

* Event flow (default)
* Event timer
* Cadence

## Why it matters [#why]

Understanding how streaming alerts works will help you fine-tune your NRQL conditions to be notified about what's important to you. 

![A diagram that demonstrates how data is streamed into New Relic.](./images/streaming-alerts.png "A diagram that demonstrates how data is streamed into New Relic.")

<figcaption>
  Only data that matches the conditions of the NRQL WHERE clause is alerted on. For more details on each step of the process, see [Streaming alerts process and descriptions](docs/alerts-applied-intelligence/new-relic-alerts/get-started/streaming-alerts-key-terms-concepts#streaming-table).
</figcaption>

As data streams into New Relic, it's filtered by the NRQL condition. Before data is evaluated, it must meet the criteria defined by the [NRQL query's `WHERE` clause](/docs/query-your-data/nrql-new-relic-query-language/get-started/nrql-syntax-clauses-functions#sel-where). Instead of evaluating that data immediately for violations, the NRQL alert conditions collect the data over a period of time known as the aggregation window. An additional offset delay allows for slower data to come through before it's processed.

Once the aggregation window plus the offset time have elapsed, New Relic groups the aggregated data into a single data point. Alerts then evaluates the data point using the NRQL condition in order to determine whether it meets the violation threshold criteria.

Even if a data point meets the criteria for a violation, a violation isn't triggered. A violation is only triggered when data points consistently meet the threshold criteria over a period of time. This is the threshold duration. If the data points are in violation for an entire threshold duration, we'll send you a notification based on your policy settings.

All of these configurable delays give you more control over how you're alerted on sporadic and missing data.

## Streaming alerts process and descriptions [#streaming-table]

<table class="alternate">
  <thead>
    <tr>
      <th style={{ width: "200px" }}>
        Process
      </th>

      <th>
        **Description**
      </th>
    </tr>
  </thead>
    <tbody>
    <tr>
      <td>
        Streaming data
      </td>
      <td>
        All data coming into New Relic.
      </td>
    </tr>
    
    <tr>
      <td>
        WHERE clause
      </td>
      <td>
        Filters all incoming streaming data. We only monitor for alerts on data that makes it through this filter.
      </td>
    </tr>

    <tr>
      <td>
        Aggregation methods
      </td>
      <td>
        One of three methods that control how data is collected before its evaluated.
      </td>
    </tr>

    <tr>
      <td>
        Delay/timer
      </td>
      <td>
        A time delay to ensure data points are placed in the correct aggregation window.
      </td>
    </tr>

    <tr>
      <td>
        Aggregated data
      </td>
      <td>
        Data in the aggregated window is collapsed to a single data point for alert evaluation.
      </td>
    </tr>

    <tr>
      <td>
        Evaluation
      </td>
      <td>
        The data point is evaluated by the NRQL condition, which is trigged by each incoming aggregated data point.
      </td>
    </tr>

    <tr>
      <td>
        Threshold duration
      </td>
      <td>
        A specific duration that determines if a violation is created. If your specified NRQL condition meets the threshold criteria over the threshold duration, a violation occurs. 
        
        When a data point lacks data, a custom value is inserted to fill the gap. 
      </td>
    </tr>
    </tbody>
</table>

## Choose your aggregation method [#aggregation-methods]

**Event flow** (default) works best for data that comes in frequently and mostly in order.

**Event timer** works best for data that arrives infrequently in batches, such as cloud integration data or infrequent error logs.

**Cadence** is the original aggregation method. It aggregates data at specific time intervals, regardless of data timestamps.

## Event flow detail [#event-flow]

Event flow starts aggregating data when the first data point arrives and for a specific custom duration, or window duration. A custom delay provides extra time for data to arrive. These times are based on the data's timestamps and not the wall clock time.

For example, suppose you're monitoring CPU usage in window durations of 1 minute and a 3 minute delay.

When a CPU usage data point comes in with a timestamp of 12:00pm, event flow will start aggregating CPU usage data with timestamps between 12:00pm and 12:01pm. When event flow receives a data point with a timestamp of 12:03pm, it sends the 12:00 to 12:01 data to be evaluated.

## Event timer detail [#event-timer]

Like event flow, event timer only starts aggregating data when data arrives. When a data point arrives, a timer starts to count down. If no data arrives before the timer counts down, the data is evaluated. When a data point arrives before the timer counts down, the timer is reset.

For example, suppose you're monitoring CloudWatch data that arrives fairly infrequently. You're using a window duration or 1 minute and a 3 minute timer.

When a CloudWatch data point comes in with a timestamp of 12:00pm, the timer will start to count down. At 12:03pm, if no other data with a timestamp between 12:00 and 12:01pm has arrived, the data with a 12:00pm timestamp is evaluated.

If a new data point with a timestamp between 12:00 and 12:01 arrives, the timer resets. It keeps resetting until data points for that window stop arriving.

If the timer for a later data point elapses before an earlier data point, event timer waits for the earlier timer to elapse before evaluating the later data point.

## Cadence [#cadence]

We recommend you use one of the other two methods.

## Streaming alerts tools [#tools]

Streaming alerts provide a set of tools you can use to more effectively alert on your streaming data, giving you greater control and reducing the number of false alerts notifications. They are:

* Loss of signal detection
* Gap filling
* Aggregation window duration customization
* Offset evaluation

<Callout variant="tip">
  This article covers these tools at a conceptual level. You'll find direct instructions on how to use these tools in [Create NRQL alert conditions](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions).
</Callout>

### Loss of signal detection [#signal-loss]

Loss of signal occurs when no data matches the NRQL condition over a specific period of time. A loss of signal is caused by different things. The `WHERE` clause in your NRQL query can filter out data before it's evaluated for violations. It could also mean a service or entity is offline or a periodic job has failed to run and no data is being sent to New Relic.

In order to avoid unnecessary notifications, you can choose how long to wait before you're notified by a loss of signal violation. You can use loss of signal detection to open violations and be notified when a signal is lost. Alternately, you can use a loss of signal to close violations for ephemeral services or sporadic data, such as error counts.

### Gap filling

Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with None, the last value received, or a static value. The default is `None`.

Gaps in streaming data can be caused by network or host issues, a signal may be sparse, or some signals, such as error counts, may only have data when something is wrong. By filling the gaps with known values, the alert evaluation process can process those gaps and determine how they should affect the loss of signal evaluation.

<Callout variant="tip">
  The alerts system fills gaps in actively reported signals. This signal history is dropped after 2 hours of inactivity. For gap filling, data points received after this period of inactivity are treated as new signals.

  To learn more about signal loss, gap filling, and how to request access to these features, see [this Explorers Hub post](https://discuss.newrelic.com/t/announcing-new-relic-one-streaming-alerts-for-nrql-conditions/115361).
</Callout>

### Aggregation window

In order to make loss of signal detection more effective and to reduce unnecessary notifications, you can customize aggregation windows to the duration that you need.

An [aggregation window](/docs/using-new-relic/welcome-new-relic/get-started/glossary#aggregation-window) is a specific block of time. We gather data points together in an aggregation window, before using the [aggregation function](/docs/using-new-relic/welcome-new-relic/get-started/glossary#aggregation-function) to evaluate the data. A longer aggregation window gives straggling data points more time to arrive before evaluating that data. When a data point arrives, its timestamp is used to put it in the proper aggregation window.

You can set your aggregation window to anything between 1 second and 15 minutes. The default is `1` minute.

### Offset evaluation

For data that consistently takes longer to arrive, you can use offset evaluation to consistently delay the NRQL condition evaluation. Waiting longer increases accuracy, but also increases latency.

The offset time value is the number of aggregation windows you want to use. The duration for each window is set in the Aggregation window field. For example, if your aggregation window is 1 minute and your Offset evaluation is 3, then a straggling data point will have about 3 minutes to arrive before it might be dropped. In other words, streaming alerts will always keep 3 aggregation windows in the queue, waiting 1 minute before evaluating each window and adding a new one.

The current default is `3`.
