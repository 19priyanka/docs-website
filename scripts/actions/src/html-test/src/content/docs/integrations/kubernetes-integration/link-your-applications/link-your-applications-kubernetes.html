
<p>You can surface Kubernetes metadata and <a href="/docs/integrations/kubernetes-integration/metadata-injection/kubernetes-apm-metadata-injection#agent-compatibility">link it to your APM agents</a> as <a href="https://docs.newrelic.com/docs/understand-dependencies/distributed-tracing/get-started/introduction-distributed-tracing">distributed traces</a> to explore performance issues and troubleshoot transaction errors. For more information, see this <a href="https://blog.newrelic.com/engineering/monitoring-application-performance-in-kubernetes/">New Relic blog post</a>.</p>
<div data-component="Callout" data-prop="W3sidHlwZSI6Im1keEF0dHJpYnV0ZSIsIm5hbWUiOiJ2YXJpYW50IiwidmFsdWUiOiJ0aXAifV0=">
  <p>Our Kubernetes metadata injection project is open source. Here's the <a href="https://github.com/newrelic/k8s-metadata-injection/">code to link APM and infrastructure data</a> and the <a href="https://github.com/newrelic/k8s-webhook-cert-manager">code to automatically manage certificates</a>.</p>
</div>
<h2>Compatibility and requirements [#compatibility]</h2>
<p>Before linking Kubernetes metadata to your APM agents, make sure you meet the following requirements:</p>
<ul>
  <li><a href="#kubernetes-req">Kubernetes requirements</a></li>
  <li><a href="#network-req">Network requirements</a></li>
  <li><a href="#agent-compatibility">APM agent compatibility</a></li>
  <li><a href="#openshift-req">OpenShift requirements</a></li>
</ul>
<h3>Kubernetes requirements [#kubernetes-req]</h3>
<p>To link your applications and Kubernetes, your cluster must have the <code>MutatingAdmissionWebhook</code> controller enabled, which requires <strong>Kubernetes 1.9 or higher</strong>.</p>
<p>To verify that your cluster is compatible, run the following command:</p>
<pre><code>kubectl api-versions | grep admissionregistration.k8s.io/v1beta1 
admissionregistration.k8s.io/v1beta1
</code></pre>
<p>If you see a different result, follow the Kubernetes documentation to <a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#how-do-i-turn-on-an-admission-controller">enable admission control in your cluster</a>.</p>
<h3>Network requirements [#network-req]</h3>
<p>For Kubernetes to speak to our <code>MutatingAdmissionWebhook</code>, the master node (or the API server container, depending on how the cluster is set up) should be allowed egress for HTTPS traffic on port 443 to pods in all of the other nodes in the cluster.</p>
<p>This might require specific configuration depending on how the infrastructure is set up (on-premises, AWS, Google Cloud, etc).</p>
<div data-component="Callout" data-prop="W3sidHlwZSI6Im1keEF0dHJpYnV0ZSIsIm5hbWUiOiJ2YXJpYW50IiwidmFsdWUiOiJ0aXAifV0=">
  <p>Until Kubernetes v1.14, users were only allowed to register admission webhooks on port 443. Since v1.15 it's possible to register them on different ports. To ensure backward compatibility, the webhook is registered by default on port 443 in the YAML config file we distribute.</p>
</div>
<h3>APM agent compatibility [#agent-compatibility]</h3>
<p>The following New Relic agents collect Kubernetes metadata:</p>
<ul>
  <li><a href="/docs/agents/go-agent/installation/install-new-relic-go#get-new-relic">Go 2.3.0 or higher</a></li>
  <li><a href="/docs/agents/java-agent/installation/upgrade-java-agent">Java 4.10.0 or higher</a></li>
  <li><a href="/docs/agents/nodejs-agent/installation-configuration/upgrade-nodejs-agent">Node.js 5.3.0 or higher</a></li>
  <li><a href="/docs/agents/python-agent/installation-configuration/upgrade-python-agent">Python 4.14.0 or higher</a></li>
  <li><a href="/docs/agents/ruby-agent/installation/upgrade-ruby-agent-versions">Ruby 6.1.0 or higher</a></li>
  <li><a href="/docs/agents/net-agent/installation/update-net-agent">.NET 8.17.438 or higher</a></li>
</ul>
<h3>Openshift requirements [#openshift-req]</h3>
<p>To link Openshift and Kubernetes you must enable mutating admission webhooks, which requires <strong>Openshift 3.9 or higher</strong>.</p>
<ol>
  <li>
    <p>During the process, install a resource that requires <code>admin</code> permissions to the cluster. Run this to log in as admin:</p>
    <pre><code>oc login -u system:admin
</code></pre>
  </li>
  <li>
    <p>Check that webhooks are correctly configured. If they are not, update the <code>master-config.yaml</code> file.</p>
    <pre><code>admissionConfig:
           pluginConfig:
             MutatingAdmissionWebhook:
               configuration:
                 apiVersion: apiserver.config.k8s.io/v1alpha1
                 kubeConfigFile: /dev/null
                 kind: WebhookAdmission
             ValidatingAdmissionWebhook:
               configuration:
                 apiVersion: apiserver.config.k8s.io/v1alpha1
                 kubeConfigFile: /dev/null
                 kind: WebhookAdmission
                 location: ""
</code></pre>
    <div data-component="Callout" data-prop="W3sidHlwZSI6Im1keEF0dHJpYnV0ZSIsIm5hbWUiOiJ2YXJpYW50IiwidmFsdWUiOiJpbXBvcnRhbnQifV0=">
      <p>Add <code>kubeConfigFile: /dev/null</code> to address <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1635918">some issues in Openshift</a>.</p>
    </div>
  </li>
  <li>
    <p>Enable certificate signing by editing the YAML file and updating your configuration:</p>
    <pre><code>kubernetesMasterConfig:
   controllerArguments:
      cluster-signing-cert-file:
      - "/etc/origin/master/ca.crt"
      cluster-signing-key-file:
      - "/etc/origin/master/ca.key"
</code></pre>
  </li>
  <li>
    <p>Restart the Openshift services in the master node.</p>
  </li>
</ol>
<h2>Configure the injection of metadata [#configure-injection]</h2>
<p>By default, all the pods you create that include APM agents have the correct environment variables set and the metadata injection applies to the entire cluster. To check that the environment variables have been set, any container that is running must be stopped, and a new instance started (see <a href="/docs/integrations/kubernetes-integration/metadata-injection/kubernetes-apm-metadata-injection#validate-injection">Validate the injection of metadata</a>).</p>
<p>This default configuration also uses the <a href="https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/">Kubernetes certificates API</a> to automatically manage the certificates required for the injection. If needed, you can limit the injection of metadata to specific namespaces in your cluster or self-manage your certificates.</p>
<h3>Default configuration</h3>
<p>To proceed with the default injection of metadata, follow these steps:</p>
<ol>
  <li>
    <p>Download the YAML file:</p>
    <pre><code>curl -O http://download.newrelic.com/infrastructure_agent/integrations/kubernetes/k8s-metadata-injection-latest.yaml
</code></pre>
  </li>
  <li>
    <p>Replace YOUR_CLUSTER_NAME with the name of your cluster in the YAML file.</p>
  </li>
  <li>
    <p>Apply the YAML file to your Kubernetes cluster:</p>
    <pre><code>kubectl apply -f k8s-metadata-injection-latest.yaml
</code></pre>
  </li>
</ol>
<h3>Custom configuration</h3>
<p>You can limit the injection of metadata only to specific namespaces by using labels.</p>
<p>To enable this feature, edit your YAML file by finding and uncommenting the following lines:</p>
<pre><code># namespaceSelector:
  #   matchLabels:
  #     newrelic-metadata-injection: enabled
</code></pre>
<p>With this option, injection is only applied to those namespaces that have the <code>newrelic-metadata-injection</code> label set to <code>enabled</code>:</p>
<pre><code>kubectl label namespace YOUR_NAMESPACE newrelic-metadata-injection=enabled
</code></pre>
<h2>Manage custom certificates [#option-activation]</h2>
<p>To use custom certificates you need a specific YAML file:</p>
<ol>
  <li>
    <p>Download the YAML file without automatic certificate management:</p>
    <pre><code>curl -O http://download.newrelic.com/infrastructure_agent/integrations/kubernetes/k8s-metadata-injection-custom-certs-latest.yaml
</code></pre>
  </li>
  <li>
    <p>Replace YOUR_CLUSTER_NAME with the name of your cluster in the YAML file.</p>
  </li>
  <li>
    <p>Apply the YAML file to your Kubernetes cluster:</p>
    <pre><code>kubectl apply -f k8s-metadata-injection-custom-certs-latest.yaml
</code></pre>
  </li>
</ol>
<p>Once you have the correct YAML file, you can proceed with the custom certificate management option.</p>
<p>You need your certificate, server key, and Certification Authority (CA) bundle encoded in PEM format.</p>
<ul>
  <li>
    <p>If you have them in the standard certificate format (X.509), install <code>openssl</code>, and run the following:</p>
    <pre><code>openssl x509 -in CERTIFICATE_FILENAME -outform PEM -out CERTIFICATE_FILENAME.pem 
openssl x509 -in SERVER_KEY_FILENAME -outform PEM -out SERVER_KEY_FILENAME.pem 
openssl x509 -in CA_BUNDLE_FILENAME -outform PEM -out BUNDLE_FILENAME.pem
</code></pre>
  </li>
  <li>
    <p>If your certificate/key pair are in another format, see the <a href="https://knowledge.digicert.com/solution/SO26449.html">Digicert knowledgebase</a> for more help.</p>
  </li>
</ul>
<p>Create the TLS secret with the signed certificate/key pair, and patch the mutating webhook configuration with the CA using the following commands:</p>
<pre><code>kubectl create secret tls newrelic-metadata-injection-secret \
--key=PEM_ENCODED_SERVER_KEY \
--cert=PEM_ENCODED_CERTIFICATE \
--dry-run -o yaml |
kubectl -n default apply -f -

caBundle=$(cat PEM_ENCODED_CA_BUNDLE | base64 | td -d '\n')
kubectl patch mutatingwebhookconfiguration newrelic-metadata-injection-cfg --type='json' -p "[{'op': 'replace', 'path': '/webhooks/0/clientConfig/caBundle', 'value':'${caBundle}'}]"
</code></pre>
<div data-component="Callout" data-prop="W3sidHlwZSI6Im1keEF0dHJpYnV0ZSIsIm5hbWUiOiJ2YXJpYW50IiwidmFsdWUiOiJpbXBvcnRhbnQifV0=">
  <p>Certificates signed by Kubernetes have an expiration of one year. For more information, see the <a href="https://github.com/kubernetes/kubernetes/blob/1b28775db1290a772967d192a19a8ec447053cd5/pkg/controller/apis/config/v1alpha1/defaults.go#L118">Kubernetes source code in GitHub</a>.</p>
</div>
<h2>Validate the injection of metadata [#validate-injection]</h2>
<p>In order to validate that the webhook (responsible for injecting the metadata) was installed correctly, deploy a new pod and check for the New Relic environment variables.</p>
<ol>
  <li>
    <p>Create a dummy pod containing Busybox by running:</p>
    <pre><code>kubectl create -f https://git.io/vPieo
</code></pre>
  </li>
  <li>
    <p>Check if New Relic environment variables were injected:</p>
    <pre><code>kubectl exec busybox0 -- env | grep NEW_RELIC_METADATA_KUBERNETES

NEW_RELIC_METADATA_KUBERNETES_CLUSTER_NAME=fsi
NEW_RELIC_METADATA_KUBERNETES_NODE_NAME=nodea
NEW_RELIC_METADATA_KUBERNETES_NAMESPACE_NAME=default
NEW_RELIC_METADATA_KUBERNETES_POD_NAME=busybox0
NEW_RELIC_METADATA_KUBERNETES_CONTAINER_NAME=busybox
</code></pre>
  </li>
</ol>
<h2>Disable the injection of metadata [#uninstall]</h2>
<p>To disable/uninstall the injection of metadata, use the following commands:</p>
<ol>
  <li>
    <p>Delete the Kubernetes objects using the <code>yaml</code> file:</p>
    <pre><code>kubectl delete -f k8s-metadata-injection-latest.yaml
</code></pre>
  </li>
  <li>
    <p>Delete the <code>TLS secret</code> containing the certificate/key pair:</p>
    <pre><code>kubectl delete secret/newrelic-metadata-injection-secret
</code></pre>
  </li>
</ol>
<h2>Troubleshooting [#metadata-troubleshooting]</h2>
<p>Follow these troubleshooting tips as needed.</p>
<div>
  <p>No Kubernetes metadata in APM or distributed tracing transactions</p>
</div>
<div>
  <p><strong>Problem</strong></p>
  <p>In Kubernetes cluster version 1.19, the creation of the secret might fail depending on the <code>kubectl</code> version currently available in the container image you use. This is a known issue currently tracked <a href="https://github.com/newrelic/k8s-webhook-cert-manager/issues/31">here</a>.</p>
  <p><strong>Workaround</strong></p>
  <ol>
    <li>Manually run the steps <a href="https://github.com/newrelic/k8s-webhook-cert-manager/blob/master/generate_certificate.sh">in this script</a> in order to create and sign a certificate.</li>
    <li>Add this certificate to the <code>kubernetes.io/tls</code> secret that the <code>nri-metadata-injection</code> pod is trying to mount.</li>
    <li>Once the certificate is signed, the <code>mutatingwebhookconfiguration</code> is patched with the CA used to sign the certificate.</li>
    <li>
      Use the CA that's appropriate for the cluster configuration you're working with; usually, you can find it in any running Pod of the cluster under <code>/run/secrets/kubernetes.io/serviceaccount/ca.cr</code>.
      In an OpenShift 4.x environment, you can usually find the CA in the secret <code>csr-signer</code> in the namespace <code>openshift-kube-controller-manager</code>.
    </li>
  </ol>
</div>
<div>
  <p><strong>Problem</strong></p>
  <p>In OpenShift version 4.x, the CA that is used in order to patch the <code>mutatingwebhookconfiguration</code> resource is not the one used when signing the certificates. This is a known issue currently tracked <a href="https://github.com/newrelic/k8s-webhook-cert-manager/issues/33">here</a>.</p>
  <p>In the logs of the Pod <code>nri-metadata-injection,</code> you'll see the following error message:</p>
  <pre><code>TLS handshake error from 10.131.0.29:37428: remote error: tls: unknown certificate authority
TLS handshake error from 10.129.0.1:49314: remote error: tls: bad certificate
</code></pre>
  <p><strong>Workaround</strong></p>
  <ol>
    <li>Manually update the certificate stored in the <code>mutatingwebhookconfiguration</code> object.<br>The correct CA locations might change according to the cluster configuration. However, you can usually find the CA in the secret <code>csr-signer</code> in the namespace <code>openshift-kube-controller-manager</code>.</li>
  </ol>
</div>
<div>
  <p><strong>Problem</strong></p>
  <p>There is no Kubernetes metadata included in the transactions' attributes of your APM agent or in distributed tracing.</p>
  <p><strong>Solution</strong></p>
  <ol>
    <li>
      <p>Verify that the environment variables are being correctly injected by following the instructions described in the <a href="/docs/integrations/kubernetes-integration/metadata-injection/kubernetes-apm-metadata-injection#validate-injection">Validate your installation</a> step.</p>
    </li>
    <li>
      <p>If they are not present, get the name of the metadata injection pod by running:</p>
      <pre><code>kubectl get pods | grep newrelic-metadata-injection-deployment
kubectl logs -f pod/podname
</code></pre>
    </li>
    <li>
      <p>In another terminal, create a new pod (for example, see <a href="/docs/integrations/kubernetes-integration/metadata-injection/kubernetes-apm-metadata-injection#validate-injection">Validate your installation</a>), and inspect the logs of the metadata injection deployment for errors. For every created pod there should be a set of 4 new entries in the logs like:</p>
      <pre><code>{"level":"info","ts":"2020-04-09T12:55:32.107Z","caller":"server/main.go:139","msg":"POST https://newrelic-metadata-injection-svc.default.svc:443/mutate?timeout=30s HTTP/2.0\" from 10.11.49.2:32836"}
{"level":"info","ts":"2020-04-09T12:55:32.110Z","caller":"server/webhook.go:168","msg":"received admission review","kind":"/v1, Kind=Pod","namespace":"default","name":"","pod":"busybox1","UID":"6577519b-7a61-11ea-965e-0e46d1c9335c","operation":"CREATE","userinfo":{"username":"admin","uid":"admin","groups":["system:masters","system:authenticated"]}}
{"level":"info","ts":"2020-04-09T12:55:32.111Z","caller":"server/webhook.go:182","msg":"admission response created","response":"[{\"op\":\"add\",\"path\":\"/spec/containers/0/env\",\"value\":[{\"name\":\"NEW_RELIC_METADATA_KUBERNETES_CLUSTER_NAME\",\"value\":\"adn_kops\"}]},{\"op\":\"add\",\"path\":\"/spec/containers/0/env/-\",\"value\":{\"name\":\"NEW_RELIC_METADATA_KUBERNETES_NODE_NAME\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"spec.nodeName\"}}}},{\"op\":\"add\",\"path\":\"/spec/containers/0/env/-\",\"value\":{\"name\":\"NEW_RELIC_METADATA_KUBERNETES_NAMESPACE_NAME\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}},{\"op\":\"add\",\"path\":\"/spec/containers/0/env/-\",\"value\":{\"name\":\"NEW_RELIC_METADATA_KUBERNETES_POD_NAME\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.name\"}}}},{\"op\":\"add\",\"path\":\"/spec/containers/0/env/-\",\"value\":{\"name\":\"NEW_RELIC_METADATA_KUBERNETES_CONTAINER_NAME\",\"value\":\"busybox\"}},{\"op\":\"add\",\"path\":\"/spec/containers/0/env/-\",\"value\":{\"name\":\"NEW_RELIC_METADATA_KUBERNETES_CONTAINER_IMAGE_NAME\",\"value\":\"busybox\"}}]"}
{"level":"info","ts":"2020-04-09T12:55:32.111Z","caller":"server/webhook.go:257","msg":"writing response"}
</code></pre>
      <p>If there are no new entries on the logs, it means that the apiserver is not being able to communicate with the webhook service, this could be due to networking rules or security groups rejecting the communication.</p>
    </li>
    <li>
      <p>To check if the apiserver is not being able to communicate with the webhook you should inspect the apiserver logs for errors like:</p>
      <pre><code>failed calling webhook "metadata-injection.newrelic.com": ERROR_REASON
</code></pre>
      <p>To get the apiserver logs:</p>
      <ol>
        <li>
          <p>Start a proxy to the Kubernetes API server by the executing the following command in a terminal window and keep it running.</p>
          <pre><code>kubectl proxy --port=8001
</code></pre>
        </li>
        <li>
          <p>Create a new pod in your cluster, this will make the apiserver try to communicate with the webhook. The following command will create a busybox.</p>
          <pre><code>kubectl create -f https://git.io/vPieo
</code></pre>
        </li>
        <li>
          <p>Retrieve the apiserver logs.</p>
          <pre><code>curl localhost:8001/logs/kube-apiserver.log > apiserver.log
</code></pre>
        </li>
        <li>
          <p>Delete the busybox container.</p>
          <pre><code>kubectl delete -f https://git.io/vPieo
</code></pre>
        </li>
        <li>
          <p>Inspect the logs for errors.</p>
          <pre><code>grep -E 'failed calling webhook' apiserver.log
</code></pre>
        </li>
      </ol>
      <p>Remember that one of the <a href="#network-req">requirements for the metadata injection</a> is that the apiserver must be allowed egress to the pods running on the cluster. If you encounter errors regarding connection timeouts or failed connections, make sure to check the security groups and firewall rules of the cluster.</p>
    </li>
    <li>
      <p>If there are no log entries in either the apiserver logs or the metadata injection deployment, it means that the webhook was not properly registered.</p>
    </li>
    <li>
      <p>Ensure the metadata injection setup job ran successfully by inspecting the output of:</p>
      <pre><code>kubectl get job newrelic-metadata-setup
</code></pre>
    </li>
    <li>
      <p>If the job is not completed, investigate the logs of the setup job:</p>
      <pre><code>kubectl logs job/newrelic-metadata-setup
</code></pre>
    </li>
    <li>
      <p>Ensure the <code>CertificateSigningRequest</code> is approved and issued by running:</p>
      <pre><code>kubectl get csr newrelic-metadata-injection-svc.default
</code></pre>
    </li>
    <li>
      <p>Ensure the TLS secret is present by running:</p>
      <pre><code>kubectl get secret newrelic-metadata-injection-secret
</code></pre>
    </li>
    <li>
      <p>Ensure the CA bundle is present in the mutating webhook configuration:</p>
      <pre><code>kubectl get mutatingwebhookconfiguration newrelic-metadata-injection-cfg -o json
</code></pre>
    </li>
    <li>
      <p>Ensure the <code>TargetPort</code> of the <strong>Service</strong> resource matches the <strong>Port</strong> of the <strong>Deployment</strong>'s container:</p>
      <pre><code>kubectl describe service/newrelic-metadata-injection-svc
kubectl describe deployment/newrelic-metadata-injection-deployment
</code></pre>
    </li>
  </ol>
</div>
